{"id": "1603.06531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Deep video gesture recognition using illumination invariants", "abstract": "In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.", "histories": [["v1", "Mon, 21 Mar 2016 18:33:29 GMT  (2032kb,D)", "http://arxiv.org/abs/1603.06531v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["otkrist gupta", "dan raviv", "ramesh raskar"], "accepted": false, "id": "1603.06531"}, "pdf": {"name": "1603.06531.pdf", "metadata": {"source": "META", "title": "Deep video gesture recognition using illumination invariants", "authors": ["Otkrist Gupta", "Dan Raviv", "Ramesh Raskar"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we present architectures based on deep neural networks for gesture recognition in videos that are invariant to local scaling. We fuse autocoder and predictor architectures using an adaptive weighting scheme that can handle a scaled-down dataset while enriching our models from enormous unlabeled sets. We improve the robustness of light conditions by introducing a new adaptive filter based on the normalization of the local temporal scale. We provide better results than known methods, including newer approaches based on neural networks. Keywords: deep learning, gesture recognition, video classification, neural networks, machine learning."}, {"heading": "1 Introduction", "text": "In fact, studies have shown that non-verbal communication is responsible for more than half of all social interactions [Frith 2009]. Therefore, the study of facial features is crucial in areas such as sociology, psychology and the automated recognition of gestures aimed at creating more user-friendly software and user agents in these areas. As a technology, we spend large amounts of our time looking at screens, interacting with computers and mobile phones. Despite their widespread use, most software interfaces are not yet verbal, impersonal, primitive and explained."}, {"heading": "1.1 Contributions", "text": "1. We develop a scale invariant architecture to generate illumination invariant depth motion characteristics. 2. We report on the state of the art in video gesture recognition using spatio-temporal Convolutionary Neural Networks. 3. We introduce an improved topology and protocol for semi-supervised learning where the number of marked data points is only a fraction of the entire dataset."}, {"heading": "2 Related Work", "text": "In the past, similar facial expression recognition methods have been used [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015; Kahou et al. 2015; Chen et al. 2015] and the combination of multiple kernel-based approaches [Liu et al. 2014c; Senechal et al. 2015], with temporal characteristics and diverse learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al.] and the combination of multiple kernel-based approaches [Liu et al. 2014c; Senechal et al al al al al al al al al al al. 2015]."}, {"heading": "3 Method", "text": "Our facial expression recognition pipeline includes the Viola-Jones algorithm [Viola and Jones 2004] for face recognition followed by a deep Convolutionary Neural Network for predicting expressions. The Deep Convolutionary Network combines an autoencoder with a predictor based on the semi-monitored learning paradigm. Autoencoder's neural network takes input videos with 9 frames of size 145 x 145 and produces output of 145 x 145 x 9 tensors. Neural network sources of the predictor are the innermost hidden layer of the autoencoder and use a cascade of fully connected layers accompanied by the Softmax layer to classify expressions. Since videos can vary in size and duration, they must be scaled in time and space using standard interpolation techniques."}, {"heading": "3.1 Autoencoder", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.2 Semi-Supervised Learner", "text": "Our predictor neural network consists of a combination of several revolutionary layers followed by several fully connected layers that end in a softmax logistic regression layer for prediction. Architecture can be described as C (96, 11, 3) \u2212 N \u2212 C (256, 5, 2) \u2212 N \u2212 C (384, 3, 2) \u2212 N \u2212 FC (8192) \u2212 FC (4096) \u2212 FC (500) \u2212 FC (8) with shorthand notation in Section 3.1. Note that our autocode architecture is superimposed on the top of the predictor architecture by adding deconvolutionary layers after the first fully connected layer to create a semi-supervised topology capable of training both the autocoder and the predictor together (see Figure 3). We use autocoders to initiate the weights for all the revolutionary layers, all the deconvolution layers, and the centrally connected layers, and all the remaining layers."}, {"heading": "3.3 Illumination Invariant Learner", "text": "We introduce scale invariance to pixel intensities by adding additional layers as illumination invariant neural networks (\u03b2 \u03b2 | \u03b2 | p). The illumination invariant layers comprise a revolutionary layer, an absolute value layer, a reciprocal layer followed by a Hadamard product layer. Scale invariance is achieved by applying an elementary multiplication between the initial layers of the proposed architecture and the original input layer. This normalization can be described as C (9, 1, 9) \u2212 Abs \u2212 Log (\u03b1, \u03b2) \u2212 Exp (-) \u2212 Prod (x1, x2) \u2212 Prod (see short notation in section 3.1). Here, C (9, 9) refers to the first revolutionary layer containing 9 filters of size 1 \u00d7 1 in the spatial domain and a size of 9 in the time domain."}, {"heading": "4 Datasets and Implementation", "text": "Amazingly, high-quality facial expression datasets are hard to find. Just as the majority of facial expression algorithms focus on stills, most facial facial datasets focus on algorithm 1. Create folding layer for scale invariant subnets. Input: Total number of frames nFrames, window size wSize Output: Caffe Weight Matrix W1: function AUTOENCODERWEIGHTS (nFrames, wSize) 2: r \u2190 (wSize \u2212 1) / 2 3: A \u2190 zeros (nFrames, nFrames) 4: for (i \u2190 0; i < nFrames; nFrames; i + + +) 5: n \u2190 min (n, r) 6: n frames min (n, r) min (n, r) min (n, r) 6: < < < 5: < < i i i i i: i + n: i \u2212 nFrames nullen (nFrames, nlt; nFrames min, r) nFrames nulhTS (n, nFrames) 7: < <"}, {"heading": "4.1 Autoencoder dataset", "text": "To train the unattended component of our neural network, we required a large amount of data to ensure that the deep features were general enough to represent any facial expression; we trained the deep-entangled auto encoder to generate a massive collection of unlabeled data points with 6.5 million video clips with 25 frames per clip; the clips were generated by using a Viola Jones facial algorithm to detect and isolate facial boundary fields on publicly available videos; and we improved the data quality by removing any clips that showed large discrepancies between consecutive frames, eliminating video clips that capture random occurrence of occurrences, rapid facial movements, or the sudden appearance of another face.As an additional step, we obtained information about facial posture through the use of active appearance models and generation of facial landmarks [Asthana et al. 2014], we allowed the facial landmarks to be limited to our 3D, or face landmarks."}, {"heading": "4.2 Cohn Kanade Dataset", "text": "The dataset by Cohn Kanade [Lucey et al. 2010] is one of the oldest and best known datasets of facial expression video clips. It contains a total of 593 video clip sequences, of which 327 clips are labeled for seven basic emotions (most of which are posed). Clips contain the frontal view of the face performing facial expressions that vary from neutral expression to maximum intensity of emotion."}, {"heading": "4.3 MMI Dataset", "text": "The MMI Facial Expression Data Set [Pantic et al. 2005] includes ongoing efforts to visualize both simulated and induced facial expressions. The data set includes 2894 video samples, of which approximately 200 video clips are labeled for six basic emotions. Clips include faces ranging from blank expression to peak emotion to neutral facial gesture. MMI, which originally contained only posed facial expressions, has recently been expanded to include natural versions of happiness, disgust, and surprise [Valstar and Pantic 2010]."}, {"heading": "4.4 Florentine dataset", "text": "The application was developed in the Python programming language and we used well-known libraries such as OpenCV for video recordings and annotations. The database contains facial clips of 160 subjects (male and female), where gestures were artificially created according to a specific requirement or were actually given based on a stimulus shown. 1032 clips for facial expressions and 1745 clips for induced facial expressions were recorded, a total of 2777 video clips. Real facial expressions were generated in subjects using visual stimuli, i.e. videos randomly selected from a Youtube video bank to generate a specific emotion. See Table 2 to see the distribution of the database, where created clips refer to artificially generated facial expressions and undirected references to the stimulus activation process."}, {"heading": "5 Experiments and results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Video autoencoder", "text": "Since deep autoencoders can show slow convergence when trained by randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to iteratively train stacked autoencoder layers [Carreira-Perpinan and Hinton 2005]. Initially, we trained the beginning and end of revolutionary layers by creating an intermediate neural network (C (96, 11, 3) \u2212 N \u2212 C (256, 5, 2) \u2212 N \u2212 DC (384, 3, 2) and trained it on facial video clips. Inner layers were successively trained by adding them to the intermedial neural network and fixing pre-trained layers until the convergence of weights was achieved. To achieve best results, we refined the entire network at the end of each iteration."}, {"heading": "5.2 Semi-Supervised predictor", "text": "The architecture of the neural predictor network can be written as FC (8192) \u2212 FC (4096) \u2212 FC (1000) \u2212 FC (500) \u2212 FC (8). The full semi-monitored neural network includes an autoencoder and a predictor that share neural connections and can be trained on the same input at the same time. Weights from the auto encoder training were used to initialize weights of the semi-monitored predictor, which were later fine-tuned using selected inputs from datasets described in Section 3.2. Weights from this step will be used to initialize our weighing invariant predictor, which we will describe next."}, {"heading": "5.3 Illumination-Invariant Semi-Supervised predictor", "text": "We test our method on three sets of data (MMI, CK and Florentine), randomly dividing each of them into subsets of traction, test and validation that do not overlap; our training dataset contains 50% inputs, while test and validation datasets contain 30% and 20% of inputs; after the split, we increase the size of the training dataset by adding rotation, translation or image flipping; for quantitative analysis, we compare our results with expression dataset bases [Liu et al. 2014b] and several core methods [Liu et al. 2014c]; we use sources we have downloaded from Visual Data Transforming and Resources [Sources] without these data being available as a contrast to our strategies; for a reasonable comparison, we use the same partitioning techniques while comparing our techniques with external methods."}, {"heading": "6 Discussions and future work", "text": "In this paper, we present a framework for facial gesture recognition that combines semi-monitored learning approaches with carefully designed neural network topologies. We demonstrate how to induce illumination invariance by incorporating specialized layers and using spatio-temporal convolutions to extract features from multiple picture frames. Currently, our system relies on the use of Viola-Jones to distinguish and segment faces, and confines itself to analyzing only the frontal views. Wildlife emotion recognition remains an elusive problem with low reported accuracies that we hope will be addressed in future work. In this work, we only look at video frames, but other, richer modalities could be taken into account. Sound has a direct impact on emotional status and can improve our current system. Higher refresh rates, multi-resolution in space and time, or interactions between our subjects are among the few that can enrich our data."}, {"heading": "6.1 Limitations", "text": "In this section, we examine the limitations of our system and discuss where our system could fail or be of lesser value. One of our biggest limitations is that the system was built and tested only using frontal perspectives, imposing a limitation on the facial alignment of input. Furthermore, the pipeline takes a fixed number of video images as input, which imposes a limitation on the minimum number of frames required for detection. We limit individual frames to a fixed size of 140 x 140 and higher resolution frames, which can result in loss of information. Both spatial and temporal limitations can be improved by increasing the size of the neural network at the expense of resource calculation. Learning for deep neural networks can be extremely computationally intensive and can impose massive limitations on the systemic space-time complexity, which is no different and requires specialized hardware (NVIDIA TeslaTMor 40 Grid TMUs) to train an additional number of graphics cards for the millions of PSIM cards that require a lower number of GPSIM."}, {"heading": "7 Conclusions", "text": "This work uses semi-supervised paradigms in Convolutionary Neural Networks to classify facial gestures in video sequences. Our topologies are trained on millions of facial video clips and use spatio-temporal convolutions to extract transient features in videos. We developed a new scale invariant subnet that showed superior gesture recognition results under variable light conditions. We demonstrate the effectiveness of our approach in both publicly available data sets and samples we collected."}, {"heading": "ANDRE\u0301, E., KLESEN, M., GEBHARD, P., ALLEN, S., AND RIST,", "text": "T. 2000. Using Personality and Emotional Models to Control the Behavior of Animated Interactive Agents. Workshop on Achieving Human-Like Behavior in Interactive Animated Agents, 3-7."}, {"heading": "ASTHANA, A., ZAFEIRIOU, S., CHENG, S., AND PANTIC, M.", "text": "2014. Incremental Face Adaptation in the Wild. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1859-1866."}, {"heading": "BENGIO, Y., ROUX, N. L., VINCENT, P., DELALLEAU, O., AND", "text": "MARCOTTE, P. 2005. Convex Neural Networks. In Advances in Neural Information Processing Systems, 123-130.CARREIRA-PERPINAN, M. A., AND HINTON, G. E. 2005. In Proceedings of International Workshop on Artificial Intelligence and Statistics, 33-40."}, {"heading": "CASSELL, J., PELACHAUD, C., BADLER, N., STEEDMAN, M.,", "text": "ACHORN, B., BECKET, T., DOUVILLE, B., PREVOST, S., AND STONE, M. 1994. Animated conversation: rule-based generation of facial expression, gesture and spoken intonation for multiple interlocutors. In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, ACM, 413-420.CEREZO, E., BALDASSARRI, S., AND SERON, F. 2007. Interactive agents for multimodal emotional user interaction. Multi Conferences on Computer Science and Information Systems, 35- 42.CHEN, H., LI, J., ZHANG, F., LI, Y., AND WANG, H. 2015. 3d model-based continuous emotion recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1836-1845."}, {"heading": "COWIE, R., DOUGLAS-COWIE, E., TSAPATSOULIS, N., VOTSIS, G., KOLLIAS, S., FELLENZ, W., AND TAYLOR, J. G.", "text": "2001. Emotion recognition in human-computer interaction. Signal Processing Magazine, IEEE 18, 1, 32-80."}, {"heading": "DHALL, A., ASTHANA, A., GOECKE, R., AND GEDEON, T.", "text": "2011. Emotion recognition using PHOG and LPQ features. In International Conference on Automatic Face & Gesture Recognition, IEEE, 878-883.FRITH, C. 2009. Role of facial expression in social interactions. Philosophical Transactions of the Royal Society B: Biological Sciences 364, 1535, 3453-3458.GARGESHA, M., AND KUCHI, P. 2002."}, {"heading": "HE, L., JIANG, D., YANG, L., PEI, E., WU, P., AND SAHLI,", "text": "H. 2015. Multimodal affective dimensional prediction using deep bidirectional recurrent neuronal long-term memories. In Proceedings of the 5th International Workshop on Audio / Visual Emotion Challenge, ACM, 73-80.HINTON, G. E., AND SALAKHUTDINOV, R. R. 2006. Reducing the dimensionality of data with neural networks. Science 313, 5786, 504-507.HINTON, G. E., OSINDERO, S., AND TEH, Y.-W. 2006. A fast learning algorithm for deep faith networks. Neural Computation 18, 7, 1527-1554."}, {"heading": "JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG,", "text": "J., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv: 1408.5093.JUNG, H., LEE, S., YIM, J., PARK, S., AND KIM, J. 2015. Shared fine-tuning in deep neural networks for facial recognition. In Proceedings of the IEEE International Conference on Computer Vision, 2983-2991."}, {"heading": "KAHOU, S. E., PAL, C., BOUTHILLIER, X., FROUMENTY, P.,", "text": "In Proceedings of the 15th ACM on International conference on multimodal interaction, ACM, 543-550. GU-LC, EHRE, C, MEMISEVIC, R., VINCENT, P., COURVILLE, A., BENGIO, Y., AND FERRARI, R. C. 2013."}, {"heading": "KAHOU, S. E., BOUTHILLIER, X., LAMBLIN, P., GULCEHRE,", "text": "C., MICHALSKI, V., KONDA, K., JEAN, S., FROUMENTY, P., DAUPHIN, Y., BOULANGER-LEWANDOWSKI, N., ET AL. 2015. Emonets: Multimodal deep learning approaches for emotion recognition in video. Journal on Multimodal User Interfaces, 1-13.KARPATHY, A., TODERICI, G., SHETTY, S., LEUNG, T., SUKTHANKAR, R., AND FEI-FEI, L. 2014. Large-scale video classification using conventional neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1725 - 1732."}, {"heading": "KINGMA, D. P., MOHAMED, S., REZENDE, D. J., AND", "text": "WELLING, M. 2014. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, 3581-3589.KLEIN, T., AND PICARD, W. 1999. Computer response to user frustration. Technical reports from MIT Media Laboratory Vision and Modelling Group, TR 480."}, {"heading": "KO\u0141AKOWSKA, A., LANDOWSKA, A., SZWOCH, M., SZWOCH,", "text": "W., AND WRO \u0301 BEL, M. 2014. Emotion recognition and its applications. In Human-Computer Systems Interaction: Backgrounds and Applications 3. Springer, 51-62.KOTSIA, I., AND PITAS, I. 2007. Recognition of facial expression in image sequences using geometric deformation features and supporting vector machines. Transactions for image processing 16, 1, 172-187."}, {"heading": "KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.", "text": "Imagenet classification with deep convolutional neuronal networks. In Advances in Neural Information Processing Systems, 1097- 1105.LEE, D.-H. 2013. Pseudo-label: The simple and efficient semisupervised learning method for deep neuronal networks. In Workshop on Challenges in Representation Learning, ICML, Vol. 3.LI, W.-J., WANG, C.-J., XU, D.-X., AND CHEN, S.-F. 2004. Illumination invariant face recognition based on Neural Network Ensemble. In International Conference on Tools with Artificial Intelligence, IEEE, 486-490.LIU, M., LI, S., SHAN, S., WANG, R., AND CHEN, X. 2014. Deep learning deformable facial action parts model for dynamic expression analysis. In Computer Vision-ACCV 2014. Springer, 143-157.LIU, M., SHAN, S., WANG, R., AND CHEN, AND CHEN, AND CHEN for Learning on IEV 2014."}, {"heading": "LIU, M., WANG, R., LI, S., SHAN, S., HUANG, Z., AND CHEN,", "text": "In Proceedings of the 16th International Conference on Multimodal Interaction, ACM, 494-501.LIU, P., HAN, S., MENG, Z., AND TONG, Y. 2014. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1805-1812."}, {"heading": "LUCEY, P., COHN, J. F., KANADE, T., SARAGIH, J., AMBADAR,", "text": "Z., AND MATTHEWS, I. 2010. The extended Cohn-Kanade dataset (ck +): A complete dataset for action unit and emotion-specific expression. In Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, 94-101."}, {"heading": "MATHUR, S. N., AHLAWAT, A. K., AND VISHWAKARMA, V. P.", "text": "2008. Enlightenment invariant face recognition using monitored and unattended learning algorithms. In Proceedings of World Academy of Science, Engineering and Technology, vol. 33.MICHEL, P., AND EL KALIOUBY, R. 2003. Real-time facial expression recognition in video using supporting vector machines. In Proceedings of the 5th International Conference on Multimodal interfaces, ACM, 258-264.NWE, T. L., FOO, S. W., AND DE SILVA, L. C. 2003. Speech emotion recognition using Hidden Markov Models. Speech communication 41, 4, 603-623."}, {"heading": "PANTIC, M., VALSTAR, M., RADEMAKER, R., AND MAAT, L.", "text": "2005. Web-based database for facial expression analysis. In International Conference on Multimedia and Expo, IEEE, 5 pages."}, {"heading": "PAPANDREOU, G., CHEN, L.-C., MURPHY, K., AND YUILLE,", "text": "A. L. 2015. Weak and semi-supervised learning of a DCNN on semantic image segmentation. arXiv: 1502.02734."}, {"heading": "PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND", "text": "SALESIN, D. H. 2006. Synthesizing realistic facial expressions from photos. In ACM SIGGRAPH 2006 Courses, ACM, 19.PRESTI, L., AND CASCIA, M. 2015. Using Hankel Matrices for Dynamic Face Motion Detection and Pain Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 26-33."}, {"heading": "ANONYMOUS SUBMISSION. 2016.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PYUN, H., KIM, Y., CHAE, W., KANG, H. W., AND SHIN, S. Y.", "text": "2003. An Exemplary Approach to Facial Expression Cloning. In Proceedings of the 2003 ACM SIGGRAPH / Eurographics symposium on Computer animation, Eurographics Association, 167-176.SCHULLER, B., RIGOLL, G., AND LANG, M. 2004. Speech emotion recognition that combines acoustic features and linguistic information in a hybrid support vector that conveys belief in the network architecture of machines. In International Conference on Acoustics, Speech, and Signal Processing, Vol. 1, IEEE, I-577.SENECHAL, T., MCDUFF, D., AND KALIOUBY, R. 2015. Facial action unit detection using active learning and an efficient non-linear kernel approximation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, 10-18.SHAN, C., AND MCOWAN, P. 2005. Robust facial expression unit detection using active learning and an efficient non-linear kernel approximation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, 10-18.SHAN, MCOWAN, P. 2005."}, {"heading": "SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S.,", "text": "ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND RABINOVICH, A. 2015. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1-9."}, {"heading": "TAIGMAN, Y., YANG, M., RANZATO, M., AND WOLF, L. 2014.", "text": "Deepface: Closing the gap to human-level performance in face verification. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1701-1708.TANG, Y., SALAKHUTDINOV, R., AND HINTON, G. 2012. Deep lambertian networks. arXiv: 1206.6445."}, {"heading": "TRAN, D., BOURDEV, L., FERGUS, R., TORRESANI, L., AND", "text": "arXiv Preprint arXiv: 1412.0767.VALSTAR, M., AND PANTIC, M. 2010. Created disgust, happiness and surprise: an addition to the mmi facial expression database. Workshop on EMOTION: Corpora for Research on Emotion and Affect, 65."}, {"heading": "VIERIU, R.-L., TULYAKOV, S., SEMENIUTA, S., SANGINETO,", "text": "E., AND SEBE, N. 2015. Facial expression recognition across a wide range of head positions. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1-7."}, {"heading": "VINCENT, P., LAROCHELLE, H., BENGIO, Y., AND MANZAGOL,", "text": "P.-A. 2008. Extracting and composing rugged features with noisy autoencoders. In Proceedings of the 25th International Conference on Machine Learning, ACM, 1096-1103.VIOLA, P., AND JONES, M. J. 2004. Robust real-time facial recognition. International Journal of Computer Vision 57, 2, 137-154."}, {"heading": "WALECKI, R., RUDOVIC, O., PAVLOVIC, V., AND PANTIC, M.", "text": "2015. Variable state latent random fields for recognition of facial expressions and units of action. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, Vol. 1, IEEE, 1-8.WANG, Z., WANG, S., AND JI, Q. 2013. Recording complex spatio-temporal relationships between facial muscles for recognition of facial expression. In Computer Vision and Pattern Recognition (CVPR), IEEE, 3422-3429.WATERS, K. 1987. A muscle model for animating three-dimensional facial expressions. In Proceedings of the Annual Conference on Computer Graphics and Interactive Technics, Vol. 21, ACM, 17-24."}, {"heading": "WESTON, J., RATLE, F., MOBAHI, H., AND COLLOBERT, R.", "text": "2012. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade. Springer, 639-655.ZHANG, Z., LUO, P., LOY, C.-C., AND TANG, X. 2015. Learning social relationship traits from face images. In Proceedings of the IEEE International Conference on Computer Vision, 3631-3639."}, {"heading": "ZHAO, K., CHU, W.-S., DE LA TORRE, F., COHN, J. F., AND", "text": "ZHANG, H. 2015. Joint patch and multi-label learning for facial recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2207-2216."}], "references": [{"title": "A neural network based facial expression recognition using fisherface", "author": ["Z. ABIDIN", "A. HARJOKO"], "venue": "International Journal of Computer Applications 59, 3, 30\u201334.", "citeRegEx": "ABIDIN and HARJOKO,? 2012", "shortCiteRegEx": "ABIDIN and HARJOKO", "year": 2012}, {"title": "Exploiting models of personality and emotions to control the behavior of animated interactive agents", "author": ["E. ANDR\u00c9", "M. KLESEN", "P. GEBHARD", "S. ALLEN", "T. RIST"], "venue": "Workshop on Achieving Human-Like Behavior in Interactive Animated Agents, 3\u20137.", "citeRegEx": "ANDR\u00c9 et al\\.,? 2000", "shortCiteRegEx": "ANDR\u00c9 et al\\.", "year": 2000}, {"title": "Incremental face alignment in the wild", "author": ["A. ASTHANA", "S. ZAFEIRIOU", "S. CHENG", "M. PANTIC"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1859\u2013 1866.", "citeRegEx": "ASTHANA et al\\.,? 2014", "shortCiteRegEx": "ASTHANA et al\\.", "year": 2014}, {"title": "Convex neural networks", "author": ["Y. BENGIO", "N.L. ROUX", "P. VINCENT", "O. DELALLEAU", "P. MARCOTTE"], "venue": "Advances in Neural Information Processing Systems, 123\u2013130.", "citeRegEx": "BENGIO et al\\.,? 2005", "shortCiteRegEx": "BENGIO et al\\.", "year": 2005}, {"title": "On contrastive divergence learning", "author": ["M.A. CARREIRA-PERPINAN", "G.E. HINTON"], "venue": "Proceedings of International Workshop on Artificial Intelligence and Statistics, 33\u201340.", "citeRegEx": "CARREIRA.PERPINAN and HINTON,? 2005", "shortCiteRegEx": "CARREIRA.PERPINAN and HINTON", "year": 2005}, {"title": "Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents", "author": ["J. CASSELL", "C. PELACHAUD", "N. BADLER", "M. STEEDMAN", "B. ACHORN", "T. BECKET", "B. DOUVILLE", "S. PREVOST", "M. STONE"], "venue": "Proceedings of the 21st annual con-", "citeRegEx": "CASSELL et al\\.,? 1994", "shortCiteRegEx": "CASSELL et al\\.", "year": 1994}, {"title": "Interactive agents for multimodal emotional user interaction", "author": ["E. CEREZO", "S. BALDASSARRI", "F. SERON"], "venue": "Multi Conferences on Computer Science and Information Systems, 35\u2013", "citeRegEx": "CEREZO et al\\.,? 2007", "shortCiteRegEx": "CEREZO et al\\.", "year": 2007}, {"title": "3d model-based continuous emotion recognition", "author": ["H. CHEN", "J. LI", "F. ZHANG", "Y. LI", "H. WANG"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1836\u20131845.", "citeRegEx": "CHEN et al\\.,? 2015", "shortCiteRegEx": "CHEN et al\\.", "year": 2015}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. COWIE", "E. DOUGLAS-COWIE", "N. TSAPATSOULIS", "G. VOTSIS", "S. KOLLIAS", "W. FELLENZ", "J.G. TAYLOR"], "venue": "Signal Processing Magazine, IEEE 18, 1, 32\u201380.", "citeRegEx": "COWIE et al\\.,? 2001", "shortCiteRegEx": "COWIE et al\\.", "year": 2001}, {"title": "Emotion recognition using PHOG and LPQ features", "author": ["A. DHALL", "A. ASTHANA", "R. GOECKE", "T. GEDEON"], "venue": "International Conference on Automatic Face & Gesture Recognition, IEEE, 878\u2013883.", "citeRegEx": "DHALL et al\\.,? 2011", "shortCiteRegEx": "DHALL et al\\.", "year": 2011}, {"title": "Role of facial expressions in social interactions", "author": ["C. FRITH"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences 364, 1535, 3453\u20133458.", "citeRegEx": "FRITH,? 2009", "shortCiteRegEx": "FRITH", "year": 2009}, {"title": "Facial expression recognition using artificial neural networks", "author": ["M. GARGESHA", "P. KUCHI"], "venue": "Artificial Neural Computer Systems, 1\u20136.", "citeRegEx": "GARGESHA and KUCHI,? 2002", "shortCiteRegEx": "GARGESHA and KUCHI", "year": 2002}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. HE", "D. JIANG", "L. YANG", "E. PEI", "P. WU", "H. SAHLI"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, ACM, 73\u201380.", "citeRegEx": "HE et al\\.,? 2015", "shortCiteRegEx": "HE et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. HINTON", "R.R. SALAKHUTDINOV"], "venue": "Science 313, 5786, 504\u2013507.", "citeRegEx": "HINTON and SALAKHUTDINOV,? 2006", "shortCiteRegEx": "HINTON and SALAKHUTDINOV", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. HINTON", "S. OSINDERO", "TEH", "Y.-W."], "venue": "Neural Computation 18, 7, 1527\u20131554.", "citeRegEx": "HINTON et al\\.,? 2006", "shortCiteRegEx": "HINTON et al\\.", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. JIA", "E. SHELHAMER", "J. DONAHUE", "S. KARAYEV", "J. LONG", "R. GIRSHICK", "S. GUADARRAMA", "T. DARRELL"], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "JIA et al\\.,? 2014", "shortCiteRegEx": "JIA et al\\.", "year": 2014}, {"title": "Joint fine-tuning in deep neural networks for facial expression recognition", "author": ["JUNG H.", "LEE S.", "YIM J.", "PARK S.", "KIM", "J."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2983\u20132991.", "citeRegEx": "H. et al\\.,? 2015", "shortCiteRegEx": "H. et al\\.", "year": 2015}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. KAHOU", "C. PAL", "X. BOUTHILLIER", "P. FROUMENTY", "\u00c7. G\u00dcL\u00c7EHRE", "R. MEMISEVIC", "P. VINCENT", "A. COURVILLE", "Y. BENGIO", "R.C. FERRARI"], "venue": "Proceedings of the 15th ACM on International con-", "citeRegEx": "KAHOU et al\\.,? 2013", "shortCiteRegEx": "KAHOU et al\\.", "year": 2013}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. KAHOU", "X. BOUTHILLIER", "P. LAMBLIN", "C. GULCEHRE", "V. MICHALSKI", "K. KONDA", "S. JEAN", "P. FROUMENTY", "Y. DAUPHIN", "N BOULANGER-LEWANDOWSKI"], "venue": "Journal on Multimodal User Inter-", "citeRegEx": "KAHOU et al\\.,? 2015", "shortCiteRegEx": "KAHOU et al\\.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. KARPATHY", "G. TODERICI", "S. SHETTY", "T. LEUNG", "R. SUKTHANKAR", "L. FEI-FEI"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1725\u2013 1732.", "citeRegEx": "KARPATHY et al\\.,? 2014", "shortCiteRegEx": "KARPATHY et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. KINGMA", "S. MOHAMED", "D.J. REZENDE", "M. WELLING"], "venue": "Advances in Neural Information Processing Systems, 3581\u20133589.", "citeRegEx": "KINGMA et al\\.,? 2014", "shortCiteRegEx": "KINGMA et al\\.", "year": 2014}, {"title": "Computer response to user frustration", "author": ["T. KLEIN", "W. PICARD"], "venue": "MIT Media Laboratory Vision and Modelling Group Technical Reports, TR 480.", "citeRegEx": "KLEIN and PICARD,? 1999", "shortCiteRegEx": "KLEIN and PICARD", "year": 1999}, {"title": "Emotion recognition and its applications", "author": ["A. KO\u0141AKOWSKA", "A. LANDOWSKA", "M. SZWOCH", "W. SZWOCH", "M. WR\u00d3BEL"], "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3. Springer, 51\u201362.", "citeRegEx": "KO\u0141AKOWSKA et al\\.,? 2014", "shortCiteRegEx": "KO\u0141AKOWSKA et al\\.", "year": 2014}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "author": ["I. KOTSIA", "I. PITAS"], "venue": "Transactions on Image Processing 16, 1, 172\u2013187.", "citeRegEx": "KOTSIA and PITAS,? 2007", "shortCiteRegEx": "KOTSIA and PITAS", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "Advances in Neural Information Processing Systems, 1097\u2013 1105.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks", "author": ["LEE", "D.-H."], "venue": "Workshop on Challenges in Representation Learning, ICML, vol. 3.", "citeRegEx": "LEE and D..H.,? 2013", "shortCiteRegEx": "LEE and D..H.", "year": 2013}, {"title": "Illumination invariant face recognition based on neural network ensemble", "author": ["LI", "W.-J.", "WANG", "XU C.-J.", "D.-X.", "CHEN", "S.-F."], "venue": "International Conference on Tools with Artificial Intelligence, IEEE, 486\u2013490.", "citeRegEx": "LI et al\\.,? 2004", "shortCiteRegEx": "LI et al\\.", "year": 2004}, {"title": "Deeply learning deformable facial action parts model for dynamic expression analysis", "author": ["M. LIU", "S. LI", "S. SHAN", "R. WANG", "X. CHEN"], "venue": "Computer Vision\u2013ACCV 2014. Springer, 143\u2013157.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["M. LIU", "S. SHAN", "R. WANG", "X. CHEN"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1749\u20131756.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild", "author": ["M. LIU", "R. WANG", "S. LI", "S. SHAN", "Z. HUANG", "X. CHEN"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, ACM, 494\u2013501.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["P. LIU", "S. HAN", "Z. MENG", "Y. TONG"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1805\u20131812.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression", "author": ["P. LUCEY", "J.F. COHN", "T. KANADE", "J. SARAGIH", "Z. AMBADAR", "I. MATTHEWS"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, 94\u2013101.", "citeRegEx": "LUCEY et al\\.,? 2010", "shortCiteRegEx": "LUCEY et al\\.", "year": 2010}, {"title": "Illumination invariant face recognition using supervised and unsupervised learning algorithms", "author": ["S.N. MATHUR", "A.K. AHLAWAT", "V.P. VISHWAKARMA"], "venue": "Proceedings of World Academy of Science, Engineering and Technology, vol. 33.", "citeRegEx": "MATHUR et al\\.,? 2008", "shortCiteRegEx": "MATHUR et al\\.", "year": 2008}, {"title": "Real time facial expression recognition in video using support vector machines", "author": ["P. MICHEL", "R. EL KALIOUBY"], "venue": "Proceedings of the 5th international conference on Multimodal interfaces, ACM, 258\u2013264.", "citeRegEx": "MICHEL and KALIOUBY,? 2003", "shortCiteRegEx": "MICHEL and KALIOUBY", "year": 2003}, {"title": "Speech emotion recognition using Hidden Markov Models", "author": ["NWE T.L.", "FOO S.W.", "SILVA L.C. DE"], "venue": "Speech communication 41, 4, 603\u2013623.", "citeRegEx": "L. et al\\.,? 2003", "shortCiteRegEx": "L. et al\\.", "year": 2003}, {"title": "Web-based database for facial expression analysis", "author": ["M. PANTIC", "M. VALSTAR", "R. RADEMAKER", "L. MAAT"], "venue": "International Conference on Multimedia and Expo, IEEE, 5\u2013pp.", "citeRegEx": "PANTIC et al\\.,? 2005", "shortCiteRegEx": "PANTIC et al\\.", "year": 2005}, {"title": "Weakly-and semi-supervised learning of a dcnn for semantic image segmentation", "author": ["G. PAPANDREOU", "CHEN", "L.-C.", "K. MURPHY", "A.L. YUILLE"], "venue": "arXiv:1502.02734.", "citeRegEx": "PAPANDREOU et al\\.,? 2015", "shortCiteRegEx": "PAPANDREOU et al\\.", "year": 2015}, {"title": "Synthesizing realistic facial expressions from photographs", "author": ["F. PIGHIN", "J. HECKER", "D. LISCHINSKI", "R. SZELISKI", "D.H. SALESIN"], "venue": "ACM SIGGRAPH 2006 Courses, ACM,", "citeRegEx": "PIGHIN et al\\.,? 2006", "shortCiteRegEx": "PIGHIN et al\\.", "year": 2006}, {"title": "Using hankel matrices for dynamics-based facial emotion recognition and pain detection", "author": ["L. PRESTI", "M. CASCIA"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 26\u201333.", "citeRegEx": "PRESTI and CASCIA,? 2015", "shortCiteRegEx": "PRESTI and CASCIA", "year": 2015}, {"title": "An example-based approach for facial expression cloning", "author": ["H. PYUN", "Y. KIM", "W. CHAE", "H.W. KANG", "S.Y. SHIN"], "venue": "Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, Eurographics Association, 167\u2013176.", "citeRegEx": "PYUN et al\\.,? 2003", "shortCiteRegEx": "PYUN et al\\.", "year": 2003}, {"title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture", "author": ["B. SCHULLER", "G. RIGOLL", "M. LANG"], "venue": "International Conference on Acoustics, Speech, and Signal Processing, vol. 1, IEEE, I\u2013577.", "citeRegEx": "SCHULLER et al\\.,? 2004", "shortCiteRegEx": "SCHULLER et al\\.", "year": 2004}, {"title": "Facial action unit detection using active learning and an efficient non-linear kernel approximation", "author": ["T. SENECHAL", "D. MCDUFF", "R. KALIOUBY"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 10\u201318.", "citeRegEx": "SENECHAL et al\\.,? 2015", "shortCiteRegEx": "SENECHAL et al\\.", "year": 2015}, {"title": "Robust facial expression recognition using local binary patterns", "author": ["C. SHAN", "S. GONG", "P.W. MCOWAN"], "venue": "International Conference on Image Processing, vol. 2, IEEE, II\u2013370.", "citeRegEx": "SHAN et al\\.,? 2005", "shortCiteRegEx": "SHAN et al\\.", "year": 2005}, {"title": "Going deeper with convolutions", "author": ["C. SZEGEDY", "W. LIU", "Y. JIA", "P. SERMANET", "S. REED", "D. ANGUELOV", "D. ERHAN", "V. VANHOUCKE", "A. RABINOVICH"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139.", "citeRegEx": "SZEGEDY et al\\.,? 2015", "shortCiteRegEx": "SZEGEDY et al\\.", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. TAIGMAN", "M. YANG", "M. RANZATO", "L. WOLF"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1701\u20131708.", "citeRegEx": "TAIGMAN et al\\.,? 2014", "shortCiteRegEx": "TAIGMAN et al\\.", "year": 2014}, {"title": "Deep lambertian networks", "author": ["Y. TANG", "R. SALAKHUTDINOV", "G. HINTON"], "venue": "arXiv:1206.6445.", "citeRegEx": "TANG et al\\.,? 2012", "shortCiteRegEx": "TANG et al\\.", "year": 2012}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. TRAN", "L. BOURDEV", "R. FERGUS", "L. TORRESANI", "M. PALURI"], "venue": "arXiv preprint arXiv:1412.0767.", "citeRegEx": "TRAN et al\\.,? 2014", "shortCiteRegEx": "TRAN et al\\.", "year": 2014}, {"title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database", "author": ["M. VALSTAR", "M. PANTIC"], "venue": "Workshop on EMOTION: Corpora for Research on Emotion and Affect, 65.", "citeRegEx": "VALSTAR and PANTIC,? 2010", "shortCiteRegEx": "VALSTAR and PANTIC", "year": 2010}, {"title": "Facial expression recognition under a wide range of head poses", "author": ["VIERIU", "R.-L.", "S. TULYAKOV", "S. SEMENIUTA", "E. SANGINETO", "N. SEBE"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20137.", "citeRegEx": "VIERIU et al\\.,? 2015", "shortCiteRegEx": "VIERIU et al\\.", "year": 2015}, {"title": "Extracting and composing robust features", "author": ["P. VINCENT", "H. LAROCHELLE", "Y. BENGIO", "MANZAGOL", "P.-A"], "venue": null, "citeRegEx": "VINCENT et al\\.,? \\Q2008\\E", "shortCiteRegEx": "VINCENT et al\\.", "year": 2008}, {"title": "Robust real-time face detection", "author": ["P. VIOLA", "M.J. JONES"], "venue": "International Journal of Computer Vision 57, 2, 137\u2013154.", "citeRegEx": "VIOLA and JONES,? 2004", "shortCiteRegEx": "VIOLA and JONES", "year": 2004}, {"title": "Variable-state latent conditional random fields for facial expression recognition and action unit detection", "author": ["R. WALECKI", "O. RUDOVIC", "V. PAVLOVIC", "M. PANTIC"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20138.", "citeRegEx": "WALECKI et al\\.,? 2015", "shortCiteRegEx": "WALECKI et al\\.", "year": 2015}, {"title": "Capturing complex spatio-temporal relations among facial muscles for facial expression recognition", "author": ["Z. WANG", "S. WANG", "JI", "Q."], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE, 3422\u20133429.", "citeRegEx": "WANG et al\\.,? 2013", "shortCiteRegEx": "WANG et al\\.", "year": 2013}, {"title": "A muscle model for animation threedimensional facial expression", "author": ["K. WATERS"], "venue": "Proceedings of the annual conference on Computer graphics and interactive techniques, vol. 21, ACM, 17\u201324.", "citeRegEx": "WATERS,? 1987", "shortCiteRegEx": "WATERS", "year": 1987}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. WESTON", "F. RATLE", "H. MOBAHI", "R. COLLOBERT"], "venue": "Neural Networks: Tricks of the Trade. Springer, 639\u2013655.", "citeRegEx": "WESTON et al\\.,? 2012", "shortCiteRegEx": "WESTON et al\\.", "year": 2012}, {"title": "Learning social relation traits from face images", "author": ["Z. ZHANG", "P. LUO", "LOY", "C.-C.", "X. TANG"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 3631\u20133639.", "citeRegEx": "ZHANG et al\\.,? 2015", "shortCiteRegEx": "ZHANG et al\\.", "year": 2015}, {"title": "Joint patch and multi-label learning for facial action unit detection", "author": ["K. ZHAO", "CHU", "W.-S.", "F. DE LA TORRE", "J.F. COHN", "H. ZHANG"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2207\u20132216.", "citeRegEx": "ZHAO et al\\.,? 2015", "shortCiteRegEx": "ZHAO et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "In fact, studies have shown that non-verbal communication accounts for more than half of all societal interactions [Frith 2009].", "startOffset": 115, "endOffset": 127}, {"referenceID": 8, "context": "Adding emotion recognition and tailoring responses towards users emotional state can help improve human computer interaction drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep users engaged.", "startOffset": 137, "endOffset": 175}, {"referenceID": 55, "context": "Adding emotion recognition and tailoring responses towards users emotional state can help improve human computer interaction drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep users engaged.", "startOffset": 137, "endOffset": 175}, {"referenceID": 21, "context": "Last two decades have seen some innovation in this area [Klein and Picard 1999; Cerezo et al. 2007; Andr\u00e9 et al. 2000] such as humanoid robots for example Pepper which can both understand and mimic human emotions.", "startOffset": 56, "endOffset": 118}, {"referenceID": 6, "context": "Last two decades have seen some innovation in this area [Klein and Picard 1999; Cerezo et al. 2007; Andr\u00e9 et al. 2000] such as humanoid robots for example Pepper which can both understand and mimic human emotions.", "startOffset": 56, "endOffset": 118}, {"referenceID": 37, "context": "Accurate characterization of face geometry and muscle motion can be used for both expression identification and synthesis [Pighin et al. 2006; Wang et al. 2013] with applications towards computer animation [Cassell et al.", "startOffset": 122, "endOffset": 160}, {"referenceID": 52, "context": "Accurate characterization of face geometry and muscle motion can be used for both expression identification and synthesis [Pighin et al. 2006; Wang et al. 2013] with applications towards computer animation [Cassell et al.", "startOffset": 122, "endOffset": 160}, {"referenceID": 5, "context": "2013] with applications towards computer animation [Cassell et al. 1994].", "startOffset": 51, "endOffset": 72}, {"referenceID": 53, "context": "Such approaches combine very high dimensional facial features from facial topology and compress them to lower dimensions using a series of parameters or transformations [Waters 1987; Pyun et al. 2003].", "startOffset": 169, "endOffset": 200}, {"referenceID": 39, "context": "Such approaches combine very high dimensional facial features from facial topology and compress them to lower dimensions using a series of parameters or transformations [Waters 1987; Pyun et al. 2003].", "startOffset": 169, "endOffset": 200}, {"referenceID": 14, "context": "Over the past decade algorithms for training neural nets have dramatically evolved, allowing us to efficiently train deep neural nets [Hinton et al. 2006; Jung et al. 2015].", "startOffset": 134, "endOffset": 172}, {"referenceID": 24, "context": "Such models have become a strong driving force in modern computer vision and excel at object classification [Krizhevsky et al. 2012], segmentation and facial recognition [Taigman et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 44, "context": "2012], segmentation and facial recognition [Taigman et al. 2014].", "startOffset": 43, "endOffset": 64}, {"referenceID": 23, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 42, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 9, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 51, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 38, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 48, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 40, "context": "Other intriguing methodologies include performing emotion recognition through speech [Nwe et al. 2003; Schuller et al. 2004], using temporal features and manifold learning [Liu et al.", "startOffset": 85, "endOffset": 124}, {"referenceID": 52, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 18, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 7, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 41, "context": "2015] and combining multiple kernel based approaches [Liu et al. 2014c; Senechal et al. 2015].", "startOffset": 53, "endOffset": 93}, {"referenceID": 24, "context": "Deep neural networks have proven to be an effective tool to classify and segment high dimensional data such as images [Krizhevsky et al. 2012; Szegedy et al. 2015], audio and videos [Karpathy et al.", "startOffset": 118, "endOffset": 163}, {"referenceID": 43, "context": "Deep neural networks have proven to be an effective tool to classify and segment high dimensional data such as images [Krizhevsky et al. 2012; Szegedy et al. 2015], audio and videos [Karpathy et al.", "startOffset": 118, "endOffset": 163}, {"referenceID": 19, "context": "2015], audio and videos [Karpathy et al. 2014; Tran et al. 2014].", "startOffset": 24, "endOffset": 64}, {"referenceID": 46, "context": "2015], audio and videos [Karpathy et al. 2014; Tran et al. 2014].", "startOffset": 24, "endOffset": 64}, {"referenceID": 44, "context": "With advances in convolutional neural nets, we have seen neural nets applied for face detection [Taigman et al. 2014; Zhao et al. 2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 56, "context": "With advances in convolutional neural nets, we have seen neural nets applied for face detection [Taigman et al. 2014; Zhao et al. 2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 0, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 11, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 12, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 54, "context": "This issue can also be solved using embeddings in lower dimensional manifold [Weston et al. 2012; Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013] thereby requiring fewer number of labeled samples.", "startOffset": 77, "endOffset": 117}, {"referenceID": 20, "context": "This issue can also be solved using embeddings in lower dimensional manifold [Weston et al. 2012; Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013] thereby requiring fewer number of labeled samples.", "startOffset": 77, "endOffset": 117}, {"referenceID": 36, "context": "Approaches based on semi supervised learning have shown to work for smaller labeled datasets [Papandreou et al. 2015] and techniques using deep neural nets to combine labels and unlabeled data in the same architecture [Liu et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 17, "context": "2015] and techniques using deep neural nets to combine labels and unlabeled data in the same architecture [Liu et al. 2014d; Kahou et al. 2013] have emerged victorious.", "startOffset": 106, "endOffset": 143}, {"referenceID": 32, "context": "Introducing invariants in neural networks is an area of active research, some examples include illumination invariant face recognition techniques [Mathur et al. 2008; Li et al. 2004] and deep lambertian networks [Tang et al.", "startOffset": 146, "endOffset": 182}, {"referenceID": 26, "context": "Introducing invariants in neural networks is an area of active research, some examples include illumination invariant face recognition techniques [Mathur et al. 2008; Li et al. 2004] and deep lambertian networks [Tang et al.", "startOffset": 146, "endOffset": 182}, {"referenceID": 45, "context": "2004] and deep lambertian networks [Tang et al. 2012; Jung et al. 2015].", "startOffset": 35, "endOffset": 71}, {"referenceID": 50, "context": "Our facial expression recognition pipeline comprises of Viola-Jones algorithm [Viola and Jones 2004] for face detection followed by a deep convolutional neural network for predicting expressions.", "startOffset": 78, "endOffset": 100}, {"referenceID": 13, "context": "Stacked autoencoders can be used to convert high dimensional data into lower dimensional space which can be useful for classification, visualization or retrieval [Hinton and Salakhutdinov 2006].", "startOffset": 162, "endOffset": 193}, {"referenceID": 24, "context": "The autoencoder topology is inspired by ImageNet [Krizhevsky et al. 2012] and comprises of convolutional layers gradually reducing data dimensionality until we reach a fully connected layer.", "startOffset": 49, "endOffset": 73}, {"referenceID": 19, "context": "In the same way that spatial convolutions consolidate nearby spatial characteristics of an image, we use the slow fusion model described in [Karpathy et al. 2014] to gradually combine temporal features across multiple frames.", "startOffset": 140, "endOffset": 162}, {"referenceID": 3, "context": "the predictor [Bengio et al. 2005].", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "[Asthana et al. 2014].", "startOffset": 0, "endOffset": 21}, {"referenceID": 31, "context": "The Cohn Kanade dataset [Lucey et al. 2010] is one of the oldest and well known dataset containing facial expression video clips.", "startOffset": 24, "endOffset": 43}, {"referenceID": 35, "context": "MMI facial expression dataset [Pantic et al. 2005] involves an ongoing effort for representing both enacted and induced facial expres-", "startOffset": 30, "endOffset": 50}, {"referenceID": 47, "context": "MMI which originally contained only posed facial expressions, was recently extended to include natural versions of happiness, disgust and surprise [Valstar and Pantic 2010].", "startOffset": 147, "endOffset": 172}, {"referenceID": 13, "context": "Since deep autoencoders can show slow convergence when trained from randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to train stacked autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005].", "startOffset": 97, "endOffset": 128}, {"referenceID": 4, "context": "Since deep autoencoders can show slow convergence when trained from randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to train stacked autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005].", "startOffset": 222, "endOffset": 257}, {"referenceID": 15, "context": "Our neural network was implemented using the Caffe framework [Jia et al. 2014] and trained using NVIDIA Tesla K40 GPUs.", "startOffset": 61, "endOffset": 78}, {"referenceID": 15, "context": "Both autoencoder and predictor network topologies are implemented as Caffe prototxt files [Jia et al. 2014] and they will be made available for public usage.", "startOffset": 90, "endOffset": 107}], "year": 2016, "abstractText": "In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.", "creator": "LaTeX acmsiggraph.cls"}}}