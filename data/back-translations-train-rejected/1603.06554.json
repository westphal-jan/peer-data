{"id": "1603.06554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Action-Affect Classification and Morphing using Multi-Task Representation Learning", "abstract": "Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.", "histories": [["v1", "Mon, 21 Mar 2016 19:38:07 GMT  (3237kb,D)", "http://arxiv.org/abs/1603.06554v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.HC cs.LG", "authors": ["timothy j shields", "mohamed r amer", "max ehrlich", "amir tamrakar"], "accepted": false, "id": "1603.06554"}, "pdf": {"name": "1603.06554.pdf", "metadata": {"source": "CRF", "title": "Action-Affect Classification and Morphing using Multi-Task Representation Learning", "authors": ["Timothy J. Shields", "Mohamed R. Amer", "Max Ehrlich", "Amir Tamrakar"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Body Affect; Multi-Task Learning; Conditional Restricted Boltzmann Machines; Deep Learning;"}, {"heading": "1 Introduction", "text": "There are a number of reasons why we are not able to establish such a system."}, {"heading": "2 Prior work", "text": "In this section, we have severely limited the literature on activity recognition in RGB-D and motion capture sequences; secondly, in recent years, we are dealing with the availability of cheap depth sensors. [12] There is an increasing number of approaches that address the problem of activity recognition in RGB-D sequences that have been popular in recent years. [12] There are a number of approaches that have addressed the problem of activity recognition using skeletal data. [18] There are a number of measures that focus on graphics, such as generating animations and transitions between animations using signal processing techniques or computer vision. Their main goal was to create natural-looking skeletons for skeletal generation; some methods that transfer the knowledge of signal processing for transformation into a neutral skeleton."}, {"heading": "3 Model", "text": "Instead of immediately defining our Multi-Task CRBM (MT-CRBM) model, we discuss a sequence of models that gradually increase in complexity so that the various components of our final model can be understood in isolation. We start with the basic RBM model (paragraph 3.1), then extend the RBM to the CRBM model (paragraph 3.2), then extend the CRBM further to a new discriminatory model (DCRBM) (paragraph 3.3), then extend the D-CRBM to our main multi-task model (MT-CRBM) (paragraph 3.4), and finally define a Multi-Task Multimodal Model (MTM-CRBM) (paragraph 3.5)."}, {"heading": "3.1 Restricted Boltzmann Machines", "text": "RBMs [39], shown in Figure 2 (a), define a probability distribution pR as a Gibbs distribution (1), where v is a visible node vector, h a vector of hidden nodes, ER the energy function, and Z the partition function. The parameters to be learned are a and b, the distortions for v and h, and the weights W. The RBM is completely connected between the layers, without lateral connections. This architecture implies that v and h are factorally given to one of the two vectors, which enables the exact calculation of pR (v | h) and pR (h | v).pR (h, v) = exp [\u2212 ER (h, v)] Z (\u03b8), Z (\u03b8) = x h, v exp [\u2212 ER, v] vi (h, v), vivi = [{a, b} -bias, {W} -fully connected."}, {"heading": "3.2 Conditional Restricted Boltzmann Machines", "text": "CRBMs [41] are a natural extension of RBMs for modeling short-term temporal dependencies. A CRBM, shown in Figure 2 (b), is an RBM that takes into account the history of the previous N time instances, t \u2212 N,.., t \u2212 1, when time is taken into account. This is done by treating the previous time instances as additional inputs. Some approximations have been made to enable efficient training and efficient conclusions, more details are available in [41]. A CRBM defines a probability distribution pC as Gibbs distribution (4).pC (ht, vt | v < t) = exp [\u2212 EC (vt, ht | v < t)] Z (\u03b8), Z (\u03b8), Z (\u03b8) = h, v exp [\u2212 EC (ht, vt | v < t).pC (ht, v < t) < < < p; < &ltt; < {W, A,} \u2212 bias."}, {"heading": "3.3 Discriminative Conditional Restricted Boltzmann Machines", "text": "We extend the CRBMs to the D-CRBMs shown in Figure 2 (c). D-CRBMs define the probability distribution pDC as Gibbs distribution (7).pDC (yt, ht, vt | v < t) = exp [\u2212 EDC (yt, ht, s} -bias, {A, B} -auto regressive, {W, U} -fully connected.] (7) The probability distribution over the visible layer follows the same distributions as in (5).The hidden layer h is defined as a function of the labels y and the visible nodes. < U} -fully connected.] (7) The probability distribution over the visible layer (ltvi) follows the same distribution as in (< p).sk."}, {"heading": "3.4 Multi-Task Conditional Restricted Boltzmann Machines", "text": "In the same way, the CRBMs can be extended to the DC RBMs by adding a discriminatory term to the model. MT CRBMs can learn a common representation layer for all tasks. Figure 3 (a). MTCRBMs define the probability distribution pMT as Gibbs distribution (10). MT CRBMs learn a common representation layer for all tasks. pMT (y L t, ht, vt | v < t) = exp [\u2212 EDC (yLt, ht, vt | v < t)] Z (ultrafast), Z (ultrafast), h, v exp [\u2212 EMT (yLt, ht, vt | v < t)], \u03b8 = [{a, b, sL} -bias, {A, B} -auto regressive, {W, UL} -fully connected. (10) The probability distribution over the visible layer follows the same distributions as {-B,} (sL)."}, {"heading": "3.5 Multi-Task Multimodal Conditional Restricted Boltzmann Machines", "text": "We can, of course, expand MT-CRBMs to MTM-CRBMs. < M-CRBMs are then treated as the visible vector of a single fusion MT-CRBMs. The result is an MTMCRBM model that refers to multiple time modalities for multiple tasks. MTMCRBMs define the probability distribution pMTM as a Gibbs distribution (13). MTM-CRBMs learn an additional representation layer for each of the modalities, which includes a modal specific representation and the common layer for all tasks.pMTM (y L t, ht, h 1: M t, v 1: M t | v1 & ltM < t = exp = exp (yLt, ht, h1: Mt)."}, {"heading": "4 Inference", "text": "We will first discuss the conclusion for the MTM-CRBM, as it is the most general case. To perform the classification at a given time in the MTM-CRBM, we use a bottom-up approach, calculating the mean value of each node starting from the nodes below; that is, we calculate the mean value of hmt with v m < t and vmt for each modality, then we calculate the mean value of ht with h 1: M < t, then we calculate the mean value of yLt for each task with ht and obtain the classification probabilities for each task. Figure 4 illustrates our follow-up approach. The conclusion in the MT-CRBM is the same as the MTM-CRBM, unless there is only one modality, and the conclusion in the D-CRBM is the same as the MT-CRBM, unless there is only one task."}, {"heading": "5 Learning", "text": "Learning our model is done using the contrastive divergence (CD) [40] > > > data, where < \u00b7 > data is the expectation of the data and < \u00b7 > Recon is the expectation of the reconstruction; learning is done in two steps: a bottom-up pass and a top-down pass using sampling equations from (8) for D-CRBM, (11) for MT-CRBM and (14) for MTM-CRBM. In the bottomup pass, the reconstruction is done in parallel by sampling the first unimodal layers p (hmt, j = 1 | vmt, vm < t, yl) for all hidden nodes. In the bottomup pass, the reconstruction is done by sampling the unimodal layers p (ht, n = 1 | yLk, t, h1: Mt, hltltltltltltlt; lt; lt; < lt; lt;"}, {"heading": "6 Experiments", "text": "We now describe the data sets in (paragraph 6.1), specify the implementation details in (paragraph 6.2) and present our quantitative results in (paragraph 6.3)."}, {"heading": "6.1 Datasets", "text": "Our problem is that we focus on multi-task learning for body movements. In the literature [4,9], most data sets were either single activity detection tasks, not publicly available, too few cases, or just RGB-D without a skeleton. We found two available data sets to evaluate our approach, which includes multiple tasks. The first data set consists of an interaction between two people performing a cooperative task consisting of a number of actors that have multiple effects; the second data set is the tower game [8], which is collected using a Kinect sensor."}, {"heading": "6.2 Implementation Details", "text": "For the processing of the tower data sets, we followed the same approach as [50], forming a body-centric transformation of the skeletons generated by the Kinect sensors. We used the 11 joints from the torso of the two players, since the tower game almost entirely affects the torso. We used the raw joint locations, which were standardized in relation to a selected place of origin. We used the same descriptor, which, according to the descriptions, was composed of 84 dimensions based on the standardized joint locations. The inclination angles formed by all triples of the anatomically connected joints, azimuth the angles between the projections of the second bone and the vector at the level of the first bones, were formed by the inclinations."}, {"heading": "6.3 Quantitative Results", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "7 Conclusion and Future Work", "text": "We have proposed a collection of hybrid models, both discriminatory and generative, that model the relationships in and distributions of temporal, multimodal, multitasal data. A comprehensive experimental evaluation of these models on two different sets of data demonstrates the superiority of our approach to state-of-the-art multi-task classification of temporal data. This improvement in classification performance is accompanied by new generative capabilities and efficient use of model parameters by factoring across different assignments.The generative capabilities of our approach enable new and interesting applications, such as the demonstrated sequence morphing. A future direction of work is to further explore and improve these generative applications of the models.The factoring of the tasks used in our approach means that the number of parameters only increases linearly with the number of tasks and classes, which is considered significant when contrasted with a single task model that uses a flat cartesian task product, the number of parameters increases with the number of tasks exposed."}, {"heading": "Acknowledgments", "text": "This research was developed in part with the support of the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laborotory (AFRL). The views, opinions and / or results expressed are those of the authors and should not be interpreted to represent the official views or policies of the Department of Defense or the U.S. government."}], "references": [{"title": "Affective Computing", "author": ["R.W. Picard"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "F.D.L.: Auto- mated face analysis for affective computing", "author": ["R. Calvo", "S. D\u2019Mello", "J. Gratch", "A. Kappas", "J.F. Cohn", "Torre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Transactions on Pat- tern Analysis and Machine Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Affective body expression perception and recognition: A survey", "author": ["A. Kleinsmith", "N. Bianchi-Berthouze"], "venue": "IEEE Transactions on Affective Computing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Emotion from motion. In: GI", "author": ["K. Amaya", "A. Bruderlin", "T. Calvert"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Verbs and adverbs: Multidimensional motion interpolation using radial basis functions", "author": ["C. Rose", "B. Bodenheimer", "M.F. Cohen"], "venue": "Computer Graphics and Applications", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A motion capture library for the study of identity, gender, and emotion perception from biological motion", "author": ["Y. MA", "H.M. PATERSON", "F.E. POLLICK"], "venue": "BMR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "The tower game dataset: A multimodal dataset for analyzing social interaction predicates", "author": ["D.A. Salter", "A. Tamrakar", "M.R.A. Behjat Siddiquie", "A. Divakaran", "B. Lande", "D. Mehri"], "venue": "ACII", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Rgb-d-based action recog- nition datasets: A survey", "author": ["J. Zhang", "W. Li", "P.O. Ogunbona", "P. Wang", "C. Tang"], "venue": "arxiv", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A survey of video datasets for human action and activity recognition", "author": ["J.M. Chaquet", "E.J. Carmona", "A. Fern\u00e1ndez-Caballero"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In: ICML", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Action recognition based on a bag of 3d points", "author": ["W. Li", "Z. Zhang", "Z. Liu"], "venue": "Com- puter Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Detecting affect from non-stylised body motions", "author": ["D. Bernhardt", "P. Robinson"], "venue": "ACII", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Regularized multi?task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In: KDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "JMLR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Learning multiple visual tasks while discovering their structure", "author": ["C. Ciliberto", "L. Rosasco", "S. Villa"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The benefit of multitask representa- tion learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "ArXiv", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sparse coding for multitask and transfer learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H.D. III"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Clustered multi-task learning via alternating structure optimization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Learning with whom to share in multi-task feature learn- ing", "author": ["Z. Kang", "K. Grauman"], "venue": "Timothy J. Shields", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A unified perspective on multi-domain and multi-task learning", "author": ["Y. Yang", "T.M. Hospedales"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Multi-task learning with low rank attribute embedding for person re-identification", "author": ["C. Su", "F. Yang", "S. Zhang", "Q. Tian", "L.S. Davis", "W. Gao"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Predicting multiple attributes via relative multi-task learning", "author": ["L. Chen", "Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Robust visual tracking via structured multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "IJCV", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Facial landmark detection by deep multi- task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Scalable multitask representation learning for scene classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Deep joint task learning for generic object extraction", "author": ["X. Wang", "L. Zhang", "L. Lin", "Z. Liang", "W. Zuo"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Multi-task cnn model for attribute prediction", "author": ["A.H. Abdulnabi", "G. Wang", "J. Lu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Representation learning via semi-supervised autoencoder for multi-task learning", "author": ["F. Zhuang", "D. Luo", "X. Jin", "H. Xiong", "P. Luo", "Q. He"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Domain generalization for object recognition with multi-task autoencoders", "author": ["M. Ghifary", "W.B. Kleijn", "M. Zhang", "D. Balduzzi"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Multi-task recurrent neural network for immediacy prediction", "author": ["X. Chu", "W. Ouyang", "W. Yang", "X. Wang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "FTML", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In: Science", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "In: NC", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Two distributed-state models for gen- erating high-dimensional time series", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": "Journal of Machine Learning Research", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "AISTATS", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G. Hinton", "G. Taylor"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Temporal autoencoding restricted boltzmann machine", "author": ["C. Hausler", "A. Susemihl"], "venue": "CoRR", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Dynamical binary latent variable models for 3d human pose tracking", "author": ["G.W. Taylor", "et. al"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Phone recognition using restricted boltzmann ma- chines", "author": ["A.R. Mohamed", "G.E. Hinton"], "venue": "In: ICASSP", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Facial expression transfer with input-output temporal restricted boltzmann machines", "author": ["M.D. Zeiler", "L.S.G.W. Taylor", "I. Matthews", "R. Fergus"], "venue": "Action-Affect Classification and Morphing", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N.B. Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Deep boltzmann machine", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2009}, {"title": "Moddrop: adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "In: PAMI", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "ECCV-W", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Skeleton-intrinsic symmetrization of shapes", "author": ["Q. Zheng", "Z. Hao", "H. Huang", "K. Xu", "H. Zhang", "D. Cohen-Or", "B. Chen"], "venue": "Computer Graphics Forum. Vol- ume", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J. Niebles", "H. Wang", "L. Fei-Fei"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["S.B. Wang", "A. Quattoni", "L.P. Morency", "D. Demirdjian", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "Deep learning. Book in preparation for MIT Press (2016", "author": ["Y.B. Ian Goodfellow", "A. Courville"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "There has been so much activity in the field of affective computing that it already contributed to the creation of new research directions in affect analysis [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "Examples from the Body Affect dataset [7] of a person Knocking with various affects: Neutral, Angry, Happy, and Sad.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 5, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 6, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 6, "context": "We use the Body Affect dataset produced by [7] and the Tower Game [8] dataset as the test cases for our novel multi-task approach.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "We use the Body Affect dataset produced by [7] and the Tower Game [8] dataset as the test cases for our novel multi-task approach.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "There are multiple approaches that designed features to reduce the data dimensionality, using mid-level features, and then use a simpler model to do classification [9,10].", "startOffset": 164, "endOffset": 170}, {"referenceID": 9, "context": "There are multiple approaches that designed features to reduce the data dimensionality, using mid-level features, and then use a simpler model to do classification [9,10].", "startOffset": 164, "endOffset": 170}, {"referenceID": 10, "context": "We propose a new hybrid model that enhances the CRBM model with multi-task, discriminative, components based on the work of [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "We evaluate our approach on the Body Affect [7] and Tower Game [8] datasets and show how our results are superior to the state-of-the-art.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "We evaluate our approach on the Body Affect [7] and Tower Game [8] datasets and show how our results are superior to the state-of-the-art.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "\u2013 Evaluations on two multi-task public datasets [7,8].", "startOffset": 48, "endOffset": 53}, {"referenceID": 7, "context": "\u2013 Evaluations on two multi-task public datasets [7,8].", "startOffset": 48, "endOffset": 53}, {"referenceID": 11, "context": "Since initial work [12], there have been an increasing number of approaches addressing the problem of activity recognition using skeletal data [9].", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Since initial work [12], there have been an increasing number of approaches addressing the problem of activity recognition using skeletal data [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "Some methods used knowledge of signal processing to transform a neutral skeleton pose to reflect a certain emotion [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "Other work used a language based modeling of affect [6] where they modeled actions (verbs) and affect (adverbs) using a graph.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "More recent work [13] modeling non-stylized motion for affect communication used segmentation techniques which divided complex motions into a set of motion primitives that they used as dynamic features.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "More recent work such as [7] collected natural body affect datasets where they have varied identity, gender, emotion, and actions of the actors but not used it for classification.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "Multi-Task Learning: Multi-task learning is a natural approach for problems that require simultaneous solutions of several related problems [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 17, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 18, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 19, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 20, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 21, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 22, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 23, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 24, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 25, "context": "of which tasks should be learned [26].", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 29, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 155, "endOffset": 159}, {"referenceID": 31, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 228, "endOffset": 232}, {"referenceID": 33, "context": "Other work used multi-task autoencoders [34] for object recognition in a generalized domain [35], where the tasks were the different domains.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "Other work used multi-task autoencoders [34] for object recognition in a generalized domain [35], where the tasks were the different domains.", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "Other work used multi-task RNNs for interaction prediction in still images [36].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "Representation Learning: Deep learning has been successfully applied to many problems [37].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "Restricted Boltzmann Machines (RBMs) form the building blocks in energy-based deep networks [38,39].", "startOffset": 92, "endOffset": 99}, {"referenceID": 38, "context": "Restricted Boltzmann Machines (RBMs) form the building blocks in energy-based deep networks [38,39].", "startOffset": 92, "endOffset": 99}, {"referenceID": 37, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 3, "endOffset": 10}, {"referenceID": 38, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 3, "endOffset": 10}, {"referenceID": 39, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 85, "endOffset": 89}, {"referenceID": 40, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 42, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 43, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 40, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 46, "endOffset": 50}, {"referenceID": 44, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 75, "endOffset": 79}, {"referenceID": 45, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 103, "endOffset": 107}, {"referenceID": 46, "context": "TRBMs have been applied for transferring 2D and 3D point clouds [47], and polyphonic music generation [48].", "startOffset": 64, "endOffset": 68}, {"referenceID": 47, "context": "TRBMs have been applied for transferring 2D and 3D point clouds [47], and polyphonic music generation [48].", "startOffset": 102, "endOffset": 106}, {"referenceID": 38, "context": "RBMs [39], shown in Figure 2(a), define a probability distribution pR as a Gibbs distribution (1), where v is a vector of visible nodes, h is a vector of hidden nodes, ER is the energy function, and Z is the partition function.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "A binary valued hidden layer hj is defined as a logistic function such that the hidden layer becomes sparse [41,49].", "startOffset": 108, "endOffset": 115}, {"referenceID": 48, "context": "A binary valued hidden layer hj is defined as a logistic function such that the hidden layer becomes sparse [41,49].", "startOffset": 108, "endOffset": 115}, {"referenceID": 40, "context": "CRBMs [41] are a natural extension of RBMs for modeling short term temporal dependencies.", "startOffset": 6, "endOffset": 10}, {"referenceID": 40, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "D-CRBMs are based on the D-RBM model presented in in [11], generalized to account for temporal phenomenon using CRBMs.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "Learning our model is done using Contrastive Divergence (CD) [40], where \u3008\u00b7\u3009data is the expectation with respect to the data and \u3008\u00b7\u3009recon is the expectation with respect to the reconstruction.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "In the literature [4,9] most of the datasets were either single task for activity recognition, not publicly available, too few instances, or only RGB-D without skeleton.", "startOffset": 18, "endOffset": 23}, {"referenceID": 8, "context": "In the literature [4,9] most of the datasets were either single task for activity recognition, not publicly available, too few instances, or only RGB-D without skeleton.", "startOffset": 18, "endOffset": 23}, {"referenceID": 6, "context": "The first dataset is the Body Affect dataset [7], collected using a motion capture sensor, which consists of a set of actors performing several actions with different affects.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "The second dataset is the Tower Game [8], collected using a Kinect sensor, which consists of an interaction between two humans performing a cooperative task, with the goal of classifying different components of entrainment.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Body Affect Dataset: This dataset [7] consists of a library of human movements captured using a motion capture sensor, annotated with actor, action, affect, and gender.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Tower Game Dataset: This dataset [8] is a simple game of tower building often used in social psychology to elicit different kinds of interactive behaviors from the participants.", "startOffset": 33, "endOffset": 36}, {"referenceID": 49, "context": "For pre-processing the Tower Game dataset, we followed the same approach as [50] by forming a body centric transformation of the skeletons generated by the Kinect sensors.", "startOffset": 76, "endOffset": 80}, {"referenceID": 50, "context": "We use the same descriptor provided by [51,52].", "startOffset": 39, "endOffset": 46}, {"referenceID": 51, "context": "We use the same descriptor provided by [51,52].", "startOffset": 39, "endOffset": 46}, {"referenceID": 52, "context": "As for the Body Affect dataset we decided to use the full body centric representation [53] for motion capture sensors resulting in 42 dimensions per frame.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "Baselines and Variants: Since we compare our approach against the results presented in [8] we decided to use the same baselines they used.", "startOffset": 87, "endOffset": 90}, {"referenceID": 53, "context": "SVM+BoW100 and SVM+BoW300: To reduce their dimensionality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52].", "startOffset": 104, "endOffset": 111}, {"referenceID": 51, "context": "SVM+BoW100 and SVM+BoW300: To reduce their dimensionality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52].", "startOffset": 104, "endOffset": 111}, {"referenceID": 54, "context": "We also evaluate our approach using HCRF [55].", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "1 This model is initially prototyped by [56] in the deep learning book.", "startOffset": 40, "endOffset": 44}, {"referenceID": 51, "context": "4 SVM+BoW300[52] 39.", "startOffset": 12, "endOffset": 16}, {"referenceID": 54, "context": "5 HCRF[55] 44.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "3 SVM+Raw [8] 59.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "5 SVM+BoW100 [8] 65.", "startOffset": 13, "endOffset": 16}, {"referenceID": 51, "context": "3 SVM+BoW300 [52] 54.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "8 HCRF[55] 67.", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.", "creator": "LaTeX with hyperref package"}}}