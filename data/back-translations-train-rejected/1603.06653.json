{"id": "1603.06653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Information Theoretic-Learning Auto-Encoder", "abstract": "We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework.", "histories": [["v1", "Tue, 22 Mar 2016 01:05:47 GMT  (441kb)", "http://arxiv.org/abs/1603.06653v1", "8 pages, 4 figures"]], "COMMENTS": "8 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eder santana", "matthew emigh", "jose c principe"], "accepted": false, "id": "1603.06653"}, "pdf": {"name": "1603.06653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 3.06 653v 1 [cs.L G] 22 M"}, {"heading": "1 Introduction", "text": "This regularization assumes classical forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as revolutionary layers [2], but also modern techniques such as dropout [3]. Lately, especially in the subfield of autocoding neural networks, regularization can be achieved using variational methods [4]. In this paper, we propose that there are divergence measures for variable regulation. In a deep learning phase, variant regulation promotes the function implemented by a neural network to be as close as possible to an imposed prior one, which is more restrictive than that imposed by point-by-point regulatory methods such as L1 or L2. Variational methods for deep learning have been proposed by the variable autocoder (VAE) framework [4]."}, {"heading": "2 Information Theoretic-Learning", "text": "Information Theory Learning (ITL) is a field at the interface of machine learning and information theory [6] that includes a family of algorithms that calculate and optimize information theory descriptors such as entropy, divergence, and mutual information. ITL targets are calculated directly (not parametrically) from random samples using Parzen windowing and Renyis entropy [13]."}, {"heading": "2.1 Parzen density estimation", "text": "The Parzen Density Estimation is a non-parametric method for empirically estimating the pdf of a distribution based on data. For samples xi obtained from a distribution p, the Parzen Window Estimation of p nonparametrically asp (x) = 1NN \u2211 i = 1G\u03c3 (x \u2212 xi) can be calculated. (1) 1We still have a problem if we cannot sample from the desired distribution. Intuitively, the Parzen Estimation, as shown in Fig. 1, corresponds to the centering of a Gausal nucleus for each sample xi extracted from p and the sum for estimating the pdf. The optimal kernel size [14] depends on the sample density approaching zero when the number of samples approaches infinity."}, {"heading": "2.2 ITL descriptors", "text": "The entropy of Renyi's alpha order for the trading density function (pdf) p is derived from: H\u03b1 (X) = 11 \u2212 \u03b1 log p\u03b1 (x) dx (2), where p \u2212 L = 2, equation (2) to H2 = \u2212 log Y (x) dx, known as the quadratic entropy of Renyi, in (2), for the entropy of H = 2, equation (2) to H2 = \u2212 log Y (x) dx, known as the quadratic entropy of Renyi, in (2), for the entropy of H = 2, equation (1NN) to H2."}, {"heading": "3 ITL Autoencoders", "text": "Let us define autoencoders as 4-fold AE = {E, D, L, R}. Where E and D are the encoder and the decoder functions, here parameterized as neural networks. L is the reconstruction cost function, which measures the difference between the original data x and their respective reconstructions x = D (E (x). A typical reconstruction cost is an error in the middle square. R is a functional regularization. Here, this functional regularization can only be applied to the encoder E. Nevertheless, although we only regulate the encoder E, the interested researcher could also regulate another intermediate layer of the autoencoder. The general cost function for the ITL-AE can be summarized with the following equation: Cost = L (x, x) + \u03bbR (E, P), (12), where the strength of the regularization is regulated by the scale parameter Gap, and P is the imposed price."}, {"heading": "4 Relation to other work", "text": "Generative Moment Matching Networks (GMMNs) [16] correspond to the specific case in which the input of the decoder D comes from a multidimensional uniform distribution and the reconstruction 2For those interested in such an investigation, we recommend modifying the accompanying code of this paper. In our method of adding more regularization, the number of adaptive weights does not increase. Function L is given by the Euclidean divergence measurement. GMMNs could be applied to generate samples from the original input space itself or from a low-dimensional previously trained autoencoder (SCA) [17] hidden space. An advantage of our approach compared to GMMNs is that we can directly adjust all elements in the 4-fold AU together without the elaborate process of training stacked autoencoders to reduce dimensionality (VAE), whereby the variational autocoding (AE) soluble boundary of the L can be determined directly."}, {"heading": "5 Experiments", "text": "In this section we will show experiments with the ITL-AE architecture described in the previous section. First, for a visual interpretation of the effects of variation regulation, we have trained autoencoders with 2-dimensional latent codes. We used a Gaussian, a Laplaker and a 2D Swissroll distribution as desired precursors. The resulting codes are shown in Fig. 3. Note that in all of these examples the autoencoder was trained in a completely unsupervised manner. However, given the simplicity of the data and the imposed reconstruction costs, some of the numbers were bundled into separate regions of the latent space. Fig. 4 shows some images obtained from the Gaussian mantle by sampling from a linear path on the Swiss roll and random samples. For easier comparisons and to avoid extensive hyperparameter searches, we limited our encoder and decoder E and D to have the same architecture as in [11]."}, {"heading": "5.1 Log-likelihood analysis", "text": "After training with ITL-AE on the training set, we generated 104 images by entering 104 samples from an N (0, 5) distribution into the decoder, and the generated MNIST images were used to calculate a distribution using parcels windows on the high-dimensional image space.3 We calculated the log probability of separate 10k samples from the test set and reported the results in Table 1. The core size of this parcels estimator was selected using the best results on a pre-set cross-validation dataset. This core size was \u03c3 = 0.16. Note that our method achieved the second best results out of all fully interconnected generative models compared. Remember that this was achieved with approximately 106 fewer adaptive parameters than the best method of Adversarial autoencoders."}, {"heading": "6 Conclusions", "text": "Here we derived and validated the Information Theoretic-Learning Autoencoders, a non-parametric and (optional) deterministic alternative to Variational Autoencoders. We also looked again at ITL for neural networks, but this time we focused not on non-parametric cost functions for non-Gaussian signal processing, but on distribution differences to regulate deep architectures. Our results with relatively small 4-layer networks with three-dimensional latent codes used for low learning standards achieved competitive results in log probability analysis of MNIST datasets. Although our results were competitive for fully networked architectures, future work should take into account the scalability of ITL estimators for large-dimensional latent spaces, which is common for large neural networks and convolutionary architectures."}], "references": [{"title": "Deep learning", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": "Book in preparation for MIT Press.(Cited on page 159), 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1929}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Information theoretic learning: Renyi\u2019s entropy and kernel perspectives", "author": ["Jose C Principe"], "venue": "Springer Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W Keith Hastings"], "venue": "Biometrika, vol. 57, no. 1, pp. 97\u2013109, 1970.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1970}, {"title": "Deep convolutional inverse graphics network", "author": ["Tejas D Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1503.03167, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep generative image models using a? laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1486\u20131494.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial autoencoders", "author": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1511.05644, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Information theoretic learning", "author": ["Jose C Principe", "Dongxin Xu", "John Fisher"], "venue": "Unsupervised adaptive filtering, vol. 1, pp. 265\u2013319, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "On measures of entropy and information", "author": ["ALFRPED Rrnyi"], "venue": "Fourth Berkeley symposium on mathematical statistics and probability, 1961, vol. 1, pp. 547\u2013561.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "Density estimation for statistics and data analysis", "author": ["Bernard W Silverman"], "venue": "CRC press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "A kernel method for the two-sample-problem", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte Rasch", "Bernhard Sch\u00f6lkopf", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2006, pp. 513\u2013520.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "arXiv preprint arXiv:1502.02761, 2015. 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "arXiv preprint arXiv:1207.4404, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "arXiv preprint arXiv:1306.1091, 2013. 8", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Deep, regularized neural networks work better in practice than shallow unconstrained neural networks [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "This regularization takes classic forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as convolutional layers [2], but also uses modern techniques such as dropout [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "This regularization takes classic forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as convolutional layers [2], but also uses modern techniques such as dropout [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "Recently, especially in the subfield of autoencoding neural networks, regularization has been accomplished with variational methods [4][5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Recently, especially in the subfield of autoencoding neural networks, regularization has been accomplished with variational methods [4][5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "In this paper we propose Information Theoretic-Learning [6] divergence measures for variational regularization.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Variational methods for deep learning were popularized by the variational autoencoder (VAE) framework proposed by [4] and [5] which also brought the attention of deep learning researchers to the reparametrization trick.", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Variational methods for deep learning were popularized by the variational autoencoder (VAE) framework proposed by [4] and [5] which also brought the attention of deep learning researchers to the reparametrization trick.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "This latent factor is then fed forward to the decoder network and the parameters \u03bc and \u03c3 are regularized using the KL-divergence KL(N(\u03bc, \u03c3)\u2016N(0, 1)) between the inferred distribution and the imposed prior, which has a simple form [4].", "startOffset": 230, "endOffset": 233}, {"referenceID": 6, "context": "This is an approach similar to the inverse cumulative distribution method and does not involve estimation of the partition function, rejection sampling, or other complicated approaches [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 7, "context": "VAE\u2019s methodology has been successfully extended to convolutional autoencoders [8] and more elaborate architectures such as Laplacian pyramids for image generation [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "VAE\u2019s methodology has been successfully extended to convolutional autoencoders [8] and more elaborate architectures such as Laplacian pyramids for image generation [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "To cope with that, generative adversarial networks (GAN) were proposed [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "proposed adversarial autoencoders [11] which use an adversarial discriminator D to tell the low dimensional codes in the output of the encoder from data sampled from a desired distribution.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "In the next section of this paper we review ITL\u2019s Euclidean and Cauchy-Schwarz divergence measures [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Information-theoretic learning (ITL) is a field at the intersection of machine learning and information theory [6] which encompasses a family of algorithms that compute and optimize informationtheoretic descriptors such as entropy, divergence, and mutual information.", "startOffset": 111, "endOffset": 114}, {"referenceID": 12, "context": "ITL objectives are computed directly from samples (non-parametrically) using Parzen windowing and Renyi\u2019s entropy [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "The optimal kernel size [14] depends on the density of samples, which approaches zero as the number of samples approaches infinity.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Furthermore, it is equivalent to maximum mean discrepancy [15] (MMD) statistical test.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "[16] used batches of 1000 samples in their experiments and also why they reduced the data dimensionality with autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "4 Relation to other work Generative Moment Matching Networks (GMMNs) [16] correspond to the specific case where the input of the decoder D comes from a multidimensional uniform distribution and the reconstruction For those interested in such investigation we recommend modifying the companion code of this paper.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "GMMNs could be applied to generate samples from the original input space itself or from a lower dimensional previously trained stacked autoencoder (SCA) [17] hidden space.", "startOffset": 153, "endOffset": 157}, {"referenceID": 3, "context": "Variational Autoencoders (VAE) [4] adapt a lower bound of the variational regularization, R, using parametric, closed form solutions for the KL-divergence.", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "Adversarial autoencoders (AA) [11] have the architecture that inspired our method the most.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "For easier comparisons and to avoid extensive hyperparameter search, we constrained our encoder and decoders, E and D, to have the same architecture as those used in [11] i.", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "Methods Log-likelihood Stacked CAE [17] 121\u00b1 1.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "6 DBN [18] 138\u00b1 2 Deep GSN [19] 214\u00b1 1.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "6 DBN [18] 138\u00b1 2 Deep GSN [19] 214\u00b1 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "1 GAN [10] 225\u00b1 2 GMMN + AE [16] 282\u00b1 2 ITL-AE\u2217 300\u00b1 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "1 GAN [10] 225\u00b1 2 GMMN + AE [16] 282\u00b1 2 ITL-AE\u2217 300\u00b1 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "5 Adversarial Autoencoder [11] 340\u00b1 2 \u2217 Proposed method.", "startOffset": 26, "endOffset": 30}], "year": 2016, "abstractText": "We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework.", "creator": "LaTeX with hyperref package"}}}