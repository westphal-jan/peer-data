{"id": "1603.06677", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Learning Executable Semantic Parsers for Natural Language Understanding", "abstract": "For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.", "histories": [["v1", "Tue, 22 Mar 2016 05:07:16 GMT  (136kb,D)", "http://arxiv.org/abs/1603.06677v1", "Accepted to the Communications of the ACM"]], "COMMENTS": "Accepted to the Communications of the ACM", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["percy liang"], "accepted": false, "id": "1603.06677"}, "pdf": {"name": "1603.06677.pdf", "metadata": {"source": "CRF", "title": "Learning Executable Semantic Parsers for Natural Language Understanding", "authors": ["Percy Liang"], "emails": ["pliang@cs.stanford.edu"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions I.2.7 [Artificial Intelligence]: Natural Language Processing - Language Analysis and Understanding"}, {"heading": "1. INTRODUCTION", "text": "A long-standing goal of artificial intelligence (AI) is to build systems that are able to understand natural language. To focus a little on the notion of \"understanding,\" we say that the system must produce an appropriate action after it has received an input from a human. Example: Context: Knowledge of mathematics Utterance: What is the largest prime number less than 10? Action: 7Context: Knowledge of geography Utterance: What is the highest mountain in Europe? Action: Mt. ElbrusContext: User calendar Utterance: Cancel all my meetings after 4pm tomorrow.Action: (removes meetings from the calendar) We are interested in expressions like the above that require deep understanding and argumentation. This art circle focuses on semantic parsing, an area within the field of natural language processing (NLP) that has grown over the last decade. Semantic parser maps put expressions into semantic representations that support this form of reasoning."}, {"heading": "2. FRAMEWORK", "text": "In this article, we focus on the following problem of natural language comprehension: Since an utterance x in a context c, output of the desired action y (Figure 1 shows the formation of an answer to a question, in which case x is a question, c is a knowledge base, and y is the answer. In a robot application, x is a command, c represents the environment of the robot, and y is the desired sequence of actions to be performed by the robot [29]. To build such a system, we assume that we are given a set of n examples {(xi, ci, yi)} ni = 1. We would like to use these examples to train a model that can generalize to new invisible utterances and contexts.Semantic parsing components. This article focuses on a statistical semantic approach to the above problem, where the key to positive logical form z that connects x and y is."}, {"heading": "3. REFINING THE COMPONENTS", "text": "After visiting the components of a semantic parsing system, we now return to each component and discuss the most important design decisions and possible improvements."}, {"heading": "3.1 Executor", "text": "A basic textbook representation of language is the prime number function (prime number), which can be used to make quantified statements about relationships between objects. For example, \"Any prime number greater than two is odd.\" would be expressed in first order logic as \"x.prime\" (x) more (x, 2) \u2192 odd (x). Note: Context c is here a model (in the sense of model theory) that predicts maps on sets of objects or pairs of objects. Execution of the above logical form in relation to the standard mathematical context would be true (x). [7] gives a detailed overview of how first order logic is used for the semantics of natural language."}, {"heading": "3.2 Grammar", "text": "Remember that the purpose of grammar in this article is merely to define a set of candidate derivatives for each utterance and each context. (This is contrary to a conventional notion of grammar in linguistics, where the goal is to accurately characterize the amount of valid sentences and interpretations. (This divergence is due to two factors: First, we will learn a statistical model of the derivations generated by grammar, so that grammar can be simple and crude.) Second, we might be interested in application-specific logical forms. (In a flight reservation domain, the logical form we take from \"I am in Boston and would like to go to Portland\" is an escape from grammar into the.Boston u to.Portland is certainly not the full linguistic meaning of the utterance, but sufficient for the task at hand. Note that the link between language and logic here is less than \"10\" in direct comparison."}, {"heading": "3.3 Model", "text": "At the core of a statistical model is a functional result (x, c, d) that assesses how good a derivative d is in terms of expression x and context c. In Section 2 we described a simple log-linear model (2) in which score (x, c, d) = \u03c6 (x, c, d) \u00b7 \u03b8 and with simple rule features. There are two ways to improve the model: use of more expressive features and use of a nonlinear scoring function. Most existing semantic parsers stick to a linear model and more targeted features. A simple class of features {a, b} is equal to the number of times in which word a appears in expression and predicate b occurs in logical form; if predicate b is generated by a floating rule, then it is allowed to reach across the entire expression; otherwise it must appear in the span of b. The above features are numerous, but require quite a lot of flexibility."}, {"heading": "3.4 Parsing", "text": "Most semantic parsing algorithms are based on diagram parses, where we generate a series of candidate derivatives recursively for each syntactic category (e.g. NP) and span (e.g. 3: 5). Diagram parsing has two disadvantages: Firstly, it is not easy to support an incremental contextual interpretation: the characteristics of a span [i: j] can only depend on the sub-derivatives in that span, not on the derivatives constructed before i. This makes modelling of the anaphora (resolution of \"es\") more difficult. A solution is not to parse the diagram but to parse it by shift reduction [40]. Here, the expression from left to right is analyzed, and new sub-derivatives can depend arbitrarily on the sub-derivatives constructed in this way. A second problem is that the diagram parses are generally used in a fixed order - for example, the increase in the size of the sub-derivatives may result in the number of parsing the agenda being applied accordingly."}, {"heading": "3.5 Learner", "text": "There are two aspects of learning: triggering the grammar rules and estimating the model parameters. It is important to remember that practical semantic parsers do not do everything from scratch, and often the hard coded grammar rules are just as important as the training examples. First, some lexical rules depicting entities (e.g. [paris \u21d2 ParisFrance]), dates and numbers are generally taken for granted [37], although we do not have to assume that these rules are perfect [21]. These rules are often implicit [21, 4]. The way the rest of grammar is treated varies from approach to approach. In the CCG-like approach, the introduction of lexical rules is an important part of learning. In [37] a procedure called GENLEX is used to generate lexical rules from a pair of forms (x, z)."}, {"heading": "4. DATASETS AND RESULTS", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5. DISCUSSION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "6. REFERENCES", "text": "[1] Y. Artzi and K. L. Zettlemoyer. Broad-coverage CCGsemantic parsing with AMR. In Empirical Methods in Natural Language Processing (EMNLP), 2015. [2] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions.Transactions of the Association for Computational Linguistics (TACL), 1: 49-62, 2013. [3] L. BanaRescu, C. B. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Palmer, and N. Schneider. Abstract meaning representation for sembanking. In 7th Linguistic Annotation Workshop and Interoperability with Discourse, 2013. [4] J. Berant, A. Chou."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Y. Artzi", "K.L.L. Zettlemoyer"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer"], "venue": " Transactions of the Association for Computational Linguistics (TACL), 1:49\u201362", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Abstract meaning representation for sembanking", "author": ["L. BanaRescu", "C.B.S. Cai", "M. Georgescu", "K. Griffitt", "U. Hermjakob", "K. Knight", "P. Koehn", "M. Palmer", "N. Schneider"], "venue": "7th Linguistic Annotation Workshop and Interoperability with Discourse", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Imitation learning of agenda-based semantic parsers", "author": ["J. Berant", "P. Liang"], "venue": "Transactions of the Association for Computational Linguistics (TACL), 0", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation and Inference for Natural Language: A First Course in Computational Semantics", "author": ["P. Blackburn", "J. Bos"], "venue": "CSLI Publishers", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "International Conference on Management of Data (SIGMOD), pages 1247\u20131250", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 82\u201390", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "An analysis of the AskMSR question-answering system", "author": ["E. Brill", "S. Dumais", "M. Banko"], "venue": "Association for Computational Linguistics (ACL), pages 257\u2013264", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Q. Cai", "A. Yates"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 859\u2013865", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "Computational Natural Language Learning (CoNLL), pages 18\u201327", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Conference on Learning Theory (COLT)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["J. Krishnamurthy", "T. Kollar"], "venue": "Transactions of the Association for Computational Linguistics (TACL), 1:193\u2013206", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T. Mitchell"], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 754\u2013765", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["N. Kushman", "R. Barzilay"], "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 826\u2013836", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1223\u20131233", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Lambda dependency-based compositional semantics", "author": ["P. Liang"], "venue": "arXiv", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Association  for Computational Linguistics (ACL), pages 590\u2013599", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Reviews of Linguistics, 1(1):355\u2013376", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": "International Conference on Machine Learning (ICML), pages 1671\u20131678", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "A fully statistical approach to natural language interfaces", "author": ["S. Miller", "D. Stallard", "R. Bobrow", "R. Schwartz"], "venue": "Association for Computational Linguistics (ACL), pages 55\u201361", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "The proper treatment of quantification in ordinary English", "author": ["R. Montague"], "venue": "Approaches to Natural Language, pages 221\u2013242", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1973}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": "Transactions of the Association for Computational Linguistics (TACL), 2(10):377\u2013392", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The Syntactic Process", "author": ["M. Steedman"], "venue": "MIT Press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy"], "venue": "Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding Natural Language", "author": ["T. Winograd"], "venue": "Academic Press", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1972}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "Association for Computational Linguistics (ACL), pages 960\u2013967", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "The lunar sciences natural language information system: Final report", "author": ["W.A. Woods", "R.M. Kaplan", "B.N. Webber"], "venue": "Technical report, BBN Report 2378, Bolt Beranek and Newman Inc.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1972}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["W. Yih", "M. Chang", "X. He", "J. Gao"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["M. Zelle", "R.J. Mooney"], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 1050\u20131055", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "Uncertainty in Artificial Intelligence (UAI), pages 658\u2013666", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678\u2013687", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning context-dependent mappings from sentences to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Type-driven incremental semantic parsing with polymorphism", "author": ["K. Zhao", "L. Huang"], "venue": "North American Association for Computational Linguistics (NAACL)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "Semantic parsing is rooted in formal semantics, pioneered by logician Richard Montague [25], who famously argued that there is \u201cno important theoretical difference between natural languages and the artificial languages of logicians.", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": "Early examples included LUNAR, a natural language interface into a database about moon rocks [34], and SHRDLU, a system that could both answer questions and perform actions in a toy blocks world environment [32].", "startOffset": 93, "endOffset": 97}, {"referenceID": 31, "context": "Early examples included LUNAR, a natural language interface into a database about moon rocks [34], and SHRDLU, a system that could both answer questions and perform actions in a toy blocks world environment [32].", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "Even question answering systems relied less on understanding and more on a shallower analysis coupled with a large collection of unstructured text documents [10], typified by the TREC competitions.", "startOffset": 157, "endOffset": 161}, {"referenceID": 35, "context": "The spirit of deep understanding was kept alive by researchers in statistical semantic parsing [36, 24, 33, 37, 19].", "startOffset": 95, "endOffset": 115}, {"referenceID": 23, "context": "The spirit of deep understanding was kept alive by researchers in statistical semantic parsing [36, 24, 33, 37, 19].", "startOffset": 95, "endOffset": 115}, {"referenceID": 32, "context": "The spirit of deep understanding was kept alive by researchers in statistical semantic parsing [36, 24, 33, 37, 19].", "startOffset": 95, "endOffset": 115}, {"referenceID": 36, "context": "The spirit of deep understanding was kept alive by researchers in statistical semantic parsing [36, 24, 33, 37, 19].", "startOffset": 95, "endOffset": 115}, {"referenceID": 18, "context": "The spirit of deep understanding was kept alive by researchers in statistical semantic parsing [36, 24, 33, 37, 19].", "startOffset": 95, "endOffset": 115}, {"referenceID": 12, "context": "The first is reducing the amount of supervision from annotated logical forms to answers [13, 21]:", "startOffset": 88, "endOffset": 96}, {"referenceID": 20, "context": "The first is reducing the amount of supervision from annotated logical forms to answers [13, 21]:", "startOffset": 88, "endOffset": 96}, {"referenceID": 20, "context": "This results in a more difficult learning problem, but [21] showed that it is possible to solve it without degrading accuracy.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "Previous semantic parsers had only been trained on limited domains such as US geography, but the creation of broad-coverage knowledge bases such as Freebase [8] set the stage for a new generation of semantic parsers for question answering.", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "Initial systems required annotated logical forms [11], but soon, systems became trainable from answers [4, 18, 5].", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "Initial systems required annotated logical forms [11], but soon, systems became trainable from answers [4, 18, 5].", "startOffset": 103, "endOffset": 113}, {"referenceID": 17, "context": "Initial systems required annotated logical forms [11], but soon, systems became trainable from answers [4, 18, 5].", "startOffset": 103, "endOffset": 113}, {"referenceID": 4, "context": "Initial systems required annotated logical forms [11], but soon, systems became trainable from answers [4, 18, 5].", "startOffset": 103, "endOffset": 113}, {"referenceID": 25, "context": "Semantic parsers have even been extended beyond fixed knowledge bases to semi-structured tables [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": "[29, 2], identifying objects in a scene [23, 15], converting natural language to regular expressions [17], and many others.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[29, 2], identifying objects in a scene [23, 15], converting natural language to regular expressions [17], and many others.", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[29, 2], identifying objects in a scene [23, 15], converting natural language to regular expressions [17], and many others.", "startOffset": 40, "endOffset": 48}, {"referenceID": 14, "context": "[29, 2], identifying objects in a scene [23, 15], converting natural language to regular expressions [17], and many others.", "startOffset": 40, "endOffset": 48}, {"referenceID": 16, "context": "[29, 2], identifying objects in a scene [23, 15], converting natural language to regular expressions [17], and many others.", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "In a robotics application, x is a command, c represents the robot\u2019s environment, and y is the desired sequence of actions to be carried by the robot [29].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "(R1) prime \u21d2 NP[primes] (R2) 10 \u21d2 NP[10] (R3) less than NP[z] \u21d2 QP[(\u2212\u221e, z)] (R4) NP[z1] QP[z2] \u21d2 NP[z1 \u2229 z2] (R5) largest NP[z] \u21d2 NP[max(z)] (R6) largest NP[z] \u21d2 NP[min(z)] (R7) What is the NP[z]? \u21d2 ROOT[z]", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "NP[primes] NP[10]", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 130, "endOffset": 151}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "For example, define F = 7 features, each counting the number of times a given grammar rule is invoked in d, so that \u03c6(x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and \u03c6(x, c, d2) = [1, 1, 1, 1, 0, 1, 1].", "startOffset": 170, "endOffset": 191}, {"referenceID": 6, "context": "[7] gives a detailed account on how first-order logic is used for natural language semantics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Another logical language, which can be viewed as syntactic sugar for lambda calculus, is lambda dependency-based semantics (DCS) [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "It is thus common to use applicationspecific logical forms, for example, regular expressions [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "One common approach to the grammar in semantic parsing is combinatory categorial grammar (CCG) [28], which had been developed extensively in linguistics before it was first used for semantic parsing [37].", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "One common approach to the grammar in semantic parsing is combinatory categorial grammar (CCG) [28], which had been developed extensively in linguistics before it was first used for semantic parsing [37].", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "f(x) \u2227 less(x, y) NP[10]", "startOffset": 20, "endOffset": 24}, {"referenceID": 37, "context": "It is common to use other combinators which both handle more complex linguistic phenomena such as NP coordination \u201cintegers that divide 2 or 3\u201d as well as ungrammatical language [38], although these issues can also be handled by having a more expansive lexicon [19].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "It is common to use other combinators which both handle more complex linguistic phenomena such as NP coordination \u201cintegers that divide 2 or 3\u201d as well as ungrammatical language [38], although these issues can also be handled by having a more expansive lexicon [19].", "startOffset": 261, "endOffset": 265}, {"referenceID": 9, "context": "N[prime] N|N[less] N[10]", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "When the logical forms are simple and we have strong type constraints, this strategy can be quite effective [5, 30, 26].", "startOffset": 108, "endOffset": 119}, {"referenceID": 29, "context": "When the logical forms are simple and we have strong type constraints, this strategy can be quite effective [5, 30, 26].", "startOffset": 108, "endOffset": 119}, {"referenceID": 25, "context": "When the logical forms are simple and we have strong type constraints, this strategy can be quite effective [5, 30, 26].", "startOffset": 108, "endOffset": 119}, {"referenceID": 3, "context": "This way, the lexical mapping need not be learned from scratch using only the training data [4, 18].", "startOffset": 92, "endOffset": 99}, {"referenceID": 17, "context": "This way, the lexical mapping need not be learned from scratch using only the training data [4, 18].", "startOffset": 92, "endOffset": 99}, {"referenceID": 39, "context": "A solution is to use shift-reduce parsing rather than chart parsing [40].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "This motivates agenda-based parsing, in which derivations that are deemed more promising by the model are built first [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 36, "context": ", [paris \u21d2 ParisFrance]), dates, and numbers are generally assumed to be given [37], though we need not assume that these rules are perfect [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": ", [paris \u21d2 ParisFrance]), dates, and numbers are generally assumed to be given [37], though we need not assume that these rules are perfect [21].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "These rules are also often represented implicitly [21, 4].", "startOffset": 50, "endOffset": 57}, {"referenceID": 3, "context": "These rules are also often represented implicitly [21, 4].", "startOffset": 50, "endOffset": 57}, {"referenceID": 36, "context": "In [37], a procedure called GENLEX is used to generate candidate lexical rules from a utterance-logical form pair (x, z).", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "A more generic induction algorithm based on higher-order unification does not require any initial grammar [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "[33] use machine translation ideas to induce a synchronous grammar (which can also be used to generate utterances from logical forms).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In approaches that learn from denotations y [21, 4], an initial crude grammar is used to generate candidate logical forms, and rest of the work is done by the features.", "startOffset": 44, "endOffset": 51}, {"referenceID": 3, "context": "In approaches that learn from denotations y [21, 4], an initial crude grammar is used to generate candidate logical forms, and rest of the work is done by the features.", "startOffset": 44, "endOffset": 51}, {"referenceID": 21, "context": "As we discussed earlier, parameter estimation can be performed by stochastic gradient descent on the log-likelihood; similar objectives based on max-margin are also possible [22].", "startOffset": 174, "endOffset": 178}, {"referenceID": 4, "context": "It can be helpful to also add an L1 regularization term \u03bb\u2016\u03b8\u20161, which encourages feature weights to be zero, which produces a more compact model that generalizes better [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 13, "context": "In addition, one can use AdaGrad [14], which maintains a separate step size for each feature.", "startOffset": 33, "endOffset": 37}, {"referenceID": 35, "context": "The Geo880 dataset [36] drove nearly a decade of semantic parsing research.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "On this dataset, learning from logical forms [18] and answers [21] both achieve around 90%", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "On this dataset, learning from logical forms [18] and answers [21] both achieve around 90%", "startOffset": 62, "endOffset": 66}, {"referenceID": 37, "context": "The ATIS-3 dataset [38] consists of 5418 utterances paired with logical forms (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The Regexp824 dataset [17] consists of 824 natural language and regular expression pairs (e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17] uses semantic unification to test for logical form equivalence and obtains 65.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The Free917 dataset [11] consists of 917 examples of questionlogical form pairs that can be answered via Freebase, e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "WebQuestions [4] is another dataset on Freebase consisting of 5810 question-answer pairs (no logical forms) such as \u201cwhat do australia call their money?\u201d Like Free917, the questions are not very compositional, but unlike Free917, they are real questions asked by people on the Web independent from Freebase, so they are more realistic and more varied.", "startOffset": 13, "endOffset": 16}, {"referenceID": 25, "context": "The goal of WikiTableQuestions [26] is to extend question answering beyond Freebase to HTML tables on Wikipedia, which are semi-structured.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "[30] proposed a new recipe for quickly using crowdsourcing to generate new compositional semantic parsing datasets consisting of question-logical form pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] introduced a dataset of 706 navigation instructions (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] obtained 65.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "One idea is to use general paraphrasing models to map input utterances to the \u201ccanonical utterances\u201d of logical forms [5, 30].", "startOffset": 118, "endOffset": 125}, {"referenceID": 29, "context": "One idea is to use general paraphrasing models to map input utterances to the \u201ccanonical utterances\u201d of logical forms [5, 30].", "startOffset": 118, "endOffset": 125}, {"referenceID": 17, "context": "One could also use domain-general logical forms that capture the basic predicate-argument structures of sentences [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "Abstract meaning representation (AMR) [3] is one popular representation backed by an extension linguistic annotation effort.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "Multiple AMR parsers have been developed, including one based on CCG [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "One can use a large corpus of text to exploit even weaker supervision [16, 27].", "startOffset": 70, "endOffset": 78}, {"referenceID": 26, "context": "One can use a large corpus of text to exploit even weaker supervision [16, 27].", "startOffset": 70, "endOffset": 78}, {"referenceID": 8, "context": "More generally, one can think about language interpretation in a reinforcement learning setting [9], where an agent who presented with an utterance in some context performs some action, and receives a corresponding reward signal.", "startOffset": 96, "endOffset": 99}, {"referenceID": 38, "context": "This framework highlights the importance of context-dependence in language interpretation [39, 2].", "startOffset": 90, "endOffset": 97}, {"referenceID": 1, "context": "This framework highlights the importance of context-dependence in language interpretation [39, 2].", "startOffset": 90, "endOffset": 97}, {"referenceID": 34, "context": "Due to their empirical success, there has been a recent surge of interest in using recurrent neural networks and their extensions for solving NLP tasks such as machine translation and question answering [35, 31].", "startOffset": 203, "endOffset": 211}, {"referenceID": 30, "context": "Due to their empirical success, there has been a recent surge of interest in using recurrent neural networks and their extensions for solving NLP tasks such as machine translation and question answering [35, 31].", "startOffset": 203, "endOffset": 211}], "year": 2016, "abstractText": "For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.", "creator": "LaTeX with hyperref package"}}}