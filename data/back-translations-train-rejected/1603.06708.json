{"id": "1603.06708", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "abstract": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "histories": [["v1", "Tue, 22 Mar 2016 09:03:40 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v1", null], ["v2", "Wed, 6 Apr 2016 14:54:28 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "fan wei", "junchi yan", "weishan dong", "qingshan liu", "xiaoyu zhang", "hongyuan zha"], "accepted": false, "id": "1603.06708"}, "pdf": {"name": "1603.06708.pdf", "metadata": {"source": "CRF", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "authors": ["Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Xiaoyu Zhang"], "emails": ["dongweis}@cn.ibm.com.", "fanwei@stanford.edu.", "jcyan@sei.ecnu.edu.cn", "qsliu@nuist.edu.cn.", "zhangxiaoyu@iie.ac.cn"], "sections": [{"heading": null, "text": "The goal is to learn a classifier to map the input instance into a label vector space where each instance is associated with multiple labels rather than a single label in China. Unlike in Beijing, multi-level labels are often assumed to be correlated in multi-level learning processes. Often, this image correlation between labels is also useful for accurately predicting labels from test situations. Due to their empirical success, multi-level learning has been suggested in various areas such as image annotation [3], video concept recognition [4], website categorization, and visual object recognition [5].In recent years, many multi-level learning algorithms have been proposed. A simplified approach is to decompose multi-student learning into multiple independent binary classification problems (one per label or category)."}, {"heading": "II. BACKGROUND", "text": "s leave X = Q = Q = Q = Q = Q = Q = Q-dimensional input space and Y = {\u2212 1, + 1} L the finite amount of possible terms. In the face of a multi-level education D = {(xi, yi) ni = 1, where xi = [xi1,.., xid] X is the i-th instance and yi = [yi1,.., yiL] Y is the label vector that with xi. yij is + 1 if xi has the j-th label and \u2212 1 otherwise. The aim of multi-level learning is to learn a multi-level learner h: X \u2192 2L from the education theorem D so that we can predict a set of labels for each invisible instance. As mentioned above, most existing multi-level learning methods attempt to exploit the correlations between the labels to exploit them."}, {"heading": "III. MULTI-LABEL SELF-PACED LEARNING", "text": "At this point, we first present a general multi-level learning formulation with a self-determined paradigm. Afterwards, we give a basic way to find the self-determined functions for the realization of desired self-step schemes. Finally, an efficient algorithm is developed to solve the proposed optimization problem."}, {"heading": "A. Proposed Formulation", "text": "As shown in (1), the objective function treats all learning instances and all learning tasks with the label Q \u00b7 i = a = a = a =. However, we should prefer the simpler instances and the simpler labels to learning. Furthermore, the non-convexity of the problem (1) makes the problem of bad local minima worse. In order to overcome these deficits, an interesting principle is that learning should first be done in the simple instances and then gradually put to the test. This is in line with the idea of self-incremental learning, which is inspired by the way people learn. Indeed, self-incremental learning has empirical usefulness to alleviate bad local minima and to achieve better model generalization [20], [25]. Specifically, here we have \"simple\" and \"hard\" labels as well as \"simple\" and \"hard\" instances. With the aim of integrating the self-incremental learning paradigm [v] into the multi-level learning regime, we propose."}, {"heading": "B. Self-Paced Function", "text": "First, we present the self-controlled function from the most recent work: Definition 1. Let us assume that v is a weight variable, l is the loss, and \u03bb is the learning tempo parameter. f (v, \u03bb) is called a self-controlled function, if1) f (v, \u03bb) is called a convex (v) in relation to v (0, 1); 2) v \u00b2 is monotonically decreasing in relation to l, and itholds, liml \u2192 0 v \u2264 1, liml \u2192 1, liml \u2192 II = 0; 3) v \u00b2 is monotonically increasing in relation to v (0, and itholds, lim \u00b2 s in relation to l, and itholds, liml \u00b2 s in relation to liml (l)."}, {"heading": "C. Optimization", "text": "We apply an alternative strategy for solving the optimization problem (2). First, we rewrite the objective function (2) as: min W, A, Q [0,1] m \u00b7 n, V [0,1] n \u00b7 Ln \u2211 i = 1 L \u2211 l = 1 v (l) i L (wl, qi; xi, yil) + \u03b1 L \u2211 l = 1 \u00b2 l \u00b2 2 + \u03b2 n \u2211 i = 1 m \u00b2 j = 1 qij \u00b2 l \u00b2 i = 1 L \u2211 l = 1 l \u00b2 l (v (l) i, qi) (11) s.t. m \u00b2 j = 1 qij = 1 \u00b2 l \u00b2 2, \u0432 i \u00b2 [1, n] where f (v (l) i \u2212 i, \u03bb) can be fixed as any function of (4), (6), (8) and (10) as the self-controlled solution of i \u00b2 l (l): the solution of V with other variables."}, {"heading": "IV. EXPERIMENTS", "text": "To verify the effectiveness of the proposed MLSPL, we perform our method using three benchmark datasets: the Flags dataset, the Scene dataset, and the Emotions dataset. 2 Flags and Scenes are two image datasets, and Emotions are one music dataset. They have 194, 593, and 2407 instances, or 7, 6, 6 possible labels, respectively. To further evaluate the performance of MLSPL, we compare them with several state-of-the-art multi-label learning algorithms 3. First, we compare with ML-LOC [1], which is the most closely related multi-label learning approach to ours. We also compare with ML-kNN [27] and RankSVM [7], which take into account first and second order correlations. Furthermore, we compare with TRAM [28], which was recently proposed."}, {"heading": "A. General Performance", "text": "In this section, we will test the overall performance of our method using the three sets of data. We will use the Sigmod activation function as our self-directed learning scheme in this experiment. Tables I, II and III summarize the performance of the different methods in terms of the five evaluation criteria. Note that for the average accuracy, a greater value means better performance, while for the other four criteria, the smaller the better. These tables show that our method significantly outperforms the other approaches in all three sets of data. For example, our method achieves 13.4%, 8.4%, 25.8%, 4.1% and 1.7% relative improvement in terms of the five evaluation criteria compared to the TRAM, which achieves the second best results in this dataset. Furthermore, as can be seen from the objective functions (1) and (2), ML-LOC is a special case for our method: If all entries in V 1, our method is reduced to ML-LOC."}, {"heading": "B. Studying the Performance of Self-Paced Functions", "text": "In this section, we examine the performance of different self-step functions on the three datasets, and the results are presented in Figure 2. First, we can see that our method performs well with different self-step functions, which shows that the self-step functions we provide in Section 2 are effective for multi-label learning. Furthermore, we find that the exponential function on the flag datasets performs best on all five criteria except for the hamming distance, while the sigmod function on the emotion dataset outperforms all other functions. Tanh is similar to Arctan on all five criteria on the three datasets, suggesting that different scenarios actually require different self-step tutorials, so it is necessary to develop more autonomous functions for multi-label learning."}, {"heading": "V. CONCLUSION", "text": "We proposed a novel multi-label learning algorithm, MLSPL. By introducing a self-regulating regularizer, MLSPL can learn labels in the order of labels and instances from easy to difficult. Given that scenarios in the real world usually require different learning schemes, we propose a general way to find the desired regularizer for self-controlled learning. Experiments with benchmark data sets have shown the effectiveness of SPMTL compared to the most modern methods."}], "references": [{"title": "Multi-label learning by exploiting label correlations locally.", "author": ["S.-J. Huang", "Z.-H. Zhou"], "venue": "in AAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A review on multi-label learning algorithms", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 8, pp. 1819\u20131837, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1819}, {"title": "Multi-modal image annotation with multi-instance multi-label lda", "author": ["C.-T. Nguyen", "D.-C. Zhan", "Z.-H. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive multi-label learning for video concept detection", "author": ["J. Wang", "Y. Zhao", "X. Wu", "X.-S. Hua"], "venue": "MIR, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting shared subspace for multilabel classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "KDD, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label multiple kernel learning by stochastic approximation: Application to visual object recognition", "author": ["S. Bucak", "R. Jin", "A.K. Jain"], "venue": "NIPS, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Hierarchical document categorization with support vector machines", "author": ["L. Cai", "T. Hofmann"], "venue": "CIKM, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning hierarchical multi-category text classification models", "author": ["J. Rousu", "C. Saunders", "S. Szedmak", "J. Shawe-Taylor"], "venue": "JMLR, vol. 7, pp. 1601\u20131626, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical classification: combining bayes with svm", "author": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"], "venue": "ICML, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale max-margin multi-label classification with priors", "author": ["B. Hariharan", "L. Zelnik-Manor", "M. Varma", "S. Vishwanathan"], "venue": "ICML, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern recognition, vol. 37, pp. 1757\u20131771, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-labelled classification using maximum entropy method", "author": ["S. Zhu", "X. Ji", "W. Xu", "Y. Gong"], "venue": "SIGIR, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Model-shared subspace boosting for multi-label classification", "author": ["R. Yan", "J. Tesic", "J.R. Smith"], "venue": "KDD, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Correlative multi-label video annotation", "author": ["G.-J. Qi", "X.-S. Hua", "Y. Rui", "J. Tang", "T. Mei", "H.-J. Zhang"], "venue": "MM, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label learning by exploiting label dependency", "author": ["M.-L. Zhang", "K. Zhang"], "venue": "KDD, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-instance multi-label learning with weak label", "author": ["S.-J. Yang", "Y. Jiang", "Z.-H. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning with multi-label svm classification", "author": ["X. Li", "Y. Guo"], "venue": "IJCAI, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-label multiple kernel learning", "author": ["S. Ji", "L. Sun", "R. Jin", "J. Ye"], "venue": "NIPS, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Selfpaced learning with diversity", "author": ["L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann"], "venue": "NIPS, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-paced learning for matrix factorization", "author": ["Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann"], "venue": "AAAI, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view self-paced learning for clustering", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "IJCAI, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A selfpaced multiple-instance learning framework for co-saliency detection", "author": ["D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han"], "venue": "ICCV, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced curriculum learning", "author": ["L. Jiang", "D. Meng", "Q. Zhao", "S. Shan", "A.G. Hauptmann"], "venue": "AAAI, 2015.  7", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "TIST, vol. 2, no. 3, p. 27, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Pattern recognition, vol. 40, no. 7, pp. 2038\u20132048, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Transductive multilabel learning via label set propagation", "author": ["X. Kong", "M.K. Ng", "Z.-H. Zhou"], "venue": "TKDE, vol. 25, no. 3, pp. 704\u2013719, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine learning, vol. 39, no. 2, pp. 135\u2013168, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-instance multi-label learning", "author": ["Z.-H. Zhou", "M.-L. Zhang", "S.-J. Huang", "Y.-F. Li"], "venue": "Artificial Intelligence, vol. 176, no. 1, pp. 2291\u2013 2320, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-label learning has attracted much attention in the past decade [1], [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "INTRODUCTION Multi-label learning has attracted much attention in the past decade [1], [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "However, such a solution does not consider the relationship among labels, whereas previous studies [7], [1] have revealed that the label relationship is quite helpful and should be considered.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "However, such a solution does not consider the relationship among labels, whereas previous studies [7], [1] have revealed that the label relationship is quite helpful and should be considered.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 272, "endOffset": 276}, {"referenceID": 19, "context": "Inspired by how children learn concepts, self-paced learning [20] advocates a paradigm that learning should first consider \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually take \u2018complex\u2019 or \u2018hard\u2019 instances into account.", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 22, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 298, "endOffset": 302}, {"referenceID": 23, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 332, "endOffset": 336}, {"referenceID": 0, "context": "BACKGROUND We first define notations and then briefly introduce the work [1] that our approach is originated from.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Among these methods, one representative algorithm is ML-LOC [1] which tries to exploit the correlations locally; it assumes that the instances are partitioned into m different clusters and each cluster shares a subset of label correlations.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n L \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 19, "context": "Indeed, selfpaced learning has empirically demonstrated its usefulness for mitigating bad local minima and achieving better model generalization [20], [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "Indeed, selfpaced learning has empirically demonstrated its usefulness for mitigating bad local minima and achieving better model generalization [20], [25].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 24, "endOffset": 29}, {"referenceID": 19, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Self-Paced Function First, we introduce definition of the self-paced function from the recent work [22]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "f(v, \u03bb) is called self-paced function, if 1) f(v, \u03bb) is convex with respect to v \u2208 [0, 1]; 2) v\u2217 is monotonically decreasing with respect to l, and it holds that liml\u21920 v\u2217 \u2264 1, liml\u2192\u221e v\u2217 = 0; 3) v\u2217 is monotonically increasing with respect to \u03bb, and it holds that lim\u03bb\u21920 v\u2217 = 0, lim\u03bb\u2192\u221e v\u2217 \u2264 1; where v\u2217(l, \u03bb) = arg minv\u2208[0,1] vl + f(v, \u03bb) for fixed l, \u03bb.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "f(v, \u03bb) is called self-paced function, if 1) f(v, \u03bb) is convex with respect to v \u2208 [0, 1]; 2) v\u2217 is monotonically decreasing with respect to l, and it holds that liml\u21920 v\u2217 \u2264 1, liml\u2192\u221e v\u2217 = 0; 3) v\u2217 is monotonically increasing with respect to \u03bb, and it holds that lim\u03bb\u21920 v\u2217 = 0, lim\u03bb\u2192\u221e v\u2217 \u2264 1; where v\u2217(l, \u03bb) = arg minv\u2208[0,1] vl + f(v, \u03bb) for fixed l, \u03bb.", "startOffset": 319, "endOffset": 324}, {"referenceID": 0, "context": "In order to find the self-paced function f , we first find a family of S-shaped functions g\u03bb(l) with range in [0, 1] such that it is monotonically decreasing with respect to l, and that liml\u2192\u221e g\u03bb(l) = 0 as well as liml\u21920 g\u03bb(l) \u2264 1.", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "A smooth function f(v, \u03bb) is a self-paced function corresponding to v\u2217 if and only if \u2202f(v,\u03bb) \u2202v = \u2212s(\u03bb, v) and \u2202s(v,\u03bb) \u2202v \u2264 0 for v \u2208 [0, 1].", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "This v\u2217 is invariant under l/\u03bb Clearly v\u2217(l, \u03bb) \u2208 [0, 1] as l \u2265 0.", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = 2\u03bb (v\u22122)v \u2264 0 when v \u2208 [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = 1 2(v\u22121)v \u2264 0 when v \u2208 [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = \u2212 1 2v \u221a \u2212 ln v which is no greater than 0 when v \u2208 [0, 1].", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "min v (l) i \u2208[0,1] v (l) i L(wl,qi;xi, yil) + f(v (l) i , \u03bb) (12)", "startOffset": 13, "endOffset": 18}, {"referenceID": 25, "context": "This is a cost-sensitive SVM model, which can be solved by LIBSVM [26] software package.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "j=1 qij = 1,qi \u2208 [0, 1]", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "We first compare with ML-LOC [1] that is the most related multi-label learning approach to ours.", "startOffset": 29, "endOffset": 32}, {"referenceID": 26, "context": "We also compare with ML-kNN [27] and RankSVM [7] that consider first-order and second order correlations, respectively.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "We also compare with ML-kNN [27] and RankSVM [7] that consider first-order and second order correlations, respectively.", "startOffset": 45, "endOffset": 48}, {"referenceID": 27, "context": "In addition, we compare with TRAM [28] that is proposed recently.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Finally, we compare with another baseline BSVM [12] that learns a binary SVM for each label.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "LibSVM [26] is used to implement the SVM models for BSVM, MLLOC and MLSPL.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "These criteria measure the performance from different aspects and the detailed definitions can be found in [29], [30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "These criteria measure the performance from different aspects and the detailed definitions can be found in [29], [30].", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "creator": "LaTeX with hyperref package"}}}