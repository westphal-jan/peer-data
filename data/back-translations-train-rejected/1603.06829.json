{"id": "1603.06829", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Multi-velocity neural networks for gesture recognition in videos", "abstract": "We present a new action recognition deep neural network which adaptively learns the best action velocities in addition to the classification. While deep neural networks have reached maturity for image understanding tasks, we are still exploring network topologies and features to handle the richer environment of video clips. Here, we tackle the problem of multiple velocities in action recognition, and provide state-of-the-art results for gesture recognition, on known and new collected datasets. We further provide the training steps for our semi-supervised network, suited to learn from huge unlabeled datasets with only a fraction of labeled examples.", "histories": [["v1", "Tue, 22 Mar 2016 15:26:26 GMT  (2955kb,D)", "http://arxiv.org/abs/1603.06829v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["otkrist gupta", "dan raviv", "ramesh raskar"], "accepted": false, "id": "1603.06829"}, "pdf": {"name": "1603.06829.pdf", "metadata": {"source": "CRF", "title": "Multi-velocity neural networks for gesture recognition in videos", "authors": ["Otkrist Gupta", "Dan Raviv", "Ramesh Raskar"], "emails": ["otkrist@mit.edu", "raviv@mit.edu", "raskar@media.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "Over the past two decades, we have seen several attempts to automate the interaction between individuals by replacing or amplifying multiple spoken words. [32] Our body language, voice pitch, intonation and volume, the movements of our students, or our chronological choices are just a few examples that highlight the richness of human communication skills. [32] A specific subset of nonverbal interactions examined in this paper are based on facial expressions. Their perception triggers rapid cognitive processes in the brain and has both communicative and reflexive components [12]. They support verbal communication by providing context to what we say, making their recognition important for the study of social interactions. We use computers daily to interact with artificial agents in an increasing number of tasks. Advances in language and natural language processing have presented us with personalized intelligent agents [42, 11] while studying our facial gestures provides a richer selection of tools for improving human computer interaction [9]."}, {"heading": "1.1. Contributions", "text": "1. To the best of our knowledge and belief, we have built up the largest ever facial video dataset, comprising 162 million facial images with facial markings (7.8 billion annotations) contained in 6.5 million video clips, and 2777 videos labeled for seven emotions. The dataset will be published for research purposes. 2. We are developing a multi-speed autoencoder architecture that uses new multi-speed layers to generate velocity-free depth movement.3. We report on the state of the art in video gesture recognition using spatially-temporally foldable neural networks. 4. We are introducing a new topology and protocol for semi-supervised learning, where the number of labeled data 1ar Xiv: 160 3.06 829v 1 [cs.C V] 22 March 201 6 points represent only a fraction of the entire dataset."}, {"heading": "2. Related Work", "text": "Machine learning techniques such as Support Vector Machines have been used to detect facial expression in the face of movement of facial fiducial points [24, 33, 41, 10] to achieve real-time performance [38]. Many of these techniques involve a multi-phase pipeline - facial recognition and alignment, feature extraction / landmark localization and classification as the final step. Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal characteristics [28, 51] and multiple cores [29], action units [54, 40] and emotion recognition from language [34, 39]. We will compare our method with some of these approaches in Section 5.Recently, deep neural networks have shown that they work well in classification tasks based on images and videos, outperforming most traditional learning systems. One of the most interesting results was presented three years ago on a large-scale data collection (LSVRC 2011), where a deep revolutionary network far outperforms all other methods [25]."}, {"heading": "3. Method", "text": "We propose a semi-supervised approach that uses a deep neural network by combining an autoencoder with a classification loss function and training both at the same time.The input for the first layer consists of a short sequence of facial gestures consisting of 9 frames tailored to a window with 145 x 145 pixels. The loss function is evaluated by combining a predictive loss of 7 different prescribed gestures (for the labeled part of the data set) and an autoencoder euclidean loss for the entire (labeled and unlabeled) collection. The weights of each layer are dynamically altered so that the significance of the autoencoder loss decreases in relation to the predicted loss over the course of the training. As we generate the data, we use Viola and Jones face recognition [49] for face cropping. We use slow fusion-based neural network with windings both in space and in overview (see figure 4 for detailed time)."}, {"heading": "3.1. Action autoencoder", "text": "Our Action Autoencoder consists of Convolutionary Autoencoders to learn deep features and to reduce the dimensionality of the data. We use Convolutionary Filters with weight distribution in the first 6 layers followed by 2 fully connected layers. This network is similar to Imagenet [25], but accepts input of size 145 x 145 x 9. Using shorthand, the complete architecture can be written as C (96, 11, 3) \u2212 N \u2212 C (256, 5, 2) \u2212 N \u2212 C (384, 3, 2) \u2212 N \u2212 FC (4096) \u2212 DC (96, 11, 3) \u2212 N \u2212 DC (256, 5, 2) \u2212 N \u2212 DC (384, 3, 2), where C (n, f, s) stands for conversion layers with n-filters of size f and step s. DC (n, f, s) stands for deconvolution layers with n-time layers and deconfusion layers stand for slow layers with n-2nd (and long) layers"}, {"heading": "3.2. Multi-Velocity Encoders", "text": "In fact, most people are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. (...) Most of us don't know what they're doing. \""}, {"heading": "3.3. Semi-Supervised Learner", "text": "One of the biggest challenges we face today in forming deep neural networks is the need for large, labeled datasets. The abundance of data is probably one of the main reasons why neural networks provide such impressive predictive results in almost every area, but it is also extremely difficult to collect and label such datasets. In the semi-supervised paradigm, we assume that only part of the data is labeled, but we would like to use the knowledge hidden within the entire set. Here, we combine the action autoencode folding layers with a softmax loss function for the labeled set. The classification neural network is inspired by Imagenet [25], with additionally connected layers shared with the autoencoder or to generate deeper classification features from the latter. The complete architecture of the predictor is C (96, 11, 3) (FC \u2212 C, 96, 256, 96, 252, 402 \u2212 C (404) \u2212 404 \u2212 C (402 \u2212 404)."}, {"heading": "3.3.1 Multi-Velocity Semi-Supervised Learner", "text": "Finally, we add the new proposed multi-velocity layers as the first structure of the semi-monitored neural network. Each sub-structure (see Figure 4) has its own autoencoder, which are all concatenated after the innermost folding layer to form a feature vector (size 12288), which is later used by the designated loss function. The learner-loss function can be expressed as a weighted sum of auto-encoder inputs and outputs specified in Equation 3 below. L = \u03b1-v-x-x-x-j-log (eoj-k e-ok) (3) Here x-v are autoencoder inputs and outputs, yj are the input marks and oj are the outputs from the predictor layer."}, {"heading": "4. Datasets", "text": "To evaluate the proposed architecture, we will use two known datasets from the literature and introduce two additional datasets that we have collected: The first dataset contains more than 160 million images, which have been combined into 6.5 million short (25 frames) clips with which we train our autoencoders; the second dataset consists of 2777 short clips labeled for seven emotions. In the following section, we will discuss the four datasets."}, {"heading": "4.1. Autoencoder dataset", "text": "To train very deep neural networks, we need a huge collection of data. Here, we collected 6.5 million video clips, each with 25 frames, for a total of more than 162 million facial images. We used Viola-jones face detectors to locate and segment the faces. Next, we located landmarks using a deformable model of the face [3] and determined the facial posture by fitting a 3D model to the landmarks. This process allowed us to limit the data set to videos containing faces that are tilted less than 30 degrees and faces looking sideways. To extract only meaningful video clips, we moved clips with static gestures or those where the faces changed quickly, either due to high-speed motion or simply due to the appearance of another face. We did this by blurring the clips and creating the difference between consecutive frames, or those where the faces changed quickly, either due to the appearance of a face tilted less than 30 degrees, or simply due to the appearance of another face."}, {"heading": "4.2. Asevo dataset", "text": "To collect and tag our own gestures, we developed a video recording and annotation tool. We developed the application using the python-based OpenCV and recorded the clips using the Logitech C920 HD camera. The database contains facial clips from 160 subjects, both male and female, where gestures were artificially created according to a specific requirement or were actually given based on a stimulus shown. We collected a total of 2777 clips, of which 1745 were recorded after the stimulus was provided, while 1032 were artificially generated. To generate natural facial expressions, we selected a database of YouTube videos for each facial expression and showed them to the subjects and recorded their response to the visual stimulus. We summarize this data set quantitatively in Table 2, where created clips relate to the artificially generated facial expressions and are not related to the stimulus activation process."}, {"heading": "4.3. Cohn Kanade Dataset", "text": "The Cohn-Kanade dataset [31] is one of the most popular datasets for facial expression recognition. The dataset contains 593 sequences, of which 327 are marked for 7 emotions. In addition to posed facial expressions, the dataset also contains unposed smile expressions. However, the dataset lacks depth in capturing other unexpressed expressions, and is not as extensive as the Asevo dataset in capturing naturally expressed emotions. Each video clip contains facial expressions that range from the baseline neutral to the peak of expressed emotions."}, {"heading": "4.4. MMI Dataset", "text": "The MMI Facial Expression Data Set [35] is an ongoing effort to visualize both posed and non-posed facial expressions. The data set comprises a total of 2894 video clips, 197 of which were labeled for six basic emotions. MMI originally contained only posed facial expressions and was recently expanded to include induced joy, disgust, and surprise [47]. Each video clip in the MMI contains people who move from a neutral point to the highest point and then back to neutral facial expression."}, {"heading": "5. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Video autoencoder", "text": "Our first experiment qualitatively shows the results of a single video autoencoder. We use 145 x 145 x 9 clips as input, reducing the spatial resolution obtained by scanning all clips using bspline interpolation to this single size, and 9 frames are extracted from the clip using every third frame. We use caffe [17] to train the system. In practice, we convert each video clip into a strip of image containing consecutive frames placed horizontally and use caffe \"Imagedata,\" \"split\" and \"concat\" layers for video data input.We minimize contrasting divergences [5] to successively train autoencoder layers. We train the first 4 initial and final layers by creating an intermediate neural network (C (96, 11, 3) \u2212 C (C \u2212 C, 256, 256, DC, 256 \u2212 N \u2212 254)."}, {"heading": "5.2. Multi-velocity video autoencoder", "text": "The semi-monitored multi-velocity learner consists of an array of three independent autoencoders and a predictor network. We initialize the autoencoders with the weights of the video autoencoder and add a folding layer as described in Section 3.2. We coordinate the multi-velocity layers by creating 3 sets of video clips at different speeds. We achieve this by selecting every third frame to create set 1 (speed = 3x), selecting every second frame to create set 2 (speed = 2x) and the first 9 frames for set 3 (speed = 1x). The weights from this step are used to initialize our multivelocity predictor, which is described next."}, {"heading": "5.3. Multi-velocity predictor", "text": "For training, testing and validation, we randomly divide each data set into 3 parts. We select 50% inputs for training, 30% of the data set for testing, and use 20% of the data set for validation. After the data set has been split, we further enlarge the size of the training data set by moving each video along both axes, rotating images and taking their mirror. We train our proposed partially monitored learners and partially monitored learners at multiple speeds on the three data sets (MMI, CK and Asevo) and compare our results with several core methods [29] and basic approaches for expressions [28]. We used sources from Visual Information Processing and Learning Resources [43] as a reference to compare our methods. Note that we have done the same data partitioning scheme (turn, validation, test) for all methods to show a fair comparison."}, {"heading": "6. Discussion and Future Work", "text": "This work represents a learning strategy for large datasets with dramatically fewer marked points, in addition to new layers that have been carefully designed to improve detection in the multivelocity setup. We are currently editing the videos onto the face window using Viola and Jones face detection, and are focusing exclusively on frontal views. Wildlife detection remains a challenge with a notoriously low success rate. We believe that this problem could be solved with a large and abundant dataset in our system, and we plan to explore this in the future. We have introduced a new layer that reproduces the videos adaptively, achieving multi-speed invariant learning, and inserting inventors into a learning process is a research direction that we need to advance. Today, training deep neural networks is still time consuming, where huge clusters are heavily deployed on relatively large datasets. We are already reaching the time-space limit of this process, and better approaches need to be considered for further development."}, {"heading": "7. Conclusions", "text": "In this thesis, we introduced a new topology and learning protocol for semi-monitored video-based Convolutionary Neural Networks. Furthermore, we developed a multi-speed layer based on time resampling and tuned to an enormous facial data set collected during the learning process. We report on current results based on our own data and publicly available data sets."}, {"heading": "Appendix A: Equations for B spline Interpolation", "text": "Let {xk, f (xk)} Nk = 0 is N + 1 Observation of a function f. (cubic spline is defined as a set of polynomials Sn (x) N \u2212 1 n = 0 with coefficients pn, i, which f turns out to be the following S (x) = Sn (x) = pn, 0 + pn, 1 (xn \u2212 xk) + pn, 2 (xn \u2212 xk) 2 + pn, 3 (xn \u2212 xk) 3, (4) where xk < xn < xk + 1. We need at least 4N constraints to uniquely restore pn, i. We can create 4N \u2212 2 constraints by specifying the values of the polynomials at the boundaries and assuming that the first and second derivatives of the adjacent polynomials < xk < xk < xk + 1 x x x x x x x x x x x x x x x x x x."}], "references": [{"title": "A neural network based facial expression recognition using fisherface", "author": ["Z. Abidin", "A. Harjoko"], "venue": "International Journal of Computer Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Exploiting models of personality and emotions to control the behavior of animated interactive agents", "author": ["E. Andr\u00e9", "M. Klesen", "P. Gebhard", "S. Allen", "T. Rist"], "venue": "In Workshop on Achieving Human-Like Behavior in Interactive Animated Agents,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Incremental face alignment in the wild", "author": ["A. Asthana", "S. Zafeiriou", "S. Cheng", "M. Pantic"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Convex neural networks", "author": ["Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "In Proceedings of International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Interactive agents for multimodal emotional user interaction", "author": ["E. Cerezo", "S. Baldassarri", "F. Seron"], "venue": "Multi Conferences on Computer Science and Information Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "3d modelbased continuous emotion recognition", "author": ["H. Chen", "J. Li", "F. Zhang", "Y. Li", "H. Wang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "On spline finite element method", "author": ["S. Chung-Tze"], "venue": "Mathematica Numerica Sinica, (MNS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1979}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz", "J.G. Taylor"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Emotion recognition using PHOG and LPQ features", "author": ["A. Dhall", "A. Asthana", "R. Goecke", "T. Gedeon"], "venue": "In International Conference on Automatic Face & Gesture Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Radar: A personal assistant that learns to reduce email overload", "author": ["M. Freed", "J.G. Carbonell", "G.J. Gordon", "J. Hayes", "B.A. Myers", "D.P. Siewiorek", "S.F. Smith", "A. Steinfeld", "A. Tomasic"], "venue": "In AAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Role of facial expressions in social interactions", "author": ["C. Frith"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Facial expression recognition using artificial neural networks", "author": ["M. Gargesha", "P. Kuchi"], "venue": "Artificial Neural Computer Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. He", "D. Jiang", "L. Yang", "E. Pei", "P. Wu", "H. Sahli"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Cubic splines for image interpolation and digital filtering", "author": ["H.S. Hou", "H. Andrews"], "venue": "Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1978}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Joint fine-tuning in deep neural networks for facial expression recognition", "author": ["H. Jung", "S. Lee", "J. Yim", "S. Park", "J. Kim"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "Y. Dauphin", "N. Boulanger-Lewandowski"], "venue": "Journal on Multimodal User Interfaces,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "\u00c7. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio", "R.C. Ferrari"], "venue": "In Proceedings of the 15th ACM on International conference on multimodal interaction,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Computer response to user frustration", "author": ["T. Klein", "W. Picard"], "venue": "MIT Media Laboratory Vision and Modelling Group Technical Reports, TR 480,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "author": ["I. Kotsia", "I. Pitas"], "venue": "Transactions on Image Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks", "author": ["D.-H. Lee"], "venue": "In Workshop on Challenges in Representation Learning, ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Deeply learning deformable facial action parts model for dynamic expression analysis", "author": ["M. Liu", "S. Li", "S. Shan", "R. Wang", "X. Chen"], "venue": "In Computer Vision\u2013ACCV", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["M. Liu", "S. Shan", "R. Wang", "X. Chen"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild", "author": ["M. Liu", "R. Wang", "S. Li", "S. Shan", "Z. Huang", "X. Chen"], "venue": "In Proceedings of the 16th International Conference on Multimodal Interaction,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["P. Liu", "S. Han", "Z. Meng", "Y. Tong"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "An approach to environmental psychology", "author": ["A. Mehrabian", "J.A. Russell"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1974}, {"title": "Real time facial expression recognition in video using support vector machines", "author": ["P. Michel", "R. El Kaliouby"], "venue": "In Proceedings of the 5th international conference on Multimodal interfaces,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Speech emotion recognition using Hidden Markov Models", "author": ["T.L. Nwe", "S.W. Foo", "L.C. De Silva"], "venue": "Speech communication,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Webbased database for facial expression analysis", "author": ["M. Pantic", "M. Valstar", "R. Rademaker", "L. Maat"], "venue": "In International Conference on Multimedia and Expo, pages 5\u2013pp. IEEE,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Using hankel matrices for dynamics-based facial emotion recognition and pain detection", "author": ["L. Presti", "M. Cascia"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "On deep generative models with applications to recognition", "author": ["M.A. Ranzato", "J. Susskind", "V. Mnih", "G. Hinton"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Face alignment at 3000 fps via regressing local binary features", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Speech emotion recognition combining acoustic features and linguistic information  in a hybrid support vector machine-belief network architecture", "author": ["B. Schuller", "G. Rigoll", "M. Lang"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Facial action unit detection using active learning and an efficient non-linear kernel approximation", "author": ["T. Senechal", "D. McDuff", "R. Kaliouby"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Robust facial expression recognition using local binary patterns", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "In International Conference on Image Processing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Application design for wearable computing", "author": ["D. Siewiorek", "A. Smailagic", "T. Starner"], "venue": "Synthesis Lectures on Mobile and Pervasive Computing,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "arXiv preprint arXiv:1412.0767,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database", "author": ["M. Valstar", "M. Pantic"], "venue": "In Workshop on EMOTION: Corpora for Research on Emotion and Affect,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Facial expression recognition under a wide range of head poses", "author": ["R.-L. Vieriu", "S. Tulyakov", "S. Semeniuta", "E. Sangineto", "N. Sebe"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}, {"title": "Variable-state latent conditional random fields for facial expression recognition and action unit detection", "author": ["R. Walecki", "O. Rudovic", "V. Pavlovic", "M. Pantic"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Capturing complex spatiotemporal relations among facial muscles for facial expression recognition", "author": ["Z. Wang", "S. Wang", "Q. Ji"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "A survey on media interaction in social robotics", "author": ["L. Yang", "H. Cheng", "J. Hao", "Y. Ji", "Y. Kuang"], "venue": "In Advances in  Multimedia Information Processing (PCM),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Joint patch and multi-label learning for facial action unit detection", "author": ["K. Zhao", "W.-S. Chu", "F. De la Torre", "J.F. Cohn", "H. Zhang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "Our body language, voice pitch, intonation and volume, movement of our pupils or our chronemics choices are just a few examples emphasizing the richness of human communication skills [32].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "Their perception initiates rapid cognitive processes in the brain and have both communicative and reflexive components [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 41, "context": "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].", "startOffset": 100, "endOffset": 108}, {"referenceID": 10, "context": "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].", "startOffset": 222, "endOffset": 225}, {"referenceID": 22, "context": "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].", "startOffset": 115, "endOffset": 125}, {"referenceID": 5, "context": "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].", "startOffset": 115, "endOffset": 125}, {"referenceID": 1, "context": "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].", "startOffset": 115, "endOffset": 125}, {"referenceID": 51, "context": "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].", "startOffset": 215, "endOffset": 219}, {"referenceID": 14, "context": "Recent advances in machine learning have shown that if we provide a neural network with enough samples, it can learn very complex structures [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 36, "context": "Today hard tasks in computer vision, such as labeling images, recognizing objects and faces or classifying videos have become a feasible task for computers which can now provide competitive results to humans and sometimes even outperform them [37, 45].", "startOffset": 243, "endOffset": 251}, {"referenceID": 43, "context": "Today hard tasks in computer vision, such as labeling images, recognizing objects and faces or classifying videos have become a feasible task for computers which can now provide competitive results to humans and sometimes even outperform them [37, 45].", "startOffset": 243, "endOffset": 251}, {"referenceID": 23, "context": "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].", "startOffset": 154, "endOffset": 170}, {"referenceID": 32, "context": "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].", "startOffset": 154, "endOffset": 170}, {"referenceID": 40, "context": "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].", "startOffset": 154, "endOffset": 170}, {"referenceID": 9, "context": "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].", "startOffset": 154, "endOffset": 170}, {"referenceID": 37, "context": "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].", "startOffset": 203, "endOffset": 207}, {"referenceID": 6, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 29, "endOffset": 44}, {"referenceID": 48, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 29, "endOffset": 44}, {"referenceID": 35, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 29, "endOffset": 44}, {"referenceID": 46, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 29, "endOffset": 44}, {"referenceID": 27, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 94, "endOffset": 102}, {"referenceID": 49, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 94, "endOffset": 102}, {"referenceID": 28, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 125, "endOffset": 129}, {"referenceID": 52, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 144, "endOffset": 152}, {"referenceID": 39, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 144, "endOffset": 152}, {"referenceID": 33, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 197, "endOffset": 205}, {"referenceID": 38, "context": "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].", "startOffset": 197, "endOffset": 205}, {"referenceID": 24, "context": "One of the most interesting results was presented three years back on a large scale dataset (LSVRC 2011), where a deep convolutional net outperformed all other methods by far [25].", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 101, "endOffset": 109}, {"referenceID": 44, "context": "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 101, "endOffset": 109}, {"referenceID": 0, "context": "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 149, "endOffset": 156}, {"referenceID": 12, "context": "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 149, "endOffset": 156}, {"referenceID": 25, "context": "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 50, "context": "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.", "startOffset": 73, "endOffset": 81}, {"referenceID": 21, "context": "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.", "startOffset": 73, "endOffset": 81}, {"referenceID": 29, "context": "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.", "startOffset": 47, "endOffset": 67}, {"referenceID": 19, "context": "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.", "startOffset": 47, "endOffset": 67}, {"referenceID": 17, "context": "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.", "startOffset": 47, "endOffset": 67}, {"referenceID": 13, "context": "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.", "startOffset": 47, "endOffset": 67}, {"referenceID": 18, "context": "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.", "startOffset": 47, "endOffset": 67}, {"referenceID": 47, "context": "While generating the data, we use Viola and Jones face detection [49] for cropping the faces.", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "This network is similar to Imagenet [25] but accepts inputs of size 145 \u00d7 145 \u00d7 9 as an input.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "lution layers in time and use slow fusion model [21] which slowly combines temporal information in successive layers.", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Piece-wise cubic b-spline interpolation is preferred over polynomial techniques as it can minimize interpolation error for fewer points and lower degree polynomials [16].", "startOffset": 165, "endOffset": 169}, {"referenceID": 24, "context": "The classifier neural net is inspired by Imagenet [25], with additional Figure 5.", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "This is one of the traditional approaches used to train autoencoders [15, 5].", "startOffset": 69, "endOffset": 76}, {"referenceID": 4, "context": "This is one of the traditional approaches used to train autoencoders [15, 5].", "startOffset": 69, "endOffset": 76}, {"referenceID": 27, "context": "Confusion matrices over test results for Cohn Kanade dataset using our methods and best performing external method which uses Expressionlets [28].", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "max loss [4].", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "Next, we localized landmarks for each frame using a deformable model for the face [3] and detected the facial pose by fitting a 3D model to the landmarks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 30, "context": "The Cohn Kanade Dataset [31] is one of the most popular datasets used for facial expression recognition.", "startOffset": 24, "endOffset": 28}, {"referenceID": 28, "context": "Confusion matrix over test results for Asevo dataset using the proposed multi-velocity semi-supervised learner (left) and external competing method using Covariance Riemann kernel [29].", "startOffset": 180, "endOffset": 184}, {"referenceID": 34, "context": "MMI facial expression dataset [35] is an ongoing effort for representing both posed and non posed facial expressions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 45, "context": "MMI originally contained only posed facial expressions and recently was extended to contain induced happiness, disgust and surprise [47].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "We use caffe [17] to train the system.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "We minimize contrastive divergence [5] to train autoencoder layers successively.", "startOffset": 35, "endOffset": 38}, {"referenceID": 28, "context": "(MMI, CK and Asevo), and compare our results against multiple kernel methods [29] and expression-lets base approaches [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "(MMI, CK and Asevo), and compare our results against multiple kernel methods [29] and expression-lets base approaches [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "For baseline comparison against other deep neural architectures, we compare our methods against [25] and GoogleNet [44].", "startOffset": 96, "endOffset": 100}, {"referenceID": 42, "context": "For baseline comparison against other deep neural architectures, we compare our methods against [25] and GoogleNet [44].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "We further verified our results against prior state of the art methods discussed in [27] by performing 10 fold cross validation.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "We add additional constraints assuming that the curve is natural [8] and has zero derivative at boundaries.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "We present a new action recognition deep neural network which adaptively learns the best action velocities in addition to the classification. While deep neural networks have reached maturity for image understanding tasks, we are still exploring network topologies and features to handle the richer environment of video clips. Here, we tackle the problem of multiple velocities in action recognition, and provide state-of-the-art results for gesture recognition, on known and new collected datasets. We further provide the training steps for our semi-supervised network, suited to learn from huge unlabeled datasets with only a fraction of labeled examples.", "creator": "LaTeX with hyperref package"}}}