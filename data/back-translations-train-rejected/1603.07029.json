{"id": "1603.07029", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Comparing Human and Automated Evaluation of Open-Ended Student Responses to Questions of Evolution", "abstract": "Written responses can provide a wealth of data in understanding student reasoning on a topic. Yet they are time- and labor-intensive to score, requiring many instructors to forego them except as limited parts of summative assessments at the end of a unit or course. Recent developments in Machine Learning (ML) have produced computational methods of scoring written responses for the presence or absence of specific concepts. Here, we compare the scores from one particular ML program -- EvoGrader -- to human scoring of responses to structurally- and content-similar questions that are distinct from the ones the program was trained on. We find that there is substantial inter-rater reliability between the human and ML scoring. However, sufficient systematic differences remain between the human and ML scoring that we advise only using the ML scoring for formative, rather than summative, assessment of student reasoning.", "histories": [["v1", "Tue, 22 Mar 2016 23:36:02 GMT  (477kb,D)", "http://arxiv.org/abs/1603.07029v1", "Submitted to ALife 2016"]], "COMMENTS": "Submitted to ALife 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael j wiser", "louise s mead", "james j smith", "robert t pennock"], "accepted": false, "id": "1603.07029"}, "pdf": {"name": "1603.07029.pdf", "metadata": {"source": "CRF", "title": "Comparing Human and Automated Evaluation of Open-Ended Student Responses to Questions of Evolution", "authors": ["Michael J. Wiser", "Louise S. Mead", "James J. Smith", "Robert T. Pennock"], "emails": ["mwiser@msu.edu"], "sections": [{"heading": "Background", "text": "The central importance of evolution for teaching and learning in the biological sciences has been discussed in all scientific disciplines (States, 1900; Brewer and Smith, 2011)."}, {"heading": "Use of Machine Learning in Education", "text": "In fact, a whole book has been written on the use of machine learning tools and techniques in educational science (Kidzin \u0301 ski et al., 2016). One area that is of particular interest is language processing. Machine learning techniques have been used to classify teacher questions according to Bloom's taxonomy (Yahya et al., 2013). Perhaps the greatest benefit of machine learning in an educational environment is the automated evaluation of student writing (reviewed in (Nehm et al., 2012b)). A domain-specific example of ML techniques in language processing is the web portal EvoGrader, which is discussed below. EvoGrader was developed to evaluate students \"understanding of natural selection, using a specific set of questions consisting of a short scenario and asking students how a biologist would explain this scenario of evolutionary changes or patterns. Our study attempts to measure how this similar mL-like process was written for human questions that are not in the scanning style."}, {"heading": "EvoGrader", "text": "EvoGrader (http: / / www.evograder.com) is a free, online service that analyzes open answers to questions about evolution and natural selection and provides users with formal ratings. It is described in detail in (Moharreri et al., 2014), but a brief description follows. EvoGrader works through supervised machine learning. Participants (n = 2,978) wrote responses to ACORNS assessment elements (Take et al., 2012a) and ACORNS-like elements (Bishop and Anderson, 1990), which consist of a request to describe a short scenario relevant to natural selection, and asked students how a biologist would explain this situation. Participants spanned many different levels of expertise, including non-majors, student biology or anthropology majors, student-student responses."}, {"heading": "Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Student data", "text": "We administered pre-instruction and post-instruction tests consisting of two questions (see box 1) about evolution in an introductory cell and molecular biology course in fall 2014. Both questions asked students about the evolutionary processes. Question 1 asks about an evolutionary gain in antibiotic resistance in a population, while question 2 asks about the evolutionary loss of toxicity in a fungal population. We evaluated student responses to two subjects: Question 1: Explain how a microbial population develops against the effects of an antibiotic population. Question 2: A type of fungal response contains a chemical that is toxic."}, {"heading": "Data", "text": "Data files with all student responses, scores and data analysis can be found at: https: / / github.com / mjwiser / ALife2016"}, {"heading": "Scoring responses", "text": "We used EvoGrader to evaluate the students \"answers to two open questions about natural selection for six key concepts and three naive ideas (see box 1); two human graders (MJW and LSM) evaluated the students\" answers for the same criteria; and disagreements between people were resolved by discussion, leading to a consensus human score."}, {"heading": "Statistical analysis", "text": "Because we were interested in the IRR of specific questions, we combined the answers before and after class into a combined data set. We calculated the IRR for each question as a whole as well as separately for the key concepts and naive ideas within each question. We chose not to calculate the IRR for each concept separately or for pre- and post-instruction questions, as the statistical performance in the separate examination of each set was lower and this would require an increase in multiple comparisons. In addition, we compared the results of the EvoGrader and human consensus in the form of paired T-tests for differences in the number of key concepts or naive ideas. We performed all statistical tests in R version 3.2.3 (R Core Team, 2013)."}, {"heading": "Results and Discussion", "text": "This year, it has come to be a reactionary, reactionary and reactionary party."}, {"heading": "Conclusions", "text": "EvoGrader is a useful tool for evaluating student arguments about natural selection. Even on questions that are not included in the course, it provides an appropriate level of reliability in evaluating student responses to open questions similar to the ACORNS assessment. However, it is not foolproof. In our study, EvoGrader certifies students as having more key concepts and less na\u00efve ideas than our human counselors. In particular, EvoGrader may inaccurately count student responses that do not address the specific question of evolutionary arguments. For formative assessments, it can be a valuable tool to get a feel for student responses in a short time, but we caution against using EvoGrader to assign students points, given its current limitations."}], "references": [{"title": "Development and evaluation of the conceptual inventory of natural selection", "author": ["D.L. Anderson", "K.M. Fisher", "G.J. Norman"], "venue": "Journal of Research in Science Teaching, 39(10):952\u2013978.", "citeRegEx": "Anderson et al\\.,? 2002", "shortCiteRegEx": "Anderson et al\\.", "year": 2002}, {"title": "Biology Undergraduates\u2019 Misconceptions about Genetic Drift", "author": ["T.M. Andrews", "R.M. Price", "L.S. Mead", "T.L. McElhinny", "A. Thanukos", "K.E. Perez", "C.F. Herreid", "D.R. Terry", "P.P. Lemons"], "venue": "CBE-Life Sciences Education, 11(3):248\u2013259.", "citeRegEx": "Andrews et al\\.,? 2012", "shortCiteRegEx": "Andrews et al\\.", "year": 2012}, {"title": "Student conceptions of natural selection and its role in evolution", "author": ["B.A. Bishop", "C.W. Anderson"], "venue": "Journal of Research in Science Teaching, 27.", "citeRegEx": "Bishop and Anderson,? 1990", "shortCiteRegEx": "Bishop and Anderson", "year": 1990}, {"title": "Vision and change in undergraduate biology education: a call to action", "author": ["C.A. Brewer", "D. Smith"], "venue": "American Association for the Advancement of Science, Washington, DC.", "citeRegEx": "Brewer and Smith,? 2011", "shortCiteRegEx": "Brewer and Smith", "year": 2011}, {"title": "Integrating Cognitive Science and Technology Improves Learning in a STEM Classroom", "author": ["A.C. Butler", "E.J. Marsh", "J.P. Slavinsky", "R.G. Baraniuk"], "venue": "Educational Psychology Review, 26(2):331\u2013340.", "citeRegEx": "Butler et al\\.,? 2014", "shortCiteRegEx": "Butler et al\\.", "year": 2014}, {"title": "Computing Inter-Rater Reliability for Observational Data: An Overview and Tutorial", "author": ["K.A. Hallgren"], "venue": "Tutorials in quantitative methods for psychology, 8(1):23\u201334.", "citeRegEx": "Hallgren,? 2012", "shortCiteRegEx": "Hallgren", "year": 2012}, {"title": "Getting to EvoDevo: Concepts and Challenges for Students Learning Evolutionary Developmental Biology", "author": ["A. Hiatt", "G.K. Davis", "C. Trujillo", "M. Terry", "D.P. French", "R.M. Price", "K.E. Perez"], "venue": "CBE-Life Sciences Education, 12(3):494\u2013508.", "citeRegEx": "Hiatt et al\\.,? 2013", "shortCiteRegEx": "Hiatt et al\\.", "year": 2013}, {"title": "A Tutorial on Machine Learning in Educational Science", "author": ["\u0141. Kidzi\u0144ski", "M. Giannakos", "D.G. Sampson", "P. Dillenbourg"], "venue": "Li, Y., Chang, M., Kravcik, M., Popescu, E., Huang, R., Kinshuk, and Chen, N.-S., editors, State-of-the-Art and Future Directions of Smart", "citeRegEx": "Kidzi\u0144ski et al\\.,? 2016", "shortCiteRegEx": "Kidzi\u0144ski et al\\.", "year": 2016}, {"title": "The measurement of observer agreement for categorical data", "author": ["J.R. Landis", "G.G. Koch"], "venue": "Biometrics,", "citeRegEx": "Landis and Koch,? 1977", "shortCiteRegEx": "Landis and Koch", "year": 1977}, {"title": "Open Source Machine Learning for Text", "author": ["E. Mayfield", "C.P. Ros\u00e9"], "venue": "Handbook of automated essay evaluation: Current applications and new directions.", "citeRegEx": "Mayfield and Ros\u00e9,? 2013", "shortCiteRegEx": "Mayfield and Ros\u00e9", "year": 2013}, {"title": "EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations", "author": ["K. Moharreri", "M. Ha", "R.H. Nehm"], "venue": "Evolution: Education and Outreach, 7(1):1\u201314.", "citeRegEx": "Moharreri et al\\.,? 2014", "shortCiteRegEx": "Moharreri et al\\.", "year": 2014}, {"title": "Reasoning about natural selection: diagnosing contextual competency using the ACORNS instrument", "author": ["R.H. Nehm", "E.P. Beggrow", "J.E. Opfer", "M. Ha"], "venue": "The American Biology Teacher, 74.", "citeRegEx": "Nehm et al\\.,? 2012a", "shortCiteRegEx": "Nehm et al\\.", "year": 2012}, {"title": "Item feature effects in evolution assessment", "author": ["R.H. Nehm", "M. Ha"], "venue": "Journal of Research in Science Teaching, 48.", "citeRegEx": "Nehm and Ha,? 2011", "shortCiteRegEx": "Nehm and Ha", "year": 2011}, {"title": "Transforming biology assessment with machine learning: automated scoring of written evolutionary explanations", "author": ["R.H. Nehm", "M. Ha", "E. Mayfield"], "venue": "Journal of Science Education and Technology, 21.", "citeRegEx": "Nehm et al\\.,? 2012b", "shortCiteRegEx": "Nehm et al\\.", "year": 2012}, {"title": "Measuring knowledge of natural selection: a comparison of the CINS, an open-response instrument, and an oral interview", "author": ["R.H. Nehm", "I.S. Schonfeld"], "venue": "Journal of Research in Science Teaching, 45.", "citeRegEx": "Nehm and Schonfeld,? 2008", "shortCiteRegEx": "Nehm and Schonfeld", "year": 2008}, {"title": "Learning the Language of Evolution: Lexical Ambiguity and Word Meaning in Student Explanations", "author": ["M.A. Rector", "R.H. Nehm", "D. Pearl"], "venue": "Research in Science Education, 43(3):1107\u20131133.", "citeRegEx": "Rector et al\\.,? 2012", "shortCiteRegEx": "Rector et al\\.", "year": 2012}, {"title": "Next generation science standards: For states, by states", "author": ["N.L. States"], "venue": "National Academies Press.", "citeRegEx": "States,? 1900", "shortCiteRegEx": "States", "year": 1900}, {"title": "Analyzing the Cognitive Level of Classroom Questions Using Machine Learning Techniques", "author": ["A.A. Yahya", "A. Osman", "A. Taleb", "A.A. Alattab"], "venue": "The 9th International Conference on Cognitive Science, 97:587\u2013595.", "citeRegEx": "Yahya et al\\.,? 2013", "shortCiteRegEx": "Yahya et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "The central importance of evolution to teaching and learning in the biological sciences has been clearly established in all science education reform (States, 1900; Brewer and Smith, 2011).", "startOffset": 149, "endOffset": 187}, {"referenceID": 3, "context": "The central importance of evolution to teaching and learning in the biological sciences has been clearly established in all science education reform (States, 1900; Brewer and Smith, 2011).", "startOffset": 149, "endOffset": 187}, {"referenceID": 2, "context": "Adequate formative assessment instruments \u2013 administered during the course of instruction to gauge student understanding and reasoning in order to provide feedback for future instruction, instead of to assign a grade at the end of a unit \u2013 that measure student understanding of evolutionary concepts (Bishop and Anderson, 1990; Anderson et al., 2002), however, have until recently been rather limited (Nehm and Schonfeld, 2008).", "startOffset": 300, "endOffset": 350}, {"referenceID": 0, "context": "Adequate formative assessment instruments \u2013 administered during the course of instruction to gauge student understanding and reasoning in order to provide feedback for future instruction, instead of to assign a grade at the end of a unit \u2013 that measure student understanding of evolutionary concepts (Bishop and Anderson, 1990; Anderson et al., 2002), however, have until recently been rather limited (Nehm and Schonfeld, 2008).", "startOffset": 300, "endOffset": 350}, {"referenceID": 14, "context": ", 2002), however, have until recently been rather limited (Nehm and Schonfeld, 2008).", "startOffset": 58, "endOffset": 84}, {"referenceID": 1, "context": "Studies find that students hold both scientifically accurate and naive or non-scientific explanations simultaneously (Andrews et al., 2012; Hiatt et al., 2013) and that accurately identifying alternative conceptions can be difficult (Rector et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 6, "context": "Studies find that students hold both scientifically accurate and naive or non-scientific explanations simultaneously (Andrews et al., 2012; Hiatt et al., 2013) and that accurately identifying alternative conceptions can be difficult (Rector et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 15, "context": ", 2013) and that accurately identifying alternative conceptions can be difficult (Rector et al., 2012).", "startOffset": 81, "endOffset": 102}, {"referenceID": 12, "context": "Undergraduates employ more naive concepts when applying explanations of natural selection to plants as compared to animals; trait loss as compared to trait gain; and unfamiliar taxa as compared to familiar taxa (Nehm and Ha, 2011).", "startOffset": 211, "endOffset": 230}, {"referenceID": 15, "context": "One study found that 81 percent of students incorporated lexically ambiguous language in their responses to open ended questions about evolutionary mechanisms (Rector et al., 2012).", "startOffset": 159, "endOffset": 180}, {"referenceID": 10, "context": "In an effort to identify effective assessment strategies, we have been investigating the applicability of a new tool, EvoGrader (Moharreri et al., 2014).", "startOffset": 128, "endOffset": 152}, {"referenceID": 10, "context": "One study found that it took an average of four minutes for a human grader to score a single response for the nine ideas we analyze in this study (Moharreri et al., 2014).", "startOffset": 146, "endOffset": 170}, {"referenceID": 4, "context": "There is growing interest in using tools and techniques from Machine Learning in the classroom environment (Butler et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": "In fact, an entire book has been written about using Machine Learning in educational science (Kidzi\u0144ski et al., 2016).", "startOffset": 93, "endOffset": 117}, {"referenceID": 17, "context": "Machine learning techniques have been used to classify instructor questions according to Bloom\u2019s taxonomy (Yahya et al., 2013).", "startOffset": 106, "endOffset": 126}, {"referenceID": 13, "context": "Perhaps the biggest use of Machine Learning in an educational environment is in the automated scoring of student writing (reviewed in (Nehm et al., 2012b)).", "startOffset": 134, "endOffset": 154}, {"referenceID": 10, "context": "It is described in detail in (Moharreri et al., 2014), but a brief description follows.", "startOffset": 29, "endOffset": 53}, {"referenceID": 11, "context": "Participants (n=2,978) wrote responses to ACORNS assessment items (Nehm et al., 2012a) and ACORNS-like items (Bishop and Anderson, 1990), generating 10,270 student responses.", "startOffset": 66, "endOffset": 86}, {"referenceID": 2, "context": ", 2012a) and ACORNS-like items (Bishop and Anderson, 1990), generating 10,270 student responses.", "startOffset": 31, "endOffset": 58}, {"referenceID": 9, "context": "These consensus scores were used to train EvoGrader, based on the supervised machine learning tools of LightSIDE (Mayfield and Ros\u00e9, 2013).", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "(Moharreri et al., 2014) Table 2 for details).", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "We measured inter-rater reliability (IRR) between the EvoGrader scores and the consensus human scores for each question, as outlined in (Hallgren, 2012).", "startOffset": 136, "endOffset": 152}, {"referenceID": 8, "context": "6 indicate moderate agreement (Landis and Koch, 1977).", "startOffset": 30, "endOffset": 53}, {"referenceID": 8, "context": "Landis and Koch (1977) suggest that IRR values from Cohens kappa in the range of 0.", "startOffset": 0, "endOffset": 23}], "year": 2016, "abstractText": "Written responses can provide a wealth of data in understanding student reasoning on a topic. Yet they are timeand laborintensive to score, requiring many instructors to forego them except as limited parts of summative assessments at the end of a unit or course. Recent developments in Machine Learning (ML) have produced computational methods of scoring written responses for the presence or absence of specific concepts. Here, we compare the scores from one particular ML program \u2013 EvoGrader \u2013 to human scoring of responses to structurallyand content-similar questions that are distinct from the ones the program was trained on. We find that there is substantial inter-rater reliability between the human and ML scoring. However, sufficient systematic differences remain between the human and ML scoring that we advise only using the ML scoring for formative, rather than summative, assessment of student reasoning.", "creator": "TeX"}}}