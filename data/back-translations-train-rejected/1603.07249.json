{"id": "1603.07249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "abstract": "Developing Intelligent Systems involves artificial intelligence approaches including artificial neural networks. Here, we present a tutorial of Deep Neural Networks (DNNs), and some insights about the origin of the term \"deep\"; references to deep learning are also given. Restricted Boltzmann Machines, which are the core of DNNs, are discussed in detail. An example of a simple two-layer network, performing unsupervised learning for unlabeled data, is shown. Deep Belief Networks (DBNs), which are used to build networks with more than two layers, are also described. Moreover, examples for supervised learning with DNNs performing simple prediction and classification tasks, are presented and explained. This tutorial includes two intelligent pattern recognition applications: hand- written digits (benchmark known as MNIST) and speech recognition.", "histories": [["v1", "Wed, 23 Mar 2016 15:55:20 GMT  (1451kb,D)", "http://arxiv.org/abs/1603.07249v1", "30 pages, 19 figures, unpublished technical report"]], "COMMENTS": "30 pages, 19 figures, unpublished technical report", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["juan c cuevas-tello", "manuel valenzuela-rendon", "juan a nolazco-flores"], "accepted": false, "id": "1603.07249"}, "pdf": {"name": "1603.07249.pdf", "metadata": {"source": "CRF", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "authors": ["Juan C. Cuevas-Tello", "Manuel Valenzuela-Rend\u00f3n", "Juan A. Nolazco-Flores", "Eugenio Garza Sada"], "emails": ["cuevastello@itesm.mx", "valenzuela@itesm.mx", "jnolazco@itesm.mx", "cuevas@uaslp.mx"], "sections": [{"heading": null, "text": "The development of intelligent systems encompasses approaches to artificial intelligence, including artificial neural networks, where a tutorial on Deep Neural Networks (DNNs) is presented and some insights into the origin of the term \"deep\" are given; references to deep learning are also given; restricted Boltzmann machines that form the core of DNNs are discussed in detail; an example of a simple two-tiered network that performs unattended learning for unlabeled data is shown; Deep Belief Networks (DBNs) that build networks with more than two layers are also described; and examples of supervised learning where DNNs perform simple prediction and classification tasks are presented and explained; this tutorial includes two smart pattern recognition applications: handwritten digits (known as MNIST) and speech recognition."}, {"heading": "1 Introduction", "text": "The core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and those proposed by Hinton et al. [13, 12, 11], where the term comes deep from the Deep Beliefs Networks (DBN), the relationship between RBMs, DBN and DNNs.ar Xiv: 160 3.07 249v 1Nowadays, the Term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22]. DL mainly refers to Deep Neural Networks (DNNs) and in particular to DBMs [15]. Some work related to DL focuses on the high-performance computing of DNNs, i.e."}, {"heading": "2 Restricted Boltzmann Machines (RBMs)", "text": "The visible layer is the input, blank data, to the neural network. The hidden layer extracts properties from the input data, and each neuron acquires a different characteristic [12]. By definition, an RBM is a split, undirected graph. An RBM has m visible units ~ V = (V1, V2,..., Vm), the input data and n hidden units ~ H = (H1, H2,..., Hn), the characteristics [6]. A common configuration (~ v, ~ h) of the visible and hidden units has an energy which is expressed by [14] E (~ v, ~ h) = \u2212 m \u00b2 i = 1 aivi \u2212 n \u00b2 j = 1 bjhj \u2212 m \u00b2 i = 1 n \u00b2 j = 1 viltjwij = 1 viltjwij, (1) where vi and hj are the binary states of the visible and hidden units."}, {"heading": "2.1 Contrastive Divergence algorithm", "text": "The first step of the CD algorithm is < vihj > 0, as shown above; the next step is the \"reconstruction\" of the visible layer byp (vi = 1 | ~ h) = sig ai + \u2211 j hjwi, (3) i.e. hidden to visible; this step is called < hjvi > 0. The new state of the hidden layer is obtained by the result of the reconstruction as input data, and this step is called < vihj > 1; at a time t = 1. Finally, the weights and distortions are adjusted as follows [12]: \u0445 wij = \u03b5 (< vihj > 0 \u2212 < vihj > 1); (4) \u0445ai = \u03b5 (v0i \u2212 v1i); (5) \u0445 bj = hj (0j) when the learning steps are performed in the CDM models (1 \u2212 6)."}, {"heading": "2.2 Deep Belief Network", "text": "If we compare Figure 1 with Figure 4, we see that a DBN is built by stacking RBMs. The more layers the DBN has, the deeper the DBN is. The hidden neurons in an RBM1 capture the characteristics of the visible neurons. Then, these characteristics become the input for RBM2 and so on until the RBMr is reached; see also Figure 5. A DBN extracts characteristics from characteristics in an unsupervised manner (deep learning). A hybrid DBN has been proposed for supervised learning [12], and it can be achieved by many different algorithms, including propagation [21], compared with a rate of hybrid neurons, VN-12]."}, {"heading": "3 Toolbox for DNN", "text": "We use a publicly available toolbox for MATLAB R \u00a9 developed by Tanaka and Okutomi and available for download.2 This toolbox is based on [12]. This toolbox includes sparseness [16], dropouts [4] and a novel inference1http: / / yann.lecun.com / exdb / mnist / 2http: / / www.mathworks.com / matlabcentral / fileexchange / 42853-deep-neural-networkfor RBM developed by Tanaka [24]. Once the toolbox has been downloaded and unpacked, it creates the following directories: \u2022 / DeepNeuralNetwork / \u2022 / DeepNeuralNetwork / mnist"}, {"heading": "3.1 MNIST", "text": "The MNIST database 3 with handwritten digits has a training set of 60,000 examples and a test set of 10,000 examples. Once the MNIST has been downloaded and unpacked, we will receive the following files: \u2022 train-images-idx3-ubyte: training set images \u2022 train-labels-idx1-ubyte: training set labels \u2022 t10k-images-idx3-ubyte: test set images \u2022 t10k-labels-idx1-ubyte: test set labelsNote that when unpacking the *.gz files, you must check the filenames and replace them with. \"You must locate the files within the / DeepNeuralNetwork / mnist / directory.3http: / / yann.lecun.com / exdb / mnist /."}, {"heading": "3.2 Running the example: DNN-MNIST", "text": "The file / mnist / testMNIST.m is the main file of the example provided by the toolbox to train a DNN for the MNIST database. The example uses a hybrid network with only two hidden layers of 800 neurons each, see Fig. 7. We tested the toolbox on Octave4 3.2.4 and MATLAB R \u00a9 7.11.0.584 (2010b), both in Linux Operating Systems. The script testMNIST.m creates the file mnistbbdbn.mat, with the DNN already trained. Once testMNIST.m is finished, it appears something like this: \u2022 For training data: rmse = 0.0155251 (1.6%); ErrorRate = 0.00196667 (0.196%); Tanaka et al. reported 0.158% [24]. \u2022 For test data: rmse = 0.00593; a total test rate of 250,000; ErrorRate = 160,000 (1.6%)."}, {"heading": "3.3 Understanding the toolbox example for MNIST", "text": "Once the example script (testMNIST.m) has been successfully executed, we run the script in Fig. 8.4https: / / www.gnu.org / software / octave / This script generates N = 10 images via imshow, see Fig. 9. The images are part of the first 10 test patterns. Each image is stored in a vector of size 784, corresponding to an image size of 28 \u00d7 28 pixels, and each pixel stores a number between 0 and 255, where 0 means background (white) and 255 means foreground (black); see Fig. 10, we represent the DNN inputs and outputs to the MNIST database. IN represents the inputs between 0 and 255, where 0 means that 0 means the difference between 0 and 255."}, {"heading": "4 Parameter Setting", "text": "When working with DNNs, several parameters need to be specified, including statistics for monitoring the contrastive divergence algorithm, batch sizes, monitoring overmatch (iterations), learning rate \u03b5, initial weights, number of hidden units and hidden layers, unit types (e.g. binary or gauss), dropouts, etc. [16, 11, 4, 6]. In practice, we can only focus on the following: \u2022 Maximum of iterations (MaxIter), also known as k for the contrastive divergence algorithm. \u2022 Learning rate (StepRatio). \u2022 Type units (e.g. Bernoulli or Gauss distribution). We analyze the effects of these DNN parameters using the XOR example; see point 5.2.4 below."}, {"heading": "5 Further Examples", "text": "In addition to the MNIST database example described above, this section presents examples of unattended and supervised learning, including prediction and classification tasks."}, {"heading": "5.1 Unsupervised learning", "text": "We will now show an example of unattended learning: The network architecture consists of a single RBM with six visible units and eight hidden units; see Fig. 11. The goal is to learn a simple pattern as shown below in Fig. 12. This pattern is a very simple example of unlabeled data. All the computing times specified in this section were determined on a PC with the following features: 2.3 GHz Intel Core i7 and 4GB of memory; Linux Ubuntu 12.04 and GNU Octave 3.2.4."}, {"heading": "5.1.1 Script", "text": "Fig. 12 shows the Matlab / Octave script for our example of unsupervised learning. We used the same toolbox as for the MNIST database [24]."}, {"heading": "5.1.2 Results", "text": "Since the script is executed in Fig. 12, several data are displayed. First, the training data (our pattern) is shown:% Matlab / Octave scriptTrainData = 1 1 0 0 0 0 0 0 1 1then the output after pretrainDBN is: out = 9.9e-01 9.9e-01 1.0e-04 1.2e-04 2.5e-05 2.6e-05 5.0e-04 4.3e-04 9.9e-01 9.9e-01 6.1e-04 6.0e-04 4.2e-06 3.6e-06 5.9e-06 5.4e-06 9.9e-01 9.9e-01This output are the probabilities [0,1], known as reconstructions, so we apply the function round, and then we get: out = 1 0 0 0 0 0 0 0 0 1 1The total elapsed time is 0.254730 seconds."}, {"heading": "5.1.3 Discussion", "text": "We found that the DNN in Fig. 11 is able to learn the given pattern unattended. We use a step ratio of 2.5 because it gives us fewer iterations, i.e. MaxIter = 50. Some authors recommend a learning rate (step ratio) of 0.01 [11, 24], but with this setting we need at least 1,000 iterations to learn the pattern; see Section 5.2.4 on parameter setting issues."}, {"heading": "5.2 Predicting Patterns", "text": "The following example simulates a prediction scenario for time series. We test two different patterns (pattern 1 and pattern 2). Our training data is a matrix with eight columns, i.e. the number of variables. We only use six variables as input and the last two column variables as output. The basic idea is that we only feed the network with six variables, then the network must \"predict\" the next two variables; see Fig. 13. Compared to the previous example, we use supervised learning as shown in the MNIST example above, see \u00a7 3. Therefore, the labels are our last two columns of the pattern (output), i.e. TrainLabels."}, {"heading": "5.2.1 Script", "text": "The script for this example is shown in Fig. 13."}, {"heading": "5.2.2 Results", "text": "The script in Abb. 14 produces the following output. First, it prints the TrainData and TrainLabels together with the statement [TrainData TrainLabels] from: Training... ans = 0 0 0 0 0 0 0 0.0000 0.0000 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "5.2.3 Discussion", "text": "In this example, we specify a DNN to predict two variables that have given six input variables. We found that the DNN is able to successfully predict the given patterns. Here, we only show results for pattern 1, but the results for pattern 2 are similar. We started testing the DNN with a different pattern and accidentally came across a pattern that the DNN cannot predict (4 inputs, 2 outputs): Training ans = 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 Testing... ans = 0.00000 0.00000 0.00000 0.49922 0.00000 0.00000 0.00000 0.00000 0.49922 0.49922 0.49922 0.00000 0.00000 0.00000 0.00000 0.00000 1.000000 0.00000 0.00000 different input values, but they have the first input values."}, {"heading": "5.2.4 XOR problem", "text": "The XOR problem is a non-linear problem that is a typical test for a classifier, because it is a problem that a simple linear classifier cannot learn. An example of a linear classifier is the Perceptron introduced by Frank Rosenblatt in 1957 [20]. A decade later Marvin Minsky and Seymour Paper wrote their famous book Perceptrons and showed that Perceptrons cannot solve the XOR problem [18]. Perhaps partly due to the publication of Perceptrons, there was a decline in research in neural networks until the backpropagation algorithm emerged about twenty years after the publication of Minsky and Paper. Here we analyze the XOR problem with a DNN; see Fig. 15."}, {"heading": "5.2.5 Script", "text": "The script for the XOR problem is shown in Fig. 16% Matlab / Octave script."}, {"heading": "5.2.6 Results", "text": "iDe \"iSe\" n, \"so iSe rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "5.2.7 Discussion", "text": "For the XOR problem, we found that the best performance is achieved with the following combination: opts.StepRatio = 2.5 and opts.MaxIter = 100. A large step ratio with few iterations allows us to achieve faster results than other settings. Performance is good enough to solve the XOR problem. Also, this setting correctly classifies if the input is real data. Therefore, this setting is used in the previous examples; see \u00a7 5.1 and \u00a7 5.2."}, {"heading": "6 Speech Processing", "text": "Speech processing has several applications, including speech recognition, speech recognition and speech recognition; see Fig. 17. Sometimes additional information is stored and associated with speech. Therefore, speech recognition can be either text-dependent or text-independent. In addition, speech recognition includes various tasks such as [2]: \u2022 speaker recognition \u2022 speaker recognition \u2022 speaker verification."}, {"heading": "6.1 Speech Features", "text": "The first step for most speech recognition systems is feature extraction from the temporally sampled acoustic waveform (audio); see Figure 18a. The temporal waveform is represented by overlapping frames. Each frame is generated every 10ms with a duration of 25ms. Subsequently, one feature is extracted for each frame. Several methods of feature extraction (acoustic representation) were investigated, including Linear Prediction Coefficient (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency Spaced Cepstral Coefficient (MFCCs) [5, 25]."}, {"heading": "6.2 DNN and Speech Processing", "text": "As we have shown above, DNNs have the flexibility to be used as either unattended or supervised learning. DNNs can therefore be used for regression or classification problems in speech recognition; see Fig. 19. Today, DNNs are successfully used in speech processing, including speech recognition [9, 26], speech recognition [7] and speech generation [17]. VOICEBOX6 is a speech processing toolbox for MATLAB R \u00a9, which is also publicly available.6http: / / www.ee.ic.ac.uk / hp / staff / dmb / voicebox / voicebox.html"}, {"heading": "7 Summary", "text": "We have introduced Deep Neural Networks (DNNs) and Restricted Boltzmann Machines (RBMs) and their relationship to Deep Learning (DL) and Deep Belief Nets (DBNs). Throughout the literature, there is some introductory work on RBMs [11, 6]. One of the contributions to this tutorial is the simple examples for a better understanding of RBMs and DNNs. The examples cover unattended and supervised learning, so we cover both blank and labeled data for prediction and classification. In addition, we present a publicly available MATLAB R \u00a9 toolbox to show the performance of DNNs and RBMs [24]. The toolbox and examples have been tested on Octave, the open source version of MATLAB R \u00a9. The last example, XOR Problem, presents some results by differentiating some hyperparameters of DNNs and detection of NT patterns: Finally, MATLAB uses two intelligent examples."}], "references": [{"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop", "G.E. Hinton"], "venue": "Clarendon Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Campbell", "author": ["J.P. Jr"], "venue": "Speaker recognition: A tutorial. Proceedings of IEEE, 85(6):1437\u20131462", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "D", "author": ["A. Coates", "B. Huval", "T. Wang", "A.Y. Wu"], "venue": "J.and Ng. Deep learning with COTS HPC systems. In Proceedings of the 30th International Conference on Machine Learning ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, volume 28", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1980}, {"title": "Training restricted Boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, 14:25\u201339", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Frame-by-frame language identification in short utterances using deep neural networks", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Mreno", "P.J. Moreno", "J. Gonzalez- Rodriguez"], "venue": "Neural Networks, 64:49\u201358", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Prentice Hall", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 29(6):82\u201397", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, 14(8):1711\u20131800", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "A practical guide to training restricted Boltzmann machines version 1", "author": ["G.E. Hinton"], "venue": "Department of Computer Science, University of Toronto", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(1):504\u2013507", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Sciences, volume 79, pages 2554\u2013\u20132558", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "J", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado"], "venue": "Dean, , and A. Ng. Building high-level features using large scale unsupervised learning. In Proceedings of International Conference on Machine Learning ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning for acoustic modeling in parametric speech generation", "author": ["Z.H. Ling", "S.Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.J. Qian", "H. Meng", "L. Deng"], "venue": "IEEE Signal Processing Magazine, 32(3):35\u201352", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptrons", "author": ["M.L. Minsky", "S.A. Papert"], "venue": "Cambridge, MA: MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1969}, {"title": "Neural Networks: A Systematic Introduction", "author": ["R. Rojas"], "venue": "Springer-Verlag", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "The perceptron\u2013A perceiving and recognizing automaton", "author": ["F. Rosenblatt"], "venue": "Technical Report 85-460-1, Cornell Aeronautical Laboratory", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1957}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(1):533\u2013536", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1986}, {"title": "Deep learning in neural networks: An overview", "author": ["J Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel Distributed Processing", "author": ["P. Smolensky"], "venue": "volume 1, chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194\u2013281. MIT Press, Cambridge", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1986}, {"title": "A novel inference of a restricted Boltzmann machine", "author": ["M. Tanaka", "M. Okutomi"], "venue": "International Conference on Pattern Recognition ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of speaker identification: Accuracy and robustness issues", "author": ["R. Togneri", "D. Pullella"], "venue": "IEEE Circuits and Systems Magazine, 2(1):23\u201361", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Single-channel mixed speech recognition using deep neural networks", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEE International Acoustics, Speech and Signal Processing (ICASSP), pages 5632\u20135636", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "The core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and widely studied by Hinton et al.", "startOffset": 84, "endOffset": 92}, {"referenceID": 9, "context": "The core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and widely studied by Hinton et al.", "startOffset": 84, "endOffset": 92}, {"referenceID": 12, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 10, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 2, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 21, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 14, "context": "However, DL mainly refers to Deep Neural Networks (DNNs) and in particular to DBNs and RBMs [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Graphics Processing Units (known as GPUs), Message Passing Interface (MPI) among other parallelization technologies [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 21, "context": "A wide survey on artificial intelligence and in particular DL has been published recently, which covers DNNs, Convolutional Neural Networks, Recurrent Neural Networks, among many other learning strategies [22].", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "The learning rule is the same as the maximum likelihood learning rule [contrastive divergence] for the infinite logistic belief net with tied weights [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Products of Experts (PoE) and Boltzmann machines are probabilistic generative models, and their intersection comes up with RBMs [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Learning by contrastive divergence of PoE is the basis of the learning algorithm of DBNs [10, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 11, "context": "Learning by contrastive divergence of PoE is the basis of the learning algorithm of DBNs [10, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 5, "context": "We recommend [6] as a gentle introduction that explains the training of RBMs and their relationship to graphical models including Markov Random Fields (MRFs); it also presents Markov chains to explain how a RBM draws samples from probability distributions such as Gibbs distribution of a MRF.", "startOffset": 13, "endOffset": 16}, {"referenceID": 11, "context": "The building blocks of a RBM are binary stochastic neurons [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Nevertheless, there are several ways to define real-valued visible neurons, where Gaussian-Binary-RBM are widely used [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 23, "context": "We use a publicly available MATLAB R \u00a9/Octave toolbox for RBMs developed by Tanaka and Okutomi [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 47, "endOffset": 50}, {"referenceID": 23, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "Section \u00a73 describes the toolbox developed by Tanaka and Okutomi [24] and the database MNIST.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "The hidden layer grabs features from the input data, and each neuron captures a different feature [12].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": ",Hn), the features [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "A joint configuration, (~v,~h) of the visible and hidden units has an energy given by [14] E(~v,~h) = \u2212 m \u2211", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "where vi and hj are the binary states of the visible and hidden units, respectively; ai, bj are the biases, and wij is a real valued weight associated with each edge in the network [11], see Fig.", "startOffset": 181, "endOffset": 185}, {"referenceID": 11, "context": "The building block of a RBM is a binary stochastic neuron [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "In this example, visible to hidden, hj is the probability of producing a spike [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 5, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 23, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 11, "context": "3 [12].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Finally, the weights and biases are adjusted in the following way [12]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "RBMs find better models if more steps of the CD algorithm are performed; CDk is used to denote the learning in k steps/iterations [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "The CD algorithm is summarized in Algorithm 1, and it uses the complete training data, batch learning [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "A Deep Belief Network (DBN) [12] is depicted in Fig.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 0, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 18, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 7, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 15, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 3, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 5, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 23, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 11, "context": "applied a DNN to the MINST handwritten digits database [12], see Fig.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "4% [12].", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "3 Toolbox for DNN We use a publicly available toolbox for MATLAB R \u00a9 developed by Tanaka and Okutomi [24], and which can be downloaded online.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "This toolbox is based on [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "This toolbox includes sparsity [16], dropout [4] and a novel inference 1http://yann.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "This toolbox includes sparsity [16], dropout [4] and a novel inference 1http://yann.", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "Figure 6: An hybrid DBN for supervised learning [12]; the MNIST database.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "for RBM devised by Tanaka [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Figure 7: Another DNN architecture for the MNIST database [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "158% [24].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "76% [24].", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 10, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 3, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 23, "context": "We used the same toolbox than for the MNIST database [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "This output are the probabilities [0,1], known as reconstructions, so we apply the function round, and then we obtain:", "startOffset": 34, "endOffset": 39}, {"referenceID": 10, "context": "01 [11, 24], but with this setting, we need at least 1,000 iterations to learn the pattern; see \u00a75.", "startOffset": 3, "endOffset": 11}, {"referenceID": 23, "context": "01 [11, 24], but with this setting, we need at least 1,000 iterations to learn the pattern; see \u00a75.", "startOffset": 3, "endOffset": 11}, {"referenceID": 19, "context": "In the neural networks literature, an example of a linear classifier is the perceptron introduced by Frank Rosenblatt in 1957 [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "A decade later, Marvin Minsky and Seymour Paper wrote their famous book Perceptrons, and they showed that perceptrons cannot solve the XOR problem [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 1, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 11, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 0, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 1, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 0, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 1, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 11, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 11, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 0, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 1, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 11, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 0, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 1, "context": "Moreover, Speaker Recognition involves different tasks such as [2]: \u2022 Speaker Identification \u2022 Speaker Detection \u2022 Speaker Verification.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Several methods have been investigated for feature extraction (acoustic representation) including Linear Prediction Coefficients (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency spaced Cepstral Coefficients(MFCCs) [5, 25].", "startOffset": 239, "endOffset": 246}, {"referenceID": 24, "context": "Several methods have been investigated for feature extraction (acoustic representation) including Linear Prediction Coefficients (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency spaced Cepstral Coefficients(MFCCs) [5, 25].", "startOffset": 239, "endOffset": 246}, {"referenceID": 8, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 97, "endOffset": 104}, {"referenceID": 25, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 97, "endOffset": 104}, {"referenceID": 6, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 130, "endOffset": 133}, {"referenceID": 16, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Across the literature, there are some introductory papers for RBMs [11, 6].", "startOffset": 67, "endOffset": 74}, {"referenceID": 5, "context": "Across the literature, there are some introductory papers for RBMs [11, 6].", "startOffset": 67, "endOffset": 74}, {"referenceID": 23, "context": "Moreover, we introduce a publicly available MATLAB R \u00a9 toolbox to show the performance of DNNs and RBMs [24].", "startOffset": 104, "endOffset": 108}], "year": 2016, "abstractText": "Developing Intelligent Systems involves artificial intelligence approaches including artificial neural networks. Here, we present a tutorial of Deep Neural Networks (DNNs), and some insights about the origin of the term \u201cdeep\u201d; references to deep learning are also given. Restricted Boltzmann Machines, which are the core of DNNs, are discussed in detail. An example of a simple two-layer network, performing unsupervised learning for unlabeled data, is shown. Deep Belief Networks (DBNs), which are used to build networks with more than two layers, are also described. Moreover, examples for supervised learning with DNNs performing simple prediction and classification tasks, are presented and explained. This tutorial includes two intelligent pattern recognition applications: handwritten digits (benchmark known as MNIST) and speech recognition.", "creator": "LaTeX with hyperref package"}}}