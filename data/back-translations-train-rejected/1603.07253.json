{"id": "1603.07253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Evaluating semantic models with word-sentence relatedness", "abstract": "Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available.", "histories": [["v1", "Wed, 23 Mar 2016 16:12:34 GMT  (250kb,AD)", "https://arxiv.org/abs/1603.07253v1", "8 pages, 2 figures, ancillary files"], ["v2", "Tue, 3 Jan 2017 17:25:08 GMT  (249kb,AD)", "http://arxiv.org/abs/1603.07253v2", "8 pages, 2 figures, ancillary files. Replaced original version to fix typos"]], "COMMENTS": "8 pages, 2 figures, ancillary files", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kimberly glasgow", "matthew roos", "amy haufler", "mark chevillet", "michael wolmetz"], "accepted": false, "id": "1603.07253"}, "pdf": {"name": "1603.07253.pdf", "metadata": {"source": "CRF", "title": "Evaluating semantic models with word-sentence relatedness", "authors": ["Kimberly Glasgow"], "emails": ["michael.wolmetz@jhuapl.edu"], "sections": [{"heading": "Author Keywords", "text": "Semantic similarity; semantic kinship; semantic; semantic textual similarity; conceptual knowledge; data sets; evaluation; benchmarking"}, {"heading": "Corresponding Author", "text": "Michael Wolmetz, michael.wolmetz @ jhuapl.edu"}, {"heading": "INTRODUCTION", "text": "This year, the time has come for the European Commission to leave the EU Presidency of the Council of the European Union in Brussels in order to leave the EU Presidency."}, {"heading": "METHODS & MATERIALS Stimuli", "text": "Each target concept is limited to a specific meaning or meaning, adapted to the sense of WordNet for the term [14]. The series of target concepts consists of nouns, verbs and adjectives and is distorted from abstract or unusual concepts to living, imaginable [15] and concrete concepts. Average concreteness of the target concepts is 4.18 on the 5-point scale reported by Brysbaert [2]. Target concepts have been selected to include the six semantic dimensions described below. Objects: things that exist physically, they can be animate or lifeless, natural or artificial (man-made) emotions. Objects usually have physical substance or are detectable by human senses."}, {"heading": "Comparison sentences", "text": "Each of the 25 target concepts was paired with 31 comparative sets. Many comparative sets were used across multiple target concepts, and a total of 240 different comparative sets were generated (listed in the attached downloadable data).The specific set of 31 sets, which were assigned and tested to a particular target concept, was constructed and selected to represent a series of semantic references between the target concept and the selected comparison sets. To achieve this, some sets included the target concept, and some sets contained at least one concept related to the target concept."}, {"heading": "Target POS Dimension Sense Concept", "text": "target term, and the remaining sentences contained no concepts that were supposed to be highly related to the target term. Where grammatically and semantically feasible, target terms or related concepts appeared in some sentences in the subject and in the predicate in others, appearing with or without adjective modifiers (for nouns)."}, {"heading": "BEHAVIORAL PROCEDURE", "text": "Most of them were rated with Maximum Difference Scaling (MDS) or a best-worst-scaling paradigm [11]. Since MDS sometimes requires a very large number of responses, we also tested the applicability of a second paradigm, which is considered to be simpler and less resource-intensive. MDS is a discrete selection method used to evaluate the relative meaning or preference of participants, to select the best and worst options from a range of elements presented. A variant of MDS was used to present three sets of a target concept, as shown in Figure 1. Participants were asked to consider the relations between the thetarget concepts and the three sets, and choose the set that best relates to the target concept."}, {"heading": "PARTICIPANTS", "text": "All participants spoke fluent English, had a high school diploma or similar, had normal or corrected vision, and self-reported no reading or speech impairments. Fifty-five participants (average age = 24) completed the MDS task for all 25 concepts using a web-based application. Data collected was analyzed for outliers, and eight of the 55 participants returned answers that deviated from the group remedies by more than three standard deviations. These participants were excluded from further analysis; 20 of the remaining 47 subjects also completed the free ranking paradigm."}, {"heading": "STS SYSTEMS", "text": "The relativization rankings produced by human participants were compared with relativization rankings. The rankings produced by four STS models were specifically replaced by two basic text-based models to help interpret them. Each of the models below was executed on the basis of the 25 target term lists, and each of them produced 25 ranking sets of comparative sets (just like the participants), which were compared with the behavior-based rankings in which the Spearman rankings are used. In practice, a matrix of co-occurrence frequencies is generated between substantive words, in which words that are related in their meaning occur in close proximity to another. [1] In practice, a matrix of co-occurrence frequencies is generated between pairs of substantive words in which co-occurrence frequencies use references to the frequency of two words within another five-word window of COPUS in contemporary English (CA)."}, {"heading": "RESULTS AND DISCUSSION", "text": "Data collected from both the MDS task and the free ranking task were used to generate group rankings for the sentences associated with each target concept (see Help File). As expected, participants often ranked sentences containing the target concept higher than sentences containing the target concept or similar concepts, but these patterns were not absolute. For example (as shown in Table 1), the sentence observed by the parents of the sick child was considered more closely related to the target concept family than to the sentence. The priest approached the solitary family and indicated that the participants took the larger context into account when evaluating relationships between target concepts and comparison sets. K-mean clusters for k = 3 to 5 were executed on the basis of the middle order (as well as the middle order and deviations) to assess whether the comparability varied continuously or discontinuously between sentences."}, {"heading": "System Correlation with Behavior (\u03c1)", "text": "The ranking of the free groups was 1.3: on average, a comparative set rank (out of 31) in the MDS task was 1.3 positions away from this set rank, reflecting the reliability of these tasks within the task. As the free task was seven times faster to complete, it could be a preferred method for making kinship judgments."}, {"heading": "STS SYSTEM COMPARISON", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "CONCLUSION", "text": "To illustrate the potential utility of this dataset, the results of the evaluation task were compared with several semantic text similarity systems. Higher-order text systems such as Word2Vec and UMBC ebiquity often accurately predicted human scores, but in some cases did not match human scores. The first major discrepancy between human and STS scores stemmed from subtle differences between semantic kinship and semantic similarity, and this distinction may need more attention in the development of ratings and applications for semantic models; the second major discrepancy stemmed from a failure of semantic models to capture entanglements and semantic similarity."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research is based on work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), on IARPA Contract No. 2012 12050800010. The views and conclusions contained therein are those of the authors and should not necessarily be interpreted to represent the official guidelines or recommendations of the ODNI, IARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes, notwithstanding the copyright notes contained therein. Authors also thank Dr. Lushan Han, Samsung Research America, and Dr. Tim Finin, University of Maryland, Baltimore County, for providing the Ebiquity STS System code, Mary Luongo for assisting in collecting behavioral responses, and Dr. Christine Piatko for discussing relevant research topics."}], "references": [{"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. Desouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational linguistics 18,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods 46,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The emergence of semantic meaning in the ventral temporal pathway", "author": ["Thomas A Carlson", "Ryan A Simmons", "Nikolaus Kriegeskorte", "L Robert Slevc"], "venue": "Journal of cognitive neuroscience 26,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "JAsIs 41,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Maximum difference scaling", "author": ["Roberto Furlan", "Graham Turner"], "venue": "International Journal of Market Research 56,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Semantic textual similarity systems", "author": ["Lushan Han", "Abhay Kashyap", "Tim Finin", "James Mayfield", "Jonathan Weese. 2013. UMBC EBIQUITY-CORE"], "venue": "Atlanta, Georgia, USA 44", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Text summarisation in progress: a literature review", "author": ["Elena Lloret", "Manuel Palomar"], "venue": "Artificial Intelligence Review 37,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A model for the largest difference judgments", "author": ["Jordan J. Louviere", "George G. Woodworth. 1991. Best-worst scaling"], "venue": "University of Alberta", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli. 2014. Semeval-2014 task 1"], "venue": "SemEval-2014", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "WordNet: a lexical database for English", "author": ["George A. Miller"], "venue": "Commun. ACM 38,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Concreteness, imagery, and meaningfulness: Values for 925 nouns. American Psychological Association", "author": ["Allan Paivio", "John C. Yuille", "Stephen A. Madigan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1968}, {"title": "Where do you know what you know? The representation of semantic knowledge in the human brain", "author": ["Karalyn Patterson", "Peter J. Nestor", "Timothy T. Rogers"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Semantic distance and the verification of semantic relations", "author": ["Lance J. Rips", "Edward J. Shoben", "Edward E. Smith"], "venue": "Journal of verbal learning and verbal behavior 12,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}, {"title": "Revisiting a Golden Age Hypothesis in the Era of Cognitive Neuroscience", "author": ["Timothy T. Rogers", "Christopher R. Cox"], "venue": "The Wiley Handbook on The Cognitive Neuroscience of Memory", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth", "author": ["Mark Steyvers", "Joshua B Tenenbaum"], "venue": "Cognitive science 29,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Towards a distributed account of conceptual knowledge", "author": ["Lorraine K Tyler", "Helen E Moss"], "venue": "Trends in cognitive sciences 5,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}], "referenceMentions": [{"referenceID": 12, "context": "lationships within and between sets [16, 18, 20].", "startOffset": 36, "endOffset": 48}, {"referenceID": 14, "context": "lationships within and between sets [16, 18, 20].", "startOffset": 36, "endOffset": 48}, {"referenceID": 16, "context": "lationships within and between sets [16, 18, 20].", "startOffset": 36, "endOffset": 48}, {"referenceID": 13, "context": "One general solution to this problem has been to abstract away from specific attributes in favor of ground-truths about the relationships between concepts [17].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "Models can then be evaluated in terms of how well they predict either the semantic distances between concepts [3] or the structures or networks estimated from those distances [19].", "startOffset": 110, "endOffset": 113}, {"referenceID": 15, "context": "Models can then be evaluated in terms of how well they predict either the semantic distances between concepts [3] or the structures or networks estimated from those distances [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 8, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "These relationships were tested experimentally using a Maximum Difference Scaling procedure [6], and replicated us-", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "Each target concept is restricted to a specific sense or meaning, adapted from WordNet senses for the term [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The set of target concepts consists of nouns, verbs and adjectives, and is biased away from abstract or uncommon concepts, and toward vivid, imageable [15], and concrete concepts.", "startOffset": 151, "endOffset": 155}, {"referenceID": 1, "context": "18 on the 5-point scale reported by Brysbaert [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "Best-Worst scaling paradigm [11].", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "N-gram: The basic n-gram model assumes that words that are related in meaning will occur in close proximity to one another [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "LSA: Latent Semantic Analysis is based on the principle that words that are related in meaning will occur in related texts or documents [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "Word2Vec: The Word2Vec system was built by Google using a neural network and a skip-gram model [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "WordNet) methods to produce semantic similarity scores [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Indeed, some automated approaches to text summarization and questionanswering have considered these factors [10].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available.", "creator": "LaTeX with hyperref package"}}}