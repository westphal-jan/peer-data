{"id": "1603.07400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "A Reconfigurable Low Power High Throughput Architecture for Deep Network Training", "abstract": "General purpose computing systems are used for a large variety of applications. Extensive supports for flexibility in these systems limit their energy efficiencies. Given that big data applications are among the main emerging workloads for computing systems, specialized architectures for big data processing are needed to enable low power and high throughput execution. Several big data applications are particularly focused on classification and clustering tasks. In this paper we propose a multicore heterogeneous architecture for big data processing. This system has the capability to process key machine learning algorithms such as deep neural network, autoencoder, and k-means clustering. Memristor crossbars are utilized to provide low power high throughput execution of neural networks. The system has both training and recognition (evaluation of new input) capabilities. The proposed system could be used for classification, unsupervised clustering, dimensionality reduction, feature extraction, and anomaly detection applications. The system level area and power benefits of the specialized architecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency over GPGPUs for big data processing.", "histories": [["v1", "Thu, 24 Mar 2016 00:52:22 GMT  (1436kb)", "http://arxiv.org/abs/1603.07400v1", "11 pages"], ["v2", "Wed, 15 Jun 2016 01:26:31 GMT  (1111kb)", "http://arxiv.org/abs/1603.07400v2", "9 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AR cs.DC", "authors": ["raqibul hasan", "tarek taha"], "accepted": false, "id": "1603.07400"}, "pdf": {"name": "1603.07400.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "II. SYSTEM OVERVIEW", "text": "This year it is more than ever before."}, {"heading": "III. AUTOENCODER AND MEMRISTOR NEURON CIRCUIT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Memristor Devices", "text": "The memristor component was first theorized in 1971 by Dr. Leon Chua [3]. Memristors are essentially nanoscale, variable resistors where the resistance is changed when a voltage is applied greater than a write threshold voltage (Vth) [17]. Therefore, their resistance can be read off by applying a voltage below Vth to them without changing the condition of the component. Several research groups have demonstrated memristive behavior using different materials. One such component, consisting of layers of HfOx and AlOx [18], exhibits a high state resistance (RON \u2248 10kB), a very high resistance ratio (ROFF / RON \u2248 1000) and a write threshold voltage of about 1.3 V. Physical memristors can be laid out in a grid known as a lattice. The circuit diagram and the layout of a memristor cross can each be viewed in Fig. 3."}, {"heading": "B. Neuron circuit", "text": "In this paper, we have used memristors as neural synaptic weights. Fig. 5 shows the memristor-based circuit design used in this paper. \u2212 In this circuit, each data input is connected to two virtually grounded operational amplifiers by a pair of memristors. \u2212 In a certain line, the conductivity of a memristor connected to the first column is higher than the conductivity of the memristor connected to the second column (A-), then the pair of memristors represents a positive synaptic weight. \u2212 In the reverse situation, the memristor pair represents a negative synaptic weight. \u2212 In Fig. 5, the memristor pair is a synaptic weight."}, {"heading": "C. Autoencoder", "text": "Several big data applications focus in particular on classification and clustering tasks. The robustness of such systems depends on how well they can extract important features from the raw data. In big data processing, we are interested in a generic feature extraction mechanism that is suitable for a variety of applications. Autoencoder [19] is a popular method for dimensionality reduction and feature extraction that automatically learns features from unlabeled data, that is, it is an unattended approach that does not require monitoring. The architecture of the autoencoder is similar to a multi-layered neural network, as shown in Fig. 7. An autoencoder attempts to learn the function hW, b (x) \u2248 x. That is, it tries to learn an approximation to the identity function, so that the output x \"of the input x is similar to the input x. In the network, restrictions are imposed, for example, 100 neural units are created by limiting the number of concrete units that we can detect within a given unit."}, {"heading": "D. Memristor Based Neural Network Implementation", "text": "The structures of a multilayer neural network and an autoencoder are similar to each other. Both can be regarded as an advanced neural network. Training of the two systems differs. The autoencoder is trained layer by layer. Fig. 7 shows a simple advanced neural network with four inputs, four outputs and three hidden layers. Fig. 8 shows a memristor crossbar-based circuit that can be used to evaluate the neural network in Fig. 7. There are two memristor crossbars in this circuit, each representing a layer of neurons. Each crossbar uses the one shown in Fig. 8. Neural crossbar uses the one shown in Fig. 7. There are two memristor crossbars in this circuit, each representing a layer of neurons. Each crossbar uses the one shown in Fig. 8. Neural crossbar uses the first layer of a third implex with the help of a third layer."}, {"heading": "E. The Training Algorithm", "text": "To ensure proper functionality, a multi-layer neural network must be trained using a training algorithm. Backpropagation (BP) [20] and variants of the BP algorithm are widely used to form such networks. BP's stochastic algorithm is used to train the multi-layer neural network on a memristor basis, and is described below: 1) Initialize the memristors with high random resistance. 2) Calculate for each input pattern x: i) the input pattern x on the crossbar circuit and evaluate the DPj values and outputs (yj) of all neurons (hidden neurons and output neurons). 2) Calculate for each output layer neuron j the error between the neuron output (yj) and the target output (tj)."}, {"heading": "F. Circuit Implementation of the Training Algorithm", "text": "The circuit is divided into the following main steps: 1. Apply the inputs on level 1 and record the layer 2: Step 1: A series of inputs is applied to layer 1 neurons, and the layer 2 neuron outputs are measured. This process is discredited in Figure 8. Errors are applied in 8 bit representations (one bit and 7 bits for size). Layer 2 errors are applied to layer 2. Layer 2 weights are applied after the conversion from digital to analog form, as in Figure 9. To generate layer 1 error (one bit and 7 bits for size). Layer 2 weights are applied to layer 2 weights."}, {"heading": "IV. HETEROGENEOUS CORES", "text": "The proposed system has a RISC core, a digital core for clusters and memristor neural cores, which are connected via an on-chip routing network, and the memristor neural cores could be used to implement autoencoders and relay neural networks of different configurations."}, {"heading": "A. Memristor Neural Core", "text": "Fig. 12 shows the memristor-based architecture of a single neural core. It consists of a memristor crossbar of the size 400 x 200, input and output buffers, a training unit and a control unit. Our goal was to use the largest possible crossbar, because this allows more calculations to be performed in parallel. In experiments with different cross bar sizes, we observed that 400 x 200 crossbars have very little effect on creeping paths for the memristor device under consideration (high resistance values). The control unit manages the input and output buffers and interacts with the specific routing switch connected to the core. The control unit is implemented as a finite state machine and is therefore of low overhead. Processing in the core is analog and the entire core process takes place in one cycle for one input. The neural cores communicate with each other in digital form, as it is expensive to exchange and lock analog signals."}, {"heading": "B. Digital Clustering Core", "text": "The core could be configured to generate up to 32 clusters and the maximum input dimension could be 32 after dimensionality reduction using the autoencoder. This system clusters based on distance calculations in Manhattan. Suppose our data dimension is n and the number of clusters is m. For one element of the data sample, the corresponding distances in Manhattan are evaluated in parallel for the current m cluster centers and collected in the distance registers (Fg. 13). If a subtractor (in the first row of subtractors) is negative, it is subtracted from the corresponding distance register, otherwise it is added. After n iterations (one for each element of the input sample), the minimum distance between the input sample and the current cluster centers in the distance registers is mapped. In the next m cycles, the minimum distance value and the corresponding cluster center index are mapped."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Applications and Datasets", "text": "We examined the MNIST [21] and ISOLET [22] datasets for classification and cluster tasks. Classification applications used deep networks in which the autoencoder was used for unattended layer-by-layer pre-training of the networks. For cluster applications, dimensionality reduction and function extraction tasks are performed with the autoencoder. Subsequently, the k-means cluster algorithm is applied to the data generated by the autoencoder. An autoencoder-based anomaly detection was examined on the KDD dataset [23]. Table I shows the configurations of neural networks for different datasets and applications."}, {"heading": "B. Mapping Neural Networks to Cores", "text": "The neural hardware is unable to schedule multiplex neurons because their synaptic weights are stored directly within the neural circuits, so the structure of a neural network may need to be altered to fit into a neural nucleus. In cases where the networks were significantly smaller than the neural core memory, several neural layers were mapped to a nucleus. In this case, the layers were executed on pipelines where the outputs of layer 1 were traced back to the same nucleus by the routing switch of the nucleus to layer 2. If a software network layer was too large to fit into a nucleus (either because it needed too many inputs or because it had too many neurons), the network layer was split into several nuclei. Splitting a layer into several nuclei due to a large number of output neurons is trivial."}, {"heading": "C. Area Power Calculations", "text": "The range, power and timing of the SRAM array used in the cluster core were calculated using CACTI [24], using the low power option. We assumed a 45 nm process in our range power calculations. Components of a typical cache that would not be needed in the neural core (such as the tag array and the tag comparator) were not included in the calculations; the cluster core also requires addition, registers, multiplexers and a control unit; the performance of these basic components was determined by SPICE simulations; for the digital system, a frequency of 200 MHz was assumed to keep power consumption low; for the memory cores, detailed SPICE simulations were used for current and timing calculations of the analog circuits (drivers, crossbars and activation circuits); these simulations considered the wire resistance and capacity within the crossbars to be used as [5] the transverse circuits, as the results of the transverse systems had to be evaluated."}, {"heading": "VI. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Supervised Training Result", "text": "MATLAB (R2014a) and SPICE (LTspice IV) were used to develop a simulation framework for applying the training algorithm to a multi-layer neural network circuit. SPICE was mainly used for detailed analog simulations of the memristor cross section and MATLAB for simulation of the rest of the system.The circuits were trained by applying input patterns one by one until the errors were below the desired levels.The simulation of the memory device used an accurate model of the device published in [27]. The memory device simulated for this circuit was published in [18] and the circuit characteristics for the model are shown in Fig. 15. This device was selected for its high minimum resistance and high resistance ratio. According to the data presented in [18], this device exhibits a minimum resistance of 10 kilowatt hours, a resistance ratio of 103 kilowatt hours and the full resistance range of the device can be switched in 20 microseconds."}, {"heading": "B. Unsupervised Training Result", "text": "The network configuration used was 4 \u2192 2 \u2192 4. After the training, the two hidden neuron outputs provide the characteristic representation of the corresponding input data in a reduced dimension of two. Fig. 17 shows the distribution of the data of three different classes (setosa, versicolor, virginica) in the characteristic space. We can observe that data of the same class appear closely in the characteristic space and could potentially be separated linearly."}, {"heading": "C. Anomaly Detection", "text": "The network configuration for this application was 39- > 15- > 39. Because the SPICE simulation of this network size takes a long time, we conducted this experiment in MATLAB. During the training, the autoencoder was trained only with normal data packets. It is expected that the differences between input and reconstruction for normal data packets will be smaller than the differences for the attack packets. The network was trained only with 5292 normal packet data (no abnormal packets were used during the training). Figures 18 and 19 show the distribution of the distances between inputs and the corresponding reconstructions for the normal packets or attack packets. Fig. 20 shows that the system can detect about 96.6% of anomalous packets with a 4% incorrect detection rate."}, {"heading": "E. Single Core Area and Power", "text": "The range of the digital cluster core is 0.039 mm2 and its power consumption is 1.36 mW. The training of 1000 samples for one epoch in this core takes 0.32 \u03bcs of time.The configuration of the neural memristor core is 400 \u00d7 100, i.e. it can accommodate a maximum of 400 inputs and process a maximum of 100 neurons.The range of a single neural memristor core is 0.0163 mm2. Table II shows the performance and timing of a single memristor core in different execution steps. The RISC core is used only for the configuration of the cores, routers and DMA engines. Consequently, we assume that the RISC core will be switched off during the actual training or evaluation phases thereafter."}, {"heading": "F. System Level Evaluations", "text": "Total system area: The entire multi-core system comprises 144 memristor neural cores, a digital cluster core, a RISC core, a DMA controller, 4 kB input buffer and 1 kB output buffer. The RISC core area was evaluated with McPat [36] and came to 0.52 mm2. The total system area was 2.94 mm2.Energy efficiency: We compared the throughput energy advantages of the proposed architecture compared to an Nvidia Tesla K20 GPU. The system consumes 225 W of power. The GPU range is 561 mm2 at a 28 nm process. Figures 22 and 23 show the throughput and energy efficiency of the proposed system over the GPU for different applications during the training. For the training, the proposed architecture provides up to 30-fold acceleration and four to six orders of magnitude more energy efficiency over GPU. Figures 24 and 25 show the throughput and speed of the proposed system over a range of energy efficiency over a range of five to 24 GPU for different applications."}, {"heading": "VII. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Digital Systems", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "B. Memristor Based Systems", "text": "The most recent results for memristor-based neuromorphic systems can be seen in [12-15]. Alibart [12] and Preioso [13] investigated only linearly separable problems. Boxun [15] demonstrates in situ training of memristor-based neural networks for nonlinear classifiers. They suggested having two copies of the same synaptic weights, one for forward and one for reverse. However, due to the shock resistance of memristor-based neural networks, it is practically not easy to have an exact copy of a memristor cross. Soudry et al. [14] proposes the implementation of gradient descend-based training on memristor-based neural networks. They used two memristor sistors and one memristor per synapse. Their synaptic weight accuracy is lower than that of the two memristors per synapeuron used in this paper. Liu [16 per memristor-based systems investigated]."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we have proposed a heterogeneous multicore architecture for big data processing, capable of processing important machine learning algorithms such as deep neural network, autoencoder and k-means clustering, with both training and recognition capabilities (evaluation of new input), using memristor crossbars to implement energy-efficient neural cores, and our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency than GPUs for big data processing."}], "references": [{"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun", "Olivier Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "PuDianNao: A Polyvalent Machine Learning Accelerator", "author": ["Daofu Liu", "Tianshi Chen", "Shaoli Liu", "Jinhong Zhou", "Shengyuan Zhou", "Olivier Teman", "Xiaobing Feng", "Xuehai Zhou", "Yunji Chen"], "venue": "In Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '15)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Memristor\u2014The Missing Circuit Element", "author": ["L.O. Chua"], "venue": "IEEE Transactions on Circuit Theory, vol. 18, no. 5, pp. 507\u2013519 (1971).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1971}, {"title": "Robust Neural Logic Block (NLB) Based on Memristor Crossbar Array", "author": ["D. Chabi", "W. Zhao", "D. Querlioz", "J.-O. Klein"], "venue": "IEEE/ACM International Symposium on Nanoscale Architectures,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex", "author": ["C. Zamarre\u00f1o-Ramos", "L.A. Camu\u00f1as-Mesa", "J.A. P\u00e9rez-Carrasco", "T. Masquelier", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience, Neuromorphic Engineering, vol. 5, Article 26, pp. 1-22, Mar. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The missing Memristor found", "author": ["D.B. Strukov", "G.S. Snider", "D.R. Stewart", "R.S. Williams"], "venue": "Nature, vol. 453, pp. 80\u201383 (2008).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploring the Design Space of Specialized Multicore Neural Processors", "author": ["T.M. Taha", "R. Hasan", "C. Yakopcic", "M.R. McLean"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristor Crossbar-Based Neuromorphic Computing System: A Case Study.", "author": ["M. Hu", "H. Li", "Y. Chen", "Q. Wu", "G.S. Rose", "R.W. Linderman"], "venue": "IEEE Trans. Neural Netw. Learning Syst", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Contrastive divergence for memristor-based restricted Boltzmann machine", "author": ["A.M. Sheri", "A. Rafique", "W. Pedrycz", "M. Jeon"], "venue": "Engineering Applications of Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training", "author": ["D. Soudry", "D.D. Castro", "A. Gal", "A. Kolodny", "S. Kvatinsky"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, accepted.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}, {"title": "A Heterogeneous Computing System with Memristor-Based Neuromorphic Accelerators.\" HPEC", "author": ["Liu", "Xiaoxiao"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Strukov, \"Pattern classification by memristive crossbar circuits with ex-situ and in-situ training", "author": ["F. Alibart", "E. Zamanidoost", "D.B"], "venue": "Nature Communications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Training and operation of an integrated neuromorphic network based on metal-oxide memristors", "author": ["M. Prezioso", "F. Merrikh-Bayat", "B.D. Hoskins", "G.C. Adam", "K.K. Likharev", "D.B. Strukov"], "venue": "Nature, 521(7550), 61-64, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training", "author": ["D. Soudry", "D.D. Castro", "A. Gal", "A. Kolodny", "S. Kvatinsky"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, issue 99, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Training itself: Mixed-signal training acceleration for memristorbased neural network", "author": ["Boxun Li", "Yuzhi Wang", "Yu Wang", "Chen, Y.", "Huazhong Yang"], "venue": "Design Automation Conference (ASP-DAC), 2014 19th Asia and South Pacific , vol., no., 20-23 Jan. 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "RENO: A high-efficient reconfigurable neuromorphic computing accelerator design", "author": ["Xiaoxiao Liu", "Mengjie Mao", "Beiye Liu", "Hai Li", "Yiran Chen", "Boxun Li", "Yu Wang", "Hao Jiang", "Barnell, M.", "Qing Wu", "Jianhua Yang"], "venue": "Design Automation Conference (DAC), 2015 52nd ACM/EDAC/IEEE , vol., no., pp.1-6, 8-12 June 2015", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory", "author": ["X. Dong", "C. Xu", "S. Member", "Y. Xie", "N.P. Jouppi"], "venue": "IEEE Trans. on Computer Aided Design of Integrated Circuits and Systems, vol. 31, no. 7, pp. 994-1007, July, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigating the switching dynamics and multilevel capability of bipolar metal oxide resistive switching memory", "author": ["S. Yu", "Y. Wu", "H.-S.P. Wong"], "venue": "Applied Physics Letters 98, 103514 (2011).", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research, vo. 11, pp. 3371-3408, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Artificial Intelligence: A Modern Approach (2nd Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Optimizing NUCA Organizations and Wiring Alternatives for Large Caches with CACTI 6.0", "author": ["N. Muralimanohar", "R. Balasubramonian", "N. Jouppi"], "venue": "In Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 40),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "ORION 2.0: A fast and accurate NoC power and area model for early-stage design space exploration", "author": ["A.B. Kahng", "B. Li", "L.S. Peh", "K. Samadi"], "venue": "Design, Automation & Test in Europe Conference & Exhibition, pp.423-428, 20-24 April 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "An energy efficient multi-bit TSV transmitter using capacitive coupling", "author": ["J. Gorner", "S. Hoppner", "D. Walter", "M. Haas", "D. Plettemeier", "R. Schuffny"], "venue": "Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on , vol., no., pp.650,653, 7-10 Dec. 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Memristor SPICE Model and Crossbar Simulation Based on Devices with Nanosecond Switching Time", "author": ["C. Yakopcic", "T.M. Taha", "G. Subramanyam", "R.E. Pino"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), August 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "High-Performance Computing for Systems of Spiking Neurons", "author": ["S.B. Furber", "S. Temple", "A.D. Brown"], "venue": "Proceedings of AISB'06 workshop on GC5: Architecture of Brain and Mind, vol.2, pp 29-36, Bristol, April, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Wafer-Scale Integration of Analog Neural Networks", "author": ["J. Schemmel", "J. Fieres", "K. Meier"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "A digital neurosynaptic core using embedded crossbar memory with  45pJ per spike in 45nm", "author": ["P. Merolla", "J. Arthur", "F. Akopyan", "N. Imam", "R. Manohar", "D.S. Modha"], "venue": "IEEE Custom Integrated Circuits Conference (CICC), vol., no., pp.1-4, 19-21 Sept. 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Building block of a programmable neuromorphic substrate: A digital neurosynaptic core", "author": ["J.V. Arthur", "P.A. Merolla", "F. Akopyan", "R. Alvarez", "A. Cassidy", "S. Chandra", "S.K. Esser", "N. Imam", "W. Risk", "D.B.D. Rubin", "R. Manohar", "D.S. Modha"], "venue": "The International Joint Conference on Neural Networks (IJCNN), pp.1-8, June 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "A million spikingneuron integrated circuit with a scalable communication network and interface.", "author": ["P. A Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "Jun Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "Ivan Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickne", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science 345,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "General-purpose code acceleration with limited-precision analog computation,", "author": ["R.S. Amant", "A. Yazdanbakhsh", "J. Park", "B. Thwaites", "H. Esmaeilzadeh", "A. Hassibi", "L. Ceze", "D. Burger"], "venue": "In Proceeding of the 41st annual international symposium on Computer architecuture (ISCA", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Continuous realworld inputs can open up alternative accelerator designs", "author": ["B. Belhadj", "A.J.L. Zheng", "R. H\u00e9liot", "O. Temam"], "venue": "SIGARCH Comput. Archit. News 41, 3 (June 2013).", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures", "author": ["Sheng Li", "Jung Ho Ahn", "R.D. Strong", "J.B. Brockman", "D.M. Tullsen", "N.P. Jouppi"], "venue": "Microarchitecture, 2009. MICRO-42. 42nd Annual IEEE/ACM International Symposium on , vol., no., pp.469-480, 12-16 Dec. 2009", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Examples of this include DaDianNao [1] and PuDianNao [2], both of which accelerate machine learning algorithms through digital circuits.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Examples of this include DaDianNao [1] and PuDianNao [2], both of which accelerate machine learning algorithms through digital circuits.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 11, "endOffset": 16}, {"referenceID": 3, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 11, "endOffset": 16}, {"referenceID": 4, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 5, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 6, "context": "This enables highly dense neuromorphic systems with great computational efficiency [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 12, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 11, "context": "[12,13] examined only linearly separable problems.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[12,13] examined only linearly separable problems.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[16] examined memristor based neural networks utilizing four memristors per synapse where the proposed systems utilize two memristors per synapse.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Leon Chua [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "Memristors are essentially nanoscale variable resistors, where the resistance is modified if a voltage greater than a write threshold voltage (Vth) is applied [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 17, "context": "One such device, composed of layers of HfOx and AlOx [18], has a high on state resistance (RON\u224810k\u03a9), a very high resistance ratio (ROFF/RON\u22481000) and a write threshold voltage of about 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "The autoencoder [19] is a popular method for dimensionality reduction and feature extraction approach which automatically learns features from unlabeled data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Back-propagation (BP) [20] and the variants of the BP algorithm are widely used for training such networks.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "The area, power, and timing of the SRAM array, used in the clustering core, were calculated using CACTI [24] with the low operating power transistor option utilized.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "The routing link power was calculated using Orion [25] (assuming 8 bits per link).", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "05 pJ/bit [26].", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Simulation of the memristor device used an accurate model of the device published in [27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The memristor device simulated for this circuit was published in [18] and the m cluster centers", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "According to the data presented in [18] this device has a minimum resistance of 10 k\u03a9, a resistance ratio of 10, and the full resistance range of the device can be switched in 20 \u03bcs by applying 2.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "Simulation results displaying the input voltage and current waveforms for the memristor model [27] that was based on the device in [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Simulation results displaying the input voltage and current waveforms for the memristor model [27] that was based on the device in [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 31, "context": "The RISC core area was evaluated using McPat [36] and came to 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 25, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 26, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 27, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 28, "context": "4 billion transistors [33].", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "DaDianNao and PuDianNao [1,2] are accelerator for deep neural networks (DNN) and convolutional neural networks (CNN).", "startOffset": 24, "endOffset": 29}, {"referenceID": 1, "context": "DaDianNao and PuDianNao [1,2] are accelerator for deep neural networks (DNN) and convolutional neural networks (CNN).", "startOffset": 24, "endOffset": 29}, {"referenceID": 1, "context": "In PuDianNao [2] neuron synaptic weights are stored in off-chip memory which requires data movement during training back and forth between the off-chip memory and the processing chip.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "In DaDianNao [1], neuron synaptic weights are stored in eDRAM and are later brought into a neural functional unit for execution.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "[34] presented a compilation approach where a user would annotate a code segment for conversion to a neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[35] proposed multicore architectures for spiking neurons and evaluated them for embedded signal processing applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 12, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 11, "context": "Alibart [12] and Preioso [13] examined only linearly separable problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "Alibart [12] and Preioso [13] examined only linearly separable problems.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Boxun [15] demonstrates in-situ training of memristor crossbar based neural networks for nonlinear classifier designs.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "[14] proposed implementation of gradient descent based training on memristor crossbar neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Liu [16] examined memristor based neural networks utilizing four memristors per synapse while the proposed systems utilized two memristors per synapse.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "General purpose computing systems are used for a large variety of applications. Extensive supports for flexibility in these systems limit their energy efficiencies. Given that big data applications are among the main emerging workloads for computing systems, specialized architectures for big data processing are needed to enable low power and high throughput execution. Several big data applications are particularly focused on classification and clustering tasks. In this paper we propose a multicore heterogeneous architecture for big data processing. This system has the capability to process key machine learning algorithms such as deep neural network, autoencoder, and kmeans clustering. Memristor crossbars are utilized to provide low power high throughput execution of neural networks. The system has both training and recognition (evaluation of new input) capabilities. The proposed system could be used for classification, unsupervised clustering, dimensionality reduction, feature extraction, and anomaly detection applications. The system level area and power benefits of the specialized architecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency over GPGPUs for big data processing. Keywords\u2013Low power architecture; memristor crossbars; autoencoder; unsupervised training; big data.", "creator": "Microsoft\u00ae Word 2013"}}}