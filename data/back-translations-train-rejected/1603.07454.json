{"id": "1603.07454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics", "abstract": "In this paper, we present Deep Extreme Feature Extraction (DEFE), a new ensemble MVA method for searching $\\tau^{+}\\tau^{-}$ channel of Higgs bosons in high energy physics. DEFE can be viewed as a deep ensemble learning scheme that trains a strongly diverse set of neural feature learners without explicitly encouraging diversity and penalizing correlations. This is achieved by adopting an implicit neural controller (not involved in feedforward compuation) that directly controls and distributes gradient flows from higher level deep prediction network. Such model-independent controller results in that every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. DEFE makes the ensembles 'deep' in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. With the application of this model, a selection regions full of signal process can be obtained through the training of a miniature collision events set. In comparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art performance: the error rate has decreased by about 37\\%, the accuracy has broken through 90\\% for the first time, along with the discovery significance has reached a standard deviation of 6.0 $\\sigma$. Experimental data shows that, DEFE is able to train an ensemble of discriminative feature learners that boosts the overperformance of final prediction.", "histories": [["v1", "Thu, 24 Mar 2016 07:12:20 GMT  (1818kb)", "http://arxiv.org/abs/1603.07454v1", "20 pages, 9 figures"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["chao ma", "tianchenghou", "bin lan", "jinhui xu", "zhenhua zhang"], "accepted": false, "id": "1603.07454"}, "pdf": {"name": "1603.07454.pdf", "metadata": {"source": "CRF", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics", "authors": ["Chao Ma"], "emails": ["20111003715@gdufs.edu.cn", "20130200814@gdufs.edu.cn", "20120200798@gdufs.edu.cn", "xujinh@indiana.edu", "zhangzhenhua@gdufs.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.07 454v 1 [cs"}, {"heading": "1 Introduction", "text": "The discovery of new particles is closely related to the optimization of the selection zone and the classification of signal events and background events. Therefore, an effective model of statistics and machine learning has an increasingly important role to play in high-energy physics. Likewise, the challenging data from the HEP would facilitate the invention and application of the new model of machine learning. Research to be carried out through one aspect of this bipartite doctorate is boson, the existence of which was temporarily confirmed in 2013, is an elementary particle in the Standard Model of particle physics [12]. To confirm the coupling effect between Higgs and Fermion, and finally the Standard Model."}, {"heading": "2 Deep Learning and Related Works", "text": "A new learning algorithm for the multi-layer network, deep learning [4], has developed a great interest in the research of machine learning, and there has been great success in various areas of classical learning. However, deep learning can not only be made more complicated, but also mitigate the local extremism problem of classical learning. However, the application of deep learning in high-energy physics has not been studied until recently. Baldi.P et al., initially, classical learning applies to the identification of the Higgs boson (the counter-channel of high-energy physics)."}, {"heading": "3 DEFE: the proposed method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Introduction", "text": "Based on the above analysis, our research focuses on the extraction of sub-characteristics - the Higgs boson - and we propose a new MVA method - the Deep Extreme Feature Extraction (DEFE) model. The idea of the model is that instead of directly approaching the ideological selection region, we divide the sample variable space between the feature space and the sample space, [27] as well as the so-called extreme selection region, which we use as a bridge to eventually approach globally and optimize the selection region of hadron signal events. More specifically, we operate the space division of the product space between feature space and sample space by a weak classifier and divide it into a number of overlapping sub-spaces (this process is called a discriminatory division), while balancing the ratio between the background events and signal events on each sub-region. On this basis, we build a SENDAum, whereby only the individual selection characteristics resulting from the extreme can be achieved."}, {"heading": "3.2 Problem Formulation", "text": "The set of the simulated event should be D = (x1, y1, w1),..., (xn, yn, wn), where xi-Rd, d is the dimensionality of the input marker, yi-Swi = ns, b) is the label of each event, meaning signal and background respectively. Let S be the set that contains all the signal events, B is the set that contains a background event, ns is the number of signal events, and nb is the number of background events. The weight of each event should be satisfactory: i-Swi = ns, i-Bwi = nb, (1) In the face of a classifier g: Rd \u2192 (b), we call the G-S-S selection unaffected (G = S-e)."}, {"heading": "3.3 Extreme Feature Extraction As Ensemble Learning With Diversity", "text": "It is not as if this is a model in which the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods in relation to the different learning methods. If these models are trained with decorative errors, their predictions can be lowered in order to improve learning performance (or experts), they are trained to solve the same tasks (e.g. different training examples, different random training). During the test phase, the predictions of several classifications are formed into a definitive predisposition that is expected."}, {"heading": "3.4 Constructing and Learning of the Extreme Selection Region", "text": "This year, it is up to us to decide whether we want to leave the country or not, \"he said in an interview with the German Press Agency.\" We have decided that we want to leave the country, \"he told the German Press Agency."}, {"heading": "3.5 Greedy Training Algorithm for DEFE", "text": "In order to solve these problems, we have to deal with the question of whether we are able to change the world, or whether we will be able to change the world, \"he told the Deutsche Presse-Agentur.\" We have to put ourselves in a position to change the world, \"he said.\" We have to put ourselves in a position to change the world, \"he said.\" We have to put ourselves in a position to change the world. \"\" We have to put ourselves in a position to change the world, \"he said.\" We have to put ourselves in a position to change the world, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it, to change it. \""}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Methodology", "text": "Based on the simulated data, the proposed Deep Extreme Feature Extraction (DEFE) is used to learn the selection region (or extreme selection region G-E. The goodness of such an approach is usually measured by different metrics. In this paper, the metric used to compare the goodness of fit is the total range under the Receiver Operating Characteristic Curve (ROC), i.e. the AUC metric. Generally, a higher value of the AUC represents a higher classification accuracy averaged over a wide range of different threshold selections. The expected significance of a discovery (in sigma units) is also calculated for 100 signal events and 1,000 background events. It denotes the significance of the zero selection region hypothesis (or discovery meaning) [8]. If the resulting P value of the zero selection hypothesis is less than a certain value and the value of a specific analysis is usually one-millionth lower than the value of a detection region, the resulting P value of the zero selection hypothesis can be evaluated as soon as the physics have been detected."}, {"heading": "4.2 Data", "text": "The data we use in our experiment comes from the Higgs Boson Machine Learning Challenge (data can be downloaded at http: / / www.kaggle.com / c / higgs-boson), which is produced by an official ATLAS simulator, mixing Higgs into tau-tau events with different backgrounds.Based on current knowledge of particle physics, random collisions are simulated, tracked and detected with a simulated detector. The mass of the Higgs boson is fixed at 125 GeV, taking into account the following collision event: 1. Signal event: The Higgs boson decays into \u03c4 + \u03c4 \u2212 2. Background event: The Z-bosons (91.2 GeV) decays into \u03c4 + \u03c4 \u2212 which resembles the signal event and becomes the difficult point in the classification.3. Background event 2: A pair of top quarks is involved, accompanied by lepton and decaying bosons becoming the most difficult point in the classification of electrons."}, {"heading": "4.3 Parameters and Training Strategy", "text": "We use hundreds of thousands of samples to train the DEFE model, and use about eighty thousand samples to test the DEFE model. ROC (Receiver Operating Characteristic Curve) is used to visualize performance and. The AUC (Area under the Curve of ROC) and the expected discovery significance are used to quantify performance. All data is normalized. After that, we do an n = 1 discriminatory partition, on each subset, with random swap ratio \u03b1 = 0.05. In other words, we divide the original data sets into four overlapped subsets. Finally, we employ SDAENN to gain the high-level function on an m = 3 partitioned feature space, on each subset, gaining a total of twelve high-level feature sets."}, {"heading": "5 Results", "text": "Table 1 shows the capture of the thirty-dimensional characteristics used in our model. In Table 2, we observe the comparison of the AUC accuracy rate between the DEFE model and other baseline models. Among them, the training sets contain 140,000 samples, and if not specifically addressed, low characteristics and high-level characteristics are all adopted (if not high-level characteristics are adopted, then the performance of DEFE and DNN is much equivalent).The expected significance of a discovery (in units of Gaussians) for 100 signal events and 1,000 background happenings.The calculation of the expected statistical significance refers to the method presented in document [3]. In [3], a slightly different task, which takes into account the case of a leptonic decay of Taus. Due to the similarities of both events and characteristics, their results are also listed for comparative purposes. Compared with the classic DEFN-Neural Network (DNN) under the limitation of 90% background stocking, the error rate decreases."}, {"heading": "6 Conclusion", "text": "In this paper, we propagated a novel method of ensemble depth learning, the Deep Extreme Feature Extraction (DEFE), for the identification of Higgs bosons (tau-tau channel) from background signals. Based on the construction and approximation of the so-called extreme selection region, the model is able to efficiently extract discriminatory features from multiple angles and dimensions, thereby increasing overall performance. The result is approximately one \u03c3 better than DNN. Compared to conventional deep learning algorithms, we find that the performance of DEFE is significantly increased by high-grade feature inputs, thereby avoiding the equivalent performance with or without high-level features. This result suggests that DEFE, unlike Vanilla Deep Neuronal Network, successfully trains a variety of learners of neural features and that the excess discriminatory information discovered in the future high-level training is still an open question."}], "references": [{"title": "Learning to discover: the higgs boson machine learning challenge", "author": ["Claire Adam-Bourdarios", "Glen Cowan", "Cecile Germain", "Isabelle Guyon", "Balzs Kgl", "David Rousseau"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Fast decorrelated neural network ensembles with random weights", "author": ["Monther Alhamdoosh", "Dianhui Wang"], "venue": "Information Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature Communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations & Trends in Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Bagging predictors", "author": ["Leo Breiman"], "venue": "In Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Asymptotic formulae for likelihood-based tests of new physics", "author": ["Glen Cowan", "Kyle Cranmer", "Eilam Gross", "Ofer Vitells"], "venue": "European Physical Journal C,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Generalization bounds for averaged classifiers", "author": ["Yoav Freund", "Yishay Mansour", "Robert E. Schapire"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of Computer & System Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Postuse review: Introduction to elementary particles", "author": ["David Griffiths", "Gerald W. Intemann"], "venue": "American Journal of Physics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Multiple choice learning: Learning to produce multiple structured outputs", "author": ["Abner Guzman-Rivera", "Dhruv Batra", "Pushmeet Kohli"], "venue": "Nips, pages 1799\u20131807,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "Eprint Arxiv,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V. Le", "Rajat Monga", "Matthieu Devin", "Greg Corrado", "Kai Chen", "Marc\u2019Aurelio Ranzato", "Jeffrey Dean", "Andrew Y. Ng"], "venue": "CoRR, abs/1112.6209,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Why M heads are better than one: Training a diverse ensemble of deep networks", "author": ["Stefan Lee", "Senthil Purushwalkam", "Michael Cogswell", "David J. Crandall", "Dhruv Batra"], "venue": "CoRR, abs/1511.06314,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Ensemble learning via negative correlation", "author": ["Y. Liu", "X. Yao"], "venue": "Neural Networks the Official Journal of the International Neural Network Society,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio Speech & Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Tau identification using multivariate techniques in atlas", "author": ["D.C. O\u2019Neil", "Atlas Collaboration"], "venue": "In Journal of Physics Conference Series,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Boosted decision trees as an alternative to artificial neural networks for particle identification", "author": ["Byron P. Roe", "Hai Jun Yang", "Ji Zhu", "Yong Liu", "Ion Stancu", "Gordon Mcgregor"], "venue": "Nuclear Instruments & Methods in Physics Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Evidence for higgs boson decays to the \u03c4\u03c4 final state with the atlas detector", "author": ["Nils Ruthmann"], "venue": "GeV-cms,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Theoretical Views of Boosting and Applications", "author": ["Robert E. Schapire"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "Eprint Arxiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "Wei Liu", "Yangqing Jia", "P. Sermanet"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih Volodymyr", "Kavukcuoglu Koray", "Silver David", "Andrei A Rusu", "Veness Joel", "Marc G Bellemare", "Graves Alex", "Riedmiller Martin", "Andreas K Fidjeland", "Ostrovski Georg"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Machine learning for event selection in high energy physics", "author": ["Shimon Whiteson", "Daniel Whiteson"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 19, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 27, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 10, "context": "Higgs boson, whose existence was temporarily confirmed in 2013, is an elementary particle in the Standard Model of particle physics [12].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "In order to affirm the coupling effect between Higgs and Fermion and finally to verify the Standard Model, the study of decay channel\u03c4\u03c4 through the large hadron collider (LHC) is of great significance [23].", "startOffset": 201, "endOffset": 205}, {"referenceID": 3, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 7, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 12, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 13, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 14, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 17, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 23, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 26, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 2, "context": ",2014 [3] initially applies the classical Deep Learning approach to the identification of the Higgs boson(the counter channel of bottom quark-anti bottom quark).", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": ",2014 [3].", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "The idea of the model is, instead of directly approximating the ideological selection region, we divide the sample-variable space supervisedly and train multiple SDAENN [27] as well as the so-called extreme selection region, using which as a bridge finally to approximate globally and optimize the selection region of the hadron signal events.", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "Then, objective of the problem is now to maximize the approximate median significance (AMS) [1], which defined as: 3", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "Ensemble deep learning forms many state-of-the-art solutions of different large scale tasks [22, 26].", "startOffset": 92, "endOffset": 100}, {"referenceID": 24, "context": "Ensemble deep learning forms many state-of-the-art solutions of different large scale tasks [22, 26].", "startOffset": 92, "endOffset": 100}, {"referenceID": 15, "context": ", not ensemble awared), and no efforts are made to improve diversity [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 11, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 16, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 4, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 8, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 9, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 22, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 6, "context": "It denotes the significance of null selection region hypothesis (or the discovery significance) [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The calculation of expected statistical significance is referred to the method presented in document [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "In [3], a slightly different task that the case of a pair of leptonic decay of Taus is considered.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In the future, it\u2019s still an open question to propose further training algorithms to train an EFE model universally and efficiently References [1] Claire Adam-Bourdarios, Glen Cowan, Cecile Germain, Isabelle Guyon, Balzs Kgl, and David Rousseau.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "[2] Monther Alhamdoosh and Dianhui Wang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Leo Breiman and Leo Breiman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Ronan Collobert and Jason Weston.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Yoav Freund, Yishay Mansour, and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] David Griffiths and Gerald W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, and Adam Coates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Quoc V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Byron P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Nils Ruthmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Karen Simonyan and Andrew Zisserman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre Antoine Manzagol.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Mnih Volodymyr, Kavukcuoglu Koray, Silver David, Andrei A Rusu, Veness Joel, Marc G Bellemare, Graves Alex, Riedmiller Martin, Andreas K Fidjeland, and Ostrovski Georg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Shimon Whiteson and Daniel Whiteson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Appendix II: Definition of Input variables [1]: 1.", "startOffset": 43, "endOffset": 46}], "year": 2016, "abstractText": "In this paper, we present Deep Extreme Feature Extraction (DEFE), a new ensemble MVA method for searching \u03c4\u03c4 channel of Higgs bosons in high energy physics. DEFE can be viewed as a deep ensemble learning scheme that trains a strongly diverse set of neural feature learners without explicitly encouraging diversity and penalizing correlations. This is achieved by adopting an implicit neural controller (not involved in feedforward compuation) that directly controls and distributes gradient flows from higher level deep prediction network. Such modelindependent controller results in that every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. DEFE makes the ensembles \u2019deep\u2019 in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. Based the construction and approximation of the so-called extreme selection region, the DEFE model is able to be trained efficiently, and extract discriminative features from multiple angles and dimensions, hence the improvement of the selection region of searching new particles in HEP can be achieved. With the application of this model, a selection regions full of signal process can be obtained through the training of a miniature collision events set. In comparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art performance: the error rate has decreased by about 37%, the accuracy has broken through 90% for the first time, along with the discovery significance has reached a standard deviation of 6.0 \u03c3. Experimental data shows that, DEFE is able to train an ensemble of discriminative feature learners that boosts the overperformance of final prediction. Furthermore, among high-level features, there are still some important patterns that are unidentified by DNN and are independent from low-level features, while DEFE is able to identify these significant patterns more efficiently.", "creator": "LaTeX with hyperref package"}}}