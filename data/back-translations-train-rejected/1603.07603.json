{"id": "1603.07603", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Semantic Regularities in Document Representations", "abstract": "Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.", "histories": [["v1", "Thu, 24 Mar 2016 14:45:20 GMT  (15kb)", "http://arxiv.org/abs/1603.07603v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fei sun", "jiafeng guo", "yanyan lan", "jun xu", "xueqi cheng"], "accepted": false, "id": "1603.07603"}, "pdf": {"name": "1603.07603.pdf", "metadata": {"source": "CRF", "title": "Semantic Regularities in Document Representations", "authors": ["Fei Sun", "Jiafeng Guo", "Yanyan Lan"], "emails": ["ofey.sunfei@gmail.com", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.07 603v 1 [cs.C L] 24 Mar 201 6"}, {"heading": "1 Introduction", "text": "Recently, Mikolov et al. (2013c) discovered that word representations through a recursive neural network (RNN) and through related log-linear models (Mikolov et al., 2013b) can capture the linguistic regularities in language that allow simple solutions to analog questions of the form \"Beijing: China as Paris\" using simple linear algebra. With this word analogy task, a flood of subsequent work has shown that similar linear structure can also be uncovered by representations from other methods (Mnih and Kavukcuoglu, 2013; Pennington et al, 2014; Levy and Goldberg, 2014b). Besides word representation is a fundamental and critical problem in natural language processing. Over the past decades, various methods have been proposed to present the document as a vector, including Bag of Words (BOW)."}, {"heading": "2 Measuring Semantic Regularities", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 A Document Analogy Test Set", "text": "We propose to create a document analogy test set so that we can quantify how well different document representations capture semantic regularities. Following the idea of the word analogy task, we try to build a test set of analogy questions of the form \"a is to b as c is to,\" where a, b, c are the identities of the documents. However, it is not trivial to directly label the relationships between two arbitrary documents due to the variety of topics. Fortunately, we have found that each Wikipedia page is a concise document that describes a particular concept, and therefore the relationships between the documents can be explained by their respective concepts. Therefore, we can convert the task of labeling between documents to that between concepts (which consist of words or phrases), where we already have a large data set, which consists of Mikolov et al. (2013b).Based on the idea above, we build a document analogy test set using Wikipedia."}, {"heading": "2.2 Analogy Reasoning", "text": "In this paper, we use the same method of vector offset (Mikolov et al., 2013c) for analogous reasoning. To answer questions such as \"a is to b as c is to,\" we try to find a document with the vector ~ x that comes closest to the vector ~ b \u2212 ~ a + ~ c due to the cosinal similarity: arg max x \u00b2 D, x 6 = a x 6 = b, x 6 = c (~ b + ~ a) \u00b7 ~ x (1), where ~ b, ~ c, ~ a and ~ x are the normalized document vectors. The question will only be considered correct if x is exactly the answer document in the assessment group. The evaluation value for this task is the percentage of correctly answered questions."}, {"heading": "3 Models", "text": "In this section, we briefly summarize the models used in this paper. First, we list the notation methods. Let D = {d1,.., dN} denote a corpus of N documents using the word vocabulary WE = {w1,.., w | V |}. Let X-RN \u00b7 | V | be a document-word matrix in which the entry VIII in X denotes the weight of the J-th word wj in the i-th document dj.Bag of Words (BOW) model treats a document as a bag (multiset) of its words. It represents a document di as a vector ~ xi = (xi1, xi | V), where xij denotes the weight of the j-th word wj in the i-th document dj. The most popular weighting scheme for xij is TF-IDF (Jones, 1972)."}, {"heading": "4 Experiments", "text": "In this section, we first describe our experimental settings, including the corpus, the selection of hyperparameters and specifications for different methods of document presentation. We then compare these methods using document analogy tasks and discuss the results."}, {"heading": "4.1 Corpus and Preprocessing", "text": "The corpus used in this experiment to learn document rendering is the same April 2010 Wikipedia dump as described in Section 2.1. In pre-processing, the corpus is lowercase, pure numerical words are removed, non-English characters occur, and the words occur less than 20 times."}, {"heading": "4.2 Experimental Settings", "text": "The basic methods used in this work, including BOW with TF-IDF weight, LSI, NMF, LDA, PVDM, PV-DBOW and BOWE. For BOW, LSI and LDA we use the popular Python themed model library gensim2. For NMF we choose the Python learning library scikit learn3. We implement PV-DM and PV-DBOW models in C + + + due to Le and Mikolov (2014) not yet published2http: / / radimrehurek.com / gensim 3http: / / scikit-learn.org source codes of PV models. For textual embedding in BOWE we use CBOW in Word2Vec-Tool4. The negative sampling method is applied instead of the hierarchical Softmax method, since we have found that the former always achieve a better performance. The learning rate is decreased from 0000000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00"}, {"heading": "4.3 Results", "text": "In Table 2, we compare the results of the 100-dimensional document vectors of all methods to different subtasks of the document analogy. As we can see, BOW is almost the worst of all methods. This shows the weakness of the simple vector space model for capturing semantic regularities. This is quite astonishing, since PV models can also be seen as implicit matrix factorization according to the explanations of Word2Vec (Levy and Goldberg, 2014a). A major difference is that conventional latent models usually work with each entry representing the frequency or TF-IDF of a word in a document."}, {"heading": "5 Conclusion", "text": "In this paper we introduce a new document analogy task for quantitative evaluation of how well the point product between normalized vectors in the analog reasoning equals the Euclidean distance. Table 3: Results for the document analogy task under different dimensions. DIMENSION MODEL50 100 150 200BOW 1.34 1.34 1.34 1.34LSI 4.19 9.88 17.0 21.81NMF 1.59 4.81 8.75 11.85LDA 3.52 8.43 10.39 10.45PV-DM 25.62 37.47 37.71 36.003PV-DBOW 25.33 37.76 40.61 39.09BOWE 42.05 60.42 66.74 69.49 different document representations capture semantic regularities. Based on the benchmark data set introduced, we perform empirical comparisons between several state-of-the-art document presentation methods. The results show that the neural embedding of document representations on this analogy works better to compare this existing task with the existing one."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. CoRR, abs/1502.03520", "author": ["Arora et al.2015] Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Document embedding with paragraph vectors", "author": ["Dai et al.2014] Andrew M Dai", "Christopher Olah", "Quoc V Le", "Greg S Corrado"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "Dai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Navigating controversy as a complex search task", "author": ["Elad Yom-Tov", "James Allan"], "venue": "In Proceedings of the First International Workshop on Supporting Complex Search Tasks", "citeRegEx": "Dori.Hacohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dori.Hacohen et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Compu-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["Karen Sprck Jones"], "venue": "Journal of Documentation,", "citeRegEx": "Jones.,? \\Q1972\\E", "shortCiteRegEx": "Jones.", "year": 1972}, {"title": "From word embeddings to document distances", "author": ["Yu Sun", "Nicholas I. Kolkin", "Kilian Q. Weinberger"], "venue": "In Proceedings of the 32th International Conference on Machine Learning", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Lee", "Seung1999] Daniel D. Lee", "H. Sebastian Seung"], "venue": "october", "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of the 2014 Conference on Em-", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta", "author": ["Shaoul", "Westbury2010] Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul et al\\.", "year": 2010}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Recently, Mikolov et al. (2013c) discovered that word representations learned by a recur-", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "other methods (Mnih and Kavukcuoglu, 2013; Pennington et al., 2014; Levy and Goldberg, 2014b).", "startOffset": 14, "endOffset": 93}, {"referenceID": 4, "context": "Over the past decades, various methods have been proposed to represent the document as a vector, including Bag of Words (BOW) (Harris, 1954), Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999)) and Latent Dirichlet Allocation (LDA) (Blei et al.", "startOffset": 173, "endOffset": 198}, {"referenceID": 2, "context": ", 1990), Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999)) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 110, "endOffset": 129}, {"referenceID": 20, "context": "Recently, there is a rising enthusiasm for applying the neural embedding methods to representing the documents (Srivastava et al., 2013; Le and Mikolov, 2014).", "startOffset": 111, "endOffset": 158}, {"referenceID": 5, "context": ", it may help controversial search (Dori-Hacohen et al., 2015) by discovering document pairs talking about oppo-", "startOffset": 35, "endOffset": 62}, {"referenceID": 3, "context": "site facts on controversial topics with some seed pairs, or help non-local corpus navigation and paper recommendation together with word vectors (Dai et al., 2014).", "startOffset": 145, "endOffset": 163}, {"referenceID": 13, "context": "data set from Mikolov et al. (2013b). Based on the idea above, we build a document analogy test set using Wikipedia and existing word and phrase analogy test set.", "startOffset": 14, "endOffset": 37}, {"referenceID": 6, "context": "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).", "startOffset": 118, "endOffset": 184}, {"referenceID": 12, "context": "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).", "startOffset": 118, "endOffset": 184}, {"referenceID": 17, "context": "adopt the publicly available April 2010 dump of Wikipedia1 (Shaoul and Westbury, 2010), which has been widely used in (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014).", "startOffset": 118, "endOffset": 184}, {"referenceID": 7, "context": "The most popular weighting scheme for xij is TF-IDF (Jones, 1972).", "startOffset": 52, "endOffset": 65}, {"referenceID": 2, "context": "The most representative work is the Latent Dirichlet Allocation (LDA) model introduced by Blei et al. (2003). It represents the documents as distributions over latent topics, where each topic is characterized by a distribution over words.", "startOffset": 90, "endOffset": 109}, {"referenceID": 1, "context": "model (Bengio et al., 2003).", "startOffset": 6, "endOffset": 27}, {"referenceID": 0, "context": "As discussed in (Arora et al., 2015), PMI is a key factor why Word2Vec can work well for word anal-", "startOffset": 16, "endOffset": 36}, {"referenceID": 8, "context": "Secondly, the calculation of Euclidean distance5 between documents under BOWE is equivalent to using a relaxed Word Mover\u2019s Distance (Kusner et al., 2015), which has", "startOffset": 133, "endOffset": 154}], "year": 2016, "abstractText": "Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-theart document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.", "creator": "LaTeX with hyperref package"}}}