{"id": "1603.07646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Recursive Neural Language Architecture for Tag Prediction", "abstract": "We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.", "histories": [["v1", "Thu, 24 Mar 2016 16:39:37 GMT  (501kb,D)", "http://arxiv.org/abs/1603.07646v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG cs.NE", "authors": ["saurabh kataria"], "accepted": false, "id": "1603.07646"}, "pdf": {"name": "1603.07646.pdf", "metadata": {"source": "CRF", "title": "Recursive Neural Language Architecture for Tag Prediction", "authors": ["Saurabh Kataria"], "emails": [], "sections": [{"heading": "Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "Related Work", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "Approach", "text": "In fact, it is a matter of a way in which people are able to determine themselves what they want and what they don't want. (...) In fact, it is a matter of being able to determine themselves. (...) It is as if people are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves what they want. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves what they want. (...) It is as if they do it, as if they do it, as if they do it, as if they want to do it, as if they want to do it. \"(...)"}, {"heading": "Objective function and Optimization", "text": "Based on the above-mentioned tag representation, we define a scoring function for a specific document-tag pair, (m, t) asL (m, t) = [f (U, V (:, t))] > Im [U > V (:, t)] (2), where Im-R {V \u00b7 V} defines a diagonal matrix in which diagonal entries in document m. We use the contrastive maximum margin criterion [Socher et al., 2013] to train our model. The basic idea is that every pair (m, t) coming from the training corpus should receive a higher score than a pair (m, t \u2212) in which the tag \u2212 is a random one. Let us reduce the set of all parameters to a minimum the following target: J (\u0432) = \"M\" - max {0, 1 \u2212 L (m, t) + L (t, t) \u2212 m \u2212."}, {"heading": "Experiments", "text": "Datasets: For our experiments, we used two real-world datasets with a CiteULike 2 dataset and a MovieLens 3 4: CiteULike dataset: Our first dataset originally came from [Wang and Lead, 2011] and was collected from the CiteULike database for more than six years between 2004 and 2010. Originally designed for personalized document recommendations, this dataset consists of documents tagged by users with at least 10 articles. [Wang et al., 2013] supplemented this dataset with tag information from citation-like websites. Each article is assigned to papers that are indexed in CiteSeerX to extract their titles and abstracts, resulting in 16,980 articles, 7,386 tags and 204,987 tag-item pairs. Tags with frequencies less than 5 were also removed from the dataset."}, {"heading": "Baselines", "text": "This is an example of the way in which we have to deal with other countries, how we have to deal with other countries, \"he said in an interview with the\" S\u00fcddeutsche Zeitung \"(Saturday).\" It's about more than just how we have to deal with each other, \"he said.\" It's about how we have to deal with each other. \"\" It's about how we have to deal with each other. \"\" It's about how we have to deal with each other. \"\" It's about how we have to deal with each other. \"\" It's about how we have to deal with each other. \"\" It's about how we have to deal with each other. \"It's about how we have to deal with each other.\" \"It's about how we have to deal with each other.\" \"It's about\". \"\" It's about how we have to deal with each other. \"\" \"It's about how we have to deal with each other.\" \"\" \"It's about.\" It's about. \"\" It's about. \"\" \"It's about.\" \"\" It's about. \"\" \"\" \"\" It's about. \""}, {"heading": "Evaluation Settings", "text": "In each dataset, similar to [Wang et al., 2013; 2015], P elements associated with each tag are randomly selected to form the training set, and the entire rest of the dataset is used as a test set. P is set to 1 or 10, respectively, to evaluate and compare the models under both sparse and dense conditions in the experiments, and for each value of P, the evaluation is repeated five times with different randomly selected training sets, and the average performance is reported. Following [Wang et al., 2015; Wang and Lead, 2011; Wang et al., 2013], we use recallas performance measurement, as the evaluation information appears in the form of implicit feedback [Rendle et al., 2009], meaning that a zero entry may arise due to the irrelevance between the tag and the element or the user's ignorance of the tags when marking elements."}, {"heading": "Parameter settings:", "text": "In fact, it is that we are able to hold our own, that we are able to put ourselves at the top, and that we are able to put ourselves at the top, \"he said."}, {"heading": "Conclusion", "text": "We have presented a novel architecture based on neural models that uses the tags associated with documents to find a meaningful representation of content and tags. We have presented a learning system for neural representation based on relevance feedback, in which the representation of tags is learned by recursively weighting word representations with the associated tags. We have shown that our modeling scheme outperforms several state-of-the-art baselines for tag recommendation tasks."}], "references": [{"title": "Learn", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model. J. Mach"], "venue": "Res., pages 1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "JMLR", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "User conditional hashtag prediction for images", "author": ["Denton et al", "2015] Emily Denton", "Jason Weston", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "and Narayan Bhamidipati", "author": ["Nemanja Djuric", "Hao Wu", "Vladan Radosavljevic", "Mihajlo Grbovic"], "venue": "Hierarchical neural language models for joint representation of streaming documents and their content,", "citeRegEx": "Djuric et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "interactive tag recommendation for flickr", "author": ["Nikhil Garg", "Ingmar Weber. Personalized"], "venue": "Proceedings of the 2008 ACM conference on Recommender systems, pages 67\u201374. ACM,", "citeRegEx": "Garg and Weber. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "RecSys \u201912", "author": ["Nikolas Landia. Utilising document content for tag recommendation in folksonomies. In ACM Recsys"], "venue": "pages 325\u2013328. ACM,", "citeRegEx": "Landia. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ICML", "author": ["Quoc V. Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Relation regularized matrix factorization", "author": ["Wu-Jun Li", "Dit-Yan Yeung"], "venue": "IJCAI,", "citeRegEx": "Li and Yeung. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning context-sensitive word embeddings with neural tensor skip-gram model", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Liu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A content-based method to enhance tag recommendation", "author": ["Yu-Ta Lu", "Shoou-I Yu", "Tsung-Chieh Chang", "Jane Yung-jen Hsu"], "venue": "IJCAI,", "citeRegEx": "Lu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In NIPS", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "author": ["Sanjay Purushotham", "Yan Liu", "C-c J Kuo. Collaborative topic regression with social matrix factorization for recommendation systems"], "venue": "pages 759\u2013766,", "citeRegEx": "Purushotham et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 727\u2013736", "author": ["Steffen Rendle", "Leandro Balby Marinho", "Alexandros Nanopoulos", "Lars Schmidt-Thieme. Learning optimal ranking with tensor factorization for tag recommendation. In SIGKDD"], "venue": "ACM,", "citeRegEx": "Rendle et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of the 25th international conference on Machine learning", "author": ["Ruslan Salakhutdinov", "Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo"], "venue": "pages 880\u2013887. ACM,", "citeRegEx": "Salakhutdinov and Mnih. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Proceedings of the 18th ACM international conference on Multimedia", "author": ["Yi Shen", "Jianping Fan. Leveraging loosely-tagged images", "inter-object correlations for tag recommendation"], "venue": "pages 5\u201314. ACM,", "citeRegEx": "Shen and Fan. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "2010] Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Chong Wang", "David M Blei. Collaborative topic modeling for recommending scientific articles"], "venue": "pages 448\u2013456. ACM,", "citeRegEx": "Wang and Blei. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Assistive tagging: A survey of multimedia tagging with human-computer joint exploration", "author": ["Meng Wang", "Bingbing Ni", "Xian-Sheng Hua", "Tat-Seng Chua"], "venue": "ACM Computing Surveys (CSUR), 44(4):25,", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Wu-Jun Li", "author": ["Hao Wang", "Binyi Chen"], "venue": "Collaborative topic regression with social regularization for tag recommendation.", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Relational stacked denoising autoencoder for tag recommendation", "author": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "venue": "AAAI,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "Weston et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale music annotation and retrieval: Learning to rank in joint semantic spaces", "author": ["Jason Weston", "Samy Bengio", "Philippe Hamel"], "venue": "Journal of New Music Research,", "citeRegEx": "Weston et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13)", "author": ["Jason Weston", "Ameesh Makadia", "Hector Yee. Label partitioning for sublinear ranking"], "venue": "pages 181\u2013189,", "citeRegEx": "Weston et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "Affinity weighted embedding", "author": ["Jason Weston", "Ron Weiss", "Hector Yee"], "venue": "arXiv preprint arXiv:1301.4171,", "citeRegEx": "Weston et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 7th ACM conference on Recommender systems", "author": ["Jason Weston", "Hector Yee", "Ron J Weiss. Learning to rank recommendations with the k-order statistic loss"], "venue": "pages 245\u2013248. ACM,", "citeRegEx": "Weston et al.. 2013c", "shortCiteRegEx": null, "year": 2013}, {"title": "and Keith Adams", "author": ["Jason Weston", "Sumit Chopra"], "venue": "# tagspace: Semantic embeddings from hashtags.", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 18th international conference on World wide web", "author": ["Lei Wu", "Linjun Yang", "Nenghai Yu", "Xian-Sheng Hua. Learning to tag"], "venue": "pages 361\u2013370. ACM,", "citeRegEx": "Wu et al.. 2009", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "Recent advances in learning latent representations, or embeddings based upon various neural architectures are proving to be successful in tag recommendation tasks [Wang et al., 2015; Weston et al., 2014], mainly due to compact representations of words and tags in continuous low dimensional space.", "startOffset": 163, "endOffset": 203}, {"referenceID": 26, "context": "Recent advances in learning latent representations, or embeddings based upon various neural architectures are proving to be successful in tag recommendation tasks [Wang et al., 2015; Weston et al., 2014], mainly due to compact representations of words and tags in continuous low dimensional space.", "startOffset": 163, "endOffset": 203}, {"referenceID": 10, "context": "For example, word2vec [Mikolov et al., 2013] tries to predict all the words in a document, given the embeddings of surrounding words.", "startOffset": 22, "endOffset": 44}, {"referenceID": 21, "context": "WSABIE [Weston et al., 2011] and its variants [Weston et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 24, "context": ", 2011] and its variants [Weston et al., 2013b], embed both the labels (or tags) and documents in a shared semantic space where document embedding predicts the \u201cclosest\u201d label embedding.", "startOffset": 25, "endOffset": 47}, {"referenceID": 21, "context": "WARP [Weston et al., 2011]) to discriminate among the relevant and non-relevant features while learning and making a prediction, causing the model to underfit.", "startOffset": 5, "endOffset": 26}, {"referenceID": 0, "context": "Distributed Representation Learning from Content and Tags Earlier research in distributed representation learning [Bengio et al., 2003] has focused on using probabilistic neural networks to build general representations of words that improve upon the classic ngram language models.", "startOffset": 114, "endOffset": 135}, {"referenceID": 10, "context": "More recently, this approach has been extended with two popular neural language models [Mikolov et al., 2013] for learning distributed representations of words, that are (1) continuous bag-ofwords model (CBOW) and (2) Skip-gram, collectively known as word2vec.", "startOffset": 87, "endOffset": 109}, {"referenceID": 6, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 3, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al., 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 6, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al., 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 21, "context": ", 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al., 2011] directly learns from tagword associations and outperform word2vec for prediction tasks [Weston et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 26, "context": ", 2011] directly learns from tagword associations and outperform word2vec for prediction tasks [Weston et al., 2014].", "startOffset": 95, "endOffset": 116}, {"referenceID": 22, "context": ", music annotation with textual tags [Weston et al., 2012], personalized video recommendation [Weston et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 23, "context": ", 2012], personalized video recommendation [Weston et al., 2013a], image annotation with labels, i.", "startOffset": 43, "endOffset": 65}, {"referenceID": 25, "context": "ImageNet [Weston et al., 2013c], personalized tag recommendation for images [Denton et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 26, "context": "[Weston et al., 2014] extended WSABIE with a convolutional neural network based document representation that can take word ordering into account in a supervised embedding framework.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Tag Recommendation for Documents Tag recommendation methods can roughly be categorized into three classes [Wang et al., 2012]: content-based methods, co-occurrence based methods, and hybrid methods.", "startOffset": 106, "endOffset": 125}, {"referenceID": 14, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 5, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 9, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 4, "context": "Co-occurrence based methods [Garg and Weber, 2008; Rendle et al., 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 28, "endOffset": 71}, {"referenceID": 12, "context": "Co-occurrence based methods [Garg and Weber, 2008; Rendle et al., 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 28, "endOffset": 71}, {"referenceID": 7, "context": ", 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 60, "endOffset": 80}, {"referenceID": 27, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 17, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 19, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 17, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 19, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al., 2013; Purushotham et al., 2012] have been proposed and adapted for tag recommendation to achieve promising performance.", "startOffset": 101, "endOffset": 146}, {"referenceID": 11, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al., 2013; Purushotham et al., 2012] have been proposed and adapted for tag recommendation to achieve promising performance.", "startOffset": 101, "endOffset": 146}, {"referenceID": 1, "context": "These models use latent Dirichlet allocation (LDA) [Blei et al., 2003] as the key component for learning item representations and use probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih, 2008] to process the co-occurrence matrix (tag-item matrix).", "startOffset": 51, "endOffset": 70}, {"referenceID": 13, "context": ", 2003] as the key component for learning item representations and use probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih, 2008] to process the co-occurrence matrix (tag-item matrix).", "startOffset": 112, "endOffset": 142}, {"referenceID": 20, "context": "Deep learning based generative models, such as deep generative autoencoders [Wang et al., 2015] remedy this drawback by representing both content and tag co-occurrences in same lower dimensional space and show improvements over CTR.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "However, a cubic learning time complexity (in terms of low dimensional space) for these models prohibit effective posterior estimation and resort to approximations [Li and Yeung, 2009; Wang et al., 2015].", "startOffset": 164, "endOffset": 203}, {"referenceID": 20, "context": "However, a cubic learning time complexity (in terms of low dimensional space) for these models prohibit effective posterior estimation and resort to approximations [Li and Yeung, 2009; Wang et al., 2015].", "startOffset": 164, "endOffset": 203}, {"referenceID": 21, "context": "1 refers to WSABIE [Weston et al., 2011].", "startOffset": 19, "endOffset": 40}, {"referenceID": 15, "context": "entity relation modeling [Socher et al., 2013], word sense disambiguation [Liu et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 8, "context": ", 2013], word sense disambiguation [Liu et al., 2015]), introducing tensor based parameters per tag is computation prohibitive and, due to addition of millions of parameters, can lead to overfitting.", "startOffset": 35, "endOffset": 53}, {"referenceID": 15, "context": "We use the contrastive max-margin criterion [Socher et al., 2013] to train our model 1.", "startOffset": 44, "endOffset": 65}, {"referenceID": 17, "context": "CiteULike: Our first dataset is originally from [Wang and Blei, 2011] and is collected from CiteULike database dump for over six years from 2004 to 2010.", "startOffset": 48, "endOffset": 69}, {"referenceID": 19, "context": "[Wang et al., 2013] further extended this dataset with corresponding tag information from citeulike website.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Latent Space based Representation: Latent space based tag recommendation methods typically combine Latent Dirichlet Allocation [Blei et al., 2003] that learns item representations in one latent space with probabilistic matrix factorization (PMF) [Salakhutdinov and", "startOffset": 127, "endOffset": 146}, {"referenceID": 21, "context": "1Note that other choices of ranking criterion include WARP [Weston et al., 2011], however, we select max-margin criteria for its wide applicability and fair to baselines.", "startOffset": 59, "endOffset": 80}, {"referenceID": 17, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 19, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 17, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 11, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 20, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 20, "context": "Generative Neural Architectures: Generative neural architecture such as Probabilistic Stacked Denoising Autoencoders [Wang et al., 2015] are generative variants of denoising autoencoders [Vincent et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 20, "context": "[Wang et al., 2015] extended the framework of denoising autoencoders by (i) proposing a deep architecture, and (2) assigning a data generating distributions to decoding layer of autoencoders and maximizing posterior probability of denoised input.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "[Wang et al., 2015](referred as SDAE) shows significant improvements over generative neural architectures for tag recommendation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "WSABIE [Weston et al., 2011] is one of the most popular supervised embeddings based representation learning approach and it\u2019s variants has been shown to perform well on various annotation and recommendation task e.", "startOffset": 7, "endOffset": 28}, {"referenceID": 25, "context": "image annotation with labels [Weston et al., 2013c], personalized tag recommendation [Denton et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 26, "context": "\u2022 TagSpace: TagSpace [Weston et al., 2014] extends", "startOffset": 21, "endOffset": 42}, {"referenceID": 26, "context": "TagSpace has shown significant improvements over WSABIE for large scale hashtag representation [Weston et al., 2014].", "startOffset": 95, "endOffset": 116}, {"referenceID": 19, "context": "In each dataset, similar to [Wang et al., 2013; 2015], P items associated with each tag are randomly selected to form the training set and all the rest of the dataset is used as the test set.", "startOffset": 28, "endOffset": 53}, {"referenceID": 20, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 17, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 19, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 12, "context": ", 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al., 2009], which means a zero entry may be due to irrelevance between the tag and the item or the user\u2019s ignorance of the tags when tagging items.", "startOffset": 120, "endOffset": 141}, {"referenceID": 19, "context": "For CTR and SDAE, we set parameters as described by [Wang et al., 2013] and [Wang et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 20, "context": ", 2013] and [Wang et al., 2015] respectively.", "startOffset": 12, "endOffset": 31}, {"referenceID": 20, "context": "For SDAE, we use a 2-layer architecture (described as setting \u201820000-200-50-200-20000\u2019 in [Wang et al., 2015]).", "startOffset": 90, "endOffset": 109}], "year": 2016, "abstractText": "We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.", "creator": "LaTeX with hyperref package"}}}