{"id": "1603.07695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "abstract": "This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method.", "histories": [["v1", "Thu, 24 Mar 2016 18:22:39 GMT  (507kb,D)", "http://arxiv.org/abs/1603.07695v1", "Word embeddings"]], "COMMENTS": "Word embeddings", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["quan liu", "zhen-hua ling", "hui jiang", "yu hu"], "accepted": false, "id": "1603.07695"}, "pdf": {"name": "1603.07695.pdf", "metadata": {"source": "CRF", "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "authors": ["Quan Liu", "Zhen-Hua Ling", "Hui Jiang", "Yu Hu"], "emails": ["quanliu@mail.ustc.edu.cn,", "zhling@ustc.edu.cn", "hj@cse.yorku.ca,", "yuhu@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "Word embedding that represents words in continuous vector space based on distributional hypothesis has been an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a). Meanwhile, this embedding of words has become an important research topic in the natural language processing community (Hinton et al., 2003; Mikolov et al., 2013a), the C & W model (Collobert et al., 2011), the continuous word embedding of words (CBOW) and Skip-gram word2vec models (Mikolov et al., 2013a), and the GloVe model (Pennington et al., 2014). Word embedding techniques have been widely applied to various word processing tasks, including machine translation (Devlin et al., 2014; Wuet al., 2014), sequential labeling (Collobert et al., 2011 and Chantonbert selection)."}, {"heading": "2 Related Work", "text": "POS has been used in various tasks of natural language processing such as speech modeling (Heeman, 1998), dependency parsing (Koo et al., 2008), and recognition of name units (Turian et al., 2010), but very few of them investigated the use of POS information for distributed word representation. The most closely related method was the method proposed by Qiu et al. (2014), which tracked a representation vector for each POS day of a word and did not attempt to improve the representation capability of word embedding. To use POS information, this work borrows from the idea of speech modeling (Jelinek, 1990; Brown et al., 1992; Heeman et al., 1998) and proposes to use matrices to weight POS relevance to model each word-context pair for embedding words."}, {"heading": "3 The proposed model", "text": "In this section, we present the proposed model that uses POS information to learn word embedding, hereinafter referred to as PWE. Given a large training corpus, the POS information can be efficiently retrieved using a modern POS tagger. Consider a sequence of words S = {w1,..., wN} with N-word tokens. After performing the POS marking, each word token wi is marked as a specific POS tag zi. The corresponding word-POS pairs are referred to as < wi, zi >."}, {"heading": "3.1 Main framework", "text": "In this paper, it is proposed to integrate POS information for embedding words based on the CBOW model (Mikolov et al., 2013a). First, we examine the objective function of a typical CBOW model, which is to maximize the log probability of each token given its contexts: Qcbow = 1T T p = 1 log (wt | wt \u2212 ct + c) (1), where c specifies the context window size, T is the symbol number of the training corpus. Meanwhile, the word predictive probability isp (wt | wt \u2212 ct + c) = exp (w (2) t \u00b7 v \u2212 c \u2212 c \u2212 c), where Vk = 1 exp (2) k \u00b7 v \u00b7 c t is the word meaning of the training corpus (2), (2) where the word predictive probability isp (wt \u2212 ct \u2212 c is the word size, Vw \u2212 V is the word size)."}, {"heading": "3.2 Optimization algorithm", "text": "In the PWE framework, all POS relevance weights are updated together with the word representations during the training. This paper proposes to apply the stochastic gradient descent algorithm (SGD) to parameter learning.The key topic for the training of the PWE model is the calculation of the derivatives of the POS relevance weight matrices. The partial derivatives of the objective function with respect to word embedding are \u2202 Qcbow \u2202 w (1) t + i = \u03a6i (zt + i, zt) \u2202 Qcbow \u2202 vt + ct \u2212 c (5), where \u2202 Qcbow \u2202 vt + ct \u2212 c could be efficiently calculated under the typical word embedding frame. Meanwhile, the partial derivative with respect to the POS relevance weight can be calculated as 16th Qcbow Yankei (zt + i, zt \u2212 c (5)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental setup", "text": "In this section, we present the experimental setup that included the training corpus, POS tagger and parameter settings for all experiments.Training Corpus: This work used the snapshot of the Wikipedia corpus from April 2010 (Shaoul and Westbury, 2010) with a total of about 2 million articles and 1 billion tokens. Wikipedia corpus was pre-processed with the Perl script from page 2 of Matt Mahoney. After normalization, we constructed a vocabulary with 212,300 different words by discarding words that occur less than 50 times.Part-of-speech tagger: To get the POS tags for each training script, we used the OpenNLP toolkit3 for parts of the language tagging. The tag set is the Penn Treebank POS tag set (Marcus et al., 1993), which consists of 36 common POS tags and 6 symbol tags."}, {"heading": "4.2 Qualitative analysis", "text": "For simplified visualization, all major POS tags are divided into 5 coarse-grained groups (N, V, J, R, Other) as created by Qiu et al. (2014). The corresponding colors in the visualization numbers are {red, canary-like, green, blue, pink, red}. We find that the proposed model could group words with a similar POS category. We also test our model on the basis of an unattended POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012). Using the k-mean cluster algorithm, we map each cluster to the gold standard tag that is most common for the words in that cluster."}, {"heading": "4.3 Syntactic word analogy task", "text": "Here we use two syntactic word analogy datasets for experiments. The first, called MSR, is a dataset containing 8000 morphosyntactic analogy questions (Mikolov et al., 2013c); the second, called SYN, is a dataset proposed in Mikolov et al. (2013a) containing 10675 syntactic questions. To answer these analogy questions, we first remove all words from the vocabulary 4 and then use the typical similarity multiplication method to find the correct answers from the entire vocabulary (Levy et al., 2014). Our experimental results are in Table 1. The experimental results of the two syntactic word analogies4This has removed 1574 instances from the MSR dataset and 66 instances from the SYN dataset. Word analogy problems have shown that the proposed PWE model achieves significant improvements over the syntactical word analogies."}, {"heading": "4.4 Word similarity tasks", "text": "We test our models on WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012) and MC (Miller and Charles, 1991) word similarity tasks. All experimental results are in Table 2. Experimental results shown in Table 1 suggest that the proposed PWE model achieves consistent improvements in three word similarity tasks, proving that the inclusion of POS relevance weights for discriminatory context weighting is useful to better model word-context patterns to enable word embedding."}, {"heading": "5 Conclusion", "text": "This paper has proposed a model for learning distributed word representations with context weighting based on POS relevance weights. This paper designs location-specific POS relevance weighting matrices for weighting word-context pairs during the training process of word embedding. In the proposed model, word embedding and POS relevance weighting matrices are jointly learned using the stochastic gradient descending algorithm. Experiments with popular word analogy and word similarity tasks have all demonstrated the effectiveness of the proposed method."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Revisiting word embedding for contrasting meaning", "author": ["Chen et al.2015] Zhigang Chen", "Wei Lin", "Qian Chen", "Xiaoping Chen", "Si Wei", "Xiaodan Zhu", "Hui Jiang"], "venue": "Proceedings of ACL", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Two decades of unsupervised pos induction: How far have we come", "author": ["Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proceedings of ACL,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "Proceedings of NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of WWW,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Pos tagging versus classes in language modeling", "author": ["Peter A Heeman"], "venue": "In Proceedings of the 6th Workshop on Very Large Corpora,", "citeRegEx": "Heeman.,? \\Q1998\\E", "shortCiteRegEx": "Heeman.", "year": 1998}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Distributed representations. In Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["James L McClelland", "David E Rumelhart"], "venue": "Volume 1: Foundations,", "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Self-organized language modeling for speech recognition", "author": ["Fred Jelinek"], "venue": "Readings in speech recognition,", "citeRegEx": "Jelinek.,? \\Q1990\\E", "shortCiteRegEx": "Jelinek.", "year": 1990}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras P\u00e9rez", "Michael Collins"], "venue": null, "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Dependencybased word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy et al.2014] Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu et al.2015] Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu"], "venue": "Proceedings of ACL", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991] George A Miller", "Walter G Charles"], "venue": "Language and cognitive processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Qiu et al.2014] Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta", "author": ["Shaoul", "Westbury2010] Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul et al\\.", "year": 2010}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["Wu et al.2014] Haiyang Wu", "Daxiang Dong", "Xiaoguang Hu", "Dianhai Yu", "Wei He", "Hua Wu", "Haifeng Wang", "Ting Liu"], "venue": "Proceedings of EMNLP", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Xu et al.2014] Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Learning syntactic categories using paradigmatic representations of word context", "author": ["Enis Sert", "Deniz Yuret"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Yatbaz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Bilinguallyconstrained phrase embeddings for machine translation", "author": ["Zhang et al.2014] Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Word embedding that represents words into continuous vector space based on distributional hypothesis is an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a).", "startOffset": 177, "endOffset": 242}, {"referenceID": 27, "context": "Word embedding that represents words into continuous vector space based on distributional hypothesis is an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a).", "startOffset": 177, "endOffset": 242}, {"referenceID": 0, "context": "State-of-theart word embedding models include the neural language models (Bengio et al., 2003; Mikolov et al., 2010), the C&W model (Collobert et al.", "startOffset": 73, "endOffset": 116}, {"referenceID": 19, "context": "State-of-theart word embedding models include the neural language models (Bengio et al., 2003; Mikolov et al., 2010), the C&W model (Collobert et al.", "startOffset": 73, "endOffset": 116}, {"referenceID": 5, "context": ", 2010), the C&W model (Collobert et al., 2011), the continuous bag-of-word (CBOW) and Skip-gram word2vec models (Mikolov et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 24, "context": ", 2013a), and the GloVe model (Pennington et al., 2014).", "startOffset": 30, "endOffset": 55}, {"referenceID": 6, "context": "Word embedding techniques have been widely applied to various natural language processing tasks, including machine translation (Devlin et al., 2014; Wu et al., 2014), sequence labelling (Collobert et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 28, "context": "Word embedding techniques have been widely applied to various natural language processing tasks, including machine translation (Devlin et al., 2014; Wu et al., 2014), sequence labelling (Collobert et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 5, "context": ", 2014), sequence labelling (Collobert et al., 2011) and antonym selection (Chen et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 3, "context": ", 2011) and antonym selection (Chen et al., 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 7, "context": "However, it is still inadequate to learn high-quality representations just relying on word level distributional information collected from text corpora (Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 152, "endOffset": 192}, {"referenceID": 16, "context": "However, it is still inadequate to learn high-quality representations just relying on word level distributional information collected from text corpora (Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 152, "endOffset": 192}, {"referenceID": 29, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 7, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 16, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 32, "context": "Meanwhile, some people attempted to utilize multilingual parallel corpora to guide the word vector training process (Zhang et al., 2014; Hermann and Blunsom, 2014; Lu et al., 2015).", "startOffset": 116, "endOffset": 180}, {"referenceID": 17, "context": "Meanwhile, some people attempted to utilize multilingual parallel corpora to guide the word vector training process (Zhang et al., 2014; Hermann and Blunsom, 2014; Lu et al., 2015).", "startOffset": 116, "endOffset": 180}, {"referenceID": 7, "context": ", 2014; Faruqui et al., 2015; Liu et al., 2015). In Levy and Goldberg (2014), they investigated to use syntactic contexts that were derived from automatically produced dependency parse-trees for word representation training.", "startOffset": 8, "endOffset": 77}, {"referenceID": 12, "context": "tic roles of words and ignore much of their lexical information (Jelinek, 1990).", "startOffset": 64, "endOffset": 79}, {"referenceID": 9, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al.", "startOffset": 91, "endOffset": 105}, {"referenceID": 13, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al., 2008) and name entity recognition (Turian et al.", "startOffset": 126, "endOffset": 144}, {"referenceID": 27, "context": ", 2008) and name entity recognition (Turian et al., 2010).", "startOffset": 36, "endOffset": 57}, {"referenceID": 12, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 1, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 9, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 8, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al., 2008) and name entity recognition (Turian et al., 2010). However, very few of them investigated to use POS information for distributed word representation. The most related one was the method proposed by Qiu et al. (2014) which trained one representation vector for each POS tag of a word and did not try to improve the representation ability of word embeddings.", "startOffset": 92, "endOffset": 361}, {"referenceID": 18, "context": "The tag set is the Penn Treebank POS tag set (Marcus et al., 1993), which consists of 36 common POS tags and 6 symbol tags.", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": "We also test our model based on unsupervised POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012).", "startOffset": 81, "endOffset": 135}, {"referenceID": 30, "context": "We also test our model based on unsupervised POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012).", "startOffset": 81, "endOffset": 135}, {"referenceID": 24, "context": "For simplified visulization, all main POS tags are divided into 5 coarse-grained groups {N, V, J, R, Other} as the work of Qiu et al. (2014). The corresponding colors in the visualization figures are {red, canary, green, blue, pink, red} respectively.", "startOffset": 123, "endOffset": 141}, {"referenceID": 14, "context": "To answer those analogy questions, we firstly remove all out-of-vocabulary words4 and then use the typical similarity multiplication method to find the correct answers from the entire vocabulary (Levy et al., 2014).", "startOffset": 195, "endOffset": 214}, {"referenceID": 17, "context": "The first one named as MSR is a dataset that contains 8000 morphosyntactic analogy questions (Mikolov et al., 2013c). The other one named as SYN is a dataset proposed in Mikolov et al. (2013a), which contains 10675 syntactic questions.", "startOffset": 94, "endOffset": 193}, {"referenceID": 8, "context": "We test our models on WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 2, "context": ", 2001), MEN (Bruni et al., 2012), and MC (Miller and Charles, 1991) tasks of word similarity.", "startOffset": 13, "endOffset": 33}], "year": 2016, "abstractText": "This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}