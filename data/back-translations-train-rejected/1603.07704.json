{"id": "1603.07704", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "abstract": "In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNNs) and relation modulated neural nets (RMNNs), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multirelational knowledge bases and common-sense reasoning. Experimental results on several popular data sets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, comparing with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples.", "histories": [["v1", "Thu, 24 Mar 2016 18:54:18 GMT  (314kb,D)", "https://arxiv.org/abs/1603.07704v1", "Probabilistic reasoning, Deep learning"], ["v2", "Wed, 3 Aug 2016 14:31:17 GMT  (1461kb,D)", "http://arxiv.org/abs/1603.07704v2", "Probabilistic reasoning, Winograd Schema Challenge, Deep learning, Neural Networks, Distributed Representation"]], "COMMENTS": "Probabilistic reasoning, Deep learning", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["quan liu", "hui jiang", "rew evdokimov", "zhen-hua ling", "xiaodan zhu", "si wei", "yu hu"], "accepted": false, "id": "1603.07704"}, "pdf": {"name": "1603.07704.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "authors": ["Quan Liu", "Hui Jiang", "Andrew Evdokimov", "Zhen-Hua Ling", "Xiaodan Zhu", "Si Wei", "Yu Hu"], "emails": ["quanliu@mail.ustc.edu.cn,", "hj@cse.yorku.ca,", "ae2718@cse.yorku.ca,", "zhling@ustc.edu.cn", "xiaodan@cse.yorku.ca,", "siwei@iflytek.com,", "yuhu@iflytek.com"], "sections": [{"heading": "Introduction", "text": "This year it is more than ever before."}, {"heading": "Motivation: Association between Events", "text": "This paper aims to model the associations between the events using neural networking methods. To clarify our main work, we will first describe the characteristics of the events and the possible associations between the events. Based on the analysis of the event association, we will present the motivation for the proposed neural association models. In common sense, the main characteristics of the events are the following: \u2022 Massive: In most natural situations, the number of events is massive, which means that the association space we model is very large. \u2022 Frugal: All events occur in our dialect life. It is a very challenging task to grasp the similarities between all these different events. At the same time, the association between the events appears everywhere."}, {"heading": "Neural Association Models (NAM)", "text": "In this paper, we propose to use a non-linear model, the neural association model, for probabilistic thinking. Our main goal is to use neural networks to model the association probability for two arbitrary events E1 and E2 in a domain, i.e. Pr (E2 | E1) of E2 conditioning to E1. All possible events in the domain are projected into a continuous space without specifying an explicit dependency structure between them. In the following, we first present neural association models (NAM) as a general modeling framework for probabilistic thinking. Next, we describe two specific NAM structures for modeling typical multirelational data."}, {"heading": "NAM in general", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Deep Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Association in DNNs", "text": "In fact, it is the case that this is a type and manner in which people are able to comply with the rules in which they are located. (...) It is as if they were able to comply with the rules. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "DNN for NAMs", "text": "The first NAM structure is a traditional DNN as shown in Figure 3. Here, we use multirelational data in KB (1) head (1) head (1) head (1) head (1). Given a KB triple xn = (ei, rk, ej) and its associated designation yn (true or false), we eject E1 = (ei, rk) and E2 = ej to compute Pr (E2 | E1) as the following VectorTail entity vectorf Score FunctionW (1) W (L)... out: z (L) In: a (L) off: a (2) off: z (1) off: z in: z in: z In: a (1) B (2) B (L) B (L + 1) New Relation Head vectorfW (2) W (2) W (L) W (L)."}, {"heading": "Relation-modulated Neural Networks (RMNN)", "text": "In particular, for multi-relationship data following the idea in (Xue et al. 2014), we propose to use the so-called Relationship Modulated Neural Networks (RMNN), as shown in Figure 4.The RMNN uses the same operations as DNNs to project all entities and relationships into low-dimensional continuous space. (As shown in Figure 4, we connect the knowledge-specific relation code c (k) to all hidden layers in the network.0Relationcode (S) Head Entity Vector f Score functionW (1) W (2) W (L)... out: z (L) out: a (L) out: z (1) out: z (1) out: z (1) In: a (1) B (2) B (L) B (L) B (B) B: B (1) B (L) B (B) B: B (L) B: B (L) B (L) B (L) B (L) B: 1) out: z (1) New Relation Head Entity W (W) L)."}, {"heading": "Experiments", "text": "In this section, we evaluate the proposed NAM models for different puzzles. First, we describe the experimental setup and then report on the results of several puzzles, including recognition of textual consequences, triple classification in multirelational CBs, healthy thinking, and knowledge transfer learning."}, {"heading": "Experimental setup", "text": "Here we first present some common experimental settings that will be used for all experiments: 1) For entity or sentence representations, we present them by composing from their word vectors as in (Socher et al. 2013). All word vectors are initialized by a pre-trained word embedding model (Mikolov et al. 2013), which is trained on a large English Wikipedia corpus. Dimensions for all word embedding are set to 100 for all experiments; 2) The dimensions of all relationship codes are set to 50. All relationship codes are randomly initialized; 3) For network structures, we use ReLU as a nonlinear activation function and all network parameters are initialized accordingly (Glorot and Bengio 2010). In the meantime, since the number of training examples for most likely thought tasks is relatively small, we adopt the dropout approach (Hinton et al. 2012) to avoid the overfit problem."}, {"heading": "Recognizing Textual Entailment", "text": "In this experiment, we use the SNLI dataset in (Bowman et al. 2015) to perform two-tier RTE experiments (entropy or contradiction), all distances that are not referred to as \"entropy\" are converted into contradiction in our experiments.The SNLI dataset contains hundreds of thousands of training examples that are useful for forming a NAM model. As this dataset does not contain multirelational data, we are only examining the DNN structure for this task. The final NAM result, together with the base performance provided in (Bowman et al. 2015) is in Table 2.From the results, we can see that the proposed DNN-Base NAM model achieves significant improvements over various conventional methods, indicating that we can better model the relationship in natural space by presenting deep consistent sets in consistent networks."}, {"heading": "Triple classification in multi-relational KBs", "text": "In this section, we evaluate the proposed NAM models based on two popular knowledge triple classification datasets, namely WN11 and FB13 in (Socher et al. 2013) (derived from WordNet and FreeBase), to predict whether some new knowledge triple classification datasets are based on other training factors in the database. WN11 dataset contains 38,696 unique units, comprising a total of 11 different relationships, while FB13 dataset covers 13 relationships and 75,043 units. Table 3 summarizes the statistics of these two datasets. The aim of the knowledge triple classification is to predict whether a given triple xn = (ei, rk, ej) is correct or not. We first use the training data to learn NAM models, and then use the development set to adjust a global threshold T to make a binary decision: the triple classification is considered true if f (xn) the NN data is classified as the wrong TN and NN data."}, {"heading": "Commonsense Reasoning", "text": "Similar to the triple classification task (Socher et al. 2013), in this paper we use the ConceptNet KB (Liu and Singh 2004) to construct a new healthy dataset called asCN14. In building CN14, we first randomly select all facts in ConceptNet related to 14 typical Common Sense relations, e.g. UsedFor, CapableOf. (see Figure 5 for all 14 relationships), then randomly divide the extracted facts into three groups, Train, Dev and Test. Finally, to create a test set for classification, we randomly switch entities (throughout the vocabulary) from correct triples and get a total of 2 \u00d7 # test triples (half are positive samples and half are negative examples). CN14 statistics are compared in Table 5.The CN14 Dataset for the Common Sensible Questions such as: Is a camel able to travel through the desert? The proposed NAM models are answered by comparing this EM relation to the probability question (EN2)."}, {"heading": "Knowledge Transfer Learning", "text": "Knowledge transfer between different domains is a characteristic feature and a crucial cornerstone of human learning. In this section, we evaluate the proposed NAM models for a knowledge transfer learning scenario, in which we adapt a prefabricated model to an invisible relationship with only a few training samples from the new relationship. However, here, we randomly select a relationship, e.g. CausesDesire in CN14 for this experiment. This relationship contains only 4,800 training samples and 480 test samples. During the experiments, we use all 13 other relationships in CN14 to train baseline NAM models (both DNN and RMNN). During transfer learning, we freeze all NAM parameters, including all weights and entity representations, and learn only a new relationship code for CausesDesire from the given samples. In the end, the learned relationship code (along with the original NAM models) is used to classify the new learning processes."}, {"heading": "Extending NAMs for Winograd Schema Data Collection", "text": "In the previous series of experiments, all the aforementioned manually generated training data were used for us. However, in many cases, if we want flexible reasoning in the real world, obtaining the training data can also be very difficult. In a first step, we are now working on recording the causal relationships between the individual words. We believe that this knowledge is a key component in modelling the association relationships between the individual countries."}, {"heading": "Association Links", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Automatic Cause-Effect Pair Collection", "text": "Based on the queries created, in this section we present the procedures for extracting cause and effect pairs from large unstructured texts. The general system framework is shown in Figure 9. Query Search The goal of the query search is to find all possible sentences that the input queries may contain. Since the number of queries is very large, we structure all queries as hashmap and behavior character matching during text scan. In detail, the search program begins with \"seal\" Active, positive-active, negative-passive, negative \"arrest\" Active, positive-active \"arrest\" Active, positive-active-passive, positive-arrest \"attachment,\" part-of-speech tagging, and dependence parsing on the source corpus. After that, we scan the corpus from the beginning to the end. When handling each sentence, we will try to find the appropriate words (or phrases) to help us find the use of the hashtag."}, {"heading": "Data Collection Results", "text": "Table 8 shows the corpora we used to collect the causal effect pairs and the corresponding data collection results. We extract approximately 240,000 pairs from different corpora."}, {"heading": "Winograd Schema Challenge", "text": "In order to further evaluate the effectiveness of the proposed model of neural association, we are conducting experiments in this paper to solve the complex problems of the Winograd Scheme (Levesque, Davis, and Morgenstern 2011; Morgenstern, Davis, and Ortiz Jr.). Winograd Scheme is a common sense task proposed in recent years, which has been treated as an alternative to the Turing Test (Turing 1950). This is a new AI task, and it would be very interesting to see if neural networking methods are suitable for solving this problem. In this section, we will then describe the progress we have made in trying to meet the Winograd Scheme challenge. To clarify what the main task of the Winograd Scheme is, we will first introduce it at a high level, and then we will present the system framework and all the corresponding modules we have proposed to solve the Winograd Scheme problems automatically, and finally we will have a discussion of the scheme."}, {"heading": "Winograd Schema", "text": "The Winograd Scheme (WS) evaluates the ability of common sense to argue on the basis of a traditional, very difficult task of processing natural language: coreference resolution (Levesque, Davis and Morgenstern 2011; Saba 2015). The Winograd Scheme problems are carefully designed to be a task that cannot be easily solved without sound knowledge. In fact, even the solution of traditional problems of coreference resolution is based on semantics or world knowledge (Rahman and Ng 2011; Strube 2016). To describe the WS in detail, let's just copy out a few words here (Levesque, Davis and Morgenstern 2011). A WS is a small reading comprehension test that includes a single binary question. Here are two examples: \u2022 The trophy would not fit in the brown suitcase because it was too big. What was too big? - the trophy - Answer 1: the suitcase \u2022 Joan made sure to thank Susan for all the help she had given."}, {"heading": "System Framework", "text": "In this paper, we suggest that the general knowledge required in many Winograd Schema problems could be formulated as some associations between individual events. On the basis of the phrase \"Joan thanked Susan for all the help she has given,\" the common sense wisdom is that the man who receives help should thank the man who gives him help. We believe that by modelling the association between event and thanks, help and thanks, we can make the decision by comparing the association probability Pr (thank | get help) and Pr (thank | give help). If the models are well trained, we should get the inequality Pr (thank | get help) > Pr (thank | give help). Following this idea, we propose to use the data constructed from the previous section and expand the NAM models for solving WS problems."}, {"heading": "Transform Transform", "text": "\u2022 RelationVec-NAM: On the other hand, in this configuration, we treat all the typical 16 dimensions shown in Figure 8 as different relationships. So there are 16 relation vectors in the corresponding NAM models. Currently, we use the RMNN structure for the NAM.cause effect Neural Association Model."}, {"heading": "Neural Association", "text": "Transform TransformTraining The NAM models based on these two configurations are simple: All network parameters, including relation vectors and linear transformation matrices, are learned using the standard stochastic descend algorithm."}, {"heading": "Experiments", "text": "This year it is more than ever before."}, {"heading": "Conclusions", "text": "In this paper, we have examined two model structures, DNN and RMNN, for NAMs. Experimental results on several reasoning tasks have shown that both DNNs and RMNNs can outperform existing methods. In this paper, we also publish some preliminary results on the use of NAMs for knowledge transfer. We have found that the proposed RMNN model can be quickly adapted to new relationships without sacrificing performance in the original relationships. After demonstrating the effectiveness of NAM models, we apply it to solve more complex common sense problems, such as the Winograd Schema Model (Levesque, Davis and Morgenstern 2011). To support model training in this task, we propose a simple method of collecting associative phrase pairs from text corpora. Experiments conducted on a number of Winograd Schema problems indicate that association problems are successfully solved."}, {"heading": "Acknowledgments", "text": "We would like to thank Prof. Gary Marcus of New York University for his useful comments on common sense. We would also like to thank Prof. Ernest Davis, Dr. Leora Morgenstern and Dr. Charles Ortiz for their wonderful organizations that made the first Winograd Schema Challenge possible. This paper was partially supported by the Science and Technology Development of Anhui Province, China (grant no. 2014z02006), the Basic Research Fund for Central Universities (grant no. WK2350000001) and the Strategic Priority Research Program of the Chinese Academy of Sciences (grant no. XDB02070006)."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio"], "venue": "The Journal of Machine Learning Research 3:1137\u20131155", "citeRegEx": "Bengio,? \\Q2003\\E", "shortCiteRegEx": "Bengio", "year": 2003}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Bordes"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Bordes,? \\Q2012\\E", "shortCiteRegEx": "Bordes", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "C", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "Manning"], "venue": "D.", "citeRegEx": "Bowman et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["Bowman"], "venue": "R.", "citeRegEx": "Bowman 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "and Bengio", "author": ["X. Glorot"], "venue": "Y.", "citeRegEx": "Glorot and Bengio 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. arXiv preprint arXiv:1511.02301", "author": ["Hill"], "venue": null, "citeRegEx": "Hill,? \\Q2015\\E", "shortCiteRegEx": "Hill", "year": 2015}, {"title": "R", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "Salakhutdinov"], "venue": "R.", "citeRegEx": "Hinton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural networks 3(5):551\u2013560", "author": ["Stinchcombe Hornik", "K. White 1990] Hornik", "M. Stinchcombe", "H. White"], "venue": null, "citeRegEx": "Hornik et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1990}, {"title": "F", "author": ["Jensen"], "venue": "V.", "citeRegEx": "Jensen 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "and Friedman", "author": ["D. Koller"], "venue": "N.", "citeRegEx": "Koller and Friedman 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "H", "author": ["Levesque"], "venue": "J.; Davis, E.; and Morgenstern, L.", "citeRegEx": "Levesque. Davis. and Morgenstern 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "and Singh", "author": ["H. Liu"], "venue": "P.", "citeRegEx": "Liu and Singh 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "C", "author": ["L. Morgenstern", "E. Davis", "Ortiz Jr"], "venue": "L.", "citeRegEx": "Morgenstern. Davis. and Ortiz Jr 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "E", "author": ["Mueller"], "venue": "T.", "citeRegEx": "Mueller 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "G", "author": ["V. Nair", "Hinton"], "venue": "E.", "citeRegEx": "Nair and Hinton 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "R", "author": ["Neapolitan"], "venue": "E.", "citeRegEx": "Neapolitan 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A review of relational machine learning for knowledge graphs. arXiv preprint arXiv:1503.00759", "author": ["Nickel"], "venue": null, "citeRegEx": "Nickel,? \\Q2015\\E", "shortCiteRegEx": "Nickel", "year": 2015}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Tresp Nickel", "M. Kriegel 2012] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of WWW,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "C", "author": ["Osgood"], "venue": "E.", "citeRegEx": "Osgood 1952", "shortCiteRegEx": null, "year": 1952}, {"title": "Solving hard coreference problems", "author": ["Khashabi Peng", "H. Roth 2015] Peng", "D. Khashabi", "D. Roth"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "and Ng", "author": ["A. Rahman"], "venue": "V.", "citeRegEx": "Rahman and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A", "author": ["Turing"], "venue": "M.", "citeRegEx": "Turing 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["Xue"], "venue": null, "citeRegEx": "Xue,? \\Q2014\\E", "shortCiteRegEx": "Xue", "year": 2014}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Zhu"], "venue": "In Proceedings of the IEEE International Conference on Computer", "citeRegEx": "Zhu,? \\Q2015\\E", "shortCiteRegEx": "Zhu", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNN) and relation-modulated neural nets (RMNN), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multi-relational knowledge bases and commonsense reasoning. Experimental results on several popular datasets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples. To further prove the effectiveness of the proposed models, in this work, we have applied NAMs to solving challenging Winograd Schema (WS) problems. Experiments conducted on a set of WS problems prove that the proposed models have the potential for commonsense reasoning. Introduction Reasoning is an important topic in artificial intelligence (AI), which has attracted considerable attention and research effort in the past few decades (McCarthy 1986; Minsky 1988; Mueller 2014). Besides the traditional logic reasoning, probabilistic reasoning has been studied as another typical genre in order to handle knowledge uncertainty in reasoning based on probability theory (Pearl 1988; Neapolitan 2012). The probabilistic reasoning can be used to predict conditional probability Pr(E2|E1) of one event E2 given another event E1. State-of-the-art methods for probabilistic reasoning include Bayesian Networks (Jensen 1996), Markov Logic Networks (Richardson and Domingos 2006) and other graphical models (Koller and Friedman 2009). Taking Bayesian networks as an example, the conditional Copyright 2015-2016. probabilities between two associated events are calculated as posterior probabilities according to Bayes theorem, with all possible events being modeled by a pre-defined graph structure. However, these methods quickly become intractable for most practical tasks where the number of all possible events is usually very large. In recent years, distributed representations that map discrete language units into continuous vector space have gained significant popularity along with the development of neural networks (Bengio et al. 2003; Collobert et al. 2011; Mikolov et al. 2013). The main benefit of embedding in continuous space is its smoothness property, which helps to capture the semantic relatedness between discrete events, potentially generalizable to unseen events. Similar ideas, such as knowledge graph embedding, have been proposed to represent knowledge bases (KB) in low-dimensional continuous space (Bordes et al. 2013; Socher et al. 2013; Wang et al. 2014; Nickel et al. 2015). Using the smoothed KB representation, it is possible to reason over the relations among various entities. However, human-like reasoning remains as an extremely challenging problem partially because it requires the effective encoding of world knowledge using powerful models. Most of the existing KBs are quite sparse and even recently created large-scale KBs, such as YAGO, NELL and Freebase, can only capture a fraction of world knowledge. In order to take advantage of these sparse knowledge bases, the state-of-the-art approaches for knowledge graph embedding usually adopt simple linear models, such as RESCAL (Nickel, Tresp, and Kriegel 2012), TransE (Bordes et al. 2013) and Neural Tensor Networks (Socher et al. 2013; Bowman 2013). Although deep learning techniques achieve great progresses in many domains, e.g. speech and image (LeCun, Bengio, and Hinton 2015), the progress in commonsense reasoning seems to be slow. In this paper, we propose to use deep neural networks, called neural association model (NAM), for commonsense reasoning. Different from the existing linear models, the proposed NAM model uses multilayer nonlinear activations in deep neural nets to model the association conditional probabilities between any two possible events. In the proposed NAM framework, all symbolic events are represented in low-dimensional continuous space and there is no need to explicitly specify any dependency structure among events as required in Bayesian networks. ar X iv :1 60 3. 07 70 4v 2 [ cs .A I] 3 A ug 2 01 6 Deep neural networks are used to model the association between any two events, taking one event as input to compute a conditional probability of another event. The computed conditional probability for association may be generalized to model various reasoning problems, such as entailment inference, relational learning, causation modelling and so on. In this work, we study two model structures for NAM. The first model is a standard deep neural networks (DNN) and the second model uses a special structure called relation modulated neural nets (RMNN). Experiments on several probabilistic reasoning tasks, including recognizing textual entailment, triple classification in multi-relational KBs and commonsense reasoning, have demonstrated that both DNNs and RMNNs can outperform other conventional methods. Moreover, the RMNN model is shown to be effective in knowledge transfer learning, where a pre-trained model can be quickly extended to a new relation after observing only a few training samples. Furthermore, we also apply the proposed NAM models to more challenging commonsense reasoning problems, i.e., the recently proposed Winograd Schemas (WS) (Levesque, Davis, and Morgenstern 2011). The WS problems has been viewed as an alternative to the Turing Test (Turing 1950). To support the model training for NAM, we propose a straightforward method to collect associated cause-effect pairs from large unstructured texts. The pair extraction procedure starts from constructing a vocabulary with thousands of common verbs and adjectives. Based on the extracted pairs, this paper extends the NAM models to solve the Winograd Schema problems and achieves a 61% accuracy on a set of causeeffect examples. Undoubtedly, to realize commonsense reasoning, there is still much work be done and many problems to be solved. Detailed discussions would be given at the end of this paper. Motivation: Association between Events This paper aims to model the association relationships between events using neural network methods. To make clear our main work, we will first describe the characteristics of events and all the possible association relationships between events. Based on the analysis of event association, we present the motivation for the proposed neural association models. In commonsense reasoning, the main characteristics of events are the following: \u2022 Massive: In most natural situations, the number of events is massive, which means that the association space we will model is very large. \u2022 Sparse: All the events occur in our dialy life are very sparse. It is a very challenging task to ideally capture the similarities between all those different events. At the same time, association between events appears everywhere. Consider a single event play basketball for example, shown in Figure 1. This single event would associate with many other events. A person who plays basketball would win a game. Meanwhile, he would be injured in some cases. The person could make money by playing basketball as well. Moreover, we know that a person who plays basketball should be coached during a regular game. Those are all typical associations between events. However, we need to recognize that the task of modeling event association is not identical to performing classification. In classification, we typically map an event from its feature space into one of pre-defined finite categories or classes. In event association, we need to compute the association probability between two arbitrary events, each of which may be a sample from a possibly infinite set. The mapping relationships in event association would be many-to-many; e.g., not only playing basketball could support us to make money, someone who makes stock trading could make money as well. More specifically, the association relationships between events include causeeffect, spatial, temporal and so on. This paper treats them as a general relation considering the sparseness of useful KBs.", "creator": "LaTeX with hyperref package"}}}