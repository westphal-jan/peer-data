{"id": "1603.07810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Conditional Similarity Networks", "abstract": "In typical perceptual tasks, higher-order concepts are inferred from visual features to assist with perceptual decision making. However, there is a multitude of visual concepts which can be inferred from a single stimulus. When learning nonlinear embeddings with siamese or triplet networks from similarities, we typically assume they are sourced from a single visual concept. In this paper, we are concerned with the hypothesis that it can be potentially harmful to ignore the heterogeneity of concepts affiliated with observed similarities when learning these embedding networks. We demonstrate empirically that this hypothesis holds and suggest an approach that deals with these shortcomings, by combining multiple notions of similarities in one compact system. We propose Multi-Query Networks (MQNs) that leverage recent advances in representation learning on factorized triplet embeddings in combination with Convolutional Networks in order to learn embeddings differentiated into semantically distinct subspaces, which are learned with a latent space attention mechanism. We show that the resulting model learns visually relevant semantic subspaces with features that do not only outperform single triplet networks, but even sets of concept specific networks.", "histories": [["v1", "Fri, 25 Mar 2016 02:52:02 GMT  (1639kb,D)", "http://arxiv.org/abs/1603.07810v1", null], ["v2", "Mon, 19 Dec 2016 12:41:01 GMT  (2061kb,D)", "http://arxiv.org/abs/1603.07810v2", null], ["v3", "Mon, 10 Apr 2017 15:18:21 GMT  (2079kb,D)", "http://arxiv.org/abs/1603.07810v3", "CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["andreas veit", "serge belongie", "theofanis karaletsos"], "accepted": false, "id": "1603.07810"}, "pdf": {"name": "1603.07810.pdf", "metadata": {"source": "CRF", "title": "Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet Networks", "authors": ["Andreas Veit", "Serge Belongie", "Theofanis Karaletsos"], "emails": ["sjb}@cs.cornell.edu,", "karaletsos@cbio.mskcc.org"], "sections": [{"heading": null, "text": "Keywords: similarity-based learning, Triplet Networks, deep learning, representation learning"}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Related Work", "text": "In fact, most of them are a kind of self-realization that is able to put oneself at the center."}, {"heading": "3 Multi-Query Triplet Networks", "text": "Our goal is to learn a nonlinear feature of nonlinear function."}, {"heading": "4 Results", "text": "In this section we present the data sets used and present qualitative and quantitative evaluations of our work. We generate treble constraints based on data sets with annotated attributes. We learn fine-grained embedding using MQNs and strong competing baselines. We focus on evaluating the quality of embedding and learned folding filters, both separately and collectively. We show experiments that highlight the semantic structure of embedding and in particular the sub-spaces, the ability to predict invisible trebles and fine-grained classification performance outside the task. We describe our experimental setup in more detail in the supplementary material."}, {"heading": "4.1 Datasets", "text": "We are conducting experiments with two different datasets. First, we are looking at a dataset of fonts1 collected by Bernhardsson. The dataset contains 3.1 million images of 1 http: / / erikbern.com / 2016 / 01 / 21 / analyzing-50k-fonts-using-deep-neuronal-networks / single characters in gray scale with a size of 64 x 64 pixels. The dataset shows variations by font and character type, e.g. \"a,\" b. \"Specifically, it contains 62 types of characters and 50,000 fonts. For our experiments, we are using only the first 1,000 fonts. We are also looking at the Zappos50k dataset [22] collected by Yu and Grauman. The dataset contains 50,000 images of single, richly annotated shoes with several complex variations. The images are iconic and have a size of 136 x 102 pixels, which we are reducing to 112 x."}, {"heading": "4.2 Visual Exploration of the Learned Subspaces", "text": "Figure 3 shows the embedding of the two sub-spaces in the fonts data set, which we project into two dimensions using t-SNE [23]. On the left side we show the embedding according to the font type and on the right side the embedding of the four sub-spaces, which were learned with an MQN on the data set Zappos50k. Figure 4 (a) shows the embedding, which corresponds to the locking mechanism of the shoes. Figure 4 (b) shows the embedding of the four sub-spaces, which were learned with an MQN on the data set Zappos50k. The embedding of the different shoe types clearly separates the different shoe types into boots, slippers, etc. Highlighted areas reveal some interesting details. Figure 4 (b) shows the embedding according to the shoe type. The embedding of the different shoe types clearly separates the different shoe types into boots, slippers and so on."}, {"heading": "4.3 Qualitative Analysis Of Subspaces", "text": "As seen in Section 3, the driving force behind this separation is both the presence of triple information on multiple concepts and the mask mechanism, which allows the model to learn from all of these concepts together. We visualize the masks for our common model decisions in Figure 6. We show the traditional triple loss, a manually defined mask that splits the embedding into fragmented features and a fully learned mask, showing that the model itself unravels masks into private and some common features for each query. This speaks to the mechanism as an exploratory tool, since it reveals the structure of common spaces. Unsurprisingly, features are not as strongly divided as a fully shared space and not as fragmented as a completely splintered space."}, {"heading": "4.4 Quantitative Analysis Using Triplet Prediction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "4.5 Quantitative Analysis Of Feature Extractors Using Off-Task Classification", "text": "We evaluate the performance of MQNs and baselines on an off-task fine-grained classification setting to test their generic value as components in other vision tasks. In particular, we perform brand classification on the Zappos dataset, which is a task we do not collect triplets for. Intuitively, if triplet knowledge transfer is useful, we would expect triplet models to solve uncertainties that arise in classification with less needed label data, much like semi-monitored learning. However, features that trade-off discriminatory features for similarity in embedding generally for fine-grained descriptive features that could compromise classification, SQN and MQN-fm."}, {"heading": "5 Conclusion", "text": "In this work, we propose Multi-Query Networks, a common non-linear architecture for collaborative triple learning of convolutionary characteristics and embedding with multiple triple queries. We demonstrated the benefits of MQNs in the context of on-task and off-task predictions. They exceed dedicated task-specific networks and basic triple networks in predicting invisible triple-step settings. In the classification settings, they also indicate that they are able to perform knowledge transfer and imagenetric characteristics in low-label-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings-settings"}, {"heading": "Acknowledgements", "text": "We would like to thank Michael Wilber and Sam Kwak for their feedback on the manuscript. This work is partly funded by the AOL Program for Connected Experiences."}, {"heading": "6 Supplementary Material", "text": "The supplementary material contains a detailed description of the experimental setup."}, {"heading": "6.1 Experimental Setup", "text": "We use different feature extractors for the two datasets. For the font datasets, we use a variant of the VGG architecture [24] with 9 levels of 3 out of 3 convolutions and two fully connected levels. As feature extractor for the Zappos datasets, we select the deep residual network architecture [25] for its strong empirical performance. In particular, we use two masks for the font datasets and four masks for the Imagenet [26] and reduced by a residual module to fit the smaller image size of 112x112. For all our experiments, we set an embedding dimension to 128. We use two masks for the font datasets and four masks corresponding to the number of concept-specific queries. 2 https: / github.com / facebook / fb.resnet.Setup of Masking modules: Training consists of learning parameters."}], "references": [{"title": "Bayesian representation learning with oracle constraints", "author": ["T. Karaletsos", "S. Belongie", "G. R\u00e4tsch"], "venue": "International Conference on Learning Representations (ICLR), San Juan, PR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic triplet embedding", "author": ["L. Van Der Maaten", "K. Weinberger"], "venue": "Machine Learning for Signal Processing (MLSP), 2012 IEEE International Workshop on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptively learning the crowd kernel", "author": ["O. Tamuz", "C. Liu", "O. Shamir", "A. Kalai", "S.J. Belongie"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiview triplet embedding: Learning attributes in multiple maps", "author": ["E. Amid", "A. Ukkonen"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Machine learning 87(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning concept embeddings with combined human-machine expertise", "author": ["M. Wilber", "I.S. Kwak", "D. Kriegman", "S. Belongie"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "The Journal of Machine Learning Research 11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201905). Volume 1., IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201906). Volume 2., IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Wang", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201914).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201915).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["S. Reed", "K. Sohn", "Y. Zhang", "H. Lee"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view convolutional neural networks for 3d shape recognition", "author": ["H. Su", "S. Maji", "E. Kalogerakis", "E. Learned-Miller"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML-04), ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multi-modal similarity", "author": ["B. McFee", "G. Lanckriet"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25st International Conference on Machine Learning (ICML-08), ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["T.Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Similarity metrics for categorization: from monolithic to category specific", "author": ["B. Babenko", "S. Branson", "S. Belongie"], "venue": "International Conference on Computer Vision (ICCV \u201909), IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-Grained Visual Comparisons with Local Learning", "author": ["A. Yu", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201914).", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research 9(2579-2605)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201909), IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We leverage recent results in representation learning [1] to port the idea of factorized triplet embeddings to convolutional triplet networks and explore applications to computer vision tasks.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Disconnected from the input image, triplet based similarity embeddings, can be learned using crowdkernels [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "[3] introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Similar work has been generalized to multiple-views and clustering settings by Amid and Ukkonen [4] as well as Van der Maaten and Hinton [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "Similar work has been generalized to multiple-views and clustering settings by Amid and Ukkonen [4] as well as Van der Maaten and Hinton [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "[6], but this work did not include joint feature and embedding learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "An early approach to connect input features with embeddings has been to learn image similarity functions through ranking [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "A foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks [8,9], which use pairwise distances to learn embeddings discriminatively.", "startOffset": 154, "endOffset": 159}, {"referenceID": 8, "context": "A foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks [8,9], which use pairwise distances to learn embeddings discriminatively.", "startOffset": 154, "endOffset": 159}, {"referenceID": 9, "context": "[10] and Schroff et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], where use of supervised features with crowd-inferred similarities boosts performance in face verification and fine-grained visual categorization tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] on representation learning which introduces a joint generative model over inputs and triplets to learn a factorized latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Multi-view learning [13,14] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "Multi-view learning [13,14] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces.", "startOffset": 20, "endOffset": 27}, {"referenceID": 14, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 25, "endOffset": 32}, {"referenceID": 15, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 25, "endOffset": 32}, {"referenceID": 16, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "Multi-task learning approaches [18] are used when information from disparate sources or using differing assumptions can be combined beneficially for a final prediction task.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "We consider disentangled latent spaces, which is also considered in multilinear network architectures [19] focusing on factorizations of feature spaces in neural networks, but differ in many details.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "An interesting link also exists to multiple similarity learning [20], where category specific similarities are used to approximate a fine-grained global embedding.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "This term may be confused with spatial attention [21], but bears similarity insofar as it shows that the ability to gate the focus of the model on relevant dimensions (in our case in latent space rather than observed space) is beneficial both to the semantics and to the quantitative performance of our model.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "We also consider the Zappos50k shoe dataset [22] collected by Yu and Grauman.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "Figure 3 shows embeddings of the two subspaces in the Fonts dataset, which we project down to two dimensions using t-SNE [23].", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "In typical perceptual tasks, higher-order concepts are inferred from visual features to assist with perceptual decision making. However, there is a multitude of visual concepts which can be inferred from a single stimulus. When learning nonlinear embeddings with siamese or triplet networks from similarities, we typically assume they are sourced from a single visual concept. In this paper, we are concerned with the hypothesis that it can be potentially harmful to ignore the heterogeneity of concepts affiliated with observed similarities when learning these embedding networks. We demonstrate empirically that this hypothesis holds and suggest an approach that deals with these shortcomings, by combining multiple notions of similarities in one compact system. We propose Multi-Query Networks (MQNs) that leverage recent advances in representation learning on factorized triplet embeddings in combination with Convolutional Networks in order to learn embeddings differentiated into semantically distinct subspaces, which are learned with a latent space attention mechanism. We show that the resulting model learns visually relevant semantic subspaces with features that do not only outperform single triplet networks, but even sets of concept specific networks.", "creator": "LaTeX with hyperref package"}}}