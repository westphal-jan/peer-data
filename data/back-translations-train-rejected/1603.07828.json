{"id": "1603.07828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation Kernels and Multiside Similarities", "abstract": "This study presents an efficient approach for incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a test instance and a training sample relies on not only their distance but also the relation between this test sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation. Furthermore, the proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability. Experiments on two databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrate the effectiveness of the proposed idea.", "histories": [["v1", "Fri, 25 Mar 2016 06:04:30 GMT  (1174kb)", "http://arxiv.org/abs/1603.07828v1", "8 pages, 11 figures"], ["v2", "Mon, 21 Nov 2016 14:20:34 GMT  (605kb)", "http://arxiv.org/abs/1603.07828v2", "Incomplete data analysis, partial similarity, multiside similarity, privacy preservation, kernel ridge regression (KRR), missing values, data imputation, kernel method, cloud computing, data analytics"]], "COMMENTS": "8 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["bo-wei chen"], "accepted": false, "id": "1603.07828"}, "pdf": {"name": "1603.07828.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dennisbwc@gmail.com;", "bo-wei.chen@monash.edu)."], "sections": [{"heading": null, "text": "The incomplete data classification, in which the entries of the samples are missing or masked because they reflect the problem of maintaining privacy. In order to deal with this incomplete data, a new kernel function with asymmetric intrinsic mappings is necessary in this study, not only their removal, but also the relationship between this test sample and the central distribution of the class in which the training sample is one of the typical methods in which only one training sample is used for the nuclear information. Furthermore, the proposed kernel is able to import data using class-dependent averages, which are enhances Fisher Discriminant Ratios and data discrimination. Experiments on two databases were conducted to evaluate the proposed method, and the result indicated that the accuracy of the proposed method is higher than that of the baseline."}, {"heading": "II. ASYMMETRIC IMPUTATION KERNEL", "text": "This study followed the method \u2012 Kernel Approach to Incomplete Data Analysis (KAIDA) \u0445 [10] with several enhanced modifications for data imputation.The original imputation [10] is fulfilled by zero-padding a vector that contains missing values. Let B represent a mask that takes imputation. Then, 1 if is given0 else x x x B (1) and x x x x B (2) when is no-th dimension of a vector, and is the element-wise operator.However, zero-padding results in a decrease of Partial Fisher Discriminant-Ratios, showed as follows x x x x x x. Let F denote the Partial Fisher Discriminant-Ratio. Thus, 22 2 F) where is the partial mean and the partial variance of the bandwidth-th dimension."}, {"heading": "III. KERNEL RIDGE REGRESSION", "text": "Kernel Ridge regression covers linear regression techniques in which a ridge parameter is imposed on the objective function to regulate a model [23]. KRR has two types of operating modes, one is intrinsic space and the other is empirical space.When functions for mapping features produce vector data, the calculation of intrinsic space yields advantageous complexity when the number of data N is much greater than the dimension M. Otherwise, empirical space operations should be used.Intrinsic SpaceLet {(xi, yi) | i = 1,..., N} denotes a pair of M-dimensional characteristic vectors xi and the associated designation yi, specifying i the indices of the N-training samples. The goal of a linear regressor is to minimize the following cost function of the least square errors (LSEs)."}, {"heading": "B. Empirical Space", "text": "According to the Learning Subspace Property in [23] the weight vector u has the following relation to \u0424 and an unknown N-dimensional vector a.u \u03a6a. (19) The combination of (16) and (19) results in 2 TKRR,.E b a Ka e y a Ka (20) The order of the equations according to (20) in relation to a and b results in 1 b a K I y e (21) and 1T1T b y K I e K I e. (22)"}, {"heading": "IV. EXPERIMENTAL RESULT", "text": "The ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref) for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the"}, {"heading": "V. CONCLUSION", "text": "This paper presents a novel learning method for processing large-scale data with missing values. In order to effectively restore the discriminability of the data, this study proposes a nuclear-based data imputation where imputation focuses on improving the similarity between test and training samples. The proposed method uses teacher information during the training phase, when missing entries of the centroids are filled in class-dependent substitution values. Subsequently, a training sample is combined with its class center, and this generates accordingly substituted values for training patterns. The equation shows that such imputation does not reduce Fisher Discriminant ratios, which indicate the discrepancy between two distributions. The proposed method can increase the discrepancy of training data by reducing the variance. In relation to SNRs, noise is minimized, although a test sample still uses masks to compensate the missing entries."}], "references": [{"title": "Geetter, \u2015Privacy and Big Data,", "author": ["B.M. Gaff", "H.E. Sussman"], "venue": "IEEE Computer Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Multiple Imputation for Nonresponse in Surveys", "author": ["D.B. Rubin"], "venue": "New York, NY: Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1987}, {"title": "Missing value estimation methods for DNA microarrays,", "author": ["O. Troyanskaya", "M. Cantor", "G. Sherlock", "P. Brown", "T. Hastie", "R. Tibshirani", "D. Botstein", "R.B. Altman"], "venue": "Bioinformatics, vol. 17,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Multiple imputation for missing data: Concepts and new development,", "author": ["Y.C. Yuan"], "venue": "SAS Institute Incorporation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Analysis of Incomplete Multivariate Data", "author": ["J.L. Schafer"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Scaling out big data missing value imputations", "author": ["C. Anagnostopoulos", "P. Triantafillou"], "venue": "Pythia vs. Godzilla,\u2016 in Proc. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Kernel approach to incomplete data analysis (KAIDA),", "author": ["S.-Y. Kung", "P.-Y. Wu"], "venue": "Proc. 1st International Conference on Advances in Big Data Analytics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "PSVM: Parallelizing support vector machines on distributed computers,", "author": ["E.Y. Chang", "K. Zhu", "H. Wang", "H. Bai", "J. Li", "Z. Qiu", "H. Cui"], "venue": "in Proc. 21st Annual Conf. Neural Information Processing System (NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Clustered support vector machines,", "author": ["Q. Gu", "J. Han"], "venue": "Proc. 16th Int. Conf. Artificial Intelligence and Statistics (AISTATS), Scottsdale, Arizona, United States,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Divide and conquer kernel ridge regression,", "author": ["Y. Zhang", "J. Duchi", "M. Wainwright"], "venue": "Proc. Conference on Learning Theory (Colt", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A divide-and-conquer solver for kernel support vector machines,", "author": ["C.-J. Hsieh", "S. Si", "I.S. Dhillon"], "venue": "in Proc. 31st International Conference on Machine Learning (ICML", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Incremental support vector machine learning: A local approach,", "author": ["L. Ralaivola", "F. d'Alch\u00e9-Buc"], "venue": "Proc. International Conference on Artificial Neural Networks (ICANN", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Proximal support vector machine classifiers,", "author": ["G. Fung", "O.L. Mangasarian"], "venue": "Proc. 7th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Incremental and decremental support vector machine learning,", "author": ["G. Cauwenberghs", "T. Poggio"], "venue": "Proc. 14th Annual Conf. Neural Information Processing System (NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "SVM incremental learning, adaptation and optimization,", "author": ["C.P. Diehl", "G. Cauwenberghs"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Incremental support vector learning: Analysis, implementation and applications,", "author": ["P. Laskov", "C. Gehl", "S. Kr\u00fcger", "K.-R. M\u00fcller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Multiple incremental decremental learning of support vector machines,", "author": ["M. Karasuyama", "I. Takeuchi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Incremental training of support vector machines,", "author": ["A. Shilton", "M. Palaniswami", "D. Ralph", "A.C. Tsoi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Kernel Methods and Machine Learning", "author": ["S.-Y. Kung"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Sensitive personal data, such as health records, faces, and names, are intentionally removed from the original data to avoiding being maliciously manipulated [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "Actually, as far back as the earlier 1970s [2], this problem already aroused much attention from scientists when they dealt with nonresponses in surveys.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": ", interpolation), subspace-based reconstruction [4, 5], and fixed-value replacement (e.", "startOffset": 48, "endOffset": 54}, {"referenceID": 3, "context": "Multiple imputation is performed by randomly taking a value from a set of stochastic numbers and then filling in the missing dataset L times [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Nevertheless, to approach real distribution, Monte Carlo methods [7] or Markov Chain Monte Carlo (MCMC) were frequently adopted to simulate theoretical scenarios by using finite sampling.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "[4] examined data imputation by K-Nearest Neighbors (KNNs) and Singular Value Decomposition (SVD).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Anagnostopoulos and Triantafillou [9] investigated the scalability of imputation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Kung and Wu [10] focused on kernelized methods and devised a zero-padding method for single imputation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 8, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 9, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 10, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 11, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 12, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 13, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 14, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 15, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 16, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 17, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "This study followed the method \u2015Kernel Approach to Incomplete Data Analysis (KAIDA)\u2016 [10] with several enhanced modifications for data imputation.", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "The original imputation [10] is fulfilled by zero-padding a vector that contains missing values.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "In terms of Signal-to-Noise Ratios (SNRs) [23], the denominator, i.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "As class information is conducive to the training phase, which was ignored in [10], this study enhances the mechanism in [10] by integrating class information into kernels to benefit incomplete-data training.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "As class information is conducive to the training phase, which was ignored in [10], this study enhances the mechanism in [10] by integrating class information into kernels to benefit incomplete-data training.", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "Thus, the Masked Partial-Cosine (MPC) function mentioned in [10] was used for nonvectorial similarities.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Kernel Ridge Regression extends linear regression techniques, in which a ridge parameter is imposed on the objective function to regularize a model [23].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Empirical Space According to the Learning Subspace Property in [23], the weight vector u has the following relation with \u0424 and an unknown N-dimensional vector a.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Two approaches \u2014 KAIDA [10] and our proposed method \u2014 were employed for assessment.", "startOffset": 23, "endOffset": 27}], "year": 2016, "abstractText": "This study presents an efficient approach for incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a test instance and a training sample relies on not only their distance but also the relation between this test sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation. Furthermore, the proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability. Experiments on two databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrate the effectiveness of the proposed idea.", "creator": "Microsoft\u00ae Word 2010"}}}