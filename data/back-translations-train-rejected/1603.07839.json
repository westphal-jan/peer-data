{"id": "1603.07839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video", "abstract": "This paper proposes an end-to-end convolutional selective autoencoder approach for early detection of combustion instabilities using rapidly arriving flame image frames. The instabilities arising in combustion processes cause significant deterioration and safety issues in various human-engineered systems such as land and air based gas turbine engines. These properties are described as self-sustaining, large amplitude pressure oscillations and show varying spatial scales periodic coherent vortex structure shedding. However, such instability is extremely difficult to detect before a combustion process becomes completely unstable due to its sudden (bifurcation-type) nature. In this context, an autoencoder is trained to selectively mask stable flame and allow unstable flame image frames. In that process, the model learns to identify and extract rich descriptive and explanatory flame shape features. With such a training scheme, the selective autoencoder is shown to be able to detect subtle instability features as a combustion process makes transition from stable to unstable region. As a consequence, the deep learning tool-chain can perform as an early detection framework for combustion instabilities that will have a transformative impact on the safety and performance of modern engines.", "histories": [["v1", "Fri, 25 Mar 2016 08:02:41 GMT  (734kb,D)", "http://arxiv.org/abs/1603.07839v1", "A 10 pages, 10 figures submission for Applied Data Science Track of KDD16"]], "COMMENTS": "A 10 pages, 10 figures submission for Applied Data Science Track of KDD16", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["adedotun akintayo", "kin gwn lore", "soumalya sarkar", "soumik sarkar"], "accepted": false, "id": "1603.07839"}, "pdf": {"name": "1603.07839.pdf", "metadata": {"source": "CRF", "title": "Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video", "authors": ["Adedotun Akintayo", "Kin Gwn Lore", "Soumalya Sarkar", "Soumik Sarkar"], "emails": ["akintayo@iastate.edu", "kglore@iastate.edu", "sms388@gmail.com", "soumiks@iastate.edu"], "sections": [{"heading": null, "text": "Tags Deep Convolutonal Networks; Selective Autoencoder; Instability of Combustion; Implicit Labeling"}, {"heading": "1. INTRODUCTION", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2. BACKGROUND", "text": "This section provides a brief overview of revolutionary networks, a description of the example problem of detecting combustion instability, and the notion of implicit labeling."}, {"heading": "2.1 Convolutional networks", "text": "Convolutional networks [18] are a type of deep networks that offer discriminatory advantages such as in the MEMM, as well as global relationships between observations such as in the CRF. Each unit of characteristic maps has common weights or cores for efficient training in relatively - compared to fully connected layers - smaller, trainable parameters that are added to an additive bias on which to squeeze. Feature extraction and classification learning are the two main functions of these networks [22]. However, to learn the most meaningful characteristics, we need to determine the invariance-rich codes embedded in the raw data, and then follow with a fully connected layer to further reduce the dimensionality of the data and map the most important codes to a low dimension of the examples."}, {"heading": "2.2 The problem of combustion instability", "text": "Combustion instability reduces the efficiency and longevity of aircraft gas turbine engines. It is considered a significant anomaly characterized by high-amplitude flame oscillations at discrete frequencies. These frequencies typically represent the natural acoustic modes of the burner. Combustion instability arises from a positive coupling between vibrations of heat release and pressure oscillations. Coherent structures are flowing mechanical structures associated with a coherent phase of vortex [16]. These structures, whose generating mechanisms vary systematically, cause large-scale oscillations of velocity and general oscillations of the flame shape through curvature and strain. These structures can be caused by separation / generation at the acoustic modes of the ducts when the forced (pressure) amplitudes are high. There is a lot of recent research into the detection and dispersion of these heat and pressure structures."}, {"heading": "2.3 Implicit labeling", "text": "A variant of structured identification by [19], implicit identification, is used to derive soft identifiers from extreme classes that are explicitly identified as positive or negative examples. Explicit identifiers can usually be used to selectively mask a characteristic, in particular that one is not interested in analyzing the class in question. However, explicit identifiers alone can only serve as a classifier for intrinsic classes in the test kits learned from the training set. Implicit identifiers are similar to sequence identifiers here [10] with an additional limitation on the use of prior knowledge provided only by explicit identification. It will then merge with a revolutionary auto-encoder architecture algorithm described in Section 3 to determine the intermediate or transitional periods - a mixed breed of dogs and wolves, for example - and more importantly, to what extent the animal is derived from a wolf, or the architecture of a dog."}, {"heading": "3. CONVOLUTIONAL SELECTIVE AUTOENCODER", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.1 Instability measure", "text": "The similarity index selected for instability measurement is the correlation ratio given in [13] and mathematically proven by [23], which has low calculation requirements in order to reduce the already large calculation architecture. Also useful was its ability to quantify the relationship between two picture frames of different intensity and not to include the actual images directly in the calculation, hence its choice as our performance metric. Assuming that the input image X pixels x x x x x x x has the intensities xi (1, 255) for i = 1, 2,... and the inferred output images designated as Y, the correlation ratio is calculated by calculating the total and conditional deviations \u03c3 and \u03c3i in Y: \u03c3i = 1Zi xi (Y [xi]) 2 \u2212 (1 Zi, Y [xi])) 2 \u2212 (1 Zi, Y [xi]))) 2 (10) and the derived images as follows."}, {"heading": "4. DATASET AND IMPLEMENTATION", "text": "This year, it has come to the point that there will only be one such event until such an event occurs."}, {"heading": "6. CONCLUSIONS AND FUTURE WORKS", "text": "The framework is designed for early detection of combustion instabilities using high-speed flame video; validation results are presented using data from a fluid-stabilized fuel rod on a laboratory scale; the results are discussed in light of a highly accurate similarity metric used to measure proximity to ground, truth-unstable flames; the same metric has been used to expand the architecture to address the issue of implicit neighborhood diagram labeling; the framework can be generalized for high-dimensional data to perform soft labeling by interpolating explicit labels; while the framework is proving to be an efficient diagnostic technique for combustion processes in laboratory experiments, large-scale validation is underway to demonstrate its wide-ranging applicability; and the framework also allows domain experts to learn more about the related structures that occur during laboratory combustion processes."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "The authors sincerely acknowledge the extensive data collection conducted by Vikram Ramanan and Dr. Satyanarayanan Chakravarthy at the Indian Institute of Technology Madras (IITM) in Chennai. The authors also thank the support of NVIDIA Corporation by donating the GeForce GTX TITAN Black GPU used for this research."}, {"heading": "8. REFERENCES", "text": "[1] M. Aharon, M. Elad, and A. Bruckstein. An algorithmfor designing overcomplete dictionaries for spare representation. IEEE Transactions on Signal Processing, 54 (11): 4311-4322, 2006. [2] Y. Bengio. Learning deep architecture for ai. Foundations and Trends in Machine Learning, pp. 1-71, 2008. [3] A. Bergner, S. A. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. Association for Computational Linguistics, 22 (1): 1 - 36, 1996. [4] J. Bergstra, O. Breulex, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a cpu and gpu math expression compiler."}], "references": [{"title": "An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architecture for ai", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.A.D. Pietra", "V.J.D. Pietra"], "venue": "Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breulex", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the 0.4231", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "The proper orthogonal decomposition in the analysis of turbulent flows", "author": ["G. Berkooz", "P. Holmes", "J.L. Lumley"], "venue": "Annual Review of Fluid Mechanics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Convolutional neural networks applied to human face classification", "author": ["B. Cheung"], "venue": "ICMLA,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A unified architecture for natural language processing:deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "25th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A tutorial on sequence labeling", "author": ["H. Erdogan"], "venue": "ICMLA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "exdb, pages", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Using multiple graphics cards as  a general purpose parallel computer: Applications to compute vision", "author": ["J. Fung", "S. Mann"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Similarity and Dissimilarity Measures, chapter 2, pages 1 \u2013 66. Number 978-1-4471-2457-3", "author": ["A.A. Goshtasby"], "venue": "Springer-Verlag London Limited,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850v5 [cs.NE], pages 1 ", "author": ["A. Graves"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures. IJCNN, pages 1 ", "author": ["A. Graves", "J. Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Coherent structures - reality and myth", "author": ["A.K.M.F. Hussain"], "venue": "Physics of Fluids,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1983}, {"title": "Convolutional autoencoders in python/theano/lasagne", "author": ["S. Jones"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Strudtured labeling to facilitate concept evolution in machine learning", "author": ["T. Kulesza", "S. Amershi", "R. Caruana", "D. Fisher", "D. Charles"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Convolutional networks for Images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "Speech and Time-Series,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc of IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Sample-based estimation of correlation ratio with polynomial approximation", "author": ["D. Lewandowski", "R.M. Cooke", "R.J.D. Tebbens"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction, pages 52\u201359. Number 6791", "author": ["J. Masci", "U. Meier", "D. Ciresan", "J. Schmidhuber"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Hmm and part of speech tagging", "author": ["A. Meyer"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "A tutorial on hidden markov models and selected applications in speech proccessing", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1989}, {"title": "Early detection of combustion instability by neural-symbolic analysis on hi-speed video. In Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches (CoCo", "author": ["S. Sarkar", "K.G. Lore"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Early detection of combustion instability from hi-speed flame images via deep learning and symbolic time series analysis", "author": ["S. Sarkar", "K.G. Lore", "V. Ramaman", "S.R. Chakravarthy", "S. Phoha", "A. Ray"], "venue": "Annual Conference of the Prognostics and Health Management Management Society,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. Muller", "S. Behnke"], "venue": "Intenational Conference on Artificial Neural Networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Dynamic mode decomposition of numerical and experimental data", "author": ["P.J. Schmid"], "venue": "Journal of Fluid Mechanics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "arXiv:1509.08967v1 [cs.CL],", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "International Conference on Machine Learning, JMLR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman"], "venue": "Science, 331:1279\u20131285,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Lasagne for python newbies, February 2016", "author": ["M. Thoma"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Adadelta:an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "For example, convolutional neural network-based applications include Graph Transformer Networks (GTN) for rapid, online recognition of handwriting [22], natural language processing [8], large vocabulary continuous speech recognition [31] and avatar CAPTCHA machine image recognition [7] by training machines to distinguish between human faces and computer generated faces.", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "For example, convolutional neural network-based applications include Graph Transformer Networks (GTN) for rapid, online recognition of handwriting [22], natural language processing [8], large vocabulary continuous speech recognition [31] and avatar CAPTCHA machine image recognition [7] by training machines to distinguish between human faces and computer generated faces.", "startOffset": 181, "endOffset": 184}, {"referenceID": 30, "context": "For example, convolutional neural network-based applications include Graph Transformer Networks (GTN) for rapid, online recognition of handwriting [22], natural language processing [8], large vocabulary continuous speech recognition [31] and avatar CAPTCHA machine image recognition [7] by training machines to distinguish between human faces and computer generated faces.", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "For example, convolutional neural network-based applications include Graph Transformer Networks (GTN) for rapid, online recognition of handwriting [22], natural language processing [8], large vocabulary continuous speech recognition [31] and avatar CAPTCHA machine image recognition [7] by training machines to distinguish between human faces and computer generated faces.", "startOffset": 283, "endOffset": 286}, {"referenceID": 9, "context": ", structured and implicit) can be considered a multi-class classification problem [10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "For example, three stage Hidden Markov Models (HMM) were used for handling speech recognition [26] problems, parts of speech tagging [25] and sequence labeling because they derive the relationships from observations to state and state to state in dynamic systems.", "startOffset": 94, "endOffset": 98}, {"referenceID": 24, "context": "For example, three stage Hidden Markov Models (HMM) were used for handling speech recognition [26] problems, parts of speech tagging [25] and sequence labeling because they derive the relationships from observations to state and state to state in dynamic systems.", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "Applications of MEMM for natural language processing can be found in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 19, "context": "Due to \u201dlabel bias\u201d defects of MEMM, a Conditional Random Field (CRF), which is a joint Markov Random Field (MRF) of the states conditioned on the whole observations was later explored by [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 9, "context": "It enabled considering the global labels of the observation as against localization of labels of MEMM [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Recurrent Neural Networks (RNNs) have been utilized for sequence labeling problems due to its cyclic connections of neurons [14] as well as its temporal modeling ability.", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "Although earlier construction of RNNs was known to have short ranged memory issues and a restrictive unidirectional information context access, formulation of a bidirectional Long Short Term Memory (LSTM) [15] resolved such issues.", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "The authors [28] introduced Deep Belief Networks, DBNs as a viable technique to achieve the aim with a view to exploring other machine learning for confirmation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "They improved on that by applying a modular neural-symbolic approach [27] in another publication.", "startOffset": 69, "endOffset": 73}, {"referenceID": 32, "context": "Conceptually, this is similar to cognitive psychologists\u2019 description of human reasoning in object classification [33].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "\u2022 the method avoids extensive expert-guided feature handcrafting [11] while addressing a complex physical phenomenon like combustion to discover coherent structures in the images,", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "Convolutional networks [18] are a type of deep networks that offer discriminative advantages as in the MEMM as well as providing globally relationship between observations as in the CRF.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Feature extraction and classifier learning are the two main functions of these networks [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Many image processing and complex simulations depend on the invariance property of the convolution neural network stated in [21] to prevent overfitting by learning expressive codes.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "The feature maps are able to preserve local neighborhood patterns for each receptive field as with over-complete dictionaries in [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 21, "context": "A more detailed review may be found in [22] where the authors note the advantage of local correlation enforcing convolution before spatio-temporal recognition.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "For efficient learning purposes, convolutional networks are able to utilize distributed mapreduce frameworks [12] as well as GPU computing.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Coherent structures are fluid mechanical structures associated with coherent phase of vorticity [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "The popular methods resorted for detection of coherent structures are proper orthogonal decomposition (POD) [5] (similar to principal component analysis [6]) and dynamic mode decomposition (DMD) [30], which use tools from spectral theory to derive spatial coherent structure modes.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The popular methods resorted for detection of coherent structures are proper orthogonal decomposition (POD) [5] (similar to principal component analysis [6]) and dynamic mode decomposition (DMD) [30], which use tools from spectral theory to derive spatial coherent structure modes.", "startOffset": 153, "endOffset": 156}, {"referenceID": 29, "context": "The popular methods resorted for detection of coherent structures are proper orthogonal decomposition (POD) [5] (similar to principal component analysis [6]) and dynamic mode decomposition (DMD) [30], which use tools from spectral theory to derive spatial coherent structure modes.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "A variant of structured labeling by [19] called implicit labeling is used to derive soft labels from extreme classes that are explicitly labeled as either positive or negative examples.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Implicit labeling here also bears similarity to the sequence labeling [10] with an extra constraint of utilizing prior knowledge provided only by explicit label.", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "3), designed and tested to examine another perspective to the current problem instead of adding a symbolic graphical model such as STSA [27] at the top level.", "startOffset": 136, "endOffset": 140}, {"referenceID": 28, "context": "While other pooling schemes exist [29], we chose the highly activated units to take up the features for the local p\u00d7 p neighborhood in the maxpooling", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Unpooling: In this layer, a reversal of the pooled dimension is done by stretching and widening [17] the identified features from the filters of the previous layer.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "The process included a regularization function as in [22] to avoid overfitting the data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "The Nesterov momentum-based [32] stochastic gradient descent was used for improved results when compared to other loss functions such as adaptive subgradient (ADAGRAD) [9] and adaptive learning rate method (ADADELTA) [35] for the reconstruction error updates given", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "The Nesterov momentum-based [32] stochastic gradient descent was used for improved results when compared to other loss functions such as adaptive subgradient (ADAGRAD) [9] and adaptive learning rate method (ADADELTA) [35] for the reconstruction error updates given", "startOffset": 168, "endOffset": 171}, {"referenceID": 34, "context": "The Nesterov momentum-based [32] stochastic gradient descent was used for improved results when compared to other loss functions such as adaptive subgradient (ADAGRAD) [9] and adaptive learning rate method (ADADELTA) [35] for the reconstruction error updates given", "startOffset": 217, "endOffset": 221}, {"referenceID": 1, "context": "The works in [2] points out that that SGD with early stopping is equivalent to an `2\u2212 regularization.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "Subsequently, the weights are updated for each time step via stochastic gradient descent [22]:", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "More details can be found in [24] while the background materials presented thus far and those in subsection 3.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "The similarity index selected for instability measure is the correlation ratio reported in [13] and mathematically proven by [23] to have low computational requirement as a way of reducing the already-large computation architecture.", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "The similarity index selected for instability measure is the correlation ratio reported in [13] and mathematically proven by [23] to have low computational requirement as a way of reducing the already-large computation architecture.", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "Figure 5 (a) shows the setup and a detail description can be found in [28].", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "CUDA cores, equipped with 6GB video memory, using the python-based machine learning frameworks such as Theano, Lasagne and NoLearn [4] [34].", "startOffset": 131, "endOffset": 134}, {"referenceID": 33, "context": "CUDA cores, equipped with 6GB video memory, using the python-based machine learning frameworks such as Theano, Lasagne and NoLearn [4] [34].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "Note that the batch iterative training in NoLearn and Lasagne functions were replaced with Theano\u2019s LeNet 5 [22] early stopping algorithm which showed further improvements in test performance.", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": "These results are similar to those reported in [27] where the framework used a neural-symbolic approach with a combination of convolutional neural networks and symbolic time series analysis to obtain instability measures.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "As reported in [27], these intermittency phenomena can not be observed from pressure data or POD analysis of image data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "Hence the proposed metric localizes the intermittencies prior to fullblown instability with more prominence than other state-ofthe-art approaches [27].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "Better accuracy in tracking intermittence leads to an early detection of instability with less false alarm [27].", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "Also implicit labeling helps in higher level learning of the flame dynamics from the video by using probabilistic graphical model such as HMM or STSA [27].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "In this application however, the gradual build up of the two lobes \u201dmushroom-like structure\u201d [28] from frame to frame as well as increasing average instability measure are a pointer to the graduated transitioning ability of the framework.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "This paper proposes an end-to-end convolutional selective autoencoder approach for early detection of combustion instabilities using rapidly arriving flame image frames. The instabilities arising in combustion processes cause significant deterioration and safety issues in various human-engineered systems such as land and air based gas turbine engines. These properties are described as self-sustaining, large amplitude pressure oscillations and show varying spatial scales periodic coherent vortex structure shedding. However, such instability is extremely difficult to detect before a combustion process becomes completely unstable due to its sudden (bifurcation-type) nature. In this context, an autoencoder is trained to selectively mask stable flame and allow unstable flame image frames. In that process, the model learns to identify and extract rich descriptive and explanatory flame shape features. With such a training scheme, the selective autoencoder is shown to be able to detect subtle instability features as a combustion process makes transition from stable to unstable region. As a consequence, the deep learning tool-chain can perform as an early detection framework for combustion instabilities that will have a transformative impact on the safety and performance of modern engines.", "creator": "LaTeX with hyperref package"}}}