{"id": "1603.07893", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction", "abstract": "We explore the effectiveness of using Long Short Term Memory Networks (LSTM) for predicting stock prices. We will construct, test and compare a range of architectures of LSTMs trained via backpropagation through time (BPTT).", "histories": [["v1", "Fri, 25 Mar 2016 12:28:02 GMT  (5kb)", "http://arxiv.org/abs/1603.07893v1", "8 pages"], ["v2", "Thu, 31 Mar 2016 11:28:45 GMT  (5kb)", "http://arxiv.org/abs/1603.07893v2", "7 pages"], ["v3", "Sun, 28 Aug 2016 09:56:23 GMT  (4kb)", "http://arxiv.org/abs/1603.07893v3", "6 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hengjian jia"], "accepted": false, "id": "1603.07893"}, "pdf": {"name": "1603.07893.pdf", "metadata": {"source": "CRF", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction", "authors": ["Hengjian Jia"], "emails": ["henryjia18@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.07 893v 1 [cs.N E] 25 Mar 201 6Research on the effectiveness of long-term short-term memory networks for stock price forecastingHengjian JiaColyton Grammar SchoolE-mail: henryjia18 @ gmail.com"}, {"heading": "1 Acknowledgements", "text": "I would like to thank Alfie Howard for providing the necessary code for pre-processing data, and Franc ois Chollet for creating the Keras framework that created all the code needed for this work."}, {"heading": "2 Abstract", "text": "We investigate the effectiveness of using Long Short Term Memory Networks (LSTM) to predict stock prices. We construct, test and compare a number of architectures of LSTMs that have been trained using Backpropagation through Time (BPTT)."}, {"heading": "3 Introduction", "text": "Share prices are a form of time series data. There are many existing economic methods for predicting stock prices. These methods can be classified as basic and technical analysis. Technical analyses are based on observing patterns in stock prices that are based on psychological effects (fear and greed) that alter supply and demand. Fundamental analyses are based on observing current news and events such as corporate profits and economic conditions. However, all of these models are ultimately based on human assessment of the situation in order to make predictions, and are not learned [1]. There are also other learning algorithms that can be used to tackle the problem of predicting stock prices, including the use of deep multi-layered perceptions [2] and revolutionary neural networks [3]. However, these methods have limited temporal memory capacities that can be provided by a fixed sliding window to predict future stock prices as a function of historical prices."}, {"heading": "4 Long Short Term Memory", "text": "The LSTM cell has an internal state that is updated to the previous activations of the layer and the inputs by connections to the previous layer and self-connecting [4]. A layer of LSTM cells takes a sequence of vectors as input x = (x1, x2, x3,., xT) and prints a sequence of vectors y = (y1, y2, y3,.., yT). The output vectors are calculated by iterating them by the following equations from t = 1 to T: ct = g (Wcxxt + Wcyt \u2212 1 + bc) it = (Wixxt + Wiyt \u2212 1 + bi) ft \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p p p = p = p \u2212 p p = p \u2212 p \u2212 p p \u2212 p p p p p = p = p \u2212 p p = p \u2212 p p = p = p \u2212 p = p = p = p = p \u2212 p = p = p = p = p = p = p = p = p = p = p = p = 2,5 x p = p \u2212 p = p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p = \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p = p = p = p \u00b7 p = p = p = p = p = p = p = p = p = p = p = p \u2212 p = p = p = p = p = p \u2212 p \u2212 p = p \u2212 p = p = p \u2212 p = p = p = p \u2212 p = p = p \u2212 p = p = p \u2212 p = p = p \u2212 p = p = p \u2212 p = p \u2212 p = p = p \u2212 p = p \u2212 p = p = p = p = p = p = p = p \u2212 p = p = p \u2212 p = p = p \u2212 p = p = p = p = p = p = p \u2212 p = p = p = p = p = p = p = p = p = p - p = p = p = p = p = p = p = p = p = p = p = p = p - p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "6 Weight Initialisations", "text": "We initialized our feed weights by samples from the uniform distribution [6] W \u0445 U [\u2212 \u221a 6 \u221a nin + nout, \u221a 6 \u221a nin + nout] We initialized our recurring weights by first samples from the Gaussian distribution with 0 mean and unit variance. Subsequently, we performed a singular talc decomposition."}, {"heading": "W = USV T", "text": "This gives us 2 random orthonormal matrices U and V, so we set W: = UThis gives us recurring weight matrices with maximum eigenvalue. Consequently, the internal state does not explode, as the multiplication with the weight matrices repeatedly does not increase the Euclidean norm of the internal state vector. Finally, we initialized the forge bias units to 1 and the other distortions to 0."}, {"heading": "7 Hyperparameters, Feature Selection and Tar-", "text": "In our experiments, we set the learning rate to 0.01 and trained our network for 200 epochs. We used Google's daily stock prices from January 1, 2005 to December 31, 2014 to create our training rate and from January 1, 2015 to June 21, 2015 to form our test rate. The data came from Yahoo Finance. We used only the open, high, low, near and volume values as characteristics for our input factors. This is because all other technical indicators used in technical analysis are simply calculated from them. Therefore, our network should learn all the necessary patterns represented by the indicators. We also normalized our data to support the training by converting them into returns by a percentage variation. x-t = xtxt-1 \u2212 1 This scaled the values to make them all small, but also transformed the time series to become limited. This is due to the fact that the shares move from day to day, making it very easy to learn our network."}, {"heading": "8 Training Methodology", "text": "Although LSTMs have largely solved the problem of the disappearing / exploding gradient, they are still insufficient for sequences of such large lengths. Therefore, we trained our network with the algorithm described as follows. Algorithm 1 LSTM Pretraining Algorithm \u2190 0 m \u2190 Sequence length for the internal state of the network 2i < m doReset sequence with a sliding window of length 2i Train Network on truncated sequence i \u2190 i + 1end for Train Sequence on whole dataset Reset network's internal test state Network on test sequenceHowever, we did not need the sequence length of the training to be as long as the data go back in time. So we truncated sliding windows to a length of 256, as this was slightly more than the number of trading days in a year. It is unlikely that data further back than a year will affect future data."}, {"heading": "10 Discussion", "text": "As shown in Table 1, the return data is generally resistant to overadjustment. The network generally performs better when deepened and expanded, with a few exceptions, even if the number of parameters in the network is much greater than the number of unique training examples. Although we know in absolute numbers whether the network works, we compare the RMSE of the networks with the RMS of the returns themselves. If the RMS of the network itself is larger, this would imply that the network works better than an algorithm that simply does not predict change and vice versa. The RMS of the test data is 0.0265, which shows that the networks are all almost or more than twice as successful as the benchmark. From this we can conclude that LSTM networks trained with first-order accelerated methods are an effective method for predicting inventory returns. We also come to the conclusion that returns and LSTM networks in general are resistant to overadjustment and larger networks work better."}], "references": [{"title": "ANN Model to Predict Stock Prices at Stock Exchange Markets", "author": ["Wanjawa Barack Wamkaya", "Muchemi Lawrence"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Long Short-Term Memory in Recurrent Neural Networks", "author": ["Felix Gers"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "These include using deep multilayer perceptrons [2] and convolutional neural networks [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The LSTM cell has an internal state which is updated based on the previous activations of the layer and inputs through connections to the previous layer and self connections[4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "6 Weight Initialisations We initialised our feed forward weights by sampling from the uniform distribution [6]", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "001 learning rate and default paramters [7].", "startOffset": 40, "endOffset": 43}], "year": 2016, "abstractText": "Stock prices are a form of time series data. There have been many existing business and economics based methods for predicting stock prices. These methods can be classed as fundamental and technical analysis. Technical analysis is based on the observing patterns in stock prices based on psychological effects (fear and greed) changing supply and demand. Fundamental analysis is based on observing current news and events such as the corporate profits and economic situation. However, all of these models ultimately rely on human judgement of the situation to make predictions and are not learned [1].", "creator": "LaTeX with hyperref package"}}}