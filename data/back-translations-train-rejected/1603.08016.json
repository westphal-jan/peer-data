{"id": "1603.08016", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Classifying Syntactic Regularities for Hundreds of Languages", "abstract": "This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label pre- diction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family.", "histories": [["v1", "Fri, 25 Mar 2016 20:09:29 GMT  (58kb,D)", "https://arxiv.org/abs/1603.08016v1", null], ["v2", "Wed, 27 Apr 2016 18:40:55 GMT  (58kb,D)", "http://arxiv.org/abs/1603.08016v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["reed coke", "ben king", "dragomir radev"], "accepted": false, "id": "1603.08016"}, "pdf": {"name": "1603.08016.pdf", "metadata": {"source": "CRF", "title": "Classifying Syntactic Regularities for Hundreds of Languages", "authors": ["Reed Coke", "Ben King", "Dragomir Radev"], "emails": ["reedcoke@umich.edu", "benking@ubiquiti.com", "radev@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the situation is that most people are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Related Work", "text": "Typological similarity has previously been shown to correlate with genealogical similarities, both in the fields of NLP (Rama and Kolachina, 2012) and in historical linguistics (Dunn et al., 2005). Therefore, WALS was used to study linguistic typology using computer-aided methods. In order to determine language similarity by means of genealogical kinship, it is important to know which rules of historical kinship are more meaningful. To do this, (Wichmann and Kamholz, 2008) the variance of linguistic rules within language families was measured, at the level of the genus language typification, to determine their stability. Rules that change less frequently within language families are then more indicative of historical relationships when they are shared between languages. To this end, the authors extracted data for language families from WALS and calculated the probability that features are shared within language families and across language families."}, {"heading": "3 Experiments", "text": "The 325 languages represented in this corpus are very diverse, ranging from standard French, German, and modern Arabic to Acholi, Basque, and Tamil. Note that there are translated Bibles for many other languages not included in this study. In total, we use over 2 million concerted sentences. The rules have been selected based on the frequency of their use in previous NLP research, a measure of their usefulness. For more information on this, see Section 3.2. For each rule, we conducted experiments with a variety of classifiers and feature-vector combinations. All results are listed in Section 4."}, {"heading": "3.1 General Textual Feature Extraction", "text": "For each rule described in Section 3.2, we extract English dependencies according to certain criteria proper to that rule. On the basis of these derived dependencies, we calculate the attribute vectors for the rule in question using word alignments specified by BerkeleyAligner (DeNero and Klein, 2007; Liang et al., 2006). Consider the English sentence and its Ma'di translation in Figure 1., and Resurrection is the verb and object of the English sentence, while i and onzika are the Ma'di alignments. These alignments are shown using blue edges. As these words are aligned, we assume that they perform the same grammatical function. We can see that the English sentence has a verb-object order, while the Ma'di sentence has an object order. As these words are aligned, we can assume that they serve the same grammatical function."}, {"heading": "3.2 Specific Feature Extraction", "text": "Each rule is described in detail below, along with definitions of its possible classes, rounded distribution information about the languages considered in this essay, and a brief explanation of why we decided to classify this rule. All the excerpts mentioned in this section were generated using a hash kernel parser (Bohnet, 2010) and presented in the CONLL dependency analysis format. We limit our system to cases where the English sentence is aligned to only one foreign language set. The number of languages specified for each rule corresponds to the size of the group of languages whose name for this rule is given in WALS and for which we have a Bible."}, {"heading": "3.2.1 Rule 83A: Order of Object and Verb", "text": "Rule 83A defines the order in which the direct object and the verb of a sentence occur; its name is known for 299 languages; the three possible classes are Object Verb (The man saw the dog. 38%), Verb Object (The man the dog saw. 58%), and No Dominant Order (both valid under certain conditions. 4%) Apart from the fact that it is of fundamental importance for any kind of analysis or chunking task, many other typological rules have been found that are statistically related to the order of subject, object, and verb (Greenberg, 1963). Since it has also been established that subject generally precedes the object (Greenberg, 1963), knowledge of the order of object and verb allows accurate guesses not only about the order of subject, object, and verb, but also about many other correlated rules. To identify all occurrences, we examine all dependencies with the label dobj. The instance is held identical as long as the English questions are both foreign to each other and the other, as both the object and the other are adjacent to each other."}, {"heading": "3.2.2 Rule 85A: Order of Adposition and Noun Phrase", "text": "Rule 85A defines the order in which adpositions occur in relation to their nouns. Its name is known for 256 languages; the five possible classes are postpositions (the dog went to the park. 43%), prepositions (the dog went to the park. 49%), inpositions (the dog went to the park. 0%), more than one type without a dominant (multiple valid. 6%) and no adpositions (the dog went to the park. 1%). This rule is crucial for any successful semantic analysis or narrative processing. We examine all dependencies with the adpmod and have a parent word in the sentence. The instance is maintained as long as the English and foreign adoptions are aligned only to each other and the English and foreign governing nouns are aligned only to each other."}, {"heading": "3.2.3 Rule 86A: Order of Genitive and Noun", "text": "Rule 86A defines the order in which genitives appear in relation to their governing noun. 254 languages in our data set have a known name for this rule. The three possible classes are genitive noun (my dog 47%), noun genitive (my dog 45%) and both occur, none dominant (both valid 8%).This rule is absolutely necessary for resolving co-references. We examine all dependencies with the label poss as long as genitive and noun are oriented only to their respective foreign-language counterparts."}, {"heading": "3.2.4 Rule 88A: Order of Demonstrative and Noun", "text": "Rule 88A defines the order in which demonstratives appear together with nouns. There are 171 languages in our set of Bibles with a known name for this rule. The six possible classes are demonstrative noun (this dog is shaggy. 50%), demonstrative noun (this dog is shaggy. 42%), prefix to noun (this dog is shaggy. 0%), suffix to noun (dog is shaggy. 2%), demonstrative noun (this dog is shaggy. 2%) and two or more of the previous options, none of which is dominant (multiple times valid. 4%). This rule is also necessary for resolving the correlation. We select all dependencies that are of type det or pron, have a lexical value of this or that, and whose parent is a noun."}, {"heading": "3.2.5 Rule 92A: Position of Polar Question Particles", "text": "Rule 92A defines the position of a polar question particle in a sentence. Its rule is known for 91 languages. Polar question particles signal grammatically that it is a yes or no question. Six possible classes are beginning of sentence (23%), end of sentence (36%), second word in sentence (3%), somewhere else (2%), each of two positions (1%), no question particle (34%). In languages with polar questions, this rule would allow much higher-quality answer and dialogue systems. Our selection of this rule also shows that the phenomenon does not even have to occur in the source language to be addressed. We select all dependencies that contain questions. We then use the presence of an English Wish word to sort these questions into polar and information questions. Using these terms, we examine the foreign-language questions after the word that occurs most frequently in polar questions, but not information questions according to the ratio of the relative frequency of each position, and then count the number of other possible words first of each possible question."}, {"heading": "3.2.6 Rule 107A: Passive Constructions", "text": "107A describes the presence or absence of passive constructions. We have Bibles for 93 of the WALS languages with a well-known label for this rule. The two possible classes are passive construction (the dog was seen by the butler. 53%) and no passive construction (the butler saw the dog. 47%). This rule was chosen not only because it would allow better grammar induction, a very relevant task for such underfunded languages, but also to show that our system and approach can handle rules other than simple word order rules. We select all dependencies that are nsubjpass, signaling a passive subject, and nsubj, signaling an active subject. We then determine the number of times the order of subject and verb differs between these two types of sentences in the foreign language."}, {"heading": "3.3 Text Classifier Training", "text": "To train the text classifiers, we used identical Biblical texts in English and each of the languages considered together with the annotations from WALS. For each rule discussed in Section 3.2, we select dependencies and generate normalized feature vectors according to the method discussed. This forms the text feature group described in Section 4."}, {"heading": "3.4 Typological Classifier Training", "text": "We also believe that some terms can be predicted from the terms of other rules within a language. Create attribute vectors for each language using five of the six rules discussed in Section 3.2 to classify the sixth rule."}, {"heading": "3.5 Genealogical Classifier Training", "text": "Previous work has shown that the multiplication of knowledge from genealogically similar languages has yielded significantly better results than the multiplication of knowledge from a random language (Naseem et al., 2012). For our purposes, we consider the genera and families of each language as indicated by WALS. To this end, we simply consider the multiplication of the majority term from all languages of the same genus and, separately, from all languages of the same family. In addition, we create character vectors such as those for the characters Text and Rules as described above."}, {"heading": "4 Results and Discussion", "text": "In logistical regression, since we do not have a development data set, we simply experiment with five common regularization parameters - 1.0, 0.5, 0.1, 0.01, and 10 \u2212 8 (the default in Weka). Since we only evaluate languages that already have designations in WALS, we consider the accuracy of each classifier through one-time cross-validation."}, {"heading": "4.1 Simple Linguistic Features", "text": "Tables 1-6 show that the combination of textual and typological characteristics works best. It can also be seen that, almost independently of the regularization parameter, logistical regression models these data with greater accuracy than naive Bayes. In cases where this is not the case, it is the linguistic characteristics that work best. This is also encouraging, since the main problem we are trying to solve is the scarcity of WALS data. There are also two rules that work quite poorly, rule 92A and rule 107A, represented in tables 5 and 6. These rules are much more subtle in linguistic terms than the rest, which are relatively easy for our parser to detect. The complexity of the pipeline for rule 92A and the numerous ways in which languages can mark passive constructions are simply consistent with respect to rule 107A, as the relationship between regularization and regularization makes a difference."}, {"heading": "4.2 Genealogy-based Classification", "text": "The genealogical experiments yielded quite different results. First, we tested a very simple approach by assigning the majority name to languages of the same genus or family. Table 7 shows these results. It is very clear that this majority voting system works much better than the other approaches. This is particularly interesting given the argument that typological traits were claimed to be an effective proxy for genealogical data (Rama and Kolachina, 2012). We also tried to include genealogical traits as a third source of traits, similar to our textual and typological traits. The combination of genealogical traits with the textual or rule traits did not exceed the systems already presented, nor did the combination of all three traits for the naive Bayes or any logistical regression model. We also believed that there may be something intrinsic to majority decision, which increases the performance of this simple approach."}, {"heading": "5 Conclusion", "text": "In this study, we examined the effectiveness of classifying a set of syntactic rules within a range of foreign languages, but this can be done by taking into account the values of other syntactic rules in the database. However, it is also worth taking into account actual linguistic characteristics for this linguistic task, as well as merging the currently available WALS data with linguistic characteristics. Finally, we also took into account genealogical data from the WALS databases. We have shown that, if properly regulated, this is only the case if the linguistic characteristics alone work well. Furthermore, it is not possible to say from our results which regulatory parameters are best. In some cases, the individual linguistic characteristics do indeed exceed the combined characteristics, but this is not generally the case and it only applies if the linguistic characteristics work well."}, {"heading": "6 Future Work", "text": "While the rules we are considering are all useful in NLP research, there are other rules that are used almost as frequently that are more complex than those that govern negation. In particular, modules on morphological traits that are not easily detectable are the next logical direction. It would also be worthwhile to include word-by-word oriented Bibles from other well-supported source languages in addition to English. As shown by rule 92A, the system can work even if the trait does not appear in the source language, but it is certainly more difficult to explain this. Multiple source languages would minimize the number of rules for which this is the case. Experiments with combinations of genetic, textual traits and typical traits should be conducted."}], "references": [{"title": "Towards creating precision grammars from interlinear glossed text: Inferring large-scale typological properties", "author": ["Michael Wayne Goodman", "Joshua Crowgey", "Fei Xia"], "venue": "In Proceedings of the 7th Workshop on Lan-", "citeRegEx": "Bender et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bender et al\\.", "year": 2013}, {"title": "Reconstructing native language typology from foreign language usage. CoRR, abs/1404.6312", "author": ["Roi Reichart", "Boris Katz"], "venue": null, "citeRegEx": "Berzak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berzak et al\\.", "year": 2014}, {"title": "Very high accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Syntactic structures of the world\u2019s languages. http://sswl.railsplayground", "author": ["Collins", "Kayne2011] Chris Collins", "Richard Kayne"], "venue": null, "citeRegEx": "Collins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2011}, {"title": "How varied typologically are the lan", "author": ["Cysouw", "Comrie2008] Michael Cysouw", "Bernard Comrie"], "venue": null, "citeRegEx": "Cysouw et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cysouw et al\\.", "year": 2008}, {"title": "Quantitative explorations of the world-wide distribution of rare characteristics, or: the exceptionality of northwestern european languages", "author": ["Michael Cysouw"], "venue": "Expecting the Unexpected,", "citeRegEx": "Cysouw.,? \\Q2011\\E", "shortCiteRegEx": "Cysouw.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "A bayesian model for discovering typological implications", "author": ["Daum\u00e9", "Campbell2007] Hal III Daum\u00e9", "Lyle Campbell"], "venue": null, "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2007}, {"title": "Tailoring word alignments to syntactic machine translation", "author": ["DeNero", "Klein2007] John DeNero", "Dan Klein"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "DeNero et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2007}, {"title": "Structural phylogenetics and the reconstruction of ancient language history", "author": ["Dunn et al.2005] Michael Dunn", "Angela Terrill", "Ger Reesink", "Robert A Foley", "Stephen C Levinson"], "venue": null, "citeRegEx": "Dunn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2005}, {"title": "Comparing language similarity across genetic and typologically-based groupings", "author": ["Georgi et al.2010] Ryan Georgi", "Fei Xia", "William Lewis"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Georgi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Georgi et al\\.", "year": 2010}, {"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["Joseph H. Greenberg"], "venue": "Universals of Human Language,", "citeRegEx": "Greenberg.,? \\Q1963\\E", "shortCiteRegEx": "Greenberg.", "year": 1963}, {"title": "The weka data mining software: An update", "author": ["Hall et al.2009] Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "A random walk based model for identifying semantic orientation", "author": ["Hassan et al.2014] Ahmed Hassan", "Amjad Abu-Jbara", "Wanchen Lu", "Dragomir R. Radev"], "venue": "Computational Linguistics,", "citeRegEx": "Hassan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2014}, {"title": "Word order universals: Quantitative analyses of linguistic structure", "author": ["John A Hawkins"], "venue": null, "citeRegEx": "Hawkins.,? \\Q1983\\E", "shortCiteRegEx": "Hawkins.", "year": 1983}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Hwa et al.2005] Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural Language Engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Automatically identifying computationally relevant typological features", "author": ["Lewis", "Xia2008] William D Lewis", "Fei Xia"], "venue": "In IJCNLP,", "citeRegEx": "Lewis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2008}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Visualising typological relationships: Plotting wals with heat maps", "author": ["Rory Turnbull", "Alexis Palmer"], "venue": "In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH,", "citeRegEx": "Littauer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Littauer et al\\.", "year": 2012}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL", "author": ["Naseem et al.2012] Tahira Naseem", "Regina Barzilay", "Amir Globerson"], "venue": null, "citeRegEx": "Naseem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "How good are typological distances for determining genealogical relationships among languages", "author": ["Rama", "Kolachina2012] Taraka Rama", "Prasanth Kolachina"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics", "citeRegEx": "Rama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rama et al\\.", "year": 2012}, {"title": "Bayesian agglomerative clustering with coalescents", "author": ["Teh et al.2008] Yee Whye Teh", "Hal Daum Iii", "Daniel Roy"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "A stability metric for typological features. STUF-Language Typology and Universals Sprachtypologie und Universalienforschung, 61(3):251\u2013262", "author": ["Wichmann", "Kamholz2008] S\u00f8ren Wichmann", "David Kamholz"], "venue": null, "citeRegEx": "Wichmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wichmann et al\\.", "year": 2008}, {"title": "Multilingual structural projection across interlinear text", "author": ["Fei Xia"], "venue": "Proc. of the Conference on Human Language Technologies (HLT/NAACL", "citeRegEx": "Xia.,? \\Q2007\\E", "shortCiteRegEx": "Xia.", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": ", 2012) as well as a tool with which to discover which linguistic rules are rare across the world\u2019s languages (Cysouw, 2011).", "startOffset": 110, "endOffset": 124}, {"referenceID": 1, "context": "Unsurprisingly, it has also served as a baseline for typological similarity measurements (Berzak et al., 2014).", "startOffset": 89, "endOffset": 110}, {"referenceID": 10, "context": "Indeed, the sparseness of WALS has been noted by several researchers (Georgi et al., 2010; Cysouw and Comrie, 2008; Teh et al., 2008).", "startOffset": 69, "endOffset": 133}, {"referenceID": 22, "context": "Indeed, the sparseness of WALS has been noted by several researchers (Georgi et al., 2010; Cysouw and Comrie, 2008; Teh et al., 2008).", "startOffset": 69, "endOffset": 133}, {"referenceID": 11, "context": "proposed a set of linguistic universals (Greenberg, 1963) that refer to statistical tendencies across the world\u2019s languages.", "startOffset": 40, "endOffset": 57}, {"referenceID": 14, "context": "Further universals have been described since, particularly with respect to word order (Hawkins, 1983).", "startOffset": 86, "endOffset": 101}, {"referenceID": 19, "context": "and many other basic NLP tasks such as machine translation (Mikolov et al., 2013) and part-of-speech tagging (Das and Petrov, 2011).", "startOffset": 59, "endOffset": 81}, {"referenceID": 9, "context": "Typological similarity has previously been shown to correlate with genealogical similarity both in the fields of NLP (Rama and Kolachina, 2012) and historical linguistics (Dunn et al., 2005).", "startOffset": 171, "endOffset": 190}, {"referenceID": 20, "context": "(Naseem et al., 2012) used language similarity information to improve multilingual parsing by defined a distance", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Bender et al., 2013) attempted a similar task, us-", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "past (Xia, 2007), (Hwa et al.", "startOffset": 5, "endOffset": 16}, {"referenceID": 15, "context": "past (Xia, 2007), (Hwa et al., 2005).", "startOffset": 18, "endOffset": 36}, {"referenceID": 17, "context": "We then project these dependencies onto the foreign language biblical sentences using word alignments determined by BerkeleyAligner (DeNero and Klein, 2007; Liang et al., 2006).", "startOffset": 132, "endOffset": 176}, {"referenceID": 2, "context": "All parses referred to in this section were obtained using a Hash Kernel parser (Bohnet, 2010) and represented in the CONLL dependency parsing format.", "startOffset": 80, "endOffset": 94}, {"referenceID": 11, "context": "Besides being fundamental to any sort of parsing or chunking task, many other typological rules have been found to be statistically related to the order of subject, object, and verb (Greenberg, 1963).", "startOffset": 182, "endOffset": 199}, {"referenceID": 11, "context": "As it has also been found that subject generally precedes object (Greenberg, 1963), knowing the order of the object and verb allows not only accurate guesses about the order of subject, object, and verb, but also about many other correlated rules.", "startOffset": 65, "endOffset": 82}, {"referenceID": 20, "context": "edge from genealogically similar languages has demonstrated markedly better results than propagating knowledge from a random language (Naseem et al., 2012).", "startOffset": 134, "endOffset": 155}, {"referenceID": 12, "context": "Na\u0131\u0308ve Bayes is run using the default implement in Weka (Hall et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 5, "context": "It has been estimated that the accuracy of the current WALS database is possibly close to 96%, based on examination of the entries for Latvian (Cysouw, 2011).", "startOffset": 143, "endOffset": 157}, {"referenceID": 13, "context": "data (Hassan et al., 2014).", "startOffset": 5, "endOffset": 26}], "year": 2016, "abstractText": "This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label prediction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family.", "creator": "LaTeX with hyperref package"}}}