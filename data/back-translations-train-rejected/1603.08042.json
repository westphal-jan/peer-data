{"id": "1603.08042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition", "abstract": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.", "histories": [["v1", "Fri, 25 Mar 2016 21:43:28 GMT  (171kb,D)", "http://arxiv.org/abs/1603.08042v1", "Accepted in ICASSP 2016"], ["v2", "Mon, 2 May 2016 15:19:30 GMT  (261kb,D)", "http://arxiv.org/abs/1603.08042v2", "Accepted in ICASSP 2016"]], "COMMENTS": "Accepted in ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["rohit prabhavalkar", "ouais alsharif", "antoine bruguier", "ian mcgraw"], "accepted": false, "id": "1603.08042"}, "pdf": {"name": "1603.08042.pdf", "metadata": {"source": "CRF", "title": "ON THE COMPRESSION OF RECURRENT NEURAL NETWORKS WITH AN APPLICATION TO LVCSR ACOUSTIC MODELING FOR EMBEDDED SPEECH RECOGNITION", "authors": ["Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Ian McGraw"], "emails": ["prabhavalkar@google.com", "oalsha@google.com", "tonybruguier@google.com", "imcgraw@google.com"], "sections": [{"heading": null, "text": "Index Terms - Model Compression, LSTM, RNN, SVD, Embedded Speech Recognition"}, {"heading": "1. INTRODUCTION", "text": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurring hidden layers [3, 4] have emerged as state-of-the-art acoustic models (AMs) for automatic speech recognition (ASR). Advances in computer skills coupled with the availability of large annotated speech corpora have made it possible to train NN-based AMs with a large number of parameters [5] with great success. As voice recognition techniques continue to improve, they are becoming increasingly ubiquitous on mobile devices: voice assistants such as Apple's Siri, Microsoft's Cortana, Amazon's Alexa, and Google Now [6] enable users to search for information using their voice. Although the traditional model for these applications has been to recognize speech remotely on large servers, there is a growing interest in developing ASR technologies that can recognize speech input directly \"on the device.\""}, {"heading": "2. RELATED WORK", "text": "It has been pointed out in previous work that there is a large amount of redundancy in the parameters of a neural network. Denil et al. [13] show that the entire neural network can be reconstructed given the values of a small number of parameters. Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15]. This approach, called \"model compression,\" is closely related to the recent approach of \"distillation\" proposed by Hinton et al. [16] Redundancy in a neural network has also been exploited in Chen al's HashNet."}, {"heading": "3. MODEL COMPRESSION", "text": "In this area, we have the opportunity to contact us with the \"new,\" \"old,\" \"new,\" \"new,\" \"new,\" \"new,\" \"old,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \",\" \"new,\" \"new,\" \"\", \"\" \"new,\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \",\" \",\" \",\" \",\", \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\", \"\", \"\", \",\" \",\", \",\" \",\", \",\", \",\", \",\" \",\", \",\" \",\" \",\", \"\", \"\" \",\", \",\" \"\", \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\", \"\" \",\" \"\" \"\" \",\" \",\" \"\", \"\" \",\" \"\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\""}, {"heading": "3.1. Applying our technique to LSTM RNNs", "text": "The generalization of the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is simple. On the basis of the notation in [3], it should be noted that the recursively weighted matrix W lh in the case of LSTM is the concatenation of the four gate weight matrices achieved by vertical stacking: [Wim, Wom, Wfm, Wcm] T, which represent the respective recurrent connections with the entrance gate, the exit gate, the gating gate and the cell state. Similarly, the inter-layer matrix W lx is the concatenation of the matrices: [Wix, Wfx, Wox, Wcx] T, which correspond to the entrance gate, the gating gate, the exit gate and the cell state (the next layer)."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "In order to determine the effectiveness of the proposed RNN compression technology, we must first of all consider the extent to which we are able, in which areas we are able to establish ourselves."}, {"heading": "4.1. Training and Evaluation Data", "text": "Our systems are trained on hand-transcribed anonymised utterances extracted from Google voice search traffic (2,000 hours); we generate \"multistylistic\" training data by generating synthetically distorted utterances to simulate background noise and reverberation using a room simulator with sound samples from YouTube videos and environmental recordings of everyday events; for each utterance in the training set, 20 distorted examples are created; in addition, the systems are adapted using the sMBR criterion [?, 27] based on a series of anonymised hand-transcribed (in-domain) dictation utterances extracted from Google traffic and processed to generate \"multistylistic\" training data as described above, improving the performance of our dictation task; all results are evaluated on a set of 13.3K hand-transcribed anonymised utterances from an open dictation range."}, {"heading": "5. RESULTS", "text": "In our experiments, we try to determine the impact of the proposed common SVD-based compression technique on system performance \u00b7 \u00b7 \u00b7 In particular, we are interested in determining how system performance varies depending on the degree of compression controlled by setting the ranks of the recurring projection matrices rl, as in Section 3. Note, however, that since the proposed compression scheme is applied to all the hidden layers of the base system, there are numerous settings of the ranks rl for the projection matrices in each layer, resulting in the same number of total parameters in the compressed network. To avoid this ambiguity, we use the different projection series using the following criterion: Given a threshold of ranks rl for each layer, we set the rank rl of the corresponding projection matrix to maintain a fraction of the declared variance after the truncated SVD."}, {"heading": "6. CONCLUSIONS", "text": "We introduced a technique for compressing RNNs using a common factorization of recurrent and interlayer weight matrices, generalizing previous work [9]. The proposed technique was applied to the task of compressing LSTM-RNN acoustic models for embedded speech recognition, where we found that we could compress our base acoustic model to one-third of its original size without suffering minor loss of accuracy. The proposed techniques, combined with weight quantization, allow us to develop a small and efficient speech recognition mechanism that runs many times faster than in real-time on modern mobile devices [?]."}, {"heading": "7. REFERENCES", "text": "[1] Frank Seide, Gang Li, and Dong Yu, \"Conversational speech transcription using context-dependent deep neural networks,\" in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2011, pp. 437-440. [2] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury, \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal Processing Magazine, vol. 29, pp. 82-97, 2012. [3] Has. im Sak, Andrew Senior, and Franc."}], "references": [{"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2011, pp. 437\u2013440.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2014, pp. 338\u2013342.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Tara N Sainath", "Oriol Vinyals", "Andrew Senior", "Ha\u015fim Sak"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning: methods and applications", "author": ["Li Deng", "Dong Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3\u20134, pp. 197\u2013387, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "your word is my command\u201d: Google search by voice: A case study", "author": ["Johan Schalkwyk", "Doug Beeferman", "Fran\u00e7oise Beaufays", "Bill Byrne", "Ciprian Chelba", "Mike Cohen", "Maryam Kamvar", "Brian Strope"], "venue": "Advances in Speech Recognition, pp. 61\u201390. Springer US, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Accurate and compact large vocabulary speech recognition on mobile devices", "author": ["Xin Lei", "Andrew Senior", "Alexander Gruenstein", "Jeffrey Sorensen"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 662\u2013665.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Personalized speech recognition on mobile devices", "author": ["Ian McGraw"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), (submitted).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 0}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 2365\u20132369.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Singular value decomposition based lowfootprint speaker adaptation and personalization for deep neural network", "author": ["Jian Xue", "Jinyu Li", "Dong Yu", "Mike Seltzer", "Yifan Gong"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 6359\u20136363.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2148\u20132156.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Model compression", "author": ["Cristian Bucilu\u0103", "Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 535\u2013541.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Do deep nets really need to be deep", "author": ["Lei Jimmy Ba", "Rich Caruana"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2014, pp. 2654\u20132662.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 1989, pp. 598\u2013605.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimizing bottleneck features for LVCSR", "author": ["Franti\u0161ek Gr\u00e9zl", "Petr Fousek"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2008, pp. 4729\u20134732.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N. Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6655\u20136659.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Smallfootprint high-performance deep neural network-based speech recognition using split-VQ", "author": ["Yongqiang Wang", "Jinyu Li", "Yifan Gong"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep neural networks using a rank-constrained topology", "author": ["Preetum Nakkiran", "Raziel Alvarez", "Rohit Prabhavalkar", "Carolina Parada"], "venue": " Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 1473\u20131477.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan \u0130rsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of the Internation Speech Communication Association (Interspeech), 2015, pp. 1468\u20131472.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the International Conference on Machine Learning (ICML). ACM, 2006, pp. 369\u2013376.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1223\u20131231.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["Ha\u015fim Sak", "Oriol Vinyals", "Georg Heigold", "Andrew Senior", "Erik McDermott", "Rajat Monga", "Mark Mao"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2014, pp. 1209\u20131213.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 49, "endOffset": 55}, {"referenceID": 1, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 83, "endOffset": 89}, {"referenceID": 3, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 83, "endOffset": 89}, {"referenceID": 4, "context": "Advances in computational capabilities coupled with the availability of large annotated speech corpora have made it possible to train NN-based AMs with a large number of parameters [5] with great success.", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "As speech recognition technologies continue to improve, they are becoming increasingly ubiquitous on mobile devices: voice assistants such as Apple\u2019s Siri, Microsoft\u2019s Cortana, Amazon\u2019s Alexa and Google Now [6] enable users to search for information using their voice.", "startOffset": 207, "endOffset": 210}, {"referenceID": 6, "context": "Although the traditional model for these applications has been to recognize speech remotely on large servers, there has been growing interest in developing ASR technologies that can recognize the input speech directly \u201con-device\u201d [7].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": ", [9, 10]), where we jointly compress both recurrent and inter-layer weight matrices, allows us to compress acoustic models up to a third of their original size with negligible loss in accuracy.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": ", [9, 10]), where we jointly compress both recurrent and inter-layer weight matrices, allows us to compress acoustic models up to a third of their original size with negligible loss in accuracy.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", handwriting recognition [11] and machine translation [12] inter alia.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": ", handwriting recognition [11] and machine translation [12] inter alia.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "[13] show that the entire neural network can be reconstructed given the values of a small number of parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15].", "startOffset": 240, "endOffset": 248}, {"referenceID": 14, "context": "Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15].", "startOffset": 240, "endOffset": 248}, {"referenceID": 13, "context": "This approach, termed \u201cmodel compression\u201d [14] is closely related to the recent \u201cdistillation\u201d approach proposed by Hinton et al.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], which imposes parameter tying in ar X iv :1 60 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "One popular technique is based on sparsifying the weight matrices in the neural network, for example, by setting weights whose magnitude falls below a certain threshold to zero [1] or based on the second-derivative of the loss function in the \u201coptimal brain damage\u201d procedure [18].", "startOffset": 177, "endOffset": 180}, {"referenceID": 17, "context": "One popular technique is based on sparsifying the weight matrices in the neural network, for example, by setting weights whose magnitude falls below a certain threshold to zero [1] or based on the second-derivative of the loss function in the \u201coptimal brain damage\u201d procedure [18].", "startOffset": 276, "endOffset": 280}, {"referenceID": 0, "context": "[1] demonstrate that up to two-thirds of the weights of the feed-forward network can be set to zero without incurring any loss in performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": ", by introducing bottleneck layers [19] or through a low-rank matrix factorization layer [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": ", by introducing bottleneck layers [19] or through a low-rank matrix factorization layer [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "[21] which uses a combination of singular value decomposition (SVD) and vector quantization to compress acoustic models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 9, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 21, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 8, "context": "[9], wherein we jointly factorize both recurrent and (non-recurrent) inter-layer weight matrices in the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The initial model (Figure (a)) is compressed by jointly factorizing recurrent (W l h) and inter-layer (W l x) matrices, using a shared recurrent projection matrix (P ) [3] (Figure (b)).", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "We jointly compress the recurrent and inter-layer matrices corresponding to a specific layer l by determining a suitable recurrent projection matrix [3], denoted by P l \u2208 Rrl\u00d7N l , of rank r < N l such that, W l h = Z hP l and W l x = Z xP , thus allowing us to re-write (1) and (2) as, ht = \u03c3(W l\u22121 x h l\u22121 t + Z l hP ht\u22121 + b ) (3)", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 22, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 23, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 2, "context": "Using the notation in [3], note that the recurrent-weight matrix W l h in the case of the LSTM is the concatenation of the four gate weight matrices, obtained by stacking them vertically:", "startOffset": 22, "endOffset": 25}, {"referenceID": 22, "context": "have demonstrated that deep LSTM-based AMs trained to predict either contextindependent (CI) phoneme targets [23] or context-dependent (CD) phoneme targets [24] approach state-of-the-art performance on speech tasks.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "have demonstrated that deep LSTM-based AMs trained to predict either contextindependent (CI) phoneme targets [23] or context-dependent (CD) phoneme targets [24] approach state-of-the-art performance on speech tasks.", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "These systems have two important characteristics: in addition to the CI or CD phoneme labels, the system can also hypothesize a \u201cblank\u201d label if it is unsure of the identity of the current phoneme, and the systems are trained to optimize the connectionist temporal classification (CTC) criterion [25] which maximizes the total probability of correct label sequence conditioned on the input sequence.", "startOffset": 296, "endOffset": 300}, {"referenceID": 22, "context": "More details can be found in [23, 24].", "startOffset": 29, "endOffset": 37}, {"referenceID": 23, "context": "More details can be found in [23, 24].", "startOffset": 29, "endOffset": 37}, {"referenceID": 22, "context": "Following [23], our baseline model is thus a CTC model: a five hidden layer RNN with 500 LSTM cells in each layer, which predicts 41 CI phonemes (plus \u201cblank\u201d).", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "Our systems are trained using distributed asynchronous stochastic gradient descent with a parameter server [26].", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "Since our goal is to build a recognizer to run efficiently on mobile devices, we minimize the size of the decoder graph used for recognition, following the approach outlined in [7]: we perform an additional pruning step to generate a much smaller first-pass language model (69.", "startOffset": 177, "endOffset": 180}, {"referenceID": 23, "context": "Following [24], we stabilize CTC training by stacking together 8 consecutive speech frames (7 right context frames); only every third stacked frame is presented as an input to the network.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "We presented a technique to compress RNNs using a joint factorization of recurrent and inter-layer weight matrices, generalizing previous work [9].", "startOffset": 143, "endOffset": 146}], "year": 2017, "abstractText": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.", "creator": "LaTeX with hyperref package"}}}