{"id": "1603.08233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2016", "title": "Evolution of active categorical image classification via saccadic eye movement", "abstract": "Pattern recognition and classification is a central concern for modern information processing systems. In particular, one key challenge to image and video classification has been that the computational cost of image processing scales linearly with the number of pixels in the image or video. Here we present an intelligent machine (the \"active categorical classifier,\" or ACC) that is inspired by the saccadic movements of the eye, and is capable of classifying images by selectively scanning only a portion of the image. We harness evolutionary computation to optimize the ACC on the MNIST hand-written digit classification task, and provide a proof-of-concept that the ACC works on noisy multi-class data. We further analyze the ACC and demonstrate its ability to classify images after viewing only a fraction of the pixels, and provide insight on future research paths to further improve upon the ACC presented here.", "histories": [["v1", "Sun, 27 Mar 2016 16:36:43 GMT  (585kb,D)", "http://arxiv.org/abs/1603.08233v1", "10 pages, 5 figures, submitted to PPSN 2016 conference"], ["v2", "Thu, 16 Jun 2016 21:00:53 GMT  (292kb,D)", "http://arxiv.org/abs/1603.08233v2", "10 pages, 5 figures, to appear in PPSN 2016 conference proceedings"]], "COMMENTS": "10 pages, 5 figures, submitted to PPSN 2016 conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["randal s olson", "jason h moore", "christoph adami"], "accepted": false, "id": "1603.08233"}, "pdf": {"name": "1603.08233.pdf", "metadata": {"source": "CRF", "title": "Evolution of active categorical image classification via saccadic eye movement", "authors": ["Randal S. Olson", "Jason H. Moore", "Christoph Adami"], "emails": ["olsonran@upenn.edu", "adami@msu.edu"], "sections": [{"heading": null, "text": "Keywords: active categorical perception, attention-based processing, evolutionary computation, machine learning, supervised classification"}, {"heading": "1 Introduction", "text": "In fact, it is so that we are able to develop ourselves in a way that captures the essence of the class while at the same time tolerating non-essential variations. A means of achieving this goal is the extraction of ingredients. In this sense, the image is transformed into a state in which it proves to be inescapable. In the moment in which it proves to be inescapable, it is necessary that it be a state in which it is a state in which it is a state in which it is itself a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a matter in which it is a state in which it is a state in which it is a matter in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a state in which it is a matter in which it is a state in which it is a state"}, {"heading": "2 Methods", "text": "In this section, we will describe the methods for developing the active categorical classifiers (ACCs), starting with the description of the simulation environment in which the ACC scans and classifies the images, and then outlining the structure and underlying neural architecture of an ACC. Finally, we will describe the evolutionary process by which we have developed the ACCs and the experiments we have performed to evaluate them."}, {"heading": "2.1 Simulation Environment", "text": "We evaluate the ACC on the basis of the MNIST dataset, which is a well-known set of handwritten digits commonly used in supervised image classification research. [8] The MNIST dataset contains 28x28 pixel images of handwritten digits - all with appropriate labels that indicate which digit the image represents (0-9) - and comes in two predefined sets of training and test data (60,000 and 10,000 images, respectively). In this project, we binarize the images so that each pixel with a gray value > 127 is assigned a value of 1, and all others are assigned a value of 0.When we evaluate an ACC, we place it at a random starting point in the 28x28 image and provide it with a maximum of 40 steps to scan the image and assign it a classification. Each simulation step that ACC decides which direction to move in, 2) which class (it is currently classified as an image, 3) and whether it is prepared for its final classification, and only the final classification."}, {"heading": "2.2 Active Categorical Classifier (ACC)", "text": "In Fig. 1, we show the ACC in its natural habitat, traversing a digitized MNIST number. Each ACC has a brain composed of 64 Markov neurons (\"states\"), which are either fire (state = 1) or still (state = 0), and represents sensory inputs from the image, internal memory, and decisions about how to interact with the image. ACC uses nine of these states to view nine pixels of the image in a 3x3 square, and four of the states to search for activated pixels outside its field of vision, with four Raycast sensors projecting over the image, 90%, and 270% angles of the image (green squares in Fig. 1). Raycast sensors make it possible to find the numerical field, even if it extends across four Raycast sensors that project out of the image."}, {"heading": "2.3 Optimization Process", "text": "To create the complex logic embodied by a Markov network, we develop the MNs to maximize classification accuracy on the training images. We use a standard genetic algorithm (GA) to stochastically optimize a population of byte strings [10], which in the simulation described above are deterministically assigned to the MNs that function as ACC's \"artificial brains.\" Due to space constraints, we cannot fully describe MNs here; a detailed description of MNs and their development can be found in [11]. In our experiments, the GA receives a population of 100-byte strings (\"candidates\") of varying lengths (maximum = 10,000 bytes) and evaluates them according to the fitness function in Equation 1. The GA selects the candidates to reproduce itself into the next-generation population using tournament selection, mixing the population of byte strings (\"candidates\") and pitting each byte against only one other."}, {"heading": "2.4 Experiments", "text": "As part of the evolutionary optimization process, the GA selects ACCs that are capable of spatially and temporally classifying MNIST digits. First, we executed 30 replicas of GA with random start populations and unique random seeds, and allowed these replicas to run for 168 hours on a high-performance computer cluster. From these 30 replicas, we identified the ACC with the highest fitness (the \"elite\") and seeded another set of 30 replicas with elite ACC mutants. We ran this second group of replicas for another 168 hours. In the following section, we report on the results of these experiments."}, {"heading": "3 Results", "text": "This year, more than ever before in the history of the city, in which it is so far that it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place, a place, a place, a place,"}, {"heading": "4 Discussion", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "5 Acknowledgments", "text": "We thank David B. Knoester, Arend Hintze and Jeff Clune for their valuable contributions to the development of this project. We also thank the High Performance Computing Center of Michigan State University for using their computing resources. This work was supported in part by the National Science Foundation BEACON Center under the DBI-0939454 Cooperation Agreement, and in part by the National Institutes of Health grants LM009012, LM010098 and EY022300."}], "references": [{"title": "Feature extraction methods for character recognition - A survey", "author": ["O.D. Trier", "A.K. Jain", "T. Taxt"], "venue": "Pattern Recognition 29", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "Denker", "J. S"], "venue": "Neural Computation 1", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Recurrent Models of Visual Attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems. NIPS \u201909", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational modelling of visual attention", "author": ["L. Itti", "C. Koch"], "venue": "Nat Rev Neurosci 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Integrated information increases with fitness in the evolution of animats", "author": ["J. Edlund", "N. Chaumont", "A Hintze"], "venue": "PLoS Comput. Biol. 7", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Predator confusion is sufficient to evolve swarming behaviour", "author": ["R. Olson", "A. Hintze", "F. Dyer", "D. Knoester", "C. Adami"], "venue": "J. Roy. Soc. Interface 10", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Evolution of an artificial visual cortex for image recognition", "author": ["S. Chapman", "D. Knoester", "A. Hintze", "C. Adami"], "venue": "In P. Li\u00f2 et al., ed.: Advances in Artificial Life (ECAL 2013), MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Cognitive systems evolve complex representations for adaptive behavior", "author": ["L. Marstaller", "A. Hintze", "C. Adami"], "venue": "Neural Computation 25", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Introduction to Evolutionary Computing", "author": ["A. Eiben", "J. Smith"], "venue": "Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Evolution of swarming behavior is shaped by how predators attack", "author": ["R.S. Olson", "D.B. Knoester", "C. Adami"], "venue": "arXiv e-print 1310.6012", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Random forests - classification description (March 2016) http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm", "author": ["L. Breiman", "A. Cutler"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A Gramfort"], "venue": "Journal of Machine Learning Research 12", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization of neural networks using DropConnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning. ICML \u201913", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "The dynamics of active categorical perception in an evolved model agent", "author": ["R.D. Beer"], "venue": "Adaptive Behavior 11", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201915). IEEE Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv eprint 1412.6572v3", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I Sutskever"], "venue": "arXiv eprint 1312.6199v4", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One means of achieving this goal is through invariant feature extraction [1], where the image is transformed into feature vectors that may be invariant with respect to a set of transformations, such as displacement, rotation, scaling, skewing, and lighting changes.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "This method can also be used in a hierarchical setting, where subsequent layers extract compound features from features already extracted in lower levels, such that the last layer extracts features that are essentially the classes themselves [2].", "startOffset": 242, "endOffset": 245}, {"referenceID": 2, "context": "In contrast to these \u201cpassive\u201d methods where transformations are applied to the image, we propose an active, attention-based method, where a virtual camera roams over and focuses on particular portions of the image, similar to how our own brain controls the focus of our attention [3].", "startOffset": 281, "endOffset": 284}, {"referenceID": 3, "context": "The method we propose here is inspired by models of visual attention [4], where attention to \u201csalient\u201d elements of an image or scene is guided by the image itself, such that only a small part of the incoming sensory information reaches short-term memory and visual awareness.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Thus, focused attention overcomes the information-processing bottleneck imposed by massive sensory input (which can easily be 10 \u2212 10 bits per second in parallel at the optic nerve [4]), and serializes this stream to achieve near-real-time processing with limited computational requirements.", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "In previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "Independently, we have shown that we can evolve simple spatial classifiers for hand-written numerals in the MNIST data set [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "We evaluate the ACC on the MNIST data set, which is a well-known set of handwritten digits commonly used in supervised image classification research [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 8, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 9, "context": "We use a standard Genetic Algorithm (GA) to stochastically optimize a population of byte strings [10], which deterministically map to the MNs that function as the ACC\u2019s \u201cartificial brains\u201d in the simulation described above.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Due to space limitations, we cannot describe MNs in full detail here; a detailed description of MNs and how they are evolved can be found in [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": ", Gini importance [12]), whereas Panel B shows the pixels that the best active categorical classifier visited most frequently when classifying the MNIST data set.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "4A depicts the most informative pixels (according to Gini importance [12]) for differentiating the classes in the MNIST data set with a Random Forest classifier as implemented in scikit-learn [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "4A depicts the most informative pixels (according to Gini importance [12]) for differentiating the classes in the MNIST data set with a Random Forest classifier as implemented in scikit-learn [13].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "3), we believe that this result is due to the lack of training data rather than any particular limitation of ACCs: Due to computational limitations, we were only able to use 1,000 training images (100 of each class) to optimize the ACCs, while modern techniques use much larger training sets that even include additional variations of the training images [14].", "startOffset": 355, "endOffset": 359}, {"referenceID": 12, "context": "Indeed, when we trained a scikitlearn Random Forest with 500 decision trees [13] on the same limited training set of 1,000 images, it achieves only 88.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "In future work, it will be illuminating to analyze the underlying neural architecture of the evolved ACCs to provide insight into the fundamentals of active categorical perception [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "This method naturally lends itself to video classification, where feature compression will play a crucial role in overcoming the massive data size challenge for real-time classification of moving objects [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 15, "context": "Lastly, recent work has shown that modern deep learning-based image classification techniques tend to be easily fooled because they are trained in a supervised, discriminative manner: they establish decision boundaries that appropriately separate the data they encounter in the training phase, but these decision boundaries also include (and thus mis-classify) many inappropriate data points never encountered during training [16].", "startOffset": 426, "endOffset": 430}, {"referenceID": 16, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 164, "endOffset": 168}, {"referenceID": 15, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 202, "endOffset": 206}, {"referenceID": 17, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 430, "endOffset": 434}, {"referenceID": 16, "context": "Due to their nature, deep neural networks are highly dependent on the training data, and only generalize to new challenges if the aforementioned challenges are similar to those accounted for in the training data [17].", "startOffset": 212, "endOffset": 216}, {"referenceID": 8, "context": "In contrast, heuristicbased machines such as the ACC learn simple, generalizable heuristics for classifying images that encode the conceptual representation [9] of the objects in question, and should not be so easily fooled.", "startOffset": 157, "endOffset": 160}], "year": 2017, "abstractText": "Pattern recognition and classification is a central concern for modern information processing systems. In particular, one key challenge to image and video classification has been that the computational cost of image processing scales linearly with the number of pixels in the image or video. Here we present an intelligent machine (the \u201cactive categorical classifier,\u201d or ACC) that is inspired by the saccadic movements of the eye, and is capable of classifying images by selectively scanning only a portion of the image. We harness evolutionary computation to optimize the ACC on the MNIST hand-written digit classification task, and provide a proof-of-concept that the ACC works on noisy multi-class data. We further analyze the ACC and demonstrate its ability to classify images after viewing only a fraction of the pixels, and provide insight on future research paths to further improve upon the ACC presented here.", "creator": "LaTeX with hyperref package"}}}