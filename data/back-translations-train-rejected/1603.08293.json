{"id": "1603.08293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis", "abstract": "Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle high-dimensional data. However, due to the high computational complexity of its eigen decomposition solution, it hard to apply PCA to the large-scale data with high dimensionality. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work used a greedy strategy to solve the eigen vectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-greedy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal component analysis.", "histories": [["v1", "Mon, 28 Mar 2016 03:37:26 GMT  (399kb)", "http://arxiv.org/abs/1603.08293v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["feiping nie", "heng huang"], "accepted": false, "id": "1603.08293"}, "pdf": {"name": "1603.08293.pdf", "metadata": {"source": "CRF", "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis", "authors": ["Feiping Nie", "Heng Huang"], "emails": ["feipingnie@gmail.com,", "heng@uta.edu"], "sections": [{"heading": null, "text": "In fact, it is such that most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "II. RELATED WORK", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "III. PRINCIPAL COMPONENT ANALYSIS WITH NON-GREEDY L21-NORM MAXIMIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. L21-norm principal component analysis", "text": "Motivated by Eq. (2), we propose to solve the following problem: Max WTW = In fact, no case seems theoretically suitable to solve the problem (2). (3) The PCA-L21 maximizes the L2 standard instead of the square L2 standard in PCA, and thus the robustness of the outliers is also improved. (8) Similar to R1-PCA and PCA-L1, PCA-L21, PCA-L21 is an invariant method. (4) The PCA-L21 is also an invariant rotation method. (4) and the problem (5) are closely related to the analysis of R1-PCA-L1, PCA-L21 is an invariant method."}, {"heading": "B. Efficient algorithm to solve the general L21-norm maximization problem", "text": "Consider a general L21 norm maximization problem as follows: max v-Cf (v) is an arbitrary vector output function, and v-C is an arbitrary constraint. We assume that the target in problem (13) has an upper limit. We rewrite the problem (13) as the following problem: max v-Cf (v) + i-Output function, and v-C is an arbitrary constraint. (14) We assume that the target in problem (13) has an upper limit. (13) We rewrite the problem (13) as the following problem: max v-Cf (v) + i-Output-Output-Output-Output-Output-Output function Output-v is an arbitrary function, and i-Output-Output-Output-Output function Output-v is an arbitrary function, and i-Output-function Output-i is a function, and i-function Output-function Output-v is an arbitrary function."}, {"heading": "C. Non-greedy maximization algorithm to solve L21-Norm Principal component analysis", "text": "Obviously, the proposed problem (8) is a specific case of the problem (13), so we can use the proposed problem (WTW = T = T) to achieve the objective of the L21 standard PCA.In algorithm 1, step 2 is the key step. So, to solve the problem (8), the key step is the solution of the following problem: max. WTW = 1 PCTi W Txi, (22) where the vector (WT xi xi xi xi xi xi xi xi xi) 2 = 0; 0 if WTxi (WTxi) 2 = 0. (23) Denoting the matrix M = 1 xixi (Z) is the matrix M = 1 xi\u03b1 T i. (22) We can rewrite the problem as: max WTW = ITr (WTM). (24) Suppose the SVD result of M is M = Uxi V T, then Tr (WTM)."}, {"heading": "D. Kernel and tensor extensions of L21-norm PCA", "text": "Similar to the traditional PCA, the proposed PCA-L21 is also a linear method and is not suitable for processing data under the non-Gaussian distribution. A popular technique to address this problem is to extend the linear method to the core method. Obviously, the PCA-L21 is invariant to rotation and displacement, so that this linear method meets the conditions of the general kerelization framework in [17]. Thus, the PCA-L21 can be nuclearized with this framework. Specifically, the given data is transformed by KPCA [18], and then algorithm 2 is executed on the basis of the transformed data. Another problem of the PCA is that it can only process data points with vector format."}, {"heading": "IV. THE EXTENSION OF OUR ALGORITHM FOR GENERAL MAXIMIZATION PROBLEM", "text": "In addition to solving the L21 standard maximization problem in the section above to provide the useful and efficient algorithm for related research problems, we expand our idea to solve the more general maximization problem as follows: max v-Cf (v) + \u2211 ihi (gi (v) is an arbitrary scatter output function, gi (v) (for each i) is an arbitrary scatter, vector or matrix output function, and hi (for each i) is an arbitrary convex function, v (v) is an arbitrary constraint function. We assume that the goal in problem (28) proposes an iterative algorithm to solve the problem (13). The algorithm is described in algorithm 3. Similar to algorithm 1, in each iteration, the solution is updated by the current solution v, and the solution is updated with the solution."}, {"heading": "V. EXPERIMENTS", "text": "In this section we will present the experimental results to demonstrate the efficacy of the proposed PCA-L21 compared to conventional PCA, R1-PCA and PCA-L1."}, {"heading": "A. Reconstruction errors with occlusions", "text": "We use six image benchmark datasets to conduct our experiments. A brief description of the datasets is shown in Table I, and the samples from each dataset are shown in Figure 1. In each dataset, we randomly select 10, 20, 30 percent images, and each selected image is masked with a randomly placed square. The width of these squares corresponds to half of the image width. 2If x = 0, then 0 is a subgradient of the function of x-2, so in all cases it is the gradient or subgradient of the function of x-2 that is the gradient or subgradient of the function. 6We use the following reconstruction error to measure the quality of the dimensionality reduction methods."}, {"heading": "B. Reconstruction errors with noise images", "text": "In this experiment two picture data sets XM2VTS and Coil20 are used. For each data set we add 10, 20, 30 percent pictures from the Palm picture set as noise images. Some samples from the Palm data set are used in Figure 5.We use the following reconstruction error to measure the quality of the dimensionality reduction methods: e (m) = 1nn x i = 1 x xorgi \u2212 WW Txorgi-L1 or PCA-L21, whereby n is the number of training data (without the image data from the Palm data set) PCA PCA PCA PCA PCA PCA PCA PCA PCA PCA PCA PCA PCA PC1-1 PCA PCA PC1-1 PCA PCA PC1-1 PCA PC1-1-1 PCA PC1-1 PCA PC1-1-1 PCA PC1-1-1 PCA PC1-1 PC1 PC1-1 PCA PC1-1 PC1-1 PC1 PC1-1 PCA PC1-1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1 PC1 PC1-1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC1 PC"}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, a main component analysis with the maximization of the L21 standard was proposed, which is theoretically associated with the minimization of the reconstruction error and is therefore better suited for the main component analysis than the maximization of the L21 standard proposed in [11]. To avoid the greedy strategy used in [11] to solve the maximization problem of the L1 standard, we propose an efficient optimization algorithm to solve a more general maximization problem of the L21 standard that is not greedy and is guaranteed to lead to a local solution. In addition, we are expanding our algorithm to solve the more general maximization problem from which solutions for many similar statistical learning models can be derived. Experimental results from real data sets show that the proposed method for the main component analysis is effective and always achieves smaller reconstruction errors than the related methods under the same reduced dimension."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was partially supported by nsf-iis 1117965, nsf-ccf 0830780, nsf-dms 0915228, nsf-ccf 0917274."}], "references": [{"title": "Principal Component Analysis,2nd Edition", "author": ["I.T. Jolliffe"], "venue": "New York: Springer-Verlag,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Robust factorization", "author": ["H. Aanas", "R. Fisker", "K. Astrom", "J. Carstensen"], "venue": "IEEE Transactions on PAMI, vol. 24, no. 9, pp. 1215\u20131225, 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A framework for robust subspace learning", "author": ["F. De La Torre", "M. Black"], "venue": "International Journal of Computer Vision, vol. 54, no. 1, pp.  117\u2013142, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "R1-PCA: rotational invariant L1-norm principal component analysis for robust subspace  factorization", "author": ["C.H.Q. Ding", "D. Zhou", "X. He", "H. Zha"], "venue": "ICML, 2006, pp. 281\u2013288.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust principal component  9 analysis: Exact recovery of corrupted low-rank matrices via convex optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Ma"], "venue": "NIPS, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrix Computations, 3rd Edition", "author": ["G.H. Golub", "C.F. van Loan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Robust tensor factorization using r1 norm", "author": ["H. Huang", "C.H.Q. Ding"], "venue": "CVPR, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Principal component analysis based on L1-norm maximization", "author": ["N. Kwak"], "venue": "IEEE Transactions on PAMI, vol. 30, no. 9, pp. 1672\u20131680, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Methods of L1 estimation of a covariance matrix", "author": ["J. Galpin", "D. Hawkins"], "venue": "Computational Statistics & Data Analysis, vol. 5, no. 4, pp. 305\u2013319, 1987.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "L1-norm-based 2DPCA", "author": ["X. Li", "Y. Pang", "Y. Yuan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 38, no. 4, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilinear maximum distance embedding via l1-norm optimization", "author": ["Y. Liu", "Y. Liu", "K.C.C. Chan"], "venue": "AAAI, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust tensor analysis with L1-norm", "author": ["Y. Pang", "X. Li", "Y. Yuan"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 20, no. 2, pp. 172\u2013178, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A general kernelization framework for learning algorithms based on kernel PCA", "author": ["C. Zhang", "F. Nie", "S. Xiang"], "venue": "Neurocomputing, vol. 73, no. 4-6, pp. 959\u2013967, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Signal processing based on multilinear algebra", "author": ["L.D. Lathauwer"], "venue": "Ph.D. dissertation, Faculteit der Toegepaste Wetenschappen. Katholieke Universiteit Leuven, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X. Yuan", "T. Zhang"], "venue": "Technical Report, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Among the dimensionality reduction algorithms, Principal Component Analysis (PCA) [1] is one of the most widely used algorithms due to its simplicity and effectiveness.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Thus, the performance is usually poor when their L1-norm based PCA is combined with K-means clustering [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "To solve this problem, the R1-PCA was proposed with rotational invariant property and demonstrated good performance [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "However, the R1-PCA iteratively performs the subspace iteration algorithm [9] in the high dimensional original space, which is computationally expensive.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "The extension of R1-PCA to tensor version can be found in [10].", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "Recently, a PCA method based on L1-norm maximization was proposed in [11], and a similar work can be found in [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Recently, a PCA method based on L1-norm maximization was proposed in [11], and a similar work can be found in [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "An efficient algorithm was proposed to solve the L1norm maximization problem in [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "More importantly, our new method is theoretically connected to the minimization of reconstruction error, and thus is more suitable for principal component analysis than previous method in [11].", "startOffset": 188, "endOffset": 192}, {"referenceID": 7, "context": "All experimental results on real world data sets show that the proposed method is effective for principal component analysis, and always obtains smaller reconstruction error than the method in [11] under the same reduced dimension.", "startOffset": 193, "endOffset": 197}, {"referenceID": 3, "context": "(1), R1-PCA was proposed to solve the following problem [7]:", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "(2), a recent work named PCA-L1 [11] was proposed to maximize the L1-norm instead of the squared L2norm in traditional PCA by solving the following problem:", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "The work in [11] proposed an iterative algorithm to solve this problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "Contrast to the name of PCA-L1 in [11] that solves problem (5), we call our PCA method with solving problem (8) as PCA-L21.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "It is conjectured in [11] that problem (4) and problem (5) are closely related.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, no theoretical analysis was provided in [11] and it seems not the case according to our extensively experimental results.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Utilizing this general algorithm, we can solve the problem (8) directly without using the greedy strategy as in [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "Contrast to the PCA-L1 algorithm in [11], the PCA-L21 algorithm directly solves the projection matrix W (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Obviously, the PCA-L21 is invariant to rotation and shift, so this linear method satisfies the conditions of the general kernelization framework in [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "Specifically, the given data are transformed by KPCA [18], and then Algorithm 2 is performed on the transformed data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "For simplicity, we only briefly discuss the case of 2D tensor, the higher order tensor cases can be readily extended by replacing the linear operator Wxi with tensor operator [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "(36) based on Algorithm 3 are exactly the classical power method (or subspace iteration method) and the recently proposed truncated power method [20], respectively.", "startOffset": 145, "endOffset": 149}], "year": 2016, "abstractText": "Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle highdimensional data. However, due to the high computational complexity of its eigen decomposition solution, it hard to apply PCA to the large-scale data with high dimensionality. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work used a greedy strategy to solve the eigen vectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-greedy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal component analysis.", "creator": "LaTeX with hyperref package"}}}