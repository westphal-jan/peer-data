{"id": "1603.08318", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Exclusivity Regularized Machine", "abstract": "It has been recognized that the diversity of base learners is of utmost importance to a good ensemble. This paper defines a novel measurement of diversity, termed as exclusivity. With the designed exclusivity, we further propose an ensemble model, namely Exclusivity Regularized Machine (ERM), to jointly suppress the training error of ensemble and enhance the diversity between bases. Moreover, an Augmented Lagrange Multiplier based algorithm is customized to effectively and efficiently seek the optimal solution of ERM. Theoretical analysis on convergence and global optimality of the proposed algorithm, as well as experiments are provided to reveal the efficacy of our method and show its superiority over state-of-the-art alternatives in terms of accuracy and efficiency.", "histories": [["v1", "Mon, 28 Mar 2016 05:58:15 GMT  (30kb)", "https://arxiv.org/abs/1603.08318v1", null], ["v2", "Mon, 16 May 2016 07:02:05 GMT  (28kb)", "http://arxiv.org/abs/1603.08318v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojie guo"], "accepted": false, "id": "1603.08318"}, "pdf": {"name": "1603.08318.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xiaojie Guo"], "emails": ["xj.max.guo@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.08 318v 2 [cs.L G] 16 May 2"}, {"heading": "1 Introduction", "text": "The question that arises is to what extent it is actually a \"new\" or \"new\" knowledge that can be learned from the training data on invisible instances. (...) The hypothesis that has been obtained, which is also known as \"classifier,\" is \"good\" if it is possible to generalize the \"knowledge\" that has been learned from the training data on invisible instances. (...) The hypothesis that has been obtained, which is also known as \"classifier,\" is \"good.\" (...)"}, {"heading": "2 Exclusivity Regularized Machine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Definition and Formulation", "text": "It is natural that the diversity in the metrics studied will lead to an irreversible impact between the individual models. (W) It is natural that the diversity between the individual models in the various models of efficiency.1Prior to that, we focus on the concept of diversity, even though diversity does not have a formal definition. (W) It is natural that the diversity among the metrics studied is the diversity between the individual models. (W) It is so that we introduce the diversity between our designed regulators. (W) We focus on the concept of diversity, even though diversity does not have a formal definition. (W) It is that the diversity among the metrics studied is. (W) We are critical in order to achieve the diversity between our designed regulators, we focus first on the concept of diversity. (W) Although diversity is not the one that has a formal definition."}, {"heading": "2.2 Optimization", "text": "With the trick that 1 \u2212 (xi) Twc + bc) yi = yiyi \u2212 (xi) Q \u2212 (xi) Twc + bc) yi = yi (yi \u2212 (\u03c6 (xi) Twc + bc))), we present auxiliary problems eci: = yi \u2212 (\u03c6 (xi) Twc + bc). Subsequently, the minimization of (5) in: argmin {W, b) 1 2 \u00b7 WT 21.2 + (Y) p + s. t P = W; E = Y \u2212 (X TP + 1bT), (6), where the minimization of (X) RM \u00b7 N: = (x2), \u03c6 (xN), c), ec RN: = [ec1, e c 2, e c)."}, {"heading": "3 Theoretical Analysis", "text": "First, we come to the concept of loss of ERM (5), which accesses the total penalty of the basic learners as follows: \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" - \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"(Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" - \"-\" Q \"-\" Q \"=\" Q \"-\" - \"Q\" - \"Q\" = \"Q\" - \"Q\" = \"Q\" - \"-\" Q \"=\" Q \"-\" - \"-\" Q \"=\" Q \"-\" - \"Q\" - \"-\" Q \"=\" Q \"-\" - \"Q\" - \"-\" Q \"=\" Q \"-\" - \"Q\" - \"-\" Q \"=\" Q \"-\" Q \"=\" - \"Q\" - \"-\" Q \"-\" - \"-\" Q \"-\" - \"Q\" - \"-\" - \"Q\" - \"Q\" = \"Q\" - \"-\" - \"-\" Q \"-\" - \"Q\" - \"-\" Q \"-\" - \"Q\" - \"-\" Q \"-\" - \"-\" Q \"-\" - \"Q\" - \"Q -\" - \"Q\" - \"-\" - \"-\" - \"Q -\" Q \"-\" - \"-\" Q - \"Q\" - \"-\" Q - \"-\" Q - \"-\" - \"Q -\" - \"-\" Q - \"Q -\" Q - \"-\" - \"Q -\" - \"Q -\" - \"-\" Q - \"Q -\" - \"Q -\" - \"-\" Q \"-\" - \"-\" Q - \"-\" - \"Q -\" - \"-\" Q - \"Q"}, {"heading": "4 Experimental Verification", "text": "We use 9 popular benchmark data sets from different sources for performance evaluation: including case studies (N = 208, M = 60), German (1, 000, 24), Australian (690, 14), ijcnn1 (49, 990, 22), heart (270, 13), ionosphere (351, 34), diabetes (768, 8), liver (345, 6) and splice (1, 000, 60).3 All experiments are performed on a machine with 2.5 GHz CPU and 64G RAM.Parameter Effect Here we evaluate ERMC's training and test errors (C = 5, 10} the number of components) against different values in the range [0.05, 4]. All results shown in this experiment are averaged over 10 independent trials, half of which data from the sonar dataset is used for testing and the other half for testing."}, {"heading": "5 Conclusion", "text": "This work has defined a new measurement of diversity, i.e. exclusivity. The inclusion of the designed regulator with the hinged loss function leads to a new model, namely exclusivity regulated machine. Theoretically, the convergence of the proposed ALM-based algorithm to a global optimal solution is guaranteed. The experimental results of several benchmark data sets compared to the state of the art have shown the clear advantages of our method in terms of accuracy and efficiency. Our framework is ready to provide more comprehensive treatments for further improvement. For example, due to the relationship of 1 to Xr (u, 1) discussed in Sec. 2.1, the scarcity can be promoted to W by extending W to [W, \u03b21], with \u03b2 being a weight coefficient of scarcity. Furthermore, it is difficult to apply the E sub-problem (13) directly with arbitrary determination to half of the E-tasks, whereby it is partially limited to p."}, {"heading": "Acknowledgment", "text": "Xiaojie Guo would like to thank Dr. Ju Sun from Columbia University's Department of Electrical Engineering for his suggestions for this work."}], "references": [{"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Z. Zhou"], "venue": "Boca Raton, FL: Taylor & Francis Group,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A modified finite newton method for fast solution of large scale linear svms", "author": ["S. Keerthi", "D. DeCoste"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 627\u2013650, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Journal of Computer and System Sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, no. 2, pp. 123\u2013140, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "NIPS, pp. 231\u2013238, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Generalization error of ensemble estimators", "author": ["P. Ueda", "R. Nakano"], "venue": "Proceedings of International Conference on Neural Network (ICNN), pp. 90\u201395, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Limits on the majority vote accuracy in classification fusion", "author": ["L. Kuncheva", "C. Whitaker", "C. Shipp", "R. Duin"], "venue": "Pattern Analysis and Applications, vol. 6, no. 1, pp. 22\u201331, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "The random subspace method for constructing decision forests", "author": ["T. Ho"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 8, pp. 832\u2013844, 1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing, vol. 19, no. 9-10, pp. 699\u2013707, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization", "author": ["T. Dietterich"], "venue": "Machine Learning, vol. 40, no. 2, pp. 139\u2013157, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Diversity regularized machine", "author": ["Y. Yu", "Y. Li", "Z. Zhou"], "venue": "IJCAI, pp. 1603\u20131608, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "On the generalization error bounds of neural networks under diversityinducing mutual angular regularization", "author": ["P. Xie", "Y. Deng", "E. Xing"], "venue": "arXiv:1511.07110v1, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent variable modeling with diversity-inducing mutual angular regularization", "author": ["P. Xie", "Y. Deng", "E. Xing"], "venue": "arXiv:1512.07336v1, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Diversity regularized ensemble pruning", "author": ["N. Li", "Y. Yu", "Z. Zhou"], "venue": "ECML PKDD, pp. 330\u2013345, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse regression using mixed norms", "author": ["M. Kowalski"], "venue": "Applied and Computational Harmonic Analysis, vol. 27, no. 3, pp. 303\u2013324, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Y. Zhang", "R. Jin", "S. Hoi"], "venue": "AISTATS, pp. 988\u2013995, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Exclusive feature learning on arbitrary structures via l1,2-norm", "author": ["D. Kong", "R. Fujimaki", "J. Liu", "F. Nie", "C. Ding"], "venue": "NIPS, pp. 1655\u20131663, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD Thesis, Stanford University, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Linearized alternating direction method with adaptive penalty for low rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "NIPS, pp. 695\u2013704, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "New primal svm solver with linear computational cost for big data classifications", "author": ["F. Nie", "Y. Huang", "X. Wang", "H. Huang"], "venue": "ICML, (Beijing, China), 2014. 9", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "ACM SIGKDD, pp. 217\u2013226, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Pegasos: Primal estimated subgradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "ICML, pp. 807\u2013814, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Bundle methods for regularized risk minimization", "author": ["C. Teo", "S. Vishwanathan", "A. Smola", "Q. Le"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 311\u2013365, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Trust region newton method for large-scale logistic regression", "author": ["C. Lin", "R. Weng", "S. Keerthi"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 627\u2013650, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Coordinate descent method for large-scale L2-loss linear svm", "author": ["K. Chang", "C. Hsieh", "C. Lin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1369\u20131398, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C. Hsieh", "K. Chang", "S. Keerthi", "S. Sundararajan", "C. Lin"], "venue": "ICML, pp. 408\u2013415, 2008. 10", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Multiple-class cases can be analogously accomplished by a group of binary classifiers [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Arguably, among existing classifiers, Support Vector Machine (SVM) [2][3] is the most popular one due to its promising performance.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Arguably, among existing classifiers, Support Vector Machine (SVM) [2][3] is the most popular one due to its promising performance.", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Furthermore, p is a constant typically in the range [1, 2] for being meaningful.", "startOffset": 52, "endOffset": 58}, {"referenceID": 1, "context": "Furthermore, p is a constant typically in the range [1, 2] for being meaningful.", "startOffset": 52, "endOffset": 58}, {"referenceID": 3, "context": "Ensemble approaches, with Boosting [4] and Bagging [5] as representatives, make use of this recognition and achieve strong generalization performance.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Ensemble approaches, with Boosting [4] and Bagging [5] as representatives, make use of this recognition and achieve strong generalization performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Error-Ambiguity decomposition [6], Bias-Variance-Covariance decomposition [7] and Strength-Correlation decomposition [8] all confirm the above principle.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Error-Ambiguity decomposition [6], Bias-Variance-Covariance decomposition [7] and Strength-Correlation decomposition [8] all confirm the above principle.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "Error-Ambiguity decomposition [6], Bias-Variance-Covariance decomposition [7] and Strength-Correlation decomposition [8] all confirm the above principle.", "startOffset": 117, "endOffset": 120}, {"referenceID": 8, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 191, "endOffset": 203}, {"referenceID": 13, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 191, "endOffset": 203}, {"referenceID": 14, "context": "The evidence includes Q-statistics measure [9], correlation coefficient measure [9], disagreement measure [10], double-fault measure [11], k-statistic measure [12] and mutual angular measure [13, 14, 15].", "startOffset": 191, "endOffset": 203}, {"referenceID": 12, "context": "One exception is Diversity Regularized Machine [13], which attempts to seek the globally-optimal solution.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "proposed a pruning strategy to improve the performance of DRM [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "It has been verified that, as one of mixed norms, the l1,2 is in nature able to capture some structured sparsity [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "In general, the regression models using such mixed norms can be solved by a modified FOCUSS algorithm [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "[18] introduced the l1,2 regularizer into a specific task, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "The responsibility of the l1,2 regularizer is to enforce the negative correlation among categories [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "[19] utilized l1,2 norm to bring out sparsity at intra-group level in feature selection, and proposed an effective iteratively re-weighted algorithm to solve the corresponding optimization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In this work, besides the view of motivating the l1,2 regularizer, its role in our target problem, say constructing an ensemble of SVMs, is also different with the previous work [17, 18, 19].", "startOffset": 178, "endOffset": 190}, {"referenceID": 17, "context": "In this work, besides the view of motivating the l1,2 regularizer, its role in our target problem, say constructing an ensemble of SVMs, is also different with the previous work [17, 18, 19].", "startOffset": 178, "endOffset": 190}, {"referenceID": 18, "context": "In this work, besides the view of motivating the l1,2 regularizer, its role in our target problem, say constructing an ensemble of SVMs, is also different with the previous work [17, 18, 19].", "startOffset": 178, "endOffset": 190}, {"referenceID": 17, "context": "The functionalities of [18] and [19] are the intra-exclusivity of multiple hypotheses (tasks) and the inter-exclusivity of a single hypothesis respectively, while our principle is the diversity of multiple components of a single ensemble hypothesis.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "The functionalities of [18] and [19] are the intra-exclusivity of multiple hypotheses (tasks) and the inter-exclusivity of a single hypothesis respectively, while our principle is the diversity of multiple components of a single ensemble hypothesis.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "Algorithm 1 is actually a special case of the algorithm proposed in [19].", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "Due to the limited space, we refer readers to [19] for the detailed proof.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "[20, 21] Let H be a real Hilbert space endowed with an inner product \u3008\u00b7, \u00b7\u3009 and a corresponding norm \u2016 \u00b7 \u2016, and any y \u2208 \u2202\u2016x\u2016, where \u2202\u2016 \u00b7 \u2016 denotes the subgradient.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[20, 21] Let H be a real Hilbert space endowed with an inner product \u3008\u00b7, \u00b7\u3009 and a corresponding norm \u2016 \u00b7 \u2016, and any y \u2208 \u2202\u2016x\u2016, where \u2202\u2016 \u00b7 \u2016 denotes the subgradient.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "1 displays the training error and testing error plots of L2 loss ERM with L2 loss PSVM [22] (denoted as L2-PSVM) as reference.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Please note that, for a better view of different settings, the objective plots are normalized into the range [0, 1].", "startOffset": 109, "endOffset": 115}, {"referenceID": 21, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 23, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "tw/\u223ccjlin/libsvmtools/datasets In [22], the authors have revealed via extensive experiments, that PSVM (SVM-ALM) is much more efficient than SVM [23], Pegasos [24], BMRM [25], and TRON [26], PCD [27] and DCD [28].", "startOffset": 208, "endOffset": 212}], "year": 2016, "abstractText": "It has been recognized that the diversity of base learners is of utmost importance to a good ensemble. This paper defines a novel measurement of diversity, termed as exclusivity. With the designed exclusivity, we further propose an ensemble model, namely Exclusivity Regularized Machine (ERM), to jointly suppress the training error of ensemble and enhance the diversity between bases. Moreover, an Augmented Lagrange Multiplier based algorithm is customized to effectively and efficiently seek the optimal solution of ERM. Theoretical analysis on convergence and global optimality of the proposed algorithm, as well as experiments are provided to reveal the efficacy of our method and show its superiority over state-of-the-art alternatives in terms of accuracy and efficiency.", "creator": "LaTeX with hyperref package"}}}