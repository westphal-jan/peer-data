{"id": "1603.08342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes", "abstract": "A hierarchical clustering algorithm based on Gaussian mixture model is presented. The key difference to regular hierarchical mixture models is the ability to store objects in both terminal and nonterminal nodes. Upper levels of the hierarchy contain sparsely distributed objects, while lower levels contain densely represented ones. As it was shown by experiments, this ability helps in noise detection (modelling). Furthermore, compared to regular hierarchical mixture model, the presented method generates more compact dendrograms with higher quality measured by adopted F-measure.", "histories": [["v1", "Mon, 28 Mar 2016 08:54:03 GMT  (130kb,D)", "http://arxiv.org/abs/1603.08342v1", "This article was presented on CORES2015 conferencethis http URL. The final publication is available at Springer viathis http URL"]], "COMMENTS": "This article was presented on CORES2015 conferencethis http URL. The final publication is available at Springer viathis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["{\\l}ukasz p olech", "mariusz paradowski"], "accepted": false, "id": "1603.08342"}, "pdf": {"name": "1603.08342.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes", "authors": ["Lukasz P. Olech", "Mariusz Paradowski"], "emails": ["lukasz.olech@pwr.edu.pl"], "sections": [{"heading": null, "text": "Keywords: background model, outlier detection, noise modelling, hierarchical clustering, hierarchical Gaussian mixing model"}, {"heading": "1 Introduction", "text": "This paper deals with the topic of hierarchical data node clustering, which is a counter-type to flat clustering. Flat cluster approaches create groups without structural connections between them. Hierarchical cluster algorithms create groups and arrange them tree-structured. In such a tree structure (known as dendrograms), all child clusters can be bound to their parent cluster. Clusters without further children are called terminal nodes or tree leaves. Clusters with attached child clusters are called non-terminal or internal nodes. Hierarchical cluster algorithms can be divided into two categories depending on the association with the groups created. The first category represents methods that attach objects only to terminal nodes, and non-terminal nodes of the hierarchy remain empty. These methods are the majority of hierarchical data cluster methods. It is possible to fill an internal node with objects by grouping all objects belonging to the child nodes."}, {"heading": "1.1 Hierarchical approaches to clustering", "text": "One of the earliest approaches to hierarchical clustering is hierarchical agglomerative clustering (HAC) [17]. HAC creates a dendrogram with all objects attached to its leaves. At each hierarchical level, two groups are merged. As a result, the structure created is an unbalanced binary tree. Various merging schemes are available, e.g. the Ward criterion [22], single-link [19], or complete-link [5]. Both binary and non-binary hierarchies can be constructed with various extensions of the k-mean algorithm [10,11]. The use of hierarchical means leads to two major consequences compared to flat k-means. First, the cluster process is much faster because the number of groups in a tree path is much smaller. This is especially important when the number of clusters and the volume of data is high. Second, the general quality of cluster formation tends to be inferior at the same time because hierarchical clusters are not optimized."}, {"heading": "1.2 Clustering in the presence of noise", "text": "There are two common solutions to this problem: the first consists of two stages, e.g. [1]. In the first stage, the data is filtered to detect and remove outliers, and in the second stage, the clusterisation is based only on the accepted data. The second solution is to integrate the noise model directly into the cluster process. Usually, the type or distribution of noise or outliers is not known. Various assumptions regarding these distributions have to be made. As an example, DBSCAN [7] and OPTICS [2] cluster algorithms assume a minimum density of meaningful data. In the probable cluster formation, noise can be modelled directly by suitable compounds, e.g. [3,9]."}, {"heading": "1.3 Problem formulation, motivation and contribution", "text": "The key problem is the formulation of an adequate probability density function (PDFs). There are several forms of the probability density function. Gaussian mixing model is one of the most prominent [9]. Let us define Gaussian mixing model G with n mixing components as follows: G (w, \u00b5, \u03a3) = n x i = 1 wiN (\u00b5i, \u041ai), wi < 0, 1 >, n x i = 1, (1) and N (\u00b5, \u0440) represents the multivariate normal distribution, w = [w1,..., wn], \u00b5 = [\u00b51,..., mixn], sub mixture = [\u04451,..., n]. In such a case, the cluster problem becomes a probability density problem where the PDFs parameters maximize the probability probability probability."}, {"heading": "2 Proposed approach", "text": "The proposed approach is an extension of a hierarchical structure of Gaussian mixture models. At each hierarchical level, an additional mixture component is introduced, the background component. This component is responsible for detecting outliers at a certain level. Unlike all other mixture components, it is not estimated, but inherited directly from the higher hierarchical level. The root level also has this additional component. Its parameters are (by definition) estimated from all available data."}, {"heading": "2.1 Formal model of the hierarchy", "text": "Define the model of hierarchy in a recursive manner: Each parent node has all its child nodes. A tree node T generated from a data set X is defined as: T (X): < n, GB, B X, [T1 (X1),..., Tn (Xn)] >, (3) where: B-n i = 1 Xi = X, i-n [1, n] B-Xi = \u2205, i, j-i [i, n] 6 = j Xi Xj = \u2205 (4) and: n is the maximum number of child nodes (and mixers), GB is the Gaussian mixture model with the background component N (\u00b5B, \u041aB): GB (\u03b1, w, \u00b5B, \u00b5, \u0445 B): GB (\u03b1, \u041aB, \u041aB, \u041aB, \u041aB, \u041aB, \u041aB, \u041aB = \u03b1N (\u00b5B), the difference (\u00b5B), the component < < &ltT, < &ltT, < &ltT, < &ltT, < &ltT, < &ltT, < T, < < T, < < T, < T, < T, <"}, {"heading": "2.2 Hierarchy generation", "text": "First, the top level is created and its parameters are estimated. Later, the child levels are added sequentially in the width-first manner. For each level, the process ends when a stop criterion is reached. This process is similar to that used in the hierarchical approach. It allows a dynamic generation of the hierarchical structure. As shown in the formal model, each level of the hierarchy contains only a subset of data. The top level begins with all data. The expectation maximization method is used to estimate the Gaussian mixture model. Since the proposed method is iterative, stochastical and strongly dependent on the cluster initialization, several cluster re-initializations should be performed. Therefore, the number of cluster re-initializations R and the number of EM iterations N are the parameters. Cluster initialization is based on the selection of different points from the data and sets them as the starting centers of new cluster components."}, {"heading": "3 Experimental verification", "text": "Experimental review of the proposed approach consists of two parts. In the first part, we will give illustrative examples to demonstrate the idea behind the method. Manually prepared toy data sets will be used for visualization purposes. In the second part, we will test the proposed approach using a series of benchmark data sets from the UCI repository [13]. We will select known iris, wine, glass identification and image segmentation data sets that differ in the number of classes, attributes and instances as shown in Table 1. As the mentioned data sets do not contain noise points that we added manually. Noise points are uniformly distributed among the starting points. In each data set, the number of noise points is equal to half of the original points. The proposed approach will be compared with a standard hierarchical structure of the Gaussian mixing method. To compare the obtained results on the benchmark data sets c, we will use c-class metric based on F data sets."}, {"heading": "3.1 Manually generated data with noise \u2013 an illustration", "text": "All the results presented in this section are two-dimensional toy examples. Their sole purpose is to illustrate the behavior of the proposed method. The following examples are presented: 1. three groups with a large central group and a small amount of noise (LC), 2. small circular data clusters with a small amount of noise (LN), 3. small circular data clusters with a large amount of noise (HN).Both the data and cluster results for the toy data sets are in Fig. 1. The method has a certain ability to capture less dense data. These data are appended to the intermediate nodes of the hierarchy, and the additional background model component captures these instances. As a result, they are automatically tied to the node connected to the background component. At the same time, densely distributed data is moved to the bottom of the hierarchy, which can be observed (to some extent) in all the test cases presented."}, {"heading": "3.2 UCI benchmark datasets", "text": "The second part of the experiments deals with clustering of UCI benchmark data sets. Instances of all processed data sets also have both feature vectors and class assignments; feature vectors without class information are used in the cluster process; the available class assignment is used in the evaluation process.Two methods are compared: (1) the proposed Gaussian outlier model and (2) the classic Gaussian blend model; the first method is referred to as B and the second as G. Both methods are trained with the same expectation maximization routine.Hierarchies of both models are constructed in the same way as the comparative values; two quality estimates are shown: (1) Log liquidity values that match the distribution; (2) f measurements to verify whether the groups produced are meaningful. Experiments performed consider the aforementioned quality estimators when W parameters vary between 2 and 10."}, {"heading": "4 Summary", "text": "A hierarchical grouping method is presented, which has the ability to bind objects to both terminal and non-terminal nodes. It is an extension of the classic Gaussian mixing model. An additional component is added to the mix, which is responsible for outlier modeling. Parameters of this mixing component are not estimated, but inherited directly from higher hierarchical levels. Experiments conducted show that the proposed modification treats a portion of the data as a sparse representation of significant objects. Although the upper hierarchy levels consist of sparsely distributed data, this can be used for noise or outlier modeling. Comparison between regular hierarchical GMM and hierarchical GMM with proposed changes shows that the background component helps to improve the quality of short hierarchies in real data sets with random noise."}], "references": [{"title": "OPTICS: ordering points to identify the clustering structure", "author": ["M. Ankerst", "M.M. Breunig", "H. Kriegel", "J. Sander"], "venue": "Proc. of ACM SIGMOD International Conference on Management of Data. pp. 49\u201360", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Linear flaw detection in woven textiles using model-based clustering", "author": ["J.G. Campbell", "C. Fraley", "F. Murtagh", "A.E. Raftery"], "venue": "Pattern Recognition Letters 18(14), 1539\u20131548", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Supervised learning of semantic classes for image annotation and retrieval", "author": ["G. Carneiro", "A.B. Chan", "P.J. Moreno", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 29(3), 394\u2013410", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient algorithm for a complete link method", "author": ["D. Defays"], "venue": "Comput. J. 20(4), 364\u2013366", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Maximum Likelihood from Incomplete Data via the EM Algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 39(1), 1\u201338", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1977}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. Kriegel", "J. Sander", "X. Xu"], "venue": "Proc. of the 2nd Internat. Conf. on Knowledge Discovery and Data Mining (KDD). pp. 226\u2013231", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Unsupervised learning of finite mixture models", "author": ["M.A.T. Figueiredo", "A.K. Jain"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 24(3), 381\u2013396", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Model-based clustering, discriminant analysis, and density estimation", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Journal of the American Stat. Assoc. 97(458), pp. 611\u2013631", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Algorithm as 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society. Series C 28(1), pp. 100\u2013108", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1979}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters 31(8), 651\u2013666", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast and effective text mining using linear-time document clustering", "author": ["B. Larsen", "C. Aone"], "venue": "Proc. of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 16\u201322", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Hierarchical gaussian mixture model for speaker verification", "author": ["M. Liu", "E. Chang", "B. Dai"], "venue": "7th International Conference on Spoken Language Processing", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "On a test of whether one of two random variables is stochastically larger than the other", "author": ["H.B. Mann", "D.R. Whitney"], "venue": "The Annals of Mathematical Statistics 18(1), pp. 50\u201360", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1947}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["T.P. Minka"], "venue": "Proc. of the 17th Conference in Uncertainty in Artificial Intelligence. pp. 362\u2013369", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of recent advances in hierarchical clustering algorithms", "author": ["F. Murtagh"], "venue": "Comput. J. 26(4), 354\u2013359", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1983}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "A.W. Moore"], "venue": "Proc. of the 17th International Conference on Machine Learning. pp. 727\u2013734", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "SLINK: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "Comput. J. 16(1), 30\u201334", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1973}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "Proc. of Workshop on Text Mining, 6th ACM SIGKDD International Conference on Data Mining (KDD\u201900) pp. 109\u2013110", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient greedy learning of gaussian mixture models", "author": ["J.J. Verbeek", "N.A. Vlassis", "B.J.A. Kr\u00f6se"], "venue": "Neural Computation 15(2), 469\u2013485", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J.H. Ward"], "venue": "Journal of the American Statistical Association 58(301), 236\u2013244", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1963}, {"title": "Recursive unsupervised learning of finite mixture models", "author": ["Z. Zivkovic", "F. van der Heijden"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 26(5), 651\u2013656", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 14, "context": "One of the earliest approaches of hierarchical clustering is hierarchical agglomerative clustering (HAC) [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Both binary and non-binary hierarchies can be constructed using various extensions of the k-means algorithm [10,11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 9, "context": "Both binary and non-binary hierarchies can be constructed using various extensions of the k-means algorithm [10,11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "x-means algorithm [18].", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": ", [14].", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "The milestone in probabilistic clustering was the formulation of the expectation maximization (EM) algorithm [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Hierarchical setup of mixture models can be trained using modified EM [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Exemplary, DBSCAN [7] and OPTICS [2] clustering algorithms assume a minimum density of the meaningful data.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Exemplary, DBSCAN [7] and OPTICS [2] clustering algorithms assume a minimum density of the meaningful data.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": ", [3,9].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": ", [3,9].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": "Gaussian mixture model is one of the most prominent [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 18, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 20, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 7, "context": "Gaussian mixture model fits to data distributed among several clusters, but does not model outliers [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": ", [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Similar to noise modelling [3,9] we add an additional mixture component to the mixture model.", "startOffset": 27, "endOffset": 32}, {"referenceID": 7, "context": "Similar to noise modelling [3,9] we add an additional mixture component to the mixture model.", "startOffset": 27, "endOffset": 32}, {"referenceID": 17, "context": "This process is similar to the one used in hierarchical kmeans approach [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "In order to compare the obtained results on the benchmark datasets we use a metric based on F-measure [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Moreover we conducted the Wilcoxon rank-sum test [15] on calculated F-measure in order to show the statistic significance of the obtained results.", "startOffset": 49, "endOffset": 53}], "year": 2016, "abstractText": "A hierarchical clustering algorithm based on Gaussian mixture model is presented. The key difference to regular hierarchical mixture models is the ability to store objects in both terminal and nonterminal nodes. Upper levels of the hierarchy contain sparsely distributed objects, while lower levels contain densely represented ones. As it was shown by experiments, this ability helps in noise detection (modelling). Furthermore, compared to regular hierarchical mixture model, the presented method generates more compact dendrograms with higher quality measured by adopted F-measure.", "creator": "LaTeX with hyperref package"}}}