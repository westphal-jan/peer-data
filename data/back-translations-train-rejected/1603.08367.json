{"id": "1603.08367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "abstract": "Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.", "histories": [["v1", "Mon, 28 Mar 2016 12:06:49 GMT  (183kb,D)", "http://arxiv.org/abs/1603.08367v1", "Seethis http URLfor the authoritative version"]], "COMMENTS": "Seethis http URLfor the authoritative version", "reviews": [], "SUBJECTS": "cs.LG cs.CG cs.CV cs.NE", "authors": ["markus thom", "g\\\"unther palm"], "accepted": false, "id": "1603.08367"}, "pdf": {"name": "1603.08367.pdf", "metadata": {"source": "CRF", "title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "authors": ["Markus Thom", "Aapo Hyv\u00e4rinen"], "emails": ["MARKUS.THOM@UNI-ULM.DE", "GUENTHER.PALM@UNI-ULM.DE"], "sections": [{"heading": "1. Introduction", "text": "Thrift is a concept of efficiency in neural networks and exists in this context in two variants (Laughlin and Sejnowski, 2003).The sparse activity property means that only a small fraction of the neurons are active at all time.The sparse connectivity property means that each neuron is associated with only a limited number of other neurons, both characteristics observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms.A remarkable result was achieved by the sparse coding model of Olshausen and Field (1996).Given small patches of images of nature scenes, the model is able to produce gabor-like filters that resemble the properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; each and Gallant, 2000)."}, {"heading": "1.1 Hoyer\u2019s Normalized Sparseness Measure", "text": "A normalized, economical measure based on the ratio of the L1 or Manhattan standard and the L2 or Euclidean standard of a vector was proposed by Hoyer (2004), \u03c3: Rn\\ {0} \u2192 [0, 1], x 7 \u2192 n-x-x-x-2, where higher values indicate sparser vectors. This measure is well defined because 0 and all x-seness-seness values are also scalable. Since the composition of differentiable functions can be differentiated across their entire domain, this sparse measure differs for all x-seness values (3 and all x-seness-seness values), this economical measure does not meet all Hurley and Rickard criteria (2009) except for Dalton's Fourth Law, which states that the economical prediction of a vector should be identical to the economical prediction of this crucial multiple vector constellation."}, {"heading": "1.2 Contributions of this Paper", "text": "This paper improves previous work in the following ways. Section 2 proposes a simple algorithm for performing economical, assertive projections in relation to Hoyer's thriftiness measurement. In addition, an improved algorithm is proposed and compared with Hoyer's original algorithm. As the projection itself is differentiable, it is the ideal tool for achieving thriftiness in gradient-based learning, which is exploited in Section 3, where thriftiness projection is used to obtain a classifier that exhibits both sparse activity and sparse connectivity naturally. The benefits of these two key properties are demonstrated in a real-life classification problem, demonstrating that thriftiness acts as a regulator and improves classification outcomes. The final sections provide an overview of related concepts and conclude this paper.On the theoretical side, a first rigorous and mathematically satisfying analysis of the properties of the thriftiness, penetrating projection is provided."}, {"heading": "2. Algorithms for the Sparseness-Enforcing Projection Operator", "text": "The projection onto a set is of particular interest in this paper and is discussed in greater detail. (...) The projection onto a set is a basic concept, for example see English (2001): This is a definition that is elaborated in greater detail. (...) The definition 1 Let x-Rn and \u2205, M-Rn. \"(...) Then every point in the projection is on M.\" (...) If there is exactly one point in the projection, then the projection on all x-Rn (...) is used as an abbreviation. (...) Since Rn is finite dimensional, projM (...) is not empty for all x-Rn when and only when M is closed, and projM (...) is a singleton for all x-Rn, and only when M is closed and convex. (...) In literature, the elements from projM (...) are also referred to as the best approximations of sentences that fulfill certain symmetries."}, {"heading": "2.1 Alternating Projections", "text": "It is not as if it is an \"imperfect\" solution, but an \"imperfect\" solution. (...) It is not as if it is an \"imperfect\" solution. (...) It is not as if it is an \"imperfect\" solution. (...) It is as if it is an \"imperfect\" solution. (...) It is as if it is an \"imperfect\" solution. (...) It is as if it is an \"imperfect\" solution. (...) It is as if it is an \"imperfect\" solution. (...) It is as if it is an \"imperfect\" solution. (...) It is as if it is an imperfect solution. (...) It is as if it is an imperfect solution. (...) It is as if it is a solution. (...) It is as if it is an imperfect solution."}, {"heading": "2.2 Optimized Variant", "text": "Due to the permutation invariance of the quantities involved in the projections, it is sufficient to sort the vector to be projected onto D. This ensures that the work vector resulting from subsequent projections is also sorted once. Therefore, when using algorithm 2 for projections on C, no additional sorting needs to be done. In addition, this has the side effect that the non-vanishing entries of the work vector are always concentrated in its first entries. Therefore, all relevant information can always be stored in a small array divided into increments, to which access is more efficient than on a large sparse array. Furthermore, the index set I of the non-vanishing entries in the work vector is always in the form I = {1, d.}, where d is the number of non-zero centers. Algorithm 3 is a variant of algorithm 1, in which these optimizations were applied, and in which the explicit formulas for the intermediate projections were used."}, {"heading": "2.3 Comparison with Hoyer\u2019s Original Algorithm", "text": "In fact, it is so that it is a matter of a way in which people go in search of themselves in the most diverse regions of the world. (...) In fact, it is so that they are able to surpass themselves. (...) \"It is so, as if.\" (...) \"It is so, as if.\" (...) \"It is so, as if.\" (...) \"It is so, as if.\" (...) \"(...)\" It is so, as if. \"(...)\" (...) \"It is so, as if.\" (...) \"(...)\" It is so, as if. \"(...)\" (...) \"(...\" (...) \"(\" (\")\" (\"(\"). \"(\" (\"(\"). \"(\" (\"(\"). \"(\") \"(\" (\").\" (\")\" (\"(\"). \"(\") (\"(\") \"(\") (\") (\") (\") (\" (\").\" (\") (\") (\"(\") (\") (\"). \"(\" (\") (\") (\") (\"). \"(\" (\") (\") (\") (\")."}, {"heading": "2.4 Function Definition and Differentiability", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3. Sparse Activity and Sparse Connectivity in Supervised Learning", "text": "The sparsely enforceable projection operator can be used almost anywhere as a vector-value function \u03c0, which can be differentiated almost anywhere, see Section 2.4. This section proposes a hybrid of an auto-encoder network and a dual-layer neural network, using the sparse projection as a neural transfer function. The proposed model is called a supervised online encoder (SOAE) and is intended for classification via a neural network that exhibits low activity and sparse connectivity. Due to the analytical properties of the sparsely enforceable projection operator, the model can be optimized end-to-end using gradient-based methods."}, {"heading": "3.1 Architecture", "text": "There is a module for the reconstruction capacities and a module for the classification of capacities. The reconstruction of the module shown on the left side of Figure 4 is characterized by the transformation of a matrix of basics into an internal representation, and then an approximation of the original input sample is made, whereby the product series can be selected to select the sparse activity of the operator in order to perform the projection in relation to the L0 pseudo-standard. This guarantees that the internal representation is sparsely populated and close to u. The reconstruction is achieved as in a linear generative model, by multiplying the basics with the internal representation."}, {"heading": "3.2 Learning Algorithm", "text": "The proposed optimization method to minimize the objective function of ESOAE is projected gradient descent (Bertsekas, 1999), where each update of degrees of freedom is followed by the application of the economical projection on the columns of W to assert sparse connectivity. There are theoretical results on the convergence of the projected gradient methods when the projections are performed on convex targets (Bertsekas, 1999), but here the objective for projection is not set convex. Nevertheless, the experiments show that the projected gradient descent is an adequate heuristics in the situation of the SOAE framework to optimize the network parameters. Appendix E performs updating steps for the gradients of the network parameters after each presentation of an input example and associated target vectors. This online learning method leads to faster learning and improves skills over learning (2003, Martinez and Wilson)."}, {"heading": "3.3 Description of Experiments", "text": "This year, the time has come for it to be a purely reactionary project, capable of retaliating."}, {"heading": "3.4 Experimental Results", "text": "In both variants, the desired degree of sparse connectivity in relation to the sparse connectivity of the digital world is necessarily compared only to a limited extent with each other. SOAE-\u03c3 points out the first variant, in which the sparse enforcement of activities is used as a transmission function f in the hidden layer. Targets of sparse activity are measured in relation to the sparse activity of Hoyer in increments of size 05,95] in increments of size 05,5] This variant was then trained using the method of learning described in Section 3.2. For each value of sparse activity, the resulting sparsity of activity is measured with the pseudo-standard after the training."}, {"heading": "3.5 Statistical Analysis and Conclusions", "text": "The procedure follows the suggestions of Pizarro et al. (2002) and Dem\u0161ar (2006) for hypotheses testing and is completed by effect size estimation, as proposed by Grissom (1994) and Acion et al. (2006). For each algorithm, a sample of size 47 was provided, which allows robust analysis results. Firstly, all results were tested for normality using the test developed by Shapiro and Wilk. The resulting test statistics W and p-values are given in Table 1. Since all p-values are large, it cannot be denied that the samples come from normally distributed populations. Therefore, normality is assumed in the remaining parts of this discussion. Next, the test proposed by Levene was applied to determine whether the equality of the variances of the groups is maintained, resulting in a test statistic F = 2.7979 with 7 and 368 degrees of freedom, and thus a p-value of 0.075 hypotheses."}, {"heading": "4. Related Work", "text": "This section discusses work related to the contents of this paper. First, the theoretical foundations of the sparsely enforceable projection operator are discussed, then its application as a neural transfer function to achieve low activity in a classification scenario is associated with alternative approaches, and possible benefits of sparse connectivity are described."}, {"heading": "4.1 Sparseness-Enforcing Projection Operator", "text": "The first great task of this volume is to deal with the question as to the extent to which it is actually about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about which it is about a way in which it is about which it is about which it is about a way in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not only about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "4.2 Supervised Online Auto-Encoder", "text": "In fact, it is such that most of them will be able to put themselves in a situation in which they have to stand in their own way, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they have to put themselves in a situation in which they have to put themselves in the way, in which they have to put themselves, in which they live, in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "5. Conclusions", "text": "This important concept of efficiency was discovered by neuroscientists, and the practical benefit was obtained by the engineers of artificial information processing systems. In this work, Hoyer's sparse measurement of \u03c3, and in particular the projection of arbitrary vectors onto sets in which \u03c3 reaches a constant value, was investigated. Furthermore, it was shown that the proposed algorithm is superior in runtime to Hoyer's original algorithm. Analysis of the theoretical properties of this projection was completed by showing that it is differentiable almost everywhere. As projections on certain constraints are well understood, they are the ideal tool for building systems that can benefit from the sparse constraints."}, {"heading": "Acknowledgments", "text": "The authors thank Patrik O. Hoyer and Xiaojing Ye for passing on the source code of their algorithms and the anonymous reviewers for their valuable comments and feedback. This work was supported by Daimler AG, Germany."}, {"heading": "Appendix A. Notation and Prerequisites", "text": "This appendix fixes the notation and provides the prerequisites for the following appendices. < B = all natural numbers including zero, R = 0 the real numbers and R = 0 the non-negative real numbers. < B = all other vectors denotes the corresponding entry of the vector, i.e. xi = eTi x for x-th Rn. The amount of non-zero entries in a vector is given by the vector given by the L0 pseudo-norm."}, {"heading": "Appendix B. Projections onto Symmetric Sets", "text": "This means that there are no entries above another, and for negative entries, usually the absolute value or the square value is used if the characters of the entries are ignored. Furthermore, the following definition of symmetries to be analyzed is called Definition 6. Then, M is called a permutation invariant if and only if it is a permutation invariant. M is called a permutation invariant if and only if b-invariant is called for all x-M and all x-invariants. In other words, a subset M of the Euclidean space is permutation-invariant if a permutation occurs."}, {"heading": "Appendix C. Proof of Correctness of Algorithm 1 and Algorithm 3", "text": "The purpose of this appendix is to rigorously prove the correctness of algorithm 1 and algorithm 3, which means that it projections on S (1, 2) on S (1, 2) on S (1, 2) on S (2) on H (2) on the target hyperplane H and the target hypersphere K (see Section 2.1) on the intersection of H and K (the intersection of H and K yields a hypercircle L, and the intersection of Rn 0 and H yields a scaled canonical simplicity C. The structure of H and L is analyzed in Section C.1.1 and Section C.1.2 respectively, and the properties of C are analyzed in Section C.2 and Section C.3. These results are then used in Section C.4 to test theorem 2 and theorem."}, {"heading": "Appendix D. Analytical Properties of the Sparseness-Enforcing Projection Operator", "text": "This appendix examines the situations in which \u03c0 0 and \u03c0, as defined in Section 2.4, are distinguished (and therefore continuous), and seeks an explicit expression for their gradients. It is clear that the projection of any point on D as a finite composition of the projections on H, L, C, and LI, in other words, exists an explicit expression for all points x, Rn, and R, if a finite sequence of index records I1,., n) with I j) I j + 1 for j, C, and LI, in other words, h \u2212 1) such a sequence of index records I1,."}, {"heading": "Appendix E. Gradients for SOAE Learning", "text": "The objective function ESOAE is a combination of two similarity scales sR and sC. (<) The degrees of freedom W, Wout, and scout of the SOAE architecture should be optimized by gradient-based methods to minimize these functions. (This appendix contains the sequence information required for reproducing the experiments. (The first statement refers to the reconstruction modalities.) The first statement refers to the gradients sR (x), sR (u) + gT, Rd (n, where g), sR (x), x), x), x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, the gradient of the similarity scales with respect to their first reasoning. (In addition (The sequence is sR), x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, etc. (x)."}], "references": [{"title": "Probabilistic index: An intuitive non-parametric approach to measuring the size of treatment effects", "author": ["L. Acion", "J.J. Peterson", "S. Temple", "S. Arndt"], "venue": "Statistics in Medicine,", "citeRegEx": "Acion et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Acion et al\\.", "year": 2006}, {"title": "Internal representations for associative memory", "author": ["E.B. Baum", "J. Moody", "F. Wilczek"], "venue": "Biological Cybernetics,", "citeRegEx": "Baum et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1988}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, 2nd edition,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "A simple, efficient and near optimal algorithm for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Large scale online learning", "author": ["L. Bottou", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bottou and LeCun.,? \\Q2004\\E", "shortCiteRegEx": "Bottou and LeCun.", "year": 2004}, {"title": "Differentiable sparse coding", "author": ["D.M. Bradley", "J.A. Bagnell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bradley and Bagnell.,? \\Q2009\\E", "shortCiteRegEx": "Bradley and Bagnell.", "year": 2009}, {"title": "Projection onto a simplex", "author": ["Y. Chen", "X. Ye"], "venue": "Technical Report arXiv:1101.6081v2, University of Florida,", "citeRegEx": "Chen and Ye.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Ye.", "year": 2011}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D.C. Cire\u015fan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Schmidhuber"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2012}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Training invariant support vector machines", "author": ["D. DeCoste", "B. Sch\u00f6lkopf"], "venue": "Machine Learning,", "citeRegEx": "DeCoste and Sch\u00f6lkopf.,? \\Q2002\\E", "shortCiteRegEx": "DeCoste and Sch\u00f6lkopf.", "year": 2002}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar.,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "For most large underdetermined systems of linear equations the minimal `1-norm solution is also the sparsest solution", "author": ["D.L. Donoho"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "The estimation of Pr(Y < X) in the normal case", "author": ["F. Downton"], "venue": "Technometrics, 15(3):551\u2013558,", "citeRegEx": "Downton.,? \\Q1973\\E", "shortCiteRegEx": "Downton.", "year": 1973}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function", "author": ["R.A. Dunne", "N.A. Campbell"], "venue": "In Proceedings of the Australasian Conference on Neural Networks,", "citeRegEx": "Dunne and Campbell.,? \\Q1997\\E", "shortCiteRegEx": "Dunne and Campbell.", "year": 1997}, {"title": "An algorithm for restricted least squares regression", "author": ["R.L. Dykstra"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Dykstra.,? \\Q1983\\E", "shortCiteRegEx": "Dykstra.", "year": 1983}, {"title": "An algorithm for least squares projections onto the intersection of translated, convex cones", "author": ["R.L. Dykstra", "J.P. Boyle"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Dykstra and Boyle.,? \\Q1987\\E", "shortCiteRegEx": "Dykstra and Boyle.", "year": 1987}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Technical Report arXiv:1001.0736v1,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K. Funahashi"], "venue": "Neural Networks,", "citeRegEx": "Funahashi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi.", "year": 1989}, {"title": "Neural networks and the bias/variance dilemma", "author": ["S. Geman", "E. Bienenstock", "R. Doursat"], "venue": "Neural Computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Gregor and LeCun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Probability of the superior outcome of one treatment over another", "author": ["R.J. Grissom"], "venue": "Journal of Applied Psychology,", "citeRegEx": "Grissom.,? \\Q1994\\E", "shortCiteRegEx": "Grissom.", "year": 1994}, {"title": "Learning sparse representations by non-negative matrix factorization and sequential cone programming", "author": ["M. Heiler", "C. Schn\u00f6rr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Heiler and Schn\u00f6rr.,? \\Q2006\\E", "shortCiteRegEx": "Heiler and Schn\u00f6rr.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "At what points is the projection mapping differentiable", "author": ["J.-B. Hiriart-Urruty"], "venue": "The American Mathematical Monthly,", "citeRegEx": "Hiriart.Urruty.,? \\Q1982\\E", "shortCiteRegEx": "Hiriart.Urruty.", "year": 1982}, {"title": "Multiple Comparison Procedures", "author": ["Y. Hochberg", "A.C. Tamhane"], "venue": null, "citeRegEx": "Hochberg and Tamhane.,? \\Q1987\\E", "shortCiteRegEx": "Hochberg and Tamhane.", "year": 1987}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling.,? \\Q1933\\E", "shortCiteRegEx": "Hotelling.", "year": 1933}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoyer.,? \\Q2004\\E", "shortCiteRegEx": "Hoyer.", "year": 2004}, {"title": "Receptive fields of single neurones in the cat\u2019s striate cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "Journal of Physiology,", "citeRegEx": "Hubel and Wiesel.,? \\Q1959\\E", "shortCiteRegEx": "Hubel and Wiesel.", "year": 1959}, {"title": "Comparing measures of sparsity", "author": ["N. Hurley", "S. Rickard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hurley and Rickard.,? \\Q2009\\E", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Sparse code shrinkage: Denoising by nonlinear maximum likelihood estimation", "author": ["A. Hyv\u00e4rinen", "P.O. Hoyer", "E. Oja"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 1999}, {"title": "How does connectivity between cortical areas depend on brain size? Implications for efficient computation", "author": ["J. Karbowski"], "venue": "Journal of Computational Neuroscience,", "citeRegEx": "Karbowski.,? \\Q2003\\E", "shortCiteRegEx": "Karbowski.", "year": 2003}, {"title": "Correlation matrix memories", "author": ["T. Kohonen"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "Kohonen.,? \\Q1972\\E", "shortCiteRegEx": "Kohonen.", "year": 1972}, {"title": "Use of ranks in one-criterion variance analysis", "author": ["W.H. Kruskal", "W.A. Wallis"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Kruskal and Wallis.,? \\Q1952\\E", "shortCiteRegEx": "Kruskal and Wallis.", "year": 1952}, {"title": "Matrix Analysis for Scientists and Engineers", "author": ["A.J. Laub"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Laub.,? \\Q2004\\E", "shortCiteRegEx": "Laub.", "year": 2004}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes.,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes.", "year": 1998}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J.S. Denker", "S.A. Solla"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "Lee and Seung.,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung.", "year": 1999}, {"title": "Robust tests for equality of variances. In Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling, pages 278\u2013292", "author": ["H. Levene"], "venue": null, "citeRegEx": "Levene.,? \\Q1960\\E", "shortCiteRegEx": "Levene.", "year": 1960}, {"title": "Learning spatially localized, parts-based representation", "author": ["S.Z. Li", "X. Hou", "H. Zhang", "Q. Cheng"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Li et al\\.", "year": 2001}, {"title": "Efficient euclidean projections in linear time", "author": ["J. Liu", "J. Ye"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liu and Ye.,? \\Q2009\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2009}, {"title": "Efficient `1/`q norm regularization", "author": ["J. Liu", "J. Ye"], "venue": "Technical Report arXiv:1009.4766v1,", "citeRegEx": "Liu and Ye.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2010}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Physiology and anatomy of synaptic connections between thick tufted pyramidal neurones in the developing rat neocortex", "author": ["H. Markram", "J. L\u00fcbke", "M. Frotscher", "A. Roth", "B. Sakmann"], "venue": "Journal of Physiology,", "citeRegEx": "Markram et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Markram et al\\.", "year": 1997}, {"title": "Synaptic transmission between individual pyramidal neurons of the rat visual cortex in vitro", "author": ["A. Mason", "A. Nicoll", "K. Stratford"], "venue": "Journal of Neuroscience,", "citeRegEx": "Mason et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1991}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of Rn", "author": ["C. Michelot"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Michelot.,? \\Q1986\\E", "shortCiteRegEx": "Michelot.", "year": 1986}, {"title": "Multiclass object recognition with sparse, localized features", "author": ["J. Mutch", "D.G. Lowe"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Mutch and Lowe.,? \\Q2006\\E", "shortCiteRegEx": "Mutch and Lowe.", "year": 2006}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Some theorems on matrix differentiation with special reference to Kronecker matrix products", "author": ["H. Neudecker"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Neudecker.,? \\Q1969\\E", "shortCiteRegEx": "Neudecker.", "year": 1969}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Sparse coding of sensory inputs", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Olshausen and Field.,? \\Q2004\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 2004}, {"title": "Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data", "author": ["P. Paatero", "U. Tapper"], "venue": "values. Environmetrics,", "citeRegEx": "Paatero and Tapper.,? \\Q1994\\E", "shortCiteRegEx": "Paatero and Tapper.", "year": 1994}, {"title": "On associative memory", "author": ["G. Palm"], "venue": "Biological Cybernetics,", "citeRegEx": "Palm.,? \\Q1980\\E", "shortCiteRegEx": "Palm.", "year": 1980}, {"title": "Multiple comparison procedures applied to model selection", "author": ["J. Pizarro", "E. Guerrero", "P.L. Galindo"], "venue": null, "citeRegEx": "Pizarro et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pizarro et al\\.", "year": 2002}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y. Boureau", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields", "author": ["M. Rehn", "F.T. Sommer"], "venue": "Journal of Computational Neuroscience,", "citeRegEx": "Rehn and Sommer.,? \\Q2007\\E", "shortCiteRegEx": "Rehn and Sommer.", "year": 2007}, {"title": "Thirteen ways to look at the correlation coefficient", "author": ["J.L. Rodgers", "W.A. Nicewander"], "venue": "The American Statistician,", "citeRegEx": "Rodgers and Nicewander.,? \\Q1988\\E", "shortCiteRegEx": "Rodgers and Nicewander.", "year": 1988}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Support Vector Learning", "author": ["B. Sch\u00f6lkopf"], "venue": "PhD thesis, Technische Universita\u0308t Berlin,", "citeRegEx": "Sch\u00f6lkopf.,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00f6lkopf.", "year": 1997}, {"title": "An analysis of variance test for normality (complete samples)", "author": ["S.S. Shapiro", "M.B. Wilk"], "venue": null, "citeRegEx": "Shapiro and Wilk.,? \\Q1965\\E", "shortCiteRegEx": "Shapiro and Wilk.", "year": 1965}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In Proceedings of the International Conference on Document Analysis and Recognition,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Fast projections onto mixed-norm balls with applications", "author": ["S. Sra"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Sra.,? \\Q2012\\E", "shortCiteRegEx": "Sra.", "year": 2012}, {"title": "Sparseness by iterative projections onto spheres", "author": ["F.J. Theis", "T. Tanaka"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Theis and Tanaka.,? \\Q2006\\E", "shortCiteRegEx": "Theis and Tanaka.", "year": 2006}, {"title": "First results on uniqueness of sparse non-negative matrix factorization", "author": ["F.J. Theis", "K. Stadlthanner", "T. Tanaka"], "venue": "In Proceedings of the European Signal Processing Conference,", "citeRegEx": "Theis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2005}, {"title": "Supervised matrix factorization with sparseness constraints and fast inference", "author": ["M. Thom", "R. Schweiger", "G. Palm"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "Thom et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thom et al\\.", "year": 2011}, {"title": "Training of sparsely connected MLPs", "author": ["M. Thom", "R. Schweiger", "G. Palm"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Thom et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thom et al\\.", "year": 2011}, {"title": "Group sparsity via linear-time projection", "author": ["E. van den Berg", "M. Schmidt", "M.P. Friedlander", "K. Murphy"], "venue": "Technical Report TR-2008-09,", "citeRegEx": "Berg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2008}, {"title": "Derivative operations on matrices", "author": ["W.J. Vetter"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Vetter.,? \\Q1970\\E", "shortCiteRegEx": "Vetter.", "year": 1970}, {"title": "Sparse coding and decorrelation in primary visual cortex during natural vision", "author": ["W.E. Vinje", "J.L. Gallant"], "venue": null, "citeRegEx": "Vinje and Gallant.,? \\Q2000\\E", "shortCiteRegEx": "Vinje and Gallant.", "year": 2000}, {"title": "Functional Operators, Volume II: The Geometry of Orthogonal Spaces", "author": ["J. von Neumann"], "venue": null, "citeRegEx": "Neumann.,? \\Q1950\\E", "shortCiteRegEx": "Neumann.", "year": 1950}, {"title": "Use of the zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weston et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2003}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Networks,", "citeRegEx": "Wilson and Martinez.,? \\Q2003\\E", "shortCiteRegEx": "Wilson and Martinez.", "year": 2003}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 32, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms.", "startOffset": 55, "endOffset": 148}, {"referenceID": 55, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms.", "startOffset": 55, "endOffset": 148}, {"referenceID": 49, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms.", "startOffset": 55, "endOffset": 148}, {"referenceID": 48, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms.", "startOffset": 55, "endOffset": 148}, {"referenceID": 32, "context": "Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; Vinje and Gallant, 2000).", "startOffset": 179, "endOffset": 228}, {"referenceID": 74, "context": "Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; Vinje and Gallant, 2000).", "startOffset": 179, "endOffset": 228}, {"referenceID": 32, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms. A notable result was achieved through the sparse coding model of Olshausen and Field (1996). Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; Vinje and Gallant, 2000).", "startOffset": 56, "endOffset": 301}, {"referenceID": 32, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms. A notable result was achieved through the sparse coding model of Olshausen and Field (1996). Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; Vinje and Gallant, 2000). Another example is the optimal brain damage method of LeCun et al. (1990), which can be used to prune synaptic connections in a neural network, making connectivity sparse.", "startOffset": 56, "endOffset": 606}, {"referenceID": 32, "context": "Both properties have been observed in mammalian brains (Hubel and Wiesel, 1959; Olshausen and Field, 2004; Mason et al., 1991; Markram et al., 1997) and have inspired a variety of machine learning algorithms. A notable result was achieved through the sparse coding model of Olshausen and Field (1996). Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex (Hubel and Wiesel, 1959; Vinje and Gallant, 2000). Another example is the optimal brain damage method of LeCun et al. (1990), which can be used to prune synaptic connections in a neural network, making connectivity sparse. Although only a small fraction of possible connections remains after pruning, this is sufficient to achieve equivalent classification results. Since then, numerous approaches on how to measure sparseness have been proposed, see Hurley and Rickard (2009) for an overview, and how to achieve sparse solutions of classical machine learning problems.", "startOffset": 56, "endOffset": 958}, {"referenceID": 61, "context": "Using it rather than other sparseness measures has been shown to induce biologically more plausible properties (Rehn and Sommer, 2007).", "startOffset": 111, "endOffset": 134}, {"referenceID": 52, "context": "However, finding of optimal solutions subject to the L0 pseudo-norm turns out to be NP-hard (Natarajan, 1995; Weston et al., 2003).", "startOffset": 92, "endOffset": 130}, {"referenceID": 76, "context": "However, finding of optimal solutions subject to the L0 pseudo-norm turns out to be NP-hard (Natarajan, 1995; Weston et al., 2003).", "startOffset": 92, "endOffset": 130}, {"referenceID": 13, "context": "The Manhattan norm of a vector is a convex relaxation of the L0 pseudo-norm (Donoho, 2006), and has been employed in a vast range of applications.", "startOffset": 76, "endOffset": 90}, {"referenceID": 31, "context": "A normalized sparseness measure \u03c3 based on the ratio of the L1 or Manhattan norm and the L2 or Euclidean norm of a vector has been proposed by Hoyer (2004),", "startOffset": 143, "endOffset": 156}, {"referenceID": 38, "context": "\u03c3 is well-defined because \u2016x\u20162\u2264\u2016x\u20161\u2264 \u221a n\u2016x\u20162 holds for all x \u2208 Rn (Laub, 2004).", "startOffset": 66, "endOffset": 78}, {"referenceID": 35, "context": "For example, sparseness of connectivity in a biological brain increases quickly with its volume, so that connectivity in a human brain is about 170 times more sparse than in a rat brain (Karbowski, 2003).", "startOffset": 186, "endOffset": 203}, {"referenceID": 32, "context": "This sparseness measure fulfills all criteria of Hurley and Rickard (2009) except for Dalton\u2019s fourth law, which states that the sparseness of a vector should be identical to the sparseness of the vector resulting from multiple concatenation of the original vector.", "startOffset": 49, "endOffset": 75}, {"referenceID": 31, "context": "A sparseness-enforcing projection operator, suitable for projected gradient descent algorithms, was proposed by Hoyer (2004) for optimization with respect to \u03c3.", "startOffset": 112, "endOffset": 125}, {"referenceID": 31, "context": "Hoyer\u2019s original algorithm for computation of such a projection is an alternating projection onto a hyperplane representing the L1 norm constraint, a hypersphere representing the L2 norm constraint, and the non-negative orthant. A slightly modified version of this algorithm has been proved to be correct by Theis et al. (2005) in the special case when exactly one negative entry emerges that is zeroed out in the orthant projection.", "startOffset": 0, "endOffset": 328}, {"referenceID": 4, "context": "In fact, the projection onto Z consists simply of zeroing out all entries but the \u03ba that are greatest in absolute value (Blumensath and Davies, 2009).", "startOffset": 120, "endOffset": 149}, {"referenceID": 7, "context": "The most relevant result is that if x\u2208Rn \\C, then there exists a separator t\u0302 \u2208R such that p := projC(x) = max(x\u2212 t\u0302 \u00b7 e, 0), where the maximum is taken element-wise (Chen and Ye, 2011).", "startOffset": 166, "endOffset": 185}, {"referenceID": 7, "context": "The most relevant result is that if x\u2208Rn \\C, then there exists a separator t\u0302 \u2208R such that p := projC(x) = max(x\u2212 t\u0302 \u00b7 e, 0), where the maximum is taken element-wise (Chen and Ye, 2011). In the cases considered in this paper it is always t\u0302 \u2265 0 as shown in Lemma 28. This implies that all entries in x that are less than t\u0302 do not survive the projection, and hence the L0 pseudo-norm of x is strictly greater than that of p. The simplex projection therefore enhances sparseness. The separator t\u0302 and the number of nonzero entries in the projection onto C can be computed with Algorithm 2, which is an adapted version of the algorithm of Chen and Ye (2011). In line 1, Sn denotes the symmetric group and P\u03c4 denotes the permutation matrix associated with a permutation \u03c4 \u2208 Sn.", "startOffset": 167, "endOffset": 656}, {"referenceID": 7, "context": "This is an adapted version of the algorithm of Chen and Ye (2011). Input: x \u2208Rn \\C and \u03bb1 \u2208R>0.", "startOffset": 47, "endOffset": 66}, {"referenceID": 31, "context": "The original algorithm for the sparseness-enforcing projection operator proposed by Hoyer (2004) is hard to understand, and correctness has been proved by Theis et al.", "startOffset": 84, "endOffset": 97}, {"referenceID": 31, "context": "The original algorithm for the sparseness-enforcing projection operator proposed by Hoyer (2004) is hard to understand, and correctness has been proved by Theis et al. (2005) in a special case only.", "startOffset": 84, "endOffset": 175}, {"referenceID": 31, "context": "As was already observed by Hoyer (2004), the number of required iterations grows very slowly with problem dimensionality.", "startOffset": 27, "endOffset": 40}, {"referenceID": 30, "context": "ponent analysis (Hotelling, 1933), restricted Boltzmann machines for deep auto-encoder networks (Hinton et al.", "startOffset": 16, "endOffset": 33}, {"referenceID": 26, "context": "ponent analysis (Hotelling, 1933), restricted Boltzmann machines for deep auto-encoder networks (Hinton et al., 2006) and to sparse encoding symmetric machine (Ranzato et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 60, "context": ", 2006) and to sparse encoding symmetric machine (Ranzato et al., 2008).", "startOffset": 49, "endOffset": 71}, {"referenceID": 31, "context": "This condition was adopted from non-negative matrix factorization with sparseness constraints (Hoyer, 2004).", "startOffset": 94, "endOffset": 107}, {"referenceID": 63, "context": "Therefore the entire processing path from x to y forms a two-layer neural network (Rumelhart et al., 1986), where W stores the synaptic weights of the hidden layer, and Wout and \u03b8out are the parameters of the output layer.", "startOffset": 82, "endOffset": 106}, {"referenceID": 3, "context": "The similarity measure for classification capabilities sC is chosen to be the cross-entropy error function (Bishop, 1995), which was shown empirically by Simard et al.", "startOffset": 107, "endOffset": 121}, {"referenceID": 3, "context": "The softmax transfer function (Bishop, 1995) is used as transfer function g of the output layer.", "startOffset": 30, "endOffset": 44}, {"referenceID": 16, "context": "It provides a natural pairing together with the cross-entropy error function (Dunne and Campbell, 1997) and supports multi-class classification.", "startOffset": 77, "endOffset": 103}, {"referenceID": 3, "context": "The similarity measure for classification capabilities sC is chosen to be the cross-entropy error function (Bishop, 1995), which was shown empirically by Simard et al. (2003) to induce better classification capabilities than the mean squared error function.", "startOffset": 108, "endOffset": 175}, {"referenceID": 2, "context": "The proposed optimization algorithm for minimization of the objective function ESOAE is projected gradient descent (Bertsekas, 1999).", "startOffset": 115, "endOffset": 132}, {"referenceID": 2, "context": "There are theoretical results on the convergence of projected gradient methods when projections are carried out onto convex sets (Bertsekas, 1999), but here the target set for projection is non-convex.", "startOffset": 129, "endOffset": 146}, {"referenceID": 77, "context": "This online learning procedure results in faster learning and improves generalization capabilities over batch learning (Wilson and Martinez, 2003; Bottou and LeCun, 2004).", "startOffset": 119, "endOffset": 170}, {"referenceID": 5, "context": "This online learning procedure results in faster learning and improves generalization capabilities over batch learning (Wilson and Martinez, 2003; Bottou and LeCun, 2004).", "startOffset": 119, "endOffset": 170}, {"referenceID": 3, "context": "Initialization of the columns of W is achieved by selecting a random subset of the learning set, similar to the initialization of radial basis function networks (Bishop, 1995).", "startOffset": 161, "endOffset": 175}, {"referenceID": 26, "context": "This continuous variant of unsupervised pre-training (Hinton et al., 2006) leads to parameters in the vicinity of a good minimizer for classification capabilities before classification is preferred over reconstruction through the trade-off parameter \u03b1.", "startOffset": 53, "endOffset": 74}, {"referenceID": 39, "context": "To assess the classification capabilities and the impact of sparse activity and sparse connectivity, the MNIST database of handwritten digits (LeCun and Cortes, 1998) was employed.", "startOffset": 142, "endOffset": 166}, {"referenceID": 39, "context": "For generation of the original data set, the placement of the digits has been achieved based on their barycenter (LeCun and Cortes, 1998).", "startOffset": 113, "endOffset": 137}, {"referenceID": 66, "context": "Further improvements can be achieved by generating artificial training samples using elastic distortions (Simard et al., 2003).", "startOffset": 105, "endOffset": 126}, {"referenceID": 8, "context": "35% (Cire\u015fan et al., 2010).", "startOffset": 4, "endOffset": 26}, {"referenceID": 9, "context": "23% is held by an approach that combines distorted samples with a committee of convolutional neural networks (Cire\u015fan et al., 2012).", "startOffset": 109, "endOffset": 131}, {"referenceID": 23, "context": "As noted by Hinton et al. (2006), the learning problem is no more permutation-invariant due to the jittering, as information on the neighborhood of the pixels is implicitly incorporated in the learning set.", "startOffset": 12, "endOffset": 33}, {"referenceID": 23, "context": "As noted by Hinton et al. (2006), the learning problem is no more permutation-invariant due to the jittering, as information on the neighborhood of the pixels is implicitly incorporated in the learning set. However, classification results improve dramatically when such prior knowledge is used. This was demonstrated by Sch\u00f6lkopf (1997) using the virtual support vector method, which improved a support vector machine with polynomial kernel of degree five from an error of 1.", "startOffset": 12, "endOffset": 337}, {"referenceID": 9, "context": "This result was extended by DeCoste and Sch\u00f6lkopf (2002), where a support vector machine with a polynomial kernel of degree nine was improved from an error of 1.", "startOffset": 28, "endOffset": 57}, {"referenceID": 8, "context": "35% (Cire\u015fan et al., 2010). The current record error of 0.23% is held by an approach that combines distorted samples with a committee of convolutional neural networks (Cire\u015fan et al., 2012). This is an architecture that has been optimized exclusively for input data that represents images, that is where the neighborhood of the pixels is hard-wired in the classifier. To allow for a plain evaluation that does not depend on additional parameters for creating artificial samples, the jittered learning set with 540 000 samples is used throughout this paper. The experimental methodology was as follows. The number of hidden units was chosen to be n := 1000 in all experiments that are described below. This is an increased number compared to the 800 hidden units employed by Simard et al. (2003), but promises to yield better results when an adequate number of learning samples is used.", "startOffset": 5, "endOffset": 795}, {"referenceID": 21, "context": "It can be explained by the bias-variance decomposition of the generalization error (Geman et al., 1992).", "startOffset": 83, "endOffset": 103}, {"referenceID": 40, "context": "Here, the optimal brain damage (OBD) algorithm (LeCun et al., 1990) was used to prune synaptic connections in the hidden layer that are irrelevant for the computation of the classification decision of the network.", "startOffset": 47, "endOffset": 67}, {"referenceID": 39, "context": "The effect of better generalization due to sparse connectivity has also been observed by LeCun et al. (1990) in the context of convolutional neural networks.", "startOffset": 89, "endOffset": 109}, {"referenceID": 28, "context": "A Tukey-Kramer type modification applied to Dunn\u2019s procedure yields this critical difference, which is less conservative than Nemenyi\u2019s procedure for the Kruskal-Wallis test (Hochberg and Tamhane, 1987).", "startOffset": 174, "endOffset": 202}, {"referenceID": 52, "context": "The procedure follows the proposals of Pizarro et al. (2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 11, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al.", "startOffset": 11, "endOffset": 25}, {"referenceID": 11, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al.", "startOffset": 11, "endOffset": 122}, {"referenceID": 0, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al. (2006). For each algorithm, a sample of size 47 was available, allowing for robust analysis results.", "startOffset": 126, "endOffset": 146}, {"referenceID": 0, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al. (2006). For each algorithm, a sample of size 47 was available, allowing for robust analysis results. First, all results were tested for normality using the test developed by Shapiro and Wilk (1965). The resulting test statistics W and p-values are given in Table 1.", "startOffset": 126, "endOffset": 337}, {"referenceID": 0, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al. (2006). For each algorithm, a sample of size 47 was available, allowing for robust analysis results. First, all results were tested for normality using the test developed by Shapiro and Wilk (1965). The resulting test statistics W and p-values are given in Table 1. As all p-values are large, it cannot be rejected that the samples came from normally distributed populations. Thus normality is assumed in the remainder of this discussion. Next, the test proposed by Levene (1960) was applied to determine whether equality of variances of the groups holds.", "startOffset": 126, "endOffset": 619}, {"referenceID": 0, "context": "(2002) and Dem\u0161ar (2006) for hypothesis testing, and is concluded by effect size estimation as proposed by Grissom (1994) and Acion et al. (2006). For each algorithm, a sample of size 47 was available, allowing for robust analysis results. First, all results were tested for normality using the test developed by Shapiro and Wilk (1965). The resulting test statistics W and p-values are given in Table 1. As all p-values are large, it cannot be rejected that the samples came from normally distributed populations. Thus normality is assumed in the remainder of this discussion. Next, the test proposed by Levene (1960) was applied to determine whether equality of variances of the groups holds. This resulted in a test statistic F = 2.7979 with 7 and 368 degrees of freedom, and therefore a p-value of 0.0075. Hence the hypothesis that all group variances are equal can be rejected with very high significance. Consequently, parametric omnibus and post-hoc tests cannot be applied, as they require the groups to have equal variance. As an alternative, the nonparametric test by Kruskal and Wallis (1952) which is based on rank information was employed to test whether all algorithms produced classifiers with equal classification errors in the mean.", "startOffset": 126, "endOffset": 1104}, {"referenceID": 12, "context": "Figure 7: Diagram for multiple comparison of algorithms following Dem\u0161ar (2006). For each algorithm, the mean rank was computed during the Kruskal-Wallis test.", "startOffset": 66, "endOffset": 80}, {"referenceID": 12, "context": "procedure proposed by Dem\u0161ar (2006) for paired observations, such that the diagrams proposed there can be adapted to the case for unpaired observations.", "startOffset": 22, "endOffset": 36}, {"referenceID": 24, "context": "To assess the benefit when an algorithm from one group is chosen over an algorithm from another group, the probability of superior experiment outcome was estimated (Grissom, 1994; Acion et al., 2006).", "startOffset": 164, "endOffset": 199}, {"referenceID": 0, "context": "To assess the benefit when an algorithm from one group is chosen over an algorithm from another group, the probability of superior experiment outcome was estimated (Grissom, 1994; Acion et al., 2006).", "startOffset": 164, "endOffset": 199}, {"referenceID": 14, "context": "However, using Rao-Blackwell theory a minimum variance unbiased estimator R\u03022 of this probability can be computed easily (Downton, 1973).", "startOffset": 121, "endOffset": 136}, {"referenceID": 24, "context": "Therefore, the effect of choosing SOAE-\u03c3 over any of the seven other algorithms is dramatic (Grissom, 1994).", "startOffset": 92, "endOffset": 107}, {"referenceID": 39, "context": "Once a synaptic connection has been removed, it cannot be recovered, as the measure for relevance of LeCun et al. (1990) vanishes for synaptic connections of zero strength.", "startOffset": 101, "endOffset": 121}, {"referenceID": 17, "context": "A similar approach can be carried out for intersections of closed, convex cones (Dykstra, 1983), which can be generalized to translated cones that can be used to approximate any convex set (Dykstra and Boyle, 1987).", "startOffset": 80, "endOffset": 95}, {"referenceID": 18, "context": "A similar approach can be carried out for intersections of closed, convex cones (Dykstra, 1983), which can be generalized to translated cones that can be used to approximate any convex set (Dykstra and Boyle, 1987).", "startOffset": 189, "endOffset": 214}, {"referenceID": 29, "context": "The first major part of this paper dealt with improvements to the work of Hoyer (2004) and Theis et al.", "startOffset": 74, "endOffset": 87}, {"referenceID": 29, "context": "The first major part of this paper dealt with improvements to the work of Hoyer (2004) and Theis et al. (2005). Here, an algorithm for the sparseness-enforcing projection with respect to Hoyer\u2019s sparseness measure \u03c3 was proposed.", "startOffset": 74, "endOffset": 111}, {"referenceID": 29, "context": "The first major part of this paper dealt with improvements to the work of Hoyer (2004) and Theis et al. (2005). Here, an algorithm for the sparseness-enforcing projection with respect to Hoyer\u2019s sparseness measure \u03c3 was proposed. The technical proof of correctness is given in Appendix C. The set that should be projected onto is an intersection of a simplex C and a hypercircle L, which is a hypersphere lying in a hyperplane. The overall procedure can be described as performing alternating projections onto C and certain subsets of L. This approach is common for handling projections onto intersections of individual sets. For example, von Neumann (1950) proposed essentially the same idea when the investigated sets are closed subspaces, and has shown that this converges to a solution.", "startOffset": 74, "endOffset": 658}, {"referenceID": 15, "context": "For the former, efficient algorithms have been proposed recently (Duchi et al., 2008; Chen and Ye, 2011).", "startOffset": 65, "endOffset": 104}, {"referenceID": 7, "context": "For the former, efficient algorithms have been proposed recently (Duchi et al., 2008; Chen and Ye, 2011).", "startOffset": 65, "endOffset": 104}, {"referenceID": 44, "context": "When only independent solutions are required, the projection of a point x onto a scaled canonical simplex of L1 norm \u03bb1 can also be carried out in linear time (Liu and Ye, 2009), without having to sort the vector that is to be projected.", "startOffset": 159, "endOffset": 177}, {"referenceID": 44, "context": "The zero of this function can be found efficiently using the bisection method, and exploiting the special structure of the occurring expressions (Liu and Ye, 2009).", "startOffset": 145, "endOffset": 163}, {"referenceID": 27, "context": "For example, the projection onto a closed, convex set is guaranteed to be differentiable almost everywhere (Hiriart-Urruty, 1982).", "startOffset": 107, "endOffset": 129}, {"referenceID": 4, "context": "The iterative hard thresholding algorithm is a gradient descent algorithm, where a projection onto an L0 pseudo-norm constraint is performed (Blumensath and Davies, 2009).", "startOffset": 141, "endOffset": 170}, {"referenceID": 4, "context": "In spite of the simplicity of the method, it can be shown that it achieves a good approximation to the optimal solution of this NP-hard problem (Blumensath and Davies, 2009).", "startOffset": 144, "endOffset": 173}, {"referenceID": 4, "context": "The iterative hard thresholding algorithm is a gradient descent algorithm, where a projection onto an L0 pseudo-norm constraint is performed (Blumensath and Davies, 2009). Its application lies in compressed sensing, where a linear generative model is used to infer a sparse representation for a given observation. Sparseness here acts as regularizer which is necessary because observations are sampled below the Nyquist rate. In spite of the simplicity of the method, it can be shown that it achieves a good approximation to the optimal solution of this NP-hard problem (Blumensath and Davies, 2009). Closely related with the work of this paper is the generalization of Hoyer\u2019s sparseness measure by Theis and Tanaka (2006). Here, the L1 norm constraint is replaced with a generalized Lp pseudo-", "startOffset": 142, "endOffset": 724}, {"referenceID": 67, "context": "In this context, mixed norm balls are of particular interest (Sra, 2012).", "startOffset": 61, "endOffset": 72}, {"referenceID": 59, "context": ", 2008) and for q = \u221e (Quattoni et al., 2009).", "startOffset": 22, "endOffset": 45}, {"referenceID": 45, "context": "The case when p = 1 and q\u2265 1 is more difficult, but can be solved as well (Liu and Ye, 2010; Sra, 2012).", "startOffset": 74, "endOffset": 103}, {"referenceID": 67, "context": "The case when p = 1 and q\u2265 1 is more difficult, but can be solved as well (Liu and Ye, 2010; Sra, 2012).", "startOffset": 74, "endOffset": 103}, {"referenceID": 78, "context": "The last problem discussed here is the elastic net criterion (Zou and Hastie, 2005), which is a constraint on the sum of an L1 norm and an L2 norm.", "startOffset": 61, "endOffset": 83}, {"referenceID": 47, "context": "As is the case for mixed norm balls, the projection onto a simplex can be generalized to achieve projections onto N (Mairal et al., 2010).", "startOffset": 116, "endOffset": 137}, {"referenceID": 30, "context": "For p = 1, Hoyer\u2019s sparseness measure up to a constant normalization is obtained. When p converges decreasingly to zero, then \u03c3p(x) converges point-wise to the L0 pseudo-norm. Hence for small values of p a more natural sparseness measure is obtained. Theis and Tanaka (2006) also proposed an extension of Hoyer\u2019s projection algorithm.", "startOffset": 11, "endOffset": 275}, {"referenceID": 36, "context": "If the entries of the training patterns are sparsely populated, the weight matrix of the memory will be sparsely populated as well after training if Hebbian-like learning rules are used (Kohonen, 1972).", "startOffset": 186, "endOffset": 201}, {"referenceID": 57, "context": "The assumption of sparsely coded inputs also results in increased completion capacity and noise resistance of the associative memory (Palm, 1980).", "startOffset": 133, "endOffset": 145}, {"referenceID": 1, "context": "If the input data is not sparse inherently, feature detectors can perform a sparsification prior to the actual processing through the memory (Baum et al., 1988).", "startOffset": 141, "endOffset": 160}, {"referenceID": 31, "context": "A purely generative model that also possesses these two key properties is non-negative matrix factorization with sparseness constraints (Hoyer, 2004).", "startOffset": 136, "endOffset": 149}, {"referenceID": 56, "context": "matrix factorization (Paatero and Tapper, 1994) which was shown to achieve sparse connectivity on certain data sets (Lee and Seung, 1999).", "startOffset": 21, "endOffset": 47}, {"referenceID": 41, "context": "matrix factorization (Paatero and Tapper, 1994) which was shown to achieve sparse connectivity on certain data sets (Lee and Seung, 1999).", "startOffset": 116, "endOffset": 137}, {"referenceID": 43, "context": "However, there are data sets on which this does not work (Li et al., 2001; Hoyer, 2004).", "startOffset": 57, "endOffset": 87}, {"referenceID": 31, "context": "However, there are data sets on which this does not work (Li et al., 2001; Hoyer, 2004).", "startOffset": 57, "endOffset": 87}, {"referenceID": 51, "context": "In the use case of object recognition, a hard shrinkage was also employed to de-noise filter responses (Mutch and Lowe, 2006).", "startOffset": 103, "endOffset": 125}, {"referenceID": 10, "context": "It is well-known that two layers in a neural network are sufficient to approximate any continuous function on a compactum with arbitrary precision (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989).", "startOffset": 147, "endOffset": 200}, {"referenceID": 20, "context": "It is well-known that two layers in a neural network are sufficient to approximate any continuous function on a compactum with arbitrary precision (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989).", "startOffset": 147, "endOffset": 200}, {"referenceID": 29, "context": "It is well-known that two layers in a neural network are sufficient to approximate any continuous function on a compactum with arbitrary precision (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989).", "startOffset": 147, "endOffset": 200}, {"referenceID": 20, "context": "An extension intended to incorporate class membership information to increase discriminative capabilities was proposed by Heiler and Schn\u00f6rr (2006). In their approach, an additional constraint was added ensuring that every internal representation is close to the mean of all internal representations that belong to the same class.", "startOffset": 122, "endOffset": 148}, {"referenceID": 20, "context": "An extension intended to incorporate class membership information to increase discriminative capabilities was proposed by Heiler and Schn\u00f6rr (2006). In their approach, an additional constraint was added ensuring that every internal representation is close to the mean of all internal representations that belong to the same class. In other words, the method can be interpreted as supervised clustering, with the number of clusters equal to the number of classes. However, there is no guarantee that a distribution of internal representations exists such that both the reproduction error is minimized and the internal representations can be arranged in such a pattern. Unfortunately, Heiler and Schn\u00f6rr (2006) used only a subset of a small data set for handwritten digit recognition to evaluate their approach.", "startOffset": 122, "endOffset": 709}, {"referenceID": 20, "context": "An extension intended to incorporate class membership information to increase discriminative capabilities was proposed by Heiler and Schn\u00f6rr (2006). In their approach, an additional constraint was added ensuring that every internal representation is close to the mean of all internal representations that belong to the same class. In other words, the method can be interpreted as supervised clustering, with the number of clusters equal to the number of classes. However, there is no guarantee that a distribution of internal representations exists such that both the reproduction error is minimized and the internal representations can be arranged in such a pattern. Unfortunately, Heiler and Schn\u00f6rr (2006) used only a subset of a small data set for handwritten digit recognition to evaluate their approach. A precursor to the supervised online auto-encoder was proposed by Thom et al. (2011a). There, inference of sparse internal representations was achieved by fitting a one-layer neural network to approximate a latent variable of optimal sparse representations.", "startOffset": 122, "endOffset": 896}, {"referenceID": 19, "context": "Similar techniques to achieve a shrinkage-like effect for increasing sparseness of activity in a neural network were used by Gregor and LeCun (2010) and Glorot et al.", "startOffset": 125, "endOffset": 149}, {"referenceID": 19, "context": "Similar techniques to achieve a shrinkage-like effect for increasing sparseness of activity in a neural network were used by Gregor and LeCun (2010) and Glorot et al. (2011). Information processing is here purely local, that is a scalar function is evaluated entrywise on a vector, and thus no information is interchanged among individual entries.", "startOffset": 153, "endOffset": 174}, {"referenceID": 19, "context": "Similar techniques to achieve a shrinkage-like effect for increasing sparseness of activity in a neural network were used by Gregor and LeCun (2010) and Glorot et al. (2011). Information processing is here purely local, that is a scalar function is evaluated entrywise on a vector, and thus no information is interchanged among individual entries. The use of non-local shrinkage to reduce Gaussian noise in sparse coding has already been described by Hyv\u00e4rinen et al. (1999). Here, a maximum likelihood estimate with only weak assumptions yields a shrinkage operation, which can be conceived as projection onto a scaled canonical simplex.", "startOffset": 153, "endOffset": 475}, {"referenceID": 19, "context": "Similar techniques to achieve a shrinkage-like effect for increasing sparseness of activity in a neural network were used by Gregor and LeCun (2010) and Glorot et al. (2011). Information processing is here purely local, that is a scalar function is evaluated entrywise on a vector, and thus no information is interchanged among individual entries. The use of non-local shrinkage to reduce Gaussian noise in sparse coding has already been described by Hyv\u00e4rinen et al. (1999). Here, a maximum likelihood estimate with only weak assumptions yields a shrinkage operation, which can be conceived as projection onto a scaled canonical simplex. In the use case of object recognition, a hard shrinkage was also employed to de-noise filter responses (Mutch and Lowe, 2006). Whenever a best approximation from a permutationinvariant set is used, a shrinkage-like operation must be employed. Using a projection operator as neural transfer function is hence a natural extension of these ideas. When the projection is sufficiently smooth, the entire model can be tuned end-to-end using gradient methods to achieve an auto-encoder or a classifier. The second building block from Thom et al. (2011a) that was incorporated into supervised online auto-encoder is the architectural concept for classification.", "startOffset": 153, "endOffset": 1186}, {"referenceID": 6, "context": "Bradley and Bagnell (2009) used the Kullback-Leibler divergence as implicit sparseness penalty term and combined this with the backpropagation algorithm to yield a classifier that achieved a 1.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "Bradley and Bagnell (2009) used the Kullback-Leibler divergence as implicit sparseness penalty term and combined this with the backpropagation algorithm to yield a classifier that achieved a 1.30% error rate on the MNIST evaluation set. The Kullback-Leibler divergence was chosen to replace the usual L1 norm penalty term, as it is smoother than the latter and therefore sparsely coded internal representations are more stable subject to subtle changes of the input. A related technique is supervised dictionary learning by Mairal et al. (2009), where the objective function is an additive combination of a classification error term, a term for the reproduction error, and an L1 norm constraint.", "startOffset": 0, "endOffset": 545}, {"referenceID": 64, "context": "Augmentation of the learning set with virtual samples would have contributed to improve classification performance, as demonstrated by Sch\u00f6lkopf (1997).", "startOffset": 135, "endOffset": 152}, {"referenceID": 70, "context": "In practice, this property can also be used to reduce the computational complexity of classification by one order of magnitude (Thom et al., 2011b). This results from exploiting sparseness and using sparse matrix-vector multiplication algorithms to infer the internal representation, which is the major computational burden in class membership prediction. It was shown in this paper and by Thom et al. (2011b) that a small number of nonzero entries in the weight matrix of the hidden layer is sufficient for achieving good classification results.", "startOffset": 128, "endOffset": 410}, {"referenceID": 38, "context": "The following basic statement will be used extensively in this paper and follows from \u3008x, x\u3009= \u2016x\u20162 for all x \u2208Rn and the fact that the scalar product is a symmetric bilinear form (Laub, 2004):", "startOffset": 179, "endOffset": 191}, {"referenceID": 31, "context": "1) Sparseness measure by Hoyer (2004) \u03c0 and \u03c0\u22650 (see Section 2.", "startOffset": 25, "endOffset": 38}, {"referenceID": 33, "context": "A great variety of sparseness measures fulfills certain symmetries as vector entries are equally weighted, see Hurley and Rickard (2009). This means that no entry is preferred over another, and for negative entries usually the absolute value or the squared value is taken, such that the signs of the entries are ignored.", "startOffset": 111, "endOffset": 137}, {"referenceID": 15, "context": "A weaker form of its statements has been described by Duchi et al. (2008) in the special case of a projection onto a simplex.", "startOffset": 54, "endOffset": 74}, {"referenceID": 30, "context": "Its second part was already observed by Hoyer (2004), in the special case of the sparseness-enforcing projection operator, and by Duchi et al.", "startOffset": 40, "endOffset": 53}, {"referenceID": 15, "context": "Its second part was already observed by Hoyer (2004), in the special case of the sparseness-enforcing projection operator, and by Duchi et al. (2008), when the connection between projections onto a simplex and onto an L1 ball was studied.", "startOffset": 130, "endOffset": 150}, {"referenceID": 69, "context": "Lemma 13 is an elaborated version of a result from Theis et al. (2005), which is included here for completeness.", "startOffset": 51, "endOffset": 71}, {"referenceID": 69, "context": "The major arguments for this result have been taken over from Theis et al. (2005). Here, the statements from Lemma 15 have been incorporated and the resulting quadratic equation was solved explicitly, simplifying the original version of Theis et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 69, "context": "The major arguments for this result have been taken over from Theis et al. (2005). Here, the statements from Lemma 15 have been incorporated and the resulting quadratic equation was solved explicitly, simplifying the original version of Theis et al. (2005).", "startOffset": 62, "endOffset": 257}, {"referenceID": 7, "context": "The following definition is likewise to definitions from Chen and Ye (2011) and Michelot (1986):", "startOffset": 57, "endOffset": 76}, {"referenceID": 7, "context": "The following definition is likewise to definitions from Chen and Ye (2011) and Michelot (1986):", "startOffset": 57, "endOffset": 96}, {"referenceID": 47, "context": "An iterative algorithm was developed by Michelot (1986) which is very similar to Hoyer\u2019s original method for computation of the projection onto D.", "startOffset": 40, "endOffset": 56}, {"referenceID": 14, "context": "A simpler and more effective algorithm has been developed by Duchi et al. (2008). Building upon this work, Chen and Ye (2011) have proposed and rigorously proved correctness of a very similar algorithm, which is more explicit than that of Duchi et al.", "startOffset": 61, "endOffset": 81}, {"referenceID": 7, "context": "Building upon this work, Chen and Ye (2011) have proposed and rigorously proved correctness of a very similar algorithm, which is more explicit than that of Duchi et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 7, "context": "Building upon this work, Chen and Ye (2011) have proposed and rigorously proved correctness of a very similar algorithm, which is more explicit than that of Duchi et al. (2008). Their algorithm can be adapted to better suit the needs for the sparseness-enforcing projection.", "startOffset": 25, "endOffset": 177}, {"referenceID": 7, "context": "Proof The arguments from Chen and Ye (2011) hold for projections onto 4n.", "startOffset": 25, "endOffset": 44}, {"referenceID": 69, "context": "The following corollary states a similar result as in Theis et al. (2005). However, the proof here uses the notion of simplex projections instead of relying on pure analytical statements.", "startOffset": 54, "endOffset": 74}, {"referenceID": 69, "context": "The following corollary states a similar result as in Theis et al. (2005). However, the proof here uses the notion of simplex projections instead of relying on pure analytical statements. The result presented here is stronger, as multiple entries of the vector can be set to zero simultaneously, while in Theis et al. (2005) at most one entry can be zeroed out in a single iteration.", "startOffset": 54, "endOffset": 325}, {"referenceID": 53, "context": "The gradient for W follows using the chain rule and the product rule for matrix calculus, see Neudecker (1969) and Vetter (1970).", "startOffset": 94, "endOffset": 111}, {"referenceID": 53, "context": "The gradient for W follows using the chain rule and the product rule for matrix calculus, see Neudecker (1969) and Vetter (1970).", "startOffset": 94, "endOffset": 129}, {"referenceID": 63, "context": "The gradients of the similarity measure for classification capabilities are essentially equal to those of an ordinary two-layer neural network, and can be computed using the back-propagation algorithm (Rumelhart et al., 1986).", "startOffset": 201, "endOffset": 225}, {"referenceID": 16, "context": "However, the pairing of the softmax transfer function with the crossentropy error function provides a particularly simple structure of the gradient (Dunne and Campbell, 1997).", "startOffset": 148, "endOffset": 174}, {"referenceID": 53, "context": "Proof Basic matrix calculus (Neudecker, 1969; Vetter, 1970) yields \u2202sC(y, t)/\u2202\u03b8out =(\u2202sC(y, t)/\u2202y) \u00b7g\u2032(y), (\u2202sC(y, t)/\u2202Wout) T = h \u00b7 (\u2202sC(y, t)/\u2202\u03b8out) and (\u2202sC(y, t)/\u2202W) = x \u00b7 ( (\u2202sC(y, t)/\u2202\u03b8out) \u00b7W T out f \u2032(u) ) .", "startOffset": 28, "endOffset": 59}, {"referenceID": 73, "context": "Proof Basic matrix calculus (Neudecker, 1969; Vetter, 1970) yields \u2202sC(y, t)/\u2202\u03b8out =(\u2202sC(y, t)/\u2202y) \u00b7g\u2032(y), (\u2202sC(y, t)/\u2202Wout) T = h \u00b7 (\u2202sC(y, t)/\u2202\u03b8out) and (\u2202sC(y, t)/\u2202W) = x \u00b7 ( (\u2202sC(y, t)/\u2202\u03b8out) \u00b7W T out f \u2032(u) ) .", "startOffset": 28, "endOffset": 59}], "year": 2016, "abstractText": "Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.", "creator": "LaTeX with hyperref package"}}}