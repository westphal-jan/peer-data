{"id": "1603.08474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Deep Embedding for Spatial Role Labeling", "abstract": "This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.", "histories": [["v1", "Mon, 28 Mar 2016 18:38:46 GMT  (370kb,D)", "http://arxiv.org/abs/1603.08474v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["oswaldo ludwig", "xiao liu", "parisa kordjamshidi", "marie-francine moens"], "accepted": false, "id": "1603.08474"}, "pdf": {"name": "1603.08474.pdf", "metadata": {"source": "CRF", "title": "Deep Embedding for Spatial Role Labeling", "authors": ["Oswaldo Ludwig", "Xiao Liu", "Parisa Kordjamshidi", "Marie-Francine Moens"], "emails": [], "sections": [{"heading": null, "text": "This paper introduces the Visually Informed Embedding of Words (VIEW), a continuous vector representation of a word extracted from a deep neural model developed using the Microsoft COCO dataset to predict spatial arrangements between visual objects based on text description.The model consists of a deep multi-layered perceptron (MLP) stacked on top of a Long Short Term Memory (LSTM) network preceded by an embedding layer. VIEW is used to transfer multimodal background knowledge to spatial Role Labeling (SpRL) algorithms that detect spatial relationships between objects mentioned in text."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "2 Problem definition and research questions", "text": "The SpRL algorithm recognizes spatial objects in language (i.e., trajector and landmark) and their spatial relationship, which is signaled by the Spatial Indicator. The Trajector is a spatial role designation associated with a word or phrase that designates an object of a spatial scene, or more precisely, an object that moves. A landmark is a spatial role designation associated with a word or phrase that designates the location of that trajector object in the spatial scene. The Spatial Indicator is a spatial role designation associated with a word or phrase that indicates the spatial relationship between trajector and landmark. In this work, we apply the SpRL algorithm developed for the work Kordjamshidi & Moens, which models this problem as a structured prediction task, Taskar et al al al al al al al al al al al al al al al al al al al al al al al al al al al (2005) (that is called the relationship to the spatial element)."}, {"heading": "3 The model setting", "text": "The Microsoft COCO dataset Lin et al. (2014) is a collection of images with complex everyday scenes that contain common visual objects in their natural context.COCO contains photos of 91 object types with a total of 2.5 million labeled instances in 328k images. Each image has five written labels. The visual objects within the image are tightly equipped with labels from annotated segmentation masks, as in Fig.1.Our labeling system automatically directs a less specific spatial annotation about the relative position between the centers of the delimiters containing visual objects, given COCO's annotation of the coordinates of the delimiter fields. Our annotation type is governed by the predicates alone (v1, v2) and next (v1, v2), with v1 and v2 being visual objects, see Fig.2. This information is encoded in a sparse target vector, where Y is the first three positions coding the predictor in a vector."}, {"heading": "3.1 The embedding model", "text": "The input of our deep model is the textual information provided by COCO's captions. (i.e. 1) The sequence1 of words represented in a series of K-dimensional one-hot vectors, in which the vocabulary size of 8,000 words per target vector, yi, of 185 dimensions, as explained in the previous figure. (The terms with less than 30 words are1The terms encoded in a hot vector are sequentially encoded into a matrix Xi, the first columns of which are filled with all zeros. Each figure has five associated descriptions, giving five training examples with the same target vector, but different input vectors.Our deep is trained to the visual objects and their relationships. (i.e1)"}, {"heading": "4 Model Training", "text": "After evaluating various objective functions combined with different activity and weight control mechanisms available in Keras, we have decided to implement a custom objective function by collecting some ideas from support vector learning. Our training method yields the following limited optimization problem: min We, \u03b8l, \u03b8m1NeNo Ne. I = 1 No j = 1 max (1 \u2212 y (i, j) (2y (i, j) \u2212 1), 0) (9) reserved example: \"Wneul\" \u2264 1, l = 1,2,3 \"new (10), where the objective function (9) is the hinge loss with the jest position of the intended output vector for the ith caption, y (i, j).0, 1}, scaled and shifted to adopt the values \u2212 1 or 1, Ne is the cardinality of the training data, number 185 is the output dimension of the output vector."}, {"heading": "5 Applying the spatial-specific embedding in SpRL", "text": "We apply VIEW in SpRL by simply linking it to the original feature vector \u03c6word (wi) generated by the SpRL algorithm Kordjamshidi & Moens (2015) for the words that are eligible for sp, tr and lm."}, {"heading": "5.1 Selecting complementary Features", "text": "Our goal is to select complementary characteristics from the VIEW to maximize the mutual information between the target variable (here represented by the random scalar variable r and the selected characteristics represented by the random variables x1, X1. The method introduced in this section requires a random scalar variable, r, as the target variable. However, the target word of SemEval is a three-dimensional one-dimensional vector that specifies the spatial roles, i.e. sp, tr and lm. Therefore, we convert this binary number with 3 binary digits into a decimal number, i.e. a scalar.The mutual information is given by: I (x1,., xn; r) = H (x1,.)."}, {"heading": "5.2 Maximizing the F1", "text": "We start with the analysis of the simplest approach: max w, bF12 and b are the adaptable parameters of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity, by maximizing the production capacity of the production capacity of the production capacity, by maximizing the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity of the production capacity."}, {"heading": "6 Experimental Settings", "text": "In this section, our methods are evaluated using experiments in the benchmark data set SemEval-2013, Task 3, SpaceEval. We begin by evaluating the embedding model on COCO, then evaluate the contribution of spatial-specific embedding to the multi-class classification of words in spatial roles and finally to the structured prediction of spatial triplets using the algorithm of Kordjamshidi & Moens (2015)."}, {"heading": "6.1 Evaluating the embedding model on COCO", "text": "This subsection reports on the performance indices of the deep model described in Section 3.1 to predict our comments on the COCO test data. The model was trained on the basis of 135015 captions and evaluated on a test set consisting of 67505 captions. According to our experiments, the deep model performs best on the test data if it has a 200-dimensional word embedding and a 300-dimensional sentence embedding, which means that the embedding matrix has the dimension 8000 x 200 and the LSTM receives a 200-dimensional vector and produces a 300-dimensional vector, i.e. Nw = 200 and Ns = 300. The best setup for the MLP is 300 x 250 x 200 x 185, i.e. with two sigmoidal hidden layers containing 250 and 200 neurons, with the performance indices available on the test data for the model designed with the mean squared error (MSE)."}, {"heading": "6.2 Multiclass classification of word roles on SemEval", "text": "In this series of experiments on the classification of multi-class systems, an MLP is trained to classify words into spatial roles; the adopted MLP has a single sigmoid, hidden layer of 10 neurons and a linear output layer of 3 neurons that encode the output (i.e. the predicted class: sp, tr, lm or no spatial role) in a uniform vector style; the MLP receives as input the original features, \u03c6word (\u00b7), extracted by the same feature function used by the SpRL algorithm of Kordjamshidi & Moens (2015); these features are linked to features from the VIEW to access the benefits of using embedding; the MLP has been trained on 15092 records and evaluated on the basis of 3711 records that merge the move and test data sets of SemEval, Task 3, Spaceval, the results are summarized in Tables 2 and 3."}, {"heading": "6.3 Structured prediction of spatial triplets on SemEval", "text": "In this series of structured prediction experiments, we used the original Kordjamshidi & Moens SpRL algorithm (2015) not only to predict the spatial role of words, but also to compose words into triplets (sp, tr, lm), i.e. the structure d4https: / / code.google.com / p / word2vec / output. As explained in Section 2, the algorithm uses descriptive word vectors (sp, tr, lm) and word pairs (\u00b7, \u00b7). VIEW is associated with only one digit word (\u00b7) and plays a secondary role in this series of experiments. VIEW leads to performance gains in classifying words into roles sp and lm, as seen in Tables 7 and 8, which summarize the performance indices only using the original characteristics of word summaries (\u00b7,) and the use of two-words (sp, 7 and 8) in the classification and m."}, {"heading": "7 Conclusion", "text": "The experiments also provide evidence of the effectiveness of the algorithms for selecting complementary characteristics and maximizing F1 introduced in sections 5.1 and 5.2 to improve the gains made from the use of VIEW. As for future work, we aim to develop a method for optimizing F1 in the structured prediction environment by extending Joachim's (2005) work on structured classification settings. We believe that the results reported in this paper can improve as the amount of commented data increases. Note that, despite a large cardinality, the COCO dataset has a small variety of visual objects in its gold standard, i.e. it has only 91 object categories (including supercategories)."}], "references": [{"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A support vector method for multivariate performance measures", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the 22Nd International Conference on Machine Learning,", "citeRegEx": "Joachims and Thorsten.,? \\Q2005\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2005}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Li", "Fei Fei F"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Global machine learning for spatial ontology population", "author": ["Kordjamshidi", "Parisa", "Moens", "Marie-Francine"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Kordjamshidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kordjamshidi et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Novel maximum-margin training algorithms for supervised neural networks", "author": ["Ludwig", "Oswaldo", "Nunes", "Urbano"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Ludwig et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ludwig et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar", "Ben", "Chatalbashev", "Vassil", "Koller", "Daphne", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al.", "startOffset": 136, "endOffset": 159}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images.", "startOffset": 136, "endOffset": 359}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information.", "startOffset": 136, "endOffset": 680}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information. However, unlike most of the works on multimodal representation learning, which jointly map visual and textual information into a common embedding space, our work aims at providing embeddings only for words, but encoding spatial information extracted from the image annotations. The idea is to learn VIEW by pipelining an embedding layer into a deep architecture trained by back propagation to predict the spatial arrangement between the visual objects annotated in the pictures, given the respective textual descriptions. In this sense, unlike Karpathy et al. (2014) and Kiros et al.", "startOffset": 136, "endOffset": 1344}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information. However, unlike most of the works on multimodal representation learning, which jointly map visual and textual information into a common embedding space, our work aims at providing embeddings only for words, but encoding spatial information extracted from the image annotations. The idea is to learn VIEW by pipelining an embedding layer into a deep architecture trained by back propagation to predict the spatial arrangement between the visual objects annotated in the pictures, given the respective textual descriptions. In this sense, unlike Karpathy et al. (2014) and Kiros et al. (2014), we don\u2019t need a CNN, because the spatial information which is relevant for our purpose is provided directly by the position of the bounding boxes containing the visual objects annotated in the images, as detailed in Section 3.", "startOffset": 136, "endOffset": 1368}, {"referenceID": 11, "context": "In this work we apply the SpRL algorithm developed for the work Kordjamshidi & Moens (2015), which models this problem as a structured prediction task Taskar et al. (2005), that is, it jointly recognizes the spatial relation and its", "startOffset": 151, "endOffset": 172}, {"referenceID": 6, "context": "The Microsoft COCO data set Lin et al. (2014) is a collection of images featuring complex everyday scenes which contain common visual objects in their natural context.", "startOffset": 28, "endOffset": 46}, {"referenceID": 6, "context": "Figure 1: This figure shows an image from COCO with annotated bounding boxes and captions Lin et al. (2014).", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "7 instances per image (see Section 5 of Lin et al. (2014)).", "startOffset": 40, "endOffset": 58}, {"referenceID": 9, "context": "Therefore, we adopt an indirect approach by applying the principle of Max-Relevance and Min-Redundancy Peng et al. (2005). According to this principle it is possible to maximize (12) by jointly solving the following two problems: max i1,.", "startOffset": 103, "endOffset": 122}, {"referenceID": 8, "context": "mance of VIEW with the usual Word2Vec embedding Mikolov et al. (2013), we trained a skip-gram model on the same COCO captions as we trained VIEW (but without visual information) and concatenated it to the original SpRL features to generate the results of Table 6.", "startOffset": 48, "endOffset": 70}, {"referenceID": 8, "context": "usual Word2Vec embedding Mikolov et al. (2013) concatenated to the original SpRL features, using the SpRL algorithm with the same setup assumed for the experiments with VIEW.", "startOffset": 25, "endOffset": 47}], "year": 2016, "abstractText": "This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the F1 measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.", "creator": "LaTeX with hyperref package"}}}