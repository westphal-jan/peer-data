{"id": "1603.08604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Classification-based Financial Markets Prediction using Deep Neural Networks", "abstract": "Deep neural networks (DNNs) are powerful types of artificial neural networks (ANNs) that use several hidden layers. They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting. However their application to algorithmic trading has not been previously researched, partly because of their computational complexity. This paper describes the application of DNNs to predicting financial market movement directions. In particular we describe the configuration and training approach and then demonstrate their application to backtesting a simple trading strategy over 43 different Commodity and FX future mid-prices at 5-minute intervals. All results in this paper are generated using a C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster than the serial version and a Python strategy backtesting environment both of which are available as open source code written by the authors.", "histories": [["v1", "Tue, 29 Mar 2016 01:26:04 GMT  (665kb,D)", "http://arxiv.org/abs/1603.08604v1", null], ["v2", "Tue, 13 Jun 2017 19:49:53 GMT  (513kb,D)", "http://arxiv.org/abs/1603.08604v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CE", "authors": ["matthew dixon", "diego klabjan", "jin hoon bang"], "accepted": false, "id": "1603.08604"}, "pdf": {"name": "1603.08604.pdf", "metadata": {"source": "CRF", "title": "Classification-based Financial Markets Prediction using Deep Neural Networks", "authors": ["Matthew Dixon", "Diego Klabjan", "Jin Hoon Bang"], "emails": ["matthew.dixon@stuart.iit.edu", "d-klabjan@northwestern.edu", "jinhoonbang@u.northwestern.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to understand themselves and understand what it is all about, both in relation to people and in relation to themselves, and in relation to people who are able to understand and understand the world, and in relation to people who are able to understand and understand the world, and in relation to people who are able to understand and understand the world. In fact, it is the case that people are able to understand the world, understand the world, and understand what they understand and understand and understand the world."}, {"heading": "2 Deep Neural Networks", "text": "We start with mathematical precursor networks in a fully connected network with L layers, in which we find the current layer l (l) ij.Input layerIn order to find optimal weighting: = 1 \u2192 L between the nodes in the current layer l (l) ij.Input layerIn the next layer, each element of the vector is connected to each node in the next layer. Although not shown in the figure, the figure is associated with each node in the previous layer and the jth layer l is a weight shift w (l) ij.Input layerIn the order of optimal weighting w (l) l: = 1 \u2192 L between the nodes in the current layer l (l) ij.Input layerIn the order we find the optimal weighting w (l) l: = 1 \u2192 L between the nodes in a fully connected layer."}, {"heading": "2.1 Mini-batching", "text": "It is well known that mini-batching improves the computational power of the forward and backward calculations (Shekhar and Amin, 1994). We process b-observations in a mini-batch, which results in a change in the SGD algorithm and the dimensions of the data structures used to store variables. In particular, x, s and E now have a batch dimension. Note, however, that the dimensions of w (l) remain the same. The above equations can now be modified. In case of slight notation misuse, we define the dimensions \u03b4 (l), X (l), S (l), Rnl \u00b7 b, E < RnL \u00b7 b, where nl is the number of neurons in layer l and b the size of the mini-batch.The calculation of the sum in the forward network can be expressed as a matrix product."}, {"heading": "3 The Data", "text": "Our historical dataset contains 5-minute intermediate prices for 43 commodity and foreign exchange futures listed on the CME between March 31, 1991 and September 30, 2014, resulting in long sections of 5-minute candles with no price movements. Each feature is normalized by subtracting the mean and dividing by the standard deviation. The training set consists of 25,000 consecutive observations, and the test set consists of the next 12,500 observations. As described in Section 6, these sets are rolled forward ten times from the beginning of the liquid observation period at 1000 intervals of the observation period to the final 37,500 observations from March 31, 2005 to the end of the dataset."}, {"heading": "4 Implementation", "text": "The architecture of our network contains five completely interconnected layers. The first of the four hidden layers contains 1000 neurons and each subsequent layer is rejuvenated by 100. The last layer contains 135 initial neurons - three values per symbol for each of the 43 future contracts. The result of incorporating a large number of features and several hidden layers is that there are 12,174,500 weights in the totality. The weights are initialized with an Intel MKL VSL implementation of the random generator that uses the Mersenne Twistor (MT19937) routine. Gaussian random numbers are derived from the transformation of the uniform random numbers with an inverse Gaussian cumulative distribution function with zero mean and standard deviation of 0.01. We initialized the neuron distortions in the hidden layers with the constant 1. We used the same learning rate for all layers. The learning rate was adapted according to a heuristic approach, described in 2012 Critical Algorithm and Critical Method."}, {"heading": "5 Results", "text": "This section describes the backtesting of DNNs for a simple algo trading strategy. The purpose is to combine classification accuracy with strategy performance measurements and is not intended to provide exhaustive research into trading strategies. Figure 2 shows the DNN's classification accuracy across the 43 CME commodity and FX futures. The mean and standard deviation are also shown. Figure 3 shows the distribution of the average classification accuracy across 10 samples of DNN across the 45 CME commodity and FX futures. There is a higher density around an accuracy of 0.35, which is slightly better than random selection.Table 1 shows the five best instruments where the sample mean of the classification rate was highest on average during the ten forward trials. Also shown are the F1 values (\"harmonic averages\"), which are considered a more robust measure of performance due to lower sensitivity to class inequalities than classification accuracy."}, {"heading": "6 Strategy Backtesting", "text": "Using historical data from commodity futures at a 5-minute interval over the period from March 31, 1991 to September 30, 2014, this section describes the application of a \"walk forward\" optimization approach to review a simple trading strategy. Following the \"walk forward\" optimization approach described in Tomasini and Jaekle (2011), an initial optimization window of 25,000 5-minute observation periods or approximately 260 days (a little over a year) is chosen to train the model using all the symbol data and their constructed time series. The learning curriculum area is then swept to find the model that provides the best prediction rate from the sample - the highest classification rate from 12,500 consecutive 5-minute observation periods or approximately 130 days."}, {"heading": "6.1 Example trading strategy", "text": "For the sake of simplicity, the strategy only takes single contract positions; the strategy takes a short position and takes a long position if the label is 1, holds the position if the label is zero, and closes the long position and takes a short position if the label is -1. When calculating the cumulative P / E margin, the following assumptions are made: \u2022 the account is opened with 100 Kg USD; \u2022 there are sufficient excess liquid funds available to maintain the brokerage margin, by realizing the profit or otherwise; there are no limits on the minimum or maximum holding period and positions that can be held overnight; \u2022 the margin account is accepted to increase interest rates; \u2022 the transaction costs are ignored; and \u2022 the market is always sufficiently liquid to fill a limit in the middle of the price."}, {"heading": "7 Conclusion", "text": "Deep Neural Networks (DNNs) are a powerful type of artificial neural networks (ANNs) that use multiple hidden layers. In this paper, we describe the implementation and training of DNNs. Using a historical dataset of a 5-minute mean price of several CME-listed futures prices and other delays and filters, we observe that DNNs have considerable predictive capabilities as classifiers when they are simultaneously trained on marked data in multiple markets. We also demonstrate the use of DNNs to backtest a simple trading strategy and demonstrate the predictive accuracy and its relationship to the profitability of the strategy. All results in this paper are generated using a C + + implementation on the Intel Xeon Phi co-processor, which is 11.4x faster than the serial version and a Python strategy backtest environment, both of which are written as open source code by the authors."}, {"heading": "8 Acknowledgements", "text": "The authors gratefully acknowledge Intel Corporation's support in funding this research."}], "references": [{"title": "High technology ETF forecasting: Application of Grey Relational Analysis and Artificial Neural Networks", "author": ["J. Chen", "J.F. Diaz", "Y.F. Huang"], "venue": "Frontiers in Finance and Economics,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Implementing deep neural networks for financial market prediction on the intel xeon phi", "author": ["M. Dixon", "D. Klabjan", "J.H. Bang"], "venue": "In Proceedings of the 8th Workshop on High Performance Computational Finance,", "citeRegEx": "Dixon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dixon et al\\.", "year": 2015}, {"title": "Time series forecasting with neural networks: a comparative study using the air line data", "author": ["J. Faraway", "C. Chatfield"], "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),", "citeRegEx": "Faraway and Chatfield.,? \\Q1998\\E", "shortCiteRegEx": "Faraway and Chatfield.", "year": 1998}, {"title": "Intel Xeon Phi Coprocessor High Performance Programming", "author": ["J. Jeffers", "J. Reinders"], "venue": "USA, 1st edition,", "citeRegEx": "Jeffers and Reinders.,? \\Q2013\\E", "shortCiteRegEx": "Jeffers and Reinders.", "year": 2013}, {"title": "Forecasting futures trading volume using neural networks", "author": ["L. Kaastra", "M.S. Boyd"], "venue": "Journal of Futures Markets,", "citeRegEx": "Kaastra and Boyd.,? \\Q1995\\E", "shortCiteRegEx": "Kaastra and Boyd.", "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Trading system capability", "author": ["A.N.T. Kumiega", "B. Van Vliet"], "venue": "Journal of Quantitative Finance,", "citeRegEx": "Kumiega and Vliet.,? \\Q2014\\E", "shortCiteRegEx": "Kumiega and Vliet.", "year": 2014}, {"title": "Forecasting stock indices: a comparison of classification and level estimation models", "author": ["M. Leung", "H. Daouk", "A.Chen"], "venue": "International Journal of Forecasting,", "citeRegEx": "Leung et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Leung et al\\.", "year": 2000}, {"title": "Forecasting S&P 500 index using artificial neural networks and design of experiments", "author": ["S. Niaki", "S. Hoseinzade"], "venue": "Journal of Industrial Engineering International,", "citeRegEx": "Niaki and Hoseinzade.,? \\Q2013\\E", "shortCiteRegEx": "Niaki and Hoseinzade.", "year": 2013}, {"title": "Neural Networks in the Capital Markets", "author": ["A.-P. Refenes"], "venue": null, "citeRegEx": "Refenes.,? \\Q1994\\E", "shortCiteRegEx": "Refenes.", "year": 1994}, {"title": "Neural Networks: A Systematic Introduction", "author": ["R. Rojas"], "venue": null, "citeRegEx": "Rojas.,? \\Q1996\\E", "shortCiteRegEx": "Rojas.", "year": 1996}, {"title": "A scalable parallel formulation of the backpropagation algorithm for hypercubes and related architectures", "author": ["V.K.S. Shekhar", "M.B. Amin"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "Shekhar and Amin.,? \\Q1994\\E", "shortCiteRegEx": "Shekhar and Amin.", "year": 1994}, {"title": "Trading equity index futures with a neural network", "author": ["R.R. Trippi", "D. DeSieno"], "venue": "The Journal of Portfolio Management,", "citeRegEx": "Trippi and DeSieno.,? \\Q1992\\E", "shortCiteRegEx": "Trippi and DeSieno.", "year": 1992}, {"title": "Designing Stock Market Trading Systems: With and Without Soft Computing", "author": ["B. Vanstone", "T. Hahn"], "venue": "Harriman House,", "citeRegEx": "Vanstone and Hahn.,? \\Q2010\\E", "shortCiteRegEx": "Vanstone and Hahn.", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting.", "startOffset": 109, "endOffset": 134}, {"referenceID": 2, "context": "While the application of artificial neural networks (ANNs) to time series methods are well documented (Faraway and Chatfield, 1998; Refenes, 1994; Trippi and DeSieno, 1992; Kaastra and Boyd, 1995) their proneness to over-fitting, convergence problems, and difficulty of implementation raised concerns.", "startOffset": 102, "endOffset": 196}, {"referenceID": 9, "context": "While the application of artificial neural networks (ANNs) to time series methods are well documented (Faraway and Chatfield, 1998; Refenes, 1994; Trippi and DeSieno, 1992; Kaastra and Boyd, 1995) their proneness to over-fitting, convergence problems, and difficulty of implementation raised concerns.", "startOffset": 102, "endOffset": 196}, {"referenceID": 12, "context": "While the application of artificial neural networks (ANNs) to time series methods are well documented (Faraway and Chatfield, 1998; Refenes, 1994; Trippi and DeSieno, 1992; Kaastra and Boyd, 1995) their proneness to over-fitting, convergence problems, and difficulty of implementation raised concerns.", "startOffset": 102, "endOffset": 196}, {"referenceID": 4, "context": "While the application of artificial neural networks (ANNs) to time series methods are well documented (Faraway and Chatfield, 1998; Refenes, 1994; Trippi and DeSieno, 1992; Kaastra and Boyd, 1995) their proneness to over-fitting, convergence problems, and difficulty of implementation raised concerns.", "startOffset": 102, "endOffset": 196}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010).", "startOffset": 123, "endOffset": 195}, {"referenceID": 8, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010).", "startOffset": 123, "endOffset": 195}, {"referenceID": 13, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010).", "startOffset": 123, "endOffset": 195}, {"referenceID": 5, "context": "They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition.", "startOffset": 117, "endOffset": 142}, {"referenceID": 3, "context": "We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013).", "startOffset": 140, "endOffset": 168}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010). A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition. The field is referred to as \u201dDeep Learning\u201d. In this paper, we shall use DNNs to partially address some of the historical deficiencies of ANNs. Specifically, we model complex non-linear relationships between the independent variables and dependent variable and reduced tendency to overfit. In order to do this we shall exploit advances in low cost many-core accelerator platform to train and tune the parameters of our model. For financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention and shall be the approach used here. Back-propagation and gradient descent have been the preferred method for training these structures due to the ease of implementation and their tendency to converge to better local optima in comparison with other trained models. However, these methods can be computationally expensive, especially when used to train DNNs. There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. Sweeping through the parameter space for optimal parameters is not feasible due to the cost in time and computational resources. We shall use mini-batching (computing the gradient on several training examples at once rather than individual examples) as one common approach to speeding up computation. We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013). General purpose hardware optimized implementations of the back-propagation algorithm are described by Shekhar and Amin (1994), however our approach is tailored for the Intel Xeon Phi co-processor.", "startOffset": 124, "endOffset": 2182}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010). A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition. The field is referred to as \u201dDeep Learning\u201d. In this paper, we shall use DNNs to partially address some of the historical deficiencies of ANNs. Specifically, we model complex non-linear relationships between the independent variables and dependent variable and reduced tendency to overfit. In order to do this we shall exploit advances in low cost many-core accelerator platform to train and tune the parameters of our model. For financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention and shall be the approach used here. Back-propagation and gradient descent have been the preferred method for training these structures due to the ease of implementation and their tendency to converge to better local optima in comparison with other trained models. However, these methods can be computationally expensive, especially when used to train DNNs. There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. Sweeping through the parameter space for optimal parameters is not feasible due to the cost in time and computational resources. We shall use mini-batching (computing the gradient on several training examples at once rather than individual examples) as one common approach to speeding up computation. We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013). General purpose hardware optimized implementations of the back-propagation algorithm are described by Shekhar and Amin (1994), however our approach is tailored for the Intel Xeon Phi co-processor. The main contribution of this paper is to describe the application of deep neural networks to financial time series data in order to classify financial market movement directions. Traditionally, researchers will iteratively experiment with a handful of signals to train a level based method, such as vector autoregression, for each instrument (see for example Kaastra and Boyd (1995); Refenes (1994); Trippi and DeSieno (1992)).", "startOffset": 124, "endOffset": 2637}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010). A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition. The field is referred to as \u201dDeep Learning\u201d. In this paper, we shall use DNNs to partially address some of the historical deficiencies of ANNs. Specifically, we model complex non-linear relationships between the independent variables and dependent variable and reduced tendency to overfit. In order to do this we shall exploit advances in low cost many-core accelerator platform to train and tune the parameters of our model. For financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention and shall be the approach used here. Back-propagation and gradient descent have been the preferred method for training these structures due to the ease of implementation and their tendency to converge to better local optima in comparison with other trained models. However, these methods can be computationally expensive, especially when used to train DNNs. There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. Sweeping through the parameter space for optimal parameters is not feasible due to the cost in time and computational resources. We shall use mini-batching (computing the gradient on several training examples at once rather than individual examples) as one common approach to speeding up computation. We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013). General purpose hardware optimized implementations of the back-propagation algorithm are described by Shekhar and Amin (1994), however our approach is tailored for the Intel Xeon Phi co-processor. The main contribution of this paper is to describe the application of deep neural networks to financial time series data in order to classify financial market movement directions. Traditionally, researchers will iteratively experiment with a handful of signals to train a level based method, such as vector autoregression, for each instrument (see for example Kaastra and Boyd (1995); Refenes (1994); Trippi and DeSieno (1992)).", "startOffset": 124, "endOffset": 2653}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010). A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition. The field is referred to as \u201dDeep Learning\u201d. In this paper, we shall use DNNs to partially address some of the historical deficiencies of ANNs. Specifically, we model complex non-linear relationships between the independent variables and dependent variable and reduced tendency to overfit. In order to do this we shall exploit advances in low cost many-core accelerator platform to train and tune the parameters of our model. For financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention and shall be the approach used here. Back-propagation and gradient descent have been the preferred method for training these structures due to the ease of implementation and their tendency to converge to better local optima in comparison with other trained models. However, these methods can be computationally expensive, especially when used to train DNNs. There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. Sweeping through the parameter space for optimal parameters is not feasible due to the cost in time and computational resources. We shall use mini-batching (computing the gradient on several training examples at once rather than individual examples) as one common approach to speeding up computation. We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013). General purpose hardware optimized implementations of the back-propagation algorithm are described by Shekhar and Amin (1994), however our approach is tailored for the Intel Xeon Phi co-processor. The main contribution of this paper is to describe the application of deep neural networks to financial time series data in order to classify financial market movement directions. Traditionally, researchers will iteratively experiment with a handful of signals to train a level based method, such as vector autoregression, for each instrument (see for example Kaastra and Boyd (1995); Refenes (1994); Trippi and DeSieno (1992)).", "startOffset": 124, "endOffset": 2680}, {"referenceID": 0, "context": "As such, there has been a recent resurgence in the method, in part facilitated by advances in modern computer architecture (Chen et al., 2013; Niaki and Hoseinzade, 2013; Vanstone and Hahn, 2010). A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. They have been popularized in the artificial intelligence community for their successful use in image classification (Krizhevsky et al., 2012) and speech recognition. The field is referred to as \u201dDeep Learning\u201d. In this paper, we shall use DNNs to partially address some of the historical deficiencies of ANNs. Specifically, we model complex non-linear relationships between the independent variables and dependent variable and reduced tendency to overfit. In order to do this we shall exploit advances in low cost many-core accelerator platform to train and tune the parameters of our model. For financial forecasting, especially in multivariate forecasting analysis, the feed-forward topology has gained much more attention and shall be the approach used here. Back-propagation and gradient descent have been the preferred method for training these structures due to the ease of implementation and their tendency to converge to better local optima in comparison with other trained models. However, these methods can be computationally expensive, especially when used to train DNNs. There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. Sweeping through the parameter space for optimal parameters is not feasible due to the cost in time and computational resources. We shall use mini-batching (computing the gradient on several training examples at once rather than individual examples) as one common approach to speeding up computation. We go further by expressing the back-propagation algorithm in a form that is amenable to fast performance on an Intel Xeon Phi co-processor (Jeffers and Reinders, 2013). General purpose hardware optimized implementations of the back-propagation algorithm are described by Shekhar and Amin (1994), however our approach is tailored for the Intel Xeon Phi co-processor. The main contribution of this paper is to describe the application of deep neural networks to financial time series data in order to classify financial market movement directions. Traditionally, researchers will iteratively experiment with a handful of signals to train a level based method, such as vector autoregression, for each instrument (see for example Kaastra and Boyd (1995); Refenes (1994); Trippi and DeSieno (1992)). More recently, however, Leung et al. (2000) provide evidence that classification based methods outperform level based methods in the prediction of the direction of stock movement and trading returns maximization.", "startOffset": 124, "endOffset": 2726}, {"referenceID": 10, "context": "Stochastic Gradient Descent Following Rojas (1996), we now revisit the backpropagation learning algorithm based on the method of stochastic gradient descent (SGD) algorithm.", "startOffset": 38, "endOffset": 51}, {"referenceID": 11, "context": "It is well known that mini-batching improves the computational performance of the feedforward and back-propagation computations (Shekhar and Amin, 1994) .", "startOffset": 128, "endOffset": 152}, {"referenceID": 5, "context": "The learning rate was adjusted according to a heuristic which is described in Algorithm 2 below and is similar to the approach taken by Krizhevsky et al. (2012) except that we use cross entropy rather than the validation error.", "startOffset": 136, "endOffset": 161}, {"referenceID": 1, "context": "The mini-batching formulation of the algorithm facilitates efficient parallel implementation, the details and timings of which are described by Dixon et al. (2015). The overall time to train a DNN on an Intel Xeon Phi using the data described above is approximately 10 hours when factoring in time for calculation of error measures on the test set and thus the training can be run as an overnight batch job.", "startOffset": 144, "endOffset": 164}], "year": 2016, "abstractText": "Deep neural networks (DNNs) are powerful types of artificial neural networks (ANNs) that use several hidden layers. They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting. However their application to algorithmic trading has not been previously researched, partly because of their computational complexity. This paper describes the application of DNNs to predicting financial market movement directions. In particular we describe the configuration and training approach and then demonstrate their application to backtesting a simple trading strategy over 43 different Commodity and FX future mid-prices at 5-minute intervals. All results in this paper are generated using a C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster than the serial version and a Python strategy backtesting environment both of which are available as open source code written by the authors.", "creator": "LaTeX with hyperref package"}}}