{"id": "1603.08702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations", "abstract": "ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.", "histories": [["v1", "Tue, 29 Mar 2016 10:00:40 GMT  (444kb)", "http://arxiv.org/abs/1603.08702v1", "in LREC 2016"]], "COMMENTS": "in LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "alessandro lenci", "tin-shing chiu", "qin lu", "chu-ren huang"], "accepted": false, "id": "1603.08702"}, "pdf": {"name": "1603.08702.pdf", "metadata": {"source": "CRF", "title": "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations", "authors": ["Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang"], "emails": ["esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@unipi.it"], "sections": [{"heading": null, "text": "ROOT13 (Santus et al., 2016) was introduced based on a random forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross-validation of 9,600 pairs evenly distributed among the three classes and covering several parts of the language (i.e. adjectives, nouns and verbs). If all classes are present, ROOT9 achieves an F1 value of 90.7%, compared to a baseline of 57.2% (vector cosine). If the classification is binary, ROOT9 achieves the following results compared to baseline: Hypernyms co-hyponyms 95.7% vs. 69.8%, Hypernyms randomes 91.8% vs. 64.1%, and Co-hyponyms randomes 97.8% vs. 79.4%. To compare performance with state-of-the-the-kind, we have ROOT9 in subsets of co-hyponyoma randomes vs. 97.8% and 64.1% randomly."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Related Work", "text": "Since the pioneering work of Hearst (1992), which used a pattern based on the approach of \"automatic adoption of hyponyms from large text corpora,\" a large number of distribution methods were applied to the identification of hypernyms, which are based on interpretations of the distribution hypothesis (Harris, 1954), according to which the meaning of a linguistic expression can be derived from its distribution in text corpora, so that linguistic expressions occurring in similar contexts are likely to be similar. These approaches, which can either be monitored or not monitored, are generally used with vector-space models (VSMs; also referred to as distribution models, DSMs), where vectors represent words and their dimensions weighted the association between the words and the contexts (Turney and Pantel, 2010). Among the unverified methods are Weeds and Weir (Weir), which suggest distributions of hypostions that represent hypostions."}, {"heading": "3. Method", "text": "ROOT13 was first introduced in Santus et al. (2016b). It uses the Random Forest algorithm implemented in Weka (Breiman, 2001) with default settings (i.e. 100 trees, 1 seed and maxDepth and numFeatures initialized to 0) and is based on thirteen characteristics, which are described in detail below. Each of these characteristics is automatically extracted from a window-based DSM, trained on a combination of ukWaC and WaCkypedia corpora (approximately 2.7 billion words), counting word coincidences within the five closest substantive words on the left and right of each target. DSM only includes adjectives, nouns and verbs with a frequency above 1,000. As the evaluation will show, four out of thirteen characteristics were redundant and did not contribute to system performance, so they were dropped, making ROOT13 ROOT9."}, {"heading": "3.1 Features", "text": "The feature set is designed to identify multiple distribution properties that characterize the terms in the pairs. In addition to the standard distribution characteristics (e.g. frequency of example and frequency of words), we have added some information that has proven effective in distinguishing paradigmatic semantic relationships in vector spaces (Santus et al., 2014a; Santus et al., 2016a). All characteristics have been calculated using the DSM mentioned above and normalized in the range of 0-1."}, {"heading": "3.1.1 Co-Occurrence", "text": "Cooc is defined as the frequency of the simultaneous occurrence of the two terms within the DSM window. According to the co-occurrence hypothesis (Charles and Miller, 1989), this measure of synonyms and antagonyms is discriminatory: antagonyms indeed occur more frequently in the same sentence than synonyms. Since co-hyponyms can often be regarded as a specific type of opposition (e.g. \"winter or summer?\"; Murphy 2003), this measure should help distinguish them from hypernyms and randomes (Santus et al., 2014b-c)."}, {"heading": "3.1.2 Frequency", "text": "Frequency is an important property of words and a very discriminatory information. As far as our task is concerned, Weeds and Weir (2003) have shown that the frequency base was very competitive in determining the directivity of hypernymia-related pairs. We can therefore expect that hypernyms have a higher frequency than hyponyms. Frequency is included in our model with three characteristics, namely one for each word involved in the pair (Freq1,2), plus one that stores the difference between frequencies (Diff Freq)."}, {"heading": "3.1.3 Entropy", "text": "This year, the time has come for only one person to be able to establish himself in the EU."}, {"heading": "3.1.5 Contexts Frequency", "text": "We have found that hypernyms tend to occur more frequently than co-hyponyms and coincidences. Our system uses two characteristics, C-Freq1,2, and captures the frequency of the top contexts of the target words in the pair."}, {"heading": "3.1.6 Contexts Entropy", "text": "Considering what is mentioned in 3.1.3 and in the DIH and DInH (Weeds and Weir, 2003; Santus et al., 2014a), general words are likely to occur in a greater variety of contexts (i.e. higher frequency) and in broader (i.e. less informative) contexts compared to certain words. Indeed, hypernyms can certainly occur in narrower contexts, but certain words are more likely to be chosen in these situations. Consider the following sentences: a) X has barked all night, but we expect such general words to be used less often in these contexts, as their hyponyms are more appropriate. Of course, X could also have been an animal and Y-human or - even - both - X and Y - a mammal, but we expect their hyponyms to be used less frequently in these contexts, as their hyperyms are more appropriate."}, {"heading": "4. Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Tasks", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.4 Baselines and Other Models", "text": "For our internal testing, we have implemented two baselines that can be used as a reference for evaluating the performance of ROOT9: COSINE and RANDOM13. The first baseline simply uses the Cosine vector (COSINE) with a Random Forest classifier in the default settings (i.e. 100 trees, 1 seed and maxDepth and numFeatures initialized to 0. This baseline should be used particularly well in discriminating against similar words (i.e. Hypernys and Co-Hyponyms) of Random. In fact, this measure has been widely used to identify word similarities in vector spaces (Turney and Pantel, 2010) because it shows the normalized correlation between the vectors of 1 and 2. \""}, {"heading": "5. Task 1: Ablation Test and Binary Classification", "text": "Considering the thirteen features of ROOT13 (Santus et al., 2016b), we removed them one by one and measured the loss (negative) or gain (positive). However, as is easily seen from the table, most features contribute to an increase of between 1.12% and 2.46%. (2016a) Interestingly, the highest contribution comes from the C-Entr1,2 inspired by SLQS (Santus et al., 2014a), and the second highest contribution is given by APSyn introduced in Santus et al. (2016a) Interestingly, four of the thirteen features do not contribute to penalizing performance anywhere between 0.11% and 0.34%. These features are Diff Entr, Diff Freq, Co-Occurrence, and APSyn and Shared when used together."}, {"heading": "6. Task 2: ROOT9 vs. State of the Art", "text": "In Table 4, we show the performance of ROOT9 compared to the best systems reported by Weeds et al. (2014), which are all calculated on subsets of Weeds et al. (2014)'s datasets, as reported in Section 4.3. Taking into account all datasets, ROOT9 is the second best performing system, after svmCAT (Weeds et al., 2014), which uses the SVM classifier on the concatenation of PPMI vectors and includes as characteristics all important grammatical dependencies with open class parts of the language. SVM classifiers on the sum (svmADD) and multiplication (svmMULT) of the same PPMI vectors perform better in identifying co-hyponyms, but worst in identifying hypernyms (SvmDIFF) and on the second PPMI vector (svmSLE)."}, {"heading": "7. Task 3: Learning Prototypical Hypernyms?", "text": "Finally, we tried to verify the claim of Levy et al. (2015) by evaluating the classifier on a data set of 3,200 hypernyms and 3,200 hypernyms (e.g. apple-RANDOM fruit and dog-RANDOM fruit). In this evaluation, we found that a large number of switched hypernyms were actually misclassified as hypernyms (up to 100% of which if the words in the tested switched pairs were exactly the same as hypernyms in the training set). In an attempt to correct the behavior of the classifier, we expanded the original data set of 9,600 pairs with other 3,200 switched hypernyme pairs designated as random. It is relevant to note that the switched hypernyms (marked as random) contain the same words that were also used for the real hypernyms, and that in this new data set the total random size of the 6,400 pairs is twice as high as the other validated classes including a loss of only 6,400."}, {"heading": "8. Conclusions", "text": "In this paper, we described ROOT9, a classifier for hypernyms, co-hyponyms, and random words derived from an optimization of ROOT13 (Santus et al., 2016b). The classifier, based on the Random Forest algorithm, uses only nine unattended corpus-based features that were described and their contribution evaluated. The impressive results in our dataset, which was developed by randomly extracting 9,600 pairs of EVALution (Santus et al., 2015), Lenci / Benotto (Benotto, 2015), and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al. (2014). The comparison showed that ROOT9 is actually competitive with the state of the art by using only an SVM that is trained on chained PMI hyper-perm (we randomly exceeded the hyper-perm classification)."}, {"heading": "9. Acknowledgements", "text": "We are very grateful for Julie Weeds for having helped us, recalculating the results of the Weeds et al. (2014) models also for our subsets of their datasets. Thanks also to Aristotelis Kostopoulos for the valuable proposals. This work is partially funded by HK PhD Fellowship Scheme under PF12-1365610. Main ReferencesBaroni, M., Shan, C.-C. (2012). How we BLESSeddistributional semantic evaluation. \"EMNLP 2011.Baroni, M., Bernardi, R., Do, N.-Q., Shan, C.-C. (2012). Entailment above the word level in distributional semantics. In Proceedings of ACL 2012, pages 23-32, Avignon, France. Benotto, Giulia. (2015). Distributional Models forSemantic Relations forSemantic Relations forSemantic Relations."}], "references": [{"title": "How we BLESSed distributional semantic evaluation", "author": ["M. BLESS. In: Baroni", "A. Lenci"], "venue": "Proceedings of the EMNLP", "citeRegEx": "Baroni and Lenci,? \\Q2011\\E", "shortCiteRegEx": "Baroni and Lenci", "year": 2011}, {"title": "Learning to Distinguish Hypernyms and Co-Hyponyms", "author": ["J. BLESS Hyper-Co-Hyp: In: Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller"], "venue": "Proceedings of COLING", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "author": ["E. EVALution. In: Santus", "F. Yung", "A. Lenci", "Huang C-R"], "venue": "Proceedings of the 4th Workshop on Linked Data in Linguistics,", "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Distributional Models for Semantic Relations: A Sudy on Hyponymy and Antonymy", "author": ["Lenci/Benotto. In: Benotto", "Giulia"], "venue": "PhD Thesis,", "citeRegEx": "Benotto and Giulia.,? \\Q2015\\E", "shortCiteRegEx": "Benotto and Giulia.", "year": 2015}, {"title": "Learning to Distinguish Hypernyms and Co-Hyponyms", "author": ["J. WN Hyper-Co-Hyp: In: Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller"], "venue": "Proceedings of COLING", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014)", "startOffset": 107, "endOffset": 127}, {"referenceID": 1, "context": "Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014).", "startOffset": 138, "endOffset": 158}, {"referenceID": 1, "context": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015).", "startOffset": 211, "endOffset": 258}, {"referenceID": 1, "context": ", 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al.", "startOffset": 8, "endOffset": 302}, {"referenceID": 1, "context": ", 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.", "startOffset": 8, "endOffset": 334}, {"referenceID": 2, "context": "The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011).", "startOffset": 143, "endOffset": 164}, {"referenceID": 0, "context": ", 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011).", "startOffset": 52, "endOffset": 76}, {"referenceID": 1, "context": "In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the Weeds et al. (2014) datasets.", "startOffset": 86, "endOffset": 106}, {"referenceID": 1, "context": "datasets only by the svmCAT model (Weeds et al., 2014), which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs.", "startOffset": 34, "endOffset": 54}, {"referenceID": 2, "context": "Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts. In their evaluation, the authors have shown that hypernyms\u2019 most typical contexts are in fact less informative than hyponyms\u2019 ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation", "startOffset": 0, "endOffset": 396}, {"referenceID": 1, "context": "(2014) used the vectors\u2019 difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.", "startOffset": 43, "endOffset": 63}, {"referenceID": 1, "context": "(2014) used the vectors\u2019 difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014)\u2019s observation was further investigated by Levy et al.", "startOffset": 43, "endOffset": 554}, {"referenceID": 1, "context": "(2014) used the vectors\u2019 difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014)\u2019s observation was further investigated by Levy et al. (2015), who claimed that supervised methods learn whether a term y is a prototypical hypernym, regardless of", "startOffset": 43, "endOffset": 615}, {"referenceID": 2, "context": "ROOT13 was firstly introduced in Santus et al. (2016b). It uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings (i.", "startOffset": 33, "endOffset": 55}, {"referenceID": 2, "context": "4 Shared and APSyn Shared and APSyn (Santus et al., 2016a-b) are two features that do not rely on the full distribution of the words, but on the top N most related contexts to the words in a pair, where N was empirically fixed at 1000. The value of this parameter was tested in other experiments, some of which reported in Santus et al. (2016a). Differently from Santus et al.", "startOffset": 37, "endOffset": 345}, {"referenceID": 2, "context": "Adopting a similar approach to Santus et al. (2014a), we have measured the average entropy of the top N", "startOffset": 31, "endOffset": 53}, {"referenceID": 1, "context": "2); ii) an evaluation against the state of the art, and \u2013 in particular \u2013 against the best performant models in Weeds et al. (2014); iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms", "startOffset": 112, "endOffset": 132}, {"referenceID": 1, "context": "proposed by Weeds et al. (2014). These datasets are described below, in Section 4.", "startOffset": 12, "endOffset": 32}, {"referenceID": 1, "context": "proposed by Weeds et al. (2014). These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in Weeds et al. (2014). The last task is described in Section 7.", "startOffset": 12, "endOffset": 190}, {"referenceID": 2, "context": "datasets: EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), which is freely available at https://github.", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), which is freely available at https://github.", "startOffset": 49, "endOffset": 73}, {"referenceID": 1, "context": "3 Weeds Dataset In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by Weeds et al. (2014). 2 These are four datasets, containing respectively:", "startOffset": 117, "endOffset": 137}, {"referenceID": 1, "context": "The WN dataset (Weeds et al., 2014) \u2013 meaning both WN Hyper and WN Co-Hyp \u2013 in particular, was built after noticing that supervised systems tended to perform well also on random vectors.", "startOffset": 15, "endOffset": 35}, {"referenceID": 1, "context": "However, Weeds et al. (2014) kindly provided", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "Coverage on Weeds et al. (2014)\u2019s datasets.", "startOffset": 12, "endOffset": 32}, {"referenceID": 1, "context": "The discrepancy with what found by Weeds et al. (2014) \u2013 namely that random vectors perform particularly well when words are re-used in the dataset \u2013 may depend on the small number of features, which does not allow the system to identify discriminative random dimensions.", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "The discrepancy with what found by Weeds et al. (2014) \u2013 namely that random vectors perform particularly well when words are re-used in the dataset \u2013 may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in Weeds et al. (2014), namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of", "startOffset": 35, "endOffset": 394}, {"referenceID": 1, "context": "3 The subsets of Weeds et al. (2014)\u2019s datasets are also available at https://github.", "startOffset": 17, "endOffset": 37}, {"referenceID": 2, "context": "The highest contribution comes from the C-Entr1,2, which were inspired at SLQS (Santus et al., 2014a), and the second highest contribute is given by APSyn, which was introduced in Santus et al. (2016a). Interestingly, four out of thirteen features were not contributing, penalizing the performance somewhere between 0.", "startOffset": 80, "endOffset": 202}, {"referenceID": 1, "context": "However, it is worth noticing here that such difference disappears with the WN datasets proposed by Weeds et al. (2014). See section 6, and \u2013 in particular \u2013 Table 4.", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "Considering all the datasets, ROOT9 is the second best performing system, after svmCAT (Weeds et al., 2014), which uses the SVM classifier on the concatenation of", "startOffset": 87, "endOffset": 107}, {"referenceID": 1, "context": "In Table 4, we show ROOT9\u2019s performance compared to the best systems reported by Weeds et al. (2014). The scores are all calculated on subsets of Weeds et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 1, "context": "In Table 4, we show ROOT9\u2019s performance compared to the best systems reported by Weeds et al. (2014). The scores are all calculated on subsets of Weeds et al. (2014)\u2019s datasets, as reported in Section 4.", "startOffset": 81, "endOffset": 166}, {"referenceID": 1, "context": "STATE OF THE ART (Weeds et al., 2014)", "startOffset": 17, "endOffset": 37}, {"referenceID": 2, "context": "The impressive results in our dataset, developed by randomly extracting 9,600 pairs from EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 0, "context": ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 0, "context": ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al. (2014). The comparison has", "startOffset": 50, "endOffset": 164}, {"referenceID": 1, "context": "We are very thankful to Julie Weeds for having helped us, recalculating the results of the Weeds et al. (2014) models also for our subsets of their datasets.", "startOffset": 91, "endOffset": 111}], "year": 2016, "abstractText": "ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.", "creator": "Microsoft\u00ae Word 2010"}}}