{"id": "1603.08776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "COCO: The Experimental Procedure", "abstract": "We present an experimental setup and procedure for benchmarking numerical optimization algorithms in a black-box scenario. This procedure can be applied with the COCO benchmarking platform. We describe initialization of and input to the algorithm and touch upon the relevance of termination and restarts. We introduce the concept of recommendations for benchmarking with COCO. Recommendations detach the solutions used in function calls from the anytime performance assessment of the algorithm.", "histories": [["v1", "Tue, 29 Mar 2016 14:10:14 GMT  (20kb,D)", "http://arxiv.org/abs/1603.08776v1", null], ["v2", "Thu, 19 May 2016 11:58:22 GMT  (20kb,D)", "http://arxiv.org/abs/1603.08776v2", "ArXiv e-prints,arXiv:1603.08776"]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["nikolaus hansen", "tea tusar", "olaf mersmann", "anne auger", "dimo brockhoff"], "accepted": false, "id": "1603.08776"}, "pdf": {"name": "1603.08776.pdf", "metadata": {"source": "CRF", "title": "COCO: The Experimental Procedure", "authors": ["Nikolaus Hansen", "Tea Tusar", "Olaf Mersmann", "Anne Auger", "Dimo Brockhoff"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction 2", "text": "1.1 Terminology..................................................."}, {"heading": "2 Conducting the Experiment 2", "text": "2.1 Initialization and input to the algorithm..................... 3 2.2 Budget, cancellation criteria and restarts............... 4"}, {"heading": "3 Parameter Setting and Tuning of Algorithms 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Recommendations 5", "text": "5 Time complexity experiment 5ar Xiv: 160 3.08 776v 1 [cs.A I] 2 9"}, {"heading": "1 Introduction", "text": "Based on [HAN2009] and [HAN2010], we describe a comparatively simple test setup for benchmarking black box optimization. We recommend using this method within the COCO platform [HAN2016co]. 1Our central measure of the performance to which the test method is adapted is the number of evaluations of the objective function to achieve a certain solution quality (functional value or value or indicator value), which is also referred to as runtime."}, {"heading": "1.1 Terminology", "text": "Function We speak of a function as a parameterized mapping of R \u2192 R with scalable input space, i.e., that is not (yet) determined, and usually 1, 2}. Functions are parameterized in such a way that different instances of the \"same\" function are available, e.g. translated or moved versions. Problem We speak of a problem, coco _ problem _ t, as a specific function instance on which the optimization algorithm is executed. Specifically, a problem can be described as a triple (dimension, function, instance). A problem can be evaluated and returns a -value or -vector. In the context of performance evaluation, each problem is assigned a target or indicator value. In other words, a target value is added to the triple, in order to define a single problem in this case. Runtime We define runtime or runtime [HOO1998] as the number of evaluations performed on a given problem, also as the number of function evaluations, to determine a target value from a series of measurement types to a series of target evaluations, Our problem is determined by a series of central measurements to determine a series of target values."}, {"heading": "2 Conducting the Experiment", "text": "For each problem, the same algorithm is used with the same parameter setting, the same initialization procedure, the same budget, the same termination and / or restart criteria, etc. There is no mandatory minimum or maximum allowed budget. The longer the experiment takes, the more data is available to accurately evaluate performance. See also Section Budget, Termination Criteria, and Restart. 1 The COCO Platform provides multiple test suites (single and dual objective) with a collection of black box optimization problems of different dimensions to be minimized. COCO collects the relevant data automatically to view the results after post-processing."}, {"heading": "2.1 Initialization and Input to the Algorithm", "text": "An algorithm can use the following input information from each problem. For initialization: Input and output dimensions as the defining interface to the problem, in particular: \u2022 The search space (input) dimension via coco _ problem _ get _ dimension, \u2022 The number of targets via coco _ problem _ get _ number _ of _ objectives, which is the \"output\" dimension of coco _ evaluate _ function. All functions of a single benchmark suite have the same number of targets, currently either one or two. \u2022 The number of constraints via coco _ problem _ get _ number _ of _ constraints, which is the \"output\" dimension of coco _ evaluate _ constraint."}, {"heading": "2.2 Budget, Termination Criteria, and Restarts", "text": "We consider the budget, termination criteria and restarts to be part of the benchmark algorithm. Eligible are algorithms with any budget for function evaluations. Choosing termination is a relevant part of the algorithm. On the one hand, in order to effectively use a larger number of function evaluations, we increase the chance of achieving better function values. On the other hand, timely termination of a stagnant run can improve performance, as these evaluations can be used more effectively. 4In order to effectively use a large number of function evaluations, we encourage independent restarts 5, especially for algorithms that are terminated naturally within a comparatively small budget. Independent restarts do not alter the central performance metric 6, but improve reliability, comparability 7, precision and \"visibility\" of the measured results. In addition, any multistart procedure (which is dependent on a provisional termination of the algorithm) is encouraged. Multistarts must not be independent, as they may have a 2005 sweep parameter [e.g. an increase in the ARG population] based on [an increase in the ARG population]."}, {"heading": "3 Parameter Setting and Tuning of Algorithms", "text": "Any adjustment of the algorithm parameters to the test suite should be described and the approximate total number of tested parameter settings or algorithm variants and the approximate total budget invested should be specified. The same parameter setting must be used for all functions (which may well depend on dimensionality, see section Initialization and Input to the algorithm).This means that the a-priori use of function-dependent parameter settings (since 2012) is forbidden. Function identification or any functional features (such as separability, multimodality,...) cannot be used as input parameter4 In the individual objective case, care should be taken to apply termination conditions that enable the final goal to be achieved for the most basic functions, such as sphere function 1, i.e. for problems 0, 360, 720, 1080, 1440 and 1800 of the bbob suites."}, {"heading": "4 Recommendations", "text": "The performance evaluation is based on a series of evaluation counters linked to the value or vector of a solution. By default, each evaluation number is associated with the particular rated solution and thus its value. The solution associated with the current (last) evaluation can be changed by calling coco _ recommend _ solution, which links the value of the recommended solution (instead of the rated solution) to the current evaluation number. A recommendation is best viewed as the currently best known approximation to the optimum 9 supplied by the optimization algorithm or as the currently most desirable return value of the algorithm. Recommendations enable the algorithm to explore solutions without compromising the performance evaluation. For example, a replacement algorithm can examine (i.e. evaluate) an arbitrarily bad solution, update the replacement model and then recommend the (new) modeller. In low-noise suites, it is neither necessary nor advantageous to recommend the same solution repeatedly."}, {"heading": "5 Time Complexity Experiment", "text": "To get a rough measurement of the time complexity of the algorithm, the wall clock or CPU time should be measured when running the algorithm on the benchmark suite, and the chosen setup should reflect a \"realistic average scenario.\" 10 The time divided by the number of function evaluations must be displayed separately for each dimension, describing the setup chosen, the coding language, the compiler, and the calculation architecture for performing these experiments."}, {"heading": "Acknowledgments", "text": "The authors thank Raymond Ros, Steffen Finck, Marc Schoenauer, Petr Posik and Dejan Tusar for their many valuable contributions to this work. [9] In the multi-objective scenario, not only the last solution is taken into account, but all the solutions for this approach. In the loud scenario, a small number of the latest solutions are taken into account in future evaluations. [10] The sample experiment code provides by default the timing output measured across all problems of a single dimension. It can also be used to record the same timing experiment with \"pure random search,\" which can serve as additional baseline data. On the bbob test suite, likewise, only the first instance of Rosenbrock function 8 was used for this experiment, namely the suites indices 105, 465, 825, 1185, 1545, 1905. The authors also recognize the support provided by the grant ANR-12-MONU-0009 (NumBBO) of the French National Research Agency."}], "references": [{"title": "A restart CMA evolution strategy with increasing population size", "author": ["A. Auger", "N. Hansen"], "venue": "In Proceedings of the IEEE Congress on Evolutionary Computation", "citeRegEx": "Auger and Hansen.,? \\Q2005\\E", "shortCiteRegEx": "Auger and Hansen.", "year": 2005}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2009: Experimental Setup", "author": ["N. Hansen", "A. Auger", "S. Finck", "R. Ros"], "venue": "Inria Research Report RR-6828", "citeRegEx": "Hansen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2009}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup", "author": ["N. Hansen", "A. Auger", "S. Finck", "R. Ros"], "venue": "Inria Research Report RR-7215", "citeRegEx": "Hansen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2010}, {"title": "COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting", "author": ["N. Hansen", "A. Auger", "O. Mersmann", "T. Tusar", "D. Brockhoff"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2016}, {"title": "A parameter-less genetic algorithm", "author": ["G.R. Harik", "F.G. Lobo"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO),", "citeRegEx": "Harik and Lobo.,? \\Q1999\\E", "shortCiteRegEx": "Harik and Lobo.", "year": 1999}, {"title": "Evaluating Las Vegas algorithms: pitfalls and remedies", "author": ["H.H. Hoos", "T. St\u00fctzle"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hoos and St\u00fctzle.,? \\Q1998\\E", "shortCiteRegEx": "Hoos and St\u00fctzle.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "We present an experimental setup and procedure for benchmarking numerical optimization algorithms in a black-box scenario. This procedure can be applied with the COCO benchmarking platform. We describe initialization of and input to the algorithm and touch upon the relevance of termination and restarts. We introduce the concept of recommendations for benchmarking with COCO. Recommendations detach the solutions used in function calls from the any-time performance assessment of the algorithm.", "creator": "LaTeX with hyperref package"}}}