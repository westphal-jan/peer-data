{"id": "1603.08831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Towards Understanding Sparse Filtering: A Theoretical Perspective", "abstract": "In this paper we present our study on a recent and effective algorithm for unsupervised learning, that is, sparse filtering. The aim of this research is not to show whether or how well sparse filtering works, but to understand why and when sparse filtering does work. We provide a thorough study of this algorithm through a conceptual evaluation of feature distribution learning, a theoretical analysis of the properties of sparse filtering, and an experimental validation of our conclusions. We argue that sparse filtering works by explicitly maximizing the informativeness of the learned representation through the maximization of the proxy of sparsity, and by implicitly preserving information conveyed by the distribution of the original data through the constraint of structure preservation. In particular, we prove that sparse filtering preserves the cosine neighborhoodness of the data. We validate our statements on artificial and real data sets by applying our theoretical understanding to the explanation of the success of sparse filtering on real-world problems. Our work provides a strong theoretical framework for understanding sparse filtering, it highlights assumptions and conditions for success behind the algorithm, and it provides a fresh insight into developing new feature distribution learning algorithms.", "histories": [["v1", "Tue, 29 Mar 2016 16:23:32 GMT  (869kb,D)", "http://arxiv.org/abs/1603.08831v1", "47 pages, 10 figures"], ["v2", "Tue, 13 Dec 2016 21:34:44 GMT  (664kb,D)", "http://arxiv.org/abs/1603.08831v2", "49 pages, 9 figures"], ["v3", "Thu, 10 Aug 2017 22:57:16 GMT  (659kb,D)", "http://arxiv.org/abs/1603.08831v3", "54 pages, 9 figures"]], "COMMENTS": "47 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fabio massimo zennaro", "ke chen"], "accepted": false, "id": "1603.08831"}, "pdf": {"name": "1603.08831.pdf", "metadata": {"source": "CRF", "title": "Towards Understanding Sparse Filtering: A Theoretical Perspective", "authors": ["Fabio Massimo Zennaro", "Ke Chen"], "emails": ["zennarof@cs.manchester.ac.uk", "chen@cs.manchester.ac.uk"], "sections": [{"heading": null, "text": "Keywords: sparse filtering, attribute distribution learning, soft clustering, information conservation, cosinal metrics"}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to achieve their goals, and that they are able to achieve their goals."}, {"heading": "1.1 Sparse Filtering and Related Work", "text": "In fact, most of them are able to survive themselves by blaming themselves and others. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "1.2 Problem Statement", "text": "So far, so good. Ngiam et al. (2011) drew connections between sparse filtering, independent component analysis, and sparse encoding, while Lederer and Guadarrama provided a deeper analysis of the normalization steps inside the sparse filter algorithm. However, the reasons why the filtering does not work have generally not been investigated. In this research, we do not aim to show whether and how well the filter steps work within the sparse filter algorithm."}, {"heading": "1.3 Contributions", "text": "We summarize the contributions in this study as follows: \u2022 We present a theoretical framework that contributes to a better understanding of learning in trait distribution, provides a way to distinguish it from learning in data distribution, and explains the role of information preservation. \u2022 We provide a profound theoretical and practical understanding of how sparse filtering works by describing its dynamics and characteristics, by proving that it preserves information by preserving data structures, and by showing what kind of data structure it preserves. \u2022 We apply our understanding to concrete scenarios, providing a justification for the success of sparse filtering in the real world by providing an explanation for the failure of alternative forms of sparse filtering, and by proposing a set of conditions under which we can expect sparse filtering to be useful by its implicit limitations."}, {"heading": "1.4 Organization", "text": "The rest of this paper is structured as follows: First we review the concepts and ideas that form the basis of our work (Section 2), then we propose a more rigorous conceptualization of the idea of feature distribution learning (Section 3); this conceptual study follows the following theoretical analysis of the sparse filter algorithm (Section 4); the theoretical results obtained feed into subsequent experimental simulations (Section 5); then we discuss the results we have gathered in the light of our analysis of the sparse filter process in particular and the feature distribution learning in general (Section 6); finally, we draw conclusions by summarizing our contributions and highlighting future developments (Section 7); mathematical notation is gradually introduced by the text, but in Table 1 we provide a brief reference summarizing the conventions we have adopted."}, {"heading": "2. Foundations", "text": "In this section we will discuss the basic concepts underlying our study, provide a strict description of unsupervised learning, link it to distribution learning, formalize the property of scarcity, and finally bring all these concepts together in the definition of the sparse filtering algorithm."}, {"heading": "2.1 Unsupervised Learning", "text": "Let X = {x (i).RO} Ni = 1 in a space we refer to the transformed space. We refer to the given representation of a sample x (i) in space RO as the original representation of the sample x (i) and to RO as the original space. From an algebraic point of view we can represent the data sets as matrix X of dimensions (O); from a probable point of view we can model the data points x (i) as original examples from a multivariate variable space. X = (X1, X2,.XO) with pdf p (X).Unsupervised learning discovers a transformation f: RO \u2192 RL mapping the set X = (i).RL mapping the set X = {RO} Ni = 1 from an O-dimensional space."}, {"heading": "2.2 Distribution Learning", "text": "Distribution learning is a form of unsupervised learning that focuses on either modelling the true pdf p (X) or designing a useful pdf p (Z).Data distribution learning. Intuitively, we define data distribution learning as any unsupervised learning algorithm that deals with learning a pdf p (Z) that comes very close to the true pdf p (X).Data distribution learning algorithms aim to estimate p (X) from the available data, and the learned representation z (i) is designed to encode the factors that explain the original data x (i).Examples of data distribution learning algorithms include the denosis of auto encoders (DAE), restricted Boltzmann machines (RBM) and independent component analysis (ICA) (Ngiam et al., 2011).In the context of representative learning of parent tasks, p (the true pZ) learning is likely to be related to the actual distribution (the pdp)."}, {"heading": "2.3 Sparsity", "text": "Definition (thriftiness): In view of a generic vector v in a N-dimensional space, we say that v is sparse if a small number of components of the vector are responsible for the greater part of the energy1 of the vector (Hurley and Rickard, 2009). Practically, we say that the vector v is sparse if n components of the vector v are active (i.e., have a value that is different from zero), while the remaining N-n components are inactive (i.e., have the value zero). A vector v is k-sparse if exactly k components are active. Similarly, we can define thriftiness for matrices (with respect to their components) 1. We are referring here to the meaning of energy from signal processing, i.e.: energy (v) = 1 | vi | 2.and for random variables (with regard to their realizations). Measures of thriftiness are in the norm."}, {"heading": "2.4 Sparse Filtering", "text": "Its objective is to learn a pdf p (Z) that maximizes the frugality of learned representations. (F) The frugality of learned representations is achieved by imposing three constraints on the matrix of learned representations. (F) The frugality of learned representations is calculated as their activation: \"1 (z) = 1 (i), 1 (i), 1 (i), 1 (i), 1 (i), is required to be frugal, that is, to be described only by a few characteristics. (F) The frugality of a sample z (i) is calculated as its activation:\" 1 (z), 1 (i), 2 (i), 2 (i). \u2022 The frugality of the individual characteristics is required to be frugal."}, {"heading": "3. Conceptual Analysis of Feature Distribution Learning", "text": "We believe that the intuitive definition of learning the distribution of traits, which is examined in Section 2.2, is unsatisfactory. On the one hand, it cannot really be used to categorize algorithms that both attempt to learn the true distribution of data p (X \u0445) and to model a useful distribution of traits p (Z) (e.g. sparse RBM, Ranzato et al., 2007). On the other hand, it is not clear what implies the fact that learning the distribution of traits p (X \u0445) ignores learning the true distribution of data; \"ignoring\" may mean anything from the extreme option of \"completely ignoring the problem of learning the distribution of data\" to the more moderate option of \"not caring about optimizing learning of the distribution of data.\" A better understanding of learning of the distribution of traits is needed in order to study concrete examples of learning of the distribution of traits, and to analyze the more theoretical option of, for example, filtering ideas already discussed on the machine."}, {"heading": "3.1 Infomax Principle and Informativeness Principle", "text": "The second condition for a better concept, according to this understanding, is the objective of distributing information as a pure optimisation of the principle of information."}, {"heading": "4. Theoretical Analysis of Sparse Filtering", "text": "Based on the fact that any unsupervised algorithm (including learning algorithms for attribute distribution) must somehow comply with the Infomax principle and the computerization principle, we will use a number of conceptual tools, definitions, proofs and lemmas to demonstrate the following thesis: sparse filtering fulfills the information principle by maximizing the proxy of scarcity, and it fulfills the Infomax principle by limiting the conservation of the structure of cosmic neighborliness of the data."}, {"heading": "4.1 Informativeness Principle", "text": "Since the explicit maximization of the relative entropy D [p (Z) \u0445 q] is mathematically difficult, the sparse filter algorithm adopts the standard proxy of entropy. By increasing the entropy of the learned distribution p (Z), the entropy of the learned distribution p (Z) increases by concentrating the mass of the pdf p (Z) by zero. Thus, the minimization of the \"1 standard of the learned representations z (i) in the objective function contributes to the maximization of the informativity of the learned representation z (i)."}, {"heading": "4.2 Infomax Principle", "text": "However, due to the fact that the sparse filtering works and its learned representations z (i) enable the achievement of state-of-the-art performance in learning p (Y | Z), the algorithm must retain the information contained in the original representations x (i). If this were not the case, the sparse filtering could simply solve its optimization problem by mapping the original data matrix X to a pre-calculated sparse representation matrix Z (with a minimal computational complexity of O (1). However, these representations z (i) would be clearly useless due to the independence between representations and labels from which the vesidence p (Y | Z) follows."}, {"heading": "4.3 Non-preservation of Euclidean Distances", "text": "The preservation of absolute or relative distances under the Euclidean metric is the most common way to obtain the structure of the data. However, it can easily be excluded that sparse filtering preserves this type of structure."}, {"heading": "4.4 Preservation of Collinearity", "text": "Once we have established that sparse filtration cannot maintain the data structure defined by Euclidean metrics, we will examine other properties of the algorithm that may lead us to the preservation of alternative data structures. A first relevant observation is that sparse filtration preserves collinearity. Let us allow x (1), x (2) and RO to be collinear points in the original space RO. Then, the results of transformations from A1 to A4, i.e. fA1: A4 (x (1)), fA1: A4 (x (2)) and RL, collinear. Evidence. With regard to transformation A1, linear transformations preserve collinearity, as shown in Appendix A.1. Regarding transformation A2, the absolute value function preserves collinearity by folding all orthantiums on the first."}, {"heading": "4.5 Homo-representation of Collinear Points", "text": "A direct consequence of the preceding proposition is the following proposition, which states that all collinear points in the original representation space are assigned to an identical representation. This result is significant because it gives us an initial understanding of the principle and type of metric that uses the sparse filtering to map original samples x (i) to their representations z (i). Theorem. Let x-RO be a point in the original space RO. Then there are a series of infinite points x (i) and RO, so that fA1: A4 (x) = fA1: A4 (x (i))."}, {"heading": "4.6 Homo-representation of Points with Same Moduli", "text": "A further analysis of the sparse filtering shows that not only are collinear points assigned to the same representation, but also points in the learned representation space with the same modules (i.e., the same absolute value for their components) are assigned to identical representations. This result is also relevant, since it sheds light on the type of structure obtained by sparse filtering. Theoretically, z-RL is a point in the codomain of the function WX. It applies that there are at least 2L points z (i) for z in the first orthant, so that fA2: A4 (z) = fA2: A4 (i).Proof. By definition, z = [z (1) z (2). z (L)] is an L-dimensional vector whose components belong to the first orthant and are therefore positive. Now, the application of the absolute value fA2 (\u00b7) z is assigned to itself."}, {"heading": "4.7 Poles and Pole Pursuit", "text": "Definition (poles): In an L-dimensional space RL, a pole is a point identified by a vector p, so that! i, 1 \u2264 i \u2264 L for pi = 1 and vice versa, j, j 6 = i, 1 \u2264 j \u2264 L then pj = 0.Poles are binary 1-sparse vectors that have put a single component on a theorem. The following properties are easily derived from the definition: (i) in an L-dimensional space there is exactly L-poles; and (ii) mapping a series of original representations x (i) RO to the poles of RL gives an optimal solution to the sparse filter algorithm, since the poles have a minimum \"1 standard in RL under the constraint of sparse filtering."}, {"heading": "4.8 Representation Cones", "text": "The positions shown above and the concept of pole tracking allow us to introduce a final conceptual tool that gives us a better insight into the properties and dynamics of sparse filtering. From the theorem of the homo representation of points with the same moduli, we can deduce that there must be a symmetrical structure around lines of collinear points. Composed of these two intuitions, we can conclude that sparse filtering defines precise maps in the original representation space. More precisely, sparse filtering represents cones2 in the original representation space RO.Definition (Representation Cone) A representation cone Rp (j) is a function Rp (j): RO \u2192 R-0 mapping points in the original representation space RO."}, {"heading": "4.9 Preservation of Cosine Neighborhoodness", "text": "Through representation cones, we have shown that sparse filter cards have dots with equal angles to the same representation and dots with similar angles to the same representation. Therefore, the structure that tries to obtain sparse filtering is the structure defined by cosine metrics in the original space RO: DC (x (i), x (j) = 1 \u2212 cos \u03b8, where DC (x (i), x (j) is the cosine distance between the vectors x (i) and x (j), and \u03b8 is the angle between the vectors x (i) and x (j). Sparse filtering only approximates cosine distances: collinear or nearby points are mapped next to each other, but points far from each other are not necessarily kept separate. This is a consequence of the fact that the linear transformation in step A1 preserves the colline distances, but does not preserve the cosine metric."}, {"heading": "4.10 Non-preservation of Cosine Neighborhoodness in Alternative Implementations of Sparse Filtering", "text": "The choice of nonlinearity applied in step A2 is crucial for ensuring the preservation of cosmic neighborhood. In fact, nonlinearity of absolute value is a suitable nonlinear function for sparse filtering precisely because of its property of preserving collinearity. Ngiam et al. (2011) suggest that the original nonlinearity can be replaced by other non-linear functions; for example, that non-linear standard functions from the neural network literature, such as sigmoid nonlinearity or the reflected linear unit (ReLU), can be used. Despite this possibility, all implementations of the sparse filtering so far rely on the absolute values of nonlinearity."}, {"heading": "4.11 Preservation of Euclidean Neighborhoodness in the Limit Case", "text": "Interestingly, we can also prove that in the neighbourhood (near an infinite dimensionality (O \u2192 \u221e), cones of representation defined by sparse filtering are preserved not only cosmic neighbourliness, but probably also the relations of the Euclidean neighbourhood. Now, let us consider a point x (j) as a point in the original space RO, and let us consider Rp (k) x (i) as a point x (j), that of Rp (k) x (i) in the neighbourhood p (k).Let us now consider a point x (j) in the same representation, the height of which x (j) Rp (i), that is, a point x (j), that of Rp (k) x (i) in the neighbourhood of p (k).Let us assume that (i) points x (m) are distributed over a limited region of space."}, {"heading": "4.12 Sparse Filtering for Representation Learning", "text": "Given the above results, we can now consider sparse filtering as a soft clustering algorithm for the representation of learning.In fact, we can claim that sparse filtering implicitly makes all the assumptions made by traditional soft clustering algorithms: (i) it aims to discover less silent representations whose pdf is automatically closer to a real stochastic generation process with pdf p (X); (ii) it models the true pdf p (X) with a blending model whose components are related to the poles p (j); and (iii) it relies on cosmic metrics to evaluate relationships of neighbourliness in the original space RO. From this perspective, we can interpret the dimensionality of the learned space as the number of clusters for soft clustering, the poles as cluster centers in a space described by the cosmic metric."}, {"heading": "5. Empirical Validation", "text": "Based on the theoretical analysis presented in the previous section, we conduct a series of simulations aimed at empirically verifying our theoretical results. In order to make our results visible and easy to understand, we generally prefer simple simulations in low dimensions; experiments in higher dimensions generalize our results, but do not add anything new to our conclusions conceptually."}, {"heading": "5.1 Properties of Sparse Filtering", "text": "These simulations aim to test: (i) the property of the homo-representation of collinear points (see Section 4.8); (iii) the usefulness of representation cones (see Section 4.8); and (iv) the dynamics of pole tracking (see Section 4.8). We generate a random set of data X from three samples (N = 3) in two-dimensional space (O = 2). Each point is generated with spherical coordinates: the radial distance is determined by a uniform distribution Unif (\u2212 5, 5); the angular coordination is set on two levels."}, {"heading": "5.2 Preservation of Cosine Neighborhoodness", "text": "Next, we run further simulations on other toy datasets to validate the characteristics of data structure preservation in sparse filtering. These simulations aim to verify the following points: (i) that the sparse filtering preserves a structure defined by cosine neighbourliness; and (ii) that absolute non-linearity is crucial for the preservation of the structure and its replacement by other non-linearities (such as sigmoid or ReLU) that negates this property. We generated a random series of data X of nine samples (N = 9) in two-dimensional space (O = 2). Each point is generated with the help of spherical coordinates; the first three points have a radial distance between the individual points, which consists of a uniform distribution Unif (\u2212 2, 0) and an angular coordinate combination generated from a uniform distribution Unif (9 \u2212 \u03b7, 9 + 9 points). The second three points have a radial distance between the individual points, which consists of a composite distribution Unif (0) and a unified coordinate (3)."}, {"heading": "5.3 Sparse Filtering for Representation Learning", "text": "In the next three years, we will be able to solve the world's problems by solving them, by solving them, by solving them, by solving them, by solving them, by solving them."}, {"heading": "5.4 Sparse Filtering on Real Data Sets", "text": "In this last set of simulations, we apply our discoveries in relation to the data sets of the real world to further verify our results. These experiments aim to verify the link between the radial structure of the data and the success of austerity measures. In the first simulation, we will focus on the result in relation to the real data sets we have adopted. In the second simulation, the structure of the data (in relation to a specific set of labels) will be better explained."}, {"heading": "6. Discussion", "text": "The theoretical analysis and empirical verification we have carried out allow us to conclude that our thesis is correct: sparse filtration fulfils the principle of informatization by maximizing the proxy of disparity, and it fulfills the Infomax principle by limiting the conservation of the radial structure of the data. In particular, the sparse filtration is able to implicitly obtain the mutual information between the original representations x (i) and the learned representations z (i) by preserving the structure defined by the cosmic neighborhood. In our experiments, we showed that the sparse filtration works as an unverified soft clustering algorithm based on cosmic metrics. This enabled us to obtain the results of the sparse filtration with other standard algorithms for clustering based on the basis of the Euclidean metric (for example, soft k-means or Gaussian mixture models), providing better data saving, not better processing of the data, but rather a better processing of the data."}, {"heading": "7. Concluding Remarks", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "Appendix A. Additional Proofs", "text": "We present here the proofs of the propositions and lemmas in the main text.A.1 Preservation of collinearity under the linear transformationLemma. Consider the two collinear vectors v = [v1 v2 v3... vo] T and u = [u1 u3... uo] T in RO. Since the two vectors are collinear, they share the two collinear vectors to the origin. We can therefore circumscribe the vectors using spherical coordinates: v = [u1 u2 u3... u3... uo] T in RO. Since the two vectors are collinear, they divide the two collinear coordinates."}], "references": [{"title": "Representation learning: a review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Pattern recognition and machine", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2007\\E", "shortCiteRegEx": "Bishop.", "year": 2007}, {"title": "Sparse coding in early visual representation: from specific properties to general principles", "author": ["Neil D.B. Bruce", "Shafin Rahman", "Diana Carrier"], "venue": null, "citeRegEx": "Bruce et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bruce et al\\.", "year": 2016}, {"title": "A database of German emotional speech", "author": ["Felix Burkhardt", "Astrid Paeschke", "Miriam Rolfes", "Walter F. Sendlmeier", "Benjamin Weiss"], "venue": "In Interspeech,", "citeRegEx": "Burkhardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Burkhardt et al\\.", "year": 2005}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Emmanuel J. Candes", "Justin K. Romberg", "Terence Tao"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Normalization as a canonical neural computation", "author": ["Matteo Carandini", "David J. Heeger"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Carandini and Heeger.,? \\Q2012\\E", "shortCiteRegEx": "Carandini and Heeger.", "year": 2012}, {"title": "The cepstrum: a guide to processing", "author": ["Donald G. Childers", "David P. Skinner", "Robert C. Kemerait"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Childers et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Childers et al\\.", "year": 1977}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random structures and algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "Recklessly approximate sparse coding", "author": ["Misha Denil", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1208.0959,", "citeRegEx": "Denil and Freitas.,? \\Q2012\\E", "shortCiteRegEx": "Denil and Freitas.", "year": 2012}, {"title": "Vehicle type classification using unsupervised convolutional neural network", "author": ["Zhen Dong", "Mingtao Pei", "Yang He", "Ting Liu", "Yanmei Dong", "Yunde Jia"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Vehicle type classification using a semisupervised convolutional neural network", "author": ["Zhen Dong", "Yuwei Wu", "Mingtao Pei", "Yunde Jia"], "venue": "Intelligent Transportation Systems, IEEE Transactions on,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Sparse coding in the primate cortex", "author": ["Peter F\u00f6ldi\u00e1k", "Malcom P. Young"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "F\u00f6ldi\u00e1k and Young.,? \\Q1995\\E", "shortCiteRegEx": "F\u00f6ldi\u00e1k and Young.", "year": 1995}, {"title": "Special functions of mathematical (geo-) physics", "author": ["Willi Freeden", "Martin Gutting"], "venue": "Springer Science & Business Media,", "citeRegEx": "Freeden and Gutting.,? \\Q2013\\E", "shortCiteRegEx": "Freeden and Gutting.", "year": 2013}, {"title": "Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis", "author": ["Surya Ganguli", "Haim Sompolinsky"], "venue": "Annual review of neuroscience,", "citeRegEx": "Ganguli and Sompolinsky.,? \\Q2012\\E", "shortCiteRegEx": "Ganguli and Sompolinsky.", "year": 2012}, {"title": "Unsupervised and supervised visual codes with restricted Boltzmann machines", "author": ["Hanlin Goh", "Nicolas Thome", "Matthieu Cord", "Joo-Hwee Lim"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Goh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2012}, {"title": "Learning deep hierarchical visual feature coding", "author": ["Hanlin Goh", "Nicolas Thome", "Matthieu Cord", "Joo-Hwee Lim"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Goh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2014}, {"title": "Challenges in representation learning: a report on three machine learning contests", "author": ["Yoshua Bengio"], "venue": "In Neural information processing,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Learning quality-aware filters for no-reference image quality assessment", "author": ["Zhongyi Gu", "Lin Zhang", "Xiaoxu Liu", "Hongyu Li", "Jianwei Lu"], "venue": "In Multimedia and Expo (ICME),", "citeRegEx": "Gu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2014}, {"title": "Deep learning human actions from video via sparse filtering and locally competitive algorithms", "author": ["William Edward Hahn", "Stephanie Lewkowitz", "Daniel C. Lacombe Jr.", "Elan Barenholtz"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Hahn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2015}, {"title": "Hierarchical approach to detect common mistakes of beginner flute players", "author": ["Yoonchang Han", "Kyogu Lee"], "venue": "In ISMIR,", "citeRegEx": "Han and Lee.,? \\Q2014\\E", "shortCiteRegEx": "Han and Lee.", "year": 2014}, {"title": "Detecting fingering of overblown flute sound using sparse feature learning", "author": ["Yoonchang Han", "Kyogu Lee"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing,", "citeRegEx": "Han and Lee.,? \\Q2016\\E", "shortCiteRegEx": "Han and Lee.", "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Comparing measures of sparsity", "author": ["Niall Hurley", "Scott Rickard"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Hurley and Rickard.,? \\Q2009\\E", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Central auditory neurons have composite receptive fields", "author": ["Andrei S. Kozlov", "Timothy Q. Gentner"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kozlov and Gentner.,? \\Q2016\\E", "shortCiteRegEx": "Kozlov and Gentner.", "year": 2016}, {"title": "Compute less to get more: using ORC to improve sparse filtering", "author": ["Johannes Lederer", "Sergio Guadarrama"], "venue": "arXiv preprint arXiv:1409.4689,", "citeRegEx": "Lederer and Guadarrama.,? \\Q2014\\E", "shortCiteRegEx": "Lederer and Guadarrama.", "year": 2014}, {"title": "An intelligent fault diagnosis method using unsupervised feature learning towards mechanical big data", "author": ["Yaguo Lei", "Feng Jia", "Jing Lin", "Saibo Xing", "Steven Ding"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "An application of the principle of maximum information preservation to linear systems", "author": ["Ralph Linsker"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Linsker.,? \\Q1989\\E", "shortCiteRegEx": "Linsker.", "year": 1989}, {"title": "Information theory, inference, and learning algorithms, volume 7", "author": ["David J.C. MacKay"], "venue": null, "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Smartphone based visible iris recognition using deep sparse filtering", "author": ["Kiran B. Raja", "R. Raghavendra", "Vinay Krishna Vemuri", "Christoph Busch"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Raja et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raja et al\\.", "year": 2015}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Marc\u2019Aurelio Ranzato", "Cristopher Poultney", "Sumit Chopra", "Yann LeCun"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["Marc\u2019Aurelio Ranzato", "Y-Lan Boureau", "Yann LeCun"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "The relative advantages of sparse versus distributed encoding for associative neuronal networks in the brain. Network: computation", "author": ["Edmund T. Rolls", "Alessandro Treves"], "venue": "in neural systems,", "citeRegEx": "Rolls and Treves.,? \\Q1990\\E", "shortCiteRegEx": "Rolls and Treves.", "year": 1990}, {"title": "A deep learning approach with an ensemble-based neural network classifier for black box ICML 2013 contest", "author": ["Lukasz Romaszko"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "Romaszko.,? \\Q2013\\E", "shortCiteRegEx": "Romaszko.", "year": 2013}, {"title": "No more meta-parameter tuning in unsupervised sparse feature learning", "author": ["Adriana Romero", "Petia Radeva", "Carlo Gatta"], "venue": "arXiv preprint arXiv:1402.5766,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Temporal responses of chemically diverse sensor arrays for machine olfaction using artificial intelligence", "author": ["Shaun K. Ryman", "Neil D.B. Bruce", "Michael S. Freund"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "Ryman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ryman et al\\.", "year": 2016}, {"title": "Recognising realistic emotions and affect in speech: state of the art and lessons learnt from the first challenge", "author": ["Bj\u00f6rn Schuller", "Anton Batliner", "Stefan Steidl", "Dino Seppi"], "venue": "Speech Communication,", "citeRegEx": "Schuller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schuller et al\\.", "year": 2011}, {"title": "Edge directed single image super resolution through the learning based gradient regression estimation", "author": ["Dandan Si", "Yuanyuan Hu", "Zongliang Gan", "Ziguan Cui", "Feng Liu"], "venue": "In Image and Graphics,", "citeRegEx": "Si et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Si et al\\.", "year": 2015}, {"title": "Transformation invariance in pattern recognition - tangent distance and tangent propagation", "author": ["Patrice Y. Simard", "Yann A. LeCun", "John S. Denker", "Bernard Victorri"], "venue": "In Neural networks: tricks of the trade,", "citeRegEx": "Simard et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1998}, {"title": "Machine learning in non-stationary environments: introduction to covariate shift adaptation", "author": ["Masashi Sugiyama", "Motoaki Kawanabe"], "venue": null, "citeRegEx": "Sugiyama and Kawanabe.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama and Kawanabe.", "year": 2012}, {"title": "Accumulating pyramid spatial-spectral collaborative coding divergence for hyperspectral anomaly detection", "author": ["Hao Sun", "Huanxin Zou", "Shilin Zhou"], "venue": "In 2015 ISPRS International Conference on Computer Vision in Remote Sensing, pages 99010J\u201399010J. International Society for Optics and Photonics,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C. Pereira", "William Bialek"], "venue": "arXiv preprint physics/0004057,", "citeRegEx": "Tishby et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2000}, {"title": "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "No free lunch theorems for optimization", "author": ["David H. Wolpert", "William G. Macready"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "Acoustic feature analysis in speech emotion primitives estimation", "author": ["Dongrui Wu", "Thomas D. Parsons", "Shrikanth S. Narayanan"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Driving posture recognition by convolutional neural networks", "author": ["Chao Yan", "Frans Coenen", "Bailing Zhang"], "venue": "IET Computer Vision,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Single-layer unsupervised feature learning with l2 regularized sparse filtering", "author": ["Zhao Yang", "Lianwen Jin", "Dapeng Tao", "Shuye Zhang", "Xin Zhang"], "venue": "In Signal and Information Processing (ChinaSIP),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Performance evaluation of typical unsupervised feature learning algorithms for visual object recognition", "author": ["Shaohua Zhang", "Hua Yang", "Zhouping Yin"], "venue": "In Intelligent Control and Automation (WCICA),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "A survey of sparse representation: algorithms and applications", "author": ["Zheng Zhang", "Yong Xu", "Jian Yang", "Xuelong Li", "David Zhang"], "venue": "Access, IEEE,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Coates et al. (2011) clearly showed that very simple unsupervised learning algorithms (such as k-means clustering), when properly tuned, can generate representations of the data that allow even basic supervised classifiers, such as support vector machines, to achieve state-of-the-art performances.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013).", "startOffset": 89, "endOffset": 110}, {"referenceID": 0, "context": "From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al.", "startOffset": 276, "endOffset": 297}, {"referenceID": 0, "context": ", 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 17, "context": ", 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 15, "context": ", 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al.", "startOffset": 65, "endOffset": 96}, {"referenceID": 4, "context": ", 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006).", "startOffset": 186, "endOffset": 207}, {"referenceID": 37, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 144, "endOffset": 168}, {"referenceID": 15, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 260, "endOffset": 291}, {"referenceID": 33, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 311, "endOffset": 338}, {"referenceID": 13, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 371, "endOffset": 396}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al.", "startOffset": 90, "endOffset": 1760}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al.", "startOffset": 90, "endOffset": 1782}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al. (2006). Several different algorithms have been developed or have been adapted to learn sparse representations; for a recent survey of these algorithms we refer the reader to Zhang et al.", "startOffset": 90, "endOffset": 1808}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al. (2006). Several different algorithms have been developed or have been adapted to learn sparse representations; for a recent survey of these algorithms we refer the reader to Zhang et al. (2015).", "startOffset": 90, "endOffset": 1995}, {"referenceID": 5, "context": "More interestingly, Kozlov and Gentner (2016) used sparse filtering to model the receptive fields of high-level auditory neurons in a songbird; relying on the idea that sparseness and normalization are canonical neural processing operations (Carandini and Heeger, 2012), their results show that sparse filtering can reproduce the receptive fields of the European starling", "startOffset": 241, "endOffset": 269}, {"referenceID": 26, "context": ", 2013) and Romaszko (2013) used sparse filtering in their machine learning systems while taking part into the Kaggle Black Box Learning Challenge, achieving respectively the first and the sixth best positions.", "startOffset": 12, "endOffset": 28}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al.", "startOffset": 66, "endOffset": 96}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al.", "startOffset": 66, "endOffset": 246}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al.", "startOffset": 66, "endOffset": 365}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al. (2014) modified the original sparse filtering algorithm by introducing a penalty on the weight matrix.", "startOffset": 66, "endOffset": 469}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al. (2014) modified the original sparse filtering algorithm by introducing a penalty on the weight matrix. These studies highlight a clear interest in the refinement and improvement of the original algorithm. At the same time, on the practical side, the simplicity of the sparse filtering algorithm favored its adoption in many real-world applications: Raja et al. (2015) deployed it in a system for iris recognition on smartphones; Dong et al.", "startOffset": 66, "endOffset": 830}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al.", "startOffset": 68, "endOffset": 182}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al.", "startOffset": 68, "endOffset": 263}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al.", "startOffset": 68, "endOffset": 343}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 469}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 566}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 589}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al.", "startOffset": 68, "endOffset": 683}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al. (2016) implemented sparse filtering in their system for hyper-spectral anomaly detection; and, Ryman et al.", "startOffset": 68, "endOffset": 797}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al. (2016) implemented sparse filtering in their system for hyper-spectral anomaly detection; and, Ryman et al. (2016) integrated it in their system for modeling the olfactive temporal response in arrays of chemically diverse sensors.", "startOffset": 68, "endOffset": 905}, {"referenceID": 2, "context": "Bruce et al. (2016) analyzed different biologically-grounded principles for representation learning of images, using sparse filtering as a starting point for the definition of new learning algorithms.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Bruce et al. (2016) analyzed different biologically-grounded principles for representation learning of images, using sparse filtering as a starting point for the definition of new learning algorithms. More interestingly, Kozlov and Gentner (2016) used sparse filtering to model the receptive fields of high-level auditory neurons in a songbird; relying on the idea that sparseness and normalization are canonical neural processing operations (Carandini and Heeger, 2012), their results show that sparse filtering can reproduce the receptive fields of the European starling", "startOffset": 0, "endOffset": 247}, {"referenceID": 27, "context": "(2011) drew connections between sparse filtering, divisive normalization, independent component analysis, and sparse coding, while Lederer and Guadarrama (2014) provided a deeper analysis of the normalization steps inside the sparse filtering algorithm.", "startOffset": 131, "endOffset": 161}, {"referenceID": 24, "context": "Given a generic vector v in an N -dimensional space, we say that v is sparse if a small number of components of the vector accounts for most of the energy1 of the vector (Hurley and Rickard, 2009).", "startOffset": 170, "endOffset": 196}, {"referenceID": 24, "context": "Several measures of sparsity have been proposed in the literature; Hurley and Rickard (2009) offer a review of different measures of sparsity and their properties.", "startOffset": 67, "endOffset": 93}, {"referenceID": 16, "context": "Lifetime sparsity is often referred to as selectivity (Goh et al., 2012).", "startOffset": 54, "endOffset": 72}, {"referenceID": 12, "context": "The weight matrix W can be interpreted as a dictionary (Denil and de Freitas, 2012) or as a filter bank (Dong et al., 2015), where each row is a codeword or a filter applied to every sample in the columns of X.", "startOffset": 104, "endOffset": 123}, {"referenceID": 29, "context": "(2010), and it corresponds to the infomax principle (Linsker, 1989).", "startOffset": 52, "endOffset": 67}, {"referenceID": 30, "context": "where D [\u00b7] is a measure of distance or divergence between the pdfs, such as the KullbackLeibler divergence (MacKay, 2003).", "startOffset": 108, "endOffset": 122}, {"referenceID": 23, "context": "Similarly, RBMs approximate the maximization of the mutual information I [X;Z] through the minimization of the divergence between the distribution of the data and the learned distribution via the maximization of the log probability of the data (Hinton et al., 2006).", "startOffset": 244, "endOffset": 265}, {"referenceID": 46, "context": "These algorithms do not tackle the problem of maximizing the relative entropy D [p(Z) \u2016 q] directly, but they often address it by using constraints, such as bottleneck architectures (Tishby et al., 2000), or by imposing priors, such as adding a sparsity penalty to the learning objective (Vincent et al.", "startOffset": 182, "endOffset": 203}, {"referenceID": 47, "context": ", 2000), or by imposing priors, such as adding a sparsity penalty to the learning objective (Vincent et al., 2010).", "startOffset": 92, "endOffset": 114}, {"referenceID": 45, "context": "For instance, DAEs approximate the maximization of the mutual information I [X;Z] through the minimization of the reconstruction error; Vincent et al. (2010) showed that minimizing the reconstruction error in a DAE is indeed equivalent to maximizing a lower bound on the mutual information.", "startOffset": 136, "endOffset": 158}, {"referenceID": 9, "context": "The JonhsonLindestrauss lemma (Dasgupta and Gupta, 2003; Johnson and Lindenstrauss, 1984) sets bounds on the preservation of the distances by random projections; however, we have no clear guarantees that the randomly initialized weight matrix W would implement such a transformation or that, during training, sparse filtering would learn a transformation that preserves Euclidean distances.", "startOffset": 30, "endOffset": 89}, {"referenceID": 25, "context": "The JonhsonLindestrauss lemma (Dasgupta and Gupta, 2003; Johnson and Lindenstrauss, 1984) sets bounds on the preservation of the distances by random projections; however, we have no clear guarantees that the randomly initialized weight matrix W would implement such a transformation or that, during training, sparse filtering would learn a transformation that preserves Euclidean distances.", "startOffset": 30, "endOffset": 89}, {"referenceID": 31, "context": "This effect is particularly obvious when we interpret the application of absolute-value as a folding of the space RL onto the positive orthant (Montufar et al., 2014).", "startOffset": 143, "endOffset": 166}, {"referenceID": 50, "context": "The choice of an appropriate metric is critical for a distance-based clustering algorithm (Xing et al., 2003), and it expresses our understanding on which spatial directions encode relevant changes (Simard et al.", "startOffset": 90, "endOffset": 109}, {"referenceID": 43, "context": ", 2003), and it expresses our understanding on which spatial directions encode relevant changes (Simard et al., 1998).", "startOffset": 96, "endOffset": 117}, {"referenceID": 30, "context": "In the following set of simulations, we compare sparse filtering against another unsupervised algorithm, the soft k-means algorithm (MacKay, 2003), in order to show under which conditions sparse filtering is a good choice for processing data.", "startOffset": 132, "endOffset": 146}, {"referenceID": 3, "context": "The Berlin Emotional (EMODB) data set is a well-known audio data set in the emotion recognition community (Burkhardt et al., 2005); it contains recordings of ten German actors expressing seven different types of emotions.", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "(ii) The same set of Mel-frequency spectrum (Childers et al., 1977) coefficient (MFCC) features may", "startOffset": 44, "endOffset": 67}, {"referenceID": 49, "context": "reasonably be used both for speaker recognition and for emotion recognition; indeed, MFCC features were primarily designed for speaker recognition, but they proved to be relevant for emotion recognition as well (Wu et al., 2010; Schuller et al., 2011).", "startOffset": 211, "endOffset": 251}, {"referenceID": 41, "context": "reasonably be used both for speaker recognition and for emotion recognition; indeed, MFCC features were primarily designed for speaker recognition, but they proved to be relevant for emotion recognition as well (Wu et al., 2010; Schuller et al., 2011).", "startOffset": 211, "endOffset": 251}, {"referenceID": 1, "context": "After this analysis, we use both an Euclidean-based unsupervised learning algorithm, Gaussian mixture model (Bishop, 2007), and a cosine-based unsupervised learning algorithm, sparse filtering, to project the data into an L-dimensional space.", "startOffset": 108, "endOffset": 122}, {"referenceID": 32, "context": "(SVHN) data set (Netzer et al., 2011).", "startOffset": 16, "endOffset": 37}, {"referenceID": 43, "context": "This result agrees with the fact that the Euclidean metric is not a suitable metric for measuring distances among samples of digits represented in the pixel space; other distances less sensitive to irrelevant transformations, such as tangent distance (Simard et al., 1998), are known to be better choices.", "startOffset": 251, "endOffset": 272}, {"referenceID": 48, "context": "Consistently with the nofree lunch theorem (Wolpert and Macready, 1997), we reached the conclusion that sparse filtering is not a better algorithm than other Euclidean-based clustering algorithms, but that there is a specific set of problems (in which the data structure is explained by the cosine metric) where the performance of sparse filtering is excellent, balanced by a set of problems (in which the data structure is explained by the Euclidean metric) where its performance is less outstanding.", "startOffset": 43, "endOffset": 71}, {"referenceID": 44, "context": "some regularity in the original representation space, we hypothesize that we could use the information in the labeled samples to address the problem of covariate shift (Sugiyama and Kawanabe, 2012) in a semi-supervised learning scenario.", "startOffset": 168, "endOffset": 197}, {"referenceID": 14, "context": "19 in Freeden and Gutting (2013):", "startOffset": 6, "endOffset": 33}], "year": 2017, "abstractText": "In this paper we present our study on a recent and effective algorithm for unsupervised learning, that is, sparse filtering. The aim of this research is not to show whether or how well sparse filtering works, but to understand why and when sparse filtering does work. We provide a thorough study of this algorithm through a conceptual evaluation of feature distribution learning, a theoretical analysis of the properties of sparse filtering, and an experimental validation of our conclusions. We argue that sparse filtering works by explicitly maximizing the informativeness of the learned representation through the maximization of the proxy of sparsity, and by implicitly preserving information conveyed by the distribution of the original data through the constraint of structure preservation. In particular, we prove that sparse filtering preserves the cosine neighborhoodness of the data. We validate our statements on artificial and real data sets by applying our theoretical understanding to the explanation of the success of sparse filtering on real-world problems. Our work provides a strong theoretical framework for understanding sparse filtering, it highlights assumptions and conditions for success behind the algorithm, and it provides a fresh insight into developing new feature distribution learning algorithms.", "creator": "LaTeX with hyperref package"}}}