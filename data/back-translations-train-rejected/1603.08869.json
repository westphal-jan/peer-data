{"id": "1603.08869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Algorithms for Batch Hierarchical Reinforcement Learning", "abstract": "Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to solve large Markov Decision Processes (MDP) and provide transferable subtask policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original MDP, given a fixed dataset collected from a flat stochastic behavioral policy. We first formally prove the convergence of the algorithm for tabular MDP. Then our experiments on the Taxi domain show that HQI converges faster than a flat Q-value Iteration and enjoys easy state abstraction. Also, we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset, which enables model comparison without recollecting data.", "histories": [["v1", "Tue, 29 Mar 2016 18:17:17 GMT  (807kb,D)", "http://arxiv.org/abs/1603.08869v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tiancheng zhao", "mohammad gowayyed"], "accepted": false, "id": "1603.08869"}, "pdf": {"name": "1603.08869.pdf", "metadata": {"source": "CRF", "title": "Algorithms for Batch Hierarchical Reinforcement Learning", "authors": ["Tiancheng Zhao", "Mohammad Gowayyed"], "emails": ["gowayyed}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "The number of parameters that need to be trained grows exponentially in relation to the size of states and actions. To make the learning of reinforcement practically comprehensible, one line of research is Hierarchical Reinforcement Learning (HRL), which develops principled ways of temporal and state abstraction to reduce the dimensionality for sequential decision-making. The basic idea of temporal abstraction is to develop macro actions that take several steps to complete before they return. Usually, good macro actions aim to solve partial goals, so that multiple macro actions divide difficult tasks into several simpler ones. Furthermore, state abstraction seeks to reduce dimensionality by removing irrelevant state variables for decision-making, reducing the cardinality of state space and contributing to excessive adjustments. These two techniques result in a natural hierarchical control architecture that intuitively resembles the solution of complex tasks."}, {"heading": "2 Related Work", "text": "The three approaches are: 1) the Options Framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) the MAXQ Framework [5]. Under the Options Framework, developers extend the original options for action, which have their own predefined policies, the Dismissal State and the Active State. Sutton et al have shown that such a system is a Semi-Markov Decision Process (SMDP), leading to a unique hierarchical optimal solution using a modified Qlearning algorithm. Sutton et al have shown that such a system is a Semi-Markov Decision Process (SMDP), leading to a unique hierarchical optimal solution."}, {"heading": "3 Batch Learning for HSMDP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Definitions", "text": "Usually we follow the definitions of the MAXQ framework. However, for the simplicity of the notation we also borrow some notations from the options box."}, {"heading": "3.2 Markov Decision Process", "text": "An MDP M is described by (S, A, P, R, P0) 1. S is the state space of M2. A is a series of primitive actions that are available3. P (s \u00b2 | s, a) defines the transition probability of performing primitive action a in the state s4. R (s \u00b2 | s, a) is the reward function defined by S and A."}, {"heading": "3.3 Hierarchical Decomposition", "text": "An MDP, M, can be broken down into a finite group of sub-tasks O = {O0, O1... On} with the convention that O0 is the basic sub-task, i.e. the solution of O0 solves the entire original MDP, M. Oi is then a Semi-Markov decision process (SMDP) that shares the same S, R, P with M and has an additional tupel < \u03b2i, Ui >, where: 1. \u03b2i (s) = 1 is the termination predicate of the sub-task Oi, which divides S into a series of active states, Si and a series of terminal states Ti. When Oi enters a state in Ti, Oi and its sub-tasks leave immediately, i.e. \u03b2i (s) = 1 if s-Ti, otherwise \u03b2i (s) = 0.2. Ui is a non-empty group of actions that can be performed by Oi. The actions can be either compressed actions of positive A, or another sub-task where dej refers to this one, where Oi is a dearchical task."}, {"heading": "3.4 Hierarchical Policy", "text": "A hierarchical policy, \u03c0, is a set of strategies for each subtask, Oi, \u03c0 = {\u03c00, \u03c01... \u03c0n}. In the terminology of the option framework, a subtask policy is a deterministic option, \u03b2i (s) = 1 representing s-Ti and 0 otherwise."}, {"heading": "3.5 Recursive Optimality", "text": "A recursive optimal policy for MDP M with hierarchical decomposition is a hierarchical policy \u03c0 = {\u03c00... \u03c0n}, so that for each subtask, Oi, the corresponding policy \u03c0i is optimal for the SMDP, which is defined by the group of states, Si, the set of measures Ui, the probability of state transition P\u03c0 (s \u2032, N | s, a) and the reward function R (s \u2032 | s, a)."}, {"heading": "4 Algorithm", "text": "The problem formulation is as follows: Given a finite set of samples, D = {(sm, am, rm, s \u00b2 m = 1, 2,..., M} and each valid hierarchical decomposition O \u2032 s of the original MDP M, we would like to learn the recursive optimal hierarchical distribution. The basic idea is to train each subtask using Subtask Q-value Iteration (SQI) in a bottom-up mode. SQI's training requirement for a specific subtask Oi is that all children Ui have converged to their poisonous optimal politics. To fulfill this condition, HQI first sorts the DAG and runs SQI from tasks whose children have only primitive measures."}, {"heading": "4.1 Extension to Function Approximation and State Abstraction", "text": "We note that it is trivial to expand SQI to Fitted SQI, which uses a function approximator to model the Q-value function for a subtask at the end of each iteration. The direct benefit of using Function Approximation is that it can integrate powerful monitored regression methods such as Gaussian processes or Neural Networks to scale to large and continuous MDPs. Although the use of Function Approximations usually compromises the theoretical convergence guarantee for tabular MDP, our experiments show that Fitted HQI is capable of converting to a unique optimal solution. FittedSQI is summarized in Algorithm 3. Moreover, state abstraction here means finding a subset of state variables that are most informative for each subtask. A good hierarchical decomposition splits the original MDP into more simple groups, each one of which just needs to take care of a small feature."}, {"heading": "5 Proof of Convergence", "text": "In this section, we prove that our HQI (in the tabular case) approaches the recursive, optimal Qi policy. Suppose that the policy is arranged in such a way that it breaks the bindings deterministically (e.g. from left to right) for each subtask, then it defines a unique, recursive, optimal hierarchical policy, \u03c0 r, and a corresponding hierarchical Q-value iteration (HQI) that is required: O, D-value iteration with only primitive children Done (A), while Train 6 = empty for Oi-value iteration (Oi, D) value iteration (Oi, D) value iteration (Oi, D-value iteration (SQI), Qi-value iteration (SQi), Qi-value iteration (SQi), Qi-Qi (SQi)."}, {"heading": "5.1 Proof", "text": "We want to prove that with an MDP M = (S, A, P, R, P0, \u03b3) with hierarchical decomposition O = {O0,.. On} HQI converges to a recursive optimal policy for the hierarchical policy of M, \u03c0 \u0445 r.1 We prove this first: In a subtask Oi, where all their children converge to their recursive optimal strategies and infinitely many position data, algorithm SQI converges after infinitely many iterations to the optimal Q-value function, Limk \u2212 > \u221e Qki = Q \u0445 i2. We then show that HQI specifies a sequence in which all subtasks are trained in the DAG diagram, so that when training a subtask, Oi, all its children are already converging to their optimal recursive strategies."}, {"heading": "5.2 Proof Sketch", "text": "Step 1 We start with the base case for subtask, whose children are all primitive measures. We can note that Equation (3) reverts to the traditional Bellman flat MDP operator, because a primitive action always ends after a step: Q (i, s, u) = r (s, a) + \u03b3 s \u2032 P (s, a) max u. \"Ui Q (i, s, u\") (6) Therefore, for subtask with only primitive children, SQI is synonymous with flat Q value iteration, which is guaranteed to be converted to an optimal policy that is sufficient. Then, for subtask with other subtask children, by definition, when we run SQI, the children have converted to their unique deterministic optimal recursive policy, which means that each action u. \"Ui, is a deterministic Markov option defined in the option box."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Setup", "text": "We applied our algorithm to the taxi domain described in [13]. This is a simple grid world that contains a taxi, a passenger, and four specially designated places labeled R, G, B, and Y. In the initial state, the taxi is in a randomly selected cell of the grid, and the passenger is in one of the four special places. The passenger has a desired destination that he wants to reach, and the task of the taxi is to go to the passenger, pick him up, go to the passenger's destination, and drop the passenger off. The taxi has six primitive actions at its disposal: one step in one of the four directions (North, South, East, and West) to pick up and drop the passenger off. To make the task more difficult, the removal actions are not deterministic, so it has a 20% chance of moving in one of the other directions."}, {"heading": "6.2 Results", "text": "This year is the highest in the history of the country."}, {"heading": "6.3 Comparison with MAXQ-Q and Intra-Option Learning", "text": "Compared to MAXQ-Q, HQI has example efficiency and the ability to operate outside of the guidelines. An advantage of the external guidelines is that they do not require hyperparameter optimization, such as the exploration rate. As high-level subtasks in MAXQ-Q first have to wait for their children to converge, developers typically set a faster exploration decay rate for subtasks, which is an additional hyperparameter to adjust. [4] The limitation of HQI is that it maintains an independent Q table for each subtask, while MAXQ-Q allows a portion of the parent value function to be retrieved recursively by their children, a technique known as value function decomposition. [4] This allows for more compact memory usage and accelerates parent learning. How to share value functions in the outside environment is a future research topic. For intra-option learning in the developer options framework, the main component of QM does not require QM to be fully pre-defined."}, {"heading": "7 Conclusion and Future Work", "text": "We have shown that it is possible to collect data blindly using a random flat policy. We then use this data to learn different structures that were not aware of the data collection. Our experiments in the field of taxis show that they approach the optimal policy faster than FQI. It also shows that different DAG structures are able to learn from these flat data at different speeds. Each DAG structure has its own number of parameters, which points to a possible line for research to try to minimize the number of parameters in the hierarchy. Further future work will include comparing different characteristic selection techniques for fitted SQI and applying the algorithm to large and complex areas."}], "references": [{"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Reducing commitment to tasks with off-policy hierarchical reinforcement learning", "author": ["Mitchell Keith Bloch"], "venue": "arXiv preprint arXiv:1104.5059,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Topological sort. Introduction to Algorithms (2nd ed.)", "author": ["Thomas H Cormen", "Charles E Leiserson", "Ronald L Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "An overview of maxq hierarchical reinforcement learning. In Abstraction, Reformulation, and Approximation, pages 26\u201344", "author": ["Thomas G Dietterich"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Batch-ifdd for representation expansion in large mdps", "author": ["Alborz Geramifard", "Thomas J Walsh", "Nicholas Roy", "Jonathan How"], "venue": "arXiv preprint arXiv:1309.6831,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Least-squares policy iteration", "author": ["Michail G Lagoudakis", "Ronald Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Greedy algorithms for sparse reinforcement learning", "author": ["Christopher Painter-Wakefield", "Ronald Parr"], "venue": "arXiv preprint arXiv:1206.6485,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Sparse reinforcement learning via convex optimization", "author": ["Zhiwei Qin", "Weichang Li", "Firdaus Janoos"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}], "referenceMentions": [{"referenceID": 7, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "There are three major approaches developed relatively independently [1], aiming to formalize the idea of abstraction into reinforcement learning.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "Using HAMQ learning [10], HAM can also converge to a hierarchical optimal solution.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "best knowledge, there is little prior work [2] in developing batch learning algorithms that allow a hierarchical SMDP to be trained from an existing data set collected from a stochastic behavior policy.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "One challenge of training a subtask with subtask children is that we cannot use the optimal SMDP Bellman equation described in the MAXQ framework [5], Qi(s, u), which is the Q-value function for subtask, Oi, at state, s and action u:", "startOffset": 146, "endOffset": 149}, {"referenceID": 12, "context": "Therefore, instead of using the above Bellman equation that updates the Q table of the parent when a child exits, we use the intra-option Bellman equation proposed in the option framework [13]:", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "This means that every action u \u2208 Ui, is a deterministic deterministic Markov option as defined in the option framework [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "[13] proved that \u201dfor any set of deterministic Markov options one step intra-option Q-learning converges w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Refer to the [13], for the detailed proof.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "ordering [3].", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "We applied our algorithm to the Taxi domain described in [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Also, as pointed out in [4], state abstraction is essential for MAXQ to have fast learning speed compared to flat Q learning.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "However, different from the aggressive state abstraction described in [4], where every subtask and child pair has a different set of state variables, we only conduct a simple state abstraction at subtask level, i.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "This is different from the behavior of the on-policy MAXQ-Q algorithm reported in [4], which needs state abstraction in order to learn faster than Qlearning.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "The limitation of HQI is that it maintains an independent Q table for each subtask, while MAXQ-Q allows a part of the parent value function recursively retrieved from its children, a technique known as value function decomposition [4].", "startOffset": 231, "endOffset": 234}, {"referenceID": 1, "context": "[2] provides a method of training options in an off-policy fashion.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to solve large Markov Decision Processes (MDP) and provide transferable subtask policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original MDP, given a fixed dataset collected from a flat stochastic behavioral policy. We first formally prove the convergence of the algorithm for tabular MDP. Then our experiments on the Taxi domain show that HQI converges faster than a flat Q-value Iteration and enjoys easy state abstraction. Also, we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset, which enables model comparison without recollecting data.", "creator": "LaTeX with hyperref package"}}}