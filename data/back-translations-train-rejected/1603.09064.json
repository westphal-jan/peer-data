{"id": "1603.09064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Semi-Supervised Learning on Graphs through Reach and Distance Diffusion", "abstract": "Semi-supervised learning algorithms are an indispensable tool when labeled examples are scarce and there are many unlabeled examples [Blum and Chawla 2001, Zhu et. al. 2003]. With graph-based methods, entities (examples) correspond to nodes in a graph and edges correspond to related entities. The graph structure is used to infer implicit pairwise affinity values (kernel) which are used to compute the learned labels.", "histories": [["v1", "Wed, 30 Mar 2016 07:51:58 GMT  (251kb,D)", "http://arxiv.org/abs/1603.09064v1", "13 pages, 6 figures"], ["v2", "Mon, 23 May 2016 18:21:01 GMT  (420kb,D)", "http://arxiv.org/abs/1603.09064v2", "21 pages, 5 figures"], ["v3", "Sat, 13 Aug 2016 05:57:56 GMT  (417kb,D)", "http://arxiv.org/abs/1603.09064v3", "21 pages, 5 figures"], ["v4", "Mon, 26 Sep 2016 07:08:42 GMT  (418kb,D)", "http://arxiv.org/abs/1603.09064v4", "21 pages, 5 figures"], ["v5", "Fri, 20 Jan 2017 18:34:52 GMT  (436kb,D)", "http://arxiv.org/abs/1603.09064v5", "13 pages, 5 figures"]], "COMMENTS": "13 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["edith cohen"], "accepted": false, "id": "1603.09064"}, "pdf": {"name": "1603.09064.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning for Asymmetric Graphs through Reach and Distance Diffusion", "authors": ["Edith Cohen"], "emails": ["edith@cohenwang.com"], "sections": [{"heading": null, "text": "Two effective techniques for defining such a nucleus are \"symmetrical\" spectral methods and Personalized Page Rank (PPR). Spectral methods can be used to scale labels using Jacobi iterations, but an inherent limiting problem is that they are applicable to symmetrical (undirected) graphs, while relationships between entities are often inherently asymmetrical, e.g. with likes, sequences, or hyperlinks. PPR works with directed graphs, of course, but even state-of-the-art techniques do not scale when we want to learn billions of labels. To achieve both high scalability and the handling of directed relationships, we propose reach diffusion and distance diffusion cores here. Our design is inspired, formalized, and derived from the groundbreaking work of [Kempe, Kleinberg, and Tardos 2003]."}, {"heading": "1. INTRODUCTION", "text": "Semi-supervised learning [7, 55, 10] is a basic machine learning tool aimed at applications where there are few labeled (seed) examples (xj, yj) j \u2264 n 'and many unlabeled examples xi for i (n', n '+ nu]. Semi-supervised learning algorithms use an auxiliary structure, such as a metric embedding or an interaction graph in the space of examples, from which paired affinity values are implied. The goal is to learn soft labels fi for the unlabeled examples that are as consistent as possible with seed labels yj and affinities - so that a learned label of an example more closely resembles the seed examples that are more closely related to it. The use of appropriate affinities is critical to the quality of the labels fi learned, but we shift this discussion briefly and consider the labels of the learning offered in relation to affinities."}, {"heading": "1.1 Strong and weak affinities", "text": "This is because the examples described represent only a fraction of all examples and the relationships of most dots to the seed dots are weak. However, the raw data typically contains only precise strong relationships between entities: friendships in a social network, word contexts, product purchases, user movie views or features in images or documents. Interactions can be determined by frequency, reciprocity, confidence or importance. Interactions included generally correspond to strong relationships. A common approach to improving these data is to embed the entities in a low-dimensional Euclidean space, so that larger internal products or denser interactions between entities are not appropriate."}, {"heading": "1.2 Capturing weak relations", "text": "Just think of a graph where entities are nodes and weighted edges that correspond to strong affinities; the all-encompassing relationships that we aspire to can be considered dependent on the totality of paths from i to j. Affinities should therefore satisfy some intuitive desirable characteristics, while the effects of the edges, for shorter paths, and if there are more independent paths between entities; in addition, we often want to establish connections through high degrees, and be able to track the effects of each property. We now offer a brief overview of existing methods that show an all-encompassing range of capabilities from such a graph; the most popular methods for label learning are spectral; in perhaps the most familiar form, the learned labels are expressed as a solution to an optimization problem that has a smooth concepts that encourages learned concepts."}, {"heading": "1.3 Contributions", "text": "We propose a novel approach to semi-supervised learning based on what we call diffusion and distance diffusion. [31] This approach is extensively applied. [28] Our cores share the intuitive desirable properties of spectral and PPR cores, but they have the advantage of survival for asymmetric relationships, such as PPR, and highly scalable algorithms, such as symmetric spectral cores. We provide statistical guarantees of the approximate labels \"estimated quality in terms of the precise structures defined in the model. We are also conducting a preliminary experimental study that will mature the application and potential of our proposed models.1.3.1 Reach diffusion by popular information models. [20, 28] and by reliability or survival analysis [37]."}, {"heading": "1.4 Overview", "text": "In Section 2, we will present in detail our range and distance diffusion cores and their application to label learning; in Section 3, we will show how we use Monte Carlo simulations and sketches to approximate the labels we learned; and we will analyze the worst statistical guarantees for approximate quality that we can obtain; in Section 4, we will present algorithms for calculating approximate labels; and the parameter settings methodology will be discussed in Section 5. Section 6 contains some preliminary experiments."}, {"heading": "2. MODEL", "text": "Our input is specified as graph G = (V, E), where the nodes V are entities and edges E (undirected or directed) correspond to interactions between entities. We associate weights with edges E, which reflect the strength of interaction and the inverse cost of connection by the header unit. We can also associate weights wv with a node v, which reflects the inverse cost of connection by the entity. A common method of modelling the cost of connection by an entity is as a non-decreasing function of its degree (number of interactions) - connections by higher-grade nodes are less meaningful and therefore costly. The strength of an interaction can reflect its frequency or frequency."}, {"heading": "2.1 Reach diffusion", "text": "We build a probabilistic model from this input by associating lifetime random variables with edges and nodes. A natural choice is to use for each component x, a Weibull, or an exponentially distributed random variable. In each Monte Carlo simulation of the model, we get a series of lifetimes tx for the components of the diagram (edges and nodes). Note: The expected lifespan is E [tx] = wx, meaning stronger interactions have longer lifetimes. In each Monte Carlo simulation of the model, we get a series of lifetimes tx for the components of the diagram (edges and nodes). Note: The expected lifespan is E [tx] = wx, meaning stronger interactions are the edges and nodes {x]. For a pair of nodes, we get tx for the components of the diagram (edges and nodes)."}, {"heading": "2.2 Distance diffusion", "text": "In each Monte Carlo simulation of the model, we obtain a fixed set of length estimates \"x\" for the components of the graph. The distance dij is the length of the shortest path dij in relation to the lengths. \"For the sake of simplicity, we overload the notation we have used for range diffusion: For \u03c4 \u2265 0 and the node i, we denote the number of nodes j at the distance shortest of i. For the nodes i, j, we denounce the number (or mass) of nodes h with dih \u2264 dij. Distance diffusion is defined as labeling labels, not as density (Nij)."}, {"heading": "3. APPROXIMATE LABELS", "text": "In this section, we begin with the question of highly scalable errors that affect the likelihood of errors most. We present our approach to calculating the labels by estimating expectations (4) by taking the mean of the simulations of the model (4), that is, the statistical guarantees we give for the mean of T independently of (exact) random variables f. \"Before addressing the calculation of f\" i, we consider the loss of quality due to the use of simulations against the exact expectation (4). That is, the statistical guarantees we give for the mean of T independently of (exact) random variables f \"i as an estimate of fi.LEMMA 3.1 with T = \u2212 2, the average estimate of each component of fi has an absolute margin of error that focuses well on the probability of errors."}, {"heading": "3.1 Sketches", "text": "The knots for the second place.The knots are in relation to the uniform random mutation of knots.The knots for the first place.The knots for the second place.The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knot.U The knots"}, {"heading": "3.2 Estimation Error Analysis", "text": "The estimated quality of f \u00b2 i (6) as an estimate of f \u00b2 i (5) is influenced by two sources of error: The first is the quality of the sample-based inverse probability estimate (6) as an estimate from (3) i = [4] j [5]; the second is the quality of the sample-based inverse probability estimate (6) as an estimate from (3); and the MinHash theory and distance sketches are as follows: THEOREM 3.1. For a sketch parameter k: \u2022 The expected size of the samples is limited by E [S (i)."}, {"heading": "4. ALGORITHMS", "text": "These sketches were originally designed to be used in the shortest possible ways, and there are several algorithms and large-scale implementations that can be used from the field. Most of these approaches can be easily adapted to estimates if mi [0, 1} (see discussion in [13]) and there is a variation that is suitable for general m. The component of sample and probability acquisition is more subtle, but uses the same calculation (see [13, 14]).For reaching the threshold, we do not work with the distances, but with the survival thresholds tij."}, {"heading": "5. PARAMETER SETTING", "text": "Our models have several unspecified hyperparameters: for range diffusion, the selection of lifetime random variables of the components and possible dependencies between them; for distance diffusion, the selection of random length variables. Another parameter is the decay function \u03b1, which is applied to the position order of a node. Our use of sketches allows easy implementation of the leaveone-out cross validation. For each member of the seed quantity U, we can calculate a learned identification fm with respect to U\\ {m} using the sample S (m) (without omitting itself). Subsequently, we can consider a setting that minimizes the cost function \u0445U | | fm \u2212 ym | 2. Note that the selection of a decay function \u03b1 can use the same sets of simulations and sketches. However, each evaluation for different component lengths / lifetime functions requires a new set of simulations and sketches."}, {"heading": "6. EXPERIMENTS", "text": "We conducted some preliminary experiments with the Movielens 1M [39] and political blogs [3] datasets. Our goal is twofold: first, to evaluate the quality of learned labels in a semi-supervised learning context; second, to demonstrate a use case for our models and to select length or lifetime variables. Our evaluation is not intended to evaluate scalability, as there are several highly scalable implementations of the basic methods for sketching distance and accessibility that we use as our main component [42, 8, 16, 23, 17, 9]. We implemented the algorithms in Python and conducted the experiments on a Macbook Air."}, {"heading": "6.1 Movielens 1M data", "text": "The data consists of approximately 1 million reviews of 6,040 users of 3,952 movies. Each movie is a member of one or more of the 18 genres: Action, Adventure, Animation, Children's, Comedy, Crime, Documentary, Drama, Fantasy, Film Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War, WesternAbout 3.7K movies both have listed genres and ratings. This is our type of examples. We represent the \"true\" label of a movie with genre as an L = 18 dimensional vector weighing 1 / c on each listed genre and weight 0 otherwise. Note that the provided labels and also our learned labels have the form of probability vectors across genres. We use the notation (m) for the group of users who rate movies, m and are rated by us (u) the series of movies that u.We build a graph with a node for each movie and user."}, {"heading": "6.2 Political blogs data", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "7. EXTENSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Large label dimension", "text": "Our presentation focused on a small label dimension L, so that the softly learned labels can have a dense representation. If L is very large, we can use the variety of composable heavy-hitter sketches (sample-based, linear, deterministic) to represent and manipulate the labels. Broadly speaking, these sketches have dimension O (\u2212 1), regardless of the label dimension, and allow us to at least identify all entries in the (L1 standardized) soft label. By spectral learning, this sketching approach was first used by [49], which used count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35]. Sketches of the label vectors can be seamlessly linked to our density estimates here to get a corresponding linear sketch of the learned labels."}, {"heading": "7.2 Assigning confidence to seed labels", "text": "Our use of leave-one-out cross-validation provides a cost-effective method of calculating a learned label (based on other seeds) for seed nodes as well. Another application is adapting the hyperparameters of our framework. If the seed labels themselves are not of high quality, it is to assign them a confidence level of \"mass\" based on the correspondence between the seed and the learned label for the seed."}, {"heading": "8. CONCLUSION", "text": "We proposed a new approach to graph-based semi-supervised learning through range diffusion and distance diffusion cores. Our models are based on well-studied models of influence diffusion and address applications with asymmetric relationships that require a highly scalable calculation of the learned labels. We designed scalable algorithms and determined the accuracy of the calculation and statistical guarantees from our approximate values. We performed a promising preliminary experimental evaluation."}, {"heading": "Acknowledgment", "text": "We would like to thank Fernando Pereira for discussions, references to literature and the exchange of views and intuitions on the challenges of the real world that led to the development of our proposed models."}, {"heading": "9. REFERENCES", "text": "[1] B. D. Abrahao, F. Chierichetti, R. Kleinberg, andA. Panconesi. Trace complexity of network inference. In KDD, 2013. [2] L. A. Adamic and E. Adar. How to search a social network. Social Networks, 27, 2005. [3] L. A. Adamic and N. Glance. The political blogosphere and the 2004 u.s. election: Divided they blog. In LinkKDD. ACM, 2005. [4] S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. Video suggestion and discovery for youtube: Taking random walks through the view graph. In WWW, 2008. [5] A. Bijral, N. Ratliff, and N. Srebro."}], "references": [{"title": "Trace complexity of network inference", "author": ["B.D. Abrahao", "F. Chierichetti", "R. Kleinberg", "A. Panconesi"], "venue": "In KDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "How to search a social network", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The political blogosphere and the 2004 u.s. election: Divided they blog", "author": ["L.A. Adamic", "N. Glance"], "venue": "In LinkKDD. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Video suggestion and discovery for youtube: Taking random walks through the view graph", "author": ["S. Baluja", "R. Seth", "D. Sivakumar", "Y. Jing", "J. Yagnik", "S. Kumar", "D. Ravichandran", "M. Aly"], "venue": "In WWW,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Semi-supervised learning with density based distances", "author": ["A. Bijral", "N. Ratliff", "N. Srebro"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The formation of networks with transfers among players", "author": ["F. Bloch", "M.O. Jackson"], "venue": "Journal of Economic Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "HyperANF: Approximating the neighbourhood function of very large graphs on a budget", "author": ["P. Boldi", "M. Rosa", "S. Vigna"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reverse ranking by graph structure: Model and scalable algorithms", "author": ["E. Buchnik", "E. Cohen"], "venue": "Technical Report cs.SI/1506.02386,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "All-distances sketches, revisited: HIP estimators for massive graphs analysis", "author": ["E. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Multi-objective weighted sampling", "author": ["E. Cohen"], "venue": "In HotWeb. IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Scalable similarity estimation in social networks: Closeness, node labels, and random edge lengths", "author": ["E. Cohen", "D. Delling", "F. Fuchs", "A. Goldberg", "M. Goldszmidt", "R. Werneck"], "venue": "In COSN. ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sketch-based influence maximization and computation: Scaling up with guarantees", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "In CIKM. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Distance-based influence in networks: Computation and maximization", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "Technical Report cs.SI/1410.6976,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Spatially-decaying aggregation over a network: Model and algorithms", "author": ["E. Cohen", "H. Kaplan"], "venue": "J. Comput. System Sci.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Efficient non-parametric function induction in semi-supervised learning", "author": ["O. Delalleau", "Y. Bengio", "N. Le Roux"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Mining the network value of customers", "author": ["P. Domingos", "M. Richardson"], "venue": "In KDD. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Scalable influence estimation in continuous-time diffusion networks", "author": ["N. Du", "L. Song", "M. Gomez-Rodriguez", "H. Zha"], "venue": "In NIPS. Curran Associates, Inc.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Efficient label propagation", "author": ["Y. Fujiwara", "G. Irie"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Scalable facility location for massive graphs on pregel-like systems", "author": ["K. Garimella", "G. De Francisci Morales", "A. Gionis", "M. Sozio"], "venue": "In CIKM. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["M. Gomez-Rodriguez", "D. Balduzzi", "B. Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Social and economic networks", "author": ["M.O. Jackson"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Scaling personalized web search", "author": ["G. Jeh", "J. Widom"], "venue": "In WWW. ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J.M. Kleinberg", "\u00c9. Tardos"], "venue": "In KDD. ACM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J. Laffery"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Statistical models and methods for lifetime data, volume 362", "author": ["J.F. Lawless"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "The multirank bootstrap algorithm: Self-supervised political blog classification and ranking using semi-supervised link classification", "author": ["F. Lin", "W.W. Cohen"], "venue": "In ICWSM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S.-F. Chang"], "venue": "In ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Fast-PPR: Scaling personalized pagerank estimation for large graphs", "author": ["P.A. Lofgren", "S. Banerjee", "A. Goel", "C. Seshadhri"], "venue": "In KDD. ACM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Approximate frequency counts over data streams", "author": ["G. Manku", "R. Motwani"], "venue": "In International Conference on Very Large Databases (VLDB),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Survival analysis", "author": ["R.G. Miller"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1975}, {"title": "Finding repeated elements", "author": ["J. Misra", "D. Gries"], "venue": "Technical report, Cornell University,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1982}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory Prob. Applic.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1964}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical report, Stanford InfoLab,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1999}, {"title": "ANF: A fast and scalable tool for data mining in massive graphs", "author": ["C.R. Palmer", "P.B. Gibbons", "C. Faloutsos"], "venue": "In KDD,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "On the estimation of a probability density function and the mode", "author": ["E. Parzen"], "venue": "Annals of Math. Stats.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1962}, {"title": "Large-scale semi-supervised learning using streaming approximation", "author": ["S. Ravi", "Q. Diao"], "venue": "In AISTATS,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1956}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "Saul L. K"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Estimating and computing density based distance metrics", "author": ["Sajama", "A. Orlitsky"], "venue": "In ICML,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Density estimation for statistics and data analysis", "author": ["B.W. Silverman"], "venue": "Monographs on statistics and applied probability", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1986}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["P.P. Talukdar", "W.W. Cohen"], "venue": "In AISTATS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "In ECML PKDD,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "author": ["P.P. Talukdar", "F.C.N. Pereira"], "venue": "In ACL,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2000}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya A,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1964}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2002}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Laffery"], "venue": "In ICML,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 53, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 9, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 51, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 63, "endOffset": 71}, {"referenceID": 38, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 63, "endOffset": 71}, {"referenceID": 43, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 41, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 46, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 46, "context": "In particular, the expression (2) is a solution of the optimization (1) also when \u03ba is asymmetric [48].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36].", "startOffset": 208, "endOffset": 216}, {"referenceID": 35, "context": "A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36].", "startOffset": 208, "endOffset": 216}, {"referenceID": 50, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 24, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 52, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 53, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 44, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 50, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 53, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 45, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 4, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 52, "context": "One such objective was proposed in an influential label propagation work by Zhu et al [54, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 53, "context": "One such objective was proposed in an influential label propagation work by Zhu et al [54, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 3, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 48, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 49, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 10, "context": "Other interpretations of the solution are as a fixed point of a stochastic sharing process or the landing probability of a random walk [11, 29].", "startOffset": 135, "endOffset": 143}, {"referenceID": 28, "context": "Other interpretations of the solution are as a fixed point of a stochastic sharing process or the landing probability of a random walk [11, 29].", "startOffset": 135, "endOffset": 143}, {"referenceID": 21, "context": "The computation of each gradient update is linear in the number of edges, but typically, hundreds of iterations are needed even with various optimizations [22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "A further optimization sparsifies the set of unlabeled points using a smaller set of anchors that is large enough to preserve the short-range structure but is much smaller than the full set [19, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 32, "context": "A further optimization sparsifies the set of unlabeled points using a smaller set of anchors that is large enough to preserve the short-range structure but is much smaller than the full set [19, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 39, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 26, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 33, "context": "To apply it to our context, however, we need to estimate visiting probabilities from all unlabeled i to sufficiently many seeds, and scalability is a game stopper even with state of the art techniques [34].", "startOffset": 201, "endOffset": 205}, {"referenceID": 33, "context": "This means that any algorithm from basic Monte Carlo generation of walks to the bidirectional approach of [34] would spend most of its \u201cwork\u201d traversing non-seed nodes.", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": "Another more scalable use of PPR is performed on the \u201ctransposed\u201d graph [32]: All seeds with the same label are grouped and PPR is then personalized to each label, and applied to obtain similarities from labels to nodes.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 78, "endOffset": 86}, {"referenceID": 27, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 78, "endOffset": 86}, {"referenceID": 36, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 30, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 19, "context": "Influence diffusion, as motivated by Richardson and Domingos [20] and formalized by Kempe, Kleinberg, and Tardos [28], is defined for a network of directed pairwise interactions between entities.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "Influence diffusion, as motivated by Richardson and Domingos [20] and formalized by Kempe, Kleinberg, and Tardos [28], is defined for a network of directed pairwise interactions between entities.", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "Independent Cascade (IC) [28], which uses independent activation probabilities pe to edges, is the simplest and most studied model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "Note that we can express the IC model of [28] in terms of this reliability formulation by choosing independent lifetime variables that are exponential random variables with parameter 1/pe \u03bce \u223c Exp[1/pe].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 20, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 16, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 27, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 5, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 25, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 0, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 20, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 14, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 11, "context": "Our scalable approximation relies on a sketching technique of reachability sets and of neighborhoods of directed graphs [12, 13].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Our scalable approximation relies on a sketching technique of reachability sets and of neighborhoods of directed graphs [12, 13].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "To do so, we design a threshold sketching algorithm which builds on the basic distance-sketching design [12, 13] but replaces the shortest-path searches by \u201csurvival threshold\u201d graph searches as a basic component.", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "To do so, we design a threshold sketching algorithm which builds on the basic distance-sketching design [12, 13] but replaces the shortest-path searches by \u201csurvival threshold\u201d graph searches as a basic component.", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "We present our approach for computing the labels approximately, using Monte Carlo simulations and a novel use of sample-based sketches [12, 13] to approximate the results of each simulation.", "startOffset": 135, "endOffset": 143}, {"referenceID": 12, "context": "We present our approach for computing the labels approximately, using Monte Carlo simulations and a novel use of sample-based sketches [12, 13] to approximate the results of each simulation.", "startOffset": 135, "endOffset": 143}, {"referenceID": 0, "context": "This is an immediate consequence of Hoeffding\u2019s inequality, noting that entries of our label vectors are in [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 11, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 12, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 13, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 12, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 13, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 12, "context": "See discussion in [13] for the handling of general m.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 40, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 8, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 12, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 12, "context": "Most of these approaches can be easily adapted to estimate m(R\u03c4 (i)), when mi \u2208 {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m.", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "Most of these approaches can be easily adapted to estimate m(R\u03c4 (i)), when mi \u2208 {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).", "startOffset": 107, "endOffset": 115}, {"referenceID": 13, "context": "The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).", "startOffset": 107, "endOffset": 115}, {"referenceID": 11, "context": "The sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13].", "startOffset": 171, "endOffset": 179}, {"referenceID": 12, "context": "The sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13].", "startOffset": 171, "endOffset": 179}, {"referenceID": 8, "context": "The algorithm has a parallel version designed to run on multi-core architectures [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "The pseudocode provided as Algorithm 2 builds on a state of the art design for computing universal monotone multi-objective samples [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "We performed some preliminary experiments using the Movielens 1M [39] and political blogs [3] datasets.", "startOffset": 90, "endOffset": 93}, {"referenceID": 40, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 7, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 15, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 22, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 16, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 8, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 0, "context": "length distribution Exp[1] regardless of |\u0393(v)|.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "With g(x) = 1/ log(x) we obtain the Adamic-Adar similarity [2] popular in social network analysis.", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "We then take seed sets S of size s \u2208 [10, 1000] as prefixes of the same permutation.", "startOffset": 37, "endOffset": 47}, {"referenceID": 47, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 75, "endOffset": 79}, {"referenceID": 42, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 181, "endOffset": 189}, {"referenceID": 34, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 181, "endOffset": 189}], "year": 2017, "abstractText": "Semi-supervised learning algorithms are an indispensable tool when labeled examples are scarce and there are many unlabeled examples [Blum and Chawla 2001, Zhu et. al. 2003]. With graph-based methods, entities (examples) correspond to nodes in a graph and edges correspond to related entities. The graph structure is used to infer implicit pairwise affinity values (kernel) which are used to compute the learned labels. Two powerful techniques to define such a kernel are \u201csymmetric\u201d spectral methods and Personalized Page Rank (PPR). With spectral methods, labels can be scalably learned using Jacobi iterations, but an inherent limiting issue is that they are applicable to symmetric (undirected) graphs, whereas often, such as with like, follow, or hyperlinks, relations between entities are inherently asymmetric. PPR naturally works with directed graphs but even with state of the art techniques does not scale when we want to learn billions of labels. Aiming at both high scalability and handling of directed relations, we propose here Reach Diffusion and Distance Diffusion kernels. Our design is inspired by models for influence diffusion in social networks, formalized and spawned from the seminal work of [Kempe, Kleinberg, and Tardos 2003]. These models apply with directed interactions and are naturally suited for asymmetry. We tailor these models to define a natural asymmetric \u201ckernel\u201d and design highly scalable algorithms for parameter setting and label learning.", "creator": "LaTeX with hyperref package"}}}