{"id": "1603.09233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs", "abstract": "We describe and study a model for an Automated Online Recommendation System (AORS) in which a user's preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multi-armed bandit problem are also presented.", "histories": [["v1", "Wed, 30 Mar 2016 14:58:32 GMT  (204kb)", "http://arxiv.org/abs/1603.09233v1", "8 pages, submitted to conference"]], "COMMENTS": "8 pages, submitted to conference", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rahul meshram", "aditya gopalan", "d manjunath"], "accepted": false, "id": "1603.09233"}, "pdf": {"name": "1603.09233.pdf", "metadata": {"source": "CRF", "title": "Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs", "authors": ["Rahul Meshram", "Aditya Gopalan"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own."}, {"heading": "A. Related Work", "text": "Multi-armed bandit models for recommendation systems and for online advertising have been modeled as contextual bandits, e.g. [1] - [3] and the interests of users are considered independent of the referral history, i.e. they have static reward models. There are several models for restless, multi-armed bandits that use state transitions and state rewards with applications in the dynamic spectrum, e.g. [4] - [6]. Such models assume (1) perfect observation of the state when the arm is sampled, and (2) state transitions that are independent of actions. There is some work in modeling alternate rewards for multi-armed bandits, e.g. [7] but it is again in the fully observable state case, thereby bypassing the critical problem of state uncertainty that arises in user adaptation. Other approaches to user responses to recommendations have taken algorithms into account [8] bandit models, which are a current basis for past user actions [8] where the bandits [8] are considered as the first-armed bandit recommendations [8]."}, {"heading": "II. MODEL DESCRIPTION AND PRELIMINARIES", "text": "The AOR system is expected to be a restless, multi-armed bandit with arm i =, which describes the state of the user in relation to point i. Each arm is modeled as an independent, partially observable Markov decision process, with two states and two actions. We first describe the model for a single generic arm. S = {0, 1} is the set of states with 0 corresponding to a low interest and 1 corresponding to a high interest. A = {0, 1} is the set of actions with 1 for scanning the arm and 0 for not trampling the arm. R: S \u00b7 A \u2192 R is the reward function with0 (1 \u2212 q) q1Reward: \u03bb state conversions and rewards if 0. Reward: 1 w.p. Reward: 1 w.p. 1state conversions and rewards if At = 1. Figure 1."}, {"heading": "III. SINGLE ARM: OPTIMAL POLICY", "text": "We solve the average reward problem described in the previous paragraph by the disappearing discount rate [\u03b2], [\u03b2 \u03b2], [\u03b2 \u03b2] by first considering a discounted reward system and then taking limits as the discount rate disappears. The infinite horizon discounts the reward according to measure and discount rates \u03b2, 0 < \u03b2 < \u03b2 < 1, isV \u03c6\u03b2 (\u03c0), (\u03c0) and discount rates \u03b21. (1) From [10] we can show that the following dynamic program finds solutions (1). (V\u03b2) = maximum constant rates according to measure and discount rates (1 \u2212 q) + (1 \u2212 constants according to measure and discount rates). (1) (10) [10] We can show that the following program (1) solves the following dynamic (1) and (1)."}, {"heading": "A. An Equivalent Form for the Optimal Policy", "text": "For the one-armed bandit, the threshold policy of Lemma 2 can be interpreted as follows: \u03b2\u03b2\u03b2\u03b2 \u03b2 q = \u03b2 q = \u03b2 q = \u03b2 \u03b2 q (= 1) = = 1 (= 1) = 0 (\u2212 \u2212 2), if \u03c0t > \u03c0T. We know that if At = 1, then \u03c0t + 1 = 1, i.e. if the item is recommended, the state becomes 0. If the item is not recommended, the belief in the state 0 is reduced by a factor of (1 \u2212 q). Since there is a single threshold and the reward decreases monotonously each time the item is not recommended, the optimal policy must be waited before recommending it again if k is the first time that the state 0 \u2212 k \u2212 \u2212 kp has exceeded. This value k is a function of q and is called kpt (q,). First, we consider the infinite horizon as a discounted reward problem. In this case, the solution (1) is equal to the optimization problem."}, {"heading": "IV. THOMPSON SAMPLING LEARNING ALGORITHM", "text": "We have seen that the optimal policy for the (single) POMDP system is unknown (Section III-A), and this corresponds to the expectation that there will be a maximum cumulative reward. (This section describes an online algorithm that learns to play the optimal policy based on experience, i.e., observations from previously played actions, while at the same time keeping the reward as high as possible (the exploit problem). The learning algorithms are a version of the popular Thompson strategy [15], originally developed for stochastic multiarmed bandit problems [16], and then extended to learning in Markov Decision Proces1as in an AOR system where user behavior is unknown. (MPs)"}, {"heading": "V. MAIN RESULT \u2013 REGRET BOUND", "text": "In this section we show an analytical performance guarantee for algorithm 1. To achieve this, we consider a commonly used measure of performance from online learning theory, namely remorse [20], [21]. The regret of a strategy for which POMDP is described is the difference between the cumulative reward, which is the optimal reward for a strategy A, the random variable RA (q), (Q), (T), (T), which is executed from a fixed initial state for a fixed time horizon T, and what the strategy earns with the same initial state and time horizon. Formally, the regret for a strategy A is the random variable RA (q), (T), (T): = 1), (Q), (T), (Q), (T), (T), (Q), (Q), (A), (A), (A), (Q), (Q), (Q), (Q), (Q), (Q), (A), (Q), (T), (A), (A), (A), (), (), (), () ()."}, {"heading": "VI. NUMERICAL RESULTS AND DISCUSSION", "text": "We present some numerical results to represent the convergence rate of the algorithm 1 with the optimal policy of the POMDP model for different configurations of the true model and the original model. We fix \u03bb = 0.3 in all simulations and consider four combinations of the true models (q *, \u03c1 *); (1) (0.05.0.25), (0.05.0.15), (3) (0.05.0.35) and (4) (0.15.0.35). For each of these four values of the true parameters, we introduce two measures of power as a function of the time step - regret and also the probability mass to the true values. These values are averages from 300 runs of the simulation. We obtain the first series of diagrams (shown in Fig. 2) by roughly dividing the parameter space into (5 \u00d7 5) raster, i.e. the convergence of the model to one (0.05, 0.15, 0.35, 0.45) and starting with the uniform distribution of the parameters (5 \u00d7 5) to the true row of the 5, i.e. to support the convergence of the model to one (0.05, 0.35, 0.45)."}, {"heading": "A. Discussion and Directions \u2013 Multi-armed bandit case", "text": "We formulated the problem of optimizing recommendations for an individual user whose taste changes in a particular item with recommendations as online learning in a two-step POMDP. With this approach, we developed and analyzed the performance of a natural Thompson sampling algorithm to learn the optimal policy for a user-item pair. A logical next step in this investigation is to treat the multi-arm bandit version of the problem with several independently evolving POMDPs, each of which represents different users / items and a resource limitation that allows users / items to be activated at any time, e.g. deciding which of several items to show to a user at any given time, since the user remembers how far it has consumed a particular item and can respond accordingly to item recommendations. The Thompson sampling-based algorithm proposed in this paper could be expanded to cover the multi-arm bandit case by sharing the optimal parameter for each POP and MP."}, {"heading": "A. Proof of Theorem 3", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "B. Preliminary Results Towards Proving Theorem 4", "text": "We collect some useful assertions here to show the result. First, we point out that the variation k = > q q q q q = q q q = q q q, and it becomes asD (f (q), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), k (k), k k (k), k k (k), k k (k), k k (k), k k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), k (k), p, p (p, p, p, p, p (p), p (p), p (p), p (p), p (), p (p), p (p), p (p), p (p), p (k), k (k), k (k), k (k), k (k), k (k, k (k), k (k (k), k (k), k (k (k), k (k), k (k (k), k (k (k), k (k, k (k), k (k, k (k), k (k, k, k (k, k), k (k (k), k (k (k), k (k, k, k (k), k (k, k (k, k), k (k (k), k, k (k (k), k (k, k), k (k (k), k (k), k (k (k), k (k (k (k"}, {"heading": "C. Proof of Theorem 4", "text": "From lesson 5 we know that for sufficiently small \u2206 > 0: 1 = kmax \u2212 1 = kk: dk (q, \u03c1) = kmax \u2212 1, we have for sufficiently small \u0445 > 0: 2 = 1ln = kmax \u2212 1 such that we can determine that all assumptions in [22, Proposition 2] are met. Note that the upper limit of C (logL) is given in [22, Proposition 2] and it is as follows. C (logL) \u2264 (kmax \u2212 2) 2 (1 + 2) 1 \u2212 logLHere we replace the upper limit of C (logL)."}], "references": [{"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems. NIPS, Dec. 2007, pp. 1\u20138.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "Arxiv, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "Conference on World Wide Web. ACM, April 2010, pp. 661\u2013670.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Indexability of restless bandit problems and optimality of Whittle index for dynamic multichannel access", "author": ["K. Liu", "Q. Zhao"], "venue": "IEEE Transactions Information Theory, vol. 56, no. 11, pp. 5557\u20135567, November 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploiting channel memory for joint estimation and scheduling in downlink networks", "author": ["W. Ouyang", "S. Murugesan", "A. Eyrilmaz", "N. Shroff"], "venue": "Proceedings of IEEE INFOCOM, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Network utility maximization over partially observable Markovian channels", "author": ["C. Li", "M.J. Neely"], "venue": "Performance Evaluation, vol. 70, no. 7\u20138, pp. 528\u2013548, July 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimality of myopic policy for a class of monotone affine restless multi-armed bandits", "author": ["P. Mansourifard", "T. Javidi", "B. Krishnamachari"], "venue": "Proceedings of IEEE CDC, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-aware music recommendation based on latent topic sequential patterns", "author": ["N. Hariri", "B. Mobasher", "R. Burke"], "venue": "Proceedings of the Sixth ACM Conference on Recommender ystems (RecSys \u201912), 2012, pp. 131\u2013138.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A restless bandit with no observable states for recommendation systems and communication link scheduling", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Proceedings of IEEE CDC, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Whittle index for restless multi-armed hidden markov bandits", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Submitted for Publication. Also available on Arxiv:1603.04739, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Applied Probability Models with Optimization Applications", "author": ["S.M. Ross"], "venue": "Dover Publications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Whittle index policy for crawling ephemeral content", "author": ["K. Avrachenkov", "V.S. Borkar"], "venue": "Tech. Rep. Research Report No. 8702, INRIA, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive playlists from hidden markov bandits", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Submitted for Publication. Also available arxiv, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Principles of mathematical analysis, McGraw-Hill Book Co., New York, third edition, 1976, International Series in Pure and Applied Mathematics", "author": ["Walter Rudin"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1976}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": "Biometrika, vol. 24, no. 3\u20134, pp. 285\u2013294, 1933.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1933}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Ian Osband", "Benjamin V. Roy"], "venue": "Advances in Neural Information Processing Systems 27, 2014, pp. 1466\u20131474.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["Aditya Gopalan", "Shie Mannor"], "venue": "Conf. on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, 2015, pp. 861\u2013898.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "Proc. NIPS, 2011, pp. 2312\u20132320.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "JMLR, vol. 11, pp. 1563\u20131600, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Thompson Sampling for Complex Online Problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay Mansour"], "venue": "Proc. International Conf. on Machine Learning, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": ", [1]\u2013[3] and the user interests are assumed to be independent of the recommendation history, i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": ", [1]\u2013[3] and the user interests are assumed to be independent of the recommendation history, i.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": ", [4]\u2013[6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [4]\u2013[6].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": ", [7] but it is again in the fully observable state case, thus circumventing the critical problem of state uncertainty arising in user adaptation.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [8]; but these are primarily numerical studies.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "In [10] it was shown that the such a system is approximately Whittle-indexable.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The restless bandit that we propose in this paper is a special case of that from [10] for which we obtain much stronger results and also a Thompson sampling-based algorithm to learn the parameters of the arms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]\u2014by first considering a discounted reward system and then taking limits as the discount vanishes.", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]\u2014by first considering a discounted reward system and then taking limits as the discount vanishes.", "startOffset": 138, "endOffset": 142}, {"referenceID": 9, "context": "From [10], we can show that the following dynamic program solves (1).", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "\u2223 < (1 \u2212 \u03c1) for all \u03c0 \u2208 [0, 1] and \u03b2 \u2208 [0, 1).", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "The first two above follow directly from [10] and the last two are derived in [13].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "The first two above follow directly from [10] and the last two are derived in [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Define V \u03b2 := V\u03b2(\u03c0) \u2212 V\u03b2(1) for \u03c0 \u2208 [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "Hence we can apply the Arzela-Ascoli theorem [14], to find a subsequence (V \u03b2k(\u03c0), (1 \u2212 \u03b2)V\u03b2k(\u03c0)) that converges uniformly to (V (\u03c0), g) as \u03b2k \u2192 1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Thus, as \u03b2k \u2192 1, along an appropriate subsequence, (3) reduces to V (\u03c0) + g = max {\u03bb+ V ((1\u2212 q)\u03c0), 1 \u2212 \u03c0(1 \u2212 \u03c1)} , (4) for all \u03c0 \u2208 [0, 1].", "startOffset": 131, "endOffset": 137}, {"referenceID": 10, "context": "17 in [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "If there exists a bounded function V (\u03c0) for \u03c0 \u2208 [0, 1] and a constant g that satisfies (4), then there exists a stationary policy \u03c6 such that", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "for all \u03c0 \u2208 [0, 1], and moreover, \u03c6 is the policy for which the RHS of (4) is maximized.", "startOffset": 12, "endOffset": 18}, {"referenceID": 14, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 303, "endOffset": 307}, {"referenceID": 17, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 308, "endOffset": 312}, {"referenceID": 0, "context": "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]\u00d7 [0, 1] containing all possible POMDPs parameterized by (q, \u03c1).", "startOffset": 178, "endOffset": 184}, {"referenceID": 0, "context": "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]\u00d7 [0, 1] containing all possible POMDPs parameterized by (q, \u03c1).", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Algorithm 1: Thompson sampling algorithm for learning the optimal policy Input: Parameter space X \u2286 [0, 1], Policy space K \u2282 {0, 1, 2, .", "startOffset": 100, "endOffset": 106}, {"referenceID": 18, "context": "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].", "startOffset": 115, "endOffset": 119}], "year": 2016, "abstractText": "We describe and study a model for an Automated Online Recommendation System (AORS) in which a user\u2019s preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multiarmed bandit problem are also presented.", "creator": "LaTeX with hyperref package"}}}