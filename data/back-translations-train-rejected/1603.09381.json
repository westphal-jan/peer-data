{"id": "1603.09381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Clinical Information Extraction via Convolutional Neural Network", "abstract": "We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.", "histories": [["v1", "Wed, 30 Mar 2016 20:57:07 GMT  (95kb)", "http://arxiv.org/abs/1603.09381v1", "arXiv admin note: text overlap witharXiv:1408.5882by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1408.5882by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["peng li", "heng huang"], "accepted": false, "id": "1603.09381"}, "pdf": {"name": "1603.09381.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jerryli1981@gmail.com", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.09 381v 1 [cs.L G] 30 Mar 201 6"}, {"heading": "1 Introduction", "text": "In recent years, we have been very interested in the use of neural networks to explore all kinds of natural language processing."}, {"heading": "2 Constructing High Quality Training Dataset", "text": "When implementing our neural network-based system for extracting clinical information, we found that because of the confusing format of clinical notes, it is not easy to generate high-quality training data, and choosing the right tokenizer is very important for span identification. After several experiments, we found that \"RegexpTokenizer\" can meet our needs; this tokenizer can generate chips for each token using sophisticated regular expressions such as below, n l t t t t t t o k e n i e.g. RegexpT oken ize r2http: / / www.nltk.org (\"\\ w + | $[\\ d\\.] + |\\ S +\"). We then use \"PerceptronTagger\" as our part of the language tag due to its fast tagging speed. Please note that when extracting context words, you use the same tokenization module instead of just splitting strings."}, {"heading": "3 Neural Network Classifier", "text": "The identification of the event span is the task of extracting expression character offsets in clinical notes. This subtask is very important because the accuracy of the identification of the event span affects the accuracy of the identification of attributes. First, we start our neural network classifier to identify event spans, and then our system tries to identify attribute values based on each span."}, {"heading": "3.1 Temporal Convolutional Neural Network", "text": "The question is how such a situation could have come about. (...) The only question is how such a situation could have come about. (...) The question is whether such a situation could have come about at all. (...) The question is whether such a situation could have come about at all. (...) The question is whether such a situation could have come about at all. (...) The question is whether such a situation could have come about at all. (...) The only question is how such a situation could have come about. \"(...) The only question is whether such a situation could have come about. (...)"}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We use Clinical TempEval corpus 3 as an evaluation dataset. This corpus is based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually decrypted by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but the temporal expression was not changed, and the notes were then commented manually with times, events, and temporal relationships in clinical notes. These notes include time expression types, event characteristics, and an increased focus on temporal relationships. Event, time, and temporal relationship annotations were distributed separately from the text in Anafora format. Table 2 shows the number of documents, event expressions in the training, development, and test sections of the 2016 THYME data."}, {"heading": "4.2 Evaluation Metrics", "text": "All tasks were evaluated using the standard metrics of precision (P), callback (R), and formula 1: 3http: / / alt.qcri.org / semeval2016 / task12 / index.php? id = dataP = | S-H | S-H | | H-1 = 2 \u00b7 P-RP + R (7), where S is the set of items predicted by the system, and H is the set of items manually noted by humans. Applying these items of tasks only requires a definition of what is considered \"items\" for each task. To evaluate the expression spans of events, items were tuples of character compensations. Therefore, the system only received recognition for identifying events with exactly the same character compensations as the manually notated items. To evaluate the attributes of event expression types, items were tuples of beginning, end, beginning, event, end, and event were assigned to the attributes, where the event was assigned a value, a string of time, and a string of events."}, {"heading": "4.3 Hyperparameters and Training Details", "text": "Objective FunctionWe want to maximize the probability of the correct class, which corresponds to minimizing the negative log probability (NLL). Specifically, given the input xh, the label y is predicted by a Softmax classifier that takes the hidden state hj as input: p. (y.) = softmax. (W.) y. (W.) = argmax yp. (y.) (8) After that, the objective function is the negative log probability of the true class names yk: J (c.) = \u2212 1mm.k = 1log. (y.)."}, {"heading": "4.4 Results and Discussions", "text": "Table 3 shows results for the event expression tasks. Our initial submissions RUN 4 and 5 exceeded the memorization limit for each metric for each task. The accuracy of calculating the event span is close to the maximum report. However, our system has been recalled less. One of the main reasons for this is that our training target function is focused on accuracy. Table 4 shows results for the phase 2 subtask."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced a new system for extracting clinical information that uses only deep neural networks to identify event spans and their characteristics from clinical notes. We trained classifiers for deep neural networks to extract clinical event spans. Our method combined each word4https: / / github.com / Lasagne / Lasagna 5http: / / nlp.stanford.edu / projects / glove / with their part-of-speech tag and form information as additional features. We then set temporal convolution neural networks to learn hidden feature representations. All the experimental results show that our approach consistently exceeds the existing basic methods for standard evaluation data. Our research has shown that we can achieve competitive results without the help of a domain-specific tool for extracting traits, such as cTAKES. In addition, we use only basic modules for processing natural language learning, such as tag representation, and deeper understanding of the cost of extraction."}], "references": [{"title": "Semeval-2015 task 6: Clinical tempeval", "author": ["Leon Derczynski", "Guergana Savova", "James Pustejovsky", "Marc Verhagen"], "venue": "In International Workshop on Semantic Evaluation", "citeRegEx": "Bethard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Blulab: Temporal information extraction for the 2015 clinical tempeval challenge", "author": ["Danielle L Mowery", "Samir Abdelrahman", "Lee Christensen", "Wendy W Chapman"], "venue": "In International Workshop on Semantic", "citeRegEx": "Velupillai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Velupillai et al\\.", "year": 2015}, {"title": "Phoneme recognition using time-delay", "author": ["Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang"], "venue": null, "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Yin", "Schutze2015] Wenpeng Yin", "Hinrich Schutze"], "venue": "In Proceedings of th 53rd Annual Meeting of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 114, "endOffset": 160}, {"referenceID": 5, "context": "From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 114, "endOffset": 160}, {"referenceID": 8, "context": ", 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al.", "startOffset": 99, "endOffset": 171}, {"referenceID": 3, "context": ", 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015).", "startOffset": 102, "endOffset": 167}, {"referenceID": 9, "context": "For example, BluLab system (Velupillai et al., 2015) extracted morphological(lemma), lexical(token), and syntactic(part-of-speech) features encoded from cTAKES.", "startOffset": 27, "endOffset": 52}, {"referenceID": 1, "context": "To address this challenge, we propose a deep neural networks based method, especially convolution neural network (Collobert et al., 2011), to learn hidden feature representations directly from raw clinical notes.", "startOffset": 113, "endOffset": 137}, {"referenceID": 1, "context": "The way we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by (Collobert et al., 2011).", "startOffset": 135, "endOffset": 159}, {"referenceID": 10, "context": "One was introduced by (Waibel et al., 1989) and also known as Time Delay Neural Networks (TDNNs).", "startOffset": 22, "endOffset": 43}, {"referenceID": 1, "context": "The other one was introduced by (Collobert et al., 2011).", "startOffset": 32, "endOffset": 56}, {"referenceID": 1, "context": "In (Collobert et al., 2011) architecture, a sequence of length n is represented as:", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute (Bethard et al., 2015).", "startOffset": 172, "endOffset": 194}, {"referenceID": 2, "context": "Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011).", "startOffset": 109, "endOffset": 129}], "year": 2016, "abstractText": "We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their partof-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.", "creator": "LaTeX with hyperref package"}}}