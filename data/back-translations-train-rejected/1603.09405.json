{"id": "1603.09405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "abstract": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.", "histories": [["v1", "Wed, 30 Mar 2016 22:39:59 GMT  (1224kb,D)", "http://arxiv.org/abs/1603.09405v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["peng li", "heng huang"], "accepted": false, "id": "1603.09405"}, "pdf": {"name": "1603.09405.pdf", "metadata": {"source": "CRF", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "authors": ["Peng Li", "Heng Huang"], "emails": ["jerryli1981@gmail.com", "heng@uta.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who fight for the rights of women and men are fighting for equality between men and women."}, {"heading": "2 Character-level Convolutional Neural Network", "text": "To achieve this, we use a deep Convolutionary Neural Network (ConvNets), which accepts a sequence of encoded characters as input, encoding by prescribing an alphabet of size m for the input language, and then quantifying each character with a uniform encoding, and then transforming the string into a sequence of such m-sized vectors of fixed length l0. Any character that exceeds the length l0 is ignored, and all characters that are not in the alphabet are quantified as all-zero vectors. The alphabet used in our model consists of 36 characters, including 26 English letters and 10 digits. Below, we will introduce character convolution of neural networks at character level."}, {"heading": "2.1 Temporal Convolution", "text": "The one-dimensional convolution is an operation between a vector of the weights m \u00b2 Rm and a vector of the inputs, which is considered as sequence x \u00b2 Rn. The vector m is the filter of the convolution. Specifically, we think of x as the input token and xi \u00b2 R as a single characteristic value associated with the i-th character in this token. The idea behind the one-dimensional convolution is to take the point product of the vector m with every m-gram in the token x, to add another sequence c: cj = m Txj \u2212 m + 1: j. (1) Usually xi is not a single value, but a ddimensional vector, so that x \u00b2 Rd \u00b7 n. There are two types of 1d convolution operations. One is called a time delay of the Neural Networks (TDNNs)."}, {"heading": "2.2 Highway MLP", "text": "In addition to the Convolutionary Neural Network Layers, we are building another Highway Multilayer Perceptron (HMLP) layer to further improve word embedding at the character level. Conventional MLP applies an affine transformation followed by a nonlinearity to obtain a new set of characteristics: z = g (Wy + b). (5) A layer of a highway network does the following: z = t g (WHy + bH) + (1 \u2212 t) y, (6) where g is a nonlinearity, t = \u03c3 (WTy + bT) is called a transformation gate, and (1 \u2212 t) is called a carry gate. Similar to memory cells in LSTM networks, highway layers allow the direct carrying of some dimensions of input to the input to train deep networks."}, {"heading": "3 Multi-Layer Bidirectional LSTM", "text": "Now that we have two types of word sequence representations: One type of sequence representation is the composition of pre-trained word vectors; the other type of sequence representation is word vectors generated from a character-level revolutionary network. We can inject the two sequence representations into bi-directional LSTM to learn sentence representation. Specifically, forward-directed LSTM accept pre-trained word embed outputs and backward-facing LSTM accept CNN embed outputs. The final sentence representation is the concatenation of the two directions."}, {"heading": "3.1 RNN vs LSTM", "text": "Recursive neural networks (RNNs) are able to model sequences of different lengths to a hidden state using the recursive application of a transition function. For example, at each step t an RNN takes the input vector xt-Rn and the hidden state vector ht-1-Rm, then applies an affine transformation, followed by an elementary nonlinearity such as the hyperbolic tangential function, to generate the next hidden state vector ht: ht = tanh (Wxt + Uht \u2212 1 + b). (7) An important problem for RNNs when using these transition functions is that it is difficult to learn long-range dependencies during training because the components of the gradient vector can grow or decay exponentially (Bengio et al., 1994). The LSTM architecture (Hochreiter and Schmidhuber, 1998) addresses the problem of learning dependencies on benvectors as long lines can be dixted \u2212 STX \u2212 growing."}, {"heading": "3.2 Model Description", "text": "One shortcoming of conventional RNNs is that they are only able to use the previous context. In the text area, the decision is made after the entire sentence pair has been digested. Therefore, it would be better to explore the future context to show the meaning of the sequence. Bidirectional RNNs architecture (Graves et al., 2013) suggested a solution to make predictions based on future words. At any time, the model maintains two hidden states, one for left-to-right propagation \u2212 ht and the other for right-to-left propagation. \u2212 ht The hidden state of bidirectional LSTM is the concatenation of forward and backward concealed states. The following equations illustrate the main ideas: \u2212 ht = tanh (\u2212 Wxt + \u2212 \u2192 U \u2212 h t \u2212 1 + \u2212 b) we can hide all levels of the NN plane (h \u2212)."}, {"heading": "4 Learning from Matching Features", "text": "Inspired by (Tai et al., 2015), we apply elemental fusion to the first sentence matrix M1, Rn x 2d and the second sentence matrix M2, Rn x 2d. Similar to the previous method, we can define two simple matching character levels (FPs) with the following equations: FP1 = M1 M2, FP2 = | M1 \u2212 M2 |, (11), where the elemental multiplication is located. FP1 measurement can be interpreted as an elementary comparison of the signs of the input representations. FP2 measurement can be interpreted as a distance between the input representations. In addition to the above measures, we also found the following feature level that can improve performance: FP3 = 1dConv (Reshape (connection (M1, M2))), (12) In FP3, 1dConv means one-dimensional confusion. The intuition behind FP3 is that the usual set pairs between set pairs are preserved."}, {"heading": "4.1 Reshape Feature Planes", "text": "Remember that the multilayer bidirectional LSTM creates a sentence representation matrix M-Rn-2d by linking the hidden sentence matrix \u2212 \u2192 M-Rn-d and the hidden sentence matrix vice versa. Then, we perform an elementary merge to form a feature layer Mfp-Rn-2d. Therefore, the final input into the temporal fold layer is a 3D tensor I-Rf-n-2d, where f is the number of matching feature layers, n the number of layers, d the memory dimensionality of the LSTM. Note that the 3D tensor fold layer input I can be viewed as an image where each feature layer is a channel. To facilitate temporal folding, spatial 2D folding is often used over an input image consisting of several input layers. In the experiment section, we will compare 2D folding with 1D folding."}, {"heading": "4.2 CNN Topology", "text": "The mechanism of temporal CNN is the same here as the temporal CNN. However, the cores are completely different. It is very important to design a good topology for CNN to learn hidden properties from heterogeneous feature layers. After several experiments, we found two topological diagrams that can be used in architecture.Figure 2 and Figure 3 show the two CNN diagrams. In Topology I we stack the temporal folding with the core width as 1 and the Tanh activation at each feature level. Then we insert another temporal folding and Tanh activation with the core width as 2. In Topology II, however, we first stack the temporal folding and Tanh activation with the core width as 2. Then we insert another temporal folding and Tanh activation with the core width as 2."}, {"heading": "5 Experiments", "text": "We selected two related tasks for modeling sentence relationships: semantic relationship task, which measures the degree of semantic kinship of a sentence pair by assigning a kinship value from 1 (fully independent) to 5 (very related); and textual connection task, which determines whether the truth of a text contains the truth of another text called hypothesis. We use the standard SICK (Sentences Involving Compositional Knowledge) 1 dataset for evaluation, which consists of approximately 10,000 English sentence pairs commented on for their kinship in meaning and dissociation."}, {"heading": "5.1 Hyperparameters and Training Details", "text": "First, we initialize our word representations with publicly available 300-dimensional glove word vectors 2. The LSTM memory dimension is 100, the number of layers is 2. On the other hand, in the CharCNN model, in addition to each time fold and max. pooling pairs, we use the activation function for thresholds. The size of the CharCNN input frame corresponds to the alphabet size, the size of the output frame is 100. The maximum sentence length is 37. The core width of each time fold is set to 3, the step is 1, the hidden units of HighwayMLP are 50. The training is done by stochastic decrease of the gradient over mixed minibatches using the AdaGrad updating rule (Duchi et al., 2011). The learning rate is 0.05. The minibatch size is 25. The model parameters have been fixed with a control strength of 10 \u2212 4. Note that word embedding was fixed during processing.toindeindeindevalse.seqas.org / http: / nqas.org"}, {"heading": "5.2 Objective Functions", "text": "The task of semantic kinship prediction attempts to measure the degree of semantic kinship of a sentence pair = j \u00b7 J = max = fty function (by assigning a kinship value in the range of 1 (completely independent) to 5 (very related). Formally, we would like to predict a real similarity value in the range of [1, K] in which K > 1 is an integer. Consequence 1, 2,..., K is the ordinal scale of similarity in which higher values indicate greater degrees of similarity. We can predict the similarity value y by predicting the probability that the learned hidden representation xh belongs to the ordinal scale. This is done by projecting an input representation onto a series of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input c is in the corresponding scale. Mathematically, the similarity value y value of the category can be categorized."}, {"heading": "5.3 Results and Discussions", "text": "Table 1 and 2 show the Pearson correlation and accuracy comparison of semantic relationships and text tasks. We can see that combining CharCNN with multi-layered bi-directional LSTM performs better than other traditional methods of machine learning such as SVM and MaxEnt approach (Proisl and Evert, 2014; Lai and Hockenmaier, 2014), which are equipped with many craft characteristics. Note that our method does not require additional handmade trait extraction methods. Also, our method does not use external linguistic resources such as word network or analysis, which produce the best results in (Tai et al., 2015). More importantly, both prediction results are close to state-of-the-art results. It has proven that our approaches can successfully predict heterogeneous tasks at the same time. Note that for semantic coherence tasks the latest research (Tai et al., 2015) can suggest tree structure prediction results as a result."}, {"heading": "5.4 Tree LSTM vs Sequence LSTM", "text": "In this experiment, we will compare tree LSTM with sequential LSTM. One limitation of the sequence LSTM architectures is that they only allow strictly sequential information propagation, but tree LSTM's allow richer network topologies in which each LSTM unit is able to integrate information from multiple child units. As in standard LSTM units, each tree LSTM unit (indexed by j) depends on input and output gates ij and oj, one memory cell cj and hidden state hj. The difference between the standard LSTM unit and tree LSTM units is that gating vectors and memory cell updates may depend on the states of many child units. Additionally, the tree LSTM unit contains a forgotten gate fjk for each child, allowing the tree LSTM unit to selectively integrate information from each child."}, {"heading": "6 Related Work", "text": "Existing neural set models fall mainly into two groups: Convolutionary Neural Networks (CNNs) and Recursive Neural Networks (RNNs). In regular 1D CNNs (Collobert et al., 2011; Kalchburner and Blunsom, 2013; Kim, 2014), a rigid window shifts over time (consecutive words) to extract local features of a set; then they bundle these features into a vector, usually taking the maximum value in each dimension for supervised learning. However, when combined with maximum pooling, the revolutionary unit can act as a compositional operator with a local selection mechanism, as in the recursive autoencoder (Socher et al, 2011b)."}, {"heading": "7 Conclusions", "text": "Our new approach first generates two types of word sequence representations as input into bidirectional LSTM to learn sentence rendering, then we construct matching functions, followed by another timeframe CNN to learn highly hidden matching feature renderings. Our model shows that combining pre-trained word embedding with auxiliary character level embedding can improve sentence rendering, and the improved sentence rendering generated by multi-layered bi-directional LSTM will include information on character and word levels, as well as enhancing matching features generated by calculating similarity scales on sentence pairs. Experimental results from benchmark datasets show that our new framework has achieved performance compared to other approaches based on deep neural networks."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Fransconi"], "venue": "In IEEE Transactions on Neural Networks", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proceedings of SemEval 2014: International", "citeRegEx": "Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves et al.2013] Alex Graves", "Navdeep Jaitly", "Abdel rahman Mohamed"], "venue": "In IEEE Workshop on Au- tomatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1998] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1998}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["George Duenas", "Julia Baquero", "Alexander Gelbukh"], "venue": "In Proceedings of SemEval 2014:", "citeRegEx": "Jimenez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Lai", "Hockenmaier2014] Alice Lai", "Julia Hockenmaier"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Robust semantic similarity at multiple levels using maximum weight matching", "author": ["Proisl", "Evert2014] Thomas Proisl", "Stefan Evert"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation", "citeRegEx": "Proisl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Proisl et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Yin", "Schutze2015] Wenpeng Yin", "Hinrich Schutze"], "venue": "In Proceedings of th 53rd Annual Meeting of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Zhao et al.2014] Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evalua-", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features.", "startOffset": 23, "endOffset": 91}, {"referenceID": 7, "context": "Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features.", "startOffset": 23, "endOffset": 91}, {"referenceID": 6, "context": "With the success of deep learning, there has been much interest in applying deep neural network based techniques to further improve the prediction performances (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015).", "startOffset": 160, "endOffset": 225}, {"referenceID": 2, "context": "From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 117, "endOffset": 163}, {"referenceID": 13, "context": "From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 117, "endOffset": 163}, {"referenceID": 18, "context": ", 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015).", "startOffset": 102, "endOffset": 174}, {"referenceID": 10, "context": "On the other hand, some researchers have found character-level convolutional networks (Kim et al., 2016; Zhang et al., 2015) are useful in extracting information from raw signals for the task such as language modeling or text classification.", "startOffset": 86, "endOffset": 124}, {"referenceID": 20, "context": "On the other hand, some researchers have found character-level convolutional networks (Kim et al., 2016; Zhang et al., 2015) are useful in extracting information from raw signals for the task such as language modeling or text classification.", "startOffset": 86, "endOffset": 124}, {"referenceID": 2, "context": "We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) (Collobert et al., 2011), Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al.", "startOffset": 151, "endOffset": 175}, {"referenceID": 4, "context": ", 2011), Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al., 2013) to learn sentence representations.", "startOffset": 106, "endOffset": 127}, {"referenceID": 2, "context": "The other one was introduced by (Collobert et al., 2011).", "startOffset": 32, "endOffset": 56}, {"referenceID": 2, "context": "In (Collobert et al., 2011) architecture, a sequence of length n is represented as: x1:n = x1 \u2295 x2 \u00b7 \u00b7 \u00b7 \u2295 xn , (2)", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "A major issue of RNNs using these transition functions is that it is difficult to learn longrange dependencies during training step because the components of the gradient vector can grow or decay exponentially (Bengio et al., 1994).", "startOffset": 210, "endOffset": 231}, {"referenceID": 4, "context": "Bidirectional RNNs architecture (Graves et al., 2013) proposed a solution of making prediction based on future words.", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": "Inspired by (Tai et al., 2015), we apply elementwise merge to first sentence matrix M1 \u2208 Rn\u00d72d and second sentence matrix M2 \u2208 Rn\u00d72d.", "startOffset": 12, "endOffset": 30}, {"referenceID": 3, "context": "Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011).", "startOffset": 109, "endOffset": 129}, {"referenceID": 18, "context": "where m is the number of training pairs and the superscript k indicates the k-th sentence pair (Tai et al., 2015).", "startOffset": 95, "endOffset": 113}, {"referenceID": 18, "context": "Also our method doesn\u2019t leverage external linguistic resources such as wordnet or parsing which get best results in (Tai et al., 2015).", "startOffset": 116, "endOffset": 134}, {"referenceID": 18, "context": "Note that for semantic relatedness task, the latest research (Tai et al., 2015) proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.", "startOffset": 61, "endOffset": 79}, {"referenceID": 18, "context": "We hope to point out that we implemented the method in (Tai et al., 2015), but the results are not as good as our method.", "startOffset": 55, "endOffset": 73}, {"referenceID": 18, "context": "Based on our experiments, we believe the method in (Tai et al., 2015) is very sensitive to the initializations, thus it may not achieve the good performance in different settings.", "startOffset": 51, "endOffset": 69}, {"referenceID": 7, "context": "804 214 (Jimenez et al., 2014)", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "827 32 (Bjerva et al., 2014)", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "828 72 (Zhao et al., 2014)", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "831 214 (Jimenez et al., 2014)", "startOffset": 8, "endOffset": 30}, {"referenceID": 21, "context": "836 72 (Zhao et al., 2014)", "startOffset": 7, "endOffset": 26}, {"referenceID": 18, "context": "We use dependency tree child-sum tree LSTM proposed by (Tai et al., 2015) as our baseline.", "startOffset": 55, "endOffset": 73}, {"referenceID": 2, "context": "In regular 1D CNNs (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kim, 2014), a fixedsize window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning.", "startOffset": 19, "endOffset": 86}, {"referenceID": 11, "context": "In regular 1D CNNs (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kim, 2014), a fixedsize window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning.", "startOffset": 19, "endOffset": 86}, {"referenceID": 9, "context": "(Kalchbrenner et al., 2014) built deep convolutional models so that local features can mix at high-level layers.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "However, deep convolutional models may result in worse performance (Kim, 2014).", "startOffset": 67, "endOffset": 78}, {"referenceID": 17, "context": "On the other hand, RNN can take advantage of the parsing or dependency tree of sentence structure information (Socher et al., 2011b; Socher et al., 2014).", "startOffset": 110, "endOffset": 153}, {"referenceID": 6, "context": "(Iyyer et al., 2014) used dependencytree recursive neural network to map text descriptions to quiz answers.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "To address this issue, (Tai et al., 2015) proposed a Tree-Structured Long Short-Term Memory Networks.", "startOffset": 23, "endOffset": 41}], "year": 2016, "abstractText": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.", "creator": "LaTeX with hyperref package"}}}