{"id": "1603.09457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "LSTM based Conversation Models", "abstract": "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.", "histories": [["v1", "Thu, 31 Mar 2016 05:14:10 GMT  (80kb,D)", "http://arxiv.org/abs/1603.09457v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yi luan", "yangfeng ji", "mari ostendorf"], "accepted": false, "id": "1603.09457"}, "pdf": {"name": "1603.09457.pdf", "metadata": {"source": "CRF", "title": "LSTM based Conversation Models", "authors": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf"], "emails": ["luanyi@uw.edu,", "jiyfeng@gatech.edu,", "ostendor@uw.edu"], "sections": [{"heading": "1. Introduction", "text": "As automated speech comprehension and generation technology improve, there is increasing interest in building human computer-assisted conversation systems that can be used for a variety of applications such as travel planning, tutorial systems, or chat-based technical support. In most works, the understanding or generation of a sequence of words associated with a single sentence or speaker rotation has been emphasized, potentially making use of the previous rotation. Beyond the local context, the use of language in a goal-oriented conversation reflects the global subject matter of the discussion and the role of each participant. In this work, we introduce a linguistic conversation model that incorporates both the local and global context along with the participant role. In particular, the participant roles (or speaker roles) influence the content of a sentence in terms of both the information to be communicated and the interaction strategy, influencing both the meaning and conversation structure. In the news broadcasts, speaking roles are shown to act as informative for the action discovery of the speaker structures, and the speaker structures are shown to be specific."}, {"heading": "2. Related Work", "text": "With the popularity of social media, such as Twitter, Sina Weibo and online discussion forums, it is now easier to collect conversation texts [6, 7]. Several different data-driven models have been proposed to build conversation systems. Ritter et al. [8] present a way to integrate contextual information via feed-forward neural networks. The flexibility of neural network models opens up the possibility of integrating different types of information into the generational process. Sordoni et al. [9] present a way to integrate contextual information via feed-forward neural networks. Li et al. suggest using Maximum Mutual Information (MMI) as an objective function in neural models to produce more diverse and interesting responses. Shang et al. [10] introduce the attention mechanism into an encoder network for a conversation model."}, {"heading": "3. Model", "text": "In this section, we propose an LSTM-based framework that integrates the role of the participants and the global topic of the conversation. As discussed in Section 1, each role, given the same context, is assumed to have its own preference for selecting words to generate a response, and each generated response should be both thematically related to the current conversation and coherent with the local context."}, {"heading": "3.1. Recurrent Neural Network Language Models", "text": "In general, an RNNLM is a generative clause model. For a clause consisting of a word sequence x1,..., xI, the probability is given of xi x1,..., xi \u2212 1, x \u2264 i \u2212 1 isp (xi | x \u2264 i \u2212 1) ig g\u03c4 (hi) (1), where hi is the current hidden state and g\u0442 (\u00b7) is the probability function parameterized by the following parameters: g\u0442 (hi) = softmax (W\u03c4hi), (2) where W\u0442 is the parameter of the output layer. The hidden state hi is repeatedly computed ashi = f\u03b8 (xi, hi \u2212 1). (3) f\u03b8 (\u00b7) is a non-linear function parameterized by falls. We use an LSTM [23] because it is well suited to capture long-term dependencies, which is a goal for our conversation."}, {"heading": "3.2. Conversation Models with Speaker Roles", "text": "To build a conversation model with different participant roles, we expand an RNNLM in two ways. First, to capture the variability of different participant roles, we integrate role-based information into the generational process. Second, to model a conversation instead of single turns, our model connects RNNLMs for all turns in order to model the entire conversation. Specifically, we look at two adjacent turns. To create a single model for the entire conversation, we simply link the RNNLMs for all sentences in order. Confrontation changes the way we calculate the first hidden state in each utterance (except the first utterance in the conversation). We look at the two turns xt \u2212 1 and xt."}, {"heading": "3.3. Incorporating global topic context", "text": "To capture the long-term context of the conversation inspired by [16], we explicitly include a topic vector that represents all previous dialog turns. We use the latent dirichlet assignment (LDA) to achieve a compact vector space representation, which represents a word sack representation of a document into a low-dimensional vector that is conventionally interpreted as a topic representation. For each turn, we calculate the LDA representation for all previous turns = fLDA (x1, x2,.., xt \u2212 1) (8), where fLDA (\u00b7) is the LDA reference function as in [15]. Then, st is associated with hidden layer to predict text, i.p (xt, i | x \u2264 t, \u2264 i \u2212 1)."}, {"heading": "4. Experiments", "text": "We evaluate our model from various points of view on the Ubuntu Dialogue Corpus [7], which offers one million two-person conversations extracted from Ubuntu chat protocols. The conversations are about obtaining technical support for various Ubuntu-related issues. In this corpus, each conversation contains two users with different roles: POSTER: the user in this conversation who initiates the conversation by suggesting a technical problem; RESPONDER: the other user who tries to provide technical support."}, {"heading": "4.1. Experimental setup", "text": "Our models are trained in a subset of the Ubuntu Dialogue Corpus, in which each conversation contains 6-20 turns. The resulting data includes 216K conversations in the training set, 10k conversations in the test set, and 13k conversations in the development set. We use a Twitter tokenizer [25] to analyze all utterances in the conversations. Vocabulary is built on the training set, filtering out low-frequency tokens and replacing them with \"UNKNOWN.\" Vocabulary size is set to 20K most common words. We have not filtered out emoticons, but treat them as individual tokens. The LDA model is trained in training data based on all conversations, with each conversation treated as an individual training instance. We use Gensim [26] both for training and for conclusions. There are three hyperparameters in our models: the dimension of the word representation K, the hidden dimension H, and the number of themes M in the LDA model. We use the grid [16], the combination of {64} and the combination of the best and H."}, {"heading": "4.2. Evaluation Metrics", "text": "The evaluation of response generation is an emerging field of research in data-driven conversation modeling. Due to the variety of possible responses for a given context, it is too conservative to compare the generated response only with the basic truth. To make a reasonable evaluation, the n-gram-based evaluation metrics, including BLEU [27] and \u0445 BLEU [28], require multiple references for a given context. On the other hand, there are indirect evaluation methods, e.g. rank-based evaluation [7, 10] or qualitative analysis [29]. In this paper, we use both rank-based evaluations (Recall @ K [30]) in all models and leave the n-gram-based evaluation for future work. To calculate the recall @ K metric of a given model K, the model is used to select the top K candidates, and it is counted as correct when the basic answer is included."}, {"heading": "4.3. Quantative Evaluation", "text": "Experiments in this section compare the performance of LDACONV, R-CONV and R-LDA-CONV with the base system LSTM."}, {"heading": "4.3.1. Perplexity", "text": "The best perplexity figures from the three models are in Table 2. R-LDA-CONV shows the least perplexity among the four models, almost 8 points improvement over the base model. Comparing roll versus global theme, a role shows a greater improvement over the perplexity of 11% reduction for roll vs. 7% for LDA theme. Combining both results in a 15% reduction in perplexity. To simplify the comparison, in the following experiments we only use the best configuration for each model."}, {"heading": "4.3.2. Response ranking", "text": "The task is to evaluate the answer to the basic truth based on a few randomly selected sentences for a given context. For each sample, we use the previous t \u2212 1 sentences as context and try to select the best test sentence. We randomly select 9 moves from other conversations in the data set and replace their role with the basic truth label. As we have noticed that sentences from the background channel, such as \"yes,\" \"thank you,\" could fit almost all conversations with different context, sample negative examples with the basic truth length as limitation - samples of similar length (\u00b1 2 words) are selected as negative examples. Recall @ K is shown in Table 3. Both R-CONV and LDA-CONV are better than the base result, while R-LDACONV overall provides the best performance. Both role factors and subject characteristics have a positive effect on the ranking of the basic truth answers. Although the base model does not explicitly use role information, the textual model itself gives better results than the basic truth combination."}, {"heading": "4.4. Qualitative Analysis", "text": "For qualitative analysis, the best R-LDA-CONV model is used to generate role-specific answers, and we examined a number of examples to determine whether the generated answer fits the expected speaker role. In Table 4 and Table 5, we find two examples due to the page constraints. In each case, we have answers generated for each of the possible roles: one more question for the POSTER and one potential solution for the RESPONDER. As we can see from the context part of Table 4, different roles clearly have different behaviors during the conversation. If we ignore the validity of this potential solution, this generated answer matches our expectation of the role of the RESPONDER. The POSTER's answer appears quite plausible. The response of the RESPONDER is clearly the right style, but more domain information in the topic vector could lead to a more useful solution.Table 5 shows another example to show the difference between the POSPONER role and the POSPONER role as a specific answer rather than a specific one in this example."}, {"heading": "5. Summary", "text": "We present three models: RCONV, LDA-CONV and R-LDA-CONV by integrating role factors and theme features into the output level. We evaluate the model both on the basis of helplessness and on the basis of the response ranking. Both RCONV and LDA-CONV outperform the basic model in all tasks. R-LDA-CONV provides the best performance by combining the two components. In addition, the generational results show topical coherence and differences in the responses associated with different roles. In addition to role and theme, our model structure can be generalized to include more monitored information. In future work, we would integrate monitored domain knowledge into our model to improve the relevance of the response."}, {"heading": "6. References", "text": "[1] Regina Barzilay, Michael Collins, Julia Hirschberg, and SteveWhittaker use the role of speaker in the context of the language course. [2] B Hutchinson, B. Zhang, and M. Ostendorf. Unsupervised broadcast speech role labeling. In ICASSP, pages 5322-5325, 2010. [3] B. Hutchinson B. Zhang, M. Marin and M. Ostendorf. Learning phrase patterns for text classification. IEEE Trans. Audio, Speech and Language Processing, 21 (6): 1180-1189, 2013. [4] Alex Marin B. Zhang, and Mari Ostendorf. In Proceedings of the Workshop on Languages in Social Media, pages 39-47. Association for Computational Linguistics, 2011. [5] Janet M Fuller."}], "references": [{"title": "The rules behind roles: Identifying speaker role in radio broadcasts", "author": ["Regina Barzilay", "Michael Collins", "Julia Hirschberg", "Steve Whittaker"], "venue": "In AAAI/IAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Unsupervised broadcast conversation speaker role labeling", "author": ["B Hutchinson", "B. Zhang", "M. Ostendorf"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Learning phrase patterns for text classification", "author": ["B. Hutchinson B. Zhang", "M.A. Marin", "M. Ostendorf"], "venue": "IEEE Trans. Audio, Speech and Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Detecting forum authority claims in online discussions", "author": ["Alex Marin", "Bin Zhang", "Mari Ostendorf"], "venue": "In Proceedings of the Workshop on Languages in Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "The influence of speaker roles on discourse marker use", "author": ["Janet M Fuller"], "venue": "Journal of Pragmatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A Dataset for Research on Short-Text Conversations", "author": ["Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen"], "venue": "In EMNLP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1506.08909,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A neural network approach to contextsensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Neural Responding Machine for Short-Text Conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1508.01745,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1511.03962,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Efficient learning for spoken language understanding tasks with word embedding based pre-training", "author": ["Yi Luan", "Shinji Watanabe", "Bret Harsham"], "venue": "In Sixteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "In SLT,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Multi-speaker language modeling", "author": ["G. Ji", "J. Bilmes"], "venue": "In HLT NAACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Rank and sparsity in language processing", "author": ["Brian Hutchinson"], "venue": "PhD thesis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A Sparse Plus Low-Rank Exponential Language Model for Limited Resource Scenarios", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Modeling topic and role information in meetings using the hierarchical Dirichlet process", "author": ["Songfang Huang", "Steve Renals"], "venue": "In Machine Learning for Multimodal Interaction,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1603.06155,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Machine learning: a probabilistic perspective", "author": ["Kevin P Murphy"], "venue": "MIT press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A Smith"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06863,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Introduction to information retrieval, volume 1", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "In online discussions, speaker role is useful for detecting authority claims [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "Other work shows that in casual conversations, speakers with different roles are likely to use different discourse markers [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "With the popularity of social media, such as Twitter, Sina Weibo, and online discussion forums, it is easier to collect conversation text [6, 7].", "startOffset": 138, "endOffset": 144}, {"referenceID": 6, "context": "With the popularity of social media, such as Twitter, Sina Weibo, and online discussion forums, it is easier to collect conversation text [6, 7].", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "[8] present a statistical machine translation based conversation system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] present a way to integrate contextual information via feed-forward neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] introduce the attention mechanism into an encoderdecoder network for a conversation model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], where a Dialog-act component is introduced into the LSTM cell to guide the generated content.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Different ways of modeling document-level context has been explored in [12] and [13] based on the LSTM framework.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Different ways of modeling document-level context has been explored in [12] and [13] based on the LSTM framework.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "[14] proposed a multi-scale recurrent architecture to incorporate both word and turn level context for spoken language understanding tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In this paper, we use a similar approach as [16], explicitly using Latent Dirichlet Analysis (LDA) as global-context feature to feed into RNNLM.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "Early work on incorporating local context in conversational language modeling is described in [17] conditioned on the most recent word spoken by other speakers.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "[18, 19] improve log-bilinear language model by introducing a multi-factor sparse matrix that could capture speaker role and topic information.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[18, 19] improve log-bilinear language model by introducing a multi-factor sparse matrix that could capture speaker role and topic information.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[20] show that language models with role information significantly reduce word error rate in speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "propose using an additional vector to LSTM in order to capture personal characteristics of a speaker [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "We start building a response generation model [9] by using a recurrent neural network language model (RNNLM) [22].", "startOffset": 46, "endOffset": 49}, {"referenceID": 21, "context": "We start building a response generation model [9] by using a recurrent neural network language model (RNNLM) [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "We use an LSTM [23] since it is good at capturing long-term dependency, which is an objective for our conversation model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "Despite the difference between the two models, they can be learned in the same way, which is similar to training a RNNLM [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "We choose cross entropy [24] as the loss function `(\u00b7, \u00b7), because it is a popular objective function used in training neural language models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "In order to capture long-span context of the conversation, inspired by [16], we explicitly include a topic vector representing all previous dialog turns.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "where fLDA (\u00b7) is the LDA inference function as in [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "We evaluate our model from different aspects on the Ubuntu Dialogue Corpus [7], which provides one million two-person conversations extracted from Ubuntu chat logs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 24, "context": "We use a Twitter tokenizer [25] to parse all utterances in the conversations.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "We use Gensim [26] for both training and inference.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "For a reasonable evaluation, the n-gram based evaluation metrics including BLEU [27] and \u2206BLEU [28] require multiple references for one given context.", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "For a reasonable evaluation, the n-gram based evaluation metrics including BLEU [27] and \u2206BLEU [28] require multiple references for one given context.", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "One the other hand, there are indirect evaluation methods, for example, ranking based evaluation [7, 10] or qualitative analysis [29].", "startOffset": 97, "endOffset": 104}, {"referenceID": 9, "context": "One the other hand, there are indirect evaluation methods, for example, ranking based evaluation [7, 10] or qualitative analysis [29].", "startOffset": 97, "endOffset": 104}, {"referenceID": 28, "context": "In this paper, we use both ranking-based evaluation (Recall@K [30]) across all models, and leave the n-gram based evaluation for future work.", "startOffset": 62, "endOffset": 66}], "year": 2016, "abstractText": "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.", "creator": "LaTeX with hyperref package"}}}