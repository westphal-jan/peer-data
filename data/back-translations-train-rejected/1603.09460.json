{"id": "1603.09460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "System Combination for Short Utterance Speaker Recognition", "abstract": "Noticeable performance degradation is often observed in text-independent speaker recognition with short test utterances. This paper presents a combination approach to improve short utterance speaker recognition (SUSR), where two phonetic-aware systems are combined together: one is the DNN-based i-vector system and the other is the subregion-based GMM-UBM system proposed by us recently. The former employs phone posteriors to construct an i-vector model in which the shared statistics offer stronger robustness against limited test data. The latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A scorelevel system combination approach is proposed to integrate the respective advantages of the two systems. Experimental results confirm that on the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the score-level system combination delivers significant performance improvement.", "histories": [["v1", "Thu, 31 Mar 2016 05:47:03 GMT  (301kb,D)", "http://arxiv.org/abs/1603.09460v1", null], ["v2", "Tue, 27 Sep 2016 13:49:05 GMT  (1288kb)", "http://arxiv.org/abs/1603.09460v2", "APSIPA ASC 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["lantian li", "dong wang", "xiaodong zhang", "thomas fang zheng", "panshi jin"], "accepted": false, "id": "1603.09460"}, "pdf": {"name": "1603.09460.pdf", "metadata": {"source": "CRF", "title": "System Combination for Short Utterance Speaker Recognition", "authors": ["Lantian Li", "Dong Wang", "Thomas Fang Zheng"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "fzheng@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "Text-independent speaker recognition with short test statements. This paper presents a combination approach to improving Short Expression Speaker Recognition (SUSR), combining two phonetically conscious systems: One is the DNN-based i-vector system and the other is the recently proposed subregion-based GMM-UBM system. The former uses telephone posteriors to construct an i-vector model in which the common statistics provide greater robustness compared to limited test data. The latter establishes a phone-dependent GMM-UBM system that represents speaker characteristics with more detail. To integrate the respective advantages of the two systems, a combination approach based on scor levels is proposed. Experimental results confirm that in the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMUM-BM system deliver their respective base lines and signal levels."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to find its way into the future."}, {"heading": "2. Related work", "text": "For example, Omar et al. [13] proposed deriving UBMs from Gaussian components of a GMM-based ASR system using a Kmeans cluster approach based on the symmetrical KL distance. DNN-based i-vector method was proposed in [11, 12]. Posteriors of seniors (context-dependent states) generated by an ASR-trained DNN were used for both model training and i-vector inference. Note that all of these studies focus on relatively long expressions (5-10 seconds), while our study in this paper focuses on expressions of only 0.5 seconds."}, {"heading": "3. Subregion modeling", "text": "We briefly describe the sub-region model we recently presented [10]: First, the basic idea is presented and then the implementation details are described."}, {"heading": "3.1. Acoustic subregions", "text": "The conventional GMM-UBM system treats the entire acoustic space as a whole probable space and calculates the probability of an input speech signal using a GMM model formulated as follows: p (x; s) = p (t) c P (c) N (xt; \u00b5sc), where x denotes the speech signal, and N (x; \u00b5) represents a Gaussian distribution with \u00b5 as the mean and \u03a3 as the covariance matrix. Furthermore, c indexes the Gaussian component and s indexes the loudspeaker. P (c) is a prior distribution to the c-th component. Roughly speaking, this model splits the acoustic space into a number of subregions, and each subregion is modeled by a Gaussian distribution component. There are at least three potential problems with this model: (1) the sugion-Bregion splitting is based on unmonitored clustering (via the EM algorithm [14]), so that it is not necessarily meaningful in a Gaussian system (1)."}, {"heading": "3.2. Speech units", "text": "The inventory of language units varies for different languages. In Chinese, the languages focused in this paper are initials and finals (IF), the most commonly used [15]. Roughly speaking, initials correspond to consonants, and finals correspond to vowels and nasals. Among IFs, it is acknowledged that finals convey more speaker-related information [16, 17] and are therefore used as language units in this study. Using finals to train the subdomain model is not particularly practical, since there are a large number of finals and most finals can find only limited data both in training and testing. One possible solution is to combine similar units and form submodels based on the resulting language units. In this study, we are developing a vector quantization method (VQ) based on the Kmeans algorithm [18] to perform clustering."}, {"heading": "3.3. Subregion modeling based on speech unit classes", "text": "Identify the language unit classes (final cluster) by {SUC-c, c = 1,..., C}, a sub-region UBM can be trained for each SUC-c with the training data aligned to the finals in SUC-c by the ASR system. The sub-region UBM of class SUC-c is designated by \u03bbUBMc. \u2022 The speaker-dependent sub-region GMMs can be trained on the basis of the sub-region UBMs, using the enrollment data aligned to the finals of each cluster. \u2022 The entire training process of the sub-region models is illustrated below. \u2022 Global UBM training, indicated by \u03bbUBM. A global UBM is trained with the entire training database using the EM algorithm. \u2022 Subregion UBM training is illustrated by the Sugiance class."}, {"heading": "4. System combination", "text": "In this section we first describe the difference between the sub-region model and another phonetically conscious method: the DNN-based i-vector model. Then we present the combination system."}, {"heading": "4.1. DNN-ivector and subregion model", "text": "The DNN-based i-vector approach [11] proposed by Lei and colleagues replaces GMM-based rear values with DNN-generated rear values in the calculation of the Tree-Welch statistics for model training and i-vector inference. The DNN model is designed for speech recognition so that the output targets correspond to telephones or states, essentially creating an UBM and loudspeaker GMMs, with the Gaussian components corresponding to telephones or states. This is similar to the sub-region model, although the model structures of the two models differ. On the one hand, the sub-region model builds GMMs for each sub-region, while the DNN-based i-vector approach continues to adopt Gaussian for each sub-region. In this respect, the sub-region model tends to be more flexible and displays loudspeaker properties with more detail. On the other hand, the sub-regions in subregion modelling are relatively independent, while the sub-regions in the DNN-vector model are more flexible with each other (VLT)."}, {"heading": "4.2. Score-level system combination", "text": "Due to the difference between the two phonetically conscious models and their perspective advantages, it makes sense to combine them. The combination system consists of three components: firstly, a DNN model for ASR is trained and used to generate the phonetic information: telephone posteriors and telephone alignments; secondly, the telephone posteriors are used to train the DNN-based i-vector model and the telephone alignment is used to build the sub-region model; thirdly, when evaluating a test speech, the values from the DNN vector system and the GMM-UBM subfield are averaged to make the final decision. Figure 1 illustrates the system framework."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Database", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1. Database for evaluation (SUD12)", "text": "There is no standard database for evaluating the performance of text-independent SUSR tasks. Therefore, we first designed and recorded a database suitable for SUSR research and published it for use in research.1 The database was called \"SUD12\" [20] and was designed according to the principle of ensuring sufficient IF coverage. To focus on short expressions and exclude other factors such as channel and emotion, the recording was done in the same room and with the same microphone, and the reading style was neutral. The database consists of 28 male and 28 female speakers, and all utterances are in Standard Chinese. For each speaker, there are 100 Chinese sentences, each containing 15 to 30 Chinese characters. The sampling rate is 16 kHz with 16-bit accuracy. The sampling database includes all 56 speakers and 28 female speakers, and after removing silent segments for registration, the effective speech signals are about 35 seconds."}, {"heading": "5.1.2. Database for UBM training (863DB)", "text": "The language data used to train the UBMs and the UBMs of the sub-region were selected from the 863 Chinese language corpus [21]. The 863 database is designed to cover all Chinese IFs, making it particularly suitable for training subregion UBMs on the basis of graduation classes. All recordings are made at a sampling rate of 16 kHz and the sampling rate is 16 bits. In this study, we select 17 hours of language data and designate the database as 863DB."}, {"heading": "5.2. Experimental conditions", "text": "The Kaldi toolkit [22] was used to perform the experiments. According to the standard recipe of SRE08, the acoustic feature was the traditional 60-dimensional Mel Frequency Receiver Coefficients (MFCCs), which included 20-dimensional static components plus the first and second order derivatives, the frame size was 25 ms and the frame shift was 10 ms. In addition, a simple energy-based speech activity detection (VAD) was performed before the feature was extracted. The ASR system for generating telephone alignment was a large-scale DNN-HMM hybrid system. Kaldi was trained according to the WSJ-S5 recipe. The feature used was 40-dimensional Fbanks. The basic features were fed through a window of 11 frames, and an LDA (linear discriminative analysis) was applied to reduce the dimensionality of the 200 DNM-200 data contained to the four DNM-1 layer each."}, {"heading": "5.3. Basic results", "text": "In our experiments, we observed that either too small or too large cluster numbers lead to suboptimal performance, and the optimal setting in our experiment was C = 6 [10]. Table 1 shows the derived unit classes. It can be seen that the resulting clusters are intuitively reasonable. Results regarding EER are presented in Table 2, where \"GMM-UBM\" is the GMM-UBM base system. \"SBM-DD\" stands for the subregion modeling system (C = 6). \"GMM i-Vector\" stands for the traditional GMM-based i-vector system and \"DNN i-Vector\" for the DNN-based i-vector system. We first find that both the subregion modeling system and the DNN-based i-vector system have a much better understanding than the two GMM-vector systems (GMM-vector systems), which rely on two GMM-Vector systems."}, {"heading": "5.4. System combination", "text": "We combine the \"DNN i-vector + PLDA\" system and the \"SBMDD\" system by a linear score fusion: \u03b1splda + (1 \u2212 \u03b1) ssbm, where \u03b1 is the interpolation factor. Figure 3 shows the performance with different \u03b1. It clearly shows that the system combination leads to better performance than any single system. Figure 3 shows that \u03b1 = 0.94 is a good choice. Table 2 and Figure 2 have shown the results of the combination system with this configuration."}, {"heading": "6. Conclusions", "text": "This paper presents a combination system for short utterances in text-independent speaker recognition that combines two phonetically conscious methods: the DNN-based i-vector system and the subregional GMMUBM system. The experimental results show that both the DNN-based i-vector system and the subregional GMMUBM system exceed their respective baselines and a simple point fusion leads to the best performance we have achieved to date. Future work will focus on combining feature and model-based remuneration for short utterances and investigating telephone discriminatory methods."}, {"heading": "7. References", "text": "[1] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. Magrin-Chagnolleau, S. Meignier, T. Merlin, J. Ortega-Garc\u0131 \"a, D. Petrovska-Delacre\" taz, \"and D. A. Reynolds,\" A tutorial on textindependent speaker verification, \"EURASIP Journal on Applied Signal Processing, vol. S. S. S. S., pp. 430-451, 2004. [2] T. Kinnunen and H. Li.\" An overview of text-independent speaker verification: From features to supervectors \"Speech communication, vol. 52, no. S. S. S., S. S. S. S., Stanford, A. F. Martin, M. Yadagiri, G. R. Doddington, J. J. Godfrey, and J. Hernandez-Cordero.\" The 2012 nist speaker recognition evaluation, \"in Proc."}], "references": [{"title": "A tutorial on textindependent speaker verification", "author": ["F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin- Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Garc\u0131\u0301a", "D. Petrovska-Delacr\u00e9taz", "D.A. Reynolds"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430\u2013451, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "An overview of text-independent speaker recognition: From features to supervectors", "author": ["T. Kinnunen", "H. Li"], "venue": "Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "The 2012 nist speaker recognition evaluation", "author": ["C.S. Greenberg", "V.M. Stanford", "A.F. Martin", "M. Yadagiri", "G.R. Doddington", "J.J. Godfrey", "J. Hernandez-Cordero"], "venue": "Proc. INTERSPEECH\u201913, 2013, pp. 1971\u20131975.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Making confident speaker verification decisions with minimal speech", "author": ["R. Vogt", "S. Sridharan", "M. Mason"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1182\u20131192, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparison of various adaptation methods for speaker verification with limited enrollment data", "author": ["M.-W. Mak", "R. Hsiao", "B. Mak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008, vol. 1. IEEE, 2006, pp. I\u2013I.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Factor analysis modelling for speaker verification with short utterances", "author": ["R.J. Vogt", "C.J. Lustri", "S. Sridharan"], "venue": "The Speaker and Language Recognition Workshop. IEEE, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "i-vector based speaker recognition on short utterances", "author": ["A. Kanagasundaram", "R. Vogt", "D.B. Dean", "S. Sridharan", "M.W. Mason"], "venue": "Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u2013 2344.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A segment selection technique for speaker verification", "author": ["M. Nosratighods", "E. Ambikairajah", "J. Epps", "M.J. Carey"], "venue": "Speech Communication, vol. 52, no. 9, pp. 753\u2013761, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Rsr2015: Database for text-dependent speaker verification using multiple pass-phrases", "author": ["A. Larcher", "K.-A. Lee", "B. Ma", "H. Li"], "venue": "Proc. INTERSPEECH\u201912, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving short utterance speaker recognition by modeling speech unit classes", "author": ["L. Li", "D. Wang", "C. Zhang", "T.Z. Zheng"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. DOI: 10.1109/TASLP.2016.2544660, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014. IEEE, 2014, pp. 1695\u2013 1699.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyseey\u20192014. Odyssey, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The expectation-maximization algorithm", "author": ["T.K. Moon"], "venue": "Signal processing magazine, IEEE, vol. 13, no. 6, pp. 47\u201360, 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Improved context-dependent acoustic modeling for continuous chinese speech recognition.", "author": ["J.-Y. Zhang", "T.F. Zheng", "J. Li", "C.-H. Luo", "G.-L. Zhang"], "venue": "in Proc. INTERSPEECH\u201901,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Fundamentals of speaker recognition", "author": ["H. Beigi"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Research on Highly Distinguishable Speech Selection Methods in Speaker Recognition", "author": ["C. Gong"], "venue": "Tsinghua University,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Methods for demonstrating resemblance in taxonomy and ecology", "author": ["A. Hall"], "venue": "Nature, vol. 214, pp. 830\u2013831, 1967.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1967}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291\u2013298, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "A k-phoneme-class based multi-model method for short utterance speaker recognition", "author": ["C. Zhang", "X.-J. Wu", "T.F. Zheng", "L.-L. Wang", "C. Yin"], "venue": "Asia-Pacific Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), vol. 20, no. 12, 2012, pp. 1\u20134.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-layer channel normalization for frequency-dynamic feature extraction", "author": ["D. Wang", "X.-Y. Zhu", "Y. Liu"], "venue": "Journal of Software, vol. 12, no. 9, pp. p1523\u20131529, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, IEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J. Prince", "J.H. Elder"], "venue": "ICCV\u201907. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "The det curve in assessment of detection task performance", "author": ["A. Martin", "G. Doddington", "T. Kamm", "M. Ordowski", "M. Przybocki"], "venue": "DTIC Document, Tech. Rep., 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].", "startOffset": 167, "endOffset": 176}, {"referenceID": 1, "context": "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].", "startOffset": 167, "endOffset": 176}, {"referenceID": 2, "context": "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].", "startOffset": 167, "endOffset": 176}, {"referenceID": 3, "context": "[4] reported that when the test speech was shortened from 20 seconds to 2 seconds, the performance degraded sharply in terms of equal error rate (EER) from 6.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] showed that when the length of the test speech was less than 2 seconds, the EER was raised to 35%.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For example, in [6], the authors showed that the performance on short utterances can be improved by JFA.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "This work was extended in [7] which reported that the i-vector model can dis-", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In addition, a score-based segment selection technique was proposed in [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "The latter is certainly more resilient to short utterances, as has been demonstrated in [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "The DNN-based i-vector method was proposed in [11, 12].", "startOffset": 46, "endOffset": 54}, {"referenceID": 11, "context": "The DNN-based i-vector method was proposed in [11, 12].", "startOffset": 46, "endOffset": 54}, {"referenceID": 9, "context": "We briefly describe the subregion model presented by us recently [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "There are at least three potential problems with this model: (1) the subregion splitting is based on unsupervised clustering (via the EM algorithm [14]), so it is not necessarily meaningful in phonetic; (2) each subregion is modeled by a Gaussian, which seems too simple; (3) the priors over the subregions are fixed, independent of the speech signal xt.", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "In Chinese, the language focused in this paper, Initials and Finals (IF) are the most commonly used [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Among the IFs, Finals are recognized to convey more speaker related information [16, 17], and therefore are used as the speech units in this study.", "startOffset": 80, "endOffset": 88}, {"referenceID": 15, "context": "Among the IFs, Finals are recognized to convey more speaker related information [16, 17], and therefore are used as the speech units in this study.", "startOffset": 80, "endOffset": 88}, {"referenceID": 16, "context": "In this study, we develop a vector quantization (VQ) method based on the kmeans algorithm [18] to conduct the clustering.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "A subregion UBM \u03bb c is then trained for the cth speech unit class based on the global UBM, by employing the MAP algorithm [19] with the speech data assigned to SUC-c.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "The DNN-based i-vector approach proposed by Lei and colleagues [11] replaces GMM-based posteriors by DNNgenerated posteriors when computing the Baum-Welch statistics for model training and i-vector inference.", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "The database was named as \u201cSUD12\u201d [20], and was designed in the principle to guarantee sufficient IF coverage.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "The speech data used to train the UBMs and subregion UBMs were chosen from the 863 Chinese speech corpus [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "The Kaldi toolkit [22] was used to conduct the experiments.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "In our experiments, we observed that either too small or too large clustering numbers lead to suboptimal performance, and the optimal setting in our experiment was C=6 [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Besides, it can be seen that the GMMUBM baseline outperforms the two i-vector systems, but after the probabilistic linear discriminant analysis (PLDA) [23] is employed, the i-vector system is significantly improved and outperforms the GMM-UBM system.", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "For a better understanding of the performance of different systems on various operation points, the DET curves are presented in Figure 2, where the horizontal axis represents the false acceptance rate (FAR) and the vertical axis represents the false rejection rate (FRR) [24].", "startOffset": 271, "endOffset": 275}], "year": 2017, "abstractText": "Noticeable performance degradation is often observed in text-independent speaker recognition with short test utterances. This paper presents a combination approach to improve short utterance speaker recognition (SUSR), where two phonetic-aware systems are combined together: one is the DNN-based i-vector system and the other is the subregion-based GMM-UBM system proposed by us recently. The former employs phone posteriors to construct an i-vector model in which the shared statistics offer stronger robustness against limited test data. The latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A scorelevel system combination approach is proposed to integrate the respective advantages of the two systems. Experimental results confirm that on the text-independent SUSR task, both the DNNbased i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the scorelevel system combination delivers significant performance improvement.", "creator": "LaTeX with hyperref package"}}}