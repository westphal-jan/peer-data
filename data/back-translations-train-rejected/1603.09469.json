{"id": "1603.09469", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "abstract": "The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system.", "histories": [["v1", "Thu, 31 Mar 2016 06:55:25 GMT  (5644kb,D)", "http://arxiv.org/abs/1603.09469v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hyunsuk ko", "rui song", "c -c jay kuo"], "accepted": false, "id": "1603.09469"}, "pdf": {"name": "1603.09469.pdf", "metadata": {"source": "CRF", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "authors": ["Hyunsuk Ko", "Rui Song", "C.-C. Jay Kuo"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move and to move."}, {"heading": "II. REVIEW OF PREVIOUS WORK", "text": "In fact, it is such that it is a way in which one sees oneself able to surpass oneself by engaging oneself in the way one imagines it: in the way in which one contemplates the world, and in the way in which one contemplates it, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the way in which it encircles the world, in the world in the way in which it encircles, in the world in the way in which it encircles, in the world in the way in which it encircles, in the world in the way in which it encircles, in the world, in the way in the way in which it encircles, in the world in the world in the way in which it encircles, encircles, in the world in the way in the way in which it encircles, in the world in the world, in the way in which it encircles, encircles, in the world in the way in the way in which it encircles, encircles, encircles, encircles, in the world, in the way in the way in the way in the way in which it encircles, encircles, encircles, encircles, encircles, encircles, encircles, encircles, in the way in the way in the way in the way in the way in the way in the way in the way in the way in which it encircles, encircles, encircles, encircles, in the world in the world, in the way in the world, in the world in the way in the"}, {"heading": "III. OVERVIEW OF PBSIQA SYSTEM", "text": "In this thesis, we will use the second approach in designing a SIQA system based on the idea of parallel increase (ParaBoost).The proposed PBSIQA system consists of two stages. Stage I will design several learning-based scorers, each of which will handle a specific type of distortion such as the blocking artifact, blurring of distortions, additive noise, etc. Output of these scorers is a standardized objective score, ranging from 0 to 1. Output of these scorers is an obedient score that only takes into account target distortion. Stage II, the fixer will take scores from all individual scorers to obtain the final quality score. In particular, we will derive the predictive model in the fixer from a learning process as well. The availability of multiple scorers in Stage I will allow us to handle complex factors that systematically affect human perception quality. Specifically, we will compare the effects of different texture distortions and depth maps on the Q2D system and then the proposed Q2D images."}, {"heading": "IV. MCL-3D, IVC-A AND LIVE-A DATABASES", "text": "This year, the time has come to put yourself at the top of the list."}, {"heading": "V. PROPOSED PBSIQA SYSTEM", "text": "The influence of texture distortion and depth map distortion on playback quality differs by a view synthesis process. In this section, we will first design scorers that are tailored to texture distortion and depth map distortion in Sec. V-A and Sec. V-B. The design of these scorers and the score fuser will be implemented by Support Vector Regression (SVR) as described in Sec. V-C."}, {"heading": "A. Scorer Design for Texture Distortions", "text": "In Dataset A of the MCL 3D database, distortions are applied to the texture image while the depth chart is unaffected. In this case, we see a similar distortion in the rendered stereo image. Example: If the texture representation is distorted by a transmission error, a similar distortion type is observed in the rendered view, since pixel values of the input texture are directly related to the pixel intensity of the rendered view. In addition, [21] reports that the interaction between left and right views in perceived 3D image quality depends on the distortion type. For example, for blurring and JPEG-2000 encoding, a high-quality view is negatively affected by distortions compensated by a high-quality view. Thus, the perceptual quality is closer to the high-quality view. On the other hand, for additive noise and JPEG encoding, a high-quality view is negatively affected."}, {"heading": "B. Scorer Design for Depth Distortions", "text": "The data B of the MCL-3D database contains only depth distortions. Research on the effects of depth distortion on rendered stereo image quality has recently been done by some researchers, e.g. [41], [42]. Generally, however, the depth value is inversely proportional to the horizontal disparity of the rendered left and right views, so that the horizontal disparity of the image appears in the form of geometric errors. [43] A depth no synthesis error model has been proposed, in which depth information is typically distorted, while the disparity for a visually comfortable stereo pair is often far less than 256additive SnoiseSonSthSpart transmissionSerrorSonSart1 9 17 25 33 49 57 01668616686868612612612612612673 stereo pair."}, {"heading": "C. Learning-based Scorers and Fuser Design", "text": "The proposed PBSIQA system, which consists of nine quality scorers, is summarized in Fig. 4. To obtain an intermediate value of scorers # 1 \u0445 # 8, we adopt the method of support vector regression (SVR). Let us consider a series of training data (xn, yn) where xn is a feature vector and yn is the target value, e.g. the subjective quality value of the n-th image. In \u03b5-SVR [44], [45] the goal is to find a mapping function f (xn) that has at most a deviation from the target value, yn, for all training data. The mapping function is in the form of off (x) = wT\u03c6 (x) + b, (22) where w is a weighting vector, \u03c6 (\u00b7) is a deviation function f (\u00b7) that has a deviation from the target value, yn, for all training data."}, {"heading": "D. Training and Test Procedures", "text": "The n-fold cross-validation [47] is a common strategy for evaluating the performance of a learning-based algorithm to ensure reliable results and prevent overmatch, where the data is split into n-chunks and a chunk is used as test data, while the remaining n-1 chunks are used as training data. Finally, the total performance is determined over all the predicted values. In the proposed framework, training data is used to generate a regression model of each goalie in level I. Then, a regression model of assurance is used by training all intermediate results from level I as input characteristics with n-fold cross-validation in level II, where the number of samples is the same as the number of stereoscopic image pairs in level I. Then, we will determine all intermediate results from level I as input characteristics with n-fold cross-validation in level II."}, {"heading": "VI. PERFORMANCE EVALUATION", "text": "In order to evaluate the performance of the proposed PBSIQA measurement, we follow the proposals of ITU-T (P.1401) [48] and use three measures of performance: 1) the Pearson correlation coefficient (PCC) to measure the linear relationship between the performance of a model and the subjective data, 2) the Spearman rank correlation coefficient (SROCC) for the monotonicity of the prediction and 3) the mean of the squared error (RMSE) for the predictive accuracy. Before calculating the measures of performance, we apply the monotonic logistics function to the predicted quality values to adjust the subjective quality grades 9 (MOS) and to adjust all non-linearity viaf (s) = \u03b21 \u2212 \u03b221 + exp (\u2212 \u03b23 | \u03b24 |) + \u03b22, (27) where s and f (s) are the predicted value or the predicted value, and \u03b2k (2) are between the mean values (1) and the mapping (3)."}, {"heading": "A. Performance Comparison of Stage I Scorers", "text": "First, we compare the performance of eight learning-based scorers (scorers # 1-8) and show that each scorer really provides a good performance for his goal distortion type. Here, we look at dataset C of the MLC 3D database only, and divide it into six sub-databases, so that each contains a distortion type, as in Section IV. The six distortion types are listed in the top row of Table IV. We calculate the PCC value of each scorer against each sub-database and rank its effectiveness individually in the descending order of PCC values. Table IV summarizes the results of the scorers in Stage I (i.e. without merging into Stage II). The performance of the scorers # 1, # 2, and # 3 matches their target design well. First, Goalscorers # 1 exhibits the highest correlation on the wiped and JPEG2000 compression database."}, {"heading": "B. Performance Improvement via Fusion", "text": "We show how the PBSIQA system can be improved by merging the results in level I in this area. To illustrate the design method, without losing the general public, we use the performance of Tornados # 8 for the geometric errors in the base, insert a Tornados in the table shown in the first column and improve the performance of six sub-databases and the entire MCL database. Results are shown in Table V, where the performance of Tornados # 8 is shown in the first column."}, {"heading": "C. Performance Comparison with Other Quality Indices", "text": "In fact, most of us are able to hold our own, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed a ParaBoost method to design a new stereoscopic image quality assessment system called PBSIQA, which includes a series of parallel quality assessors that address various distortions in Stage I and merge the results to obtain the ultimate quality assessment value in Stage II. The excellent performance of the PBSIQA system has been supported by extensive experimental results. Our research has implications for the area of 3D content delivery, as reliable 3D quality assessment serves as a valuable objective function in optimizing the overall performance of the system. As future enhancements, it is interesting to include the masking effect, for example, distortions in prominent regions such as foreground objects have a negative impact on perception quality. Furthermore, textured regions are generally more robust against distortions, while distortions in homogeneous regions are more visible."}, {"heading": "ACKNOWLEDGMENT", "text": "The calculation of the work described in this paper was supported by the Center for High-Performance Computing (hpc.usc.edu) at the University of Southern California."}], "references": [{"title": "Text of ISO/IEC FDIS 23000-11 for Stereoscopic Video AF", "author": ["ISO/IEC JTC1/SC29/WG11"], "venue": "N10283, Busan, Korea, Oct. 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Overview of the Stereo and Multiview Video Coding Extensions of the H.264/ MPEG-4 AVC Standard", "author": ["A. Vetro", "T. Wiegand", "G.J. Sullivan"], "venue": "Proc. of the IEEE, vol. 99, no. 4, pp. 626-642, Apr. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Wiegand,\u201c3-D Video Representation Using DepthMaps,", "author": ["K. Muller", "P. Merkle"], "venue": "Proc. of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Quality assessment of stereoscopic 3D image compression by binocular integration behaviors", "author": ["L. Yu-Hsun", "JL. Wu"], "venue": "IEEE Transactions on Image Processing, vol. 23, pp. 1527-1542, Apr. 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Human perception of mismatched stereoscopic 3D inputs", "author": ["L.B. Stelmach", "W.J. Tam", "D.V. Meegan", "A. Vincent", "P. Corriveau"], "venue": "in Proc. IEEE ICIP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Unequal weighting of monocular inputs in binocular combination: Implications for the compression of stereoscopic imagery", "author": ["D.V. Meegan", "L.B. Stelmach", "W.J. Tam"], "venue": "Journal of Experimental Psychology: Applied,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Perceived quality of compressed stereoscopic images: Effects of symmetric and asymmetric JPEG coding and camera separation", "author": ["P. Seuntiens", "L. Meesters", "W. Ijsselsteijn"], "venue": "ACM Transactions on Applied Perception (TAP), vol. 3, pp. 95-109, Apr. 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Depth map creation and image based rendering for advanced 3DTV services providing interoperability and scalability", "author": ["P. Kauff", "N. Atzpadin", "C. Fehn", "M. Muller", "O. Schreer", "A. Smolic", "R. Tanger"], "venue": "Signal Process., Image Commun., vol. 22, no. 2, pp. 217-234, Feb. 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Digital video quality: vision models and metrics", "author": ["S. Winkler"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Image quality assessment using multi-method fusion", "author": ["T.-J. Liu", "W. Lin", "C.-C.J. Kuo"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 5, pp. 1793-1807, May. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1807}, {"title": "Stereoscopic image quality assessment", "author": ["P. Campisi", "P. Le Callet", "E. Marini"], "venue": "Proc. 15th European Signal Processing Conference,, Sep. 2007", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Using disparity for quality assessment of stereoscopic images", "author": ["A. Benoit", "P. Le Callet", "P. Campisi", "R. Cousseau"], "venue": "Proc. IEEE ICIP, pp. 389-392, Oct. 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Quality assessment of a stereo pair formed from decoded and synthesized views using objective metrics", "author": ["P.Hanhart", "T. Ebrahimi"], "venue": "Proc. 3DTV-CON, pp. 1-4, Oct. 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Quality assessment of a stereo pair formed from two synthesized views using objective metrics", "author": ["P. Hanhart", "T. Ebrahimi"], "venue": "7th VPQM, Jan. 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Objective No- Reference Stereoscopic Image Quality Prediction Based on 2D Image Features and Relative Disparity", "author": ["Z.M.P. Sazzad", "R. Akhter", "J. Baltes", "Y. Horita"], "venue": "Advances in Multimedia, vol. 2012, Jan. 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereoscopic Image Quality Metrics and Compression", "author": ["P. Gorley", "N. Holliman"], "venue": "International Society for Optics and Photonics Electronic Imaging, pp. 680305-680305, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "CYCLOP: A stereo color image quality assessment metric", "author": ["A. Maalouf", "M.C. Larabi"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1161-1164, May, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Reduced-reference quality assessment for 3D video compression and transmission", "author": ["C.T. Hewage", "M.G. Martini"], "venue": "IEEE Transactions on Consumer Electronics, vol. 57, no. 3, pp. 1185-1193, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal interactive continuous scoring of subjective 3D video quality of experience", "author": ["T. Kim", "J. Kang", "S. Lee", "A. Bovik"], "venue": "IEEE Transactions on Multimedia, vol. 16, no. 2, pp. 387-402, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Stereoscopic image quality metric based on binocular perception model", "author": ["S. Ryu", "D. Kim", "K. Sohn"], "venue": "Proc. IEEE ICIP, vol. 1, pp. 609-612, Sep. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "3D image quality index using SDP-based binocular perception model", "author": ["H. Ko", "C. Kim", "S. Choi", "C.-C.J. Kuo"], "venue": "IEEE 11th IVMSP Workshop, pp. 1-4, Jun. 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Perceptual full-reference quality assessment of stereoscopic images by considering binocular visual characteristics", "author": ["F. Shao", "L. Weisi", "G. Shanbo", "J. Gangyi", "S. Thambipillai"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 5, pp. 1940-1953, May, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1940}, {"title": "No-Reference Quality Assessment for Stereoscopic Images Based on Binocular Quality Perception", "author": ["S. Ryu", "K. Sohn"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 4, pp. 591-602, Apr, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward Assessing and Improving the Quality of Stereo Images", "author": ["M. Park", "L. Jiebo", "C.G. Andrew"], "venue": "IEEE Journal of Selected Topics in Signal Processing,, vol. 6, no. 5, pp. 460-470, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Objective Quality Assessment of Stereo Images Based on ICA and BT-SVM", "author": ["J. Cheng", "L. Sumei"], "venue": "IEEE 7th International Conference on Computer Science & Education (ICCSE), pp. 154-159, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Recommendation ITU-T P.910, Subjective video quality assessment methods for multimedia applications", "author": ["ITU"], "venue": "tech. rep., ITU-T, Geneva, 1996.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Recommendation ITU-R BT.2021: Subjective methods for the assessment of stereoscopic 3DTV systems", "author": ["ITU-R"], "venue": "tehc.rep., 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "BT.500-11: Methodology for the subjective assessment of the quality of television pictures", "author": ["ITU-R"], "venue": "tech. rep., 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source", "author": ["R. Song", "H. Ko", "C.-C.J. Kuo"], "venue": "accepted by Journal of Information Science and Engineering.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 0}, {"title": "IRC- CyN/IVC 3D images database", "author": ["A. Benoit", "P.L. Callet", "P. Campisi", "R. Cousseau"], "venue": "ACM Transactions on Applied Perception (TAP), IVC-Database [Online]. Available: \u201chttp://www.irccyn.ecnantes.fr/spip.php?article876\u201d, 2008", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Subjective evaluation of stereoscopic image quality", "author": ["A.K. Moorthy", "C.C. Su", "A. Mittal", "A.C. Bovik"], "venue": "Signal Processing: Image Communication, LIVE-Database [Online]. Available: \u201clive.ece.utexas.edu/research/quality/live 3dimage phase1.html\u201d, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving robustness of image quality measurement with degradation classification and machine learning", "author": ["T.H. Falk", "Y. Guo", "W. Y Chan"], "venue": "Proc. Conference Record of the Forty-First Asilomar Conference on Signals, Systems and Computers (ACSSC 2007), pp. 503-507, 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Image quality measures and their performance", "author": ["A.M. Eskicioglu", "P.S. Fisher"], "venue": "IEEE Transaction on Communications, vol. 43, pp. 29592965, Dec. 1995", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing, vol. 13, pp. 600-612, Apr. 2004.  13", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical evaluation of image quality measures", "author": ["I. Avcibas", "B. Sankur", "K. Sayood"], "venue": "Journal of Electronic imaging, vol. 11, pp. 206-223, 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "FSIM: a feature similarity index for image quality assessment,", "author": ["L. Zhang", "X. Mou", "D. Zhang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "SVD-based quality metric for image and video using machine learning", "author": ["M. Narwaria", "W. Lin"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42, pp. 347-364, Apr. 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A Universal Image Quality Index", "author": ["Z. Wang", "A. Bovik"], "venue": "IEEE Signal Processing Letter, vol. 9, no 3, Mar. 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Depth map coding with distortion estimation of rendered view", "author": ["W.S. Kim", "A. Ortega", "P.L. Lai", "D. Tian", "C. Gomila"], "venue": "SPIE Vis. Inf. Process. Commun., 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Depth map coding based on synthesized view distortion function", "author": ["B.T. Oh", "J. Lee", "D.-S. Park"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 5, no. 7, pp. 13441352, Nov. 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Depth no-synthesis-error model for view synthesis in 3-D video", "author": ["Y. Zhao", "C. Zhu", "Z. Chen", "L. Yu"], "venue": "IEEE Trans. Image Process., vol. 20, no. 8, pp. 22212228, Aug. 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with kernels: support vector machines, regularization, optimization and beyond,", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2002}, {"title": "Patranabis \u201cSupport vector regression,", "author": ["D. Basak", "S. Pal", "D. C"], "venue": "Neural Information Processing-Letters and Reviews,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "A practical guide to support vector classification,", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J Lin"], "venue": "Dept. Comput.Sci.,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Vol. 1, New York: springer, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "P.1401: Statistical analysis, evaluation and reporting guidelines of quality measurements", "author": ["ITU-T"], "venue": "tech. rep., 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Image quality assessment based on a degradation model", "author": ["N. Damera-Venkata", "T.D. Kite", "W.S. Geisler", "B.L. Evans", "A.C. Bovik"], "venue": "IEEE Transactions on Image Processing, vol. 4, no. 4, pp. 636-650, Apr. 2000.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2000}, {"title": "Image information and visual quality", "author": ["H.R. Sheikh", "A.C. Bovik"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 2, pp. 430-444, Feb. 2006.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "VSNR: A wavelet-based visual signal-to-noise ratio for natural images", "author": ["D.M. Chandler", "S.S. Hemami"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 9, pp. 2284-2298, Sep. 2007.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "On between-coefficient contrast masking of dct basis functions,\u201din", "author": ["N. Ponomarenko", "F. Silvestri", "K. Egiazarian", "M. Carli", "J. Astola", "V. Lukin"], "venue": "Proc. 3rd Int. Workshop Video Process. Quality Metrics Consum. Electron.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "An image quality assessment method based on perception of structural information", "author": ["M. Carnec", "P. Le Callet", "D. Barba"], "venue": "Proc. IEEE ICIP, vol. 3, pp. III-185, Sep. 2003.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "The PSNR measure (or any other 2D quality metric) does not correlate well with human visual experience of 3D visual stimuli [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 6, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 190, "endOffset": 193}, {"referenceID": 7, "context": "That is, both the texture and the depth data are compressed at the encoder and transmitted to the decoder and, then, virtual views are synthesized using the depth-image-based-rendering (DIBR) technique [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 8, "context": "Traditionally, objective image quality metrics were classified into pixel-based metrics and the human visual system (HVS) inspired metrics [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Both pixel-based metrics and the human visual system (HVS) inspired metrics in [9] belong to this class.", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": ", MMF in [10] is obtained using the \u201clearning-based prediction approach\u201d.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "By following the first approach, 2D metrics were extended to 3D metrics by combining distortions of a depth map and images of both views linearly in [11], [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "By following the first approach, 2D metrics were extended to 3D metrics by combining distortions of a depth map and images of both views linearly in [11], [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "[13], [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[15] proposed a SIQA metric for JPEG compressed images using segmented local features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Gorley and Holliman [16] proposed a metric based on the sensitivity of HVS to contrast and luminance changes.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Maalou and Larab [17] proposed a metric by exploiting the color disparity tensor and the contrast sensitivity function.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Hewage and Martini [18] presented a reduced-reference quality metric based on edge detection in the depth map and demonstrated a good approximation for the full-reference quality metric.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "[19] proposed a methodology for subjective 3D QoE assessment experiments using external stimuli such as vibration, flickering and sound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Stelmach and Meegan [5], [6] reported that binocular perception is dominated by the higher quality image in face of low-pass filtering operations yet by the average of both images for quantized distortions.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "Stelmach and Meegan [5], [6] reported that binocular perception is dominated by the higher quality image in face of low-pass filtering operations yet by the average of both images for quantized distortions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "[7] observed that the perceived quality of a JPEG-coded stereo image pair is close to the average quality of two individual views.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] proposed an extended version of the SSIM index based on a binocular model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] introduced the notion of structural distortion parameter (SDP), which varies according to distortion types, and employed the SDP as a control parameter in a binocular perception model to provide robust QA results for both symmetric and asymmetric distortions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] proposed an SIQA method by considering the binocular combination property and the binocular just noticeable difference model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] proposed a no-reference quality metric by considering perceptual blurriness and blockiness scores and taking visual saliency into account.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] offers a good example for 2D learning-based image QA, among many others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] used a set of geometric stereo features for anaglyph images and built a regression model to capture the relationship between these features and the quality of stereo images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Cheng and Sumei [25] extracted a set of basis images using independent component analysis and used a binary-tree support vector machine (SVM) to predict scores of distorted stereo images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "To capture the content variety, the Spatial Information (SI) defined by ITU-T recommendation [26] for each texture image and its depth map was calculated.", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "Based on the recommendations of ITU [28], we consider five quality levels in subjective tests.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The test was performed in a controlled environment as recommended by ITU [28], including the display equipment, viewing distance, ambient light, etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "500 [29], and outliers were removed.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "For further details, we refer to [30].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "Pixel difference MD (Maximum Difference) [35]", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "Structural Similarity SSIM index [36]", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "MAE (Mean Absolute Error) [37] SSIM Luminance [36]", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "MAE (Mean Absolute Error) [37] SSIM Luminance [36]", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "PSNR SSIM Contrast [36] ABV (Average Block Variance) SSIM Similarity [36]", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "PSNR SSIM Contrast [36] ABV (Average Block Variance) SSIM Similarity [36]", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "MIN (Modified Infinity Norm) [37] UQI (Universal Quality Index) [40] Blockiness -", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": "MIN (Modified Infinity Norm) [37] UQI (Universal Quality Index) [40] Blockiness -", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 116, "endOffset": 120}, {"referenceID": 34, "context": "related ES (Edge Stability) [37] Spectral", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "PRATT [37] Contrast measure GM (Gradient Magnitude) [38]", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "PRATT [37] Contrast measure GM (Gradient Magnitude) [38]", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "Correlation MAS (Mean Angle Similarity) [37] HVS HVS-MSE [34]", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Correlation MAS (Mean Angle Similarity) [37] HVS HVS-MSE [34]", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "NCC (Normalized Cross Correlation) [37] View synthesis NDSE (Noticeable Depth Synthesis Error) [43]", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": "NCC (Normalized Cross Correlation) [37] View synthesis NDSE (Noticeable Depth Synthesis Error) [43]", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 19, "endOffset": 25}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 27, "endOffset": 33}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 35, "endOffset": 41}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 83, "endOffset": 89}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 99, "endOffset": 105}, {"referenceID": 26, "context": "We conducted a subjective test to obtain MOS using the absolute category rating (ACR) [28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "from IVC [32] database Six stereoscopic image pairs", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "from LIVE [33] database # of test images 648 144", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 8, "endOffset": 13}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 15, "endOffset": 20}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 15, "endOffset": 20}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 22, "endOffset": 27}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 22, "endOffset": 27}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 39, "endOffset": 44}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 39, "endOffset": 44}, {"referenceID": 0, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 41, "endOffset": 44}, {"referenceID": 20, "context": "Furthermore, it is reported in [21] that the interaction between left and right views in perceived 3D image quality depends on the distortion type.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "(ILD) [20].", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "The first one is the edge stability mean squre error (ESMSE) [37], which characterizes the consistency of edges that are evident across multiple scales between the original and distorted images.", "startOffset": 61, "endOffset": 65}, {"referenceID": 34, "context": "The other one is Pratt\u2019s measure [37] that considers the accuracy of detected edge locations, missing edges and false alarm edges.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "When stereoscopic images with blocking distortion are shown, the average quality of both views or that of the lower quality is perceived [7], [21].", "startOffset": 137, "endOffset": 140}, {"referenceID": 20, "context": "When stereoscopic images with blocking distortion are shown, the average quality of both views or that of the lower quality is perceived [7], [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Such artifacts are called the information additive distortion (IAD) [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "As stated in [10], [34], there are two good features in detecting blockiness.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "As stated in [10], [34], there are two good features in detecting blockiness.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "Mathematically, the HVS MSE [34] is given by", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "We select three such features to characterize additive noise: the peak-signal-to-noise ratio (PSNR), the maximum difference (MD) [35] and the modified infinity norm (MIN) [37].", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "We select three such features to characterize additive noise: the peak-signal-to-noise ratio (PSNR), the maximum difference (MD) [35] and the modified infinity norm (MIN) [37].", "startOffset": 171, "endOffset": 175}, {"referenceID": 33, "context": "Structural errors can be captured by three components of the SSIM index [36], which are luminance, contrast and structural similarities between images x and y.", "startOffset": 72, "endOffset": 76}, {"referenceID": 35, "context": "Two features were proposed in [38] at this end: phase congruency (PC) and gradient magnitude (GM).", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "Readers are referred to [38] for its detailed definition and properties.", "startOffset": 24, "endOffset": 28}, {"referenceID": 36, "context": "It was shown in [39] that the first several singular vectors offer a good set to represent the structural information of objects while subsequent vectors account for finer details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "Based on the feature analysis, both the universal quality index (UQI) [40] and the mean angle similarity (MAS) [37] are useful in characterizing transmission error.", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "Based on the feature analysis, both the universal quality index (UQI) [40] and the mean angle similarity (MAS) [37] are useful in characterizing transmission error.", "startOffset": 111, "endOffset": 115}, {"referenceID": 38, "context": ", [41], [42].", "startOffset": 2, "endOffset": 6}, {"referenceID": 39, "context": ", [41], [42].", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "[43] proposed a Depth No-Synthesis-Error (D-NOSE) model by exploiting that the depth information is typically stored in 8-bit grayscale format while the disparity range for a visually comfortable stereo pair is often far less than 256 additiveSnoiseSonSdepthSpart transmissionSerrorSonSdepthSpart", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "For more details on the D-NOSE profile, we refer to [43].", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "That is, we use the SDP index [21] that is computed based on three depth maps as a candidate scorer.", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "In the \u03b5-SVR [44], [45], the objective is to find a mapping function f(xn) that has a deviation at most \u03b5 from the target value, yn, for all training data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "In the \u03b5-SVR [44], [45], the objective is to find a mapping function f(xn) that has a deviation at most \u03b5 from the target value, yn, for all training data.", "startOffset": 19, "endOffset": 23}, {"referenceID": 43, "context": "where \u03c1 is the radius controlling parameter, since it provides good performance in applications [46].", "startOffset": 96, "endOffset": 100}, {"referenceID": 42, "context": "Thus, we use a different version of the regression algorithm called the \u03bd-SVR [45], where \u03bd \u2208 (0, 1) is a control parameter to adjust the number of support vectors and the accuracy level.", "startOffset": 78, "endOffset": 82}, {"referenceID": 44, "context": "The n-fold cross validation [47] is a common strategy to evaluate the performance of a learning-based algorithm to ensure reliable results and prevent over-fitting, where the data are split into n chunks, and one chunk is used as the test data while the remaining n \u2212 1 chunks are used as training data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "We scale the feature values of each scorer to the unit range of [0,1] at Stage I.", "startOffset": 64, "endOffset": 69}, {"referenceID": 43, "context": "We conduct parameter search on C and \u03b3 at the training stage using the cross validation scheme in [46].", "startOffset": 98, "endOffset": 102}, {"referenceID": 45, "context": "1401) [48] and use three performance measures: 1) the Pearson correlation coefficient (PCC) to measure the linear relationship between a model\u2019s performance and the subjective data , 2) the Spearman rank-order correlation coefficient (SROCC) for the prediction monotonicity, and 3) the root mean squared error (RMSE) for the prediction accuracy.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "As observed in [21], the information additive distortion (IAD) is more obvious than the information loss distortion (ILD) among all structural distortions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Our observation is consistent with that in [21].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "The latter two are more challenging than their sources, IVC [32] and LIVE [33], by including asymmetric distortions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 30, "context": "The latter two are more challenging than their sources, IVC [32] and LIVE [33], by including asymmetric distortions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 46, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 151, "endOffset": 155}, {"referenceID": 37, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 191, "endOffset": 195}, {"referenceID": 33, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 235, "endOffset": 239}, {"referenceID": 47, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 268, "endOffset": 272}, {"referenceID": 48, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 314, "endOffset": 318}, {"referenceID": 49, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 382, "endOffset": 386}, {"referenceID": 50, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 402, "endOffset": 406}], "year": 2016, "abstractText": "The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system.", "creator": "LaTeX with hyperref package"}}}