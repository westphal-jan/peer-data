{"id": "1603.09509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Learning Multiscale Features Directly From Waveforms", "abstract": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms.", "histories": [["v1", "Thu, 31 Mar 2016 09:54:44 GMT  (2977kb,D)", "https://arxiv.org/abs/1603.09509v1", null], ["v2", "Tue, 5 Apr 2016 14:17:09 GMT  (2977kb,D)", "http://arxiv.org/abs/1603.09509v2", "\"fix typo in the title\""]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE cs.SD", "authors": ["zhenyao zhu", "jesse h engel", "awni hannun"], "accepted": false, "id": "1603.09509"}, "pdf": {"name": "1603.09509.pdf", "metadata": {"source": "CRF", "title": "Learning Multiscale Features Directly From Waveforms", "authors": ["Zhenyao Zhu", "Jesse H. Engel", "Awni Hannun"], "emails": ["zhuzychn@gmail.com,", "jengel@baidu.com,", "awnihannun@baidu.com"], "sections": [{"heading": null, "text": "In this paper, we describe an approach to using coil filters to overcome the inherent trade-off between time and frequency resolution that exists for spectral representation. At increased computational costs, we show that increasing time resolution through reduced step speed and increasing frequency resolution through additional filters leads to significant performance improvements. In addition, we find more efficient representations through simultaneous learning on multiple scales, resulting in an overall 20.7% reduction in word error rate for a difficult internal language test compared to networks with the same number of parameters trained on spectrograms. Index terms: speech recognition, multiscale learning, Convolutional Neural Networks, Raw Waveforms"}, {"heading": "1. Introduction", "text": "In this context, it should be noted that this is a very complex issue, it is a political project, it is about putting the interests of the people at the centre and it is about putting the interests of the people at the centre."}, {"heading": "2. Experimental Setup", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we are able to assert ourselves, that we are able, that we are able, that we are able, that we are able, that we are able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to put ourselves in a position. \""}, {"heading": "3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Decoupling temporal resolution", "text": "While many studies have compared the folding of raw waveforms with features such as spectrograms, MFCCs and Melscale filter banks, they often compare the two with similar steps and window sizes [5, 6]. Spectrograms can take a big step because of the unique analytical structure of the basic functions. Integration twice, once via the real cosine and once via its imaginary sine counterpart, identifies both the phase and the order of magnitude of the reaction. This is similar in many ways to performing a folding over each time step and maximum pooling, whereby the phase is represented by the index at which the maximum occurs, but is much more computationally efficient to test with an FFT. We have decided to test this hypothesis that a folding with low step and pooling can find at least as good a basis as the spectrogram. Figure 2 and Table 2 show the effects of replacing the spectrogram with a single folding layer and a number of filters and pooling."}, {"heading": "3.2. Multiscale features are more efficient", "text": "Similarly, displaying low-frequency information with a small filter is challenging, as separate filters are required for individual parts of the shaft. We believe that applying folds on multiple scales simultaneously could allow each scale to learn selective filters on the frequencies it can most efficiently represent. To test our hypothesis, we compare the performance of folding front ends with a constant number of filters on three different scales (high: (1ms window, 1 / 4ms step), medium: (4ms, 1ms) and low: (40ms, 10ms), with a front end that uses all three scales (Diagram 1.Table 3 shows that WHO decreases significantly from high to medium to low: filter bands, with the RF bands having improved performance of the lower frequency banks."}, {"heading": "3.3. Decoupling frequency resolution", "text": "With methods based on the Fourier transformation, there is an intrinsic compromise between time and frequency resolution. As the number of basic functions increases and helps to determine which frequencies are present, the window size also suffers, thus smearing knowledge of when they occur. A decrease in the step cannot increase the time resolution, it can only provide more samples of the smeared signal. Convolutions do not suffer from the same compromise, since the time resolution is limited by the number of filters and the time resolution of the step, both of which are independent of each other. This advantage is accompanied by higher costs for calculation and memory, both by increasing the number of filters and by decreasing the step. We examine the value of the increased resolution by conducting the same multi-scale experiments as in the previous section, but increasing the number of filters. Table 4 shows that increasing the number of filters, even by a factor of 3, leads to a significant increase in the number of production characteristics (8% ER of each fully associated with the number of small-dimensional filters in 7.)."}, {"heading": "4. Discussion", "text": "In this paper, we have consistently shown that waveform learning can exceed spectra, especially when applied at multiple levels. However, several interesting research questions remain unanswered before such techniques are likely to become widely used."}, {"heading": "5. References", "text": "[1] G. Hinton, L. Deng, D. Yu, A. rahman Mohamed, N. Jaitly, A. Senior, P. Nguyen, T. Sainath, and B. Kingsbury, B. C. Catanzaro, J. Chen, M. Chrzanowski, A. Elsen, L. Engel, L. Fan, C. Fougner, A. Y. Hannun, B. LeGresley, L. Lin, A. Yates, G. Diamanowski, G. Diamos, E. Engel, L. Engel, C. Fougner, S. Sengupta, S. Sengupta, C. Wang, B. Wang, B. Xiao, D. Yogama, J. Ozair, R. Prenger, J. Raiman, S. Satheesh, D. Satheesh, D. Seetapun, S. Sengupta."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B.C. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A.Y. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A.Y. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "CoRR, vol. abs/1512.02595, 2015. [Online]. Available: http://arxiv.org/abs/1512.02595", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["D. Palaz", "R. Collobert"], "venue": "Proceedings of Interspeech, no. EPFL-CONF-210029, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["T.N. Sainath", "R.J. Weiss", "A. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["P. Golik", "Z. T\u00fcske", "R. Schl\u00fcter", "H. Ney"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a better representation of speech soundwaves using restricted boltzmann machines.", "author": ["N. Jaitly", "G.E. Hinton"], "venue": "in ICASSP. IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 6964\u2013 6968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1915\u2013 1929, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1915}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "Computer Vision-ECCV 2014 Workshops. Springer, 2014, pp. 474\u2013490.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015. [Online]. Available: http: //arxiv.org/abs/1502.03167", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, 8 2013. [Online]. Available: http://kheafield.com/professional/edinburgh/estimate paper.pdf", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "CoRR, vol. abs/1509.08967, 2015. [Online]. Available: http: //arxiv.org/abs/1509.08967", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.L. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR \u201907. IEEE Conference on, June 2007, pp. 1\u20138.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "A clockwork RNN", "author": ["J. Koutn\u0131\u0301k", "K. Greff", "F.J. Gomez", "J. Schmidhuber"], "venue": "CoRR, vol. abs/1402.3511, 2014. [Online]. Available: http://arxiv.org/abs/1402.3511", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning\u2019s recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Deep learning\u2019s recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 3, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 4, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 5, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 6, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 7, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 3, "context": "While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4].", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 1, "context": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 12, "context": "Batch normalization [13], is employed between each layer, but not between individual timesteps [2].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Batch normalization [13], is employed between each layer, but not between individual timesteps [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 1, "context": "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 11, "context": "(superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "Following [2], we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.", "startOffset": 214, "endOffset": 218}, {"referenceID": 1, "context": "As previously observed [2], deep neural networks trained on sufficient data perform better as the model size grows.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "While many studies have compared convolution on raw waveforms to features such as spectrograms, MFCCs, and melscale filterbanks, they often compare the two at a similar strides and window sizes [5, 6].", "startOffset": 194, "endOffset": 200}, {"referenceID": 5, "context": "While many studies have compared convolution on raw waveforms to features such as spectrograms, MFCCs, and melscale filterbanks, they often compare the two at a similar strides and window sizes [5, 6].", "startOffset": 194, "endOffset": 200}, {"referenceID": 3, "context": "The improved performance of the lower frequency banks is evidence for the importance of low frequency vocalization features in speech recognition [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "[4] found noticeable improvements from supplementing log-mel filterbanks in such a manner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16].", "startOffset": 179, "endOffset": 186}, {"referenceID": 15, "context": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16].", "startOffset": 179, "endOffset": 186}, {"referenceID": 16, "context": "Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Recently proposed architectures that operate simultaneously at different timescales, such as the Clockwork RNN [18], could provide a more elegant way of combining multiscale signals.", "startOffset": 111, "endOffset": 115}], "year": 2016, "abstractText": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-toend learning, where features are learned directly from waveforms, has only recently reached the performance of handtailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms.", "creator": "LaTeX with hyperref package"}}}