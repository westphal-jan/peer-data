{"id": "1603.09631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Data Collection for Interactive Learning through the Dialog", "abstract": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "histories": [["v1", "Thu, 31 Mar 2016 15:13:51 GMT  (497kb,D)", "https://arxiv.org/abs/1603.09631v1", null], ["v2", "Sun, 15 May 2016 13:03:26 GMT  (497kb,D)", "http://arxiv.org/abs/1603.09631v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["miroslav vodol\\'an", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": false, "id": "1603.09631"}, "pdf": {"name": "1603.09631.pdf", "metadata": {"source": "CRF", "title": "Data Collection for Interactive Learning through the Dialog", "authors": ["Miroslav Vodol\u00e1n", "Filip Jur\u010d\u0131\u0301\u010dek"], "emails": ["jurcicek}@ufal.mff.cuni.cz"], "sections": [{"heading": null, "text": "Keywords: dataset, data collection, dialog, knowledge diagram, interactive learning"}, {"heading": "1. Introduction", "text": "Today, dialog systems are typically designed for a single domain (Mrksic et al., 2015), storing data in a well-defined format with a fixed number of attributes for units that the system can provide. Since data in this format can be stored as a two-dimensional table within a relational database, we call the data flat. This data representation allows the system to query the database in a simple and efficient way. It also allows to maintain the dialog status in the form of slots (which normally correspond to columns in the table) and track it through the dialog using probabilistic belief capture (Williams et al., 2013; Henderson et al., 2014). However, the well-defined structure of the database of a typical dialog system comes at a high cost, as each piece of new information must correspond to the format. This is particularly a problem if we adapt the system for a new database, as its units may have different attributes. A dialog system based on knowledge 4. Offers enormous advantages that can be presented as initial structured information."}, {"heading": "2. Motivation", "text": "From the point of view of dialog systems that provide general information from a knowledge base, the most limited factor is that a large proportion of the questions are poorly understood. Current approaches (Berant and Liang, 2015; Bordes et al., 2014) can only achieve an accuracy of about 50% when answering some questions. We call this approach interactive learning from dialogues. We can improve dialog systems in several aspects by interactive learning in direct interaction with users. First, the easiest way is obviously to get the correct answer to questions that the system does not know. We can try to ask users for answers to questions that the system has encountered in a conversation with another user and that have not understood it. Second, the system can ask the user for a more comprehensive explanation of a question. This explanation could help the system understand the question and provide the willing response system to find the answers that the user has available to interact with other users."}, {"heading": "3. Dialog policies", "text": "The obvious difficulty in developing a dialog system is to find a way to identify the piece of information that interests Xiv: 160 3.09 631v 2 [cs.C L] 15 May 201 6the user. In particular, this is a problem for dialog systems that are based on knowledge diagrams that contain a large amount of complex structured information. While a similar problem is solved in answering questions, dialog systems have more opportunities to identify the true intention of the user. For example, a dialog system can request additional information during the dialog. We distinguish three different basic approaches to requesting knowledge bases: handmade policy - the policy consists of fixed rules implemented by system developers, offline policy - the policy is interacted from some kind of (normally commented) offline training data without interacting with system users (Bordes et al., 2015), interactively learned policy - the system learns the policy from the dialog implemented by its users by interacting with it's users, where it is also possible for them to actively interact with the above mentioned information."}, {"heading": "4. Dialog Simulation", "text": "Offline evaluation of interactive dialogs on real data is difficult because different policies can lead to different variants of the dialog. Our solution to this problem is to collect data in a way that allows us to simulate all dialog variants that are possible according to any policy. Dialogue variants that we consider for interactive learning differ only in the presence of several parts of the dialog. Therefore, we can collect dialogs that contain all the information used for interactive learning and omit those parts that were not requested by the policy. We have collected the data set (see Section 5.), which allows a simulation in which the policy can decide how much additional information is required for the question it is requesting. If the question is clear to the system, it can try to answer the question without any other information. It can also request a more comprehensive explanation, with the possibility to answer the question afterwards. If the system decides not to answer the question, we can forward the question to another user to try to get the answer from them."}, {"heading": "5. Dataset Collection Process", "text": "A perfect data collection scenario for our data set would use a real-life dialog system that provides real users with general information from the knowledge base, which could then ask for explanations and answers to questions it cannot answer. However, it is usually difficult to access systems with real users. Therefore, we used the crowdsourcing platform CrowdFlower1 (CF) for our data collection. A CF employee is given a task that instructs him to use our chat-like user interface to help the system with a question randomly selected from training examples for simple questions (Bordes et al., 2015). To complete the task, the user must communicate with the system through the three-phase dialog, the question paraphrase (see Section 5.1.), explanation (see Section 5.2.) and answer of the question (see Section 5.3.)."}, {"heading": "5.1. Question Paraphrasing", "text": "At the beginning of the dialog, the system asks the user to paraphrase questions that the system does not understand. The main objective of this first phase is to familiarize the user with the presented question and to obtain alternative formulations of the asked question."}, {"heading": "5.2. Question Explanation", "text": "In the second phase, the user is asked to explain the question. We expect the explanation to be sufficiently different from the original question (in terms of the number of common words between question and explanation). If the explanation is too similar to the question, the user is notified that their explanation is not comprehensive enough and they must provide a better.1http: / / crowdflower.com"}, {"heading": "5.3. Question Answer", "text": "With the valid declaration, the dialog moves to the last stage, in which the user is asked for a correct answer to the original question. The system requires the user to answer with a full sentence. In practical experiments, this has proven to be a useful decision because it improves the system's ability to detect fraudsters. We can simply measure the connection (in common words) between question and answer set, making it possible to reject completely irrelevant answers."}, {"heading": "5.4. Annotation", "text": "The correct answer to each question in each dialog can be found in the Simple questions dataset. The answers are in the form of Freebase2 units, which are identified by a unique ID. For evaluation purposes, we need information about whether the dialog contains the answer that matches the one in Simple questions, the answer with another unit, or whether the dialog does not contain an answer. Although the annotation process is fairly simple, we did not need crowdsourcing for the process."}, {"heading": "5.5. Natural Language Understanding (NLU)", "text": "The capture system must recognize the following dialog files based on user comments at all stages of the dialog: negate - user does not want to provide requested information, affirm - user agrees to provide requested information, DontKnow - user does not know the requested information, 2https: / / www.freebase.com / ChitChat - user tries to chat with the system (hello, bye, who are you...), inform - none of the above is interpreted as giving the user the information requested by the system. Parsing the dialog files is done according to handwritten rules using templates and keyword spotting. The templates and keywords were manually collected from frequent expressions used by CF staff during the preparation of the data collection process (google it, check wikipedia, I would need... \u2192 negate)."}, {"heading": "6. Dataset Properties", "text": "We collected the dataset with 1900 dialogs and 8533 revolutions. Topics discussed in dialogs are questions randomly selected from training examples for simple questions (Bordes et al., 2015). From this dataset we also took the correct answers in the form of freebase units. Our dataset consists of standard data divided into training, development and test files. Basic features of these files are as follows: Each file contains complete dialogs enriched by NLU output (see Section 5.5.) used during data collection. Furthermore, each dialog is marked by the correct answer for the question and expert comments for the user answer, indicating whether the reference to the correct answer, incorrect answer or no answer at all. 351 of the collected dialogs contain correct answers provided by users, and 702 dialogs have incorrect comments as comments that the 847 users did not want to answer the remaining question."}, {"heading": "7. Interactive Learning Evaluation", "text": "A perfect interactive learning model would be able to learn everything interactively from test dialogs during the test, which would allow us to measure the progress of the model from scratch over time. However, developing such a model would be unnecessarily difficult, so we offer training dialogs that can be used for feature extraction and other engineering skills related to interactive learning from dialogues in natural language. Model development is also supported by labeled validation data for parameter optimization. We propose two evaluation metrics for comparing interactive learning models: first, a metric (see Section 7.1.) receives the amount of information required by the model; second, a metric (see Section 7.2.) is the accuracy of response extraction from user comments. All models need to base their answers only on information obtained from training dialogs and test dialogues during the simulation to ensure that the score reflects the general response of the learning rather than interactive questions."}, {"heading": "7.1. Efficiency Score", "text": "The simulation of dialogs from our data set allows to judge how efficient a dialog system is in using information obtained from the user. The dialog system should maximize the number of correctly answered questions without requiring too many explanations and answers from the users. In order to evaluate different systems based on the collected data, we suggest the following evaluation standards: SD = nc \u2212 wini \u2212 wene \u2212 wana | D | (1) Here, nc denotes the number of correctly answered questions, ni the number of incorrectly answered questions, ne the number of requested explanations, na the number of requested answers and | D | the number of simulated dialogues in the datasets. wi, we, wa are penalty measures. Penalties are used to compensate for different costs of obtaining different types of information from the user. For example, it is relatively easy to obtain broader explanations from the user because it is convenient for them to cooperate with the system on a question they are interested in."}, {"heading": "7.2. Answer Extraction Accuracy", "text": "Therefore, we propose another measurement relevant to our dataset, which is the accuracy of entity extraction, which measures the number of times a correct response has been extracted from the user's response clues in dialogs that are described as correctly answered."}, {"heading": "8. Future Work", "text": "Our future work will focus mainly on providing a basic system for interactive learning, which will be evaluated on the data set. We also plan improvements to the dialogue management, which is used to gain explanations during the data collection. We believe that by talking about specific aspects of the discussed question it will be possible to get even more interesting information from users. The other area of our interest is in ways to improve the accuracy of the answer to questions in test questions of the Simple Question data set with the additional information contained in the collected dialogs."}, {"heading": "9. Conclusion", "text": "In this paper, we have presented a novel method for evaluating different interactive learning approaches for dialogue models. Evaluation covers two challenging aspects of interactive learning: firstly, it assesses the efficiency of the use of information obtained by users in simulated questionnaires; secondly, it measures the accuracy of understanding responses; for the purpose of evaluation, we have collected a data set from conversations with workers on the crowdsourcing platform CrowdFlower. These dialogues were commented on with expert comments and published under the Creative Commons 4.0 BY-SA license on lindat3; and we provide evaluation scripts with the data set to ensure a comparable evaluation of different interactive learning approaches."}, {"heading": "10. Acknowledgments", "text": "This work was financed by the Ministry of Education, Youth and Sport of the Czech Republic under the funding agreement LK11221 and nuclear research funding, the SIA project 260 224 and the GAUK project 1170516 of Charles University in Prague using language resources stored and distributed in the LINDAT / CLARIN project of the Ministry of Education, Youth and Sport of the Czech Republic (project LM2015071)."}, {"heading": "11. Bibliographical References", "text": "Berant, J. and Liang, P. (2015). Imitation learning ofagenda-based semantic parsers. Transactions of the Association for Computational Linguistics (TACL), 3: 545-558.Berant, J., Chou, A., Frostig, R., and Liang, P. (2013). Semantic Parsing on Freebase from Question-Answer Pairs. Proceedings of EMNLP, (October): 1533-1544.Bordes, A., Chopra, S., and Weston, J. (2014). Question Answering with Subgraph Embeddings.Bordes, A., Usunier, N., Chopra, S., and Weston, J. (2015). Large-scale Simple Question Answering with Memory Networks.3hdl.handle.net / 11234 / 1-1670France, F. D., Yvon, F., and Collin T. (2003)."}], "references": [{"title": "Imitation learning of agenda-based semantic parsers", "author": ["J. Berant", "P. Liang"], "venue": "Transactions of the Association for Computational Linguistics (TACL), 3:545\u2013 558.", "citeRegEx": "Berant and Liang,? 2015", "shortCiteRegEx": "Berant and Liang", "year": 2015}, {"title": "Semantic Parsing on Freebase from Question-Answer Pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "Proceedings of EMNLP, (October):1533\u20131544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Question Answering with Subgraph Embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale Simple Question Answering with Memory Networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Learning paraphrases to improve a question-answering system", "author": ["F.D. France", "F. Yvon", "O. Collin"], "venue": "In Proceedings of the 10th Conference of EACL Workshop Natural Language Processing for QuestionAnswering.", "citeRegEx": "France et al\\.,? 2003", "shortCiteRegEx": "France et al\\.", "year": 2003}, {"title": "The second dialog state tracking challenge", "author": ["M. Henderson", "B. Thomson", "J. Williams"], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, volume 263.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "If you ask nicely , I will answer : Semantic Search and Today\u2019s Search Engines", "author": ["T. Imielinski"], "venue": "Search.", "citeRegEx": "Imielinski,? 2009", "shortCiteRegEx": "Imielinski", "year": 2009}, {"title": "Multidomain dialog state tracking using recurrent neural networks", "author": ["N. Mrksic", "D.\u00d3. S\u00e9aghdha", "B. Thomson", "M. Gasic", "P. Su", "D. Vandyke", "T. Wen", "S.J. Young"], "venue": "CoRR, abs/1506.07190.", "citeRegEx": "Mrksic et al\\.,? 2015", "shortCiteRegEx": "Mrksic et al\\.", "year": 2015}, {"title": "The Dialog State Tracking Challenge", "author": ["J. Williams", "A. Raux", "D. Ramachandran", "A. Black"], "venue": "Sigdial, (August):404\u2013413.", "citeRegEx": "Williams et al\\.,? 2013", "shortCiteRegEx": "Williams et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Nowadays, dialog systems are usually designed for a single domain (Mrksic et al., 2015).", "startOffset": 66, "endOffset": 87}, {"referenceID": 8, "context": "It also allows to keep the dialog state in the form of slots (which usually correspond to columns in the table) and track it through the dialog using probabilistic belief tracking (Williams et al., 2013; Henderson et al., 2014).", "startOffset": 180, "endOffset": 227}, {"referenceID": 5, "context": "It also allows to keep the dialog state in the form of slots (which usually correspond to columns in the table) and track it through the dialog using probabilistic belief tracking (Williams et al., 2013; Henderson et al., 2014).", "startOffset": 180, "endOffset": 227}, {"referenceID": 0, "context": "Current approaches (Berant and Liang, 2015; Bordes et al., 2014) can only achieve around 50% accuracy on some question answering datasets.", "startOffset": 19, "endOffset": 64}, {"referenceID": 2, "context": "Current approaches (Berant and Liang, 2015; Bordes et al., 2014) can only achieve around 50% accuracy on some question answering datasets.", "startOffset": 19, "endOffset": 64}, {"referenceID": 6, "context": "This area is extensively studied for the purposes of information retrieval (Imielinski, 2009; France et al., 2003).", "startOffset": 75, "endOffset": 114}, {"referenceID": 4, "context": "This area is extensively studied for the purposes of information retrieval (Imielinski, 2009; France et al., 2003).", "startOffset": 75, "endOffset": 114}, {"referenceID": 3, "context": "offline policy \u2013 the policy is learned from some kind of offline training data (usually annotated) without interaction with system users (Bordes et al., 2015),", "startOffset": 137, "endOffset": 158}, {"referenceID": 1, "context": "Many public datasets for offline learning have been published (Berant et al., 2013; Bordes et al., 2015).", "startOffset": 62, "endOffset": 104}, {"referenceID": 3, "context": "Many public datasets for offline learning have been published (Berant et al., 2013; Bordes et al., 2015).", "startOffset": 62, "endOffset": 104}, {"referenceID": 3, "context": "A CF worker gets a task instructing them to use our chatlike interface to help the system with a question which is randomly selected from training examples of Simple questions (Bordes et al., 2015) dataset.", "startOffset": 176, "endOffset": 197}, {"referenceID": 3, "context": "Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions (Bordes et al., 2015) dataset.", "startOffset": 101, "endOffset": 122}], "year": 2016, "abstractText": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "creator": "LaTeX with hyperref package"}}}