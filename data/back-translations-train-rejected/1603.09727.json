{"id": "1603.09727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Neural Language Correction with Character-Based Attention", "abstract": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014 Shared Task. We further demonstrate that training the network on additional data with synthesized errors can improve performance.", "histories": [["v1", "Thu, 31 Mar 2016 19:16:54 GMT  (191kb,D)", "http://arxiv.org/abs/1603.09727v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ziang xie", "anand avati", "naveen arivazhagan", "dan jurafsky", "rew y ng"], "accepted": false, "id": "1603.09727"}, "pdf": {"name": "1603.09727.pdf", "metadata": {"source": "CRF", "title": "Neural Language Correction with Character-Based Attention", "authors": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng"], "emails": ["zxie@cs.stanford.edu,", "avati@cs.stanford.edu,", "naveen67@cs.stanford.edu,", "ang@cs.stanford.edu,", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Although tools such as spell checkers have been useful, detecting and correcting errors in the natural language, even at the sentence level, remains far from being solved. Newer methods, which take into account a wider range of error classes, often rely on language models to evaluate N-grams or statistical machine translation approaches (Ng et al., 2014). However, these methods do not flexibly address spelling, capitalization, and punctuation errors. As a motivating example, we consider the following incorrect sentence: \"I visited Tokyo in November 2003.\" Several errors in this sentence illustrate the difficulties of language proofreading."}, {"heading": "2 Model Architecture", "text": "Given an input set x that we want to associate with an output set y, we try to model P (y | x). Our model consists of an encoder and a decoder (Sutskever et al., 2014; Cho et al., 2014).The encoder forms the input set of a parent representation with a pyramid-shaped bi-directional RNN architecture similar to that of Chan et al. (2015).The decoder is also a recursive neural network that uses a content-based attention mechanism (Bahdanau et al., 2014) to take care of the encoded representation and generate the output set one character at a time."}, {"heading": "2.1 Character-Level Reasoning", "text": "Our neural network model works at the character level, both in the encoder and in the decoder. This is for two reasons, as our motivational example shows: First, we do not expect the input to be spell-checked and often find spelling errors in the sentences written by English learners in the records we are looking at. Second, neural MT models at the word level with a fixed vocabulary are ill-suited to deal with OOVs such as multi-digit numbers, emoticons and web addresses (Graves, 2013), although recent work has suggested solutions to this problem (Luong et al., 2014). Despite longer sequences in the character-based model, optimization does not seem to be a major problem, as the network often only has to copy characters from the source to the destination."}, {"heading": "2.2 Encoder Network", "text": "Considering the xt input vector, the forward, backward and combined activations of the jth hidden layer are calculated as follows: f (j) t = GRU (f (j) t \u2212 1, c (j \u2212 1) t), b (j) t = GRU (b (j) t + 1, c (j \u2212 1) t, h (j) t = f (j) t + b (j) twhere GRU denotes the gated recurrent unit function which, similar to long-term storage units (LSTMs), has improved the performance of RNNs (Cho et al., 2014; Hochreiter and Schmidhuber, 1997). Input from the previous layer input c (0) t = xt and c (j) t = tanh (W (j) pyr [h (j \u2212 1) 2t, h (j \u2212 1) 2t + 1] > + b (j) pyr) for j > 0."}, {"heading": "2.3 Decoder Network", "text": "The decoder network is a recursive neural network that uses recursive units with hidden M layers. After the last hidden layer, the network also conditions the encoded representation c by means of an attention mechanism. On the jten decoder layer, the hidden activations asd (j) t = GRU (d (j) t \u2212 1, d (j \u2212 1) t are calculated, with the output of the last hidden layer d (M) t then used as part of the content-based attention mechanism, similar to the one proposed by Bahdanau et al. (2014): utk = \u03c61 (d (M) t) > \u03c62 (ck) \u03b1tk = utk = sequentially j utjat = \u0445 j \u03b1tjcjat, where \u03c61 and \u04322 represent forward-directed affine transformations, followed by a tanh nonlinearity. The weighted sum of the encoded states is then combined with d (M) and further transformations."}, {"heading": "2.4 Attention and Pyramid Structure", "text": "In preliminary experiments, we found that an attention mechanism was critical to the model's ability to generate output data by characters that did not deviate from input. While character-based approaches to large-scale translation and language modeling tasks are not state-of-the-art, the decoder network only needs to copy input tokens in most time steps in this setting. Although character models reduce Softmax over the vocabulary at each step compared to word-level models, they also increase the total number of time steps of the RNN. The content-based attention mechanism then needs to take into account all the hidden states of the encoder c1: T at each step of the decoder. Therefore, we use a pyramid architecture that reduces computational complexity (as shown by Chan et al. (2015)). For longer batches, we observe more than 2 x the same number of parameters when using 400 hidden units per hidden layer (4 x hidden layers)."}, {"heading": "3 Decoding", "text": "While it is easier to integrate a language model by using it as a re-Ranker, here you combine the probabilities of the language model with the encoder decoder network via the beam search. This is possible because the attention mechanism in the decoder network prevents the decoded output from deviating too far from the initial set."}, {"heading": "3.1 Language Model", "text": "To model the distribution LM (y1: T) = T-T = 1 P (yt | y < t) (1), we build a smoothed 5-gram Kneser-Ney language model on a subset of the Common Crawl Repository1 collected in 2012 and 2013. After cutting, we obtain 2.2 billion N-grams. To create and query the model, we use the KenLM toolkit (Heafield et al., 2013)."}, {"heading": "3.2 Beam Search", "text": "To conclude, we use a beam search decoder that combines the neural network and the probability of the language model. Similar to Hannun et al. (2014), we classify the hypotheses on the beam based on the score (y1: k | x) = logPNN (y1: k | x) + \u03bb logPLM (y1: k), with the hyperparameter \u03bb determining how heavily the language model is weighted. To avoid penalizing longer hypotheses, we also normalize the score according to the number of words in the hypothesis | y |. Since decoding occurs at character level, the probability of PLM (\u00b7) of the language model is inserted only after a space or the end of a sentence has been found."}, {"heading": "3.3 Controlling Precision", "text": "For many error correction tasks, more emphasis is placed on precision than recall; for users, a wrong suggestion is worse than a missed error. To filter incorrect edits, we train a machining classifier to classify edits as correct or not. We use our decoder on uncorrected sentences from our training data to generate corrected sentences for candidates. 2 We repeat this alignment and edit the extraction process for the correct sentences by minimizing the distance between each candidate and uncorrected sentence. \"Good\" edits are defined as the intersection between the proposed and the \"bad\" edits that are defined as proposed machines.2 The proposed edits that are not included in the gold machining are abbreviated from the machining functions."}, {"heading": "4 Experiments", "text": "We conduct experiments with two sets of corrected sentences written by learners of English. The first is the Lang-8 Corpus, which contains incorrect sentences and their corrected versions collected in a forum for learners of social language (Tajiri et al., 2012). Due to the user-generated online environment, the Lang-8 data is noisy, with sentences often containing misspellings, emoticons and other loose punctuations. Example sentences are in Table 4.The other data set we are looking at comes from the CoNLL 2013 and 2014 Shared Tasks, which contain about 60K sentences of essays written by English learners with corrections and incorrect annotations. We primarily use the larger Lang-8 Corpus to train our network, and then rate on the CoNLL Shared Tasks."}, {"heading": "4.1 Training and Decoding Details", "text": "Our pyramid-shaped encoder has 3 layers, which reduces the sequence length by a factor of 4 in its output, and our RNN decoder also has 3 layers. Both the encoder and the decoder use a hidden size of 400 and gated recurrent units (GRUs), which, along with LSTMs (Hochreiter and Schmidhuber, 1997), have been proven to be easier to optimize and better preserved over many time steps than vanilla recurrent networks. Our vocabulary includes 98 characters: the print ASCII character set and special < sos >, < sos >, < unk > symbols that indicate the beginning, end of sentence and unknown symbols. To train the encoder decoder network, we use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0003, default decay rates of 1 and \u03b22, and a minibatch size of 12, which we classify with a uniform 0.15 train size."}, {"heading": "4.2 Noisy Data: Lang-8 Corpus", "text": "We use the train test split of the Lang8 Corpus of Learner English (Tajiri et al., 2012), which contains 100K and 1K entries with approximately 550K and 5K parallel records, respectively. We also separate 5K records from the training set to use as a separate development set for model and parameter selection. As we do not have gold annotations that distinguish adjacent records as separate records, we report on BLEU score3 only with the encoder decoder network and in combination with the ngram language model (Table 1). As there may be several ways to correct an error and some errors remain uncorrected, the baseline of using uncorrected records is harder to improve than it may first appear. As another baseline, we apply the top suggestions of 3Using case-sensitive multibleu.perl from Moses. A spell checker with standard configurations4."}, {"heading": "4.3 Main Results: CoNLL Shared Tasks", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves, \"he told the German Press Agency.\" We have to play by the rules, \"he said."}, {"heading": "5 Discussion", "text": "This year is the highest in the history of the country."}, {"heading": "6 Related Work", "text": "Our work builds primarily on previous work on the training of encoder decoders RNNs for machine translation (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).The attention mechanism that enables the decoder network to copy parts of the source set and handle long inputs is based on that of Bahdanau et al. (2014), and the overall network architecture described by Chan et al. (2015) Our model is also inspired by character models proposed by Graves (2013).Recent work has applied character models for machine translation and speech recognition, suggesting that it may be applicable that the problem of OOOOOOOOOVs et al al al al al., 2015; Chan et al al al al al al al al al al al al al al al al al."}, {"heading": "Acknowledgments", "text": "We would like to thank Kenneth Heafield, Jiwei Li, Thang Luong, Peng Qi and Anshul Samar for their helpful conversations, as well as the developers of Theano (Bergstra et al., 2010).Some of the GPUs used in this work were donated by NVIDIA Corporation. ZX was supported by an NDSEG Fellowship, which was partially funded by the DARPA MUSE Award FA8750-15-C-0242 AFRL / RIKF."}, {"heading": "In Association for Computational Linguistics (ACL)", "text": "[Mizumoto et al.2011] Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining revision log of language learning sns for automated Japanese error correction of second language learners. In International Joint Conference on Natural Language Processing (IJCNLP). [Mizumoto et al.2012] Tomoya Mizumoto et al.2012], Yuta Hayashibe, Mamoru Komachi, Masaaki Nagata, and Yuji Matsuomto. 2012. The effect of learner corpus size in grammatical error correction of ESL writings. In International Conference on Computational Linguistics. [Ng et al. 2013] Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-2013 shared task on grammatical error correction."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Better evaluation for grammatical error correction. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Dahlmeier", "Ng2012] Daniel Dahlmeier", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Dahlmeier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2012}, {"title": "Generating artificial errors for grammatical error correction", "author": ["Felice", "Yuan2014] Mariano Felice", "Zheng Yuan"], "venue": "In EACL", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Zheng Yuan", "istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar"], "venue": "Proceedings of the Eighteenth Conference on Computational", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Detecting errors in english article usage by non-native speakers", "author": ["Han et al.2006] Na-Rae Han", "Martin Chodorow", "Claudia Leakcock"], "venue": "Natural Language Engineering", "citeRegEx": "Han et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Han et al\\.", "year": 2006}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun et al.2014] Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos"], "venue": "arXiv preprint arXiv:1412.5567", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Cmu multi-engine machine translation for wmt 2010", "author": ["Heafield", "Lavie2010] Kenneth Heafield", "Alon Lavie"], "venue": "In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,", "citeRegEx": "Heafield et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2010}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Heafield et al.2013] K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "In ACLHLT,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The amu system in the conll-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation", "author": ["Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Postech grammatical error correction system in the conll-2014 shared task", "author": ["Lee", "Lee2014] Kyusong Lee", "Gary Geunbae Lee"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Maas et al.2015] Andrew L. Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Mining revision log of language learning sns for automated japanese error correction of second language learners", "author": ["Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Mizumoto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2011}, {"title": "The effect of learner corpus size in grammatical error correction of ESL writings", "author": ["Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsuomto"], "venue": null, "citeRegEx": "Mizumoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2012}, {"title": "The CoNLL-2013 shared task on grammatical error correction", "author": ["Ng et al.2013] Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Predicting good probabilities with supervised learning", "author": ["Niculescu-Mizil", "Rich Caruana"], "venue": "In International Conference on Machine learning (ICML)", "citeRegEx": "Niculescu.Mizil et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Generating confusion sets for contextsensitive error correction", "author": ["Rozovskaya", "Roth2010] Alla Rozovskaya", "Dan Roth"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language (EMNLP)", "citeRegEx": "Rozovskaya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2010}, {"title": "The ui system in the hoo 2012 shared task on error correction", "author": ["Mark Sammons", "Dan Roth"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP", "citeRegEx": "Rozovskaya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2012}, {"title": "The illinois-columbia system in the conll-2014 shared task", "author": ["Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural", "citeRegEx": "Rozovskaya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Systems combination for grammatical error correction", "author": ["Raymond Hendy Susanto"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Susanto.,? \\Q2014\\E", "shortCiteRegEx": "Susanto.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Tense and aspect error correction for esl learners using global context. In Association for Computational Linguistics: Short Papers", "author": ["Mamoru Komachi", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Tajiri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "types, such as article or preposition errors (Han et al., 2006; Rozovskaya and Roth, 2010).", "startOffset": 45, "endOffset": 90}, {"referenceID": 32, "context": "Our model consists of an encoder and a decoder (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 47, "endOffset": 89}, {"referenceID": 0, "context": "The decoder is also a recurrent neural network that uses a content-based attention mechanism (Bahdanau et al., 2014) to attend to the encoded representation and generate the output sentence one character at a time.", "startOffset": 93, "endOffset": 116}, {"referenceID": 1, "context": "The encoder maps the input sentence to a higher-level representation with a pyramidal bidirectional RNN architecture similar to that of Chan et al. (2015). The decoder is also a recurrent neural network that uses a content-based attention mechanism (Bahdanau et al.", "startOffset": 136, "endOffset": 155}, {"referenceID": 7, "context": "Second, wordlevel neural MT models with a fixed vocabulary are poorly suited to handle OOVs such as multidigit numbers, emoticons, and web addresses (Graves, 2013), though recent work has proposed workarounds for this problem (Luong et al.", "startOffset": 149, "endOffset": 163}, {"referenceID": 18, "context": "Second, wordlevel neural MT models with a fixed vocabulary are poorly suited to handle OOVs such as multidigit numbers, emoticons, and web addresses (Graves, 2013), though recent work has proposed workarounds for this problem (Luong et al., 2014).", "startOffset": 226, "endOffset": 246}, {"referenceID": 0, "context": "with the output of the final hidden layer d t then being used as part of the content-based attention mechanism similar to that proposed by Bahdanau et al. (2014):", "startOffset": 139, "endOffset": 162}, {"referenceID": 2, "context": "Thus we use a pyramid architecture, which reduces computational complexity (as shown by Chan et al. (2015)).", "startOffset": 88, "endOffset": 107}, {"referenceID": 11, "context": "To build and query the model, we use the KenLM toolkit (Heafield et al., 2013).", "startOffset": 55, "endOffset": 78}, {"referenceID": 9, "context": "Similar to Hannun et al. (2014), at step k, we rank the hypotheses on the beam using the score", "startOffset": 11, "endOffset": 32}, {"referenceID": 25, "context": "\u2022 embedding features: sum of 100 dimensional GloVe (Pennington et al., 2014) vectors of words in s and t, GloVe vectors of left and right context words in s.", "startOffset": 51, "endOffset": 76}, {"referenceID": 33, "context": "The first is the Lang-8 Corpus, which contains erroneous sentences and their corrected versions collected from a social language learner forum (Tajiri et al., 2012).", "startOffset": 143, "endOffset": 164}, {"referenceID": 30, "context": "We found that using dropout (Srivastava et al., 2014) at a rate of 0.", "startOffset": 28, "endOffset": 53}, {"referenceID": 26, "context": "15 on the non-recurrent connections (Pham et al., 2014) helped reduce perplexity.", "startOffset": 36, "endOffset": 55}, {"referenceID": 33, "context": "We use the train-test split provided by the Lang8 Corpus of Learner English (Tajiri et al., 2012), which contains 100K and 1K entries with about 550K and 5K parallel sentences, respectively.", "startOffset": 76, "endOffset": 97}, {"referenceID": 31, "context": "33 Susanto (2014) 53.", "startOffset": 3, "endOffset": 18}, {"referenceID": 23, "context": "Description For our second set of experiments we evaluate on the CoNLL 2014 Shared Task on Grammatical Error Correction (Ng et al., 2013; Ng et al., 2014).", "startOffset": 120, "endOffset": 154}, {"referenceID": 31, "context": "We compare to the top submissions in the 2014 Challenge as well as the method by Susanto (2014), which combines 3 of the weaker systems to achieve the state-of-the-art result.", "startOffset": 81, "endOffset": 96}, {"referenceID": 28, "context": "Following prior work, we additionally explore synthesizing additional sentences containing errors using the CoNLL 2014 training data (Felice and Yuan, 2014; Rozovskaya et al., 2012).", "startOffset": 133, "endOffset": 181}, {"referenceID": 20, "context": "To obtain sentence parses we use the Stanford CoreNLP Toolkit (Manning et al., 2014).", "startOffset": 62, "endOffset": 84}, {"referenceID": 26, "context": "Following prior work, we additionally explore synthesizing additional sentences containing errors using the CoNLL 2014 training data (Felice and Yuan, 2014; Rozovskaya et al., 2012). Our data augmentation procedure generates synthetic errors for two of the most common error types in the development set: article or determiner errors (ArtOrDet) and noun number errors (Nn). Similar to Felice and Yuan (2014), we first collect error distribution statistics from the CoNLL 2014 training data.", "startOffset": 157, "endOffset": 408}, {"referenceID": 31, "context": "On the CoNLL 2014 test set, which contains the full set of 28 error types, our method achieves a state-of-the-art result, beating all systems from the 2014 Challenge as well as a system combination method (Susanto, 2014).", "startOffset": 205, "endOffset": 220}, {"referenceID": 23, "context": "System descriptions for participating teams are given in Ng et al. (2014).", "startOffset": 57, "endOffset": 74}, {"referenceID": 26, "context": "The same phenomenon has been observed by Rozovskaya et al. (2012). Interestingly, the recall of other error types (see Ng et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 23, "context": "Interestingly, the recall of other error types (see Ng et al. (2014) for descriptions) decreases.", "startOffset": 52, "endOffset": 69}, {"referenceID": 32, "context": "Our work primarily builds on prior work on training encoder-decoder RNNs for machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 97, "endOffset": 171}, {"referenceID": 17, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 19, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 2, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al.", "startOffset": 186, "endOffset": 209}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al. (2015). Our model is also inspired by character-level models as proposed by Graves (2013).", "startOffset": 186, "endOffset": 296}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al. (2015). Our model is also inspired by character-level models as proposed by Graves (2013). More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al.", "startOffset": 186, "endOffset": 379}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014).", "startOffset": 205, "endOffset": 266}, {"referenceID": 29, "context": "Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014).", "startOffset": 167, "endOffset": 211}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.", "startOffset": 206, "endOffset": 301}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.5-score of 37.33 in that year\u2019s challenge using a combination of rule-based, language-model ranking, and statistical machine translation techniques. Many other teams used a language model for re-ranking hypotheses as well. Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014). The rule-based systems often focus on only a subset of the error types. The previous state of the art was achieved by Susanto (2014) using the system combination method proposed by Heafield and Lavie (2010) to combine three weaker systems.", "startOffset": 206, "endOffset": 891}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.5-score of 37.33 in that year\u2019s challenge using a combination of rule-based, language-model ranking, and statistical machine translation techniques. Many other teams used a language model for re-ranking hypotheses as well. Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014). The rule-based systems often focus on only a subset of the error types. The previous state of the art was achieved by Susanto (2014) using the system combination method proposed by Heafield and Lavie (2010) to combine three weaker systems.", "startOffset": 206, "endOffset": 965}, {"referenceID": 21, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 22, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 23, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 28, "context": "Prior work has also proposed data augmentation for the language correction task (Felice and Yuan, 2014; Rozovskaya et al., 2012).", "startOffset": 80, "endOffset": 128}, {"referenceID": 1, "context": "We additionally thank the developers of Theano (Bergstra et al., 2010).", "startOffset": 47, "endOffset": 70}], "year": 2016, "abstractText": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art F0.5-score on the CoNLL 2014 Shared Task. We further illustrate that training the network on additional data with synthesized errors can improve performance.", "creator": "LaTeX with hyperref package"}}}