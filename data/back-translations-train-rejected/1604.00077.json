{"id": "1604.00077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection", "abstract": "Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.", "histories": [["v1", "Thu, 31 Mar 2016 23:17:46 GMT  (4037kb,D)", "http://arxiv.org/abs/1604.00077v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sheng-syun shen", "hung-yi lee"], "accepted": false, "id": "1604.00077"}, "pdf": {"name": "1604.00077.pdf", "metadata": {"source": "CRF", "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection", "authors": ["Sheng-syun Shen", "Hung-yi Lee"], "emails": ["r03942071@ntu.edu.tw,", "hungyilee@ntu.edu.tw"], "sections": [{"heading": null, "text": "The consequence is that most people are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2. Neural Attention Model for Sequence Classification", "text": "The overall structure of the proposed method is shown in Figure 1. The input of the model would be presented as a dense sequence vector OT described in Section 2.1. An attention mechanism is then applied to the sequence vector to extract related information from the input sequence in Section 2.2. In Section 2.3, the model will predict the target based on the selected feature vectors."}, {"heading": "2.1. Sequence Representation", "text": "We use recursive neural networks (RNN) for encoding. RNNs are able to process sequence information over time, so in recent years they have performed excellently in natural language understanding tasks [24-26]. We select long-term short-term memory networks (LSTM), a type of recursive neural networks with a more complex computing unit, toar Xiv: 160 4.00 077v 1 [cs.C L] 31 Mar 201 6processes inputs sequentially. A brief introduction to LSTMs is given in [22, 23]. In the upper part of Figure 1, we demonstrate the encoding method for transforming fixed-length input sequences into fixed-length vector representation. For example, the set x = (x1, x2,. xT) denotes the input sequence, with T being the sequence length. Each element in x represents a fixed-length vector."}, {"heading": "2.2. Attention Mechanism", "text": "If the input sequence x is long, the summary vector OT will probably contain noisy information from many irrelevant feature vectors Vi, so we apply an attention mechanism to select only relevant frames within the entire sequence. Procedures are shown in the lower part of Figure 1. There is also an embedding layer to transform input sequences into dense vectors, and all parameters in the embedding layer are shared with the previous one. We then calculate the cosinal similarity between the sequence vector OT and the word embedding set V: ei = OT Vi, (1), where the cosinine similarity between two vectors is designated. As a result, we have a list of scores e = (e1, e2,., eT). Attention weights \u03b1 = (\u03b11, \u03b12,.) come from the normalized score list e."}, {"heading": "2.3. Target Selection", "text": "The right part of Figure 1 illustrates the methods for selecting targets. We have weighted all characteristic vectors as \u2211 \u03b1iVi and sent them to a completely connected layer. Normally, the neurons in this layer are activated by non-linear functions."}, {"heading": "3. Experiments", "text": "In this section, we have performed two sequence classification tasks: In Section 3.1, we describe the definition of the recognition of dialog files and also demonstrate the experimental results; in Section 3.2, we present the application of the proposed methodology to key term extraction tasks; the role of the attention mechanism during the classification process is discussed in Section 3.3, and we also show the visualization results."}, {"heading": "3.1. Dialogue Act Detection", "text": "Recognizing the Dialogue Act (DA) [15-17] is about categorizing the intention behind the speaker's move in conversations, and recognizing a speaker's action can help to understand the entire dialogue. This predictive task is still challenging because there are several different ways to formulate an intention. In this work, DAs are labeled with one of several tags. For example, the tag < OFFER > is related to the situation that someone has ordered the partner to perform actions, for example: \"You have to give me your ideas, and then I have to see if that would sell on the market.\""}, {"heading": "3.1.1. Experimental setup", "text": "We conducted experiments on the Switchboard Dialog Act (SwDA) Corpus [27], a corpus of telephone conversations on selected topics. It consists of approximately 2,500 conversations by 500 speakers from the U.S. The conversations in the corpus are labeled with 43 unique dialogue act tags and divided into 1,115 train calls and 19 test calls. The training / test corpus comprises 213,543 and 4,514 statements with an average length of about 8 words, respectively."}, {"heading": "3.1.2. Baselines", "text": "Silva et al. [28] chose sentence diagrams as the input feature vector and trained the SVM model. We extracted one-of-N encoding characteristics for each word in the dataset and aggregated them for each training example. To reduce the number of dimensions, we set minimum word counts to 5. The radial base function (RBF) [29] was also used. Multiple-layer perceptron: The work presented by Ries et. al [30] is the first approach in which artificial neural networks (ANN) are imported for the detection of dialogue files. We also extracted unigram characteristics as model input for experiments. We trained an MLP model with 3 hidden layers. Each hidden layer has 512 neurons. The relay activation function was applied to each hidden layer, and we set the word input as 20. Attention was placed on the superordinate 12M layer."}, {"heading": "3.1.3. Experimental results", "text": "The LSTM part of the proposed model is the same as the original LSTM part, which was briefly illustrated in the previous subsection, and the hyperparameters for the model training were also the same. As explained in the previous paper [31], context information from previous statements can be helpful in predicting the dialog act. Therefore, we have also appended n earlier statements to the statement that was classified, and n was set to be in the experiments 3. The results will be in Table 1. Rows (a) to (d) are the basic results, and the results of the proposed approaches are in rows (e) to (h). It is clear that the LSTM networks have already surpassed the other baselines (rows (c), (b), because the LSTM networks have a better ability to handle sequence information than to handle multi-layered perceptions and have precepts to support the proposed ones (beyond the STM)."}, {"heading": "3.2. Key Term Extraction", "text": "The purpose of key term extraction [18-21] is to automatically extract relevant terms from a given document. Key terms may possibly describe the core concept or summary of a document, which can help the user efficiently understand, organize, and extract important information. These terms are usually labeled manually by people by knowledge and domain knowledge, so that automatic key term extraction is not an easy task. Key term extraction can be considered a sequence classification problem [18]. Model input is a document, while the model selects some terms as key terms from a series of candidates. Each term in the series of candidate terms is considered a class, and the documents containing the same key terms belong to the same class. In our task, there is a chance that some terms do not exist in the document, but they represent the core terms of the document. These terms are also considered key terms that belong to the same key terms, which makes this document more difficult than a task."}, {"heading": "3.2.1. Experimental setup", "text": "We have collected the data from the Stack Overflow1 website, which serves as a platform for users to ask questions and answer them. As users of Stack Overflow ask questions on the forum, they are asked to mark 2-6 key terms for each post. The record we have collected includes a total of 290,000 examples (250,000 for training and 40,000 for testing), and there are about 24,000 types of key terms. Each example contains one post and 2-6 key terms, and the average length of the article is about 120 words. The data set collected is available for downloading.2In practice, to reduce training complexity, we have selected only the 1000 most common key terms in the training set as candidates. These top 1,000 candidates cover over 76% of the key terms in the training set, so we can still expect reasonable outcomes.1 http: / / stackoverflow.com / 2http: / / speech.ee.ntu.e.w / stackoverflow / stackoverflow _"}, {"heading": "3.2.2. Baselines", "text": "As basic models, we have implemented Multilayer Perceptrons (MLP) and Long-Term Memory (LSTM) networks, which have already been described in Section 3.1.2.Tf-idf Sorting. \"Tf-idf\" is the abbreviation for Term Frequency-inverse Document Frequency. This is a numerical statistic intended to reflect how important a word is for a document in a collection or corpus. A brief introduction on how Tf-idf Sorting extracts key terms can be found in [32]. We calculated the tf-idf values of a number of key terms for candidates according to the data set, and these candidates were sorted according to their values. Afterwards, we reported the ranking list for evaluation."}, {"heading": "3.2.3. Experimental results", "text": "To study the predictive result, we chose MAP and P @ R as evaluation methods. The MAP score for a number of documents is the mean of the average precision results for each document. P @ R is defined as the precision after R elements have been selected by the system, where R is also the total number of assessed relevant results for the given input factors. Precision is defined as the percentage of returned results that really correspond to the principle of truth. Experimental results are demonstrated in Table 2. Row (a), using the oracle value as a reference. As we have selected only 1,000 common key terms as candidates from the training set, we cannot achieve 100% accurate performance. The score of the baseline approaches we use is in the rows (b) to (d) and rows (e), (f) are the performance of the proposed neural maintenance model."}, {"heading": "3.3. Visualization and Analysis", "text": "Figure 2 illustrates how the attention mechanism works in sequence classification. The upper row is used to detect dialog files and the lower row is used for key word extraction. The darker the color, the higher the weights. We chose the smoothing mechanism for visualization only because of its better performance. Based on this figure, we found that attention weights are able to reduce problems of sentence disfluidity and filter out most unimportant elements such as function words."}, {"heading": "4. Conclusions", "text": "The main difficulty is that if the input sequence is long, the noisy or irrelevant part may affect the classification performance. The proposed model can reduce the influences because it is able to highlight important parts of the entire sequence. [1] In the experiments, the neural attention model can achieve 72.6% accuracy for dialogue activity and 50.5% MAP score for key terms extraction task, which shows discreet improvements compared to the other approaches. [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. New-ral machine translation by jointly learning to align and translate. arXiv preprint arXiv: 1409.0473, 2014. [2] Jeffrey L Elman. Finding structure in time. Cognitive, 14-1990."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "End-toend memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network encoder with attention for community question answering, 2016", "author": ["Wei-Ning Hsu", "Yu Zhang", "James Glass"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification", "author": ["Najim Dehak", "Reda Dehak", "Patrick Kenny", "Niko Brummer", "Pierre Ouellet", "Pierre Dumouchel"], "venue": "In INTERSPEECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "The INTER- SPEECH 2009 emotion challenge", "author": ["Bjorn Schuller", "Stefan Steidl", "Anton Batliner"], "venue": "In INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples. Audio, Speech, and Language Processing", "author": ["Hung-Yi Lee", "Lin-Shan Lee"], "venue": "IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A hybrid HMM/DNN approach to keyword spotting of short words", "author": ["I.-F. Chen", "C.-H. Lee"], "venue": "In INTERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Exploiting discriminative point process models for spoken term detection", "author": ["A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas"], "venue": "In INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dialog act classification using n-gram algorithms", "author": ["Max M Louwerse", "Scott A Crossley"], "venue": "In FLAIRS Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "An affect-enriched dialogue act classification model for task-oriented dialogue", "author": ["Kristy Elizabeth Boyer", "Joseph F Grafsgaard", "Eun Young Ha", "Robert Phillips", "James C Lester"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Dialog act tagging with support vector machines and hidden markov models", "author": ["Dinoj Surendran", "Gina-Anne Levow"], "venue": "In INTER- SPEECH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A new approach to keyphrase extraction using neural networks", "author": ["Kamal Sarkar", "Mita Nasipuri", "Suranjan Ghose"], "venue": "arXiv preprint arXiv:1004.3274,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features", "author": ["Yun-Nung Chen", "Yu Huang", "Sheng-Yi Kong", "Lin-Shan Lee"], "venue": "In Spoken Language Technology Workshop (SLT), 2010 IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A simple but powerful automatic term extraction method", "author": ["Hiroshi Nakagawa", "Tatsunori Mori"], "venue": "In COLING-02 on COMPUT- ERM 2002: second international workshop on computational terminology-Volume", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "An empirical investigation of sparse log-linear models for improved dialogue act classification", "author": ["Yun-Nung Chen", "Wei Yu Wang", "Alexander I Rudnicky"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Biasca. Switchboard swbd-damsl shallow-discourse-function annotation coders manual", "author": ["Dan Jurafsky", "Elizabeth Shriberg", "Debra"], "venue": "Institute of Cognitive Science Technical Report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Yin-Wen Chang", "Cho-Jui Hsieh", "Kai-Wei Chang", "Michael Ringgaard", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Hmm and neural network based speech act detection", "author": ["Klaus Ries"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "The influence of context on dialogue act recognition", "author": ["Eug\u00e9nio Ribeiro", "Ricardo Ribeiro", "David Martins de Matos"], "venue": "arXiv preprint arXiv:1506.00839,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] in the task of machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "They proposed an recurrent neural network (RNN) [2,3] encoder-decoder model for end-to-end translation, and this mechanism is intuitively designed in order to take care about the positions of input elements according to previous output result.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "They proposed an recurrent neural network (RNN) [2,3] encoder-decoder model for end-to-end translation, and this mechanism is intuitively designed in order to take care about the positions of input elements according to previous output result.", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "[4] then proposed attention-based models for speech recognition, which are claimed to be robust to long inputs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and Huijuan Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] also demonstrated how attention mechanism works while reading a picture.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 47, "endOffset": 52}, {"referenceID": 7, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 47, "endOffset": 52}, {"referenceID": 8, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 11, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 12, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 13, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 14, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 15, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 16, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 17, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 18, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 19, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 6, "context": "Attention-mechanism shows the potential of automatically ignoring the unimportant parts in the entire input sequence and highlighting the important parts [7\u20139].", "startOffset": 154, "endOffset": 159}, {"referenceID": 7, "context": "Attention-mechanism shows the potential of automatically ignoring the unimportant parts in the entire input sequence and highlighting the important parts [7\u20139].", "startOffset": 154, "endOffset": 159}, {"referenceID": 20, "context": "In this paper, we present a novel attention-mechanism long short-term memory (LSTM) [22, 23] network architecture for sequence classification, in which the LSTM network reads the entire input, attention-mechanism highlights the important elements, and the sequence classes are predicted by the highlighted parts.", "startOffset": 84, "endOffset": 92}, {"referenceID": 21, "context": "In this paper, we present a novel attention-mechanism long short-term memory (LSTM) [22, 23] network architecture for sequence classification, in which the LSTM network reads the entire input, attention-mechanism highlights the important elements, and the sequence classes are predicted by the highlighted parts.", "startOffset": 84, "endOffset": 92}, {"referenceID": 16, "context": "We further formulate the key term extraction as sequence classification task [18], and apply the proposed model.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 23, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 24, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 20, "context": "A brief introduction of LSTMs can be found in [22, 23].", "startOffset": 46, "endOffset": 54}, {"referenceID": 21, "context": "A brief introduction of LSTMs can be found in [22, 23].", "startOffset": 46, "endOffset": 54}, {"referenceID": 3, "context": "in [4]: Sharpening: The score list is normalized using softmax activation function:", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 4, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 5, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 7, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 13, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 14, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 15, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 25, "context": "We conducted experiments on Switchboard Dialog Act (SwDA) Corpus [27], which is a corpus of telephone conversations on selected topics.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "[28] chose sentence unigrams as input feature vector, and trained the SVM model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The Radial basis function (RBF) [29] kernel was also applied.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "al [30] is the first approach that importing artificial neural networks (ANN) for dialogue act detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "As the previous work stated [31], context information from previous utterances may help for the dialogue act prediction.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 17, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 18, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 19, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 16, "context": "Key term extraction can be regarded as a sequence classification problem [18].", "startOffset": 73, "endOffset": 77}], "year": 2016, "abstractText": "Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence labeling tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.", "creator": "LaTeX with hyperref package"}}}