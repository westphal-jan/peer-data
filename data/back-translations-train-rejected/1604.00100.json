{"id": "1604.00100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "A Compositional Approach to Language Modeling", "abstract": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models.", "histories": [["v1", "Fri, 1 Apr 2016 01:51:34 GMT  (300kb,D)", "http://arxiv.org/abs/1604.00100v1", "submitted to ACL 2016"]], "COMMENTS": "submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kushal arora", "anand rangarajan"], "accepted": false, "id": "1604.00100"}, "pdf": {"name": "1604.00100.pdf", "metadata": {"source": "CRF", "title": "A Compositional Approach to Language Modeling", "authors": ["Kushal Arora", "Anand Rangarajan"], "emails": ["karora@cise.ufl.edu", "anand@cise.ufl.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Compositional View of an N-gram model", "text": "Consider a sequence of words wn1. A linear chain model would factorize the probability of this proposition p (wn1) as a product of conditional probabilities p (wi | hi), which result in the following factorization: p (wn1) = n \u00b2 i = 1 p (wi | hi). (1) All the models discussed in the previous section differ in the way history or context hi is presented. (2) If we shift the probability space in sequences of words, the same factorization can be written in Equation (2) as follows: p (wn1,.., w i 1,., w 2 1, wn,...., wi \u2212 n = sequence of words (wi1 | wi \u2212 11, wi) p (wi)."}, {"heading": "3 The Compositional Language Model", "text": "In this section we provide the framework for performing the marginalization over all possible trees. Let W be the sentence and T (W) the sentence over all composition trees for sentence W. The probability of the sentence p (W) can then be written in relation to the common probability over the sentence and composition structure asp (W) = \u2211 T (W) p (W | t) p (t) (5) check (5), we see that we have two problems to solve: i) enumerating and creating probability distributions over trees p (t) and ii) modelling the probability of sentences conditioned on composition trees p (W | t). Probabilistic context-free grammars (PCFGs) fit perfectly for the first application case. We define a PCFG as a quintuple of probability distributions over trees p and ii) modelling the probability of sentences conditioned on composition trees p (W | t)."}, {"heading": "3.1 The Composition Tree Representation", "text": "We now turn our attention to the problem of modelling the probability of the proposition to the proposition tree (w51 | t) = p (w51 | w21, w53) p (w21 | w1, w2) p (w53 | w3w54) p (w54 | w4, w5) p (w1) p (w2) p (w4) p (w5)). (8) We now try to represent any tree t in such a way that it can be factored easily as in (8). We do this by presenting the proposition tree t for a sentence W as a set of composition rules and sheet nodes. Let us call this proposition Rt (W) wp (W) wp wp, the rule set for the proposition w51 with the probability of composition t, Rt (wist) w (51) w (51) w (51) p (1) p), wp wp (4), wp (5) wp, wp wp wp wp."}, {"heading": "3.2 Computing the Sentence Probability", "text": "Using the definition of p (t) of (7) and the definition of p (W) of (11), we write the common probability p (W, t) = c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (c), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W, c, c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W), c (W, c, c (W), c (W), c (W, c (W), c (W), c (W, c (W), c (W, c (W), c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c (W), c (W, c, c (W), c (W, c (W, c), c (W (W), c, c (W (W), c (W (W, c, c), c (W, c (W), c (W (W), c (W (W, c), c (W (W), c (W (W), c), c, c, c (W (W (W (W), c"}, {"heading": "3.3 Modeling the compositional probability", "text": "In a first step, we project each word into a continuous space using X, a d \u00b7 | V | embedding matrix, to obtain a continuous space vector xi = X [i] that matches the word wi.A non-terminal parent node pa consists of the child nodes c1 and c2 aspa = f (W [c1 c2]) (20), where W is a parameter with the dimensions d \u00b7 2d and f, which is a nonlinear function like Tanh or Sigmoid. The probability distribution p (r) via rule r-Rt (W) is modelled as Gibbs distribution p (r) = 1Z exp {\u2212 E (r)} (21), where E (r) is the energy for a compositional rule or leaf node and as an estimated value asE (r) =."}, {"heading": "4 Training", "text": "Let us write D as a set of training sets. We can write the probability function asL (\u03b1; D) = \u03b5Wd-D p (Wd; \u03b1). (23) This leads to the negative log probability objective functionEML (\u03b1; D) = \u2212 \u2211 Wd-D ln (p (Wd; \u03b1)). (24) Replacing the definition of p (W; \u03b1) from (5) and p (W, t; \u03b1) from (14) in (24), we obtain EML (\u03b1; D) = \u2212 \u2211 Wd-D ln \u2211 t-T (Wd), instead the formulation in Equation (25) is very similar to the standard expectation maximization (EM), whereby the compositional tree t can be regarded as a latent variable."}, {"heading": "4.1 Expectation Step", "text": "In the E step we calculate the expected probability Q (\u03b1; \u03b1old, \u03b2) as a consequence (\u03b2 \u03b2). Q (\u03b1; \u03b1old, W) = \u2212 \u2211 t (W) p (t | W; \u03b1old) ln (p (t, W; \u03b1)). (26) k wji became the subsequence to its leaf substitution p (t, W) from (14) to (26), we can further simplify the expression by adding sums about the trees within the following expressionQ (\u03b1; W) = \u2212 ji (W) p (t | W) p (t | W) p (t) p (t) p (t) p (r) p (r) p) p (r) p) s (n). (27) We can further simplify the expression by adding sums within the following expressionQ (\u03b1; W) = \u2212 1 p (W) p (r) p (r) r (r) ln (r) ln (28)."}, {"heading": "4.2 Minimization Step", "text": "In the M step, the goal is to minimize Q (\u03b1; \u03b1old) in order to obtain the definition of what is there and the value of p (r; \u03b1) from Equation (21) and differentiation of Q (\u03b1; \u03b1old) w.r.t. (33) Replacing the definition of p (r; \u03b1) and the value of p (r; \u03b1) from Equation (21) and differentiation of Q (\u03b1; \u03b1old) w.r.t. to \u03b1, one gets Z Z Z Z (r; u, W, X).R (r).R (r).Z (r; \u03b1).Z).Z.Z.Z.Z.Z.Z.Z."}, {"heading": "4.3 Phrasal Representation", "text": "One of the inherent assumptions we made while using the Inside-Outside algorithm was that each span has multiple value. (This is an important assumption, because the states in the Inside and Outside algorithms are that span. \u2192 Distributed representation breaks this assumption. To understand this, we consider a sentence with three words w31. Figure 4 shows possible derivatives of this sentence. Embedding for p {1 {23} and p {12} 3} are different in that they follow different composition paths, although both represent the same phrase w31. A generalization of this sentence or a phrase of length 3 or greater would suffer from the multiple representation problem arising from multiple possible composition paths. To understand why this is a problem, let us examine the Inside algorithm recursion. Dynamic programming works while calculating \u03c0 (A, wji) is the probability, because we assume that there is only one value for each of the two, and vice versa."}, {"heading": "5 Evaluation", "text": "The most commonly used metric for benchmarking language models is helplessness. Despite its widespread use, due to its assumption of the word level model and its dependence on exact probabilities, it cannot evaluate sentence level models like ours. A recently proposed discriminatory metric contrast entropy (Arora and Rangarajan, 2016) fits perfectly with our assessment application case. The aim of this new metric is to evaluate the ability of the model between test sentences and their distorted version.Contrastive entropy, HC, is defined as the difference between the entropy of the test set Wn and the entropy of the distorted version of the test sentences W-n i.HC (D; d) = 1N-Wn-D H (W-n; d) \u2212 H (Wn) \u2212 H (Wn) (42) HereD is the test set, d the distortion N and the number of sentences or the word level invasive (HC), or no word level invasive (HC)."}, {"heading": "5.1 Results", "text": "We use the sample data set provided with the RNLM toolkit (Mikolov et al., 2011b) for evaluation purposes. The data set is divided into training, test and validation sets of 10000, 1000 and 1000 sets, respectively. The training set contains 3720 different words and the test set contains 206 vocabulary words. All reported values are averaged here over 10 rounds. Figure 5 shows the monotonous increase in contrast perplexity as the distortion level of the test increases. This is in line with the hypothesis that the discriminatory ability of a language model should increase with the distortion levels of the test set. Table 1 compares our language model with standard language modeling techniques. The n-gram language models here use the smoothing of Kneser Ney (KN5) and were generated and evaluated with the SRILM toolkit (SRILM-Toolkit 2002)."}, {"heading": "6 Conclusion", "text": "In this paper, we challenged the linear chain assumption of traditional language models by developing a model that uses the compositional structure endowed with context-free grammars. We formulated it as a marginalization problem beyond the common probability of sentences and structure and reduced it to one of modelling compositional rule probabilities p (r). To the best of our knowledge, this is the first model that goes beyond linear chain assumption and uses the compositional structure to model language. It is important to note that this compositional framework is much more general and the way in which this paper model p (r) is only one of many possible ways to do so. Furthermore, this paper proposed a compositional framework that embeds phrases recursively in a latent space and then builds a distribution over it. This provides us with a distributional language representation framework that, if properly trained, can be used as a basis for various voice processing tasks such as speech processing and PNER."}], "references": [{"title": "Contrastive entropy: A new evaluation metric for unnormalized language models. arXiv preprint arXiv:1601.00248", "author": ["Arora", "Rangarajan2016] Kushal Arora", "Anand Rangarajan"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Distributional clustering of words for text classification", "author": ["Baker", "McCallum1998] L Douglas Baker", "Andrew Kachites McCallum"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in in-", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "JeanLuc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Immediatehead parsing for language models", "author": ["Eugene Charniak"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak.,? \\Q2001\\E", "shortCiteRegEx": "Charniak.", "year": 2001}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech & Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Structure and performance of a dependency language model", "author": ["David Engle", "Frederick Jelinek", "Victor Jimenez", "Sanjeev Khudanpur", "Lidia Mangu", "Harry Printz", "Eric Ristad", "Ronald Rosenfeld", "Andreas Stolcke"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 1997}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A bit of progress in language modeling", "author": ["Joshua T Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "De Mori1990] Roland Kuhn", "Renato De Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "The estimation of stochastic context-free grammars using the inside-outside algorithm", "author": ["Lari", "Young1990] Karim Lari", "Steve J Young"], "venue": "Computer speech & language,", "citeRegEx": "Lari et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Lari et al\\.", "year": 1990}, {"title": "Trigger-based language models: A maximum entropy approach", "author": ["Lau et al.1993] Raymond Lau", "Ronald Rosenfeld", "Salim Roukos"], "venue": null, "citeRegEx": "Lau et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Lau et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "RNNLM-recurrent neural network language modeling toolkit", "author": ["Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Distributional clustering of english words", "author": ["Naftali Tishby", "Lillian Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Srilm-an extensible language modeling toolkit. In INTERSPEECH", "author": ["Andreas Stolcke"], "venue": null, "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "A good overview of various smoothing techniques and their relative performance on language modeling tasks can be found in (Goodman, 2001).", "startOffset": 122, "endOffset": 137}, {"referenceID": 3, "context": "Class based models (Brown et al., 1992; Baker and McCallum, 1998; Pereira et al., 1993) try to solve the generalization issue by deterministically or probabilistically mapping words to one or multiple classes based on manually designed or probabilistic criteria.", "startOffset": 19, "endOffset": 87}, {"referenceID": 17, "context": "Class based models (Brown et al., 1992; Baker and McCallum, 1998; Pereira et al., 1993) try to solve the generalization issue by deterministically or probabilistically mapping words to one or multiple classes based on manually designed or probabilistic criteria.", "startOffset": 19, "endOffset": 87}, {"referenceID": 11, "context": "The issue of longer context dependencies has been addressed using various approximations such as cache models (Kuhn and De Mori, 1990), trigger models (Lau et al., 1993) and structured language models (Charniak, 2001; Chelba et al.", "startOffset": 151, "endOffset": 169}, {"referenceID": 4, "context": ", 1993) and structured language models (Charniak, 2001; Chelba et al., 1997; Chelba and Jelinek, 2000).", "startOffset": 39, "endOffset": 102}, {"referenceID": 6, "context": ", 1993) and structured language models (Charniak, 2001; Chelba et al., 1997; Chelba and Jelinek, 2000).", "startOffset": 39, "endOffset": 102}, {"referenceID": 2, "context": "ward neural network based models (Bengio et al., 2006; Mnih and Hinton, 2009; Morin and Bengio, 2005) embed the concatenated n-gram history in this latent space and then use a softmax layer over these embeddings to predict the next word.", "startOffset": 33, "endOffset": 101}, {"referenceID": 12, "context": "Recurrent neural network based models (Mikolov et al., 2011a; Mikolov et al., 2010) attempt to address this by recursively embedding history in the latent space, predicting the next word based on it and then updating the history with the word.", "startOffset": 38, "endOffset": 83}, {"referenceID": 19, "context": "Instead of approximating the marginalization using the n-best trees as in (Socher et al., 2013), we restore this independence assumption by averaging over phrasal representations leading to a single phrase representation.", "startOffset": 74, "endOffset": 95}, {"referenceID": 18, "context": "The approach we follow here is similar to the one taken in (Socher et al., 2010).", "startOffset": 59, "endOffset": 80}, {"referenceID": 20, "context": "The n-gram language models here use Kneser Ney (KN5) smoothing and were generated and evaluated using the SRILM toolkit (Stolcke, 2002).", "startOffset": 120, "endOffset": 135}, {"referenceID": 7, "context": "The compositional language models (CLM) in Table 1 have latent space size of 25 and 50 and were trained using Adagrad (Duchi et al., 2011) with an initial learning rate of 1 and `2 regularization coefficient of 0.", "startOffset": 118, "endOffset": 138}], "year": 2016, "abstractText": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models.", "creator": "LaTeX with hyperref package"}}}