{"id": "1604.00117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "abstract": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.", "histories": [["v1", "Fri, 1 Apr 2016 03:24:32 GMT  (138kb,D)", "http://arxiv.org/abs/1604.00117v1", null], ["v2", "Wed, 10 Aug 2016 02:53:00 GMT  (174kb,D)", "http://arxiv.org/abs/1604.00117v2", "Interspeech 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron jaech", "larry heck", "mari ostendorf"], "accepted": false, "id": "1604.00117"}, "pdf": {"name": "1604.00117.pdf", "metadata": {"source": "CRF", "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "authors": ["Aaron Jaech", "Larry Heck", "Mari Ostendorf"], "emails": ["ajaech@uw.edu,", "larryheck@google.com,", "ostendor@uw.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do, and they are able to determine for themselves what they want to do."}, {"heading": "2. Model", "text": "Our model has a word embedding layer, followed by a bi-directional LSTM (bi-LSTM), and a softmax output layer. The bi-LSTM allows the model to use information from the right and left context of each word when making predictions. We choose this architecture because similar models have been used in previous work on filling slots and have achieved good results [16, 11]. The LSTM gates are used in the same way as by Sak et al. including the use of the linear projection layer on the output of the LSTM [20]. The purpose of the projection layer is to produce a model with fewer parameters without reducing the number of LSTM memory cells. For the multi-task model, the word embedding and the biLSTM parameters are distributed across tasks, but each task has its own Softmax layer. This means that the multi-task model has half a million parameters, only a few of them are unique for each 95% task."}, {"heading": "3. Data", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules."}, {"heading": "4. Experiments", "text": "The section describes two types of experiments: the first is to test the effectiveness of the multi-task model and the second is to test the generalisability of the open vocabulary model. The scenario is that we already have n \u2212 1 models and we want to find out how much data will be needed to build a model for an additional application."}, {"heading": "4.1. Training and Model Configuration Details", "text": "The data is split to use 30% for training, with 70% being used for test data. The reason that much of the data is used for testing is that in the second experiment, the results are reported separately for sentences that contain vocabulary and a large amount of data is needed to obtain a sufficient sample size. Hyperparameter tuning is challenging when working in a scenario with limited resources. If there is barely enough data to train the model, no one can save for a validation set. We used data from the United App for Hyperparameter Tuning, as it is the largest and assumes that the hyperparameter settings are applied to the others. Training is done using stochastic derivation of gradients with mini batches of 25 sets. The initial learning rate is 0.3 and is decompressed to 98% of its value per 100 minibatches. For the multi-task model, training is done by dividing between the individual tasks with mini-dimension selection."}, {"heading": "4.2. Multi-task Model Experiments", "text": "We compare a single task model with the multi-task model for different amounts of training data. In the multi-task model, the full amount of data is used for n \u2212 1 apps and the amount of data may only vary for the n \u2212 th application. In these experiments, the traditional word embedding with a closed vocabulary is used. As the data for the United App is larger than the other three apps combined, they are used as anchors for the multi-task model. The other three apps alternate in the position of the n \u2212 th app. Data usage for the n \u2212 th app is different, while the other n \u2212 1 apps in each experiment use the full amount of available training data. The full amount of training data is different for each app. The data used for the n \u2212 th app is 200, 400 or 800 sets or all available training data depending on the experiment. The test set remains fixed for all experiments, even as part of the training data that the multi-task model uses to compare the low resource sets, we show in each of the different scenarios in the figure below."}, {"heading": "4.3. Open Vocabulary Model Experiments", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5. Conclusions", "text": "In summary, we note that the use of a multi-task model with shared embedding results in a large reduction in the minimum amount of data required to form a slot fill model for a new app, resulting in cost savings in the use of slot fill models for new applications. Combining the multi-task model with open vocabulary embedding increases the generalization of the model, especially if OOVs are included in the sentence. These two contributions allow for scalable slot fill models. For future work, there are some improvements that could be made to the model, such as the addition of an attention mechanism to help with dependencies over long distances [15], the use of beam search to improve decoding and the exploration of uncontrolled adjustments as in [19]. Another point for future work is the collection of additional tasks to examine the scalability of the multi-task model beyond the four-time applications required in these models."}, {"heading": "6. References", "text": "[1] P. Price, \"Evaluation of spoken language systems: The ATIS do-main,\" in Proc. of the DARPA Speech and Natural Language Workshop. Morgan Kaufmann, 1990, pp. 91-95. [2] H. Meng, S. Busayapongchai H., J. Giass, D. Goddeau, L. Hethetingron, E. Hurley, C. Pao, J. Polifroni, S. Seneff, and V. Zue, \"Wheels: A conversations system in the automobile classifieds domain,\" in Spoken Language, 1996. ICSLP 96. Proceedings, Fourth International Conference on, Fourth International Conference on, vol. 1, 1996, pp. 542-545. [3] J. R. Glass and T. J. Hazen, \"Telphone-based speech recognition in the JUPITER domain.\""}], "references": [{"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P. Price"], "venue": "Proc. of the DARPA Speech and Natural Language Workshop. Morgan Kaufmann, 1990, pp. 91\u201395.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Wheels: A conversational system in the automobile classifieds domain", "author": ["H. Meng", "S. Busayapongchai", "J. Giass", "D. Goddeau", "L. Hethetingron", "E. Hurley", "C. Pao", "J. Polifroni", "S. Seneff", "V. Zue"], "venue": "Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 1, 1996, pp. 542\u2013545.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Telephone-based conversational speech recognition in the JUPITER domain.", "author": ["J.R. Glass", "T.J. Hazen"], "venue": "in ICSLP, vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of the International Conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep belief network based semantic taggers for spoken language understanding.", "author": ["A. Deoras", "R. Sarikaya"], "venue": "in Proc. Interspeech,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding.", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "in Proc. Interspeech,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Recurrent neural networks for language understanding.", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "in Proc. Interspeech,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. of the IEEE Automatic Speech Recognition and Understanding, 2013, pp. 78\u201383.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "Proc. of the IEEE Spoken Language Technology Workshop, 2014, pp. 189\u2013194.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual spoken language understanding using recurrent neural networks", "author": ["Y. Shi", "K. Yao", "H. Chen", "Y.-C. Pan", "M.-Y. Hwang", "B. Peng"], "venue": "Proc. ICASSP, 2015, pp. 5271\u20135275.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani- Tur", "X. He", "L. Heck", "G. Tur", "D. Yu"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Lstm neural networks for language modeling.", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "in Proc. Interspeech,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Recurrent neural networks with external memory for spoken language understanding", "author": ["B. Peng", "K. Yao", "L. Jing", "K.-F. Wong"], "venue": "Natural Language Processing and Chinese Computing. Springer, 2015, pp. 25\u201335.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Leveraging sentencelevel information with encoder LSTM for natural language understanding", "author": ["G. Kurata", "B. Xiang", "B. Zhou", "M. Yu"], "venue": "arXiv preprint arXiv:1601.01530, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.01280, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Bi-directional recurrent neural network with ranking loss for spoken language understanding.", "author": ["T.N. Vu", "P. Gupta", "H. Adel", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "Prox. of the IEEE Spoken Language Technology Workshop, 2014, pp. 360\u2013365.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling.", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "in Proc. Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Text chunking using transformation-based learning", "author": ["L.A. Ramshaw", "M.P. Marcus"], "venue": "1995, pp. 82\u201394.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Introduction to the conll- 2000 shared task: Chunking", "author": ["E.F. Tjong Kim Sang", "S. Buchholz"], "venue": "Proc. of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7, 2000, pp. 127\u2013132.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "What is left to be understood in ATIS?", "author": ["G. Tur", "D. Hakkani-Tur", "L. Heck"], "venue": "in Proc. of the IEEE Spoken Language Technology Workshop (SLT),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have been exploring datadriven approaches to learning models for automatic identification of slot information since the 90\u2019s, and significant advances have been made [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "For example, one model understands queries about classified ads for cars [2] and another model handles queries about the weather [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "For example, one model understands queries about classified ads for cars [2] and another model handles queries about the weather [3].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Multi-task learning in combination with neural networks has been show to be effective for natural language processing tasks [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 79, "endOffset": 85}, {"referenceID": 6, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 79, "endOffset": 85}, {"referenceID": 7, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 8, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 9, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 10, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 11, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 13, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 14, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 15, "context": "The particular variant that we build on is a bidirectional LSTM, similar to [16, 11].", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "The particular variant that we build on is a bidirectional LSTM, similar to [16, 11].", "startOffset": 76, "endOffset": 84}, {"referenceID": 16, "context": "into our model [17].", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Our approach to handling words unseen in training data is different from the delexicalization proposed in [19] in that we do not require the vocabulary items associated with slots and values to be prespecified.", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "We choose this architecture because similar models have been used in prior work on slot filling and have achieved good results [16, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 10, "context": "We choose this architecture because similar models have been used in prior work on slot filling and have achieved good results [16, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 18, "context": "including the use of the linear projection layer on the output of the LSTM [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "The slot labels are encoded in BIO format [21] indicating if a word is the beginning, inside or outside any particular slot.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Evaluation is done using the CoNLL evaluation script [22] to calculate the F1 score.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "In recent work on language modeling, a neural architecture that combined fixed word embeddings with character-based embeddings was found to to be useful for handling previously unseen words [23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "One thing to notice is that the the number of slot types is relatively small when compared to the popular ATIS dataset that has over one hundred slot types [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "Another possible source of data is the Air Travel Information Service (ATIS) data set collected in the early 1990\u2019s [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 22, "context": "The new data collected for this work fills a need raised in [24], which concluded that lack of data was an impediment to progress in slot filling.", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "Dropout is used for regularization on the word embeddings and on the outputs from each LSTM layer with the dropout probability set to 60% [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "For future work, there are some improvements that could be made to the model such as the addition of an attentional mechanism to help with long distance dependencies [15], use of beam-search to improve decoding, and exploring unsupervised adaptation as in [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "For future work, there are some improvements that could be made to the model such as the addition of an attentional mechanism to help with long distance dependencies [15], use of beam-search to improve decoding, and exploring unsupervised adaptation as in [19].", "startOffset": 256, "endOffset": 260}, {"referenceID": 24, "context": "Due to their extra depth, character-based methods usually require more data than word based models [26].", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.", "creator": "LaTeX with hyperref package"}}}