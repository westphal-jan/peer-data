{"id": "1604.00317", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "A Semisupervised Approach for Language Identification based on Ladder Networks", "abstract": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.", "histories": [["v1", "Fri, 1 Apr 2016 16:26:57 GMT  (678kb,D)", "http://arxiv.org/abs/1604.00317v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ehud ben-reuven", "jacob goldberger"], "accepted": false, "id": "1604.00317"}, "pdf": {"name": "1604.00317.pdf", "metadata": {"source": "CRF", "title": "A Semisupervised Approach for Language Identification based on Ladder Networks", "authors": ["Ehud Ben-Reuven", "Jacob Goldberger"], "emails": ["udi@benreuven.com", "jacob.goldberger@biu.ac.il"], "sections": [{"heading": null, "text": "There are many types of language that one must be able to understand in order to understand it. There are only a few that are able to understand it. There are only a few that are able to understand it."}, {"heading": "2. The Ladder Network Architecture", "text": "In fact, we see ourselves in a position to put ourselves at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "3. Dataset and Evaluation Metrics", "text": "The challenge of speech recognition i-vector [6] of 2015 includes 50 target languages and a number of unnamed \"out-ofset\" (oos) languages. Labeled training data (300 language segments per language) were provided for the target languages and a set of approximately 6,500 unlabeled examples for the target and out-of-set languages was provided for development. The test set consisted of 6,500 test segments for the target and out-of-set languages. The linguistic duration of the audio segments used to create the i-vectors for the challenge was sampled from a log-normal distribution with an average of about 35s. Language segments originate from telephone and narrowband speech data. Each language segment is represented by an i-vector of 400 components [17]. The task is to classify each language segment either as one of the 50 target languages or as out-ofset. According to the challenge rules, the goal is to minimize the following cost function # \u00b7 k = pok = 1 \u2212 1."}, {"heading": "4. Model and Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Network and cost function", "text": "We assume that we have additional costs, which are composed of two components."}, {"heading": "4.2. A post processing step", "text": "The final decision is made by selecting the most likely label. Once the predictions are made on the basis of the test data, we can measure the ratio of the number of examples for which oos was predicted. If this ratio is below the predefined ratio, poos, we can relativize more examples than oos until the expected ratio is fulfilled. The candidate examples for re-labeling are those in which the probability of the most likely language is lowest. If this ratio is above the predefined ratio, poos, we can pass on fewer examples than oos until the expected ratio is reached. This re-labeling is done by multiplying the predicted probability of being oos, with a reduction bias that gives other labels a higher probability of being selected than the predicted label. In the test results, we show that this method actually helps if standard network training is only based on the marked data (i.e., if 0 = 0)."}, {"heading": "4.3. Parameter Tuning", "text": "We created a modified dataset for adjusting hyperparameters, using only the labeled portion of the training dataset. We randomly selected 38 out of 50 languages as the language set and considered the data of the remaining 12 languages obsolete. We took a subset of the 38 x 300 examples as the labeled training dataset and then used the remaining labeled data to construct a development set and a test set containing both examples from all 50 languages. Development and test sets were constructed so that all 50 languages (either in-set or out-of-set) were in the same proportionality. The process of creating a modified set can be repeated many times, and each time we selected a different group of languages that should be non-of-set, allowing each language to be non-of-set. The above cross-validation was used to determine the hyperparameters of the circuit network, and each time we selected a different group of languages that should be non-of-set, allowing each language to be non-of-set."}, {"heading": "4.4. Model Configuration", "text": "The network we implemented1 has an i-vector input of dimension 400 running through four hidden ReLU layers of size 500, 500, 500 and 100, and a final soft-max output layer of 51 (or 39 during cross-validation for parameter adjustment).In the coding step, a Gaussian noise with a standard deviation of \u03c3 = 0.5 was added to the input data and all hidden intermediate layers. Training was performed with mini-stacks with a stack size of 1024. Note that this size has a secondary effect through the loss function C2, which mediates the predictions about the mini-stack before calculating the lose.The training consisted of 1000 epoch iterations. The order of samples was 1code available at https: / github.com / udibr / LREshuffled before each iteration. It turned out that due to the unsecured 2000 learning method, the number of the epoch layer was not similar to the number of the other 800 epoch layer."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. NIST challenge results", "text": "Next, we report on the results for several variants of the method described above at the NIST 2015 Language Recognition i-vector Challenge = two tested results (defined as i-vector Challenge [6]. Each combination of models and parameters was applied to the test set of the challenge and transmitted to the competition website. In accordance with the challenge rules, an unknown subset of 30% of the test samples were used to calculate a score for the progress set and the results for the remaining 70% sets are not reported by the website. The performance score is defined in Equation (5), where a smaller number is better. Table 1 shows the progress set results we have achieved for various models and parameters on the competition website. We examined two training procedures: one with the full conductor structure and one with a baseline variant in which we set the weight of the ladder reconstruction score to zero. The baseline method is still based on a function of a reconstruction layer that turned out to be a forward-cost, similar to a reconstruction layer."}, {"heading": "5.2. Analysis", "text": "The conductor model had superior regulation, as demonstrated by the cross-validation tests we performed. Table 2 shows the cross-validation results for the conductors and the base model (excluding the conductor recovery score), and the score was defined in (5), where the number of languages is k = 38. Each line of the table corresponds to a different division of the 50 languages into 38 fixed and 12 unfixed languages. Selecting different sets of languages that can be fixed and not fixed can produce drastically different results. We therefore repeated the process five times to allow each language to be removed from the given series at least once. Table 2 also reports on the number of eras needed to obtain the best test results. In the base method, there was a problem of match and this required an early stop in order to achieve the best results. In contrast, the conductor model enforces aggressive regulation that avoids the need for an early stop."}, {"heading": "6. References", "text": "[1] G. Hinton et al. \"Deep neural networks for acoustic mod-eling in speech recognition,\" IEEE Signal Processing Magazine, vol. 29, no. 8, pp. 82-97, 2012. [2] Y. Song, B. Jiang, Y. Bao, S. Wei, and L.-R. Dai, \"I-vector representation based on bottleneck features for language identification,\" electron. Lett., pp. 1569-1580, 2013. [3] P. Matejka, L. Zhang, T. Ng, H. S. Mallidi, O. Glembek, J. Ma, and B. Zhang, \"Neural network features for language identification,\" in IEEE Odyssey, 2014, pp. 299-304] I. Lopez-Moreno, J. Gonzalez-Dominguez, D. O."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 8, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electron. Lett., pp. 1569\u20131580, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural network bottleneck features for language identification", "author": ["P. Matejka", "L. Zhang", "T. Ng", "H.S. Mallidi", "O. Glembek", "J. Ma", "B. Zhang"], "venue": "IEEE Odyssey, 2014, pp. 299\u2013304.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "D. Martinez O. Plchot", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "ICASSP, 2014, pp. 5374\u20135378.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network approaches to speaker and language recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1671\u2013 1675, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Semisupervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "arXiv:1507.02672, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "From neural PCA to deep unsupervised learning", "author": ["H. Valpola"], "venue": "arXiv:1411.7783, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deconstructing the ladder network architecture", "author": ["M. Pezeshki", "L. Fan", "P. Brakel", "A. Courville", "Y. Bengio"], "venue": "arXiv:1511.06430, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Int. Conference on Machine Learning (ICML), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2011, pp. 2348\u20132356.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1929}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["H. Larochelle Y. Bengio H. Vincent", "P.A. Manzagol"], "venue": "Int. Conference on Machine Learning (ICML), 2008, pp. 1096\u20131103.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Language recognition via Ivectors and dimensionality reduction", "author": ["N. Dehak", "P.A. Torres-Carrasquillo", "D. Reynold", "R. Dehak"], "venue": "Interspeech, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The impressive performance improvement obtained using deep neural networks (DNNs) for automatic speech recognition (ASR) [1] have motivated the application of DNNs to speaker and language recognition.", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "DNN have been trained for a different purpose to learn frame-level features that were then used to train a secondary classifier for the intended language recognition task [2][3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "DNN have been trained for a different purpose to learn frame-level features that were then used to train a secondary classifier for the intended language recognition task [2][3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "DNNs have also been applied to directly train language classification systems [4][5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "DNNs have also been applied to directly train language classification systems [4][5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "The standard pre-training strategy is based on a greedy layer-wise procedure using either Restricted Boltzmann Machines (RBM) [7] or noisy autoencoders [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "The standard pre-training strategy is based on a greedy layer-wise procedure using either Restricted Boltzmann Machines (RBM) [7] or noisy autoencoders [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "Our approach is based on the recently proposed Ladder Network training procedure that has proven to be very successful, especially in cases where the training dataset is composed of both labeled and unlabeled data [9] [10].", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "Our approach is based on the recently proposed Ladder Network training procedure that has proven to be very successful, especially in cases where the training dataset is composed of both labeled and unlabeled data [9] [10].", "startOffset": 218, "endOffset": 222}, {"referenceID": 7, "context": "A detailed description of Ladder Networks can be found in [9] [10].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "A detailed description of Ladder Networks can be found in [9] [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Figure 1: A conceptual illustration of the Ladder Network for L = 2 [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "A comparison of other possible choices for the combinator function is presented in [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "The concept of batch normalization was recently proposed by Ioffe and Szegedy [12] as a method to improve convergence as a result of reduced covariance shift.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The noise is used here to regularize supervised learning in a way similar to the weight noise regularization method [13] and dropout [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "The noise is used here to regularize supervised learning in a way similar to the weight noise regularization method [13] and dropout [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "The cost of the standard denoising autoencoder is the input reconstruction error [15] [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "The cost of the standard denoising autoencoder is the input reconstruction error [15] [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "A recent study [11] conducted an extensive experimental investigation of variants of the Ladder Network in which individual components were removed or replaced to gain insights into their relative importance.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Ladder Networks have been found to obtain state-of-the-art results on standard datasets with only a small portion of labeled examples [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 10, "context": "[12] [14]) applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be crucial for success when only a small amount of labeled data is available.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] [14]) applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be crucial for success when only a small amount of labeled data is available.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Each speech segment is represented by an i-vector of 400 components [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": ") We tried to use another standard unsupervised cost function that is based on minimizing the entropy of the distribution obtained by the soft-max layer, thus encouraging the decision in the unlabeled data to focus on one of the languages [19]:", "startOffset": 239, "endOffset": 243}, {"referenceID": 7, "context": "A direct skip of information from the encoder to the decoder was used only on the input layer using the Gaussian method described in [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 12, "context": "The baseline method is still based on a noisyforward step which, without the layer-reconstruction cost function, turned out to be a regularized learning method similar to dropout [14].", "startOffset": 179, "endOffset": 183}], "year": 2016, "abstractText": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.", "creator": "LaTeX with hyperref package"}}}