{"id": "1604.00400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "histories": [["v1", "Fri, 1 Apr 2016 20:06:46 GMT  (33kb,D)", "http://arxiv.org/abs/1604.00400v1", "LREC 2016"]], "COMMENTS": "LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1604.00400"}, "pdf": {"name": "1604.00400.pdf", "metadata": {"source": "CRF", "title": "Revisiting Summarization Evaluation for Scientific Articles", "authors": ["Arman Cohan", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu,", "nazli@ir.cs.georgetown.edu"], "sections": [{"heading": null, "text": "Keywords: abstract, evaluation, scientific articles"}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Summarization evaluation by ROUGE", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "3. Summarization Evaluation by Relevance Analysis (SERA)", "text": "It is not as if this is a way of referring to the same concepts and therefore relying only on lexical overlaps. To overcome this problem, we propose an approach based on the premise that concepts are taken from the context in which they are located and that related concepts occur frequently. Our proposed metric is based on analyzing the relevance of content between a system-generated summary and the associated human-written gold standard summaries. At a high level, we indirectly evaluate the relevance of content between the candidate and the human summary using information. To achieve this, we use the summaries as search queries and compare the overlapping applications of the retrieved results. More overlaps suggest that candidates have higher quality in relation to the gold standards."}, {"heading": "4. Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "To our knowledge, the only scientific summary benchmark is the TAC 2014 Summary Track. To evaluate the effectiveness of ROUGE variants and our metric (SERA), we use this benchmark, which consists of 20 topics, each with an article in a biomedical journal and 4 golden human summaries."}, {"heading": "4.2. Annotations", "text": "In the TAC 2014 Summary Track it was proposed to use ROUGE = as a yardstick for evaluation of summary and no human evaluation was provided for the topics. Therefore, in order to study the effectiveness of evaluation metrics, we use the semi-manual pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007). In the pyramid scoring, the units of content in the gold human written summaries are organized in a pyramid. In this pyramid, the units of content organized in tiers and higher stages of the pyramid show greater importance. The quality of a particular candidate's summary is evaluated in relation to this pyramid. To analyze the quality of evaluation metrics, according to the pyramid framework, we design an annotation scheme that is based on the identification of key units of content."}, {"heading": "4.3. Summarization approaches", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "5. Results and Discussion", "text": "We calculated all variants of the ROUGE score, our proposed metrics, SERA, and the Pyramid score based on the summaries produced of the summaries described in Section 4.3. We do not report on the ROUGE, SERA, or Pyramid scores of individual systems, as they are not at the center of this study. Our goal is to analyze the effectiveness of the evaluation metrics, not the summation approaches. Therefore, we look at the correlations of automatic evaluation metrics with the manual pyramid scores to evaluate their effectiveness; more effective are the metrics that have higher correlations with manual assessments. Table 2 shows the correlation of ROUGE and SERA between Pearson, Spearman, and Kendall, with pyramid scores. Both ROUGE and SERA are calculated using stopwords that are removed and with stemming. Our experiments with the inclusion of stopwords, and therefore, we do not include redundancies and do not show similar results with stemming."}, {"heading": "5.1. SERA", "text": "The results of our proposed method (SERA) are shown in the lower part of Table 2 KW-off KW (KW-NP shows the highest ERA results SERA = 59. SERA shows a better correlation with pyramid values compared to ROUGE. We observe that the Pearson correlation of SERA with the cut-off point NP 5 (shown by SERA-5) is 0.823, which is higher than most of the ROUGE variants. Likewise, the count of Spearman and Kendall shows correlations of the SERA score score 0.941 and 0.857, respectively, which are higher than all ROUGE correlation values. This shows the effectiveness of the simple variant of our proposed summary evaluation metric. Table 2 also shows the results of other SERA variants, including discounting and query reformulation methods. Some of these variants are the result of the application of query reformulation in the process of the SERA-query correlation between the relevant correlations described in section 3."}, {"heading": "5.2. ROUGE", "text": "Another important observation concerns the effectiveness of the ROUGE values (top of Table 2).Interestingly, we note that many variants of the ROUGE values do not have high correlations with the values of the human pyramid; the lowest F values correlate with ROUGE-1 and ROUGE-L (with r = 0.454).Weak correlations of ROUGE-1 show that the agreement of the unigrams between the candidate summary and the gold summary is inaccurate in terms of quantifying the quality of the summary. However, on n-grams of higher order, we can see that ROUGE correlates better with the pyramid. In fact, the highest overall result is obtained by ROUGE-3. ROUGE-L and its weighted version ROUGE-W, both exhibit weak correlations with the pyramid. Skip bigrams (ROUGE-S) and its combination with the unigrams (UGE-SU) also have optimal correlations."}, {"heading": "5.3. Correlation of SERA with ROUGE", "text": "Table 3 shows correlations of our metric SERA with ROUGE-2 and ROUGE-3, the most highly correlated ROUGE variants with pyramid. We see that the correlation is generally not strong. Keyword-based reduction variants are the only variants where the correlation with ROUGE is high. Looking at the correlations of the KW variants of SERA with pyramid (Table 2, lower part), we find that these variants are also strongly correlated with the manual evaluation."}, {"heading": "5.4. Effect of the rank cut-off point", "text": "Finally, Figure 1 shows the correlation of different variants of SERA with the pyramid based on the selection of different intersections (r and \u03c4 correlations yield very similar graphs).As the intersection increases, more documents are found for the candidate and the gold summaries, and therefore the final SERA value is finer-grained. A general observation is that the correlation with the pyramid results decreases as the search time progresses, because the more the size of the retrieved list of results increases the likelihood of including less related documents, which negatively impacts on the correct assessment of the similarity of candidate and gold summaries. The most accurate estimates refer to metrics with intersections of 5 and 10, which are included in the reported results of all variants in Table 2."}, {"heading": "6. Related work", "text": "ROUGE (Lin, 2004) evaluates the content quality of a candidate summary in relation to a number of human gold summaries based on their lexical overlaps. ROUGE consists of several variants. Since its introduction, ROUGE has been considered one of the most frequently reported metrics in the abstract literature, and its high acceptance has been due to its high correlation with human assessment results in DUC records (Lin, 2004). However, subsequent research has cast doubt on the accuracy of ROUGE against manual assessments. Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE results in relation to human summaries, the linguistic and responsibility scores of these systems do not correspond to the high ROUGE scores. We investigated the effectiveness of ROUGE by correlation analysis using manual scores. In addition to correlation with human assessments, other approaches metrics were examined to analyze the effectiveness of the summary."}, {"heading": "7. Conclusions", "text": "We have shown that ROUGE may not be the best metric for evaluating summaries, especially in summaries with high terminological discrepancies and paraphrases (e.g. scientific summaries). Furthermore, we have shown that different variants of ROUGE lead to different correlation values with human judgments, suggesting that not all ROUGE results are equally effective. Of all variants of ROUGE, ROUGE-2 and ROUGE-3 are better correlated with manual judgments in the context of scientific summaries. Furthermore, we have proposed an alternative and more effective approach to evaluating scientific summaries (Summary Evaluation by Relevance Analysis - SERA). Results have shown that the proposed evaluation metric generally achieves higher correlations with semi-metric measures than comparative qualities for this relevance analysis (SERA)."}, {"heading": "Acknowledgments", "text": "We would like to thank all three anonymous reviewers for their feedback and comments, and Maryam Iranmanesh for her help in commenting. This work was partially supported by the National Science Foundation (NSF) through the CNS-1204347 grant."}, {"heading": "8. Bibliographical References", "text": "Abu-Jbara, A. and Radev, D. (2011). Coherentcitation-based summarization of scientific papers. In ACL '11, pp. 500-509. Association for Computational Linguistics. Lead, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of Machine Learning research, pp. 335-336. ACM. Cohan, A. and Goharian, N. (2015). Scientific article summarization using citation context and article's discourse structure. In EMNLP, pp. 390-400. Association for Computational Linguistics. Conroy, J. T. and Dang, T.."}], "references": [{"title": "Coherent citation-based summarization of scientific papers", "author": ["A. Abu-Jbara", "D. Radev"], "venue": "ACL \u201911, pages 500\u2013509. Association for Computational Linguistics.", "citeRegEx": "Abu.Jbara and Radev,? 2011", "shortCiteRegEx": "Abu.Jbara and Radev", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "SIGIR, pages 335\u2013336. ACM.", "citeRegEx": "Carbonell and Goldstein,? 1998", "shortCiteRegEx": "Carbonell and Goldstein", "year": 1998}, {"title": "Scientific article summarization using citation context and article\u2019s discourse structure", "author": ["A. Cohan", "N. Goharian"], "venue": "EMNLP, pages 390\u2013400. Association for Computational Linguistics.", "citeRegEx": "Cohan and Goharian,? 2015", "shortCiteRegEx": "Cohan and Goharian", "year": 2015}, {"title": "Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality", "author": ["J.M. Conroy", "H.T. Dang"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 145\u2013152. Association for", "citeRegEx": "Conroy and Dang,? 2008", "shortCiteRegEx": "Conroy and Dang", "year": 2008}, {"title": "Nouveau-rouge: A novelty metric for update summarization", "author": ["J.M. Conroy", "J.D. Schlesinger", "D.P. O\u2019Leary"], "venue": "Computational Linguistics,", "citeRegEx": "Conroy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Conroy et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR), 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev,? 2004", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "Summary evaluation: Together we stand npower-ed", "author": ["G. Giannakopoulos", "V. Karkaletsis"], "venue": "Computational Linguistics and Intelligent Text Processing, pages 436\u2013450. Springer.", "citeRegEx": "Giannakopoulos and Karkaletsis,? 2013", "shortCiteRegEx": "Giannakopoulos and Karkaletsis", "year": 2013}, {"title": "Re-evaluating automatic summarization with bleu and 192 shades of rouge", "author": ["Y. Graham"], "venue": "EMNLP \u201915\u2019, pages 128\u2013137, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Graham,? 2015", "shortCiteRegEx": "Graham", "year": 2015}, {"title": "Exploring content models for multi-document summarization", "author": ["A. Haghighi", "L. Vanderwende"], "venue": "NAACL-HLT \u201909, pages 362\u2013370. Association for Computational Linguistics.", "citeRegEx": "Haghighi and Vanderwende,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende", "year": 2009}, {"title": "Automated summarization evaluation with basic elements", "author": ["E. Hovy", "Lin", "C.-Y.", "L. Zhou", "J. Fukumoto"], "venue": "LREC \u201906, pages 604\u2013611. Citeseer.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8.", "citeRegEx": "Lin and C..Y.,? 2004", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "The automatic creation of literature abstracts", "author": ["H.P. Luhn"], "venue": "IBM Journal of research and development, 2(2):159\u2013165.", "citeRegEx": "Luhn,? 1958", "shortCiteRegEx": "Luhn", "year": 1958}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["A. Nenkova", "R. Passonneau"], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.", "citeRegEx": "Nenkova and Passonneau,? 2004", "shortCiteRegEx": "Nenkova and Passonneau", "year": 2004}, {"title": "The pyramid method: Incorporating human content selection variation in summarization evaluation", "author": ["A. Nenkova", "R. Passonneau", "K. McKeown"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(2):4.", "citeRegEx": "Nenkova et al\\.,? 2007", "shortCiteRegEx": "Nenkova et al\\.", "year": 2007}, {"title": "Overview of the tac 2011 summarization track: Guided task and aesop task", "author": ["K. Owczarzak", "H.T. Dang"], "venue": "TAC 2011.", "citeRegEx": "Owczarzak and Dang,? 2011", "shortCiteRegEx": "Owczarzak and Dang", "year": 2011}, {"title": "An assessment of the accuracy of automatic evaluation in summarization", "author": ["K. Owczarzak", "J.M. Conroy", "H.T. Dang", "A. Nenkova"], "venue": "Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1\u20139.", "citeRegEx": "Owczarzak et al\\.,? 2012", "shortCiteRegEx": "Owczarzak et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "ACL \u201902, pages 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Automatic evaluation of linguistic quality in multi-document summarization", "author": ["E. Pitler", "A. Louis", "A. Nenkova"], "venue": "Proceedings of the ACL 2010, pages 544\u2013554. Association for Computational Linguistics.", "citeRegEx": "Pitler et al\\.,? 2010", "shortCiteRegEx": "Pitler et al\\.", "year": 2010}, {"title": "Generating extractive summaries of scientific paradigms", "author": ["V. Qazvinian", "D.R. Radev", "S. Mohammad", "B.J. Dorr", "D.M. Zajic", "M. Whidby", "T. Moon"], "venue": "J. Artif. Intell. Res.(JAIR), 46:165\u2013201.", "citeRegEx": "Qazvinian et al\\.,? 2013", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2013}, {"title": "Ranking human and machine summarization systems", "author": ["P. Rankel", "J.M. Conroy", "E.V. Slud", "D.P. O\u2019Leary"], "venue": "EMNLP \u201911,", "citeRegEx": "Rankel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rankel et al\\.", "year": 2011}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. ISIM\u201904, pages 93\u2013100.", "citeRegEx": "Steinberger and Jezek,? 2004", "shortCiteRegEx": "Steinberger and Jezek", "year": 2004}, {"title": "Summarizing scientific articles: experiments with relevance and rhetorical status", "author": ["S. Teufel", "M. Moens"], "venue": "Computational linguistics, 28(4):409\u2013445.", "citeRegEx": "Teufel and Moens,? 2002", "shortCiteRegEx": "Teufel and Moens", "year": 2002}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "Information Processing & Management, 43(6):1606\u20131618.", "citeRegEx": "Vanderwende et al\\.,? 2007", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2007}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342.", "citeRegEx": "Zhai and Lafferty,? 2001", "shortCiteRegEx": "Zhai and Lafferty", "year": 2001}], "referenceMentions": [{"referenceID": 8, "context": "However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce (Giannakopoulos and Karkaletsis, 2013).", "startOffset": 107, "endOffset": 145}, {"referenceID": 18, "context": "It is inspired by the success of a similar metric BLEU (Papineni et al., 2002) which is being used in Machine Translation (MT) evaluation.", "startOffset": 55, "endOffset": 78}, {"referenceID": 16, "context": "ROUGE has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC1 (Owczarzak and Dang, 2011).", "startOffset": 105, "endOffset": 131}, {"referenceID": 23, "context": "Thus, effective summarization approaches usually have much higher compression rate, terminology variations and paraphrasing (Teufel and Moens, 2002).", "startOffset": 124, "endOffset": 148}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al.", "startOffset": 90, "endOffset": 117}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al. (2013), and Cohan and Goharian (2015)).", "startOffset": 90, "endOffset": 142}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al. (2013), and Cohan and Goharian (2015)).", "startOffset": 90, "endOffset": 173}, {"referenceID": 26, "context": "For retrieval model, we use the Language Modeling retrieval model with Dirichlet smoothing (Zhai and Lafferty, 2001).", "startOffset": 91, "endOffset": 116}, {"referenceID": 14, "context": "Therefore, to study the effectiveness of the evaluation metrics, we use the semi-manual Pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007).", "startOffset": 117, "endOffset": 169}, {"referenceID": 15, "context": "Therefore, to study the effectiveness of the evaluation metrics, we use the semi-manual Pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007).", "startOffset": 117, "endOffset": 169}, {"referenceID": 7, "context": "LexRank (Erkan and Radev, 2004): LexRank finds the most important (central) sentences in a document by using random walks in a graph constructed from the document sentences.", "startOffset": 8, "endOffset": 31}, {"referenceID": 22, "context": "Latent Semantic Analysis (LSA) based summarization (Steinberger and Jezek, 2004): In this summarization method, Singular Value Decomposition (SVD) (Deerwester et al.", "startOffset": 51, "endOffset": 80}, {"referenceID": 6, "context": "Latent Semantic Analysis (LSA) based summarization (Steinberger and Jezek, 2004): In this summarization method, Singular Value Decomposition (SVD) (Deerwester et al., 1990) is used for deriving latent semantic structure of the document.", "startOffset": 147, "endOffset": 172}, {"referenceID": 2, "context": "Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998): Maximal Marginal Relevance (MMR) is a greedy strategy for selecting sentences for the summary.", "startOffset": 33, "endOffset": 64}, {"referenceID": 20, "context": "Citation based summarization (Qazvinian et al., 2013): In this method, citations are used for summarizing an article.", "startOffset": 29, "endOffset": 53}, {"referenceID": 13, "context": "Using frequency of the words (Luhn, 1958): In this method, which is one the earliest works in text summarization, raw word frequencies are used to estimate the saliency of sentences in the document.", "startOffset": 29, "endOffset": 41}, {"referenceID": 25, "context": "SumBasic (Vanderwende et al., 2007): SumBasic is an approach that weights sentences based on the distribution of words that is derived from the document.", "startOffset": 9, "endOffset": 35}, {"referenceID": 3, "context": "Summarization using citation-context and discourse structure (Cohan and Goharian, 2015): In this method,", "startOffset": 61, "endOffset": 87}, {"referenceID": 10, "context": "KL Divergence (Haghighi and Vanderwende, 2009) In this method, the document unigram distribution P and the summary unigram distributation Q are considered; the goal is to find a summary whose distribution is very close to the document distribution.", "startOffset": 14, "endOffset": 46}, {"referenceID": 10, "context": "Summarization based on Topic Models (Haghighi and Vanderwende, 2009): Instead of using unigram distributions for modeling the content distribution of the document and the summary, this method models the document content using an LDA based topic model (Blei et al.", "startOffset": 36, "endOffset": 68}, {"referenceID": 1, "context": "Summarization based on Topic Models (Haghighi and Vanderwende, 2009): Instead of using unigram distributions for modeling the content distribution of the document and the summary, this method models the document content using an LDA based topic model (Blei et al., 2003).", "startOffset": 251, "endOffset": 270}, {"referenceID": 9, "context": "Machine translation evaluation metrics such as BLUE have also been compared and contrasted against ROUGE (Graham, 2015).", "startOffset": 105, "endOffset": 119}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries.", "startOffset": 0, "endOffset": 505}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers.", "startOffset": 0, "endOffset": 795}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries.", "startOffset": 0, "endOffset": 1041}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries. BEs were extracted based on syntactic structure of the sentence. The work by Conroy et al. (2011) was another attempt for improving ROUGE for update summarization which combined two different ROUGE variants and showed higher correlations with manual judgments for TAC 2008 update summaries.", "startOffset": 0, "endOffset": 1260}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries. BEs were extracted based on syntactic structure of the sentence. The work by Conroy et al. (2011) was another attempt for improving ROUGE for update summarization which combined two different ROUGE variants and showed higher correlations with manual judgments for TAC 2008 update summaries. Apart from the content, other aspects of summarization such as linguistic quality have been also studied. Pitler et al. (2010) evaluated a set of models based on syntactic features, language models and entity coherences for assessing the linguistic quality of the summaries.", "startOffset": 0, "endOffset": 1580}, {"referenceID": 21, "context": "An alternative approach to follow would be to use statistical significance testing on the ability of the metrics to distinguish between the summarizers (similar to Rankel et al. (2011)).", "startOffset": 164, "endOffset": 185}], "year": 2016, "abstractText": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE\u2019s effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "creator": "LaTeX with hyperref package"}}}