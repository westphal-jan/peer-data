{"id": "1604.00449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction", "abstract": "Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).", "histories": [["v1", "Sat, 2 Apr 2016 01:28:27 GMT  (8149kb,D)", "http://arxiv.org/abs/1604.00449v1", "Appendix can be found atthis http URL"]], "COMMENTS": "Appendix can be found atthis http URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["christopher b choy", "danfei xu", "junyoung gwak", "kevin chen", "silvio savarese"], "accepted": false, "id": "1604.00449"}, "pdf": {"name": "1604.00449.pdf", "metadata": {"source": "CRF", "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction", "authors": ["Christopher B. Choy", "Danfei Xu", "JunYoung Gwak", "Kevin Chen", "Silvio Savarese"], "emails": ["ssilvio}@stanford.edu"], "sections": [{"heading": null, "text": "Keywords: multiview, reconstruction, recurrent neural network"}, {"heading": "1 Introduction", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2 Recurrent Neural Network", "text": "In this section, we offer a brief overview of Long Short-Term Memory (LSTM) networks and a variation of the LSTM called Gated Recurrent Units (GRU).Long Short-Term Memory Unit. One of the most successful implementations of the hidden states of an RNN is the Long Short-Term Memory (LSTM) unit [33]. An LSTM unit explicitly controls the flow from input to output \u2212 rt, allowing the network to overcome the problem of the vanishing gradient [33,38]. Specifically, an LSTM unit consists of four components: memory units (one memory cell and one hidden state) and three gates that control the flow of information from the entrance gate to the hidden state (entrance gate), from the hidden state to the exit gate (exit gate) and from the previous hidden state to the current hidden state (forget gate). Formally, the Uxt unit is considered to be a new input gate."}, {"heading": "3 3D Recurrent Reconstruction Neural Network", "text": "In this section, we present a novel architecture called 3D Recurrent Reconstruction Network (3D-R2N2), which builds on the standard states LSTM and GRU. The goal of the network is to perform both 3Dreconstructions with one and more views. The basic idea is to use the power of LSTM to maintain previous observations and gradually refine the output reconstruction as more observations become available.The network consists of three components: a 2D Convolutional Neural Network (2D-CNN), a novel architecture called 3D Convolutional LSTM (3DLSTM) and a 3D Deconvolutional Neural Network (3D-DCNN) (see Fig. 2). In the face of one or more images of an object from arbitrary angles, 2D-CNN first encodes each input image x in low-dimensional features T (x) (Section 3.1)."}, {"heading": "3.1 Encoder: 2D-CNN", "text": "We have designed two different 2D CNN encoders, as shown in Fig. 2: A standard feed forward CNN and a deep residual variant thereof. The first network consists of standard folding layers, pooling layers and leaky linear units followed by a fully connected layer. Motivated by a recent study [40], we have also created a deep residual variation of the first network and report on the performance of this variant in Section 5.2. According to the study, adding residual connections between standard folding layers effectively improves and accelerates the optimization process for very deep networks. The deep variation of the encoder network exhibits after each 2 folding layers except for the fourth pair of identity mapping connections. To match the number of channels after turns, we use a 1 x 1 conversion for residual connections. The encoder output is then laid flat and transferred to a fully connected layer, which compresses the output into a 1024 dimensional feature."}, {"heading": "3.2 Recurrence: 3D Convolutional LSTM", "text": "The core of our 3D LSTM unit is a recurrence module that allows the network to retain what it has seen and update memory when it sees a new image. \u2212 A naive approach would be to use a vanilla LSTM network. However, the network consists of a series of structured LSTM units with limited connections. 3D LSTM units are spatially distributed in a 3D network structure, with each unit responsible for reconstructing a specific portion of the final output (see Fig. 3). Within the 3D network there are N \u00b7 N 3D LSTM units, where N \u00b7 N is the spatial resolution of the 3D LSTM network. Each 3D LSTM unit that indexes (i, j, k) has a hidden state. We are N \u00b7 N 3D LSTM units, where N is the spatial resolution of the 3D LSTM network."}, {"heading": "3.3 Decoder: 3D Deconvolutional Neural Network", "text": "After receiving an input image sequence x1, x2, \u00b7 \u00b7 \u00b7, xT, the 3D-LSTM passes the hidden state hT to a decoder, which increases the resolution of the hidden state by applying 3D turns, nonlinearity and 3D unpooling [41] until it reaches the target output resolution. As with encoders, we propose a simple decoder network with 5 turns and a deep residual version with 4 residual connections, followed by a final folding. After the last layer in which the activation reaches the target output resolution, we convert the final activation V-RNvox-Nvox-Nvox-Nvox-2 to the occupancy probability p (i, j-k) of the voxel cell (i, j-k) using voxel-wise softmax."}, {"heading": "3.4 Loss: 3D Voxel-wise Softmax", "text": "s leave the final output at each voxel (i, j, k) to be Bernoulli distributions [1 \u2212 p (i, j, k), p (i, j, k)], omitting the dependence on the input X = {xt} t \u00b2 {1,..., T}, and leaving out the corresponding basic truth allocation y (i, j, k) (1 \u2212 p (i, j, k))) log (1 \u2212 p (i, j, k)) (16)"}, {"heading": "4 Implementation", "text": "Data augmentation: In the training, we used 3D CAD models to generate input images and basic voxel allocation maps. First, we rendered the CAD models with a transparent background and then randomly expanded the input images with snippets from the PASCAL VOC 2012 dataset [42]. In addition, we colored the color of the models and randomly translated the images. Note that all viewpoints were randomly scanned. Training: In the training of the network, we used variable length inputs ranging from an image to an arbitrary number of images. Specifically, the input length (number of views) for each training example within a single mini-stack was kept constant, but the input length of the training examples in different mini-stacks varied randomly. This allowed the network to perform both single and multi-view reconstructions. In the training, we calculated both the loss of input power and the loss of memory at the end of the training."}, {"heading": "5 Experiments", "text": "This section validates and demonstrates the capability of our approach using multiple experiments based on the data sets described in Section 5.1. First, we show the results of various variations of the 3D-R2N2 (Section 5.2). Second, we compare the performance of our network on the PASCAL 3D dataset [45] with that of a state-of-the-art process by Kar et al. [32] for reconstructing real images from a single view (Section 5.3). Then, we show the network's ability to reconstruct multiple views on the ShapeNet dataset [1] and the online product dataset [46] (Section 5.4, Section 5.5). Finally, we compare our approach with a multi-view stereo method for reconstructing objects with different texture layers and viewpoints (Section 5.6)."}, {"heading": "5.1 Dataset", "text": "ShapeNet: The ShapeNet dataset consists of 50,000 models and 13 main categories (see Table 5 (c) for a full list).We divided the dataset into training and test sets, using 4 / 5 for training and the remaining 1 / 5 for testing. We refer to these two datasets as the ShapeNet training set and test set throughout the experiment section. PASCAL 3D: The PASCAL 3D dataset consists of PASCAL 2012 detection images augmented with 3D CAD model alignment [45].Online Products: The dataset [46] contains images of 23,000 items sold online. MVS and SFM methods fail on these images (PASCAL 3D datasets).Since the dataset does not have the ground-level 3D CAD models, we have used only the datasets for qualitative evaluation."}, {"heading": "5.2 Network Structures Comparison", "text": "We tested 5 variations of our 3D-R2N2 as in Section 3. The first four networks are based on the standard feed-forward CNN (Fig. 2 above) and the fifth network is the residual network (Fig. 2 below). For the first four networks, we used either GRU or LSTM units and varied the folding core as either 1 x 1 x 1 [3D-LSTM / GRU-3] or 3 x 3 x 3 [3D-LSTM / GRU-3]. The residual network used GRU units and 3 x 3 x 3 volumes [Res3D-GRU-3]. These networks were trained on the ShapeNet training set and tested on the ShapeNet test set. We used 5 views in the experiment. Table 1 shows the results. We found that 1) the GRU-based networks outperform the LSTM-based networks, 2) that the networks with adjacent recursive connections (3 x 3 x 3 volumes) have networks that have no additional volume, 1 x 1 and 1 x."}, {"heading": "5.3 Single Real-World Image Reconstruction", "text": "To make a quantitative comparison, we used images from the PASCAL VOC 2012 dataset [42] and the corresponding 3D models from the PASCAL 3D + dataset [45]. We performed the experiments with the same configuration as Kar et al. except that we allowed the Kar et al. method to use grounded object segmentation masks and keypoint labels as additional inputs for both training and testing. We refined a network trained on the ShapeNet dataset with PASCAL 3D +. We used the PASCAL 3D + validation set to find hyperparameters such as the number of fine tunings and the voxelization threshold."}, {"heading": "5.4 Multi-view Reconstruction Evaluation", "text": "In this section, we report on a quantitative assessment of the performance of our network in reconstructing multiple views on the ShapeNet testset.Experiment Setup. We used the network [Res3D-GRU-3] in this experiment. We evaluated the network using the ShapeNet testset. The test set consisted of 8725 models in 13 important categories. We rendered five random views for each model and applied a uniform color background to the image. We report both Softmax losses and intersections in the Union (IoU) with a voxelization threshold of 0.4 between the predicted and actual voxel models. Overall results. We first examine the quality of the reconstructed models among different viewpoints. Fig. 5 (a) and (b) show that the reconstruction quality improves when the number of views increases. The fact that the marginal gain coincides with our assumption that each additional view provides less information, as two random views are very likely."}, {"heading": "5.5 Reconstructing Real World Images", "text": "In this experiment, we tested our network using the Online Products dataset for qualitative evaluation. Images that were not square were filled in with white pixels.Fig.6 (c) shows some sample reconstructions. The result shows that the network is able to reconstruct real objects using only synthetic data as training samples. It also shows that the network improves the reconstructions after seeing additional views of the objects.An example is the reconstruction of the couch as shown in row 1. The initial side view of the couch led the network to believe that it was a single-seated sofa, but after seeing the front of the couch, the network immediately refined its reconstruction to reflect the observation. Similar behaviors are shown in other examples as well. Some error cases are shown in Fig.6 (d)."}, {"heading": "5.6 Multi View Stereo(MVS) vs. 3D-R2N2", "text": "In this experiment, we compare our approach to an MVS method for reconstructing objects that are of different layers with different number of views. MVS methods are limited by the accuracy of feature correspondence in different views. Therefore, they tend to fail to reconstruct textureless objects or images from sparsely positioned camera perspectives. In contrast, our method does not require exact feature correspondence or adjacent camera perspectives. We use high-quality CAD models of 4 different categories and augment their texture thicknesses by manually editing their textures."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a novel architecture that combines the 3D reconstruction of single and multiple views in a single framework. Although our network can process input of varying lengths, we showed that it outperforms the method of Kar et al. [32] in reconstructing single views with real-world images. We also tested the network's ability to perform multiple views using the ShapeNet dataset [1] and the online product dataset [46], showing that the network is able to gradually improve its reconstructions when it sees more views of an object. Finally, we analyzed the network's performance in reconstructing multiple views and found that our method can produce accurate reconstructions when techniques such as MVS fail. In summary, our network does not require a minimum number of input images to produce a plausible reconstruction, and is able to overcome past challenges in dealing with images that have insufficient texture or texture."}, {"heading": "7 Acknowledgements", "text": "We would like to thank the NSF CAREER Scholarship N.1054127 and Toyota Award # 122282 for their support, as well as the Korea Foundation for Advanced Studies and the NSF GRFP for their support."}], "references": [{"title": "ShapeNet: An Information-Rich 3D Model Repository", "author": ["A.X. Chang", "T. Funkhouser", "L. Guibas", "P. Hanrahan", "Q. Huang", "Z. Li", "S. Savarese", "M. Savva", "S. Song", "H. Su", "J. Xiao", "L. Yi", "F. Yu"], "venue": "Technical report, Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A large dataset of object scans", "author": ["S. Choi", "Q.Y. Zhou", "S. Miller", "V. Koltun"], "venue": "arXiv preprint arXiv:1602.02481", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic 3d model acquisition and generation of new images from video sequences", "author": ["A. Fitzgibbon", "A. Zisserman"], "venue": "Signal Processing Conference (EUSIPCO 1998), 9th European, IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "A quasi-dense approach to surface reconstruction from uncalibrated images", "author": ["M. Lhuillier", "L. Quan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 27(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Building rome in a day", "author": ["S. Agarwal", "N. Snavely", "I. Simon", "S.M. Seitz", "R. Szeliski"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Lsd-slam: Large-scale direct monocular slam", "author": ["J. Engel", "T. Sch\u00f6ps", "D. Cremers"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The structure-from-motion reconstruction pipeline\u2013a survey with focus on short image sequences", "author": ["K. H\u00e4ming", "G. Peters"], "venue": "Kybernetika 46(5)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual simultaneous localization and mapping: a survey", "author": ["J. Fuentes-Pacheco", "J. Ruiz-Ascencio", "J.M. Rend\u00f3n-Mancha"], "venue": "Artificial Intelligence Review 43(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision 60(2)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Ordinal measures for image correspondence", "author": ["D.N. Bhat", "S.K. Nayar"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20(4)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Reconstruction of textureless regions using structure from motion and image-based interpolation", "author": ["P. Saponaro", "S. Sorensen", "S. Rhein", "A.R. Mahoney", "C. Kambhamettu"], "venue": "Image Processing (ICIP), 2014 IEEE International Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Photorealistic scene reconstruction by voxel coloring", "author": ["S.M. Seitz", "C.R. Dyer"], "venue": "International Journal of Computer Vision 35(2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "A theory of shape by space carving", "author": ["K.N. Kutulakos", "S.M. Seitz"], "venue": "International Journal of Computer Vision 38(3)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Methods for volumetric reconstruction of visual scenes", "author": ["Gregory G Slabaugh", "T.M. W Bruce Culbertson", "M.R. Stevens", "R.W. Schafer"], "venue": "International Journal of Computer Vision 57(3)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Towards robust voxel-coloring: Handling camera calibration errors and partial emptiness of surface voxels", "author": ["Z. Anwar", "F. Ferrie"], "venue": "Proceedings of the 18th International Conference on Pattern Recognition - Volume 01. ICPR \u201906, Washington, DC, USA, IEEE Computer Society", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A probabilistic framework for space carving", "author": ["A. Broadhurst", "T.W. Drummond", "R. Cipolla"], "venue": "Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on. Volume 1., IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Dense reconstruction using 3d object shape priors", "author": ["A. Dame", "V.A. Prisacariu", "C.Y. Ren", "I. Reid"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Dense object reconstruction using semantic priors", "author": ["Y. Bao", "M. chandraker", "Y. Lin", "S. Savarese"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine perception of three-dimensional solids", "author": ["G.R. Lawrence"], "venue": "Ph. D. Thesis", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1963}, {"title": "Description and recognition of curved objects", "author": ["R. Nevatia", "T.O. Binford"], "venue": "Artificial Intelligence 8(1)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1977}, {"title": "Detailed 3d representations for object modeling and recognition, TPAMI", "author": ["M.Z. Zia", "M. Stark", "B. Schiele", "K. Schindler"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Completing 3d object shape from one depth image", "author": ["J. Rock", "T. Gupta", "J. Thorsen", "J. Gwak", "D. Shin", "D. Hoiem"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Enriching object detection with 2d-3d registration and continuous viewpoint estimation", "author": ["C. Bongsoo Choy", "M. Stark", "S. Corbett-Davies", "S. Savarese"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Face recognition based on fitting a 3d morphable model", "author": ["V. Blanz", "T. Vetter"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 25(9)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "2d vs", "author": ["I. Matthews", "J. Xiao", "S. Baker"], "venue": "3d deformable face models: Representational power, construction, and real-time fitting. International journal of computer vision 75(1)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "3d face reconstruction from a single image using a single reference face shape", "author": ["I. Kemelmacher-Shlizerman", "R. Basri"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(2)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous monocular 2d segmentation, 3d pose recovery and 3d reconstruction", "author": ["V.A. Prisacariu", "A.V. Segal", "I. Reid"], "venue": "Computer Vision\u2013ACCV 2012. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "A nonrigid kernel-based framework for 2d-3d pose estimation and 2d image segmentation", "author": ["R. Sandhu", "S. Dambreville", "A. Yezzi", "A. Tannenbaum"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(6)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A.Y. Ng"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 31(5)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic photo pop-up", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "ACM transactions on graphics (TOG) 24(3)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Reconstructing pascal voc", "author": ["S. Vicente", "J. Carreira", "L. Agapito", "J. Batista"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Category-specific object reconstruction from a single image", "author": ["A. Kar", "S. Tulsiani", "J. Carreira", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput. 9(8)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}, {"title": "Lstm neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "INTERSPEECH.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems 27.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks 5(2)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A.Dosovitskiy", "J.T.Springenberg", "T.Brox"], "venue": "IEEE International Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv e-prints", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond pascal: A benchmark for 3d object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on, IEEE", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep metric learning via lifted structured feature embedding", "author": ["H.O. Song", "Y. Xiang", "S. Jegelka", "S. Savarese"], "venue": "ArXiv e-prints", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Patchmatch: A randomized correspondence algorithm for structural image editing", "author": ["C. Barnes", "E. Shechtman", "A. Finkelstein", "D. Goldman"], "venue": "ACM Transactions on Graphics-TOG 28(3)", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Global fusion of relative motions for robust, accurate and scalable structure from motion", "author": ["P. Moulon", "P. Monasse", "R. Marlet"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "This trend has been boosted now that 3D printing is a democratized technology and 3D acquisition methods are accurate and efficient [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "Moreover, the trend is also coupled with the diffusion of large scale repositories of 3D object models such as ShapeNet [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 68, "endOffset": 77}, {"referenceID": 3, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 68, "endOffset": 77}, {"referenceID": 4, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 68, "endOffset": 77}, {"referenceID": 5, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 68, "endOffset": 77}, {"referenceID": 6, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 146, "endOffset": 151}, {"referenceID": 7, "context": "One typical assumption is that features can be matched across views [3,4,5,6] as hypothesized by the majority of the methods based on SFM or SLAM [7,8].", "startOffset": 146, "endOffset": 151}, {"referenceID": 8, "context": "It has been demonstrated (for instance see [9]) that if the viewpoints are separated by a large baseline, establishing (traditional) feature correspondences is extremely problematic due to local appearance changes or self-occlusions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Moreover, lack of texture on objects and specular reflections also make the feature matching problem very difficult [10,11].", "startOffset": 116, "endOffset": 123}, {"referenceID": 10, "context": "Moreover, lack of texture on objects and specular reflections also make the feature matching problem very difficult [10,11].", "startOffset": 116, "endOffset": 123}, {"referenceID": 11, "context": "In order to circumvent issues related to large baselines or non-Lambertian surfaces, 3D volumetric reconstruction methods such as space carving [12,13,14,15] and their probabilistic extensions [16] have become popular.", "startOffset": 144, "endOffset": 157}, {"referenceID": 12, "context": "In order to circumvent issues related to large baselines or non-Lambertian surfaces, 3D volumetric reconstruction methods such as space carving [12,13,14,15] and their probabilistic extensions [16] have become popular.", "startOffset": 144, "endOffset": 157}, {"referenceID": 13, "context": "In order to circumvent issues related to large baselines or non-Lambertian surfaces, 3D volumetric reconstruction methods such as space carving [12,13,14,15] and their probabilistic extensions [16] have become popular.", "startOffset": 144, "endOffset": 157}, {"referenceID": 14, "context": "In order to circumvent issues related to large baselines or non-Lambertian surfaces, 3D volumetric reconstruction methods such as space carving [12,13,14,15] and their probabilistic extensions [16] have become popular.", "startOffset": 144, "endOffset": 157}, {"referenceID": 15, "context": "In order to circumvent issues related to large baselines or non-Lambertian surfaces, 3D volumetric reconstruction methods such as space carving [12,13,14,15] and their probabilistic extensions [16] have become popular.", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "Thus, shape prior-based methods can work with fewer images and with fewer assumptions on the object reflectance function as shown in [17,18].", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "Thus, shape prior-based methods can work with fewer images and with fewer assumptions on the object reflectance function as shown in [17,18].", "startOffset": 133, "endOffset": 140}, {"referenceID": 18, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 117, "endOffset": 124}, {"referenceID": 19, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 117, "endOffset": 124}, {"referenceID": 20, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 176, "endOffset": 186}, {"referenceID": 21, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 176, "endOffset": 186}, {"referenceID": 22, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 176, "endOffset": 186}, {"referenceID": 23, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 285, "endOffset": 295}, {"referenceID": 24, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 285, "endOffset": 295}, {"referenceID": 25, "context": "The shape priors are typically encoded in the form of simple 3D primitives as demonstrated by early pioneering works [19,20] or learned from rich repositories of 3D CAD models [21,22,23], whereby the concept of fitting 3D models to images of faces was explored to a much larger extent [24,25,26].", "startOffset": 285, "endOffset": 295}, {"referenceID": 26, "context": "Sophisticated mathematical formulations have also been introduced to adapt 3D shape models to observations with different degrees of supervision [27] and different regularization strategies [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "Sophisticated mathematical formulations have also been introduced to adapt 3D shape models to observations with different degrees of supervision [27] and different regularization strategies [28].", "startOffset": 190, "endOffset": 194}, {"referenceID": 28, "context": "Inspired by early works that used machine learning to learn a 2D-to-3D mapping for scene understanding [29,30], data driven approaches have been recently proposed to solve the daunting problem of recovering the shape of an object from just a single image [31,32] for a given number of object categories.", "startOffset": 103, "endOffset": 110}, {"referenceID": 29, "context": "Inspired by early works that used machine learning to learn a 2D-to-3D mapping for scene understanding [29,30], data driven approaches have been recently proposed to solve the daunting problem of recovering the shape of an object from just a single image [31,32] for a given number of object categories.", "startOffset": 103, "endOffset": 110}, {"referenceID": 30, "context": "Inspired by early works that used machine learning to learn a 2D-to-3D mapping for scene understanding [29,30], data driven approaches have been recently proposed to solve the daunting problem of recovering the shape of an object from just a single image [31,32] for a given number of object categories.", "startOffset": 255, "endOffset": 262}, {"referenceID": 31, "context": "Inspired by early works that used machine learning to learn a 2D-to-3D mapping for scene understanding [29,30], data driven approaches have been recently proposed to solve the daunting problem of recovering the shape of an object from just a single image [31,32] for a given number of object categories.", "startOffset": 255, "endOffset": 262}, {"referenceID": 32, "context": "Inspired by the success of Long Short-Term Memory (LSTM) [33] networks [34,35] as well as recent progress in single-view 3D reconstruction using Convolutional Neural Networks [36,37], we propose a novel architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "Inspired by the success of Long Short-Term Memory (LSTM) [33] networks [34,35] as well as recent progress in single-view 3D reconstruction using Convolutional Neural Networks [36,37], we propose a novel architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).", "startOffset": 71, "endOffset": 78}, {"referenceID": 34, "context": "Inspired by the success of Long Short-Term Memory (LSTM) [33] networks [34,35] as well as recent progress in single-view 3D reconstruction using Convolutional Neural Networks [36,37], we propose a novel architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).", "startOffset": 71, "endOffset": 78}, {"referenceID": 35, "context": "Inspired by the success of Long Short-Term Memory (LSTM) [33] networks [34,35] as well as recent progress in single-view 3D reconstruction using Convolutional Neural Networks [36,37], we propose a novel architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).", "startOffset": 175, "endOffset": 182}, {"referenceID": 36, "context": "Inspired by the success of Long Short-Term Memory (LSTM) [33] networks [34,35] as well as recent progress in single-view 3D reconstruction using Convolutional Neural Networks [36,37], we propose a novel architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).", "startOffset": 175, "endOffset": 182}, {"referenceID": 31, "context": "\u2013 Our extensive experimental analysis shows that our reconstruction framework outperforms the state-of-the-art method for single-view reconstruction [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 32, "context": "One of the most successful implementations of the hidden states of an RNN is the Long Short Term Memory (LSTM) unit [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 32, "context": "An LSTM unit explicitly controls the flow from input to output, allowing the network to overcome the vanishing gradient problem [33,38].", "startOffset": 128, "endOffset": 135}, {"referenceID": 37, "context": "An LSTM unit explicitly controls the flow from input to output, allowing the network to overcome the vanishing gradient problem [33,38].", "startOffset": 128, "endOffset": 135}, {"referenceID": 38, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "We use two versions of 3D-R2N2: (top) a shallow network and (bottom) a deep residual network [40].", "startOffset": 93, "endOffset": 97}, {"referenceID": 39, "context": "Motivated by a recent study [40], we also created a deep residual variation of the first network and report the performance of this variation in Section 5.", "startOffset": 28, "endOffset": 32}, {"referenceID": 40, "context": "After receiving an input image sequence x1, x2, \u00b7 \u00b7 \u00b7 , xT , the 3D-LSTM passes the hidden state hT to a decoder, which increases the hidden state resolution by applying 3D convolutions, non-linearities, and 3D unpooling [41] until it reaches the target output resolution.", "startOffset": 221, "endOffset": 225}, {"referenceID": 41, "context": "We first rendered the CAD models with a transparent background and then augmented the input images with random crops from the PASCAL VOC 2012 dataset [42].", "startOffset": 150, "endOffset": 154}, {"referenceID": 40, "context": "For deconvolution, we followed the unpooling scheme presented in [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "We used Theano [43] to implement our network and used Adam [44] for the SGD update rule.", "startOffset": 15, "endOffset": 19}, {"referenceID": 43, "context": "We used Theano [43] to implement our network and used Adam [44] for the SGD update rule.", "startOffset": 59, "endOffset": 63}, {"referenceID": 44, "context": "Next, we compare the performance of our network on the PASCAL 3D [45] dataset with that of a state-of-the-art method by Kar et al.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "[32] for single-view real-world image reconstruction (Section 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Then we show the network\u2019s ability to perform multi-view reconstruction on the ShapeNet dataset [1] and the Online Products dataset [46] (Section 5.", "startOffset": 96, "endOffset": 99}, {"referenceID": 45, "context": "Then we show the network\u2019s ability to perform multi-view reconstruction on the ShapeNet dataset [1] and the Online Products dataset [46] (Section 5.", "startOffset": 132, "endOffset": 136}, {"referenceID": 44, "context": "PASCAL 3D: The PASCAL 3D dataset is composed of PASCAL 2012 detection images augmented with 3D CAD model alignment [45].", "startOffset": 115, "endOffset": 119}, {"referenceID": 45, "context": "Online Products: The dataset [46] contains images of 23,000 items sold online.", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "To make a quantitative comparison, we used images from the PASCAL VOC 2012 dataset [42] and its corresponding 3D models from the PASCAL 3D+ dataset [45].", "startOffset": 83, "endOffset": 87}, {"referenceID": 44, "context": "To make a quantitative comparison, we used images from the PASCAL VOC 2012 dataset [42] and its corresponding 3D models from the PASCAL 3D+ dataset [45].", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "[32] Input Ground Truth Ours Kar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32]", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] Input Ground Truth Ours Kar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32]", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] is trained/tested per category and takes ground-truth object segmentation masks and keypoint labels as additional input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] in every category.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] took ground-truth object segmentation masks and keypoint labels as additional inputs for both training and testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Sample reconstructions on (a) the ShapeNet [1] testing set and (c) the Online Products dataset [46].", "startOffset": 43, "endOffset": 46}, {"referenceID": 45, "context": "Sample reconstructions on (a) the ShapeNet [1] testing set and (c) the Online Products dataset [46].", "startOffset": 95, "endOffset": 99}, {"referenceID": 46, "context": "We used a Patch-Match [48]-based off-the-shelf implementation [49] as the MVS method.", "startOffset": 22, "endOffset": 26}, {"referenceID": 47, "context": "The MVS method takes images along with their camera positions estimated by Global SFM [50] and outputs the reconstructed model.", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "[32] in single-view reconstruction using real-world images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We further tested the network\u2019s ability to perform multi-view reconstruction on the ShapeNet dataset [1] and the Online Products dataset [46], which showed that the network is able to incrementally improve its reconstructions as it sees more views of an object.", "startOffset": 101, "endOffset": 104}, {"referenceID": 45, "context": "We further tested the network\u2019s ability to perform multi-view reconstruction on the ShapeNet dataset [1] and the Online Products dataset [46], which showed that the network is able to incrementally improve its reconstructions as it sees more views of an object.", "startOffset": 137, "endOffset": 141}], "year": 2016, "abstractText": "Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data [1]. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-theart methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).", "creator": "LaTeX with hyperref package"}}}