{"id": "1604.00466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Automatic Annotation of Structured Facts in Images", "abstract": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., &lt;flower, red&gt;), actions (e.g., &lt;baby, smile&gt;), interactions (e.g., &lt;man, walking, dog&gt;), and positional information (e.g., &lt;vase, on, table&gt;). The collected annotations are in the form of fact-image pairs (e.g.,&lt;man, walking, dog&gt; and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83\\% according to human judgment. %that we obtained using both Amazon Mechanical Turk and volunteer scientists in our laboratory. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms. %Our work enable large-scale fact-level understanding of images. %Based on human j evaluation shows that more than 83\\% of the automatically collected annotation %We focus on collecting higher order visual facts annotations which include attributed objects (e.g. &lt;car, black&gt;, &lt;flower, red&gt;, actions &lt;baby, smile&gt;, and interactions &lt;dog, riding, wave&gt;.", "histories": [["v1", "Sat, 2 Apr 2016 06:35:45 GMT  (8911kb,D)", "https://arxiv.org/abs/1604.00466v1", null], ["v2", "Tue, 5 Apr 2016 18:58:10 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v2", null], ["v3", "Fri, 8 Apr 2016 00:04:22 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["mohamed elhoseiny", "scott cohen", "walter chang", "brian price", "ahmed elgammal"], "accepted": false, "id": "1604.00466"}, "pdf": {"name": "1604.00466.pdf", "metadata": {"source": "CRF", "title": "Automatic Annotation of Structured Facts in Images", "authors": ["Mohamed Elhoseiny", "Scott Cohen", "Walter Chang", "Brian Price", "Ahmed Elgammal"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "People acquire visual knowledge by exposing themselves to both visual facts and semantic or language-based facts. < p > Facts, which we define as structures that provide attributes about an object, and / or the actions and interactions that that object can have with other objects. < p > We emphasize the idea of automatically collecting annotations for second-order and third-order visual facts, where second-order facts < S, P > are recognizable on-tributed objects (e.g., < S: Auto, P: red >) and singleframe actions (e.g., < S: Person, Jump >) third-order facts specify interactions (i.e., boy, petting, dog >). This structure is helpful for designing machine learning algorithms that learn from text data and allow us to model relationships between facts to collect them."}, {"heading": "2 Motivation", "text": "Our goal in proposing this automatic method is to generate factual language and visual comments > factual comments in order to examine language and vision for a structured understanding of visual facts. Existing systems are already working to directly relate captions to the overall picture, such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015). This raises a key question about our work: Why is it useful to collect such a large amount of structured facts in comparison to captioning systems? We illustrate the difference between factual comments that motivate this work using the example in Figure 1. Chapter-level learning systems correlate captions such as the descriptions in Figure 1 (Figure 1)."}, {"heading": "3 Approach Overview", "text": "In fact, the people he mentions, who are able to identify themselves, are a group of people who are able to identify themselves, and a group of people who are able to identify themselves. (...) It is not a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves. \"(...) It is a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves.\" (...) It is a group of people who are able to identify themselves. (...) It is a group of people who are able to identify themselves."}, {"heading": "4 Fact Extraction from Captions", "text": "We extract facts from the descriptions with Clausie (Del Corro and Gemulla, 2013) and our proposed SedonaNLP systems. < < p > p > p > p > p > p > p > p > p > p \"p > p > p > p > p > p > p > p > p > p > p > p > p > p > p\" p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" and \"p\" p \"p\" p \"p\" p \"p\" p \"p\" and \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \") p\" p \"p\" p \"p\" p \"and\" p \"p\" p \"p\" and \"p\" p \"p\"."}, {"heading": "5 Locating facts in the Image", "text": "In this section we present details of the second step of our automatic annotation process introduced in paragraph 3. After the candidate facts have been extracted from the sentences, we get a set of Fs = {f il}, i = 1: Ns for statement s, where Ns is the number of candidate factors extracted, and from the statements either Clausie (Del Corro and Gemulla, 2013) or Sedona-3.0. The localization step is further divided into two steps. The mapping step maps nouns in the facts onto candidate fields in the image. The grounding step processes any fact associated with the candidate fields and, if successful, outputs a final delimiting box. Both steps are described in the following subsections."}, {"heading": "5.1 Mapping", "text": "In fact, it is a purely reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project bordering on the limits of reason. (...) In fact, it is as if it is a reactionary project. (...) In fact, it is as if it is a reactionary project. (...) It is as if it is a reactionary project. (...) It is as if it is a project, that it is a project. (...) It is as if it is a project. (...) It is as if it is a project. (...) It is as if it is a project. (...) It is as if it is a project. (...)"}, {"heading": "5.2 Grounding", "text": "The grounding process is the process of linking the individual fields f * F * s with an image fv = > Facts by mapping fl to a delimiting field in the given MS COCO image scene. < If the grounding process is relatively different for the two records due to the difference in entity annotations. < M COCO (Training and Validation sets) In the MS COCO data set, one challenging aspect is that the S or O record may be singular, plural or related to the scene. This means that an S could be mapped to multiple fields in the image. < M \"Cards for Multiple Fields\" Person. \"In addition, this case could exist for both the S and the O. In cases where either S or O is plural, the bounding box mapped to the field is the union of all candidates in biS. < The grounding then proceeds as follows < S matters are followed as follows."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Human Subject Evaluation", "text": "We suggest three questions to evaluate each comment < (Q1) Is the extracted fact correct (yes / no)? The purpose of this question is to evaluate errors that were detected in the first step and from which facts from Sedona or Clausie are extracted. (Q2) How exactly is the field assigned to a particular fact (a to g)? a (about right), b (about right), c (a bit large), d (a bit small), d (too small), e (too big), f (completely wrong), g (fact does not exist or others) Our instructions on these questions to participants can be found in this anonymous judgment (Eval, 2016), b (a bit large), c (too small), d (too small), e (completely wrong), g (fact does not exist)."}, {"heading": "6.2 Hardness Evaluation of the collected data", "text": "We begin by defining the hardness of an extracted fact in our case and its dependence on the fact type. Our method captures second- and third-order facts. We refer to candidates as all instances of the entity in the image that correspond to the object type of a second-order fact < S, P > or a third-order fact < S, P, O >. We refer to candidate objects as all instances in the image that correspond to the object type of a third-order fact < S, P, O >. The selection of candidates and candidate objects is part of our method, which we describe in detail in Sec 5 < S, P >. We define the hardness of second-order facts by the number of candidates and the hardness of third-order facts < COS, P, O >."}, {"heading": "7 Conclusion", "text": "We present a new method whose main purpose is to collect visual fact annotations through a linguistic approach. The data collected helps train visual systems at the fact level with the variety of facts described by a caption. We demonstrated the effectiveness of the proposed method by extracting hundreds of thousands of fact annotations from MSCOCO and Flickr30K data sets. We verified and analyzed the data collected and showed that more than 80% of the data collected is good for training visual systems."}, {"heading": "8 SAFA Qualitative Results", "text": "On the left is the input, and on the right is the output. SAFA failures"}, {"heading": "8.1 Statistics on collected data (MTurk subset)", "text": "In order to examine how the method behaves in both simple and hard examples, in this section statistics of successfully extracted facts are presented and these are related to the hardness of the extraction of these facts. We begin by defining the hardness of an extracted fact in our case and its dependence on the fact type. Our method captures second- and third-order facts. We designate candidates as all instances of the entity in the image that correspond to the object type of a third-order fact < S, P > or a third-order fact < S, P, O >. We designate candidates as all instances in the image that correspond to the object type of a third-order fact < S, P, O >. The selection of candidates and candidate objects is part of our method, which we describe in detail in Sec. 4. We define the hardness of the second-order objects according to the number of candidates and the hardness of the third-order facts according to the order of the candidate objects."}, {"heading": "8.1.1 Statistics on both all MTurk data compared to the subset where Q3 = about right for each dataset", "text": "The following figures show statistics on the facts verified by MTurkers (from the union of all datasets). Figure 16 shows a histogram of the difficulties for all the examples evaluated by MTurk. Figure 17 shows a similar histogram, but only for a subset of the facts verified by the Turks by Q3 as (approximately right) MTurker. The figures show that the method is capable of handling more than 150 ways of grounding problem cases."}, {"heading": "8.1.2 Statistics on Q3 = about right for each dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 SAFA collected data statistics (the whole 380K collected data)", "text": "This section shows the number of subject and object statistics of candidates for all successfully determined facts for all MS COCO (Association of Education and Validation Subsets) and Flickr30K data sets. SAFA collects second- and third-order facts. We refer to candidates as all instances of the company that correspond to the subject of a second-order fact < S, P > or a third-order fact < S, P, O >. Our method is designed to achieve a high degree of precision so that the determined facts are as accurate as possible as we have shown in our experiments. In all the following numbers, the Y axis is the number of facts for each trash. The X-axes are trash bodies that (1) correspond to the subjects for the second and third order & < < third-order subjects (2 > fact pairs for the third order)."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["Chen et al.2013] Xinlei Chen", "Ashish Shrivastava", "Arpan Gupta"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Clausie: clause-based open information extraction", "author": ["Del Corro", "Gemulla2013] Luciano Del Corro", "Rainer Gemulla"], "venue": null, "citeRegEx": "Corro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Corro et al\\.", "year": 2013}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["Alireza Farhadi", "Carlos Guestrin"], "venue": null, "citeRegEx": "Divvala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Divvala et al\\.", "year": 2014}, {"title": "Prismatic: Inducing knowledge from a large scale lexicalized relation resource", "author": ["Fan et al.2010] James Fan", "David Ferrucci", "David Gondek", "Aditya Kalyanpur"], "venue": "In NAACL HLT. Association for Computational Linguistics", "citeRegEx": "Fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Armand Joulin", "Fei Fei F Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models. TACL", "author": ["Kiros et al.2015] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Combining local context and wordnet similarity for word sense identification", "author": ["Leacock", "Chodorow1998] Claudia Leacock", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR", "author": ["Mao et al.2015] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Sherlock: Scalable fact learning", "author": ["Scott Cohen"], "venue": null, "citeRegEx": "Elhoseiny and Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Elhoseiny and Cohen.", "year": 2016}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Triplet extraction from sentences", "author": ["Rusu et al.2007] Delia Rusu", "Lorand Dali", "Blaz Fortuna", "Marko Grobelnik", "Dunja Mladenic"], "venue": "In International Multiconference\u201d Information Society-IS", "citeRegEx": "Rusu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Xiao et al.2010] Jianxiong Xiao", "James Hays", "Krista Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "(Chen et al., 2013) showed that visual concepts, from a predefined ontology, can be learned by querying the web about these concepts using image-web search engines.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "More recently, (Divvala et al., 2014) presented an approach to learn", "startOffset": 15, "endOffset": 37}, {"referenceID": 1, "context": "It is further restricting to define it based on a predefined ontology such as (Chen et al., 2013) or a particular object such as (Divvala et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 3, "context": ", 2013) or a particular object such as (Divvala et al., 2014).", "startOffset": 39, "endOffset": 61}, {"referenceID": 8, "context": "MS COCO image caption dataset (Lin et al., 2014) and the newly collected Flickr30K entities (Plummer et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": ", 2014) and the newly collected Flickr30K entities (Plummer et al., 2015).", "startOffset": 51, "endOffset": 73}, {"referenceID": 5, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 6, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 17, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 19, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 11, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 0, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 9, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 15, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 6, "context": "The results shows that fact-level learning is superior compared to caption-level learning like (Kiros et al., 2015), as shown in Table 4 in (Mohamed Elhoseiny, 2016) (16.", "startOffset": 95, "endOffset": 115}, {"referenceID": 6, "context": "48% for (Kiros et al., 2015)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "For the MS COCO dataset (Lin et al., 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 12, "context": ", 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 18, "context": ", 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al., 2010) (e.", "startOffset": 453, "endOffset": 472}, {"referenceID": 16, "context": "For instance, (Rusu et al., 2007) describe a basic set of methods based on traversing the parse graphs generated by various commonly available parsers.", "startOffset": 14, "endOffset": 33}, {"referenceID": 4, "context": "Larger scale text mining methods for learning structured facts for question answering have been developed in the IBM Watson PRISMATIC framework (Fan et al., 2010).", "startOffset": 144, "endOffset": 162}, {"referenceID": 10, "context": "While parsers such as CoreNLP (Manning et al., 2014) are available to generate comprehensive dependency graphs, these have historically required significant processing time for each sentence or have traded accuracy for performance.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "3 for MSCOCO dataset (Lin et al., 2014).", "startOffset": 21, "endOffset": 39}, {"referenceID": 4, "context": "By limiting a frame to be only a small subset of a complex parse tree, we reduce the chance of error parse in each frame (Fan et al., 2010).", "startOffset": 121, "endOffset": 139}], "year": 2016, "abstractText": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms.", "creator": "LaTeX with hyperref package"}}}