{"id": "1604.00545", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "The AGI Containment Problem", "abstract": "There is considerable uncertainty about what properties, capabilities and motivations future AGIs will have. In some plausible scenarios, AGIs may pose security risks arising from accidents and defects. In order to mitigate these risks, prudent early AGI research teams will perform significant testing on their creations before use. Unfortunately, if an AGI has human-level or greater intelligence, testing itself may not be safe; some natural AGI goal systems create emergent incentives for AGIs to tamper with their test environments, make copies of themselves on the internet, or convince developers and operators to do dangerous things. In this paper, we survey the AGI containment problem - the question of how to build a container in which tests can be conducted safely and reliably, even on AGIs with unknown motivations and capabilities that could be dangerous. We identify requirements for AGI containers, available mechanisms, and weaknesses that need to be addressed.", "histories": [["v1", "Sat, 2 Apr 2016 19:26:05 GMT  (316kb)", "https://arxiv.org/abs/1604.00545v1", null], ["v2", "Thu, 12 May 2016 15:37:38 GMT  (318kb)", "http://arxiv.org/abs/1604.00545v2", null], ["v3", "Wed, 13 Jul 2016 14:54:16 GMT  (318kb)", "http://arxiv.org/abs/1604.00545v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["james babcock", "janos kramar", "roman yampolskiy"], "accepted": false, "id": "1604.00545"}, "pdf": {"name": "1604.00545.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Roman Yampolskiy"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 4.00 545v 3 [cs.A I] 1 3Ju l 2"}, {"heading": "1 Introduction", "text": "Some commentators are reassured by the observation [16] that the current AGI software, like other software, can be easily interrupted, for example by shutting down the hardware. However, it is a mistake to assume that this will always be enough, because an AGI that understands its situation can develop strategies to avoid or circumvent this security measure. Containment, in short, is the problem of doing this work: preventing AGI from manipulating its environment without authorization, and maintaining the integrity of AGI's observations during testing. Existing work by Yampolskiy [17], Yudkowsky [19], Christiano [2] and others have highlighted the challenges of mitigating super-intelligent AGI and have begun to explore some possibilities that we find very challenging, and the measures proposed seem too onerous to implement through competitive AGI projects."}, {"heading": "2 Motivation", "text": "The first AGIs are likely to appear in an experimental environment. Consider an AGI that is being tested in the same way as AIs are being tested today, but that is more capable - say, as capable as one of the smartest humans. (Whether this is true of one of the first AGIs is uncertain, but there is reason to think it is plausible [18]. Consider, for example, the case of an AGI trained to play Super Mario Brothers. It chooses strategies that achieve high scores; this is its only optimization goal. Some contemporary projects pursue this goal [14]. It would begin by exploring familiar game mechanics such as running and jumping, but getting the best score it could give to thoroughly explore game mechanics. An observational AGI would notice some effects that we would consider \"bugs,\" although these effects are just as important as running and jumping."}, {"heading": "2.1 Testing and Experimentation in Safe AGI Development", "text": "To avoid undesirable behaviors like the above, we need the ability to think properly about the behavior of AGI software. As with other software, this requires that the system be well designed and understood by its creators. Experience teaches us that testing is also needed - not as a substitute for good design, but as a complement to it. At the same time, the AGI process requires experimenting with systems whose behavior is not fully understood; the first system that exhibits general intelligence at the human level can be a surprise, as it stems from a design for a narrower range of tasks. This experiment could then proceed without the caution and protections that would be appropriate for a human-level AGI."}, {"heading": "2.2 Emergent Goals of Test AGIs", "text": "The first human-level AGI is likely to be an experimental system, with some bugs being tested for the performance of the target task or for undesirable behavior. What goals and subgoals should we expect from such test AGIs? The range of possible goals is as broad as the range of things developers want to test, but most will likely include the tests and the test environment itself. Similar to Omohundro's list of AGI drives [11], we can infer some likely subgoals that would consider a wide range of AGI motivation systems (though certainly not all motivation systems) desirable, such as: - AGIs that know they exist and have goals would likely want AGIs like them to persist so that these goals could be met by these similar AGIs. - AGIs that know they want to pass (even if they are tested for a trait they lack). - AGIs that know they can take control of the AGIs when they want to test their environments."}, {"heading": "3 Requirements for an AGI Container", "text": "It is not the first time that we find ourselves in a situation where we are in a situation where we are in a situation where we are not able to find a solution. It is the first time that we are able to bring about a solution. It is the second time that we find a solution that is able to bring about a solution. It is the second time that we find a solution that is able to bring about a solution. It is the first time that we find a solution."}, {"heading": "3.1 Human Factors and Information Hazards", "text": "An AGI containment system is tasked with enforcing guidelines about who the AGI can communicate with and through what media. The container provides the mechanism; developers will have to set guidelines, and depending on how smart they think an AGI is appropriate. A sufficiently clever AGI could convince or cause a human operator to \"let them out of the box\"; humans are quite prone to manipulation [12]. In an experiment called the role-playing game as AGI, Eliezer Yudkowsky was able to convince others who act as gatekeepers to let him out of the box [19], despite a financial incentive not to do so, and this result was replicated by others [4,15]. This suggests that an untrusted AGI should not be allowed to conduct bidirectional conversations over a certain level of intelligence; and at some level of AGI intelligence beyond that, even log files and metrics should be used with great care when information is not properly contained in a more sensitive information system I can be."}, {"heading": "4 Defense in Depth", "text": "Since an AGI could potentially detect novel vulnerabilities, it is important that an AGI containment system remains secure, even if there are weaknesses in its components. Therefore, we propose that a containment system should have a profound defense [10], with multiple techniques that offer overlapping guarantees of integrity. Conceptually, these overlapping techniques act as layers, in the sense that the attack surfaces of an outer layer are only accessible for probing and attack when the integrity guarantees of the inner layers have been breached, which would be combined with intrusion detection systems [9,8], so that any attempt by an AGI to breach the integrity constraints imposed on it would be noticed, allowing developers to shut it down before using its progress to move forward. Ideally, detection systems would be implemented in higher layers, making them more difficult to detect and circumvent."}, {"heading": "5 Light, Medium and Heavy Containment", "text": "Some proposed containment mechanisms would impose high costs on AGI developers, such as limiting its output channel to a small number of multiple-choice issues. [17] While this restriction would be effective, it would seriously impede the usefulness of AGI and make its development and investigation much more difficult. On the other hand, some mechanisms impose negligible costs; for example, you could make the AGI computer accessible somewhere so that it can be turned off. There is a trade-off between the reliability of the containment system, the cost of implementation, and the computing costs. We call a containment mechanism light when the costs are low - comparable to the development and use of a software library; medium when its use entails discernible compromises in terms of performance or ease of development of AGI; or heavy when it severely impairs the performance, usefulness, usefulness, or AGI."}, {"heading": "6 Existing Mechanisms", "text": "It may be that most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "7 Topics for future work", "text": "As we discussed in Section 4, none of the mechanisms in Section 6 are sufficient on their own, but we think that a system built through the layering of these mechanisms can make a significant contribution to solving the containment problem in practice. In this section, we have collected some project proposals that will help solve some of the remaining issues. Code integrity enforcement: Write something that can inspect a paused (or running) virtual machine and determine whether it is executing code that is not on a whitelist, including support for just-in-time compiled languages like Lua. Customize off-the-shelf software: Set up containers using existing virtual machine software and OS sandbox mechanisms, make them work with conventional machine learning libraries, and document the configuration needed to make them safe. Hardened AGI infrastructure: Machine learning libraries are typically written, without much thought of in terms of security and performance, such as C +."}, {"heading": "8 Conclusion", "text": "We have outlined an approach to containing the AGI problem that seems feasible but requires additional work to actually implement it. Although there is likely to be a significant amount of time before anyone is able to develop an AGI that is generic enough and intelligent enough to make containment relevant, history shows that secure software takes significant time to mature. [13] The development of containment technologies is now likely to reveal problems that will affect the next and subsequent generations of AGI containers. There is considerable uncertainty about when AGI will be developed and what designs will be needed to avert adverse outcomes [13], but in most plausible scenarios, the use of AGI containment systems would help ensure that reliable AGI testing can also be performed with AGIs that approach the human ability to understand the world around them."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research 47, 253\u2013279", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Cryptographic boxes for unfriendly", "author": ["P. Christiano"], "venue": "ai (December", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Software random number generation based on race conditions", "author": ["A. Cole\u015fa", "R. Tudoran", "S. B\u0103nescu"], "venue": "Symbolic and Numeric Algorithms for Scientific Computing, 2008. SYNASC\u201908. 10th International Symposium on. pp. 439\u2013444. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Rowhammer.js: a remote software-induced fault attack in javascript", "author": ["D. Gruss", "C. Maurice", "S. Mangard"], "venue": "arXiv preprint arXiv:1507.06955", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Confidentiality issues on a gpu in a virtualized environment", "author": ["C. Maurice", "C. Neumann", "O. Heen", "A. Francillon"], "venue": "Financial Cryptography and Data Security, pp. 119\u2013135. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Anomaly detection based intrusion detection", "author": ["D. Novikov", "R.V. Yampolskiy", "L. Reznik"], "venue": "Information Technology: New Generations, 2006. ITNG 2006. Third International Conference on. pp. 420\u2013425. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Artificial intelligence approaches for intrusion detection", "author": ["D. Novikov", "R.V. Yampolskiy", "L. Reznik"], "venue": "Systems, Applications and Technology Conference, 2006. LISAT 2006. IEEE Long Island. pp. 1\u20138. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "The basic ai drives. AGI", "author": ["S. Omohundro"], "venue": "https://selfawaresystems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Constructing rich false memories of committing crime", "author": ["J. Shaw", "S. Porter"], "venue": "Psychological science 26(3), 291\u2013301", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Responses to catastrophic AGI risk: a survey", "author": ["K. Sotala", "R.V. Yampolskiy"], "venue": "Physica Scripta 90(1),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The Mario AI championship 2009\u20132012", "author": ["J. Togelius", "N. Shaker", "S. Karakovskiy", "G.N. Yannakakis"], "venue": "AI Magazine 34(3), 89\u201392", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial intelligence will not turn into a frankenstein\u2019s monster", "author": ["A. Winfield"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Leakproofing the singularity: Artificial intelligence confinement problem", "author": ["R. Yampolskiy"], "venue": "Journal of Consciousness Studies 19(1-2),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Intelligence explosion microeconomics", "author": ["E. Yudkowsky"], "venue": "Machine Intelligence Research Institute, accessed online October 23, 2015", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "The AI-box experiment (2002), http://www.yudkowsky.net/ singularity/aibox", "author": ["E.S. Yudkowsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}], "referenceMentions": [{"referenceID": 11, "context": "Some commentators are reassured [16] by the observation that current AGI software, like other software, can be interrupted easily, for example by powering down the hardware.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "Existing work by Yampolskiy [17], Yudkowsky [19], Christiano [2], and others has highlighted the challenges of containing superintelligent AGI and started to explore some possibilities.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Existing work by Yampolskiy [17], Yudkowsky [19], Christiano [2], and others has highlighted the challenges of containing superintelligent AGI and started to explore some possibilities.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "Existing work by Yampolskiy [17], Yudkowsky [19], Christiano [2], and others has highlighted the challenges of containing superintelligent AGI and started to explore some possibilities.", "startOffset": 61, "endOffset": 64}, {"referenceID": 13, "context": "(Whether this will be true of one of the first AGIs is uncertain, but there is reason to think it\u2019s plausible [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Some contemporary projects are pursuing this goal [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "For example, the Arcade Learning Environment [1] typecasts scores from an Atari game console\u2019s memory to wider machine integers before showing them to the AGI.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Similarly to Omohundro\u2019s list of AGI drives [11], we can infer some likely subgoals, that a wide range of AGI motivational systems (though certainly not all motivational systems) would see as desirable, as follows:", "startOffset": 44, "endOffset": 48}, {"referenceID": 2, "context": "Any stochasticity should come entirely from an explicit random seed; the software stack should not allow the AGI to pump randomness out of race conditions [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "A sufficiently clever AGI might convince or trick a human operator into \u201cletting it out of the box\u201d; humans are quite vulnerable to manipulation [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "In an experiment, role-playing as an AGI, Eliezer Yudkowsky was able to convince others role-playing as the gatekeeper to let him out of the box [19], despite a monetary incentive not to do so, and this result was replicated by others [4,15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "This would be combined with intrusion detection systems [9,8], so that any attempt by an AGI to break the integrity constraints imposed on it would be noticed, allowing the developers to shut it down before it leveraged its progress to get further.", "startOffset": 56, "endOffset": 61}, {"referenceID": 5, "context": "This would be combined with intrusion detection systems [9,8], so that any attempt by an AGI to break the integrity constraints imposed on it would be noticed, allowing the developers to shut it down before it leveraged its progress to get further.", "startOffset": 56, "endOffset": 61}, {"referenceID": 12, "context": "For example, the problem of an AGI potentially talking its way out of the box could be solved by limiting its output channel to a small number of multiple-choice questions [17].", "startOffset": 172, "endOffset": 176}, {"referenceID": 12, "context": "As mentioned in Section 1, existing work on AGI containment [17,2] has focused on mechanisms that are heavy, but may be amenable to theoretical guarantees of effectiveness against particular sorts of vulnerability.", "startOffset": 60, "endOffset": 66}, {"referenceID": 1, "context": "As mentioned in Section 1, existing work on AGI containment [17,2] has focused on mechanisms that are heavy, but may be amenable to theoretical guarantees of effectiveness against particular sorts of vulnerability.", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "Programs written in memory-safe languages usually call some libraries which aren\u2019t, and even replacing all the libraries is not a total defense; for example, an AGI could perform a computation which causes a hardware glitch, such as RowHammer, which has been successfully used to cause memory corruption from within a Javascript interpreter [5].", "startOffset": 341, "endOffset": 344}, {"referenceID": 4, "context": "[7]", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "There is considerable uncertainty about when AGI will be developed and what kinds of designs will be required to avert undesired outcomes [13], but across most plausible scenarios, deploying AGI containment systems would help ensure that reliable AGI testing can be performed even with AGIs that approach human-level capability at understanding the world around them.", "startOffset": 138, "endOffset": 142}], "year": 2016, "abstractText": "There is considerable uncertainty about what properties, capabilities and motivations future AGIs will have. In some plausible scenarios, AGIs may pose security risks arising from accidents and defects. In order to mitigate these risks, prudent early AGI research teams will perform significant testing on their creations before use. Unfortunately, if an AGI has human-level or greater intelligence, testing itself may not be safe; some natural AGI goal systems create emergent incentives for AGIs to tamper with their test environments, make copies of themselves on the internet, or convince developers and operators to do dangerous things. In this paper, we survey the AGI containment problem \u2013 the question of how to build a container in which tests can be conducted safely and reliably, even on AGIs with unknown motivations and capabilities that could be dangerous. We identify requirements for AGI containers, available mechanisms, and weaknesses that need to be addressed.", "creator": "LaTeX with hyperref package"}}}