{"id": "1604.00644", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2016", "title": "An electronic-game framework for evaluating coevolutionary algorithms", "abstract": "One of the common artificial intelligence applications in electronic games consists of making an artificial agent learn how to execute some determined task successfully in a game environment. One way to perform this task is through machine learning algorithms capable of learning the sequence of actions required to win in a given game environment. There are several supervised learning techniques able to learn the correct answer for a problem through examples. However, when learning how to play electronic games, the correct answer might only be known by the end of the game, after all the actions were already taken. Thus, not being possible to measure the accuracy of each individual action to be taken at each time step. A way for dealing with this problem is through Neuroevolution, a method which trains Artificial Neural Networks using evolutionary algorithms. In this article, we introduce a framework for testing optimization algorithms with artificial agent controllers in electronic games, called EvoMan, which is inspired in the action-platformer game Mega Man II. The environment can be configured to run in different experiment modes, as single evolution, coevolution and others. To demonstrate some challenges regarding the proposed platform, as initial experiments we applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the context of competitively coevolving two distinct agents in this game.", "histories": [["v1", "Sun, 3 Apr 2016 14:57:24 GMT  (652kb,D)", "https://arxiv.org/abs/1604.00644v1", "This paper is a translation of \\cite{karine2015}, published in Portuguese at Brazilian Congress on Computational Intelligence, 2015"], ["v2", "Mon, 11 Apr 2016 18:35:29 GMT  (652kb,D)", "http://arxiv.org/abs/1604.00644v2", "This paper is a translation of \\cite{karine2015}, published in Portuguese at Brazilian Congress on Computational Intelligence, 2015"]], "COMMENTS": "This paper is a translation of \\cite{karine2015}, published in Portuguese at Brazilian Congress on Computational Intelligence, 2015", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["karine da silva miras de ara\\'ujo", "fabr\\'icio olivetti de fran\\c{c}a"], "accepted": false, "id": "1604.00644"}, "pdf": {"name": "1604.00644.pdf", "metadata": {"source": "CRF", "title": "An electronic-game framework for evaluating coevolutionary algorithms", "authors": ["Karine da Silva", "Miras de Ara\u00fajo"], "emails": ["karine.smiras@gmail.com", "folivetti@ufabc.edu.br"], "sections": [{"heading": null, "text": "There are several applications for such agents, ranging from manufacturing industries to unmanned vehicles, to explore inhospitable places. [13] These test environments allow to test the ability of an algorithm while creating a toy is successful. [14] The flexibility and variability of the rules of the game can lead to players being able to outdo themselves. [16] These test environments allow to verify the ability of a player by creating a toy that is successful in the proposed form."}, {"heading": "II. BACKGROUND", "text": "In this section, we review some key concepts related to the algorithms used during the experiments."}, {"heading": "A. Artificial Neural Networks", "text": "An artificial neural network [7] (ANN) is a computational model inspired by how the animal brain works. In the most basic and traditional ANN, the computational flow begins with some input variables related to a pattern-recognition task. These input variables are then mixed by linear combinations and sent to the next layer of neurons, where they may or may not connect those neurons directly to the output neurons. An ANN is usually organized into layers of neurons, the trivial case being a single layer that connects the input neurons directly to the output neurons, or the non-trivial case when multiple layers produce intermediate results through the combination of variables to the emission of an output. Defining a suitable topology for the ANN is relevant to the success of a learning process."}, {"heading": "B. Genetic Algorithms", "text": "A Genetic Algorithm (GA) [9] is a meta-heuristic that applies the principles of natural selection observed and described by Darwin to find solutions to a variety of problems.In GA, the variables for a problem are encoded as artificial genes vectorized as chromosomes, which should be able to represent the search area of the problem to be solved.GA begins with a random population of chromosomes and then repeats the following procedures: recombination of chromosome pairs (crossover), disturbance of selected chromosomes (mutation), and execution of a probabilistic selection that favors the fit test (selection).During crossover surgery, new chromosomes are generated by combining pairs of 2 chromosomes from the current population, this procedure should be performed in such a way that the offspring select their characteristics within the genotype of both parasites to inherit a specific amount of the mutation (the amount normally suggested in a given mutation)."}, {"heading": "C. Neuroevolution", "text": "As mentioned in a previous section, these weights are optimized whenever it is not possible to apply the standard optimization algorithms to adjust the weights of an ANN using an evolutionary algorithm such as a GA. This method is called neuroevolution (NE) [6]. However, in addition to optimizing the weights of the compounds, the topology of ANN can also have a major impact on its performance in relation to the problem to be solved. Therefore, it is reasonable to try to develop the optimal topology for the network and thus maximize the performance of the learning task. To this end, the Neuroevolution of Augmenting Topologies (NEAT) [15] was created, which is described in the next subsection."}, {"heading": "D. NEAT algorithm", "text": "The algorithm starts with a random population of ANNs that are composed of the same topology and consist of a single layer that relates the inputs to the outputs but has different random connection weights. Chromosome representation for each network consists of two different types of genes: \u2022 Node genes that represent inputs and neurons; \u2022 Connection genes that represent estimated connections (weights) between neurons. These genes hold a flag indicating whether they are currently active or not. After creating the original population, the algorithm performs the following steps for each generation: \u2022 Specification: divides the population into species based on the similarities of the genomes. \u2022 In some variations, the recombination is performed using more than two chromosomes \u2022 Fitness Sharing: adjusts the fit of all genomes, with the aim of adding groups of chromosomes near the same base of attractiveness in the search space to calculate the size of the existing species by following proportions."}, {"heading": "E. NEAT on Computational Intelligence in Games", "text": "When trying to develop an agent controller to play the game Frogs, [1] some experiments were developed on the application of NEAT. Twelve sensors were used to allow the agent to detect the current state of the game, measuring the proximity of objects around the game. Each one was tested three times in the game, and the final fitness was the average value, the value being the proximity of the frog to the target area. Application of NEAT for driving in TORCS with minimal input was discussed in [10]. The implementation presented in the paper used sensors towards the relative angle of the car to the centerline of the lane and the speed of the car. NEAT [16] was successfully used to find a good fitness function for a football robot that was expected to learn to avoid opponents, carry a ball and kick it to the goal.Another interesting application was made in [13] for the Ms.Pacman game framework."}, {"heading": "F. Competitive Coevolution", "text": "In nature, coevolution occurs when one species increases the selective pressure on another species and forces it to adapt in order to survive. Mutually, this process can lead to an arms race, as both species try to outdo each other [5]. Competitive coevolution (CC) [12] is a method of applying evolutionary algorithms in which, instead of a single population, two populations are developed through competition with each other. Figure 1 illustrates an arms race scheme caused by a coevolutionary process in which, over generations, the winning role between competing species is reversed due to their adaptive changes. In this example, geometric forms compete, with size considered to be their fitness, so that the largest wines play a role. In games [3], the CC is sometimes used for the predator domain [11]. In this case, as it develops synchronously with the population of the player, the population of enemies mixes into the environment and generates an uncertainty of space, resulting in an objective space exploration."}, {"heading": "G. Coevolution in Computational Intelligence for Games", "text": "In [3] a coevolutionary algorithm was analyzed using the Ms.Pacman framework. In this work it was confirmed that the coevolutionary controllers achieved a better generalization for the challenges of the game than the standard evolution. The authors also noted that it was more difficult to develop controllers for the spirits than for the Ms.Pacman, suggesting that the success of this method could depend on the problem domain. In another interesting work, a robot architecture was developed aimed at developing predator-prey behavior, which enabled the observation that the developed agent controllers acquired interesting behaviors such as obstacle avoidance, object discrimination and visual perception."}, {"heading": "III. EVOMAN FRAMEWORK", "text": "The EvoMan 3 framework proposed in [4] is an environment for developing agents for action-jump format games, inspired by the classic Mega Man II 4.This framework contains eight different enemies against which the player agent must learn how to beat them by performing one of the following simple actions: move to the left, move to the right, jump, jump and shoot. It was developed in Python 2.7 5 using the Pygame 6.3https: / / github.com / karinemiras / evoman framework 4https: / / www.megaman.capcom.com 5https: / / www.python.org 6http: / / www.pygame.org / The game screen consists of a rectangular area that may contain some obstacles depending on the game stage. At the start of the game, each character (the player and the enemy) is positioned in opposing screens. At each step, the player and the enemy can perform one or more combined actions to interact with the environment and their opponent."}, {"heading": "A. Simulation modes", "text": "The framework allows experiments to be carried out with the combination of different simulation modes (Fig. 2): \u2022 Human Player: where the player character is controlled by a human input device (i.e. joystick). \u2022 AI Player: where the player character is controlled by a machine learning algorithm. \u2022 Static Enemy: where the enemy character adopts a rules-based fixed attack / defence strategy based on the original Mega Man II. \u2022 AI Enemy: where the enemy character is controlled by a machine learning algorithm. In this study, we use the simulation mode combinations \"AI Player VS Static Enemy\" and \"AI Player VS AI Enemy\" to assess the learning of artificial agent controllers for the characters of the game using all the levels provided in the framework. The second combination is the reproduction of a competitive coevolution process to test the behaviour of the learning agents."}, {"heading": "B. Game Sensors and Actions", "text": "The environment offers 68 game state variables that act as sensors for the agents (AI player or AI enemy): 1) coordinates of the rectangles that envelop each character (8 sensors in total); 2) flag that indicates that each character is above a surface (2 sensors in total); 3) time steps that remain until the character is allowed to shoot a projectile again (2 sensors in total); 4) flag that indicates whether each character is shooting (2 sensors in total); 5) vertical and horizontal acceleration for each character (4 sensors in total); 6) direction that each character is aiming at (2 sensors in total); 7) flag that indicates whether each character is attacking or not (2 sensors in total); 8) the coordinates of the rectangles that envelop each of the three projectiles show the player's projectiles (12 sensors in total)."}, {"heading": "IV. EXPERIMENTS", "text": "7https: / / github.com / karinemiras / evoman frameworkFig. 3. In this context, each time the user has to test a solution (agent controller) using his or her algorithm, he or she has to go through a game loop in which sensors for the agent to be tested are provided at each step of the time, as well as their decision-making actions. At the end of the loop, the assessment of the agent's fitness is returned to the algorithm."}, {"heading": "A. Methodology", "text": "Before assessing the behavior of a coevolutionary competition, we performed evolutions with the mode \"AI Player Vs Static Enemy\" to verify the ability of each algorithm to defeat each Enemy. Afterwards, we experimented with the mode \"AI Player Vs AI Enemy,\" in which each agent (player or enemy) had a limited number of iterations to develop a better strategy than their adversaries.The experiments were performed with an ANN with a single layer whose weights were developed by a standard genetic algorithm, and also an ANN developed the application of the NEAT algorithm. The ANNs received each of the 68 sensors normalized as input between \u2212 1 and 1, and the networks produced 5 neurons according to one of the possible actions that the agent could perform. The activation function we used was the logistic function and each output value higher than 0.5 translated to perform the corresponding activity."}, {"heading": "B. Results", "text": "In fact, we are able to solve the problems we have by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "The purpose of such agents may be to fight against a pre-programmed enemy by learning a general strategy to win against all enemies, or to test a co-evolutionary approach in which the player and the enemy both have a limited number of generations to learn how to beat each other, thus driving an arms race. To illustrate the capabilities of the proposed framework, we conducted two simple experiments: i) the application of two different learning algorithms against the pre-programmed behavior of each enemy, ii) the conduct of a co-evolutionary learning experiment to test whether the player could keep pace with an ever-evolving mindset. The heuristic experiments demonstrated the feasibility of learning how to win against the pre-programmed behavior of all enemies, and, likewise, the difficulty levels of such a learning experiment to check whether the player could keep pace with an ever-evolving mindset."}], "references": [{"title": "Flow: the psychology of optimal experience (book review)", "author": ["R Buchanan", "M Csikszentmihalyi"], "venue": "Design Issues,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "Competitive coevolution in ms. pac-man", "author": ["Andrew Borg Cardona", "Julian Togelius", "Mark J Nelson"], "venue": "In Evolutionary Computation (CEC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Um ambiente de jogo eletronico para avaliar algoritmos coevolutivos", "author": [], "venue": "CBIC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Arms races between and within species", "author": ["Richard Dawkins", "John R Krebs"], "venue": "Proceedings of the Royal Society of London B: Biological Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1979}, {"title": "Neuroevolution: from architectures to learning", "author": ["Dario Floreano", "Peter D\u00fcrr", "Claudio Mattiussi"], "venue": "Evolutionary Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Artificial neural networks (the multilayer perceptron)\u2014a review of applications in the atmospheric sciences", "author": ["Matt W Gardner", "SR Dorling"], "venue": "Atmospheric environment,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "The experimental study of the competitive co-evolution using predator-prey tasks", "author": ["Gyongyike Gebeov\u00e1", "Miroslav Hudec", "Peter Kosteln\u0131\u0301k", "Vratislav Kov\u00e1c"], "venue": "Intelligent Technologies: Theory and Applications: New Trends in Intelligent Technologies,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Coevolution of a backgammon player", "author": ["Jordan B Pollack", "Alan D Blair", "Mark Land"], "venue": "In Artificial Life V: Proc. of the Fifth Int. Workshop on the Synthesis and Simulation of Living Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Constructing competitive and cooperative agent behavior using coevolution", "author": ["Aditya Rawal", "Padmini Rajagopalan", "Risto Miikkulainen"], "venue": "In Computational Intelligence and Games (CIG),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Evolving multimodal behavior with modular neural networks in ms. pac-man", "author": ["Jacob Schrum", "Risto Miikkulainen"], "venue": "In Proceedings of the 2014 conference on Genetic and evolutionary computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A review on back-propagation algorithms for feedforward networks", "author": ["Shital Solanki", "HB Jethva"], "venue": "Int. Res. Anal,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Evolving neural networks through augmenting topologies", "author": ["Kenneth O Stanley", "Risto Miikkulainen"], "venue": "Evolutionary computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Extending robot soccer using neat", "author": ["Phyo Thiha"], "venue": "Final Projects,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "Testing environments which may help in such cases are those emulating electronic video games [16] [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Testing environments which may help in such cases are those emulating electronic video games [16] [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "1This paper is a translation of [4], published in Portuguese at Brazilian Congress on Computational Intelligence, 2015.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "a game, in order to verify the difficulty and feasibility to overcome a given challenge, or in order to develop a daunting AI agent that meets the amusement level expected by a game player [2].", "startOffset": 189, "endOffset": 192}, {"referenceID": 5, "context": "An Artificial Neural Network [7] (ANN) is a computational model inspired by how the animal brain works.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "An ANN is commonly organized in layers of neurons [7], with the most trivial case being a single layer connecting the input neurons directly to the output neurons, or the nontrivial case when multiple layers generate intermediate results by the combination of variables until emitting an output.", "startOffset": 50, "endOffset": 53}, {"referenceID": 10, "context": "When the task at hand is a supervised learning problem, having a sequence of sample input-output from which to learn, the weights for the inner linear combinations of the ANN can be estimated by Gradient Descent algorithms, the most wellknown being the Backpropagation [14].", "startOffset": 269, "endOffset": 273}, {"referenceID": 4, "context": "This methodology is called Neuroevolution (NE) [6].", "startOffset": 47, "endOffset": 50}, {"referenceID": 11, "context": "For this purpose, the Neuroevolution of Augmenting Topologies (NEAT) [15] was created and it will be described in the next subsection.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "Using NEAT, [16] tried and succeed in figuring out a good fitness function for a soccer player robot, which was expected to learn how to avoid opponents, carry a ball and kick it to the goal.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "Another interesting application was made in [13], for the Ms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "When reciprocal, this process may generate an arms race as both species try to surpass each other for survival [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Competitive Coevolution (CC) [12] is a method of applying Evolutionary Algorithms, so that instead of evolving a single population, two populations are mutually evolved through competition.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "In games [3], the CC is sometimes used for the predatorprey domain [11].", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "In games [3], the CC is sometimes used for the predatorprey domain [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "A Coevolutive algorithm was analyzed in [3], with the Ms.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "In another interesting work, a robotic architecture was elaborated [8] aiming to evolve predator-prey behavior.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "The EvoMan 3 framework, proposed in [4] is an environment for evolving game playing agents for action-platformer games inspired by the classic Mega Man II 4.", "startOffset": 36, "endOffset": 39}], "year": 2016, "abstractText": "One of the common artificial intelligence applications in electronic games consists of making an artificial agent learn how to execute some determined task successfully in a game environment. One way to perform this task is through machine learning algorithms capable of learning the sequence of actions required to win in a given game environment. There are several supervised learning techniques able to learn the correct answer for a problem through examples. However, when learning how to play electronic games, the correct answer might only be known by the end of the game, after all the actions were already taken. Thus, not being possible to measure the accuracy of each individual action to be taken at each time step. A way for dealing with this problem is through Neuroevolution, a method which trains Artificial Neural Networks using evolutionary algorithms. In this article, we introduce a framework for testing optimization algorithms with artificial agent controllers in electronic games, called EvoMan, which is inspired in the action-platformer game Mega Man II. The environment can be configured to run in different experiment modes, as single evolution, coevolution and others. To demonstrate some challenges regarding the proposed platform, as initial experiments we applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the context of competitively coevolving two distinct agents in this game. 1", "creator": "LaTeX with hyperref package"}}}