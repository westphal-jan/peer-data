{"id": "1604.00790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Image Captioning with Deep Bidirectional LSTMs", "abstract": "This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \"translate\" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task.", "histories": [["v1", "Mon, 4 Apr 2016 09:43:04 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v1", "11 pages, 10 figures"], ["v2", "Sun, 10 Jul 2016 07:45:25 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v2", "11 pages, 10 figures"], ["v3", "Wed, 20 Jul 2016 14:19:37 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v3", "accepted by ACMMM 2016 as full paper and oral presentation"]], "COMMENTS": "11 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["cheng wang", "haojin yang", "christian bartz", "christoph meinel"], "accepted": false, "id": "1604.00790"}, "pdf": {"name": "1604.00790.pdf", "metadata": {"source": "CRF", "title": "Image Captioning with Deep Bidirectional LSTMs", "authors": ["Cheng Wang", "Haojin Yang", "Christian Bartz", "Christoph Meinel"], "emails": ["haojin.yang@hpi.de", "christoph.meinel@hpi.de", "christian.bartz@student.hpi.uni-potsdam.de", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Computer Methods \u2192 Natural Language Generation; Neural Networks; Computer Visual Presentations; Keywords LSTM, Deep Learning, Captions, Visual Language"}, {"heading": "1. INTRODUCTION", "text": "It requires not only the recognition of the visual context between the objects and the interaction. [11, 10, 13, 17, 16, 22, 32, 37] It is a demanding task that is integrable to make digital or hard copies of all or part of this work, which is granted free of charge, provided that copies are not made or distributed for profit or commercial use and that copies require this note and the full citation of the source on the first page. Copyrights for components of this work that belong to others must be honored. Abstracting with credit is allowed others, or republish, to post on server or to redistribution to lists, requires prior specific permission and / or a fee. Request permissions from permissions @ acm.org. WOOODSTOCK '97 El Paso, Texas USA c \u00a9 2016 ACM 123-4567-24 / 06."}, {"heading": "2. RELATED WORK", "text": "The common concept across modalities plays an important role in bridging the \"semantic gap\" of multimodal data. Captions fall into this general category of learning multimodal representations. Recently, several approaches to captions have been proposed. We can roughly divide these methods into three categories. The first category is based on procedures that generate captions based on the detection of objects and the discovery of attributes within the image. For example, work [20] was proposed to analyze an entire sentence into several phrases and to learn the relationships between phrases and objects within an image. [15] conditional random fields (CRFs) were used to correspond to objects, attributes and prepositions of image contents and predict the best label. Other similar work was presented in [25, 17, 16] which are typically hard-designed and rely on fixed templates that usually result in poor performance when varied over long sentences."}, {"heading": "3. MODEL", "text": "In this section, we describe our multimodal bidirectional LSTM model (Bi-LSTM for short) and examine its depth variants. First, we briefly introduce LSTM, which is at the center of the model. LSTM, which we use, is described in [41]."}, {"heading": "3.1 Long Short Term Memory", "text": "Our model is based on the LSTM cell, which is a special form of the traditional recursive neural network (RNN). It has been successfully applied to machine translation [3], speech recognition [8], and sequence learning [34]. As shown in Figure 2, the reading and spelling of the memory cell c is controlled by a group of sigmoid gates. In due course, LSTM receives input from various sources: current input x, the previous hidden state of all LSTM units ht \u2212 1, as well as previous memory wave state \u2212 1. The update of these gates in due time takes place step t for given input xt, ht \u2212 1, and extract \u2212 1 as follows. It is = \u03c3 (Wxixt + Whiht \u2212 1 + bi) (1) ft = Wxfxt + Whfht \u2212 1 + bf) (2) ot = nix (Wxoxt + Whoht + Whoht \u2212 1 + 1 + bo) p (3 ft gt = c + xx + 1 Whxt) p (xxx) x x x) x (Whxt) (p) x x x) x x) (Whoht \u2212 1 + bo) p (p)."}, {"heading": "3.2 Bidirectional LSTM", "text": "To use both the past and future context information of a sentence in word prediction, we propose an abi-directional model by inserting the sentence of forward and backward into the LSTM. Figure 3 presents the overview of our model, which consists of three modules: a CNN for encoding image input, a text LSTM (T-LSTM) for encoding sentence input, a multimodal LSTM (M-LSTM) for embedding visual and textual vectors in a common semantic space, and decoding into the sentence. The bidirectional LSTM is implemented with two separate LSTM layers for calculating forward hidden sequences \u2212 \u2192 h and backward hidden sequences. \u2212 Forward LSTM begins at time t = 1 and backward LSTM begins at time t = T = 1. Formally, our model works as follows the code of forward, S and backward \u2212"}, {"heading": "3.3 Deeper LSTM architecture", "text": "In order to design deeper LSTM architectures, in addition to directly stacking several LSTMs on top of each other, which we call Bi-S-LSTM (Figure 4 (c)), we propose to use a completely connected layer as a transition layer. Our motivation stems from the realization that [28], in which DT (S) -RNN (deep transition RNN with link) is designed by connecting to hidden multi-layer perception (MLP). It is probably easier to train such a network. Inspired by this, we extend Bi-LSTM (Figure 4 (b)) by a completely connected layer, which we have called Bi-F-LSTM (Figure 4 (d), to establish a link between input and hidden states, in order to make it easier to learn a model. The goal of extension models is to represent an additional hidden transition function Fh (Figure 4 (Uhl) + Ub = \u2212 1 layer \u2212 1 \u2212 1 layer \u2212 1 \u2212 1 In layer \u2212 1 \u2212 1 \u2212 1 layer \u2212 1 \u2212 1 layer \u2212 1 - 1 layer \u2212 1)."}, {"heading": "3.4 Data Augmentation", "text": "One of the most challenging aspects of training deep bidirectional LSTM models is to avoid overadjustment. As our largest dataset has only 80K images [21], which could easily lead to overadjustment, we have applied several techniques such as finetuning on pre-trained visual models, weight loss, failures, and early stopping of the techniques commonly used in previous work. Furthermore, it has been proven that data adjustments such as random cropping and horizontal mirrors can effectively alleviate overadjustments. Inspired by this circumstance, we designed new data augmentation techniques to increase the number of picture-set pairs. Our implementation in the visual model is as follows: \u2022 Multi-Corp: Instead of randomly clipping the input image, we cut at the four corners and in the middle region."}, {"heading": "3.5 Training and Inference", "text": "Our model can be trained end-to-end using Stochastic Gradient Descent (SGD), although the common loss function L = \u2212 \u2192 L + \u2190 \u2212 L is calculated by accumulating Softmax losses in forward and backward direction. Our goal is to minimize L, which is equivalent to maximizing the probabilities of correctly generated sentences. We calculate the OL gradient using the Back-Propagation Through Time (BPTT) algorithm. The trained model is used to predict a word wt with given image context I and previous word context w1: t \u2212 1 by P (wt | w1, I) in forward order or by P (wt | wt + 1: T, I) in reverse order. We set w1 = wT = 0 at the starting point or for forward and backward word contexts w1: t (wt) in forward order, or by P (wt + 1: T, I) in reverse order, we ultimately choose the final sentence from two sentences I \u2212 1."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we will design several groups of experiments to achieve the following objectives: \u2022 Qualitatively analyze and understand how bi-directional multimodal LSTM learns to generate sentences conditioned by visual context information over time. \u2022 Measure the utility and performance of the proposed bi-directional model and its deeper variant models to increase their nonlinearity depth in different ways. \u2022 Compare our approach with state-of-the-art methods for sentence generation and image-set retrieval on popular benchmark datasets."}, {"heading": "4.1 Datasets", "text": "To confirm the effectiveness, universality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].Flickr8K consists of 8,000 images and each of them has 5 set-level captions. We follow the standard dataset subdivisions provided by authors, 6,000 / 1,000 / 1,000 images for training, validation / testing, or Flickr30K. An extension version of Flickr8K. It has 31,783 images and each of them has 5 captions. We follow the publicly available dataset subdivision by Karpathy and others [11]. In this dataset split, 28,000 / 1,000 / 1,000 images are used for training / validation / testing. MSCOCO. This is a recently released dataset that includes 82,783 images for training and 40,504 images for validation."}, {"heading": "4.2 Implementation Details", "text": "We used two visual image encoding models: Caffe [9] reference model, which is pre-trained with AlexNet [14] and the 16-layer VggNet model [31], and the features fc-7 and fc-15 are extracted and fed in to train the visual language model with LSTM. Previous work [37, 22] has shown that promising improvements can be achieved with more powerful image models such as GoogleNet [35] and VggNet [31]. To make a fair comparison with recent work, we chose the widely used two models for experiments. Textual features: First, we present each word w within a sentence as a hot vector, while w-RK, K is a word size based on training sets and different data sets. By performing basic tokenization and removing the words that occur less than 5 times in the training set, we have 2028, 7400 and 8888 words for patchwork sets, 800K and M30K."}, {"heading": "4.3 Evaluation Metrics", "text": "We evaluate our models based on two tasks: Caption Creation and Caption Retrieval. When creating captions, we follow previous work to use BLEU-N (N = 1,2,3,4) values [27]: BN = min (1, e 1 \u2212 r c) \u00b7 e 1 N \u2211 N = 1 log pn (16), where r, c are the length of the reference set and the generated sentence, pn is the modified n-gram accuracy. We also specify METETOR [18] and CIDER [36] values for further comparison. In the Caption Query (image query set and vice versa), we use R @ K (K = 1,5,10) and Med r as evaluation yardsticks. R @ K is the call rate R for the K candidates with the highest K rate, and Med r is the middle rank of the first image truth set and sentence retrieved. All values mentioned are commonly used by COM2 for captioning."}, {"heading": "4.4 Visualization and Qualitative Analysis", "text": "The aim of this experiment is to visualize the properties of the proposed bidirectional LSTM model and explain how it works by distributing sentences word for word over time. First, we examine the temporal evolution of internal gate states and understand how bidirectional LSTM units contain valuable context information and unimportant information. Second, we show that the input and output data of three signature types (input, forgetting and output) are modulated to input gate values within 0.1, as are the cell states in which dynamic states are periodically distilled to units from time t = 0 to 11. At t = 0, the input data is modulated to input gate values within 0.1. In this step, the values of forgetting gates f (t) of different LSTM units are zeros. Along with the increase in time, we begin to decide which unimportant information should be retained."}, {"heading": "4.6 Results on Image-Sentence Retrieval", "text": "This is an example of intermodal retrieval [6, 29, 38], which has been a hot topic of research in the multimedia field. Table 2 illustrates our results on various data sets. The performance of our models exceeds those comparative methods on most metrics or the consistency of existing results. In a few metrics, our model showed no better result than Mind's Eye [2], which combined image and text features in the ranking (it makes this task more like multimodal retrieval) and NIC [37], which used more powerful vision models, large beam size and a model ensemble. While adopting a more powerful visual model VggNet leads to significant improvements in all metrics, with less powerful AlexNet models, our results are still competitive with some metrics, e.g. R @ 1, R @ 5 on Flickr8K and Flickr30K."}, {"heading": "4.7 Discussion", "text": "Caption generation costs include: Caption generation and scanning of bidirectional captions, calculation of final captions. The recovery time costs are taken into account: Calculation of image set results (10 x 50 pairs in total), ranking rates for each image query. As shown in Tables 1, 2 and 3, deep models have only slightly higher time consumption, but bring significant improvements, and proposed Bi-F-LSTM can achieve the balance between performance and efficiency."}, {"heading": "5. CONCLUSIONS", "text": "We proposed a bi-directional LSTM model that generates descriptive sentences for images by taking into account both the history and the future context. We also developed deep bi-directional LSTM architectures to embed the image and sentence in a high semantic space for learning the visual language model. We also qualitatively visualized the internal states of the proposed model to understand how multimodal LSTM generates word in successive time increments. The effectiveness, universality and robustness of the proposed models were evaluated on numerous data sets. Our models achieve highly complex or state-of-the-art results for both generation and on-demand tasks. Our future work will focus on the exploration of more complex language representation (e.g. word2vec) and the inclusion of multitasking learning and attention mechanisms in our model."}, {"heading": "6. REFERENCES", "text": "[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machinetranslation S. S. 12b. Networks through shared learning to align and translate. ICLR, 2015. [2] X. Chen and C. Lawrence Zitnick. Mind's eye: A recurrent visual representation for image caption generation. In CVPR, pp. 2422-2431, 2015. [3] K. Cho, B. Van Merrie \ufffd nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learphrase representations using rnn encoder-decoder-decoder for statistical machine translation. EMNLP, 2014. J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR, pages 2422\u20132431", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "CVPR, pages 1473\u20131482", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACMMM, pages 7\u201316. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Devise: A deep visual-semantic  embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, pages 2121\u20132129", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP, pages 6645\u20136649. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACMMM, pages 675\u2013678. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F-F. Li"], "venue": "NIPS, pages 1889\u20131897", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F-F. Li"], "venue": "CVPR, pages 3128\u20133137", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML, pages 595\u2013603", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T. Berg"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 35(12):2891\u20132903", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T. Berg", "Y. Choi"], "venue": "ACL, volume 1, pages 359\u2013368. ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T. Berg", "Y. Choi"], "venue": "Trans. of the Association for Computational Linguistics(TACL), 2(10):351\u2013362", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor universal: language specific translation evaluation for any target language", "author": ["M. Lavie"], "venue": "ACL, page 376", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.E. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "CoNLL, pages 220\u2013228. ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J.H. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z.H. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J.H. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "ICASSP, pages  5528\u20135531. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "ACL, pages 747\u2013756. ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Ng"], "venue": "ICML, pages 689\u2013696", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "ACL, pages 311\u2013318. ACL", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6026", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 36(3):521\u2013535", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT Workshop, pages 139\u2013147. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Trans. of the Association for Computational Linguistics(TACL), 2:207\u2013218", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, pages 2222\u20132230", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, pages 1\u20139", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "Z. Lawrence", "D. Parikh"], "venue": "CVPR, pages 4566\u20134575", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compositional cross-modal learning to rank via local-global alignment", "author": ["X.Jiang", "F. Wu", "X. Li", "Z. Zhao", "W. Lu", "S. Tang", "Y. Zhuang"], "venue": "In ACMMM,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. ICML", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Trans. of the Association for  Computational Linguistics(TACL), 2:67\u201378", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV, pages 818\u2013833. Springer", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 9, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 12, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 16, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 15, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 21, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 31, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 36, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 19, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 14, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 16, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 15, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 10, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 9, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 12, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 21, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 31, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 36, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 13, "context": "Why deeper LSTMs? The recent success of deep CNN in image classification and object detection [14, 31] demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones.", "startOffset": 94, "endOffset": 102}, {"referenceID": 30, "context": "Why deeper LSTMs? The recent success of deep CNN in image classification and object detection [14, 31] demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones.", "startOffset": 94, "endOffset": 102}, {"referenceID": 27, "context": "As claimed in [28], if we consider LSTM as a composition of multiple hidden layers that unfolded in time, LSTM is already deep network.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Multimodal representation learning [26, 33] has significant value in multimedia understanding and retrieval.", "startOffset": 35, "endOffset": 43}, {"referenceID": 32, "context": "Multimodal representation learning [26, 33] has significant value in multimedia understanding and retrieval.", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "For example, the work [20] was proposed to parse a whole sentence into several phrases, and learn the relationships between phrases and objects within an image.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "In [15], conditional random field(CRF) was used to correspond objects, attributes and prepositions of image content and predict the best label.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 15, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "By leveraging distance metric to retrieve similar captioned images, then modify and combine retrieved captions to generate caption [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 35, "endOffset": 43}, {"referenceID": 41, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 35, "endOffset": 43}, {"referenceID": 22, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 23, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 0, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 36, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 38, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 12, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 9, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 10, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 11, "context": "[12] can been as pioneer work to use neural network for image captioning with multimodal neural language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In their follow up work [13], Kiro et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "[32] presented a DT-RNN (Dependency Tree-Recursive Neural Network) to embed sentence into a vector space in order to retrieve images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] proposed m-RNN which replace feed-forward neural language model in [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[22] proposed m-RNN which replace feed-forward neural language model in [13].", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "Similar architectures were introduced in NIC [37] and LRCN [4], both approaches use LSTM to learn text context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "Similar architectures were introduced in NIC [37] and LRCN [4], both approaches use LSTM to learn text context.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "[22] and LRCN [4]\u2019s model consider image context at each time step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[22] and LRCN [4]\u2019s model consider image context at each time step.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Another group of neural network based approaches has been introduced in [10, 11] where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.", "startOffset": 72, "endOffset": 80}, {"referenceID": 10, "context": "Another group of neural network based approaches has been introduced in [10, 11] where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.", "startOffset": 72, "endOffset": 80}, {"referenceID": 4, "context": "[5] used multi-instance learning and traditional maximum-entropy language model for description generation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed to learn visual representation with RNN for generating image caption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "In [39], Xu et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "The LSTM we used is described in [41].", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 80, "endOffset": 83}, {"referenceID": 33, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Our motivation comes from the finding of [28], in which DT(S)-RNN (deep transition RNN with shortcut) is designed by adding", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "[22, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[22, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Since our largest dataset has only 80K images [21] which might cause overfitting easily, we adopted several techniques such as finetuning on pre-trained visual model, weight decay, dropout and early stopping that commonly used in previous work.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "In our work, we fix k = 1 on all experiments although the average of 2 BLEU [27] points better results can be achieved with k = 20 compare to k = 1 as reported in [37].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "In our work, we fix k = 1 on all experiments although the average of 2 BLEU [27] points better results can be achieved with k = 20 compare to k = 1 as reported in [37].", "startOffset": 163, "endOffset": 167}, {"referenceID": 29, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 137, "endOffset": 141}, {"referenceID": 36, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 14, "endOffset": 22}, {"referenceID": 21, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 14, "endOffset": 22}, {"referenceID": 34, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Our work uses the LSTM implementation of [4] on Caffe framework.", "startOffset": 41, "endOffset": 44}, {"referenceID": 26, "context": "In caption generation, we follow previous work to use BLEU-N (N=1,2,3,4) scores [27]:", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "We also report METETOR [18] and CIDEr [36] scores for further comparison.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": "We also report METETOR [18] and CIDEr [36] scores for further comparison.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "At t = 0, the input data are sigmoid modulated to input gate i(t) where values lie within in [0,1].", "startOffset": 93, "endOffset": 98}, {"referenceID": 36, "context": "Models B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 NIC[37]G,\u2021 63 41 27.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "6 LRCN[4]A,\u2021 - - - 58.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "4 DeepVS[11]V 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "1 23 m-RNN[22]A,\u2021 56.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "0 54 36 23 15 - - - m-RNN[22]V,\u2021 - - - 60 41 28 19 67 49 35 25 Hard-Attention[39]V 67 45.", "startOffset": 25, "endOffset": 29}, {"referenceID": 38, "context": "0 54 36 23 15 - - - m-RNN[22]V,\u2021 - - - 60 41 28 19 67 49 35 25 Hard-Attention[39]V 67 45.", "startOffset": 77, "endOffset": 81}, {"referenceID": 38, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 119, "endOffset": 127}, {"referenceID": 38, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 119, "endOffset": 127}, {"referenceID": 38, "context": "Although we believe incorporating such powerful mechanism into our framework can make further improvements, note that our current model Bi-LSTM achieves the best or second best results on most of metrics while the small gap in performance between our model and Hard-Attention [39] is existed.", "startOffset": 276, "endOffset": 280}, {"referenceID": 10, "context": "Without integrating object detection and more powerful vision model, our model (Bi-LSTM) outperforms DeepVS [11] in a certain margin.", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 28, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 37, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 1, "context": "In a few metrics, our model didn\u2019t show better result than Mind\u2019s Eye [2] which combined image and text", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Flickr 8K DeViSE[7] 4.", "startOffset": 16, "endOffset": 19}, {"referenceID": 31, "context": "6 29 SDT-RNN[32] 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "0 29 DeFrag[10]+O 12.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "[13]A 13.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]V 18 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 10 m-RNN[22]A 14.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "4 15 Mind\u2019s Eye[2]V 17.", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "1 8 DeepVS[11]+O,V 16.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "4 NIC[37]G 20 60 6 19 64 5 Bi-LSTMA 21.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "Flickr 30K DeViSE[7] 4.", "startOffset": 17, "endOffset": 20}, {"referenceID": 31, "context": "7 25 SDT-RNN[32] 9.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "[13]A 14.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]V 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "5 8 LRCN[4]A 14 34.", "startOffset": 8, "endOffset": 11}, {"referenceID": 36, "context": "9 47 11 NIC[37]G 17 56 7 17 57 8 m-RNN[22]A 18.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "9 47 11 NIC[37]G 17 56 7 17 57 8 m-RNN[22]A 18.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "5 16 Mind\u2019s Eye[2]V 18.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "9 8 DeFrag [10]+O 16.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "5 13 DeepVS[11]+O,V 22.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "MSCOCO DeepVS[11]+O,V 16.", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "features in ranking (it makes this task more like multimodal retrieval) and NIC [37] which employed more powerful vision model, large beam size and model ensemble.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Since we strictly follow dataset splits as in [11], we compare to it in most cases.", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \u201ctranslate\u201dimage to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task.", "creator": "LaTeX with hyperref package"}}}