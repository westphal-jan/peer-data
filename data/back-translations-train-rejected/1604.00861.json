{"id": "1604.00861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings", "abstract": "In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.", "histories": [["v1", "Mon, 4 Apr 2016 13:54:09 GMT  (1918kb,D)", "http://arxiv.org/abs/1604.00861v1", "To appean in Proceedings of IEEE ICASSP 2016"]], "COMMENTS": "To appean in Proceedings of IEEE ICASSP 2016", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["giambattista parascandolo", "heikki huttunen", "tuomas virtanen"], "accepted": false, "id": "1604.00861"}, "pdf": {"name": "1604.00861.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS FOR POLYPHONIC SOUND EVENT DETECTION IN REAL LIFE RECORDINGS", "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "emails": [], "sections": [{"heading": null, "text": "Recognition of recursive neural networks (RNNs) in real-world recordings based on bidirectional short-term memory (BLSTM). Our method is tested using a large database of real-world recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method far exceeds previous approaches and the results are further improved using data augmentation techniques. Overall, our system has an average F1 value of 65.5% in 1-second blocks and 64.7% in frames, a relative improvement over the previous state of the art of 6.8% and 15.1%. Index Terms - Recurrent Neural Network, Bidirectional LSTM, Deep Learning, Polyphonic Sound Event Detection"}, {"heading": "1. INTRODUCTION", "text": "This year has come to the point where it is a place where there is a time when people are able to unfold, and where people are able to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold."}, {"heading": "2. RECURRENT NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Feedforward neural networks", "text": "In a feedback neural network (FNN), all observations are processed independently of each other. Due to the lack of context information, FNNs may have difficulty processing sequential input such as audio, video, and text. Often, a fixed-size window (causal or non-causal) is used that links the current feature vector to past (and ultimately future) feature vectors to give context to the input. However, this approach has significant shortcomings, such as increased dimensionality (which forces the need for more data, longer training times, and larger models) and a short fixed context."}, {"heading": "2.2. Recurrent neural networks", "text": "This network architecture is known as the recursive neural network (RNN). In an RNN, information from earlier time steps can circulate in principle indefinitely within the network through the directed cycles, with the hidden layers also acting as memory. For a sequence of input vectors {x1,..., xT}, an RNN calculates a sequence of hidden activations {h1,..., hT} and output vectors {y1,..., yT} asht = F (Wxhxt + Whhht \u2212 1 + bh) (1) yt = G (Whyht + by) (2) for all time steps t = 1,..., T, where the matrices W???? denote the weights that connect two layers, b? are bias terms and F and G activation functions. In the case of a deep network with multiple hidden layers, the input layer to activate the previous layer of the Nj-future instance is completely inserted into the NNNN."}, {"heading": "2.3. Long short-term memory", "text": "One of the main reasons for this is the phenomenon of the disappearing gradient problem [19], which causes the influence of past inputs to decay exponentially over time. Long-term memory architecture (LSTM) [13] has been suggested as a solution to this problem. Simple self-connecting neurons are replaced, as in a standard RNN, by units called LSTM memory blocks (Fig. 2). An LSTM memory block is a subnet containing a self-connected memory cell with its tanh input and output activation functions, and three gating neurons - input, forgetting and output - with their corresponding multiplicative units. Equation 1, definition of the hidden activation unit connects, is replaced by the following series of equations: it is one (W xix + W hides \u2212 1 + W cict \u2212 ft i b + b b b b cb) cb = W fox (W x b = W fox b = W W x b = W W x)."}, {"heading": "3. METHOD", "text": "The proposed system receives a raw audio signal as an input signal, extracts spectral characteristics and then assigns them to binary activity indicators of each event class using a BLSTM-RNN (Fig. 1). Each step is described in more detail in this section."}, {"heading": "3.1. Feature extraction", "text": "The input to the system is raw audio signals. In order to take into account different recording conditions, the amplitudes in each recording are normalized to be in [\u2212 1, 1]. The signals are divided into 50 millisecond frames with 50% overlap, and we calculate the log orders of magnitude within the 40 millisecond bands in each frame. We then normalize each frequency band by subtracting the mean value of each frame across all recordings and specifying a variance of the unit (calculation of constants on the training set), a standard procedure when working with neural networks. For each recording, we obtain a long sequence of feature vectors, which is then divided into smaller sequences. We divide each original sequence into three different scales, i.e. in non-overlapping lengths of 10, length 25 and length 100 sequences (corresponding to lengths of 0.25, 0.62 and 2.5 seconds respectively). This allows the network to more easily identify a target component in different time periods, each of which has a specific event."}, {"heading": "3.2. Proposed neural network", "text": "We propose the use of multi-label BLSTM RNNs with multiple hidden levels to map the acoustic characteristics to class activity indicators. The output layer has logistic activation functions and a neuron for each class. We use Gaussian input noises and early pausing to reduce overadjustment, and interrupt training if the cost of the validation set does not decrease for 20 epochs. The output of the network at the time t is a vector yt, where L is the number of classes. Its components yk can be interpreted as subordinate probabilities that each class is active or inactive in the xt frame. These outputs do not need to be summed up to 1, as several classes can be active at the same time. Therefore, unlike most multiclass approaches with neural networks, the output is not normalized by computing the Softmax. Finally, the continuous outputs are undercut to obtain binary indicators of class activity."}, {"heading": "3.3. Data augmentation", "text": "As an additional measure to reduce over-matching, which occurs easily when the data set is small compared to the network size, we also extend the workout quantity through simple transformations. All transformations are applied directly to the extracted characteristics in the frequency range. \u2022 Time shift: We mimic the process of slight deceleration or acceleration of the recordings by stretching the mel spectrogram over time using linear interpolation by factors slightly smaller or greater than 1. \u2022 Time shift in the subrange: We mimic small time shifts in the recordings - in the subrange - linear interpolation of new functional frames between existing frames, maintaining the same frame rate. \u2022 Block blending: new recordings with equal or higher polyphony can be created by combining different parts of the signals within the same context."}, {"heading": "4. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Dataset", "text": "We evaluate the performance of the proposed method using a database consisting of recordings from 10 to 30 minutes in length, from ten real contexts. [24] The contexts are: basketball game, beach, inside a bus, inside a car, hallway, offices, restaurants, shops, streets and stadiums with athletics events. Each context has 8 to 14 recordings, for a total of 103 recordings (1133 minutes). The recordings were recorded with a binaural microphone with a sampling rate of 44.1 kHz and a resolution of 24 bits. Stereo signals from the recordings are converted into mono by averaging the two channels into a single one. Sound events were commented manually within 60 classes, including language, applause, music, pause squeals, keyboard; plus 1 class for rare or unknown events marked as unknown, for a total of 61 classes. All events appear several times in the recordings; some of them are present in different contexts, others are average contextual results - 53% are average contextual results at the same time."}, {"heading": "4.2. Neural networks experiments", "text": "The network has an input layer of 40 units, each of which reads one component of the feature frames, and 4 hidden layers of 200 LSTM units each (100 read the sequence forward, 100 backwards). We train a network only with the original data used in previous work, and one with the data augmentation techniques reported in Section 3.3 to further reduce overmatch. To compare performance with standard LSTM layers, we also train a similar network architecture without bidirectional units on the same data set without augmentation. The network is initialized with evenly distributed weights in [\u2212 0.1, 0.1] and trained as a cost function based on the root average of squared errors. Training is done by backpropagating through time (BPTT) [25]. The extracted features are presented as sequences cut off from the original data - in sequences of 10, 25, and 100 frames - in random sequences of 600 miniatures."}, {"heading": "4.3. Metrics", "text": "To evaluate the performance of the system, we calculate the F1 score for each context in two ways: the average framewise F1 score (F1AvgFram) and the average F1 score in non-overlapping second blocks (F11-sec), as proposed in [4], with each target class and prediction marked as active throughout the block if it is active in at least one frame of the block."}, {"heading": "4.4. Results", "text": "In Table 1, we compare the averages across all contexts for the FNN in [12] with our BLSTM and LSTM networks trained on the same data, and BLSTM networks trained on the same extended data. FNN uses the same functions, but at any time it reads a concatenation of 5 input frames (the current frame and the two previous and two subsequent frames), it has two hidden layers of 1600 hidden units each, downloaded to 800 with maxout activations.The BLSTM network achieves better results than the FNN trained on the same data, and improves performance by a relative 13.5% for the average frame F1 and 4.3% for the 1 second block F1. The unidirectional LSTM network does not work as well as the BLSTM network, but is still better than the FNN trained on the same data."}, {"heading": "5. CONCLUSIONS", "text": "RNNs can encode context information directly in the hidden layers and learn the longer patterns present in the data. Data augmentation techniques effectively reduce overfit and further improve performance. The approach presented exceeds the previous state of the art FNN [12], which was tested on the same large database of real recordings, and has half as many parameters. The average improvement of the entire data set is 15.1% for the average frame F1 and 6.8% for the 1-second block F1.Future work will focus on the search for new data augmentation techniques. Further studies on the model will be developed on attention mechanisms and the expansion of RNNNs by coupling with Convolutionary Neural Networks."}, {"heading": "6. REFERENCES", "text": "In this connection, it is also worth mentioning the fact that the two cases are two cases in which they are two cases in which they are two cases, namely two, three, four, five, five, six, six, seven, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twel"}], "references": [{"title": "Automatic surveillance of the acoustic activity in our living environment", "author": ["Aki H\u00e4rm\u00e4", "Martin F McKinney", "Janto Skowronek"], "venue": "IEEE International Conference on Multimedia and Expo (ICME), 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Environmental sound recognition with time\u2013frequency audio features", "author": ["Selina Chu", "Shrikanth Narayanan", "CC Jay Kuo"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 6, pp. 1142\u20131158, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio keywords generation for sports video analysis", "author": ["Min Xu", "Changsheng Xu", "Lingyu Duan", "Jesse S Jin", "Suhuai Luo"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 4, no. 2, pp. 11, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-dependent sound event detection", "author": ["Toni Heittola", "Annamaria Mesaros", "Antti Eronen", "Tuomas Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, pp. 1\u201313, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Acoustic event detection in real life recordings", "author": ["Annamaria Mesaros", "Toni Heittola", "Antti Eronen", "Tuomas Virtanen"], "venue": "18th European Signal Processing Conference, 2010, pp. 1267\u2013 1271.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised model training for overlapping sound events based on unsupervised source separation", "author": ["Toni Heittola", "Annamaria Mesaros", "Tuomas Virtanen", "Moncef Gabbouj"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 8677\u20138681.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "NMF-based environmental sound source separation using time-variant gain features", "author": ["Satoshi Innami", "Hiroyuki Kasai"], "venue": "Computers & Mathematics with Applications, vol. 64, no. 5, pp. 1333\u20131342, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Realtime detection of overlapping sound events with non-negative matrix factorization", "author": ["Arnaud Dessein", "Arshia Cont", "Guillaume Lemaitre"], "venue": "Matrix Information Geometry, pp. 341\u2013371. Springer, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection using non-negative dictionaries learned from annotated overlapping events", "author": ["Onur Dikmen", "Annamaria Mesaros"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["Annamaria Mesaros", "Toni Heittola", "Onur Dikmen", "Tuomas Virtanen"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 606\u2013618.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Overlapping sound event recognition using local spectrogram features and the generalised hough transform", "author": ["Jonathan Dennis", "Huy Dat Tran", "Eng Siong Chng"], "venue": "Pattern Recognition Letters, vol. 34, no. 9, pp. 1085\u20131093, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["Emre Cakir", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013 610, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Universal onset detection with bidirectional long short-term memory neural networks", "author": ["Florian Eyben", "Sebastian B\u00f6ck", "Bj\u00f6rn Schuller", "Alex Graves"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2010, pp. 589\u2013594.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Polyphonic piano note transcription with recurrent neural networks", "author": ["Sebastian B\u00f6ck", "Markus Schedl"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 121\u2013124.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition using noise-adaptive prototypes", "author": ["Arthur N\u00e1das", "David Nahamoo", "Michael Picheny"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 37, no. 10, pp. 1495\u20131503, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Exploring data augmentation for improved singing voice detection with neural networks", "author": ["Jan Schl\u00fcter", "Thomas Grill"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A software framework for musical data augmentation", "author": ["Brian McFee", "Eric J. Humphrey", "Juan P. Bello"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio context recognition using audio event histograms", "author": ["Toni Heittola", "Annamaria Mesaros", "Antti Eronen", "Tuomas Virtanen"], "venue": "Proc. of the 18th European Signal Processing Conference (EUSIPCO), 2010, pp. 1272\u20131276.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Introducing currennt: The munich opensource cuda recurrent neural network toolkit", "author": ["Felix Weninger"], "venue": "Journal of Machine Learning Research, vol. 16, pp. 547\u2013551, 2015. 5", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "SED in single-source environment is called monophonic detection, which has been the major area of research in this field [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Initial approaches to polyphonic SED include traditional methods for speech recognition, such as the use of mel frequency cepstral coefficients (MFCCs) as features, with Gaussian mixture models (GMMs) combined with hidden Markov models (HMMs) [5, 6].", "startOffset": 243, "endOffset": 249}, {"referenceID": 5, "context": "Initial approaches to polyphonic SED include traditional methods for speech recognition, such as the use of mel frequency cepstral coefficients (MFCCs) as features, with Gaussian mixture models (GMMs) combined with hidden Markov models (HMMs) [5, 6].", "startOffset": 243, "endOffset": 249}, {"referenceID": 6, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "The work in [9] was extended in [10] making learning feasible for long recordings by reducing the dictionary size.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "The work in [9] was extended in [10] making learning feasible for long recordings by reducing the dictionary size.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Other approaches are based on spectrogram analysis with image processing techniques, such as the work in [11] that studies polyphonic SED using generalized Hough transform over local spectrogram features.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "The best results to date in polyphonic SED for real life recordings have been achieved by feedforward neural networks (FNNs), in the form of multilabel time-windowed multi layer perceptrons (MLPs), trained on spectral features of the mixture of sounds [12], temporally smoothing the outputs for continuity.", "startOffset": 252, "endOffset": 256}, {"referenceID": 11, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 183, "endOffset": 191}, {"referenceID": 13, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 183, "endOffset": 191}, {"referenceID": 14, "context": "These networks have obtained excellent results on complex audio detection tasks, such as speech recognition [15] and onset detection [16] (multiclass), polyphonic", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "These networks have obtained excellent results on complex audio detection tasks, such as speech recognition [15] and onset detection [16] (multiclass), polyphonic", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "piano note transcription [17] (multilabel).", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "When instances from future timesteps are available, also future context can be provided to the network by using bi-directional RNN (BRNN) [18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "One of the main reasons is the phenomenon called vanishing gradient problem [19], which makes the influence of past inputs decay exponentially over time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "The long short-term memory (LSTM) [13] architecture was proposed as a solution to this problem.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "All gating neurons represent binary switches but use the logistic function\u2014thus outputting in the range [0, 1]\u2014to preserve differentiability.", "startOffset": 104, "endOffset": 110}, {"referenceID": 13, "context": "A bidirectional long short-term memory (BLSTM) [14] network is obtained by substituting the simple recurrent neurons in a BRNN with LSTM units.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "More details about LSTM, BLSTM and training algorithms can be found in [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "The output of the network at time t is a vector yt \u2208 [0, 1], where L is the number of classes.", "startOffset": 53, "endOffset": 59}, {"referenceID": 11, "context": "Contrarily to [12], where the outputs are smoothed over time using a median filter on a 10-frame window, we do not apply any post-processing since the outputs from the RNN are already smooth.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "In frequency domain we directly achieve a similar result using the mixmax principle [21], overlapping blocks of the log mel spectrogram two at the time.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "Similar techniques have been used in [22, 23].", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "Similar techniques have been used in [22, 23].", "startOffset": 37, "endOffset": 45}, {"referenceID": 23, "context": "We evaluate the performance of the proposed method on a database consisting of recordings 10 to 30 minutes long, from ten real-life contexts [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "All results are presented as averages of the 5-fold cross validation results, with the same train/validation/test partitions used in previous experiments on the same dataset ([10, 12]).", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "All results are presented as averages of the 5-fold cross validation results, with the same train/validation/test partitions used in previous experiments on the same dataset ([10, 12]).", "startOffset": 175, "endOffset": 183}, {"referenceID": 24, "context": "Training is done by back propagation through time (BPTT) [25].", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "After a mini-batch is processed the weights are updated using RMSProp [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "The networks were trained on a GPU (Tesla K40t), with the open-source toolkit Currennt [27] modified to use RMSprop.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "To evaluate the performance of the system we compute F1-score for each context in two ways: average of framewise F1-score (F1AvgFram) and average of F1-score in non-overlapping 1 second blocks (F11-sec) as proposed in [4], where each target class and prediction is marked as active on the whole block if it is active in at least one frame of the block.", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "In Table 1 we compare the average scores over all contexts for the FNN in [12] to our BLSTM and LSTM networks trained on the same data, and BLSTM network trained with the augmented data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "In Table 2 we report the results for each context for the FNN in [12] (FNN), our BLSTM trained on the same data (BLSTM) and our Table 1: Overall F1 scores, as average of individual contexts scores, for the FNN in [12] (FNN) compared to the proposed LSTM, BLSTM and BLSTM with data augmentation (BLSTM+DA).", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "In Table 2 we report the results for each context for the FNN in [12] (FNN), our BLSTM trained on the same data (BLSTM) and our Table 1: Overall F1 scores, as average of individual contexts scores, for the FNN in [12] (FNN) compared to the proposed LSTM, BLSTM and BLSTM with data augmentation (BLSTM+DA).", "startOffset": 213, "endOffset": 217}, {"referenceID": 11, "context": "FNN [12] 58.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "The presented approach outperforms the previous state-of-the-art FNN [12] tested on the same large database of real-life recordings, and has half as many parameters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Table 2: Results for each context in the dataset for the FNN in [12] (FNN), and our approach without data augmentation (BLSTM) and with data augmentation (BLSTM+DA).", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "FNN [12] BLSTM BLSTM+DA FNN [12] BLSTM BLSTM+DA", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "FNN [12] BLSTM BLSTM+DA FNN [12] BLSTM BLSTM+DA", "startOffset": 28, "endOffset": 32}], "year": 2016, "abstractText": "In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.", "creator": "LaTeX with hyperref package"}}}