{"id": "1604.00933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding", "abstract": "We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of 97% using the proposed distributional representations ensemble.", "histories": [["v1", "Mon, 4 Apr 2016 16:18:44 GMT  (497kb,D)", "http://arxiv.org/abs/1604.00933v1", "A short version of this paper has been accepted in \"COMPSAC 2016: The 40th IEEE Computer Society International Conference on Computers, Software &amp; Applications\""]], "COMMENTS": "A short version of this paper has been accepted in \"COMPSAC 2016: The 40th IEEE Computer Society International Conference on Computers, Software &amp; Applications\"", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["walid shalaby", "khalifeh al jadda", "mohammed korayem", "trey grainger"], "accepted": false, "id": "1604.00933"}, "pdf": {"name": "1604.00933.pdf", "metadata": {"source": "CRF", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding", "authors": ["Walid Shalaby", "Khalifeh Al Jadda", "Mohammed Korayem", "Trey Grainger"], "emails": ["wshalaby@uncc.edu", "trey.grainger@careerbuilder.com"], "sections": [{"heading": null, "text": "In fact, most people who are able to survive themselves, to survive and survive themselves, to alienate themselves and their fellow human beings, to alienate and alienate themselves, to alienate and alienate themselves, to alienate and alienate themselves, to alienate, to alienate and alienate, to alienate, to alienate and alienate, to alienate and alienate, to alienate and alienate, to alienate and alienate, to alienate and alienate, to alienate and alienate."}, {"heading": "II. RELATED WORK", "text": "In recent years, the number of those who are able to take to the streets has multiplied, and multiplied, \"he said in an interview with the New York Times.\" I don't think we will be able to get the situation under control, \"he said.\" But I don't think we will be able to get it under control. \"He added,\" I don't think they will be able to get the situation under control. \""}, {"heading": "III. METHODOLOGY", "text": "In this section, we explain our method for recognizing search query types. Our approach uses two distributional semantic representations of search queries. In addition, we use ontological characteristics and linguistic characteristics of search queries to improve the overall performance of the system. The ultimate goal of our system is to categorize a particular search query into one of four categories: company, job title, skills and school. We plan to expand these categories in the future, but these four are the most important ones we originally target."}, {"heading": "A. System Overview", "text": "Before performing ETR, it is of course necessary to perform ER on incoming queries, so that we know the entities for which we are trying to identify an entity type. Our method of detecting known entities and performing entity extraction from queries has already been described in [29]. Essentially, we perform data mining using historical search queries, perform cooperative filters to determine which entities are commonly used by many users, and build a semantic knowledge base that includes entities and related entities found within the mined search results. Based on this semantic knowledge base, we are able to perform entity extraction in future queries for known entities, but we lack two important components: 1) Identification of entities that are not found in our semantic knowledge base. 2) Knowledge of the entity type of each entity identified as an entity entity to solve a mantity query."}, {"heading": "B. Entity Type Recognition Process", "text": "The proposed system combines attributes from different sources to make precise entity type predictions for a particular search unit. This set of attributes represents both our domain-specific knowledge and the real knowledge of the search unit. We call these attributes notes. Figure 1 shows the system design for how a user's search query is analyzed, as well as how the system uses these attributes to perform an accurate ETR. The first clue models contextual information about the query unit in the real world by searching for that unit within Wikipedia using a custom search index. The second clue models domain-specific knowledge by building synonymous vectors of search units using the word2vec model [30]. These vectors are collected using millions of job ads from CareerBuilder. Two other clues that DBpedia and WordNet use are collected to increase the accuracy and scope of search terms across these known job categories, and to enhance them."}, {"heading": "C. Constructing Contextual Vectors", "text": "The purpose of this phase is to enrich the contextual search units with contextual information. To do this, we use Wikipedia as the source for these contextual vectors for all the search units displayed. Since query units must be categorized online, context vectors must be constructed as efficiently as possible. Therefore, we create a reverse index of all Wikipedia articles as a pre-processing step. We create the index using Apache Lucene1, an open source indexing and search engine. For each article, we index title, content, length and categories. We exclude all ambiguities, lists and diversions of all Wikipedia articles as a pre-processing step. As shown in Eq.1, we construct the context vector Xej by first searching for this unit."}, {"heading": "D. Constructing Synonymy Vectors", "text": "The purpose of this phase is to enrich the search units with domain-specific knowledge. CareerBuilder has millions of job openings that are published or changed daily. To use this information, we use the job openings as an intermediate corpus to train a word2vec model. For a specific search unit ej, we generate the synonym vector Sej from words that are most likely to be relevant for distribution in the trained word2vec model.Distributional semantic vectors generated in this phase represent domain-specific knowledge about a given unit. Table II shows the same search units as in Table I along with corresponding synonymous vectors. We can find that the companies and school units are slightly bad and unrepresentative."}, {"heading": "E. Entity Ontological Features", "text": "Another representative feature is extracted from DBpedia by assigning search hits (representing Wikipedia concepts) to their corresponding entries in the DBpedia ontology. We use the type property to determine whether the concept type found belongs to our target categories, in particular companies. After searching for a specific unit ej in the Wikipedia ontology, we call up the top search meeting points (concepts). Then we check whether the title of one of these concepts is identical to ej. If at all, we check whether the type of this concept in the DBpedia ontology is company and then add a new binary feature indicating that Finding.Given the fact that companies are already explicitly found in DBPedia, why do we not simply use the DBpedia type function exclusively for categorization in the enterprise type company ej? There are five reasons why we choose to combine multiple function types in DBpedia type 2, e.g. if we have DBpedia group of companies in the DBpedia-2K category, where DBpedia-2K does not have a lot of scope)."}, {"heading": "F. Entity Linguistic Features", "text": "We use the lexical properties of the search terms to determine whether they belong to any of the target categories, particularly the Job Title. The motivation behind this approach is the fact that almost all Job Title units contain an agent noun (e.g. Director, Developer, Nurse, Manager... etc.) To determine whether a company could represent a Job Title, we search its words within the WordNet dictionary, where all agents nouns are stored in the < noun.person > lexical file. After finding one, we add a new binary feature indicating that Finding. Although it might be tempting to rely solely on the agent noun from the WordNet lexicon when categorizing Job Title units, two challenges prevent this: 1) CareerBuilder operates job boards in many countries and in many different languages. Therefore, if possible, we tend to use Job Title models that are language-independent of the Job Restriction Category for Enordings only."}, {"heading": "G. Building the Prediction Model", "text": "To build the ETR model, we use supervised machine learning based on a very large set of search terms derived from CareerBuilder's search protocols. For each search unit discovered, we generate: 1) a Contextual Vector (Xej) using the Wikipedia index. 2) a Synonym Vector (Sej) using the word2vec model. 3) an Ontological Type (ontej) when the entity refers to the aDBpedia concept. This is a binary attribute that applies if DBpedia type companies are. 4) A Lexical Type (lexej). This is a binary attribute that applies if one of the entity terms has a < noun.person > type in WordNet, i.e., it is an agent number that applies. 4https: / en.wikia.org / wiki.org / wiki.org, but we combine these simple Turnkey _ Group.org / Turnkey _ Systems _ to a simple set."}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": "In this section, we present our empirical results. We begin by describing the data set used in experiments, and then explain various models developed for ETR, together with their results."}, {"heading": "A. Data set", "text": "We create our ETR models with the largest tagged data set of companies owned by CareerBuilder. The data set contains more than 177K tagged companies divided into four categories, as shown in Table III. These companies have been identified from CareerBuilder search logs, job openings, and CVs, and manually reviewed by commenters working at CareerBuilder."}, {"heading": "B. Experimental Setup", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "C. Results", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "V. CONCLUSION", "text": "In this paper, we presented an effective approach to ETR of search units in the field of job search and recruitment. We proposed a novel set of features that enriches short query units with real and domain-specific knowledge. The assembler presentation model includes features that represent: 1) context information in Wikipedia, 2) embedding information in millions of job ads, 3) class types in DBpedia for companies, and 4) linguistic properties in WordNet for Job Title Entities. Our approach is new and different from other ETR approaches. To our knowledge, the generation of distribution-related semantic vectors of query units that use contextual information from Wikipedia as a search index has never been used before in the bibliography. Assessment results for a data set of more than 177K search units were very promising. Results showed that our Wikipedia-based job category model, based on Wikipedia's state-of-the-art 2D representation of four Entients, was trained to achieve the goal of four."}], "references": [{"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["J. Kazama", "K. Torisawa"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 698\u2013 707, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, vol. 30, no. 1, pp. 3\u201326, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting personal names from email: Applying named entity recognition to informal text", "author": ["E. Minkov", "R.C. Wang", "W.W. Cohen"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 443\u2013450, Association for Computational Linguistics, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Person name entity recognition for arabic", "author": ["K. Shaalan", "H. Raza"], "venue": "Proceedings of the 2007 Workshop on Computational Approaches to Semitic Languages: Common Issues and Resources, pp. 17\u201324, Association for Computational Linguistics, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity recognition using an hmm-based chunk tagger", "author": ["G. Zhou", "J. Su"], "venue": "proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pp. 473\u2013480, Association for Computational Linguistics, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Building taxonomy of web search intents for name entity queries", "author": ["X. Yin", "S. Shah"], "venue": "Proceedings of the 19th international conference on World wide web, pp. 1001\u20131010, ACM, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Pgmhd: A scalable probabilistic graphical model for massive hierarchical data problems", "author": ["K. AlJadda", "M. Korayem", "C. Ortiz", "T. Grainger", "J.A. Miller", "W.S. York"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 55\u201360, IEEE, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving the quality of semantic relationships extracted from massive user behavioral data", "author": ["K. AlJadda", "M. Korayem", "T. Grainger"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on, pp. 2951\u20132953, IEEE, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Query sense disambiguation leveraging large scale user behavioral data", "author": ["M. Korayem", "C. Ortiz", "K. AlJadda", "T. Grainger"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on, pp. 1230\u20131237, IEEE, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to the conll- 2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pp. 142\u2013147, Association for Computational Linguistics, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pp. 147\u2013155, Association for Computational Linguistics, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Erd\u201914: entity recognition and disambiguation challenge", "author": ["D. Carmel", "M.-W. Chang", "E. Gabrilovich", "B.-J.P. Hsu", "K. Wang"], "venue": "ACM SIGIR Forum, vol. 48, pp. 63\u201377, ACM, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Semeval-2015 task 13: Multilingual all-words sense disambiguation and entity linking", "author": ["A. Moro", "R. Navigli"], "venue": "Proceedings of SemEval-2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustype: Effective entity recognition and typing by relation phrasebased clustering", "author": ["X. Ren", "A. El-Kishky", "C. Wang", "F. Tao", "C.R. Voss", "J. Han"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 995\u2013 1004, ACM, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to link with wikipedia", "author": ["D. Milne", "I.H. Witten"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, pp. 509\u2013518, ACM, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "EMNLP-CoNLL, vol. 7, pp. 708\u2013716, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity recognition approaches", "author": ["A. Mansouri", "L.S. Affendey", "A. Mamat"], "venue": "International Journal of Computer Science and Network Security, vol. 8, no. 2, pp. 339\u2013344, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Named entity recognition in query", "author": ["J. Guo", "G. Xu", "X. Cheng", "H. Li"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pp. 267\u2013274, ACM, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective annotation of wikipedia entities in web text", "author": ["S. Kulkarni", "A. Singh", "G. Ramakrishnan", "S. Chakrabarti"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 457\u2013466, ACM, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective entity linking in web text: a graph-based method", "author": ["X. Han", "L. Sun", "J. Zhao"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 765\u2013774, ACM, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "An entity-topic model for entity linking", "author": ["X. Han", "L. Sun"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 105\u2013115, Association for Computational Linguistics, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Entity linking at web scale", "author": ["T. Lin", "O. Etzioni"], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pp. 84\u201388, Association for Computational Linguistics, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining wiki resources for multilingual named entity recognition", "author": ["A.E. Richman", "P. Schone"], "venue": "ACL, pp. 1\u20139, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Understanding user\u2019s query intent with wikipedia", "author": ["J. Hu", "G. Wang", "F. Lochovsky", "J.-t. Sun", "Z. Chen"], "venue": "Proceedings of the 18th international conference on World wide web, pp. 471\u2013480, ACM, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Transforming wikipedia into named entity training data", "author": ["J. Nothman", "J.R. Curran", "T. Murphy"], "venue": "Proceedings of the Australian Language Technology Workshop, pp. 124\u2013132, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Identifying and extracting named entities from wikipedia database using entity infoboxes", "author": ["M. Mohamed", "M. Oussalah"], "venue": "International Journal of Advanced Computer Science and Applications (IJACSA), vol. 5, no. 7, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Entity extraction, linking, classification, and tagging for social media: a wikipedia-based approach", "author": ["A. Gattani", "D.S. Lamba", "N. Garera", "M. Tiwari", "X. Chai", "S. Das", "S. Subramaniam", "A. Rajaraman", "V. Harinarayan", "A. Doan"], "venue": "Proceedings of the VLDB Endowment, vol. 6, no. 11, pp. 1126\u20131137, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Search query categorization at scale", "author": ["M. Laclav\u00edk", "M. Ciglan", "S. Steingold", "M. Seleng", "A. Dorman", "S. Dlugolinsky"], "venue": "Proceedings of the 24th International Conference on World Wide Web Companion, pp. 1281\u20131286, International World Wide Web Conferences Steering Committee, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Crowdsourced query augmentation through semantic discovery of domain-specific jargon", "author": ["K. AlJadda", "M. Korayem", "T. Grainger", "C. Russell"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 808\u2013815, IEEE, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A sub-task related to ER is the Entity Type Recognition (ETR) which refers to categorizing these entities into a predefined set of types [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "The focus of the majority of ETR research has been on Named Entity Recognition (NER), which typically limits entity types to Person, Location, and Organization [2]\u2013[5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "The focus of the majority of ETR research has been on Named Entity Recognition (NER), which typically limits entity types to Person, Location, and Organization [2]\u2013[5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 5, "context": "ETR in search queries is considered extremely important; a Microsoft\u2019s study reported that 71% of queries submitted to their Bing search engine contain named entities somewhere, while 20 \u2212 30% are purely named entities [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "We integrated this model within CareerBuilder\u2019s semantic search engine [7]\u2013[9], which improved the quality of search results for tens of millions of job seekers every month.", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "We integrated this model within CareerBuilder\u2019s semantic search engine [7]\u2013[9], which improved the quality of search results for tens of millions of job seekers every month.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Both ETR and NER have experienced a surge in the research community in recent years [10]\u2013[16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Both ETR and NER have experienced a surge in the research community in recent years [10]\u2013[16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "[2] and Mansouri et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] presented comprehensive reviews about different approaches for NER including several representations that leverage dictionaries, corpora, and various classification methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] presented a formulation for both NER and ETR in search queries using a probabilistic approach and Latent Dirichlet Allocation (LDA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 147, "endOffset": 150}, {"referenceID": 18, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "[1] proposed a methodology which relies on having a Wikipedia page whose title is similar to the given entity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Richman and Schone proposed a novel system for multilingual NER [23] .", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Using Wikipedia concepts as a representation space for query\u2019s intent was introduced in [24].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "The system introduced in [25] transforms links to Wikipedia articles into named entity annotations by classifying the target articles into the classic named entity types Person, Location, and Organization.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "Utilizing Wikipedia infobox for ETR was presented in [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "The system introduced in [27] converted Wikipedia into a structured knowledge base (KB).", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "We consider [28] as the most related work to ours.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Our methodology for recognizing known entities and performing Entity Extraction from queries was previously described in [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The second clue models domain-specific knowledge by building synonym vectors of search entities using the word2vec model [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "Finally, we evaluate all the ETR models using a Support Vector Machine (SVM) classifier with a linear kernel, leveraging the scikit-learn machine learning library [31].", "startOffset": 163, "endOffset": 167}], "year": 2016, "abstractText": "We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve microaveraged F1 score of 97% using the proposed distributional representations ensemble.", "creator": "LaTeX with hyperref package"}}}