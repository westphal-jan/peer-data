{"id": "1604.00938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Multi-Field Structural Decomposition for Question Answering", "abstract": "This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an absolute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.", "histories": [["v1", "Mon, 4 Apr 2016 16:33:15 GMT  (141kb,D)", "http://arxiv.org/abs/1604.00938v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomasz jurczyk", "jinho d choi"], "accepted": false, "id": "1604.00938"}, "pdf": {"name": "1604.00938.pdf", "metadata": {"source": "CRF", "title": "Multi-Field Structural Decomposition for Question Answering", "authors": ["Tomasz Jurczyk", "Jinho D. Choi"], "emails": ["tomasz.jurczyk@emory.edu", "jinho.choi@emory.edu"], "sections": [{"heading": "1 Introduction", "text": "Regarding machine reading, answering questions has recently aroused great interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information gathering (Schiffman et al., 2007; Kolomiyets and Moens, 2011). People from these two fields of research, NLP and IR, have shown tremendous progress in answering questions, but little effort has been made to adapt technologies from both sides. NLP often tackles the task by analyzing linguistic aspects, while the IR side tackles it by looking for more likely patterns. While these two approaches work well individually, more complex solutions are needed to address a wide range of questions."}, {"heading": "2 Related Work", "text": "Shen and Lapata (2007) examined the contribution of semantic roles to factoid response to questions and showed promising results. Pizzato and Molla (2008) proposed a question prediction language model that provided rich information and achieved improved speed and accuracy. Although our work is distinguished from hers because we consider several fields, while the others consider only one field that represents semantic roles. Ferrucci et al. (2010) introduced IBM Watson with a hybrid approach between NLP and IR and expanded the question-answer task to another level. Fader et al. (2013) proposed a paraphrase-driven perceptual learning approach using a seed lexicon. Our learning process is similar, but different in that we learn weights for individual fields rather than lexiccones. Yih et al. (2014) introduced a semantic parsing framework for open domain questions that uses commutative units to measure neural similarities."}, {"heading": "3 Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overall framework", "text": "Figure 1 shows the overall frame. Our system is designed in a modular architecture so that any further expansion of the fields can be easily integrated. The system takes input documents, generates linguistic structures using NLP tools, splits them into several fields and indexes these fields. Questions are processed in the same way. To answer a question, the system queries the index for each field extracted from the question and measures the relevance value. All documents are linked to the fields in terms of relevance values and their weighting, and the document with the highest score is selected as the answer."}, {"heading": "3.2 Modules", "text": "Our system consists of several modules that are closely interconnected and offer a fully functional solution to the question of answering the selection task."}, {"heading": "3.2.1 Documents and questions", "text": "Documents provide the context in which the questions find their answers. Each document can contain one or more sentences in which answers to upcoming questions are commented for training purposes. Documents can simply be Wikipedia articles, news articles, fictitious stories, etc. Questions are treated like normal documents that contain only one sentence."}, {"heading": "3.2.2 NLP tools", "text": "To generate syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role marker (Choi and Palmer, 2011) and the correlation resolution tool in ClearNLP1. It is important to ensure good and robust accuracy for these NLP tools, as all the following modules depend on their performance."}, {"heading": "3.2.3 Field extractor", "text": "The Field Extractor extracts the linguistic structures from the NLP tools and splits them into several fields (Section 3.3). All fields extracted from the documents are passed to the index engine, while fields extracted from the questions are sent directly to the Answer Ranker module."}, {"heading": "3.2.4 Index engine", "text": "The index engine is a search server that receives a list of fields that have been broken down by the field extractor, indexes terms in the fields, and answers to questions generated from questions with their relevance values. We used Elastic Search2 because it provides a distributed, multi-tenant search."}, {"heading": "3.2.5 Answer ranker", "text": "The Answer Ranker takes the fields extracted from a question, converts them into queries, and creates a matrix of documents with their relevance values across all fields using the index engine (Section 3.4), and uses different weights for individual fields trained by statistical modeling (Section 3.5).1http: / / www.clearnlp.com 2https: / / www.elastic.co"}, {"heading": "3.3 Structural decomposition", "text": "Figure 2 shows an example of how the sentence is divided into several fields, consisting of syntactic and semantic structures. Due to the expandability of our field extractor, additional groups and fields can be easily integrated. Currently, our system supports 24 fields grouped into the following three categories: \u2022 Lexic fields (e.g. word forms, lemmas) \u2022 Syntactic fields (e.g. dependency labels) \u2022 Semantic fields (e.g. semantic roles, spacing between predicates)."}, {"heading": "3.4 Answer ranking", "text": "When a question is asked q, it breaks down into the n-number of fields. Each field is converted into a query in which certain words are replaced by placeholders (e.g., {where a1, pred, she a2} \u2192 {* a1 pred she a2}). Subsequently, the relevance value r is added between each field in the question and the same field in each document dt-D. Note that in our dataset each document contains only one sentence, so that retrieving a document equals retrieving a sentence. The following equations describe how the document d is retrieved by measuring the overall score f (q, dt) using the relevance score f (qi, dti) and the weights xi (id = gdt, q (q) (q) (q) (q), q (q) (q), q (q) (t), q (q), q (t), q (q), q (q)."}, {"heading": "3.5 Training weights for individual fields", "text": "Algorithm 1 shows how the weights for all fields are learned during the training. We adapt the averaged perceptron algorithm, which is widely used for many NLP tasks. All weights are initialized to 1. For each question q-Q it predicts the document d-q, which most likely contains the answer. If d-q is wrong, it compares the relevance value r between (q, d-q) and (q, d) for each field and updates the weight accordingly, where d is the true document from the oracle. This procedure is repeated several times by iterations. Finally, the algorithm returns the averaged weights, with each dimension representing the weight for each field.Algorithm 1 Average perceptron trainings.Input: D: Document set, Q: Question set. M: maximum number of iterations, \u03b1: Learning rate. Output: The average weight vector.1: ~ Profile 1; ~ 0: 2 \u00b7 sign, Q: 4-xi = Q: Q: Q: 4: xi = Q: Q: Q: 1: Q: Q: 4-xi = Q"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data and evaluation metrics", "text": "Our approach is based on a subset of the bAbI tasks (Weston et al., 2015). The original data contain 20 tasks, each of which represents a different kind of question-answer challenge. We select 8 tasks in which the answer to a single question is located within a sentence. To ensure consistency and replicability, we follow the same training, development and evaluation set splits as intended, with each set containing 1,000 questions.For the evaluation metrics we use the mean average precision (MAP) and the mean reciprocity (MRR) of the top 3 predictions. The mean average precision is measured by the number of questions for which sentences containing the answers are correctly selected as the best predictions. The reciprocal rank of a query answer is the multiplicative inversion of the rank of the first correct answer."}, {"heading": "4.2 Evaluation", "text": "Table 1 shows the results of our system on different types of questions. The MAP and MRR show a clear correlation in terms of the number of active fields. For most tasks, using only the lexical fields is not a good performance. The fictional stories contained in these data often contain several occurrences of the same lexical fields, and the lexical fields alone are not able to select the correct answer. Significantly lower accuracy for the last task is due to the fact that in addition to an answer within a sentence, several passages for the same question are required to correctly locate the sentence with the answers. Lexical fields, paired with only syntactic fields, do not perform much better. It could be due to the fact that the syntactic fields containing ordinary dependency designations do not provide sufficient context-related information, so that they do not generate enough characteristics for static learning to capture specific characteristics of the context. The significant improvement is achieved, however, when the fields are added to allow for a deeper understanding of the semantic content."}, {"heading": "5 Conclusion", "text": "Our system splits linguistic structures into several fields, indexes terms of individual fields, and retrieves the documents that contain the answers in terms of weighted relevance values. We see a marked improvement when we add more semantic fields and apply average perceptron learning to determine statistical weights for the fields. In the future, we plan to expand our work by integrating additional field levels (e.g. Freebase, WordNet). In addition, we plan to improve our NLP tools to allow an even deeper understanding of the context for more complex questions."}], "references": [{"title": "Transition-based Dependency Parsing with Selec", "author": ["Jinho D. Choi", "Andrew McCallum"], "venue": null, "citeRegEx": "Choi and McCallum.,? \\Q2013\\E", "shortCiteRegEx": "Choi and McCallum.", "year": 2013}, {"title": "Transitionbased Semantic Role Labeling Using Predicate Argument Clustering", "author": ["Jinho D. Choi", "Martha Palmer."], "venue": "Proceedings of ACL workshop on Relational Models of Semantics, RELMS\u201911, pages 37\u201345.", "citeRegEx": "Choi and Palmer.,? 2011", "shortCiteRegEx": "Choi and Palmer.", "year": 2011}, {"title": "Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection", "author": ["Jinho D. Choi", "Martha Palmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL\u201912, pages 363\u2013367,", "citeRegEx": "Choi and Palmer.,? 2012", "shortCiteRegEx": "Choi and Palmer.", "year": 2012}, {"title": "Paraphrase-Driven Learning for Open Question Answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL\u201913, pages 1608\u20131618.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Building Watson: An Overview", "author": ["David A. Ferrucci", "Eric W. Brown", "Jennifer ChuCarroll", "James Fan", "David Gondek", "Aditya Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John M. Prager", "Nico Schlaefer", "Christopher A. Welty"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Learning Knowledge Graphs for Question Answering through Conversational Dialog", "author": ["Ben Hixon", "Peter Clark", "Hannaneh Hajishirzi."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Hixon et al\\.,? 2015", "shortCiteRegEx": "Hixon et al\\.", "year": 2015}, {"title": "A Survey on Question Answering Technology from an Information Retrieval Perspective", "author": ["Oleksandr Kolomiyets", "Marie-Francine Moens."], "venue": "Information Sciences, 181(24):5412\u20135434.", "citeRegEx": "Kolomiyets and Moens.,? 2011", "shortCiteRegEx": "Kolomiyets and Moens.", "year": 2011}, {"title": "Linguistic kernels for answer re-ranking in question answering systems", "author": ["Alessandro Moschitti", "Silvia Quarteroni."], "venue": "Information and Processing Management, 47(6):825\u2013842.", "citeRegEx": "Moschitti and Quarteroni.,? 2011", "shortCiteRegEx": "Moschitti and Quarteroni.", "year": 2011}, {"title": "Indexing on Semantic Roles for Question Answering", "author": ["Luiz Augusto Pizzato", "Diego Moll\u00e1."], "venue": "Proceedings of the 2nd workshop on Information Retrieval for Question Answering, IR4QA\u201908, pages 74\u201381.", "citeRegEx": "Pizzato and Moll\u00e1.,? 2008", "shortCiteRegEx": "Pizzato and Moll\u00e1.", "year": 2008}, {"title": "Question Answering Using Integrated Information Retrieval and Information Extraction", "author": ["Barry Schiffman", "Kathleen McKeown", "Ralph Grishman", "James Allan."], "venue": "The Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Schiffman et al\\.,? 2007", "shortCiteRegEx": "Schiffman et al\\.", "year": 2007}, {"title": "Using Semantic Roles to Improve Question Answering", "author": ["Dan Shen", "Mirella Lapata."], "venue": "Proceedings of the 2007 Joint Conference on Empirical", "citeRegEx": "Shen and Lapata.,? 2007", "shortCiteRegEx": "Shen and Lapata.", "year": 2007}, {"title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Question Answering Using Enhanced Lexical Semantic Models", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL\u201913, pages", "citeRegEx": "Yih et al\\.,? 2013", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic Parsing for Single-Relation Question Answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL\u201914, pages 643\u2013648.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.", "startOffset": 137, "endOffset": 207}, {"referenceID": 12, "context": "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.", "startOffset": 137, "endOffset": 207}, {"referenceID": 5, "context": "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.", "startOffset": 137, "endOffset": 207}, {"referenceID": 9, "context": ", 2015) and information retrieval (Schiffman et al., 2007; Kolomiyets and Moens, 2011).", "startOffset": 34, "endOffset": 86}, {"referenceID": 6, "context": ", 2015) and information retrieval (Schiffman et al., 2007; Kolomiyets and Moens, 2011).", "startOffset": 34, "endOffset": 86}, {"referenceID": 6, "context": "Pizzato and Moll\u00e1 (2008) proposed a question prediction language model providing rich information and achieved improved speed and accuracy.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Ferrucci et al. (2010) presented IBM Watson taking a hybrid approach between NLP and IR, and advanced the question answering task to another level.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon. Our learning process is similar; however, it is distinguished in a way that we learn weights for individual fields instead of lexicons. Yih et al. (2014) introduced a semantic parsing framework for open domain question answering, which used convolutional neural networks for measuring similarities between decomposed entities.", "startOffset": 0, "endOffset": 254}, {"referenceID": 3, "context": "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon. Our learning process is similar; however, it is distinguished in a way that we learn weights for individual fields instead of lexicons. Yih et al. (2014) introduced a semantic parsing framework for open domain question answering, which used convolutional neural networks for measuring similarities between decomposed entities. Weston et al. (2015) presented the Memory Networks models designed to memorize information about known objects and ar X iv :1 60 4.", "startOffset": 0, "endOffset": 448}, {"referenceID": 2, "context": "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.", "startOffset": 91, "endOffset": 114}, {"referenceID": 0, "context": "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.", "startOffset": 138, "endOffset": 163}, {"referenceID": 1, "context": "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.", "startOffset": 191, "endOffset": 214}, {"referenceID": 11, "context": "Our approach is evaluated on a subset of the bAbI tasks (Weston et al., 2015).", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": "Not that this data set has also been used for evaluating the Memory Networks approach to question answering (Weston et al., 2015).", "startOffset": 108, "endOffset": 129}], "year": 2016, "abstractText": "This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an absolute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.", "creator": "TeX"}}}