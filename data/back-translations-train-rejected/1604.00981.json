{"id": "1604.00981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe &amp; Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary.", "histories": [["v1", "Mon, 4 Apr 2016 18:40:05 GMT  (418kb,D)", "http://arxiv.org/abs/1604.00981v1", "5 pages with 3 figures"], ["v2", "Fri, 15 Apr 2016 18:49:12 GMT  (52kb,D)", "http://arxiv.org/abs/1604.00981v2", "5 pages with 3 figures"], ["v3", "Tue, 21 Mar 2017 07:44:39 GMT  (570kb,D)", "http://arxiv.org/abs/1604.00981v3", "10 pages"]], "COMMENTS": "5 pages with 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NE", "authors": ["jianmin chen", "xinghao pan", "rajat monga", "samy bengio", "rafal jozefowicz"], "accepted": false, "id": "1604.00981"}, "pdf": {"name": "1604.00981.pdf", "metadata": {"source": "CRF", "title": "REVISITING DISTRIBUTED SYNCHRONOUS SGD", "authors": ["Jianmin Chen", "Rajat Monga", "Samy Bengio"], "emails": ["jmchen@google.com", "rajatmonga@google.com", "bengio@google.com", "rafalj@google.com"], "sections": [{"heading": null, "text": "REVISITING DISTRIBUTED SYNCHRONOUS SGDJianmin Chen, Rajat Monga, Samy Bengio & Rafal Jozefowicz Google Brain Mountain View, CA, USA {jmchen, rajatmonga, bengio, rafalj} @ google.com"}, {"heading": "1 THE NEED FOR A LARGE SCALE DEEP LEARNING INFRASTRUCTURE", "text": "The recent success of deep learning approaches in areas such as speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements, but also from the fact that the size of available training data has grown significantly over the years along with computing power in both CPUs and GPUs. While a single GPU often offers algorithmic simplicity and speed up to a certain scale of data and models, there is an operating point where distributed implementation of training algorithms for deep architectures becomes necessary."}, {"heading": "2 ASYNCHRONOUS STOCHASTIC GRADIENT DESCENT", "text": "In 2012, Dean et al. (2012) presented their approach to a distributed stochastic gradient descendant algorithm, which consists of two main components: firstly, the parameters of the model can be distributed across multiple servers, depending on the architecture; this server offset is referred to as a parameter server; secondly, several workers can process data in parallel and communicate with the parameter servers; each worker processes a mini-batch of data independently of the others, as follows: \u2022 he retrieves from the parameter servers the most up-to-date parameters of the model required to process the current mini-stack; \u2022 he then calculates gradients of loss in relation to these parameters; \u2022 finally, these gradients are sent back to the parameter servers, which then update the model accordingly. Since each worker communicates with the parameter servers that are independent of the others, this model is referred to as asynchronous stochastic gradient descent (or Async-SD)."}, {"heading": "3 REVISITING SYNCHRONOUS SGD AND ITS VARIANTS", "text": "Both Dean et al. (2012) and Chilimbi et al. (2014) use versions of Async-SGD where the main problem is that each worker calculates slopes over a potentially old version of the model. To eliminate this discrepancy, we propose here to reconsider a synchronous version of the distributed stochastic slope (Sync-SGD), where the parameter servers wait for all workers Xiv: 160 4.00 981v 1 [cs.L G] 4A pr2 016 to send their slopes, aggregate them, and then send the updated parameters to all workers to ensure that the actual algorithm is a true stochastic slope drop, where the actual stack size is the sum of all minichar sizes of the workers. While this approach solves the discrepancy problem, it also addresses two potential problems: the effective size of the stack is significantly larger, the sum of the stack and the actual size of all the stacks."}, {"heading": "4 IMAGENET EXPERIMENTS", "text": "We conducted experiments with the ImageNet Challenge dataset (Russakovsky et al., 2015), where the task is to classify images from 1000 categories. We used the latest Inception model from Szegedy et al. (2016) and trained it under various conditions, including with 50 to 200 workers, each running on a k40 GPU, and with asynchronous SGD, synchronous SGD and synchronous SGD with backups. All experiments in this paper use the TensorFlow system Abadi et al. (2015)."}, {"heading": "50 78.83 97.67 1.89", "text": "As a result, the number of unemployed has tripled in recent years, while the number of unemployed has increased by more than half in the last ten years."}, {"heading": "5 OTHER EXPERIMENTS", "text": "We also conducted experiments on large-scale character language models (LMs) trained on the One Billion Word benchmark dataset (Chelba et al. (2013), which is a popular yardstick for evaluating LMs. The goal is to predict the next letter of a sequence based on a history of past words. To make it mathematically efficient, we started with the best-trained model by Jozefowicz et al. (2016), which takes character as input and matches probabilities to the next words. As we were interested in predicting characters, the last layer of the model was replaced by a smaller LSTM, which attempts to predict the next word at once. The resulting architecture works with a limitless vocabulary, as it can consume any input and produce any output. Further details on the experimental setting are available in Section 5.5 of Jozefowicz et al. (2016) The use of 32 synchronous workers resulted in a significantly lower per-word performance improvement in a RAx49 problem: one word was required under one final D."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "Distributed training strategies for deep learning architectures will become increasingly important as the data sets grow in size. In this work, we have shown that synchronous SGD with backup staff is a viable and scalable strategy. We are currently experimenting with different types of data sets, including word-level language models, where parts of the model (the embedding layers) are often very sparse, resulting in very different communication constraints. We are also working to further improve the performance of synchronous training, such as combining grits of multiple workers sharing the same machine before sending them to the parameter servers to reduce communication effort."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "Chilimbi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M.A. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "The tail at scale", "author": ["Jeffrey Dean", "Luiz Andr Barroso"], "venue": "Communications of the ACM,", "citeRegEx": "Dean and Barroso.,? \\Q2013\\E", "shortCiteRegEx": "Dean and Barroso.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "In ArXiv", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "In International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In ArXiv", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs.", "startOffset": 83, "endOffset": 104}, {"referenceID": 2, "context": "In 2012, Dean et al. (2012) presented their approach for a distributed stochastic gradient descent algorithm.", "startOffset": 9, "endOffset": 28}, {"referenceID": 2, "context": "A similar approach was later proposed by Chilimbi et al. (2014). In practice, it means that while a worker computes gradients of the loss with respect to its parameters on a given mini-batch, other workers also interact with the parameter servers and thus potentially update its parameters; hence when a worker sends back its gradients to the parameter server, these gradients are usually computed w.", "startOffset": 41, "endOffset": 64}, {"referenceID": 2, "context": "Both Dean et al. (2012) and Chilimbi et al.", "startOffset": 5, "endOffset": 24}, {"referenceID": 2, "context": "(2012) and Chilimbi et al. (2014) use versions of Async-SGD where the main potential problem is that each worker computes gradients over a potentially old version of the model.", "startOffset": 11, "endOffset": 34}, {"referenceID": 9, "context": "We conducted experiments on the ImageNet Challenge dataset (Russakovsky et al., 2015), where the task is to classify images out of 1000 categories.", "startOffset": 59, "endOffset": 85}, {"referenceID": 9, "context": "We conducted experiments on the ImageNet Challenge dataset (Russakovsky et al., 2015), where the task is to classify images out of 1000 categories. We used the latest Inception model from Szegedy et al. (2016) and trained it in several conditions, including using from 50 to 200 workers, each of which runs on a k40 GPU, and using asynchronous SGD, synchronous SGD and synchronous SGD with backups.", "startOffset": 60, "endOffset": 210}, {"referenceID": 9, "context": "We conducted experiments on the ImageNet Challenge dataset (Russakovsky et al., 2015), where the task is to classify images out of 1000 categories. We used the latest Inception model from Szegedy et al. (2016) and trained it in several conditions, including using from 50 to 200 workers, each of which runs on a k40 GPU, and using asynchronous SGD, synchronous SGD and synchronous SGD with backups. All the experiments in this paper are using the TensorFlow system Abadi et al. (2015).", "startOffset": 60, "endOffset": 485}, {"referenceID": 1, "context": "We also conducted experiments on large scale character language models (LMs) trained on the One Billion Word Benchmark dataset (Chelba et al. (2013)), which is a popular benchmark for evaluating", "startOffset": 128, "endOffset": 149}, {"referenceID": 7, "context": "In order to make it computationally efficient, we started with the best pre-trained model from Jozefowicz et al. (2016) that takes characters as inputs and assigns probabilities to the next words.", "startOffset": 95, "endOffset": 120}, {"referenceID": 7, "context": "In order to make it computationally efficient, we started with the best pre-trained model from Jozefowicz et al. (2016) that takes characters as inputs and assigns probabilities to the next words. Since we were interested in predicting characters, the last layer of the model was replaced with a smaller LSTM that tries to predict the next word one character at a time. The resulting architecture works over an unbounded vocabulary as it can consume any input and produce any output. More details about the experimental setting is available in Section 5.5 of Jozefowicz et al. (2016). Using 32 synchronous workers gave us a few percent improvement of the final performance: below 49 per-word perplexity.", "startOffset": 95, "endOffset": 584}, {"referenceID": 5, "context": "Finally, we explored distributed training on the DRAW model (Gregor et al. (2015)).", "startOffset": 61, "endOffset": 82}], "year": 2016, "abstractText": "In 2012, Dean et al. (2012) presented their approach for a distributed stochastic gradient descent algorithm. It consists of two main ingredients. First, the parameters of the model can be distributed on multiple servers, depending on the architecture. This set of servers are called the parameter servers. Second, there can be multiple workers processing data in parallel and communicating with the parameter servers. Each worker processes a mini-batch of data independently of the other ones, as follows:", "creator": "LaTeX with hyperref package"}}}