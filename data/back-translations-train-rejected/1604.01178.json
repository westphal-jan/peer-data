{"id": "1604.01178", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks", "abstract": "In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.", "histories": [["v1", "Tue, 5 Apr 2016 08:50:27 GMT  (2422kb,D)", "http://arxiv.org/abs/1604.01178v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aliaksei severyn", "alessandro moschitti"], "accepted": false, "id": "1604.01178"}, "pdf": {"name": "1604.01178.pdf", "metadata": {"source": "CRF", "title": "Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks", "authors": ["Aliaksei Severyn", "Alessandro Moschitti"], "emails": ["severyn@google.com", "amoschitti@qf.org.qa"], "sections": [{"heading": "1 Introduction", "text": "This year we will be able to move to another world, to move to another world, to move to another world, to be able to move to another world, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in that world, \"he said."}, {"heading": "2 Our Deep Learning Model", "text": "This section explains the architecture of our deep learning model for modeling question-answer pairs to reorder answer sets. We treat the problem of selecting answer sets as a simple binary classification, in which answer candidates with higher predictive values are ranked above those with lower results. Formally, each question Qi is associated with a list of labeled candidate answer sets {(Yi1, AI1),.., (Yin, AIN)}, where the terms Yij, {0, 1} correspond to 1 answers that contain a correct answer and 0 otherwise. Our goal is to learn a decision function that assigns each question-answer pair to a rating that reflects their similarity: f (Inequality, (Qi, AIJ), which is a function that encodes question-answer pairs into a common attribute space, and vice versa."}, {"heading": "2.1 Distributional sentence model", "text": "The architecture of our network for mapping sentences to vectors is illustrated in Fig. 1. It is mainly inspired by the revolutionary architectures used in (Kalchbrenner et al., 2014; Kim, 2014) to perform various sentence classification tasks. However, unlike previous work, the goal of our distribution theorem model is to learn intermediate representations of questions and answers used to calculate their semantic match. Our sentence model consists of a single broad revolutionary layer followed by nonlinearity and simple max pooling. Below, we give a brief explanation of its main components: sentence matrix, activations, wave and pooling layers."}, {"heading": "2.1.1 Sentence matrix", "text": "The input is a sentence treated as a word sequence: [w1,.., w | s |], with each word taken from a finite vocabulary V. The architecture of neural networks is not well suited for dealing with individual words, so they are represented by low-dimensional, real-weighted, dense vectors w-Rdw, which were searched in a matrix W-Rdw-Rdw-V-V-V-V (whose columns correspond to the words in V. The assignment of words to their word embeddings is done by a reference table operationLTW (wi) = wi. Therefore, for each input sentence s we build a sentence matrix S, in which each i-th column corresponds to a word embedding. To learn to capture and compose characteristics of individual words in a given sentence operationLTW (wi) = wi, we apply a sentence matrix S for each input sentence, where each word column corresponds to an embedding column."}, {"heading": "2.1.2 Convolutional feature maps", "text": "The goal of the revolutionary layer is to extract patterns, i.e., discriminatory word sequences, which are common throughout the entire training. Formally, the convolution between an input matrix S-Rd and a filter (or a convolution kernel) F-Rd-M results from a vector c-Rs-1 in which each component is calculated as follows: ci = (S-R) i = (S-R) i, (S-R) i-R-R-R-R-R-R-R-R-R-R, (1) i-R-R-R-R-R-R, (S-R) i-R-R-R-R-R-R-R-R-R-R-R-R"}, {"heading": "2.2 Our relational model", "text": "When we learn to compare pairs of text, modeling the relational links between sentences has been shown to greatly improve the accuracy of semantic similarity models. For example, powerful systems on semantic textual similarity benchmarks (Agirre et al., 2015) also use latent word alignment structures to calculate similarities between question and answer sentences. Yu et al. (2014) achieves great improvements by combining the results of their deep learning model with word count features in a logistic regression model. To allow our evolutionary neural network to capture the connections between related words in a pair, we feed it with an additional binary input via overlapping words. Specifically, for each word w in the input set we associate an additional word overlap indicator o- {0, 1} in which a word overlaps with an overlap table."}, {"heading": "2.3 The matching model", "text": "The architecture of our model for the concordance of question-answer pairs is illustrated in Fig. 2. Our sentence models (described in Fig. 2.1) learn to map input sentences to vectors that can then be used to calculate their similarity, which are then used to calculate a similarity value that is used together with the distribution vector of question and answer sentences in a single common representation.In the following, we describe how the intermediate representations generated by the sentence model can be used to calculate question and answer similarity values and give a brief explanation of the remaining layers, e.g. hidden and softmax. similarity models. Given the output of our sentence models, their resulting vector representations xq and xa can be used to calculate a question and answer similarity value. We follow the approach of (Bordes et al., 2014), which defines the similarity between xq and xa vectors as follows."}, {"heading": "2.4 The information flow", "text": "The output of our sentence models (paragraph 2.1) are distribution representations of a question xq and an answer xa, which are then matched using a similarity matrix M according to Equation 2. This results in a single score xsim, which captures various (syntactical and semantic) aspects of the similarity of the question pair. Note that it is also easy to add additional features xfeat to the model. The connecting layer then concatenates all intermediate vectors, the similarity score and any additional features into a single vector: xjoin = [xTq; xsim; x T a; x T feat] This vector is then guided through a fully connected hidden layer, allowing modeling of interactions between the components of the connected representation vector. Finally, the output of the hidden layer is routed to the Softmax classification layer, which generates a distribution across the class labels."}, {"heading": "2.5 Training", "text": "The model is designed to minimize the negative conditional log probability of the training set: C = \u2212 log-Ni = 1 p (yi | qi, ai; \u03b8), where \u03b8 contains all network parameters: \u03b8 = {W; Wo; Fq; bq; Fa; ba; M; wh; bh; ws; bs}, namely the word embedding matrix W and the word overlap matrix Where, filter weights and distortions of the coil layers, similarity matrix M, parameters of the hidden and softmax layers. The parameters of the network are optimized by stochastic gradient descent (SGD) using a backpropogation algorithm to calculate the gradients."}, {"heading": "3 Experiments and Evaluation", "text": "This section describes the data set and our experimental setup and also gives details on how we get the word embed matrix W and how we train our network."}, {"heading": "3.1 Data and setup", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "3.2 Training and hyperparameters", "text": "The parameters of our Deep Learning model were selected on the basis of a Dev set: the width of the folding filters is 5 and the number of convolutional characteristics 100. We use the ReLU activation function and a simple max pooling. The size of the hidden layer corresponds to the size of the xjoin vector, which is achieved by concatenating question and answer vectors from the distribution models, similarity values and additional characteristics. To train the network, we use stochastic gradient downward movements with mixed mini-batches. We eliminate the need to set the learning rate using the Adadelta updating rule (pointer, 2012). The batch size is set to 50 examples. The network is trained for 25 epochs with an early stop, i.e. we stop the training if no update of the best possible accuracy has been made on the Dev set for the last 5 epochs. The accuracy calculated on the AP is the set of the More."}, {"heading": "3.3 Results and discussion", "text": "Our goal is to evaluate the impact of using our: (i) more powerful Convolutionary Sentence Modeling Network; (ii) distributional representations of questions and answers in addition to the similarity value; and (iii) approach to modeling matching words by adding additional dimensions to word embedding compared to providing a pre-calculated feature vector of overlapping word counts as in (Yu et al., 2014)."}, {"heading": "3.3.1 Distributional sentence models", "text": "Table 2 summarizes the results for the setting, if the network is trained with input / response pairs only, without using additional features, i.e. we leave the properties of the word overlapping.First, we report on the results of our model, if we only use a similarity value xsim. It should be noted that the network of Yu et al. (2014), similar to ours, relies on a revolutionary neural network to learn intermediate representations. However, their revolutionary neural network works only on unigrams or bigrams, while in our architecture we use a wider width of the folding filter, which allows for the acquisition of greater range dependencies. Additionally, in addition to the question-answer similarity value from the equivalent, our architecture includes 2 intermediate representations of the question and the answer xq and xa in the final vector representation xjoin, which together form a much richer representation for the compression of the end result. We simply call this network CNN."}, {"heading": "3.3.2 Relational models", "text": "Yu et al. (2014) show that combining the results of their deep learning system with a simple feature vector containing the number of word overlaps in a logistic regression model provides a significant increase in accuracy and delivers new state-of-the-art results. Table 3 delivers the results when we include information about overlapping words in two modes: (i) feature vector (fvec) mode - when we replicate the overlapping word count (Yu et al., 2014), which is represented by a feature vector xfeat inserted into the final xjoin representation (see Figure 2); and (ii) embeddings mode - when we supplement the representation of input words with additional word overlap indicator functions (as described in Sec. 2.1.1) 3. First, we find that the results are significantly better than in Table 2 when no overlapping information is used."}, {"heading": "3.4 Comparing with the state of the art", "text": "It should be noted that, in order to be consistent with the results of previous work on TREC13, we must evaluate our models in the same setting as (Wang et al., 2007; Yih et al., 2014), i.e. we must (i) remove the questions that have only correct or only false answer candidates, and (ii) use the same evaluation script and gold review file as they were used. As stated in footnote 7 in (Yih et al., 2014), the evaluation script always considers 4 questions to be incorrect, thereby penalizing the overall evaluation of the systems. This essentially reduces the performance of CNNR from an MAP and an MRR of.7654 and an MRR of.8186 to.7186 and 7828, respectively. We have used these numbers in Table 4 to accurately compare with the results of already published systems."}, {"heading": "4 Related Work", "text": "Most of the work to date to answer the question of the selection criterion has used different approaches to modeling syntactic trees between a question and its candidate, e.g. Wang et al. (2007) use quasi-synchronous grammar, Heilman & Smith (2010) develop an improved Tree Edit Distance (TED) model, Wang & Manning (2010) develop a probabilistic model for processing dependencies, while Yao et al. (2013) applies linear chain reactions derived from TED. Severyn and Moschitti (2013) apply SVM with flat syntactic representations."}, {"heading": "5 Conclusions and future work", "text": "In this paper, we propose a novel deep-learning architecture for the selection of response sentences. Our experimental results show that our model can achieve the accuracy of state-of-the-art networks that are much more complex, largely due to our use of more meaningful input and response set models, and our approach to injecting word overlap indicators directly into word embeddings. However, our word overlap indicator functions are based on simple string matching, which is clearly a very crude method of modeling the relationship between words in a question-answer pairing. Recently, deep-learning architectures have been successfully applied to learn word alignment in machine translation (Yang et al., 2013). It sounds promising to allow the network to dynamically align the related words in a question and their answer, which in turn requires maximizing latent alignment configurations to form future text configurations that will require a much larger pre-pairing of architectures."}], "references": [{"title": "Open question answering with weakly supervised embedding models", "author": ["Jason Weston", "Nicolas Usunier"], "venue": "In ECML", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "A noisy-channel approach to question answering", "author": ["Echihabi", "Marcu2003] Abdessamad Echihabi", "Daniel Marcu"], "venue": null, "citeRegEx": "Echihabi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi et al\\.", "year": 2003}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Heilman", "Smith2010] Michael Heilman", "Noah A. Smith"], "venue": null, "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A deep architecture for matching short texts", "author": ["Lu", "Li2013] Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Neural variational inference for text processing", "author": ["Miao et al.2015] Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1511.06038", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Automatic feature engineering for answer selection and extraction", "author": ["Severyn", "Moschitti2013] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2013}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Faq-based question answering via word alignment", "author": ["Wang", "Ittycheriah2015] Zhiguo Wang", "Abraham Ittycheriah"], "venue": "arXiv preprint arXiv:1507.02628", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question", "author": ["Wang", "Manning2010] Mengqiu Wang", "Christopher D. Manning"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Wang et al.2007] Mengqiu Wang", "Noah A. Smith", "Teruko Mitaura"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Xuchen Yao", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Word alignment modeling with context dependent deep neural network", "author": ["Yang et al.2013] Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Wikiqa: A challenge dataset for opendomain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] Wen-Tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-Tau Yih", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs. arXiv preprint arXiv:1512.05193", "author": ["Yin et al.2015] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection. CoRR", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Adadelta: An adaptive learning rate method. CoRR", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "swers (Yih et al., 2013).", "startOffset": 6, "endOffset": 24}, {"referenceID": 6, "context": "into low-dimensional vector space preserving important syntactic and semantic aspects of the input sentence, which leads to state-of-the-art results in many NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Yu et al., 2014).", "startOffset": 167, "endOffset": 222}, {"referenceID": 7, "context": "into low-dimensional vector space preserving important syntactic and semantic aspects of the input sentence, which leads to state-of-the-art results in many NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Yu et al., 2014).", "startOffset": 167, "endOffset": 222}, {"referenceID": 23, "context": "into low-dimensional vector space preserving important syntactic and semantic aspects of the input sentence, which leads to state-of-the-art results in many NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Yu et al., 2014).", "startOffset": 167, "endOffset": 222}, {"referenceID": 23, "context": "To compute question-answer similarity score we adopt an approach used in the deep learning model of (Yu et al., 2014), which produces excellent results on the answer sentence selection task.", "startOffset": 100, "endOffset": 117}, {"referenceID": 23, "context": "Yu et al. (2014) combine the output of their deep learning model with additional features in the final logistic regression model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "selection benchmark TREC13 (Wang et al., 2007) and on the more recent dataset WikiQA (Yang et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 19, "context": ", 2007) and on the more recent dataset WikiQA (Yang et al., 2015).", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "It is mainly inspired by the convolutional architectures used in (Kalchbrenner et al., 2014; Kim, 2014) for performing various sentence classification tasks.", "startOffset": 65, "endOffset": 103}, {"referenceID": 7, "context": "It is mainly inspired by the convolutional architectures used in (Kalchbrenner et al., 2014; Kim, 2014) for performing various sentence classification tasks.", "startOffset": 65, "endOffset": 103}, {"referenceID": 20, "context": "Yih et al. (2013) also uses latent word-alignment structure in their semantic similarity model to compute similarity between ques-", "startOffset": 0, "endOffset": 18}, {"referenceID": 23, "context": "Yu et al. (2014) achieves large improvements by combining the output of their deep learning model with word count features in a logistic regression model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "We follow the approach of (Bordes et al., 2014) that defines the similarity between xq and xa vectors as follows: sim(xq,xa) = x T q Mxa, (2) where M \u2208 Rd\u00d7d is a similarity matrix.", "startOffset": 26, "endOffset": 47}, {"referenceID": 14, "context": "1 Data and setup We test our model on the manually curated TREC QA dataset1 from Wang et al. (2007), which appears to be one of the most widely used benchmarks for answer sentence reranking.", "startOffset": 81, "endOffset": 100}, {"referenceID": 14, "context": "An additional training set TRAIN-ALL provided by Wang et al. (2007) contains 1,229 questions", "startOffset": 49, "endOffset": 68}, {"referenceID": 19, "context": "(Yang et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "Consistently with (Yin et al., 2015), we remove the questions without answers for our evaluations.", "startOffset": 18, "endOffset": 36}, {"referenceID": 1, "context": "Hence, similar to (Denil et al., 2014; Kim, 2014; Yu et al., 2014) we keep the word embeddings fixed and initialize the word matrix W from an unsupervised neural language model.", "startOffset": 18, "endOffset": 66}, {"referenceID": 7, "context": "Hence, similar to (Denil et al., 2014; Kim, 2014; Yu et al., 2014) we keep the word embeddings fixed and initialize the word matrix W from an unsupervised neural language model.", "startOffset": 18, "endOffset": 66}, {"referenceID": 23, "context": "Hence, similar to (Denil et al., 2014; Kim, 2014; Yu et al., 2014) we keep the word embeddings fixed and initialize the word matrix W from an unsupervised neural language model.", "startOffset": 18, "endOffset": 66}, {"referenceID": 10, "context": "We run word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump and the AQUAINT corpus2 containing roughly 375 million words.", "startOffset": 21, "endOffset": 43}, {"referenceID": 23, "context": "We set the dimensionality of our word embeddings to 50 to be on the line with (Yu et al., 2014).", "startOffset": 78, "endOffset": 95}, {"referenceID": 24, "context": "We eliminate the need to tune the learning rate by using the Adadelta update rule (Zeiler, 2012).", "startOffset": 82, "endOffset": 96}, {"referenceID": 23, "context": "providing the network with a pre-computed feature vector of overlapping word counts as in (Yu et al., 2014).", "startOffset": 90, "endOffset": 107}, {"referenceID": 23, "context": "It should be noted that the network by Yu et al. (2014), similarly to ours, relies on a convolutional neural network to learn intermediate representations.", "startOffset": 39, "endOffset": 56}, {"referenceID": 23, "context": "clude overlapping word counts replicating (Yu et al., 2014), which is represented by a feature vector xfeat that is plugged into the final representation xjoin (see Fig.", "startOffset": 42, "endOffset": 59}, {"referenceID": 23, "context": "As argued by Yu et al. (2014), distributional word embeddings have certain shortcomings especially when dealing with proper nouns and cardinal numbers, which are frequent in factoid questions.", "startOffset": 13, "endOffset": 30}, {"referenceID": 9, "context": "7633 Miao et al. (2015) .", "startOffset": 5, "endOffset": 24}, {"referenceID": 16, "context": "It should be noted that, to be consistent with the results of previous work on TREC13, it is required to evaluate our models in the same setting as (Wang et al., 2007; Yih et al., 2014), i.", "startOffset": 148, "endOffset": 185}, {"referenceID": 21, "context": "It should be noted that, to be consistent with the results of previous work on TREC13, it is required to evaluate our models in the same setting as (Wang et al., 2007; Yih et al., 2014), i.", "startOffset": 148, "endOffset": 185}, {"referenceID": 21, "context": "As pointed out by Footnote 7 in (Yih et al., 2014), the evaluation script always considers 4 questions to be answered incorrectly thus penalizing the overall score of the systems.", "startOffset": 32, "endOffset": 50}, {"referenceID": 19, "context": "State of the art CNNc (Yang et al., 2015) .", "startOffset": 22, "endOffset": 41}, {"referenceID": 22, "context": "6652 n/a ABCNN (Yin et al., 2015) .", "startOffset": 15, "endOffset": 33}, {"referenceID": 9, "context": "7127 n/a LSTMa,c (Miao et al., 2015) .", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "7041 n/a NASMc (Miao et al., 2015) .", "startOffset": 15, "endOffset": 34}, {"referenceID": 9, "context": "It shows that our model reaches the accuracy of the best system, ABCNN, and outperforms NASMc by Miao et al. (2015), which was superior to our models on TREC13.", "startOffset": 97, "endOffset": 116}, {"referenceID": 14, "context": ", Wang et al. (2007) use quasi-synchronous grammar, Heilman & Smith (2010) develop an improved Tree Edit Distance (TED) model, Wang & Manning (2010) develop a probabilistic model to learn tree-edit op-", "startOffset": 2, "endOffset": 21}, {"referenceID": 14, "context": ", Wang et al. (2007) use quasi-synchronous grammar, Heilman & Smith (2010) develop an improved Tree Edit Distance (TED) model, Wang & Manning (2010) develop a probabilistic model to learn tree-edit op-", "startOffset": 2, "endOffset": 75}, {"referenceID": 14, "context": ", Wang et al. (2007) use quasi-synchronous grammar, Heilman & Smith (2010) develop an improved Tree Edit Distance (TED) model, Wang & Manning (2010) develop a probabilistic model to learn tree-edit op-", "startOffset": 2, "endOffset": 149}, {"referenceID": 17, "context": "erations on dependency parse trees, while Yao et al. (2013) applies linear chain CRFs with features derived from TED.", "startOffset": 42, "endOffset": 60}, {"referenceID": 17, "context": "erations on dependency parse trees, while Yao et al. (2013) applies linear chain CRFs with features derived from TED. Severyn and Moschitti (2013) applied SVM with tree kernels to shallow syntactic representations.", "startOffset": 42, "endOffset": 147}, {"referenceID": 17, "context": "erations on dependency parse trees, while Yao et al. (2013) applies linear chain CRFs with features derived from TED. Severyn and Moschitti (2013) applied SVM with tree kernels to shallow syntactic representations. Yih et al. (2013) use distributional models based on lexical semantics to match seman-", "startOffset": 42, "endOffset": 233}, {"referenceID": 6, "context": ", (Kalchbrenner et al., 2014; Kim, 2014), and for modelling text pairs, e.", "startOffset": 2, "endOffset": 40}, {"referenceID": 7, "context": ", (Kalchbrenner et al., 2014; Kim, 2014), and for modelling text pairs, e.", "startOffset": 2, "endOffset": 40}, {"referenceID": 18, "context": ", Yih et al. (2014) applied convolutional neural networks to open-domain question answering; Bordes et al.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": "(2014) applied convolutional neural networks to open-domain question answering; Bordes et al. (2014) propose a neural embedding model combined with the knowledge base for open-domain QA; Iyyer et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 0, "context": "(2014) applied convolutional neural networks to open-domain question answering; Bordes et al. (2014) propose a neural embedding model combined with the knowledge base for open-domain QA; Iyyer et al. (2014) applied recursive neural net-", "startOffset": 80, "endOffset": 207}, {"referenceID": 23, "context": "The work closest to ours is (Yu et al., 2014), where they present a deep learning architecture for answer sentence selection.", "startOffset": 28, "endOffset": 45}, {"referenceID": 23, "context": "Finally, our deep learning model is trained end-to-end, while in (Yu et al., 2014) they use the output of their neural network in a separate logistic scoring model.", "startOffset": 65, "endOffset": 82}, {"referenceID": 18, "context": ", (Yang et al., 2013).", "startOffset": 2, "endOffset": 21}], "year": 2016, "abstractText": "In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.", "creator": "LaTeX with hyperref package"}}}