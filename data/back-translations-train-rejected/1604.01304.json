{"id": "1604.01304", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "Towards Label Imbalance in Multi-label Classification with Many Labels", "abstract": "In multi-label classification, an instance may be associated with a set of labels simultaneously. Recently, the research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large. The existing works focus on how to design scalable algorithms that offer fast training procedures and have a small memory footprint. However they ignore and even compound another challenge - the label imbalance problem. To address this drawback, we propose a novel Representation-based Multi-label Learning with Sampling (RMLS) approach. To the best of our knowledge, we are the first to tackle the imbalance problem in multi-label classification with many labels. Our experimentations with real-world datasets demonstrate the effectiveness of the proposed approach.", "histories": [["v1", "Tue, 5 Apr 2016 15:44:33 GMT  (240kb,D)", "http://arxiv.org/abs/1604.01304v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["li li", "houfeng wang"], "accepted": false, "id": "1604.01304"}, "pdf": {"name": "1604.01304.pdf", "metadata": {"source": "CRF", "title": "Towards Label Imbalance in Multi-label Classification with Many Labels", "authors": ["Li Li", "Houfeng Wang"], "emails": ["wanghf}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they,"}, {"heading": "2 Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-label Classification with Many Labels", "text": "We categorize the existing approaches to multi-label classification with many labels into two types: Label Space Dimension Reduction = Y Dimension Reduction (LSDR) and Representation-Based Learning (RBL). Figure 1 (it is from (Lin et al., 2014) is the schematic diagram of LSDR. However, LSDR encodes the high-dimensional label vectors into low-dimensional code vectors. Then predictive models of instances are formed into code vectors. To predict an invisible instance, a low-dimensional code vector is first created using the learned predictive models and then for the label Vector.Compressive Sensing (CS et al., 2009) is the first LSDR approach. Specifically, CS linearly encodes original label space as compressed sensing and uses standard recovery algorithms for decoding."}, {"heading": "2.2 Label Imbalance Problem", "text": "One solution to the problem of label imbalance is to train a label classifier and correct the imbalance in each classifier using popular binary imbalance methods such as random or synthetic subsample / oversample (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015). Besides integrating binary decomposition, Petterson et al. (Petterson et al., 2015) and Dembczynski et al. (Dembczynski et al., 2013) improve this approach by aggregating a binary imbalance learner corresponding to the current label with other labels for prediction. Besides integrating binary decomposition, Petterson et al. (Petterson et al., 2010) and Dembczynski et al. (Dembczynski et al., 2013) aim to directly optimize the problem of the imbalance."}, {"heading": "3 Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "Let X specify the instance attribute space and Y = {0, 1} m specify the label space with m labels. An instance x-X is associated with a label vector y = (y1, y2,..., ym), where yj = 1 is the j-th label relevant for the instance and yj = 0 otherwise. The goal of multi-label learning is to create a function f: X \u2192 Y. In general, function f consists of m functions, one for a label, i.e. f (x) = [f1 (x), f2 (x),..., fm (x)], where f j (x) is the prediction of the relevance between the instance x and the j-th label."}, {"heading": "3.2 Representation Learning", "text": "This year is the highest in the history of the country."}, {"heading": "3.3 Sampling Strategy", "text": "Our goal is to identify relevant labels from irrelevant labels. To achieve this goal, we minimize a loss function through training to obtain the model parameters W. The loss function L is as shown in the following formular.L = m \u2211 j = 1 '(f j (x), yj) (7) \"refers to the classification loss function. Various classification loss functions can be described as\" e.g. cross entropy loss, least square loss and L2 hinge loss.With the serious problem of imbalance in classifying relevant labels as irrelevant, the classification of relevant labels as relevant is higher than the classification of irrelevant labels as relevant. By including this consideration in the loss function, the loss function becomes follows.L = [f] j [P] (x), yj) + 1 C [f] labels as relevant (f), yj) (f), Wyj), Wyj ej = 1 is relevant."}, {"heading": "W ,l = argmin", "text": "W, l {\u2211 j \u0109P '(f j (x), yj) + \u2211 j \u0109S' (f j (x), yj) + \u03bb | | W | | 2F + \u03bb m \u2211 j = 1 | | lj | | 22} | S | = \u03b1 \u00b7 | P | (11), where \u03bb denotes the regularization coefficient."}, {"heading": "3.4 Training Our Models", "text": "We use the Adagrad (Duchi et al., 2011) to adjust the learning rate. Sampling labels can be distorted and unstable. A simple approach to this problem is to train different models with different sampling results and apply the ensemble strategy. However, it is very expensive to train different models for the large-scale multi-label classification with many labels. A practical solution to this problem is to sample different labels in each batch of the minibatch SGD. Let B label the index of the labeled training data in a batch, with the pseudocode for training RLML being given with a batch of labeled data in algorithm 1.Algorithm 1 Mini-batch SGD with a batch of labeled data: (xi, y i) where i-B, the pseudocode for training of RLML with a batch of labeled data in algorithm 1.Algorithm 1 Mini-batch SGD with a batch of labeled data is given: (xi, y i) where i-B, the pseudocode for training of RLML with a batch of labeled data in algorithm 1.Algorithm 1.953 Algorithm 1.95icious Datalgorithm 1.95xi 453"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We conduct experiments with four data sets from the real world. These data sets are available online 2. To reduce time costs, we use only the accessible marked training part of the wiki data set and select the labels with at least 5 relevant instances. Table 1 shows these multi-label data sets and the associated statistics, with n indicating the number of instances, d indicating the number of attributes, and m indicating the number of labels."}, {"heading": "4.2 Evaluation Criteria", "text": "In our experiments, we use three common evaluation metrics. Let p denotes the prediction vector. Hammingloss is defined as the percentage of false labels in the total number of labels. Hammingloss = 1m | p \u0445 y | (12) 2http: / / mulan.sourceforge.net / datasets.html and http: / / mlkd.csd.auth. gr / multilabel.html and https: / / www.kaggle. com / c / lshtc / datawhere \u0445 denotes the symmetric difference between two sentences corresponding to the XOR operator in Boolean logic. Let pi and ri denote the accuracy and callback for the i-instance, which means that the flour label label pyn = | p | and that i | p | is better defined."}, {"heading": "4.3 Experimentation Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Performance Comparison", "text": "We compare RMLS with some state-of-the-art apoaches and a baseline approach. - Principles Label Space Transformation (PLST) (Tai and Lin, 2012). PLST runs PCA on the label matrix to obtain the compressing matrix. - Feature-aware Implicit Label Space Encoding (FaiE) (Lin et al., 2014). FaiE balances predictability with recoverability. - Column Subset Selection for Multi-Label (CSS ML) (Bi and Kwok, 2013). CSS ML tries to select exactly the representative labels of LES to capture all labels as much as possible. - Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011). WSABIE trains the representation model by minimizing the Weighted Approximate Rank Pairwise (WSABIE)."}, {"heading": "4.3.2 Time Cost", "text": "The training time of the RBL approaches on small datasets (Enron, Delicious, Eurlex desc) is similar, but WSABIE and LEML spend much more time training on the wiki dataset than RMLS. Because the 3https: / / github.com / hsuantien / mlc _ lsdrsampling scheme in RMLS drastically reduces the time cost when the number of labels is very large. 2) PLST and FaiE spend little time training on the small datasets (Enron, Delicious, Eurlex desc), but they run out of memory and spend too much time on the wiki dataset as both perform a partial SVD on the dense 50k x x x. 3) ML CSSP is the only LSDR approach applicable to the wiki dataset."}, {"heading": "4.3.3 Influence of the Sampling Ratio", "text": "In order to investigate the influence of the sampling ratio, i.e. the parameters \u03b1, we perform RMLS with a step size of 1 that varies from 1 to 10. Due to the side limitation, we only report the results of the Eurlex Desc dataset, while experiments with other datasets produce similar results. Detailed results are shown in Figure 3The Fscore and accuracy when the sampling ratio is low. As the sampling ratio becomes large, these two evaluation criteria lead first up and then down. If the sampling ratio is small, too many irrelevant labels drop out, resulting in poor performance. If the sampling ratio is high, the number of irrelevant labels is much greater than the number of relevant labels, the problem of label imbalance leads to poor performance. This implies that the sampling scheme with the correct sampling ratio is lower than the problem of improving the sampling ratio, if the problem of label imbalance is greater."}, {"heading": "5 Conclusions", "text": "In multi-label classification, an instance is associated with a number of labels at the same time. Recently, researchers have focused on multi-label learning with many labels. Existing approaches to multi-label learning with many labels ignore and even exacerbate the problem of label imbalance. To address this problem, we propose a novel, representation-based multi-label learning with sampling (RMLS) approach. Our experiments demonstrate the effectiveness of the proposed approach."}], "references": [{"title": "Multilabel learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Archit Gupta", "Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 22nd international conference on", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "A first approach to deal with imbalance in multilabel datasets", "author": ["Antonio Rivera", "Mar\u0131\u0301a Jos\u00e9 del Jesus", "Francisco Herrera"], "venue": "In Hybrid Artificial Intelligent Systems,", "citeRegEx": "Charte et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Charte et al\\.", "year": 2013}, {"title": "Addressing imbalance in multilabel classification: Measures and random resampling algorithms", "author": ["Antonio J Rivera", "Mar\u0131\u0301a J del Jesus", "Francisco Herrera"], "venue": null, "citeRegEx": "Charte et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Charte et al\\.", "year": 2015}, {"title": "Feature-aware label space dimension reduction for multi-label classification", "author": ["Chen", "Lin2012] Yao-Nan Chen", "Hsuan-Tien Lin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Optimizing the f-measure in multi-label classification: Plug-in rule approach versus structured loss", "author": ["Arkadiusz Jachnik", "Wojciech Kotlowski", "Willem Waegeman", "Eyke H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Annotating subsets of the enron email corpus. In CEAS", "author": ["Andres Kwasinksi", "Paul Kingsbury", "Roberta Evans Sabin", "Albert McDowell"], "venue": null, "citeRegEx": "Goldstein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2006}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu et al.2009] Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Multi-label relieff and f-statistic feature selections for image annotation", "author": ["Kong et al.2012] Deguang Kong", "Chris Ding", "Heng Huang", "Haifeng Zhao"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2012}, {"title": "Muli-label text categorization with hidden components", "author": ["Li Li", "Longkai Zhang", "Houfeng Wang"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Multi-label classification via feature-aware implicit label space encoding", "author": ["Lin et al.2014] Zijia Lin", "Guiguang Ding", "Mingqing Hu", "Jianmin Wang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain", "author": ["Mencia", "Johannes F\u00fcrnkranz"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Mencia et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mencia et al\\.", "year": 2008}, {"title": "Multi-label classification based on analog reasoning", "author": ["Andreu SanchoAsensio", "Elisabet Golobardes", "Albert Fornells", "Albert Orriols-Puig"], "venue": "Expert Systems with Applications,", "citeRegEx": "Nicolas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nicolas et al\\.", "year": 2013}, {"title": "Reverse multi-label learning", "author": ["Petterson", "Caetano2010] James Petterson", "Tib\u00e9rio S Caetano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Petterson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petterson et al\\.", "year": 2010}, {"title": "Large-scale bayesian multi-label learning via topic-based label embeddings", "author": ["Rai et al.2015] Piyush Rai", "Changwei Hu", "Ricardo Henao", "Lawrence Carin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rai et al\\.", "year": 2015}, {"title": "Multilabel image categorization with sparse factor representation", "author": ["Sun et al.2014] Fuming Sun", "Jinhui Tang", "Haojie Li", "Guo-Jun Qi", "Thomas S Huang"], "venue": "Image Processing, IEEE Transactions", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Inverse random under sampling for class imbalance problem and its application to multi-label classification", "author": ["Josef Kittler", "Fei Yan"], "venue": "Pattern Recognition,", "citeRegEx": "Tahir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tahir et al\\.", "year": 2012}, {"title": "Multilabel classification with principal label space transformation", "author": ["Tai", "Lin2012] Farbound Tai", "Hsuan-Tien Lin"], "venue": "Neural Computation,", "citeRegEx": "Tai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2012}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Decision trees for hierarchical multi-label classification", "author": ["Vens et al.2008] Celine Vens", "Jan Struyf", "Leander Schietgat", "Sa\u0161o D\u017eeroski", "Hendrik Blockeel"], "venue": "Machine Learning,", "citeRegEx": "Vens et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vens et al\\.", "year": 2008}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Yu et al.2014] Hsiang-fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit Dhillon"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Towards class-imbalance aware multi-label learning", "author": ["Zhang et al.2015] Min-Ling Zhang", "Yu-Kun Li", "Xu-Ying Liu"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI\u201915)", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.", "startOffset": 79, "endOffset": 155}, {"referenceID": 12, "context": "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.", "startOffset": 79, "endOffset": 155}, {"referenceID": 15, "context": "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.", "startOffset": 79, "endOffset": 155}, {"referenceID": 9, "context": "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.", "startOffset": 79, "endOffset": 155}, {"referenceID": 20, "context": "Due to several motivating real-life applications, such as image/video annotation (Weston et al., 2011; Kong et al., 2012) and query/keyword suggestions (Agrawal et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 8, "context": "Due to several motivating real-life applications, such as image/video annotation (Weston et al., 2011; Kong et al., 2012) and query/keyword suggestions (Agrawal et al.", "startOffset": 81, "endOffset": 121}, {"referenceID": 0, "context": ", 2012) and query/keyword suggestions (Agrawal et al., 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 0, "context": ", 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al., 2013; Bi and Kwok, 2013; Lin et al., 2014).", "startOffset": 182, "endOffset": 261}, {"referenceID": 10, "context": ", 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al., 2013; Bi and Kwok, 2013; Lin et al., 2014).", "startOffset": 182, "endOffset": 261}, {"referenceID": 7, "context": "The mainstream approaches are called Label Space Dimension Reduction (LSDR) (Hsu et al., 2009; Tai and Lin, 2012; Chen and Lin, 2012; Lin et al., 2014; Bi and Kwok, 2013).", "startOffset": 76, "endOffset": 170}, {"referenceID": 10, "context": "The mainstream approaches are called Label Space Dimension Reduction (LSDR) (Hsu et al., 2009; Tai and Lin, 2012; Chen and Lin, 2012; Lin et al., 2014; Bi and Kwok, 2013).", "startOffset": 76, "endOffset": 170}, {"referenceID": 20, "context": "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.", "startOffset": 117, "endOffset": 173}, {"referenceID": 21, "context": "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.", "startOffset": 117, "endOffset": 173}, {"referenceID": 14, "context": "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.", "startOffset": 117, "endOffset": 173}, {"referenceID": 1, "context": "As the papers (Spyromitros-Xioufis, 2011; Charte et al., 2013; Zhang et al., 2015) pointed out, the label imbalance problem exists in ar X iv :1 60 4.", "startOffset": 14, "endOffset": 82}, {"referenceID": 22, "context": "As the papers (Spyromitros-Xioufis, 2011; Charte et al., 2013; Zhang et al., 2015) pointed out, the label imbalance problem exists in ar X iv :1 60 4.", "startOffset": 14, "endOffset": 82}, {"referenceID": 22, "context": "To show the phenomenon, we can use the imbalance ratio defined in (Zhang et al., 2015) to evaluate the label imbalance degree.", "startOffset": 66, "endOffset": 86}, {"referenceID": 6, "context": "The Enron dataset (Goldstein et al., 2006) has 45 labels and its average imbalance ratio is 3.", "startOffset": 18, "endOffset": 42}, {"referenceID": 10, "context": "Figure 1 (it is from (Lin et al., 2014)) is the schematic diagram of LSDR.", "startOffset": 21, "endOffset": 39}, {"referenceID": 7, "context": "Compressive Sensing (CS) (Hsu et al., 2009) is the first LSDR approach.", "startOffset": 25, "endOffset": 43}, {"referenceID": 10, "context": "Feature-aware Implicit label space Encoding (FaIE) (Lin et al., 2014) balances predictability with recoverability, and optimize the following problem.", "startOffset": 51, "endOffset": 69}, {"referenceID": 20, "context": "Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011) trains the representation model by minimizing the Weighted Approximate-Rank Pairwise (WARP) loss function.", "startOffset": 49, "endOffset": 70}, {"referenceID": 21, "context": "Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014) develops a fast optimization scheme for the representation model with different loss functions, and analyses the representation model\u2019s generalization error.", "startOffset": 69, "endOffset": 86}, {"referenceID": 14, "context": "Bayesian Multi-label Learning via Positive Labels (BMLPL) (Rai et al., 2015) uses the topic model to represent instance, and learns the model with only relevant labels", "startOffset": 58, "endOffset": 76}, {"referenceID": 16, "context": "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).", "startOffset": 230, "endOffset": 319}, {"referenceID": 1, "context": "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).", "startOffset": 230, "endOffset": 319}, {"referenceID": 2, "context": "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).", "startOffset": 230, "endOffset": 319}, {"referenceID": 22, "context": "The paper (Zhang et al., 2015) improves this approach by aggregating one binary-class imbalance learner corresponding to the current label and several multi-class imbalance learners coupling with other Figure 2: An illustration of the representations learning framwork.", "startOffset": 10, "endOffset": 30}, {"referenceID": 4, "context": "Besides integrating binary decomposition, Petterson et al (Petterson and Caetano, 2010) and Dembczynski et al (Dembczynski et al., 2013) address the label imbalance problem by directly optimizing imbalance-specific metric.", "startOffset": 110, "endOffset": 136}, {"referenceID": 18, "context": "The constrained norms acts as a regularizer in the same way as is used in lasso (Tibshirani, 1996)", "startOffset": 80, "endOffset": 98}, {"referenceID": 5, "context": "We use the Adagrad (Duchi et al., 2011) to adapt the learning rate.", "startOffset": 19, "endOffset": 39}, {"referenceID": 10, "context": "- Feature-aware Implicit label space Encoding (FaiE) (Lin et al., 2014).", "startOffset": 53, "endOffset": 71}, {"referenceID": 20, "context": "- Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011).", "startOffset": 51, "endOffset": 72}, {"referenceID": 21, "context": "- Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014).", "startOffset": 71, "endOffset": 88}], "year": 2016, "abstractText": "In multi-label classification, an instance may be associated with a set of labels simultaneously. Recently, the research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large. The existing works focus on how to design scalable algorithms that offer fast training procedures and have a small memory footprint. However they ignore and even compound another challenge the label imbalance problem. To address this drawback, we propose a novel Representation-based Multilabel Learning with Sampling (RMLS) approach. To the best of our knowledge, we are the first to tackle the imbalance problem in multi-label classification with many labels. Our experimentations with realworld datasets demonstrate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}