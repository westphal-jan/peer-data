{"id": "1604.01475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Learning A Deep $\\ell_\\infty$ Encoder for Hashing", "abstract": "We investigate the $\\ell_\\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \\textit{Deep $\\ell_\\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \\textit{Deep Siamese $\\ell_\\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.", "histories": [["v1", "Wed, 6 Apr 2016 03:54:33 GMT  (338kb,D)", "http://arxiv.org/abs/1604.01475v1", "To be presented at IJCAI'16"]], "COMMENTS": "To be presented at IJCAI'16", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhangyang wang", "yingzhen yang", "shiyu chang", "qing ling", "thomas s huang"], "accepted": false, "id": "1604.01475"}, "pdf": {"name": "1604.01475.pdf", "metadata": {"source": "CRF", "title": "Learning A Deep `\u221e Encoder for Hashing", "authors": ["Zhangyang Wang", "Yingzhen Yang", "Shiyu Chang", "Qing Ling", "Thomas S. Huang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Problem Definition and Background", "text": "While \"0 and\" 1 regularizations are well known and are successfully applied in sparse signal approximations, it is less researched to use the \"\u221e standard for regularizing signal representations. In this paper, we are particularly interested in the following\" \u221e -limited smallest square problem: minx | Dx \u2212 y | 22 s.t. | | | immanently quantifiable errors (1), where y-Rn \u2212 1 denotes the input signal, D-Rn \u00b7 N denotes the (overcomplete) basis (often referred to as a frame or dictionary) with N < n, and x-RN \u00d7 1 the learned representation. Furthermore, the maximum absolute size of x is limited by a positive constant \u03bb, so that each entry of x has the smallest dynamic range [Lyubarskii and Vershynin, 2010]. As a result, the model (1) tends to spread the information of y roughly evenly among the coefficients of x."}, {"heading": "1.2 Related Work", "text": "Similar to the case of \"0 / '1 sparse approximation problems, the solution (1) and its variants (e.g., [Studer et al., 2014]) relies on iterative solutions. [Stark and Parker, 1995] proposedar Xiv: 160 4.01 475v 1 [cs.L G] 6A pr2 016an active set strategy similar to that of [Lawson and Hanson, 1974]. In [Adlers, 1998] the authors examined a primaldual path-following interior point method. However, iterative approximation algorithms suffer from their inherent sequential structures, as well as data-dependent complexity and latency, which often represent a major bottleneck in computational efficiency. Furthermore, the common optimization of (non-superimposed) feature learning and the superimposed steps in solving complex bi-level optimization problems."}, {"heading": "2 An ADMM Algorithm", "text": "ADMM is known for its remarkable effectiveness in minimizing targets with linear separable structures [Bertsekas, 1999]. We first introduce an auxiliary variable z-RN \u00b7 1 and rewrite (1) as: minx, z 1 2 | | Dx \u2212 y | | 2 s.t. | | z | | 2 s-x \u2264, z \u2212 x = 0. (2) The advanced lagrange function of (2) is: 1 2 | Dx \u2212 y | | 2 2 2 + p T (z \u2212 x) + \u03b2 is a positive constant (with a default value of 0.6), and while this indicator function (z \u2212 x) is the indicator function that extends into infinity when it extends to the z-z shape and otherwise."}, {"heading": "3 Deep `\u221e Encoder", "text": "We first replaced (7) in (8) to derive an update form that is explicitly dependent on only z and p: zt + 1 = B\u03bb ((D TD + \u03b2I) \u2212 1 (DT y + \u03b2zt + pt) \u2212 pt\u03b2))), (9) where B\u03bb is defined as a box-delimited element-wise operator (u denotes a vector and ui is its i-th element): [DTD + \u03b2I] i = min (ui, \u2212 \u03bb), \u03bb). (10) Eqn. (9) could alternatively be rewritten as: zt + 1 = B\u03bb (Wy + Szt + bt), where: W = \u03b2 (DTD + \u03b2I) \u2212 1, bt = [D TD + \u03b2I) \u2212 1 \u2212 1 \u2212 1 \u2212 1, expressed as a block diagram in Fig."}, {"heading": "4 Deep `\u221e Siamese Network for Hashing", "text": "Rather, as a solution (1), we initially identified the encoder as a general regression rather than [Gregor and LeCun, 2009], while instead linking encoder (s) to a task-related loss and optimizing the end-to-end pipeline. In this paper, we focus on discussing its application in hashing, although the proposed model is not limited to a specific application. Background is that the ever-growing number of image data on the Web devotes much attention to searching for the nearest neighbor using hashing methods. [Gionis et al., 1999] For large data applications, compact, bitter representations improve efficiency in both storage and search. The state-of-the-art approach, learning-based hashing, learns similarity-preserving hash functions to encode input data into binary codes. Furthermore, while previous methods such as linear search hashing (LSH) [Gionis et al., 1999], quantifying the quantification of quantity."}, {"heading": "5 Experiments in Image Hashing", "text": "It is not only because more free parameters allow greater learning capacity, but also because of the important fact that pt (and thus the other images) are essentially not shared, as in (11) and Fig. While many neural networks are well formed with random initializations, it has been discovered that sometimes bad initializations cannot yet achieve the effectiveness of first-order methods. (Sutskever et al., 2013) It is much easier to initialize the proposed models."}, {"heading": "6 Conclusion", "text": "This paper examines how to import the quantization-robust property of a \u221e-restricted minimization model into a specially designed deep model, first by deriving an ADMM algorithm, which is then reformulated as a feedback-forward neural network. We present the Siamese architecture, which is associated with a pair-by-pair loss, for the purpose of hashing, and we thoroughly analyze the performance and behavior of the proposed model vis-\u00e0-vis its competitors, hoping that it will generate more interest in the community."}], "references": [{"title": "Sparse least squares problems with box constraints", "author": ["Mikael Adlers"], "venue": "Citeseer,", "citeRegEx": "Adlers. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear programming", "author": ["Dimitri P Bertsekas"], "venue": "Athena scientific Belmont,", "citeRegEx": "Bertsekas. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Nuswide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo", "Yantao Zheng"], "venue": "ACM CIVR, page 48. ACM,", "citeRegEx": "Chua et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "TIP", "author": ["Michael Elad", "Michal Aharon. Image denoising via sparse", "redundant representations over learned dictionaries"], "venue": "15(12):3736\u20133745,", "citeRegEx": "Elad and Aharon. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 814\u2013817", "author": ["Jean-Jacques Fuchs. Spread representations. In ASILOMAR"], "venue": "IEEE,", "citeRegEx": "Fuchs. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "Similarity search in high dimensions via hashing. In VLDB, volume 99, pages 518\u2013529,", "citeRegEx": "Gionis et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik"], "venue": "CVPR. IEEE,", "citeRegEx": "Gong and Lazebnik. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In ICML", "author": ["Karol Gregor", "Yann LeCun. Learning fast approximations of sparse coding"], "venue": "pages 399\u2013406,", "citeRegEx": "Gregor and LeCun. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In CVPR", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun. Dimensionality reduction by learning an invariant mapping"], "venue": "IEEE,", "citeRegEx": "Hadsell et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Hinton", "2009] Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "NIPS,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In NIPS", "author": ["Brian Kulis", "Trevor Darrell. Learning to hash with binary reconstructive embeddings"], "venue": "pages 1042\u20131050,", "citeRegEx": "Kulis and Darrell. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["Hanjiang Lai", "Yan Pan", "Ye Liu", "Shuicheng Yan"], "venue": "CVPR,", "citeRegEx": "Lai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 161", "author": ["Charles L Lawson", "Richard J Hanson. Solving least squares problems"], "venue": "SIAM,", "citeRegEx": "Lawson and Hanson. 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "Feature learning based deep supervised hashing with pairwise labels", "author": ["Wu-Jun Li", "Sheng Wang", "Wang-Cheng Kang"], "venue": "arXiv:1511.03855,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "pages 2074\u20132081", "author": ["Wei Liu", "Jun Wang", "Rongrong Ji", "Yu-Gang Jiang", "Shih-Fu Chang. Supervised hashing with kernels. In CVPR"], "venue": "IEEE,", "citeRegEx": "Liu et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Information Theory", "author": ["Yurii Lyubarskii", "Roman Vershynin. Uncertainty principles", "vector quantization"], "venue": "IEEE Trans.,", "citeRegEx": "Lyubarskii and Vershynin. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse similarity-preserving hashing", "author": ["Jonathan Masci", "Alex M Bronstein", "Michael M Bronstein", "Pablo Sprechmann", "Guillermo Sapiro"], "venue": "arXiv preprint arXiv:1312.5479,", "citeRegEx": "Masci et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 49\u201362", "author": ["Jonathan Masci", "Davide Migliore", "Michael M Bronstein", "J\u00fcrgen Schmidhuber. Descriptor learning for omnidirectional image matching. In Registration", "Recognition in Images", "Videos"], "venue": "Springer,", "citeRegEx": "Masci et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance evaluation in content-based image retrieval: overview and proposals", "author": ["Henning M\u00fcller", "Wolfgang M\u00fcller", "David McG Squire", "St\u00e9phane Marchand-Maillet", "Thierry Pun"], "venue": "PRL,", "citeRegEx": "M\u00fcller et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "IJCV,", "citeRegEx": "Oliva and Torralba. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "In ICCV", "author": ["Gregory Shakhnarovich", "Paul Viola", "Trevor Darrell. Fast pose estimation with parameter-sensitive hashing"], "venue": "IEEE,", "citeRegEx": "Shakhnarovich et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In NIPS", "author": ["Pablo Sprechmann", "Roee Litman", "Tal Ben Yakar", "Alexander M Bronstein", "Guillermo Sapiro. Supervised sparse analysis", "synthesis operators"], "venue": "pages 908\u2013916,", "citeRegEx": "Sprechmann et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning efficient sparse and low rank models", "author": ["Pablo Sprechmann", "Alexander Bronstein", "Guillermo Sapiro"], "venue": "TPAMI,", "citeRegEx": "Sprechmann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bounded-variable least-squares: an algorithm and applications", "author": ["Philip B Stark", "Robert L Parker"], "venue": "Computational Statistics, 10:129\u2013129,", "citeRegEx": "Stark and Parker. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["Christoph Strecha", "Alexander M Bronstein", "Michael M Bronstein", "Pascal Fua"], "venue": "TPAMI, 34(1):66\u201378,", "citeRegEx": "Strecha et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Democratic representations", "author": ["Christoph Studer", "Tom Goldstein", "Wotao Yin", "Richard G Baraniuk"], "venue": "arXiv preprint arXiv:1401.3420,", "citeRegEx": "Studer et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton. On the importance of initialization", "momentum in deep learning"], "venue": "pages 1139\u20131147,", "citeRegEx": "Sutskever et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A joint optimization framework of sparse coding and discriminative clustering", "author": ["Zhangyang Wang", "Yingzhen Yang", "Shiyu Chang", "Jinyan Li", "Simon Fong", "Thomas S Huang"], "venue": "IJCAI,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "D3: Deep dualdomain based fast restoration of jpeg-compressed images", "author": ["Zhangyang Wang", "Shiyu Chang", "Ding Liu", "Qing Ling", "Thomas S Huang"], "venue": "IEEE CVPR,", "citeRegEx": "Wang et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a taskspecific deep architecture for clustering", "author": ["Zhangyang Wang", "Shiyu Chang", "Jiayu Zhou", "Meng Wang", "Thomas S Huang"], "venue": "SDM,", "citeRegEx": "Wang et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep l0 encoders", "author": ["Zhangyang Wang", "Qing Ling", "Thomas Huang"], "venue": "AAAI,", "citeRegEx": "Wang et al.. 2016c", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "NIPS,", "citeRegEx": "Weiss et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["Rongkai Xia", "Yan Pan", "Hanjiang Lai", "Cong Liu", "Shuicheng Yan"], "venue": "AAAI,", "citeRegEx": "Xia et al.. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "Further, the maximum absolute magnitude of x is bounded by a positive constant \u03bb, so that each entry of x has the smallest dynamic range [Lyubarskii and Vershynin, 2010].", "startOffset": 137, "endOffset": 169}, {"referenceID": 27, "context": "Thus, x is called \u201cdemocratic\u201d [Studer et al., 2014] or \u201canti-sparse\u201d [Fuchs, 2011], as all of its entries are of approximately the same importance.", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": ", 2014] or \u201canti-sparse\u201d [Fuchs, 2011], as all of its entries are of approximately the same importance.", "startOffset": 25, "endOffset": 38}, {"referenceID": 27, "context": "In practice, x usually has most entries reaching the same absolute maximum magnitude [Studer et al., 2014], therefore resembling to an antipodal signal in an N -dimensional Hamming space.", "startOffset": 85, "endOffset": 106}, {"referenceID": 17, "context": "C of [Lyubarskii and Vershynin, 2010]:", "startOffset": 5, "endOffset": 37}, {"referenceID": 7, "context": "Based on the Alternating Direction Methods of Multipliers (ADMM) algorithm, we formulate (1) as a feedforward neural network [Gregor and LeCun, 2010], called Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases.", "startOffset": 125, "endOffset": 149}, {"referenceID": 27, "context": ", [Studer et al., 2014]) relies on iterative solutions.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": "[Stark and Parker, 1995] proposed ar X iv :1 60 4.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "an active set strategy similar to that of [Lawson and Hanson, 1974].", "startOffset": 42, "endOffset": 67}, {"referenceID": 0, "context": "In [Adlers, 1998], the authors investigated a primaldual path-following interior-point method.", "startOffset": 3, "endOffset": 17}, {"referenceID": 29, "context": "In addition, the joint optimization of the (unsupervised) feature learning and the supervised steps has to rely on solving complex bi-level optimization problems [Wang et al., 2015].", "startOffset": 162, "endOffset": 181}, {"referenceID": 1, "context": "Since the inference complexity of those iterative algorithms increases more than linearly with respect to the dictionary size [Bertsekas, 1999], their scalability turns out to be limited.", "startOffset": 126, "endOffset": 143}, {"referenceID": 10, "context": "Deep learning has recently attracted great attentions [Krizhevsky et al., 2012].", "startOffset": 54, "endOffset": 79}, {"referenceID": 32, "context": "The feed-forward networks could be naturally tuned jointly with task-driven loss functions [Wang et al., 2016c].", "startOffset": 91, "endOffset": 111}, {"referenceID": 7, "context": "In [Gregor and LeCun, 2010], a feed-forward neural network, named LISTA, was proposed to efficiently approximate the sparse codes, whose hyperparameters were learned from general regression.", "startOffset": 3, "endOffset": 27}, {"referenceID": 23, "context": "In [Sprechmann et al., 2013], the authors leveraged a similar idea on fast trainable regressors and constructed feedforward network approximations of the learned sparse models.", "startOffset": 3, "endOffset": 28}, {"referenceID": 24, "context": "It was later extended in [Sprechmann et al., 2015] to develop a principled process of learned deterministic fixedcomplexity pursuits, for sparse and low rank models.", "startOffset": 25, "endOffset": 50}, {"referenceID": 32, "context": "Lately, [Wang et al., 2016c] proposed Deep `0 Encoders, to model `0 sparse approximation as feed-forward neural networks.", "startOffset": 8, "endOffset": 28}, {"referenceID": 31, "context": "The authors extended the strategy to the graph-regularized `1 approximation in [Wang et al., 2016b], and the dual sparsity model in [Wang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 30, "context": ", 2016b], and the dual sparsity model in [Wang et al., 2016a].", "startOffset": 41, "endOffset": 61}, {"referenceID": 1, "context": "ADMM has been popular for its remarkable effectiveness in minimizing objectives with linearly separable structures [Bertsekas, 1999].", "startOffset": 115, "endOffset": 132}, {"referenceID": 32, "context": "Since the threshold \u03bb is less straightforward to update, we repeat the same trick in [Wang et al., 2016c] to rewrite (10) as:", "startOffset": 85, "endOffset": 105}, {"referenceID": 10, "context": "proposed neurons: Rectifier Linear Unit (ReLU) [Krizhevsky et al., 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 31, "context": ", 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al., 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 32, "context": ", 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al., 2016c], as depicted in Fig.", "startOffset": 51, "endOffset": 71}, {"referenceID": 7, "context": "Rather than solving (1) first and then training the encoder as general regression, as [Gregor and LeCun, 2010] did, we instead concatenate encoder(s) with a task-driven loss, and optimize the pipeline from end to end.", "startOffset": 86, "endOffset": 110}, {"referenceID": 5, "context": "Background With the ever-growing large-scale image data on the Web, much attention has been devoted to nearest neighbor search via hashing methods [Gionis et al., 1999].", "startOffset": 147, "endOffset": 168}, {"referenceID": 5, "context": "Furthermore, while earlier methods, such as linear search hashing (LSH) [Gionis et al., 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 6, "context": ", 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 33, "context": ", 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al., 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 11, "context": ", 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al., 2012].", "startOffset": 173, "endOffset": 216}, {"referenceID": 16, "context": ", 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al., 2012].", "startOffset": 173, "endOffset": 216}, {"referenceID": 19, "context": "[Masci et al., 2014] first applied the siamese network [Hadsell et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": ", 2014] first applied the siamese network [Hadsell et al., 2006] architecture to hashing, which fed two input patterns into two parameter-sharing \u201cencoder\u201d columns and minimized a pairwise-similarity/dissimilarity loss function between their outputs, using pairwise labels.", "startOffset": 42, "endOffset": 64}, {"referenceID": 18, "context": "The authors further enforced the sparsity prior on the hash codes in [Masci et al., 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 7, "context": ", 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al.", "startOffset": 54, "endOffset": 78}, {"referenceID": 19, "context": ", 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al., 2014] [Xia et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 34, "context": ", 2014] [Xia et al., 2014; Li et al., 2015] utilized tailored convolution networks with the aid of pairwise labels.", "startOffset": 8, "endOffset": 43}, {"referenceID": 14, "context": ", 2014] [Xia et al., 2014; Li et al., 2015] utilized tailored convolution networks with the aid of pairwise labels.", "startOffset": 8, "endOffset": 43}, {"referenceID": 12, "context": "[Lai et al., 2015] further introduced a triplet loss with a divide-and-encode strategy applied to reduce the hash code redundancy.", "startOffset": 0, "endOffset": 18}, {"referenceID": 18, "context": "final training step of quantization, [Masci et al., 2013] relied on an extra hidden layer of tanh neurons to approximate binary codes, while [Lai et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 12, "context": ", 2013] relied on an extra hidden layer of tanh neurons to approximate binary codes, while [Lai et al., 2015] exploited a piece-wise linear and discontinuous threshold function.", "startOffset": 91, "endOffset": 109}, {"referenceID": 19, "context": "Our Approach In view of its robustness to quantization noise, as well as BLU\u2019s property as a natural binarization approximation, we construct a siamese network as in [Masci et al., 2014], and adopt a pair of parameter-sharing deep `\u221e encoders as the two columns.", "startOffset": 166, "endOffset": 186}, {"referenceID": 19, "context": "In this paper, we follow [Masci et al., 2014] to use a default m = 5 for all experiments.", "startOffset": 25, "endOffset": 45}, {"referenceID": 10, "context": "Implementation The proposed networks are implemented with the CUDA ConvNet package [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 32, "context": "Different from prior findings such as in [Wang et al., 2016c; Wang et al., 2016b], we discover that untying the values of S1, b1 and S2, b2 boosts the performance more than sharing them.", "startOffset": 41, "endOffset": 81}, {"referenceID": 31, "context": "Different from prior findings such as in [Wang et al., 2016c; Wang et al., 2016b], we discover that untying the values of S1, b1 and S2, b2 boosts the performance more than sharing them.", "startOffset": 41, "endOffset": 81}, {"referenceID": 28, "context": "While many neural networks are trained well with random initializations, it has been discovered that sometimes poor initializations can still hamper the effectiveness of first-order methods [Sutskever et al., 2013].", "startOffset": 190, "endOffset": 214}, {"referenceID": 3, "context": "We first estimate the dictionary D using the standard K-SVD algorithm [Elad and Aharon, 2006], and then inexactly solve (1) for up to K (K = 2) iterations, via the ADMM algorithm in Section 2, with the values of Lagrange multiplier pt recorded for each iteration.", "startOffset": 70, "endOffset": 93}, {"referenceID": 21, "context": "The images are represented using 384-dimensional GIST descriptors [Oliva and Torralba, 2001].", "startOffset": 66, "endOffset": 92}, {"referenceID": 18, "context": "Following the classical setting in [Masci et al., 2013], we used a training set of 200 images for each class, and a disjoint query set of 100 images per class.", "startOffset": 35, "endOffset": 55}, {"referenceID": 2, "context": "NUS-WIDE [Chua et al., 2009] is a dataset containing 270K annotated images from Flickr.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": "In training and evaluation, we followed the protocol of [Liu et al., 2011]: two images were considered as neighbors if they share", "startOffset": 56, "endOffset": 74}, {"referenceID": 16, "context": "\u2022 four representative \u201cshallow\u201d hashing methods: kernelized supervised hashing (KSH) [Liu et al., 2012], anchor graph hashing (AGH) [Liu et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 15, "context": ", 2012], anchor graph hashing (AGH) [Liu et al., 2011] (we compare with its two alternative forms: AGH1 and AGH2; see the original paper), parameter-sensitive hashing (PSH) [Shakhnarovich et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 22, "context": ", 2011] (we compare with its two alternative forms: AGH1 and AGH2; see the original paper), parameter-sensitive hashing (PSH) [Shakhnarovich et al., 2003], and LDA Hash (LH) [Strecha et al.", "startOffset": 126, "endOffset": 154}, {"referenceID": 26, "context": ", 2003], and LDA Hash (LH) [Strecha et al., 2012] 2.", "startOffset": 27, "endOffset": 49}, {"referenceID": 19, "context": "\u2022 two latest \u201cdeep\u201d hashing methods: neural-network hashing (NNH) [Masci et al., 2014], and sparse neuralnetwork hashing (SNNH) [Masci et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 18, "context": ", 2014], and sparse neuralnetwork hashing (SNNH) [Masci et al., 2013].", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": "Most of the results are collected from the comparison experiments in [Masci et al., 2013], under the same settings.", "startOffset": 69, "endOffset": 89}, {"referenceID": 20, "context": "We adopt the following classical criteria for evaluation: 1) precision and recall (PR) for different Hamming radii, and the F1 score as their harmonic average; 2) mean average precision (MAP) [M\u00fcller et al., 2001].", "startOffset": 192, "endOffset": 213}, {"referenceID": 18, "context": "Besides, for NUS-WIDE, as computing mAP is slow over this large dataset, we follow the convention of [Masci et al., 2013] to compute the mean precision (MP) of top-5K returned neighbors (MP@5K), as well as report mAP of top-10 results (mAP@10).", "startOffset": 101, "endOffset": 121}, {"referenceID": 34, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 14, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 12, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 12, "context": ", [Lai et al., 2015], into comparison because they will require three parallel encoder columns .", "startOffset": 2, "endOffset": 20}, {"referenceID": 18, "context": "That is also evidenced by Table 2 in [Masci et al., 2013], i.", "startOffset": 37, "endOffset": 57}], "year": 2016, "abstractText": "We investigate the `\u221e-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a Deep Siamese `\u221e Network, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.", "creator": "LaTeX with hyperref package"}}}