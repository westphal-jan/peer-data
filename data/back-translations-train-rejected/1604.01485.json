{"id": "1604.01485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "A Focused Dynamic Attention Model for Visual Question Answering", "abstract": "Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.", "histories": [["v1", "Wed, 6 Apr 2016 05:16:10 GMT  (5635kb,D)", "http://arxiv.org/abs/1604.01485v1", "Submitted to ECCV 2016"]], "COMMENTS": "Submitted to ECCV 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.NE", "authors": ["ilija ilievski", "shuicheng yan", "jiashi feng"], "accepted": false, "id": "1604.01485"}, "pdf": {"name": "1604.01485.pdf", "metadata": {"source": "CRF", "title": "A Focused Dynamic Attention Model for Visual Question Answering", "authors": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "emails": ["ilija.ilievski@u.nus.edu,", "eleyans@nus.edu.sg", "elefjia@nus.edu.sg"], "sections": [{"heading": null, "text": "Keywords: Answering Visual Questions, Attention"}, {"heading": "1 Introduction", "text": "The question that arises is whether and in what form and in what form people will be able to change and change the world. The question is only whether people are able to change the world or change it. The question is only whether people are able to change and change the world. The question is only whether people are able to change and change the world. The question is only whether people are able to change and change the world. The question is only whether people are able to change and change the world. The question is only why they are able to change and change the world."}, {"heading": "2 Related Work", "text": "VQA has recently received a great deal of research attention, and some methods have been developed to solve this problem. The most similar model we know of are the Stacked Attention Networks (SAN) proposed by Yang et al. [19]. Both models use an attention mechanism that combines words and image regions. However, they use Convolutionary Neural Networks to draw attention to image regions based on the question word Unigrams, Bigrams, and Trigrams. Furthermore, their attention mechanism does not use object boxes, which makes attention less focused. Another model that uses the attention mechanism to solve VQA problems is the ABC-CNN model described in [18]. ABC-CNN uses the question to configure revolutionary cores that define a weighted map of image functions. The advantage of our FDA model over ABC-CNN is that it is twofold."}, {"heading": "3 Method Overview", "text": "In this section we briefly describe the motivation and give formal problem formulations."}, {"heading": "3.1 Problem Formulation", "text": "The visual answer problem can be presented as a prediction of the best answer, depending on whether there is a picture I or a question q. Common practice [6,16,3,19] is to use the 1000 most common answers in the training set, thus simplifying the VQA task into a classification problem. The following equation presents the problem mathematically: a-arg max a-arg max a-p (a-I, q-\u03b8) (1), with b being the set of all possible answers and c being the model weights."}, {"heading": "3.2 Motivation", "text": "The basic methods in [6] show only a modest increase in accuracy when the image characteristics are included (4.98% for open questions and 2.42% for multiple choice questions).We believe that the image contains much more information and the accuracy should be increased much more. Therefore, we focus on improving the image characteristics and design a visual attention mechanism that learns to focus on the question-related image areas.The proposed attention mechanism is loose inspiration for the human visual attention mechanism.People shift the focus from one image region to another before they understand how the regions relate to each other and grasp the meaning of the overall image. Likewise, we feed our model image regions relevant to the question before showing the overall image."}, {"heading": "4 Focused Dynamic Attention for VQA", "text": "The FDA model consists of question and image understanding components, attention mechanisms, and a multimodal representational fusion network (Figure 1), which we describe in detail in this section."}, {"heading": "4.1 Question Understanding", "text": "Following common practice, our FDA model uses an LSTM network to encode the question in a vector representation [15,5,18,8]. The LSTM network learns to maintain the characteristic vectors of the important question words in their state, thus giving the question understanding component a word attention mechanism."}, {"heading": "4.2 Image Understanding", "text": "After previous work [3,4,5], we use a pre-trained Convolutionary Neural Network (CNN) to extract image function vectors. Specifically, we use the Deep Residual Networks model, which came first in the ILSVRC and COCO 2015 competitions: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection and COCO segmentation [24]. We extract the weights of the layer immediately before the last SoftMax layer and consider them as visual features. We extract these features for the overall picture (global visual features) and for the specific image regions (local visual features). However, unlike the existing approaches, we use an LSTM network to combine the local and global visual features into a common representation."}, {"heading": "4.3 Focused Dynamic Attention Mechanism", "text": "We introduce a focused dynamic attention mechanism that learns to focus on image regions related to the question words. The attention mechanism works as follows: For each image object, it uses word2vec word embedding [26] to measure the similarity between the question words and the object name. Next, it selects objects with similarity values greater than 0.5 and extracts the feature vectors of the object embedding boxes using a pre-trained ResNet model [24]. Following the question, it feeds the LSTM network with the corresponding object feature vectors. Finally, it feeds the LSTM network with the feature vector of the overall image and uses the resulting LSTM state as a visual representation. Thus, the attention mechanism m1 allows us to use the basic truth objects, the boxes and labels to get the boxed [To pre-compile the test time, we use the object composite]."}, {"heading": "4.4 Multimodal Representation Fusion", "text": "We consider the final state of the two LSTM networks as question and image representation. We begin to merge them into a single representation by applying Tanh to the question representation and ReLU 2 to the image representation 3. We then proceed by performing an elementary multiplication of the two vector representations, and the resulting vector is fed into a fully networked neural network. Finally, a SoftMax layer classifies the multimodal representation into one of the possible 4 responses."}, {"heading": "5 Evaluation", "text": "In this section we will explain the implementation of the model and compare our model with the current state of methodologies.2 Defined as f (x) = max (0, x).3 The use of different activation functions resulted in slightly worse overall outcomes 4 We will follow [6] and use the 1000 most common answers."}, {"heading": "5.1 Dataset", "text": "For all experiments, we use the Visual Question Answering (VQA) dataset [6], which is the largest and most complex image dataset for answering visual questions. It contains three human-asked questions and ten answers given by different subjects for each of the 204,721 images in the Microsoft COCO dataset [7]. Figure 2 shows two representative examples found in the dataset, evaluated using two test splits test developer and test hour and two tasks: - An open-ended task in which the method should produce a natural-language answer; - A multiple-choice task in which the method should choose one of the 18 different answers for each question."}, {"heading": "5.2 Baseline Model", "text": "We compare our model with the basic models of the authors of the VQA dataset [27], who currently achieve the best performance in the test standard split for the multiple choice task. [6] The model described for the first time is a standard implementation of an LSTM + CNN VQA model. It uses an LSTM to encode the question and CNN features to encode the image. To answer a question, it multiplies the last LSTM state with the CNN features of the image and feeds the result into a SoftMax layer for classification into one of the 1000 most common answers. Implementation in [27] uses a deeper two-tiered LSTM network to encode the question and normalized CNN features of the image that were critical to reaching the state of the art."}, {"heading": "5.3 Model Implementation and Training Details", "text": "We transform the question words into a vector form by multiplying a one-dimensional vector representation with a word embedding matrix. Word size is 12,602 and word embedding is 300-dimensional. We feed a pre-trained ResNet network [24] and use the 2048-dimensional weight vector of the layer before the last fully connected layer. Word and image vectors are fed into two separate LSTM networks. LSTM networks are the standard implementation of a single-layer LSTM network [15] with a 512-dimensional state vector. The final state of the question LSTM is passed through Tanh, while the final state of the image LSTM is passed through ReLU5. We multiply the resulting vectors element by element to obtain a multimodal representation vector, which is then embedded in a fully connected neural network (max = 5)."}, {"heading": "5.4 Model Evaluation and Comparison", "text": "We compare our model with the baselines provided by the VQA authors [6]. The results for the open-ended task are listed in Table 1, and the results for the multiple-choice task are listed in Table 2. In the tables, the \"question\" and \"picture\" baselines contain only the question words and the FDA image, respectively. The \"Q + I\" approach is a baseline that combines the two but does not use an LSTM network. \"LSTM Q + I\" and \"D-LSTM\" are LSTM models, with one and two layers interacting. If we compare the performance of the baselines, we can compare the increase in accuracy with the addition of information from each modality.From Table 1, we can see that our proposed achieves the best performance on these benchmark datasets, surpassing the state of the art (SAN) by about 0.6%."}, {"heading": "5.5 Qualitative Results", "text": "Figure 3 shows particularly difficult examples (the dominant image color is not the right answer) for the type of \"what color\" questions. But by focusing on the question-related image regions, the FDA model is still able to give the right answer. In Figure 4, we show examples where the model focuses on different regions of the same image depending on the words in the question. Focusing on the right image region is critical when it comes to answering unusual questions for an image (row 1), questions about small image objects (row 2), or when the most dominant image object partially encloses the question-related region and can lead to an incorrect answer (row 3). Representative examples of questions that require identification of image objects are shown in Figure 5. We can observe that focused attention enables the model to answer complex questions (row 1, left) and to count questions (row 1, right)."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a novel Focused Dynamic Attention (FDA) focused attention model to solve the challenging VQA issues. FDA builds on a generic object-centric attention model to extract question-related visual features from an image, as well as on a stack of multiple LSTM layers for feature fusion. By focusing only on the identified regions specific to proposed questions, it was shown that FDA was able to filter out overwhelming irrelevant information from cluttered backgrounds or other regions, significantly improving the quality of visual representations in terms of answering proposed questions. By merging clean regional representation, global context, and questioning about LSTM layers, the FDA delivered significant performance improvements over baselines on the VQA benchmark datasets, both for the open and for Vice-choice tasks."}], "references": [{"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences 112(12)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "The IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013 ECCV 2014. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "arXiv preprint arXiv:1511.05756", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Freeform visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "Hengel", "A.v.d.", "A. Dick"], "venue": "arXiv preprint arXiv:1511.06973", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1506.00333", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "arXiv preprint arXiv:1511.07394", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "arXiv preprint arXiv:1511.02274", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Doll\u00e1r"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Compositional memory for visual question answering", "author": ["A. Jiang", "F. Wang", "F. Porikli", "Y. Li"], "venue": "arXiv preprint arXiv:1511.05676", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiscale combinatorial grouping for image segmentation and object proposal generation", "author": ["J. Pont-Tuset", "P. Arbel\u00e1ez", "J. Barron", "F. Marques", "J. Malik"], "venue": "arXiv:1503.00848.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Jiasen Lu", "D.B. Xiao Lin", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/VQA_LSTM_CNN", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1,2], and besides contributing to the advancement of the involved research areas, it has other important applications, such as blind person assistance and image retrieval.", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1,2], and besides contributing to the advancement of the involved research areas, it has other important applications, such as blind person assistance and image retrieval.", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "The first feasible solution to VQA problems was provided by Malinowski and Fritz in [2], where they used a semantic language parser and a Bayesian reasoning model, to understand the meaning of questions and to generate the proper answers.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Malinowski and Fritz also constructed the first VQA benchmark dataset, named as DAQUAR, which contains 1,449 images and 12,468 questions generated by humans or automatically by following a template and extracting facts from a database [2].", "startOffset": 235, "endOffset": 238}, {"referenceID": 2, "context": "[3] released the TORONTO\u2013QA dataset, which contains a large number of images (123,287) and questions (117,684), but the questions are automatically generated and thus can be answered without complex reasoning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Nevertheless, the release of the TORONTO\u2013QA dataset was important since it provided enough data for deep learning models to be trained and evaluated on the VQA problem [4,5,3].", "startOffset": 168, "endOffset": 175}, {"referenceID": 4, "context": "Nevertheless, the release of the TORONTO\u2013QA dataset was important since it provided enough data for deep learning models to be trained and evaluated on the VQA problem [4,5,3].", "startOffset": 168, "endOffset": 175}, {"referenceID": 2, "context": "Nevertheless, the release of the TORONTO\u2013QA dataset was important since it provided enough data for deep learning models to be trained and evaluated on the VQA problem [4,5,3].", "startOffset": 168, "endOffset": 175}, {"referenceID": 5, "context": "[6] published the currently largest VQA dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It consists of three human posed questions and ten answers given by different human subjects, for each one of the 204,721 images found in the Microsoft COCO dataset [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 35, "endOffset": 43}, {"referenceID": 8, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 35, "endOffset": 43}, {"referenceID": 9, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 35, "endOffset": 43}, {"referenceID": 10, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 153, "endOffset": 166}, {"referenceID": 11, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 153, "endOffset": 166}, {"referenceID": 12, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 153, "endOffset": 166}, {"referenceID": 13, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 153, "endOffset": 166}, {"referenceID": 14, "context": "The current top performing methods [8,9,10] employ deep neural network model that predominantly uses the convolutional neural network (CNN) architecture [11,12,13,14] to extract image features and a Long Short-Term Memory (LSTM) [15] network to extract the representations for questions.", "startOffset": 229, "endOffset": 233}, {"referenceID": 15, "context": "The CNN and LSTM representation vectors are then usually fused by concatenation [16,3,5] or", "startOffset": 80, "endOffset": 88}, {"referenceID": 2, "context": "The CNN and LSTM representation vectors are then usually fused by concatenation [16,3,5] or", "startOffset": 80, "endOffset": 88}, {"referenceID": 4, "context": "The CNN and LSTM representation vectors are then usually fused by concatenation [16,3,5] or", "startOffset": 80, "endOffset": 88}, {"referenceID": 16, "context": "element-wise multiplication [17,18].", "startOffset": 28, "endOffset": 35}, {"referenceID": 17, "context": "element-wise multiplication [17,18].", "startOffset": 28, "endOffset": 35}, {"referenceID": 17, "context": "Other approaches additionally incorporate some kind of attention mechanism over the image features [18,19,20].", "startOffset": 99, "endOffset": 109}, {"referenceID": 18, "context": "Other approaches additionally incorporate some kind of attention mechanism over the image features [18,19,20].", "startOffset": 99, "endOffset": 109}, {"referenceID": 19, "context": "Other approaches additionally incorporate some kind of attention mechanism over the image features [18,19,20].", "startOffset": 99, "endOffset": 109}, {"referenceID": 18, "context": "However, using features from all image regions [19,18] may provide too much noise or overwhelming information irrelevant to the question and thus hurt the overall VQA performance.", "startOffset": 47, "endOffset": 54}, {"referenceID": 17, "context": "However, using features from all image regions [19,18] may provide too much noise or overwhelming information irrelevant to the question and thus hurt the overall VQA performance.", "startOffset": 47, "endOffset": 54}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "However, [19] use convolutional neural network to put attention over the image regions, based on the question word unigrams, bigrams, and trigrams.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Another model that uses attention mechanism in solving VQA problems is the ABC-CNN model described in [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Yet another attention model for visual question answering is proposed in [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "The work, is closely related to the work by [18], in that it also applies a weighted map over the image and the question word features.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "However, similar to our work, they use object proposals from [21] to select image regions instead of the whole image.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "In contrast, the model proposed in [21] straightforwardly concatenate all the image region features with the question word features and feed them all at once to a two layer network.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "propose another model that combines the CNN image features and an LSTM network for encoding the multimodal representation, with the addition of a Compositional Memory units which fuse the image and word feature vectors [22].", "startOffset": 219, "endOffset": 223}, {"referenceID": 9, "context": "in [10] take an interesting approach and use three convolutional neural networks to represent not only the image, but also the question, and their common representation in a multimodal space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Common practice [6,16,3,19] is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem.", "startOffset": 16, "endOffset": 27}, {"referenceID": 15, "context": "Common practice [6,16,3,19] is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem.", "startOffset": 16, "endOffset": 27}, {"referenceID": 2, "context": "Common practice [6,16,3,19] is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem.", "startOffset": 16, "endOffset": 27}, {"referenceID": 18, "context": "Common practice [6,16,3,19] is to use the 1,000 most common answers in the training set and thus simplify the VQA task to a classification problem.", "startOffset": 16, "endOffset": 27}, {"referenceID": 5, "context": "The baseline methods from [6] show only modest increase in accuracy when including the image features (4.", "startOffset": 26, "endOffset": 29}, {"referenceID": 14, "context": "Following a common practice, our FDA model uses an LSTM network to encode the question in a vector representation [15,5,18,8].", "startOffset": 114, "endOffset": 125}, {"referenceID": 4, "context": "Following a common practice, our FDA model uses an LSTM network to encode the question in a vector representation [15,5,18,8].", "startOffset": 114, "endOffset": 125}, {"referenceID": 17, "context": "Following a common practice, our FDA model uses an LSTM network to encode the question in a vector representation [15,5,18,8].", "startOffset": 114, "endOffset": 125}, {"referenceID": 7, "context": "Following a common practice, our FDA model uses an LSTM network to encode the question in a vector representation [15,5,18,8].", "startOffset": 114, "endOffset": 125}, {"referenceID": 2, "context": "Following prior work [3,4,5], we use a pre-trained convolutional neural network (CNN) to extract image feature vectors.", "startOffset": 21, "endOffset": 28}, {"referenceID": 3, "context": "Following prior work [3,4,5], we use a pre-trained convolutional neural network (CNN) to extract image feature vectors.", "startOffset": 21, "endOffset": 28}, {"referenceID": 4, "context": "Following prior work [3,4,5], we use a pre-trained convolutional neural network (CNN) to extract image feature vectors.", "startOffset": 21, "endOffset": 28}, {"referenceID": 23, "context": "Specifically, we use the Deep Residual Networks model used in ILSVRC and COCO 2015 competitions, which won the 1 places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation [24].", "startOffset": 230, "endOffset": 234}, {"referenceID": 25, "context": "For each image object it uses word2vec word embeddings [26] to measure the similarity between the question words and the object label.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "5 and extracts the feature vectors of the objects bounding boxes with a pre-trained ResNet model [24].", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "At test time we use the precomputed bounding boxes from [25] and classify them with [24] to obtain the object labels.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "At test time we use the precomputed bounding boxes from [25] and classify them with [24] to obtain the object labels.", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "Examples provided by [6].", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "3 Applying different activation functions gave slightly worse overall results 4 We follow [6] and use the 1000 most common answers", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "For all experiments we use the Visual Question Answering (VQA) dataset [6], which is the largest and most complex image dataset for the visual question answering task.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "The dataset contains three human posed questions and ten answers given by different human subjects, for each one of the 204,721 images found in the Microsoft COCO dataset [7].", "startOffset": 171, "endOffset": 174}, {"referenceID": 26, "context": "We compare our model against the baseline models provided by the VQA dataset authors [27], which currently achieve the best performance on the test-standard split for the multiple-choice task.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "The model, first described in [6], is a standard implementation of an LSTM+CNN VQA model.", "startOffset": 30, "endOffset": 33}, {"referenceID": 26, "context": "The implementation in [27] uses a deeper two layer LSTM network for encoding the question, and normalized image CNN features, which showed crucial for achieving the state-of-the-art.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "We feed a pre-trained ResNet network [24] and use the 2,048 dimensional weight vector of the layer before the last fully-connected layer.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "The LSTM networks are standard implementation of one layer LSTM network [15], with a 512 dimensional state vector.", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Comparison between the baselines from [6], the state-of-the-art models and our FDA model on VQA test-dev and test-standard data for the open-ended task.", "startOffset": 38, "endOffset": 41}, {"referenceID": 21, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 46, "endOffset": 50}, {"referenceID": 8, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 82, "endOffset": 85}, {"referenceID": 22, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "Results from most recent methods including CM [22], ACK [9], iBOWIMG [16], DPPnet [8], D-NMN [23], D-LSTM [27], and SAN [19] are provided and compared with.", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "Comparison between the baselines from [6], the state-of-the-art models and our FDA model on VQA test-dev and test-standard data for the multiple-choice task.", "startOffset": 38, "endOffset": 41}, {"referenceID": 16, "context": "Results from most recent methods including WR [17], iBOWIMG [16], DPPnet [8], and D-LSTM [27] are also shown for comparison.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "Results from most recent methods including WR [17], iBOWIMG [16], DPPnet [8], and D-LSTM [27] are also shown for comparison.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Results from most recent methods including WR [17], iBOWIMG [16], DPPnet [8], and D-LSTM [27] are also shown for comparison.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "Results from most recent methods including WR [17], iBOWIMG [16], DPPnet [8], and D-LSTM [27] are also shown for comparison.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "We compare our model with the baselines provided by the VQA authors [6].", "startOffset": 68, "endOffset": 71}], "year": 2016, "abstractText": "Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions \u2013 as some region-based image recognition methods do \u2013 cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over wellestablished baselines.", "creator": "LaTeX with hyperref package"}}}