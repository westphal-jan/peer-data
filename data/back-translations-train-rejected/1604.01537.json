{"id": "1604.01537", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "abstract": "We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve performance.", "histories": [["v1", "Wed, 6 Apr 2016 08:26:31 GMT  (501kb)", "http://arxiv.org/abs/1604.01537v1", "12 pages, 8 figures, 4 tables"]], "COMMENTS": "12 pages, 8 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["xiaoyuan yi", "ruoyu li", "maosong sun"], "accepted": false, "id": "1604.01537"}, "pdf": {"name": "1604.01537.pdf", "metadata": {"source": "CRF", "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "authors": ["Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun"], "emails": ["yxyz2012yxy@163.com", "liruoyusince1995@gmail.com", "sms@mail.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to dance, to dance, to move,"}, {"heading": "2 Related Work", "text": "Research on poetry began in the 1960s and has been a focal point in recent decades. Manurung (2003) proposed three criteria for automatically generated poetry: grammar (the generated lines must obey grammar rules and be legible), meaningfulness (the lines should have something to do with the subject), and poetry (generated poems must be filled with words, such as rhythm, cadence, and the particular use of words). Early methods are based on rules and templates. ASPERA (Gerv\u00e1s, 2001) uses the changes in accents to fill the system with words."}, {"heading": "3 The Poem Generator", "text": "The structure RNN encoder decoder is therefore suitable for sequential learning tasks. In machine translation tasks, the pairs of sentences are semantically similar, allowing the model to learn the corresponding relationships. In classical Chinese quatrains, there is close semantic relevance between two adjacent lines. Such two lines of poetry are semantically relevant sequence pairs. We use RNN encoder decoders to learn the relevance, which is then used to generate a line of poetry in the previous line. To use context information at different levels, we build three poem line generation blocks (word-to-line, line-to-context) to generate an entire line. As illustrated in Figure 2, the user enters a keyword as a topic to show the main content and emotions that the poem should convey. First, WPB generates a relevant keyword for the first line."}, {"heading": "3.1 Sentence Poem Block (SPB)", "text": "SPB (Sentence Poem Block) is used for line-to-line generation. In generating the second line, the only context information available is the first line. Therefore, we use SPB to create the second line, using the first line as input. As shown in Figure 3, we use bidirectional RNN with attention mechanism proposed by (Bahdanau et al., 2015) to build SPB. Let us designate an input request tight line by line by X,..., and an output line by Y,..., e is the word embedding of the t-th character. And represent the forward and backward hidden lines in encoder respectively meaning.In encoder: 1 2 31 \u2219 4; 5 (5) are formulas for the compilation of forward hidden lines. The compilation of forward hidden lines is similar to \u2022 is elementally multiplicable."}, {"heading": "3.2 Context Poem Block (CPB)", "text": "In order to use more context information, we build another encoder decoder called CPB (Context Poem Block). The structure of CPB is similar to that of SPB. The difference is that we concatenate two adjacent lines in a quadruple line as a long input sequence and use the third line as a target sequence in training. Thus, the model can use information from the last two lines to create the current line. The last characters of the second and fourth lines must rhyme and the last character of the third line must not rhyme. When creating the fourth line, SPB cannot determine the rhyme. By taking into account the second line, CPB can learn the rhyme. Thus, we can use CPB to create the third and fourth lines. Zhang (2014) uses the context by compressing all lines into a 200-dimensional vector that causes a severe loss of semantic information."}, {"heading": "3.3 Word Poem Block (WPB)", "text": "A major drawback of SMT-based methods is that they require a different model to generate the first line. He (2012), for example, extends the user's keywords, then uses constraint templates and a language model to search for a line. In the RNN encoder decoder, words and sentences are mapped into the same vector space. As our system is character-based, words can be considered short sequences. Ideally, SPB will generate a relevant line using a word as input, but the training pairs are all long sequences, it will not work well if the input is a short word. Therefore, we train the third encoder decoder, called Word Poem Block (WPB). Based on the model parameters of trained SPB, we will use some < word, line > pairs to further improve the WPB's ability to generate long sequences with short sequences."}, {"heading": "3.4 Qualitative Analysis", "text": "Our task to generate poems is based on sequence pairs with semantic relevance. We conducted several qualitative experiments and the results show that RNN encoder decoders can well capture the semantic meaning and relevance of pairs in such relevance learning tasks, which is why we use the model to build our system."}, {"heading": "3.4.1 Poem Line Representations", "text": "We selected three types of classical poetry: borderline poetry (poetry about wars), boudoir-plaint poetry (poetry about women's sadness), and historical nostalgia poetry (poetry about history); for each type, we received ten lines and used Barnes-Hut-SNE (van der Maaten, 2013) to place their representations in a two-dimensional space. As shown in Figure 4, lines of the same type gather together; the representations can well grasp the semantic meaning of lines of poetry; we also generated representations for 20,000 lines, then calculated their Cos spacing and calculated their Lcs (longest common sequences) as a comparison. As shown in Table 1, the results of Lcs are simple string mappings, but the results of nodes have the same meaning as the inputs. Although there are no unique boundaries in input sequences for SPB, the vector representations may appear \"one word or two for a single word.\""}, {"heading": "3.4.2 Gated Units in Word Boundary Recognition", "text": "As we can see in formula (2) and formula (3), gated units tend to use current input to update the hidden state. While gated units tend to continue earlier hidden states, we used the average value of each element as the reset value of the t-th character. Along the propagation direction of hidden states, we calculated the difference between the reset values of the latter and the previous character to obtain the reset tendency. Likewise, we obtained the updating tendency. As shown in figure 5, higher reset tendencies and smaller updating tendencies mean that the two characters tend to be an entity. While they tend to be separate. In line... \""}, {"heading": "3.4.3 Attention Mechanism in Capturing Associations", "text": "Unlike the word alignment in the translation task, the attention mechanism can capture the implicit associations between two characters. Figure 6 is the visualization of, in formula (11); the top two plots show the associations between input and output lines generated by an SPB with inverted target lines; the bottom two plots are the results of SPBs trained with normal target lines; and in the top two plots the outputs are syntactically similar to inputs; while in the right two ports the output-output outputs are syntactically different from the inputs, the output-output outputs are syntactically different; in the top left plot-plot-input-input-input-input-input-inputs-inputs-inputs-inputs-syntactically different."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data and Settings", "text": "Our body comprises 398,391 poems from the Tang Dynasty to the present day. We did not use earlier poems because regular tone patterns developed during the Tang Dynasty. We used 10,000 poems as a test set and others as a training set. We extracted three pairs from each quatrain. We used 999,442 pairs to train SPB and 596,610 pairs to train CPB. To train WPB, we selected 3,000 words, and for each word we received 150 lines containing the word. Finally, we received 450,000 word-to-line pairs (half consisting of 5 characters and the other half of 7 characters)."}, {"heading": "4.2 Evaluation Design", "text": "We compared the system with three criteria: http: / / github.com / lisa-groundhog / GroundHog. 2 http: / / duilian.msra.cn / jueju /. 3 http: / / www.poeming.com / web / index.htm. 4 Because the keywords in the SMT are limited, we evaluated the output line in the previous line as input. Since most words in Chinese classical poetry consist of one or two characters, BLEU-2 is effective. It is difficult to obtain human-authorized references for poetry lines, so we automatically evaluate the output line."}, {"heading": "4.3 Evaluation Results", "text": "In fact, most of them will be able to move to another world in which they are able to live, in which they want to live."}, {"heading": "5 Conclusion and Future Work", "text": "Compared to other methods, our system can jointly learn semantic meanings, semantic relevance and the use of rhythmic and tonal patterns without using any limitations. Both automatic evaluation and human evaluation show that our system outperforms other systems, but there is still a gap between our system and the poets of antiquity. We show that RNN encoder decoders are also suitable for learning tasks on semantically relevant sequences. The attention mechanism can capture character associations, and gated units can roughly detect word boundaries. In addition, the inversion of target lines in education will lead to better performance. There is much to be done for our system in the future. Based on various blocks, our system is expandable. We will improve our system to use more context information and generate other types of Chinese poems, sonci and similar works such as the exploration of poems."}, {"heading": "In Proceedings of the 22nd International", "text": "Robert P. Levy. 2001. A computational model of poetic creativity with neural network as measure of adaptive fitness. In Proceedings of the ICCBR-01 Workshop on Creative Systems. Hisar Maruli Manurung. 2003. An evolutionary algorithm approach to poetry generation. Ph.D. thesis, University of Edinburgh. T. Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, and S. Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH, pp. 1045-1048, Makuhari, Japan."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2015 International Conference on Learning Representations, San", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Diego", "CA.K. Cho", "B. Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Diego et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diego et al\\.", "year": 2014}, {"title": "Generating Chinese classical poems with statistical machine translation models", "author": ["Jing He", "Ming Zhou", "Long Jiang."], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence, pages 1650 \u2013 1656,", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating Chinese couplets using a statistical mt approach", "author": ["Toronto", "Canada. Long Jiang", "Ming Zhou."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics, pages", "citeRegEx": "Toronto et al\\.,? 2008", "shortCiteRegEx": "Toronto et al\\.", "year": 2008}, {"title": "A computational model of poetic creativity with neural network as measure of adaptive fitness", "author": ["Manchester", "UK. Robert P. Levy"], "venue": "In Proceedings of the ICCBR-01 Workshop on Creative Systems", "citeRegEx": "Manchester and Levy.,? \\Q2001\\E", "shortCiteRegEx": "Manchester and Levy.", "year": 2001}, {"title": "An evolutionary algorithm approach to poetry generation", "author": ["Hisar Maruli Manurung."], "venue": "Ph.D. thesis, University of Edinburgh. T. Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, and S. Khudanpur. 2010. Recurrent neural", "citeRegEx": "Manurung.,? 2003", "shortCiteRegEx": "Manurung.", "year": 2003}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, 4:3104-3112. Laurens van der Maaten. 2013. Barnes-hut-sne.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu."], "venue": "Proceedings of the 8th International Conference on", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "I, poet:automatic Chinese poetry composition", "author": ["Paris", "France. Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": "Entertainment Computing,", "citeRegEx": "Paris et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paris et al\\.", "year": 2013}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Doha, Qatar.", "citeRegEx": "Zhang and Lapata.,? 2014", "shortCiteRegEx": "Zhang and Lapata.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Haiku generation system (Wu et al., 2009) expands the input queries to haiku sentences according to the rules extracted from the corpus.", "startOffset": 24, "endOffset": 41}, {"referenceID": 5, "context": "Then through genetic heredity and variation, good results are selected by the evaluation functions (Manurung, 2003; Levy, 2001).", "startOffset": 99, "endOffset": 127}, {"referenceID": 5, "context": "Manurung(2003) proposed three criteria for automatically generated poetry: grammaticality(the generated lines must obey grammar rules and be readable), meaningfulness(the lines should express something related to the theme) and poeticness(generated poems must have poetic features, such as the rhythm, cadence and the special use of words).", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Firstly, we use RNN Encoder-Decoder as the basic structure of our system compared with the method of (He et al., 2012).", "startOffset": 101, "endOffset": 118}, {"referenceID": 9, "context": "Secondly, compared with (Zhang and Lapata, 2014), our model is based on bidirectional RNN with gated units instead of the simple RNN.", "startOffset": 24, "endOffset": 48}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on.", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on. Sentences generated with such method is good at the relevance, but cannot obey the rules and forms. With the cross field of Deep Learning and Natural Language Process becoming focused, neural network has been applied on poetry generation. Zhang and Lapata (2014) compress all the previous information into a vector with RNN to produce the probability distribution of the next character to be generated.", "startOffset": 0, "endOffset": 423}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on. Sentences generated with such method is good at the relevance, but cannot obey the rules and forms. With the cross field of Deep Learning and Natural Language Process becoming focused, neural network has been applied on poetry generation. Zhang and Lapata (2014) compress all the previous information into a vector with RNN to produce the probability distribution of the next character to be generated. Our work differs from the previous work mainly as follows. Firstly, we use RNN Encoder-Decoder as the basic structure of our system compared with the method of (He et al., 2012). Moreover, in He's system the rhythm is controlled externally and the results perform poorly on tonal patterns. While our system can learn all these things jointly. Secondly, compared with (Zhang and Lapata, 2014), our model is based on bidirectional RNN with gated units instead of the simple RNN. Besides, Zhang (2014) compress all context information into a small vector, losing much of the useful information.", "startOffset": 0, "endOffset": 1062}, {"referenceID": 0, "context": "As shown in figure 3, we use bidirectional RNN with attention mechanism proposed by (Bahdanau et al., 2015) to build SPB.", "startOffset": 84, "endOffset": 107}, {"referenceID": 6, "context": "Furthermore, we find inverting target lines can improve the performance, as the way of inverting the source lines mentioned in (Sutskever et al., 2014).", "startOffset": 127, "endOffset": 151}, {"referenceID": 2, "context": "It's hard to obtain human-authored references for poem lines so we used the method in (He et al., 2012) to extract references automatically.", "startOffset": 86, "endOffset": 103}, {"referenceID": 2, "context": "We compared our system with the system in (He et al., 2012).", "startOffset": 42, "endOffset": 59}, {"referenceID": 5, "context": "Referring to the three criteria in (Manurung, 2003), we designed five criteria: Fluency (are the lines fluent and wellformed?), Coherence(does the quatrain has consistent topic across four lines?), Meaningfulness(does the poem convey some certain messages?), Poeticness(does the poem have poetic features such as the poetic images?), Entirety(the reader's general impression on the poem).", "startOffset": 35, "endOffset": 51}, {"referenceID": 2, "context": "It's hard to obtain human-authored references for poem lines so we used the method in (He et al., 2012) to extract references automatically. We selected 4,400 quatrains from testing set (2,200 of them are 5-char and other 2,200 are 7-char) and extracted 20 references for each line in a quatrain(except the last line). We compared our system with the system in (He et al., 2012). Human Evaluation Since poetry is a kind of creative text, human evaluation is necessary. Referring to the three criteria in (Manurung, 2003), we designed five criteria: Fluency (are the lines fluent and wellformed?), Coherence(does the quatrain has consistent topic across four lines?), Meaningfulness(does the poem convey some certain messages?), Poeticness(does the poem have poetic features such as the poetic images?), Entirety(the reader's general impression on the poem). Each criterion was scored from 0 to 5. We compared four comparison systems. PG, our system. SMT, He (2012)'s system2.", "startOffset": 87, "endOffset": 965}, {"referenceID": 2, "context": "Because of the 6  We got lower BLEU\u20101 scores of SMT system than those reported in (He et al., 2012), because we limited training data, we used pairs in all positions to train SPB.", "startOffset": 82, "endOffset": 99}, {"referenceID": 6, "context": "In (Sutskever et al., 2014), they find reversing source sentences can improve the LSTM\u2019s performance and conclude that this preprocessing can introduce short term dependencies between the source and the target sentences which will make the optimization problem easier.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "Therefore He et al.(2012) use pairs in different positions to train corresponding position-sensitive models.", "startOffset": 10, "endOffset": 26}], "year": 2016, "abstractText": "We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve", "creator": "PScript5.dll Version 5.2.2"}}}