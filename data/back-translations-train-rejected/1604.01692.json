{"id": "1604.01692", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "An Ensemble Method to Produce High-Quality Word Embeddings", "abstract": "A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of $\\rho = .596$ on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.", "histories": [["v1", "Wed, 6 Apr 2016 16:58:35 GMT  (96kb,D)", "http://arxiv.org/abs/1604.01692v1", "12 pages, 3 figures"]], "COMMENTS": "12 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert speer", "joshua chin"], "accepted": false, "id": "1604.01692"}, "pdf": {"name": "1604.01692.pdf", "metadata": {"source": "CRF", "title": "An Ensemble Method to Produce High-Quality Word Embeddings", "authors": ["Robert Speer", "Joshua Chin"], "emails": ["rspeer@luminoso.com", "joshuarchin@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a purely reactionary project, capable of putting itself in a reactionary situation."}, {"heading": "1.1 Related Work", "text": "Agirre et al. (2009) notes that distributional similarity and structured knowledge can be combined into a benefit that exceeds what any individual would achieve on their own, especially by expanding the vocabulary. Their system uses a WordNet similarity measurement and uses distributional similarities to recognize words outside the vocabulary of WordNet. Levy et al. (2015) investigates modern methods of distributional similarity and experiments with them based on specific data while varying their parameters. They compare word2vec and GloVe, fine-tune their hyperparameters in ways that word2vec particularly improves, and then suggest a method based on the SVD of the Pointwise Mutual Information Matrix that surpasses both. We use Levy's results as a point of comparison here.AutoExtend (Rothe and Protectors, 2015) is a system with methods similar to ours: it extends word2vec embedding to cover all the words, not actually synchronizing the results with iWords."}, {"heading": "2 Knowledge Sources", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 ConceptNet and PPDB", "text": "ConceptNet (Speer and Havasi, 2012) is a semantic network of terms linked by marked relationships. Its terms are words or multiword phrases in a variety of natural languages. In continuity with previous work, these terms are often referred to as concepts. ConceptNet emerged as a machine-analyzed version of the early crowd-sourcing project called Open Mind Common Sense (OMCS) (Singh et al., 2002) and has been expanded to include several other data sources, both crowd-sourced and expert, by unifying their vocabularies into a single representation. ConceptNet now includes representations of WordNet (Miller et al., 1998), Wiktionary (Wiktionary, 2014) and JMDict (Breen, 2004), as well as data from \"games with a purpose\" in multiple languages (by Ahn et al., 2006; Kuo et al., 2009; Nakahara and Yamada, 2011)."}, {"heading": "2.2 word2vec and GloVe", "text": "The loss of vector representations of words according to their respective distribution semantics is only a matter of time before the vectors produce similarities in the way the words occur in common with other words. (The word2vec software also implements another distribution system called Skip-Grams with Negative Sampling (SGNS), popularly known as its software implementation. (The word2vec software also implements another representation, Continuous Bag-ofWords or CBOW, which is less commonly used for word similarity.) In SGNS, a neural network with a hidden layer is trained to recognize words that are likely to appear close together."}, {"heading": "3 Methods", "text": "Faruqui et al. (2015) introduced the \"retrofitting\" process, which adjusts dense matrices of embedding (such as GloVe output) to take into account external insights from a sparse semantic network; they tried different sources of external knowledge, and the most helpful for GloVe was PPDB. We found that using ConceptNet is more effective and that further marginal improvements in some evaluations could be achieved by combining ConceptNet and PPDB. Our goal is to create a 300-dimensional vector space that represents terms based on a combination of GloVe and word2vec's downloadable embedding and structured data from ConceptNet and PPDB. The resulting vector space enables information to be shared between these different representations, including words not included in the vocabulary of the original representations, including low-frequency words and even words that do not occur in English."}, {"heading": "3.1 Transforming and Aligning Vocabularies", "text": "We can only properly combine these resources if these string representations are comparable. (Pre-processing steps that use different resources include: tokenizing text to separate words from punctuation (which all inputs except GloVe 840B do), adding multiword phrases with underscores (ConceptNet and GloVe 42B), replacing multiple digits with the # character (word2vec only), removing a small list of stopwords from multiword phrases (ConceptNet and GloVe 42B), replacing multiple digits with the # character (word2vec only), and lemmatizing English words to their root form with a modification of WordNet's Morphy algorithm (ConceptNet only).We adapt a text preprocessing function from ConceptNet to apply a combination of all these processes."}, {"heading": "3.2 Feature Normalization", "text": "As Pennington et al. (2014) briefly mentioned, the L2 normalization of the columns (i.e. the 300 characteristics) of the GloVe matrix leads to a remarkable increase in performance. One effect of normalization is to increase the weight of the distinguishing characteristics and reduce the influence of noisy characteristics. Characteristics differ more in terms of cosinal similarity when they contain a few large and many small values. We find that the L1 normalization of GloVe works even better than the L2 normalization. L1 causes occasionally large values to have a smaller impact on the normalization than the L2 normalization. If a learning method such as GloVe has provided highly selective characteristics, the L1 normalization allows us to use them more effectively to measure similarity."}, {"heading": "3.3 Retrofitting", "text": "In fact, most of us are able to abide by the rules that they apply in practice."}, {"heading": "3.4 ConceptNet as an Association Matrix", "text": "In order to apply the advanced retrofitting method, we need to consider the data in ConceptNet as a sparse, symmetrical matrix of associations between terms. What ConceptNet offers is more complex because it combines terms with a variety of not necessarily symmetrical terms referred to as relations. Havasi et al. (2010) introduced a vector space embedding of ConceptNet, \"spectral association,\" which disregards the relation labels for the purpose of measuring the relativity of terms. Previous embedding of ConceptNet et al. (2008) preserved the relationships, but were largely suitable for direct similarity and inference, not relationality. Because most of the evaluation data for word similarity also evaluates relationality, unless there was a specific effort to separate them (Agirre et al., 2009), we delete the labels as speculative associations."}, {"heading": "3.5 Locally Linear Alignment", "text": "In order to use both word2vec and GloVe simultaneously, we need to match their partially overlapping vocabularies and merge their characteristics, which is easy to do on the terms shared between the two words, but we would rather not lose the other terms if we can learn more about these terms later from ConceptNet. Before merging characteristics, we need to calculate GloVe representations for terms represented in word2vec but not in GloVe, and vice versa. The way we do this is inspired by Zhao et al. (2015), which derives translations between languages of unknown phrases using a local-linear projection of known translations of similar phrases. Instead of known translations, we have the terms overlapping between word2vec and GloVe, using a non-overlapping term Udx, we calculate its vector value as the nearest vector to the weighted termectors of the termecosystems of their overlapping characteristics."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word-Similarity Datasets", "text": "We evaluate the performance of our model in identifying similar words with a variety of word similarity gold standards: \u2022 MEN-3000 (Bruni et al., 2014), crowd-sourced similarity judgments for 3000 word pairs. \u2022 Stanford's Rare Words (RW) dataset (Luong et al., 2013), crowd-sourced similarity judgments for 2034 word pairs, with a penchant for unusual words. \u2022 WordSim-353 (Finkelstein et al., 2001), a widespread corpus of similarity judgments for 353 word pairs. \u2022 RG-65 (Rubenstein and Goodenough, 1965), a classic corpus of similarity judgments for 65 word pairs, which was additionally translated into German. (Gurevych, 2005) and French (Joubarne and Inkpen, 2011)."}, {"heading": "4.2 Results", "text": "Table 1 shows the performance of the ensemble as different components of the ensemble are enabled. G and g indicate that the first embedding is from GloVe (840B and 42B, respectively), and W indicates that it is word2vec's SGNS embedding that was built by Google News. If both W and G are present, the embedding is combined as in Section 3.5. L1 indicates that the embedding columns L1 have been normalized; otherwise we used the existing scale of features. 7St indicates that the labels have been transformed and rows combined with the method of Section 3.1, which is a prerequisite for combining multiple data sources. CN and PP indicate that data from ConceptNet, PPDB or both have been added with advanced retrofitting. Note that the line labeled with g alone is a rating of GloVe 42B, which reflects the rating of Pennington al (2014)."}, {"heading": "4.3 Benefits of Lemmatization", "text": "Comparing the results of GloVe 42B with the results of 840B (Table 1), we find that GloVe 42B works better \"out of the box\" and contains 840B chaotic data, which causes problems especially in the evaluation of rare words. However, our strategy for standardizing and selecting the term labels of GloVe 840B, which combine their lines using the Zipf estimate, makes GloVe 42B and other published results more powerful, as seen in the series St, G, L1. We call this configuration \"Modified7 When GloVe is not L1-normalized, it is L2-normalized instead, following Pennington et al. (2014).GloVe\" and similar, our best configuration of word2vec is \"Modified word2vec.\""}, {"heading": "4.4 Comparisons to Other Published Results", "text": "In Table 3, we compare our results for the RW and MEN 3000 datasets with the best published results known to us. Levy et al. (2015) present results, including an SVD-based method with optimized hyperparameters. We also compare the original results of GloVe 42B and the best MEN3000 result from Faruqui et al. (2015). We use the full RW data, not our test set, so that we can directly compare with previous results. Levy's evaluation uses a version of WordSim353, which is split into separate sets for similarity and kinship. We estimate the overall results based on a weighted average of vowel shapes, not on the size of the split datasets. RW and MEN data are also presented in Figure 3, although Error Bars are better net systems based on Fisher's hyperinformation systems."}, {"heading": "4.5 Varying the System", "text": "In fact, the fact is that most of them will be able to play by the rules they have given themselves, and that they will be able to play by the rules they have given themselves."}, {"heading": "5 Conclusions", "text": "As Levy et al. (2015) found, decisions at the highest level about how a system should be used can significantly influence its performance. While Levy found settings of hyperparameters that made word2vec more powerful than GloVe, we found that GloVe can do better than tuned word2vec by pre-editing the words in GloVe by case-folding and lamatizing and reweighting the features using L1 normalization. We also showed that it is not necessary to use just one of word2vec or GloVe as a starting point. Instead, we can benefit from both by using a locally linear interpolation between the topics. We showed that ConceptNet is a useful source of structured knowledge that was not included in previous work on retooling distribution mantics with structured knowledge, especially if the conversion to our generalized technique benefits from restoring the original connections that can benefit from restoring outside the existing ones."}, {"heading": "5.1 Future Work", "text": "One aspect of our method that is clearly in need of improvement is the fact that we are ignoring the relationship labels in ConceptNet. There is valuable knowledge that we could take into account with a more nuanced extension of the retrofit, one that goes beyond the mere knowledge that certain words should be related to each other in order to treat them differently according to their kinship, as in the RESCAL representation (Nickel et al., 2011). This seems to be particularly important for antonyms that suggest that words are similar overall, but different in one key aspect, such as two ends of the same scale.Lemmatization is clearly a useful component of our word similarity representation, but is losing information. Representing morphological relationships as operations in vector space, as in Soricut and Och (2015), could produce a better representation of the similarities between word shapes. We believe that the diversity of data sources presented in ConceptNet has helped improve the system's assessment results by enhancing the knowledge."}, {"heading": "6 Reproducing These Results", "text": "We strive to make these results reproducible and reusable by others, and the code we used to create the results in this essay is available in the GitHub repository https: / / github.com / LuminosoInsight / conceptnet-vector-ensemble. We have marked this revision as submitted-20160406 so that this work can be reproduced as it is, even if we update the code later. The README repository of this repository also points to URLs where the computed results can be downloaded as a 1453348 x 600 matrix of NumPy-format embedding and a corresponding list of line markups."}, {"heading": "Acknowledgements", "text": "We thank Catherine Havasi for overseeing and reviewing this work, and Avril Kenney, Dennis Clark and Alice Kaanta for providing feedback on drafts of this paper. We thank the word2vec, GloVe and PPDB teams for opening their data so that new techniques can build on their results. We also thank our employees who have donated code and data to ConceptNet over the years, and the tens of thousands of pseudonymous contributors to Wiktionary, Open Mind Common Sense and related projects for their often unrecognized work in providing freely available lexicon knowledge."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "DBpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "JMDict: a Japanesemultilingual dictionary", "author": ["James Breen"], "venue": "In Proceedings of the Workshop on Multilingual Linguistic Ressources,", "citeRegEx": "Breen.,? \\Q2004\\E", "shortCiteRegEx": "Breen.", "year": 2004}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Towards a universal wordnet by learning from combined evidence", "author": ["De Melo", "Weikum2009] Gerard De Melo", "Gerhard Weikum"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "Melo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "JAsIs,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "Proceedings of NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population", "author": ["Ronald A Fisher"], "venue": null, "citeRegEx": "Fisher.,? \\Q1915\\E", "shortCiteRegEx": "Fisher.", "year": 1915}, {"title": "PPDB: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In HLT-NAACL,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Iryna Gurevych"], "venue": "In Natural Language Processing\u2013", "citeRegEx": "Gurevych.,? \\Q2005\\E", "shortCiteRegEx": "Gurevych.", "year": 2005}, {"title": "Automated color selection using semantic knowledge", "author": ["Robert Speer", "Justin Holmgren"], "venue": "In AAAI Fall Symposium: Commonsense Knowledge", "citeRegEx": "Havasi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Havasi et al\\.", "year": 2010}, {"title": "Comparison of semantic similarity for different languages using the Google N-gram corpus and second-order co-occurrence measures", "author": ["Joubarne", "Inkpen2011] Colette Joubarne", "Diana Inkpen"], "venue": "In Advances in Artificial Intelligence,", "citeRegEx": "Joubarne et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Joubarne et al\\.", "year": 2011}, {"title": "Communitybased game design: experiments on social games for commonsense data collection", "author": ["Kuo et al.2009] Yen-ling Kuo", "Jong-Chuan Lee", "Kaiyang Chiang", "Rex Wang", "Edward Shen", "Cheng-wei Chan", "Jane Yung-jen Hsu"], "venue": null, "citeRegEx": "Kuo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kuo et al\\.", "year": 2009}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "eulerAPE: Drawing area-proportional 3-venn diagrams using ellipses", "author": ["Micallef", "Rodgers2014] Luana Micallef", "Peter Rodgers"], "venue": "PLoS ONE,", "citeRegEx": "Micallef et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Micallef et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Open Mind Common Sense: Knowledge acquisition from the general public. In On the move to meaningful internet systems 2002: CoopIS", "author": ["Singh et al.2002] Push Singh", "Thomas Lin", "Erik T Mueller", "Grace Lim", "Travell Perkins", "Wan Li Zhu"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] Radu Soricut", "Franz Och"], "venue": "In Proc. NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Representing general relational knowledge in ConceptNet 5", "author": ["Speer", "Havasi2012] Robert Speer", "Catherine Havasi"], "venue": "In LREC,", "citeRegEx": "Speer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2012}, {"title": "AnalogySpace: Reducing the dimensionality of common sense knowledge", "author": ["Speer et al.2008] Robert Speer", "Catherine Havasi", "Henry Lieberman"], "venue": "In AAAI,", "citeRegEx": "Speer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2008}, {"title": "Verbosity: a game for collecting common-sense facts", "author": ["von Ahn et al.2006] Luis von Ahn", "Mihir Kedia", "Manuel Blum"], "venue": "In Proceedings of the SIGCHI conference on Human Factors in computing systems,", "citeRegEx": "Ahn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2006}, {"title": "Learning translation models from monolingual continuous representations", "author": ["Zhao et al.2015] Kai Zhao", "Hany Hassan", "Michael Auli"], "venue": "In Proceedings of NAACL", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Human behavior and the principle of least effort: an introduction to human ecology", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf.,? \\Q1949\\E", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 19, "context": "We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al.", "startOffset": 73, "endOffset": 98}, {"referenceID": 17, "context": ", 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 9, "context": ", 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary.", "startOffset": 106, "endOffset": 133}, {"referenceID": 15, "context": "596 on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.", "startOffset": 35, "endOffset": 55}, {"referenceID": 5, "context": "This kind of vector space has been used in applications such as search, topic detection, and text classification, dating back to the introduction of latent semantic analysis (Deerwester et al., 1990).", "startOffset": 174, "endOffset": 199}, {"referenceID": 5, "context": "This kind of vector space has been used in applications such as search, topic detection, and text classification, dating back to the introduction of latent semantic analysis (Deerwester et al., 1990). In recent years, there has been a surge of interest in natural-language embeddings, as machine-learning techniques such as Mikolov et al. (2013)\u2019s word2vec and Pennington et al.", "startOffset": 175, "endOffset": 346}, {"referenceID": 5, "context": "This kind of vector space has been used in applications such as search, topic detection, and text classification, dating back to the introduction of latent semantic analysis (Deerwester et al., 1990). In recent years, there has been a surge of interest in natural-language embeddings, as machine-learning techniques such as Mikolov et al. (2013)\u2019s word2vec and Pennington et al. (2014)\u2019s GloVe have begun to show dramatic improvements.", "startOffset": 175, "endOffset": 386}, {"referenceID": 5, "context": "This kind of vector space has been used in applications such as search, topic detection, and text classification, dating back to the introduction of latent semantic analysis (Deerwester et al., 1990). In recent years, there has been a surge of interest in natural-language embeddings, as machine-learning techniques such as Mikolov et al. (2013)\u2019s word2vec and Pennington et al. (2014)\u2019s GloVe have begun to show dramatic improvements. Word embeddings are often suggested as an initialization for more complex methods, such as the sentence encodings of Kiros et al. (2015).", "startOffset": 175, "endOffset": 573}, {"referenceID": 0, "context": "Some methods and evaluations (Agirre et al., 2009) distinguish word similarity from word relatedness.", "startOffset": 29, "endOffset": 50}, {"referenceID": 22, "context": "ConceptNet originated as a machine-parsed version of the early crowd-sourcing project called Open Mind Common Sense (OMCS) (Singh et al., 2002), and has expanded to include several other data sources, both crowd-sourced and expert-created, by unifying their vocabularies into a single representation.", "startOffset": 123, "endOffset": 143}, {"referenceID": 2, "context": ", 1998), Wiktionary (Wiktionary, 2014), and JMDict (Breen, 2004), as well as data from \u201cgames with a purpose\u201d in multiple languages (von Ahn et al.", "startOffset": 51, "endOffset": 64}, {"referenceID": 13, "context": ", 1998), Wiktionary (Wiktionary, 2014), and JMDict (Breen, 2004), as well as data from \u201cgames with a purpose\u201d in multiple languages (von Ahn et al., 2006; Kuo et al., 2009; Nakahara and Yamada, 2011).", "startOffset": 132, "endOffset": 199}, {"referenceID": 1, "context": "We choose not to include ConceptNet\u2019s alignment to DBPedia (Auer et al., 2007) here, as DBPedia focuses on relations between specific named entities, which do not help with general word similarity.", "startOffset": 59, "endOffset": 78}, {"referenceID": 9, "context": "PPDB (Ganitkevitch et al., 2013) is another resource that is useful for learning about word similarity, providing different information from ConceptNet.", "startOffset": 5, "endOffset": 32}, {"referenceID": 6, "context": "PPDB is used as an external knowledge source by Faruqui et al. (2015), so we have evaluated the effect of adding it to our ensemble as well.", "startOffset": 48, "endOffset": 70}, {"referenceID": 17, "context": "2 Given different goals \u2013 such as achieving a high score on Mikolov et al. (2013)\u2019s analogy evaluation that tests for implicit relations such as \u201cA is the CEO of company B\u201d \u2013 including an appropriate representation of DBPedia would of course be helpful.", "startOffset": 60, "endOffset": 82}, {"referenceID": 19, "context": "GloVe (Pennington et al., 2014) is an unsupervised learning algorithm that learns a set of word embeddings such that the dot product of two words\u2019 embeddings is approximately equal to the logarithm of their co-occurrence count.", "startOffset": 6, "endOffset": 31}, {"referenceID": 18, "context": "GloVe is presented by Pennington et al. (2014) as performing better than word2vec on wordsimilarity tasks, but Levy et al.", "startOffset": 22, "endOffset": 47}, {"referenceID": 14, "context": "(2014) as performing better than word2vec on wordsimilarity tasks, but Levy et al. (2015) finds that word2vec performs better with an optimized setting of hyperparameters than GloVe does, when retrained with a particular corpus.", "startOffset": 71, "endOffset": 90}, {"referenceID": 14, "context": "As Levy et al. (2015) notes, \u201c[.", "startOffset": 3, "endOffset": 22}, {"referenceID": 28, "context": "distributed according to Zipf\u2019s law (Zipf, 1949): the nth token in rank order has a frequency proportional to 1/n.", "startOffset": 36, "endOffset": 48}, {"referenceID": 15, "context": "For example, the Rare Words (RW) dataset (Luong et al., 2013) tends to encounter terms that are poorly represented or out-of-vocabulary in most word embeddings.", "startOffset": 41, "endOffset": 61}, {"referenceID": 19, "context": "As briefly mentioned by Pennington et al. (2014), L2 normalization of the columns (that is, the 300 features) of the GloVe matrix provides a notable increase in performance.", "startOffset": 24, "endOffset": 49}, {"referenceID": 6, "context": "Retrofitting (Faruqui et al., 2015) is a process of combining existing word vectors with a semantic lexicon.", "startOffset": 13, "endOffset": 35}, {"referenceID": 0, "context": "Because most evaluation data for word similarity is also evaluating relatedness, unless there has been a specific effort to separate them (Agirre et al., 2009), we erase the labels as in spectral association.", "startOffset": 138, "endOffset": 159}, {"referenceID": 27, "context": "The way we do this is inspired by Zhao et al. (2015), who infer translations between languages of unknown phrases using a locally-linear projection of known translations of similar phrases.", "startOffset": 34, "endOffset": 53}, {"referenceID": 3, "context": "\u2022 MEN-3000 (Bruni et al., 2014), crowd-sourced similarity judgments for 3000 word pairs.", "startOffset": 11, "endOffset": 31}, {"referenceID": 15, "context": "\u2022 The Stanford Rare Words (RW) dataset (Luong et al., 2013), crowd-sourced similarity judgments for 2034 word pairs, with a bias toward uncommon words.", "startOffset": 39, "endOffset": 59}, {"referenceID": 7, "context": "\u2022 WordSim-353 (Finkelstein et al., 2001), a widely-used corpus of similarity judgments for 353 word pairs.", "startOffset": 14, "endOffset": 40}, {"referenceID": 10, "context": "\u2022 RG-65 (Rubenstein and Goodenough, 1965), a classic corpus of similarity judgments for 65 word pairs, which has additionally been translated into German (Gurevych, 2005) and French (Joubarne and Inkpen, 2011).", "startOffset": 154, "endOffset": 170}, {"referenceID": 19, "context": "Note that the row labeled with g alone is simply an evaluation of GloVe 42B that reproduces the evaluation of Pennington et al. (2014). Our results here match the published results to within .", "startOffset": 110, "endOffset": 135}, {"referenceID": 19, "context": "7 When GloVe is not L1-normalized, it is L2-normalized instead, following Pennington et al. (2014). Method RW [all] MEN WS", "startOffset": 74, "endOffset": 99}, {"referenceID": 14, "context": "Levy et al. (2015) present results including an SVD-based method that scores \u03c1 = .", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "of skip-grams with negative sampling (SGNS), originally introduced by Mikolov et al. (2013), with optimized hyperparameters.", "startOffset": 70, "endOffset": 92}, {"referenceID": 6, "context": "We also compare to the original results from GloVe 42B, and the best MEN3000 result from Faruqui et al. (2015). We use the complete RW data, not our test set, so that we can compare directly to previous results.", "startOffset": 89, "endOffset": 111}, {"referenceID": 8, "context": "Error bars indicate 95% confidence intervals based on the Fisher transformation of \u03c1 (Fisher, 1915), supposing that each evaluation is randomly sampled from a hypothetical larger data set.", "startOffset": 85, "endOffset": 99}, {"referenceID": 6, "context": "Table 4 shows the performance of these systems on gold standards that have been translated to other languages, in comparison to the multilingual results published by Faruqui et al. (2015). Our system performs well in non-English languages even though the vocabularies of word2vec and GloVe are assumed to be English only.", "startOffset": 166, "endOffset": 188}, {"referenceID": 14, "context": "As Levy et al. (2015) found, high-level choices about how to use a system can significantly affect its performance.", "startOffset": 3, "endOffset": 22}, {"referenceID": 18, "context": "There is valuable knowledge there that we might be able to take into account with a more sophisticated extension of retrofitting, one that goes beyond simply knowing that particular words should be related, to handling them differently based on how they are related, as in the RESCAL representation (Nickel et al., 2011).", "startOffset": 299, "endOffset": 320}], "year": 2016, "abstractText": "A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of \u03c1 = .596 on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.", "creator": "LaTeX with hyperref package"}}}