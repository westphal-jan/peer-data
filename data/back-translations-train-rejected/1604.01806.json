{"id": "1604.01806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Generalising the Discriminative Restricted Boltzmann Machine", "abstract": "We present a novel theoretical result that generalises the Discriminative Restricted Boltzmann Machine (DRBM). While originally the DRBM was defined assuming the {0, 1}-Bernoulli distribution in each of its hidden units, this result makes it possible to derive cost functions for variants of the DRBM that utilise other distributions, including some that are often encountered in the literature. This is illustrated with the Binomial and {-1, +1}-Bernoulli distributions here. We evaluate these two DRBM variants and compare them with the original one on three benchmark datasets, namely the MNIST and USPS digit classification datasets, and the 20 Newsgroups document classification dataset. Results show that each of the three compared models outperforms the remaining two in one of the three datasets, thus indicating that the proposed theoretical generalisation of the DRBM may be valuable in practice.", "histories": [["v1", "Wed, 6 Apr 2016 21:01:35 GMT  (31kb)", "http://arxiv.org/abs/1604.01806v1", "Submitted to ECML 2016 conference track"]], "COMMENTS": "Submitted to ECML 2016 conference track", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["srikanth cherla", "son n tran", "tillman weyde", "artur d'avila garcez"], "accepted": false, "id": "1604.01806"}, "pdf": {"name": "1604.01806.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["srikanth.cherla.1@city.ac.uk", "son.tran.1@city.ac.uk", "t.e.weyde@city.ac.uk", "a.garcez@city.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.01 806v 1 [cs.L G] 6A prKeywords: restricted Boltzmann machine, discriminatory learning, hidden activation function"}, {"heading": "1 Introduction", "text": "In the last ten years, it has gained popularity in many areas, including in the discriminatory Boltzmann Machine (DRBM). As the name suggests, DRBM is a classifying actor that discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates, discriminates."}, {"heading": "2 Related Work", "text": "The subject of activation functions has received attention from time to time in the literature. Previous work has either introduced a new type of activation in a new context [18] [20] [14], have a comparative evaluation function [4], or have reassessed the importance of an existing activation function in a new context [3]. In the case of feedback neural networks, while [4] they are closed in favor of the logistic sigmoid activation function, they have a disadvantage in their use with deep feedback forward networks and propose the hyperbolic tangent function as a suitable alternative."}, {"heading": "3 Restricted Boltzmann Machine", "text": "The RBM is fully characterized by the parameters W, b and c. Its bipartite structure is illustrated in Figure 1.RBM. The RBM is a special case of energetics - there are a number of visible units, which are composed of their visible and hidden layers. The two layers are fully connected, but there are no connections between any hidden units, or any two visible units. In addition, the units of each layer are connected to a hidden unit, the value of which is always 1. The edge between the visible unit and the hidden unit is associated with a weightwij. All these weights are collectively represented as a weight matrix W. The weights of the connections between visible units and the biase unit are contained in a visible biasector b. Likewise, for the hidden units there is a hidden biasector c, Rnh. The RBM is fully characterized by the parameters W, c and c."}, {"heading": "4 The Discriminative Restricted Boltzmann Machine", "text": "The generative RBM described above models the common probability P (v) of the set of binary variables v. For prediction, one is interested in a conditional distribution of the form P (y | x). It was shown in [9] how discriminatory learning can be carried out in the RBM, which makes it possible to use it as a standalone classifier, starting from a subset of its visible units that must be entered x, and the remaining set of categorical units y that represent the class-conditioned probabilities P (y | x), as illustrated in Figure 1. The weight matrix W can be represented as two matrices R, Rni \u00b7 nh and U, Rnc \u00b7 nh, where ni is the input dimension and nc is the number of classes and nv = ni + nc, x x x. Likewise, the visible bias vector b, Rnv, is also divided into a series of two vectors - one."}, {"heading": "5 Generalising the DRBM", "text": "This section describes a novel generalization of the cost function of the DRBM [9]. This facilitates the formulation of similar cost functions for variants of the model with other distributions in their hidden units, as they often occur in the literature. This is illustrated here first by the {\u2212 1, + 1} Bernoulli distribution and then by the binomial distribution."}, {"heading": "5.1 Generalising the Conditional Probability", "text": "We start with the term for the conditional distribution P (y | x), as derived in [9], which is given by P (y | x) = \u2211 h (x, y, h) (12), where y is the only hot encoding of a class name y, and \u2212 log \u2211 h exp (\u2212 E (x, y, h)) is the free energy that is free from the RBM. We consider the term that contains the sum of h in (12): exp (Efree (x, y))) = \u2212 hexp (\u2212 E (x, h) is the original energy free from the RBM."}, {"heading": "5.2 Extensions to other Hidden Layer Distributions", "text": "We first use the result in (15) to derive the expression for the conditional probability P (y | x) in the original DRBM (9), followed by its extension, first to the {\u2212 1, + 1} -Bernoulli distribution (here referred to as the bipolar DRBM) and then to the binomial distribution (the binomial DRBM). Section 6 compares the performance of the DRBM with these different activations.DRBM: The {0, 1} -Bernoulli DRBM corresponds to the model originally introduced in [9]. In this case, any hidden unit hj can be either a 0 or a 1, i.e.sk = {0, 1} This reduces P (y | x) in (15) toPber (y) = exp (by).j exp (by).j exp exp (by) exp (by exp)."}, {"heading": "6 Experiments", "text": "We rated the bipolar and binomial DRBMs on three benchmark datasets for machine learning. These are two handwritten digits of racognition datasets - USPS [5] and MNIST [10], and a document classification dataset - 20 newsgroups [8]. Before we talk about the results of the experiments performed on each of these datasets, we describe the evaluation methodology used for all three and the evaluation metrics common to the test set. Methodology A grid search was performed to determine the best set of model hyperparameters. The procedure first included evaluating each of the trained models on a validation set and then selecting the best of these evaluation methodologies to be evaluated on the test set. If a dataset did not contain predefined validation sets, it was created using a subset of the training set hyperparameters. The initial learning rate was evaluated with 1DRY-Wet units for stochastic derivatives {0.0001}, 0.0001, 0.0001."}, {"heading": "6.1 MNIST Handwritten Digit Recognition", "text": "The MNIST dataset [10] consists of optical characters of handwritten digits. Each digit is a 28 x 28 pixel grayscale image (or a vector x [0, 1] 784).Each pixel of the image corresponds to a floating point value in the range [0, 1] after normalization of an integer value in the range [0, 255].The dataset is divided into a single split of predetermined training, validation, and test folding containing 50,000 images, 10,000 images, and 10,000 images.Table 1 lists the classification performance on this dataset of the three DRBM variants derived from the result in (15) above; the first line of the table corresponds to the DRBM introduced in [9].We did not perform a grid search, in the case of this one model, and used only the specified hyperparameter setting Difference 1 to reproduce its result as a significant percentage of that 0.2 was found to be significant in three units."}, {"heading": "6.2 USPS Handwritten Digit Recognition", "text": "Each digit is a 16 x 16 pixel grayscale image (or a vector x [0, 1] 256), and each pixel of the image corresponds to a floating point value in the range [0, 255] after normalization from an integer value in the range [0, 1]. The data set is divided into a single split of predetermined training, validation, and test folds containing 7, 291 images, 1, 458 images, and 2, 007 images. Table 3 lists the classification performance on this data set of the three DRBM variants derived from the result above in (15). It was determined that the binomial DRBM (from nbins = 8) has the best classification accuracy, followed by the bipolar DRBM, and then the DRBM. The number of hidden units used by each of these models is reversed to their average accuracy."}, {"heading": "6.3 20 Newsgroups Document Classification", "text": "The 20 newsgroups dataset [8] is a collection of approximately 20,000 newsgroup documents evenly divided into 20 different categories. A version of the dataset is used here, in which the training and test sets contain documents collected at different times. The goal is to predict the correct category of a document published after a certain date, using a model developed before the date. We used the 5,000 most common words for the binary input characteristics of the models. This pre-processing follows the example [9], since it was the second data used to evaluate the DRBM there. We tried to apply the evaluation method there as accurately as possible, in order to be comparable with their results, despite the unavailability of the exact validation set. Therefore, a validation set of the same number of samples was created. Table 5 lists the classification performance on this dataset of the three DRBM variants derived from the result in (15)."}, {"heading": "7 Conclusions and Future Work", "text": "This work introduced a novel theoretical result that makes it possible to generalize the hidden activations of discriminatory RBM (DRBM). This result was first used to reproduce the derivation of the cost function of DRBM and additionally to derive the derivation from two new variants of DRBM (bipolar DRBM and binomial DRBM).The three models thus derived were evaluated using three benchmark machine learning datasets - MNIST, USPS and 20 newsgroups. It was found that each of the three variants of DRBM exceeded the rest of the DRBM datasets, confirming that generalizations of DRBM datasets can be useful in practice. In the experiments in Section 6, it was found that the DRBM achieved the best classification accuracy of DRBM datasets, the bipolar DRBM datasets and the binomial DRBM datasets on the DRB20 datasets."}], "references": [{"title": "Unsupervised Learning of Distributions on Binary Vectors using Two Layer Networks", "author": ["Y. Freund", "D. Haussler"], "venue": "Advances in Neural Information Processing Systems. pp. 912\u2013919", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1992}, {"title": "Understanding the Difficulty of Training Deep Feedforward Neural Networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 249\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 315\u2013323", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The Power of Approximation: A Comparison of Activation Functions", "author": ["B.D. Gupta", "G. Schnitger"], "venue": "Advances in Neural Information Processing Systems. pp. 615\u2013622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Replicated Softmax: An Undirected Topic Model", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems. pp. 1607\u20131614", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks", "author": ["B. Karlik", "A.V. Olgac"], "venue": "International Journal of Artificial Intelligence and Expert Systems 1(4), 111\u2013122", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Newsweeder: Learning to Filter Netnews", "author": ["K. Lang"], "venue": "Proceedings of the 12th international conference on machine learning. pp. 331\u2013339", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Classification using discriminative restricted Boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "International Conference on Machine Learning. pp. 536\u2013543. ACM Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "A Tutorial on EnergyBased Learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F. Huang"], "venue": "Predicting Structured Data", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "International Conference on Machine Learning. pp. 609\u2013616. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Acoustic Modeling using Deep Belief Networks", "author": ["A.R. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing 20(1), 14\u201322", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning. pp. 791\u2013798. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["P. Smolensky"], "venue": "1. chap. Information Processing in Dynamical Systems: Foundations of Harmony Theory, pp. 194\u2013281. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning Multilevel Distributed Representations for High-Dimensional Sequences", "author": ["I. Sutskever", "G. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 548\u2013555", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Rate-Coded Restricted Boltzmann Machines for Face Recognition", "author": ["Y.W. Teh", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems pp. 908\u2013914", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient", "author": ["T. Tieleman"], "venue": "International Conference on Machine Learning. pp. 1064\u20131071. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Exponential Family Harmoniums with an Application to Information Retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems. pp. 1481\u20131488", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "In the case of feedforward neural networks, while [4] concluded in favour of the Logistic Sigmoid activation function, recent work in [2] found a drawback in its use with deep feedforward networks and suggested the Hyperbolic Tangent function as a more suitable alternative to it.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "In the case of feedforward neural networks, while [4] concluded in favour of the Logistic Sigmoid activation function, recent work in [2] found a drawback in its use with deep feedforward networks and suggested the Hyperbolic Tangent function as a more suitable alternative to it.", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Even more recently, [3] highlighted the biological plausibility of the Rectified Linear activation function and its potential to outperform both the aforementioned activations without even the need to carry out unsupervised pre-training.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "An early variant of this model [1] known as the Influence Combination Machine, while still modelling binary data, employed units of values {\u22121,+1}.", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "In [20], an extension of the standard RBM to model real-valued variables in its visible layers was proposed with the aid of the Gaussian activation function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "With the same goal of modelling non-binary variables, the rate-coded RBM [18] was introduced in which both the visible and hidden layers could model integer-values by grouping the activations", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "This idea was extended in [14] which introduced Rectified Linear activations for the hidden layer of the RBM.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "In the context of topic modelling, the replicated softmax RBM [6] was introduced which models the categorical distribution in its visible layer.", "startOffset": 62, "endOffset": 65}, {"referenceID": 14, "context": "3 Restricted Boltzmann Machine The Restricted Boltzmann Machine (RBM) [16] is an undirected bipartite graphical model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "The RBM is a special case of the Boltzmann Machine \u2014 an energy-based model [11] \u2014 which gives the joint probability of every possible pair of visible and hidden vectors via an energy function E, according to the equation P (v,h) = 1 Z e (1) where the \u201cpartition function\u201d, Z, is given by summing over all possible pairs of visible and hidden vectors Z = \u2211", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "To avoid the difficulty in computing the above gradient, an efficiently computable and widely adopted approximation of the gradient was proposed in the Contrastive Divergence method [19].", "startOffset": 182, "endOffset": 186}, {"referenceID": 7, "context": "It has been demonstrated in [9] how discriminative learning can be carried out in the RBM, thus making it feasible to use it as a standalone classifier.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "5 Generalising the DRBM This section describes a novel generalisation of the cost function of the DRBM [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "1 Generalising the Conditional Probability We begin with the expression for the conditional distribution P (y|x), as derived in [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "The result in (15) generalises the conditional probability of the DRBM first introduced in [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "2 Extensions to other Hidden Layer Distributions We first use the result in (15) to derive the expression for the conditional probability P (y|x) in the original DRBM [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "DRBM: The {0, 1}-Bernoulli DRBM corresponds to the model originally introduced in [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "which is identical to the result obtained in [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Bipolar DRBM: A straightforward adaptation to the DRBM involves replacing its hidden layer states by {\u22121,+1} as previously done in [1] in the case of the RBM.", "startOffset": 131, "endOffset": 134}, {"referenceID": 16, "context": "Binomial DRBM: It was demonstrated in [18] how groups of N (where N is a positive integer greater than 1) stochastic units of the standard RBM can be combined in order to approximate discrete-valued functions in its visible layer and hidden layers to increase its representational power.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "These are two handwritten digit racognition datasets \u2014 USPS [5] and MNIST [10], and one document classification dataset \u2014 20 Newsgroups [8].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "These are two handwritten digit racognition datasets \u2014 USPS [5] and MNIST [10], and one document classification dataset \u2014 20 Newsgroups [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": "1 MNIST Handwritten Digit Recognition The MNIST dataset [10] consists of optical characters of handwritten digits.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Each digit is a 28 \u00d7 28 pixel gray-scale image (or a vector x \u2208 [0, 1]).", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255].", "startOffset": 81, "endOffset": 87}, {"referenceID": 7, "context": "The first row of the table corresponds to the DRBM introduced in [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "81% reported in [9].", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Each digit is a 16 \u00d7 16 pixel gray-scale image (or a vector x \u2208 [0, 1]).", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255].", "startOffset": 81, "endOffset": 87}, {"referenceID": 6, "context": "3 20 Newsgroups Document Classification The 20 Newsgroups dataset [8] is a collection of approximately 20, 000 newsgroup documents, partitioned evenly across 20 different categories.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "This preprocessing follows the example of [9], as it was the second data used to evaluate the DRBM there.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "6% reported in [9].", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "This idea is inspired by [14], where the Rate-coded RBM [18] (analogous to the Binomial DRBM here) is extended to derive an RBM with Rectified Linear units by increasing the number of replicas of a single binary unit to infinity.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "This idea is inspired by [14], where the Rate-coded RBM [18] (analogous to the Binomial DRBM here) is extended to derive an RBM with Rectified Linear units by increasing the number of replicas of a single binary unit to infinity.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "We present a novel theoretical result that generalises the Discriminative Restricted Boltzmann Machine (DRBM). While originally the DRBM was defined assuming the {0, 1}-Bernoulli distribution in each of its hidden units, this result makes it possible to derive cost functions for variants of the DRBM that utilise other distributions, including some that are often encountered in the literature. This is illustrated with the Binomial and {\u22121,+1}-Bernoulli distributions here. We evaluate these two DRBM variants and compare them with the original one on three benchmark datasets, namely the MNIST and USPS digit classification datasets, and the 20 Newsgroups document classification dataset. Results show that each of the three compared models outperforms the remaining two in one of the three datasets, thus indicating that the proposed theoretical generalisation of the DRBM may be valuable in practice.", "creator": "LaTeX with hyperref package"}}}