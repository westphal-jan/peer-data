{"id": "1604.01904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Neural Headline Generation with Sentence-wise Optimization", "abstract": "Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks.", "histories": [["v1", "Thu, 7 Apr 2016 07:47:11 GMT  (237kb,D)", "http://arxiv.org/abs/1604.01904v1", null], ["v2", "Sun, 9 Oct 2016 07:16:24 GMT  (182kb,D)", "http://arxiv.org/abs/1604.01904v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ayana", "shiqi shen", "yu zhao", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1604.01904"}, "pdf": {"name": "1604.01904.pdf", "metadata": {"source": "CRF", "title": "Neural Headline Generation with Minimum Risk Training", "authors": ["Ayana", "Shiqi Shen", "Zhiyuan Liu", "Maosong Sun"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In the neeisrmtlrVnre\u00fc\u00fc\u00fceegnln rf\u00fc ide nree\u00fcGsrteeeirsn nvo nlrf\u00fc ide nree\u00fcGt ni ende nlrlhtee\u00fccnlrhee\u00fccnlhsrtee\u00fccnlrhe\u00fccS ni ende nlrhe\u00fce\u00fccnlrgVnlrhee\u00fccnlrgVnlrrhe\u00fce ni rde nlrlrrrrrrrrf\u00fc-eaeaeaeaeaeaeaeaeaeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeedeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "2 Neural Headline Generation Model", "text": "We first introduce the Neural Headline Generation (NHG) in this section, then the Minimum Risk Training (MRI) for NHG in the next section. We formalize the task of the Headline Generation as follows: Name the input document x as a word string x = {x1, \u00b7 \u00b7, xM} with M words in which each word xi comes from a fixed word vocabulary V of size | V |. The headline generator aims to use x as input, and generates a short heading y = {y1, \u00b7, yN} with the length N < M to maximize the conditional probability of y given x, i.e., arg max Pr (y | x). We follow the Markov assumption to generate headings yj in order, and the log conditional word yj in order, and the log conditional word yj in order, Nltyy probability can be formalized further than log: Pr | x (x)."}, {"heading": "2.1 Encoder", "text": "In this process, we use the Gated Recurrent Unit (GRU) to build a bidirectional RNN as an encoder of NHG."}, {"heading": "Gated Recurrent Unit", "text": "The RNN in NHG is implemented with GRU, originally proposed for NMT [3]. As shown in Fig. 2, GRU can adaptively capture the dependencies of the input sequence by introducing the updategate ui and introducing the reset gate ri, which determines whether earlier hidden states are ignored. If the reset gate is close to 1, the updategate controls how much of previous hidden states is passed on. In GRU, hi and h-i = hidden states and candidate activation are generated. GRU calculates the i-th hidden state as follows: ri = \u03c3 (WrExi + Urhi \u2212 1) (4) ui = \u03c3 (WuExi + Uuhi \u2212 1) (5) h-i = tanh (WhExi + Uh (ri hi \u2212 1))) (6) hi = ui-hidden states (Hi \u2212 1), 1 \u2212 uten (1) (hi \u2212 1) \u2212 h \u2212 h."}, {"heading": "Bidirectional RNN", "text": "Conventional RNNs typically deal with the text sequence from beginning to end and form the hidden state of each word only taking into account its preceding words. It has been confirmed that the hidden state should also take into account its following words. Therefore, we use bidirectional RNN (BRNN) [16] to learn hidden states using both preceding and subsequent words. As shown in Figure 1, BRNN processes the input document both forward and backward with two separate hidden layers calculated with GRUs, obtaining the forward hidden states (\u2212 \u2192 h 1,.., \u2212 hM) and the backward hidden states (\u2190 \u2212 h,.,. \u2212 hM). For each position i, we simply link their two forward and backward states to the final hidden state: hi = \u2212 \u2192 h i,."}, {"heading": "2.2 Decoder", "text": "The decoder calculates the j-th headline word as follows: rj = \u03c3 (WrEyj \u2212 1 + Ursj \u2212 1 + Crcj) (9) uj = \u03c3 (WuEyj \u2212 1 + Uusj \u2212 1 + Cucj) (10) s \u0433j = tanh (WhEyj \u2212 1 + Uh (rj sj \u2212 1) + Chcj) (11) sj = uj sj \u2212 1 + (1 \u2212 uj) s \u041aj (12) The notations are identical to those of the GRUs in the encoder and Cr, Cu, Ch'RH \u00d7 2H also weigh matrices. We simply insert s0 = uj \u2212 1 + (Wf \u2190 \u2212 h 1) with Wf-RH \u00d7 H, where the context vector cj weights the attention of the input document to the given word Nihji =."}, {"heading": "3 Minimum Risk Training for NHG", "text": "The model parameters of NHG can be estimated with large document-headline pairs. We call the training set D = {(x (1), y (1),.., (x (T), y (T))}. Before introducing minimal risk training, we start with the conventional optimization technique, the maximum probability estimation, for NHG."}, {"heading": "3.1 Maximum Likelihood Estimation", "text": "The Maximum Likelihood Estimation (MLE) finds optimized parameters that can increase the log probability of generating headlines via training set D: LMLE (\u03b8) = \u2211 (x, y) \u0192Dlog Pr (y | x; \u03b8), (15) where Pr (y | x; \u03b8) is defined in Equation (1). Therefore, the model is essentially trained to maximize Pr (yj | x, y < j; \u03b8) step by step, which inevitably looses global information. On the other hand, y < j are authentic words from the standard heading during training. However, y < j are predicted during the test and may not be correct, leading to error spread and inaccurate headline generation. Minimum risk training, on the other hand, can improve these problems."}, {"heading": "3.2 Minimum Risk Training", "text": "Minimal risk training (MRI) aims to minimize the expected loss, i.e. the risk, of the training data. In the light of document x, we define the loss of a generated headline y as the semantic distance between y and the standard y, which is called \"y.\" MRI defines the objective function as follows: LMRI (\"X,\" \"Y,\" \"Y,\" \"Y,\" \"Y\"). (16) Here, EY (\"X,\" \"Y\") indicates the expectation of the specified Y (\"X,\" \"Y\"). Thus, the objective function of MRI can be formalized further as: LMRI (\"X,\" \"Y\") the objective function of MRI = \"DEY\" (\"Y,\" \"Y,\" \"Y,\" \"Y\") the objective evaluation as: LMRI (\"X,\" \"Y\") the objective function of MRI (\"Y,\" \"Y\"), \"the objective function of the ability (\" X, \"Y,\" \"Y,\" \"(\"), \"Y,\" (\"Y,\")."}, {"heading": "3.3 ROUGE", "text": "ROUGE automatically measures the quality of the summary by comparing computer-generated summaries with standard man-made summaries. ROUGE is the usual evaluation metric in the Document Understanding Conference (DUC), a large-scale summary sponsored by NIST [11]. The basic idea of ROUGE is to count the number of overlapping units such as overlapping n-grams, word strings and word pairs between computer-generated summaries and standard summaries. In this project, we are looking at two types of ROUGE: ROUGE-N and ROUGE-L. ROUGE-N counts n-grams and ROUGE-L counts the longest common sub-sequence. Suppose y \u2032 is the summary created and y is the standard summary. ROUGE-N is defined as follows: ROUGE-N = total gram and L-gram and L-gram long is the standard summary."}, {"heading": "4 Experiments", "text": "We conduct experiments with English and Chinese data sets and compare the performance of our model with several base systems. In this section we present data sets, base systems and experiment results in detail."}, {"heading": "4.1 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "English Dataset", "text": "For English, we use the English Gigaword Fifth Edition [13] for training models. It is one of the largest static corpus of English news, consisting of nearly 10 million news articles from 7 news agencies with a total of more than 4 billion words. To compare our work with [15], we use the same pre-processing techniques as theirs. We select the first sentence of each news article and pair it with its corresponding headline as an article-headline pair. To avoid noise in articles and headlines that might affect performance, we filter out bylines, insignificant edit marks, and question marks. The training set contains about 4 million article-headline pairs after the filter step. We use the DUC-2003 evaluation data set as a validation set to determine the hyperparameters that give the best performance in a minimum risk training. DUC-2003 contains 624 short articles, each of which four human-based reference references corresponds to a minimum risk set."}, {"heading": "Chinese Dataset", "text": "We are also conducting experiments with a Chinese data set LCSTS [8], which consists of pairs of articles and headlines extracted from Sina Weibo 2, a Chinese social media medium that allows users to spread information and share it with their friends. A typical news article posted on Weibo is limited to 140 Chinese characters, and the corresponding headline is usually placed in square brackets at the top of the news article.1The data set can be obtained from http: / / duc.nist. gov / with agreements.2The Sina Weibo website is http: / / weibo.com / The LCSTS consists of three parts. Part I consists of about 2.4 million article headline pairs. Part II and Part III contain pairs of articles with human labeled results, indicating the affiliation between the article and its headline. The higher the value, the more related they are to each other. Part II consists of 10, 66 letters, and Part III pairs of letters, with only three pairs of Chinese headlines."}, {"heading": "4.2 Baseline Systems", "text": "In this progress, we compare our model to the following basics of the English headline generation task: \u2022 TOPIARY [21] is the winning system of the DUC2004 Task1. This system uses a linguistic sentence compression method and, at the same time, unattended topic recognition, and performs well. \u2022 MOSES + [15] generates headlines based on a widely used phrase-based MOSES machine translation system [10]. The MOSES + system also takes two steps to improve the quality of generated headlines. One step is to enlarge the phrase table and the other is to tune models with MERT. \u2022 ABS and ABS + [15] are attention-based neural modeling to generate short summaries in one sentence. The difference between the two systems is that ABS + performs an extractive tuning procedure to revise model parameters based on ABS."}, {"heading": "4.3 Implementation Details", "text": "In MRI, we initialize model parameters using the optimized parameters we learned from NHG with MLE. Specifically, the size of S (x; \u03b8) is important for each x: If the size is too small, the sample will not suffice and affect performance; if the size is too large, the learning time will increase accordingly. On this scale, we set the size to 100 to achieve a trade-off between effectiveness and efficiency. We select most of the hyperparameters we use in our systems, i.e. those with the highest probabilities of the generation generated by the current NHG model in S (x; \u03b8). The hyperparameter in Equation (18) is 5 \u00d7 10 \u2212 3.Table 1 shows most of the hyperparameters we use in our systems, and they remain the same in both MRI trainings, MRI trainings and MRI trainings, and we have not coordinated them."}, {"heading": "4.4 Experiment Results and Analysis", "text": "We use ROUGE [11], introduced in Section 3.3, to measure the performance of different models. For each model, we give ROUGE-1, ROUGE-2 and ROUGE-L values in percent."}, {"heading": "Evaluation Results on English and Chinese", "text": "Table 2 shows the evaluation results of the English headline generation on DUC-2004. TOPIARY, MOSES +, ABS and ABS + are baselines introduced in Section 4.2. Because we perform training and evaluation on the same data set and therefore simply use the evaluation results reported in [15], NHG + MLE and NHG + MRT are neural headline generation models that we have described in Section 2, and are learning with MLE and MRT accordingly. From Table 2, we can conclude that (1) NHG learned with MLE performs competitively with the best baseline systems, especially with ROUGE-2 and ROUGE-L, and is slightly inferior to ABS + on ROUGE-1. This indicates that NHG is effective for headline generation. (2) NHG learned with MRT performs significantly worse than NHG and is more consistent with MG than with three HG."}, {"heading": "Effectiveness of Evaluation Metrics", "text": "We are interested in the effectiveness of using different MRI evaluation metrics, i.e. ROUGE-1, ROUGE-2 and ROUGE-L, for the performance of the headline generation. Tables 4 and 5 show the results of MRI with different evaluation metrics for the English validation and the test kit. Tables 4 and 5 show that: (1) All NHG + MRI models with three evaluation metrics consistently perform better than NHG + MLE. This indicates that MRI technology is robust when it comes to evaluating metric variations. (2) It is straightforward that NHG + MRI models with ROUGE-1 and ROUGE-2 perform slightly better in evaluation with the corresponding metrics."}, {"heading": "Case Study", "text": "To demonstrate the effectiveness of MRI, we present several sample results for comparison, as shown in Table 6. Using these examples, we can determine the following: (1) NHG with MRI is generally able to capture the important part of a document. In Article 1, the main topic is Honduras, which is prepared for a hurricane. NHG + MRT can successfully find the topic and generate a headline about Honduras, but NHG + MLE has failed. In Article 2, there is a similar situation. (2) If both systems capture the right topic, NHG + MRT can capture and generate a more informative headline. As shown in Article 3, NHG + MLE has overlooked the information that reported on it, but NHG + MRT has delivered succinctly with \"UNO\": (3) NHG + MLE usually suffers from the generation of repeated words or phrases as we present in Article 4. NHG + MLE repeats the phrase \"minimum Asian headlines\" and appears to be inconsistent with a HG twice."}, {"heading": "5 Related Work", "text": "Headline generation is a well-defined task that is standardized in DUC-2003 and DUC-2004. Different approaches have been proposed for headline generation: rule-based, statistical and neuronal-based, introduced in detail as follows. Rule-based models create a headline for a news article using handmade and linguistically motivated rules to guide the selection of a potential headline. Hedge-Trimmer [4] is a representative example of this approach, which creates a headline by removing components from the parse tree of the first sentence until it reaches a certain length limit. Systems in this approach are consistent with human intuition and are easy to understand. However, it is unrealistic and impossible to induce each rule due to the complexity of human languages. Statistical-based methods use large-scale training data to learn correlations between words in headlines and those in articles. MRT applies binary models for content selection and surface realisation."}, {"heading": "6 Conclusion and Future Work", "text": "In this progress, we are building an end-to-end model of neural headline generation that does not require complex linguistic analysis and is fully data-driven. We are applying minimal risk training to model learning that is capable of incorporating specific evaluation metrics. Evaluation results show significant and consistent improvements in NHG with MRI compared to English and Chinese data sets compared to other baselines, including NHG with MLE.There are many outstanding issues that need to be explored as future work: (1) In addition to article-headline pairs, there is also rich plaintext data that is not included in NHG training. We are investigating the likelihood of integrating these plaintegrations to improve NHG for semi-monitored learning. (2) We will examine the hybrid approach to integrate NHG with other successful approaches to generate headlines such as sentence compression models. (3) Both input and output of NHG for semi-monitored learning will be performed at the phrase level, and more complex summary models will be performed at the phrase level."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O Mittal", "Michael J Witbrock"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Hedge trimmer: A parse-and-trim approach to headline generation", "author": ["Bonnie Dorr", "David Zajic", "Richard Schwartz"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Sentence compression by deletion with lstms", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "English gigaword fifth edition, june", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1512.02433,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Minimum risk annealing for training log-linear models", "author": ["David A Smith", "Jason Eisner"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Keyword extraction and headline generation using novel word features", "author": ["Songhua Xu", "Shaohui Yang", "Francis Chi-Moon Lau"], "venue": "In Proceedings of AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of HLT- NAACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "When the generated summary is required to be a single compact sentence, we name the summarization task as headline generation [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Moreover, the attention mechanism [1] is introduced in NHG, which learns a soft alignment over input document to generate more accurate headline [15].", "startOffset": 34, "endOffset": 37}, {"referenceID": 14, "context": "Moreover, the attention mechanism [1] is introduced in NHG, which learns a soft alignment over input document to generate more accurate headline [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "The RNN in NHG is implemented using GRU, which is originally proposed for NMT [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 15, "context": "Hence, we apply bidirectional RNN (BRNN) [16] to learn hidden states using both preceding and following words.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "(18) in which is a hyper-parameter [12] that controls the smoothness of the objective function.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "As we know, the most widely adopted evaluation metric for document summarization is ROUGE [11] (Recall-Oriented Understudy of Gisting Evaluation).", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "ROUGE is the common evaluation metric in Document Understanding Conference (DUC), a largescale summarization evaluation sponsored by NIST [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "For English, we utilize the English Gigaword Fifth Edition [13] for training models.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "In order to compare our work with [15], we take the same preprocessing techniques as theirs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "We also implement experiments on a Chinese dataset LCSTS [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 20, "context": "\u2022 TOPIARY [21] is the winner system of DUC2004 Task1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 MOSES+ [15] generate headlines based on a widelyused phrase-based machine translation system MOSES [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "\u2022 MOSES+ [15] generate headlines based on a widelyused phrase-based machine translation system MOSES [10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "\u2022 ABS and ABS+ [15] are attention-based neural models to generate short summaries given a sentence.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "When generating headlines, we utilize a [UNK] replacement technique following the idea in [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "We utilize ROUGE [11] that introduced in Section 3.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Since we carry out training and evaluation on the same dataset, and hence we simply use the evaluation results reported in [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Hedge Trimmer [4] is a representative example of this approach which creates a headline by removing constituents from the parse tree of the first sentence until it reaches a specific length limit.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "For example, [2] applies statistical models for content selection and surface realization to produce headlines.", "startOffset": 13, "endOffset": 16}, {"referenceID": 20, "context": "The best system on DUC-2004, TOPIARY [21] combines both linguistic and statistical information to generate headlines.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "For example, [20] utilizes Wikipedia to extract features like word inlinks, outlinks and categories to select keywords from an article and constitute a headline.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "[15] proposes an attention-based model to generate headlines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] proposes a recurrent neural network with long short term memory (LSTM) [7] for headline generation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[5] proposes a recurrent neural network with long short term memory (LSTM) [7] for headline generation.", "startOffset": 75, "endOffset": 78}], "year": 2017, "abstractText": "Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks.", "creator": "LaTeX with hyperref package"}}}