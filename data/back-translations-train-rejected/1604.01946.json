{"id": "1604.01946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Optimizing Performance of Recurrent Neural Networks on GPUs", "abstract": "As recurrent neural networks become larger and deeper, training times for single networks are rising into weeks or even months. As such there is a significant incentive to improve the performance and scalability of these networks. While GPUs have become the hardware of choice for training and deploying recurrent models, the implementations employed often make use of only basic optimizations for these architectures. In this article we demonstrate that by exposing parallelism between operations within the network, an order of magnitude speedup across a range of network sizes can be achieved over a naive implementation. We describe three stages of optimization that have been incorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing a single cell, secondly a single layer, and thirdly the entire network.", "histories": [["v1", "Thu, 7 Apr 2016 10:31:01 GMT  (28kb)", "http://arxiv.org/abs/1604.01946v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jeremy appleyard", "tomas kocisky", "phil blunsom"], "accepted": false, "id": "1604.01946"}, "pdf": {"name": "1604.01946.pdf", "metadata": {"source": "CRF", "title": "Optimizing Performance of Recurrent Neural Networks on GPUs", "authors": ["Jeremy Appleyard"], "emails": ["jappleyard@nvidia.com", "tomas.kocisky@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.01 946v 1 [cs.L G] 7A prA's recurrent neural networks get bigger and deeper, training times for individual networks increase in weeks or even months. Therefore, there is a significant incentive to improve the performance and scalability of these networks. While GPUs have become the hardware of choice for training and the use of recurring models, the implementations used often use only basic optimizations for these architectures. In this article, we show that by detecting parallelism between operations within the network, an order of magnitude of acceleration can be achieved across a bandwidth of network sizes, which is naively implemented. We describe three stages of optimization that have been integrated into the fifth version of the cuDNN of NVIDIA: first, the optimization of a single cell; second, a single layer; and third, the entire network."}, {"heading": "1 Introduction", "text": "Recurrent neural networks have become a standard tool for modeling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative image models [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9]. A key factor in these recent successes has been the availability of powerful graphics processing units (GPUs), which are particularly effective for accelerating the large matrix products at the heart of recursive networks. However, as recursive networks [10] deepen and the structuring of their core units [11, 12] it is becoming increasingly difficult to make optimum use of the computing capabilities of the latest generation of GPUs. There are several studies that optimize the implementation of regional networks for GPUs, especially in the case of revolutionary neural networks [13, 14]. While GPUs are already widely used to optimize NRs, NRs, NRs, NRs, and NRs, there are [17, NRs, NRs, NRs, and NRs.]"}, {"heading": "2 Implementation", "text": "In this section we will look at the performance of a forward and backward propagation through an LSTM network [11]. This is a standard four-gate LSTM network without peephole connections. The equations for calculating the output in the forward trajectory of this LSTM are listed below: for l in layers for j in the iterations i (l, j) '= A _ i (l, j) h (l, j)' = A _ h (l, j) * h (l, j) for pointwiseOp in pointwiseOpsdo pointwiseOpListing 1: pseudocode that shows the starting point for optimizing the forward traject. ii = \u03c3 (Wixi + Riht \u2212 1 + bi) ft = \u03c3 (Wfxt + Rfht \u2212 1 + bf) for pointwiseOpsdo pointwiseOpListing 1: Pseudocode that shows the starting point for optimizing the forward. ii 't' t = ft (Wfxt + Rfht \u2212 1 + Rfht \u2212 1 + Rf) 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t 't' t."}, {"heading": "2.1 Naive implementation", "text": "There are many ways to naively implement a single propagation step of a recursive neural network. As a starting point, we consider an implementation where each kernel (i.e. matrix multiplication, sigmoid, point-by-point addition, etc.) is implemented as a separate kernel. While the GPU performs the operations within each kernel in parallel, the cores are executed one after the other. Forwarding performance of this implementation is poor, reaching about 0.4 TFLOPS in a hidden state 512 and minibatch 64 test case, less than 10% of the peak performance of the hardware (about 5.8 TFLOPS when executed on base clocks).1A widespread optimization is matrix operations that share the same input into a single major matrix operation."}, {"heading": "2.2 Single Cell", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Streamed matrix operations", "text": "The ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref"}, {"heading": "2.2.2 Fusion of point-wise operations", "text": "Although parallels with point-by-point operations are inherently inefficient, they have been found to be inefficient for two reasons: firstly, because starting a kernel involves costs to the GPU; secondly, because it is inefficient to move the output of a point-by-point operation far into the GPU main memory before re-reading it a few moments later. By nature, point-by-point operations are independent, and as such it is possible to merge all point-by-point operations into a larger kernel."}, {"heading": "2.3 Single Layer", "text": "A single recursive layer consists of many cells, the recursive input of each depends on the output of the previous layer. Input from the previous layer may not have such a dependency, and it is often possible to concatenate the inputs for several time steps, thus generating a larger, more efficient matrix multiplication. (The exact amount of steps concatenated is not trivial: more steps lead to a more efficient matrix multiplication, but fewer steps reduce the time that a recursive operation can potentially be waited for. (The exact amount of steps depends not only on the hyperparameters, but also on the target hardware. Another operation that is possible when looking at a layer as a whole is the reordering of the layout of the weight matrices. Since the same weight matrices are used repeatedly over the course of a shift, the costs of the reordering are typically small compared to the cost of operating the matrices."}, {"heading": "2.4 Multiple Layers", "text": "In this situation, it is possible to exploit the parallelism between recurring layers: The completion of a recurring cell not only resolves the dependence on the next iteration of the current layer, but also on the current iteration of the next layer. This allows multiple layers to be calculated in parallel, significantly increasing the workload of the GPU at a given time."}, {"heading": "2.4.1 Scheduling", "text": "Since starting the GPU takes a small but not insignificant amount of time, it is important to take into account the order in which the cores are booted to the GPU. For example, if GPU resources are available, it is almost always preferable to start a kernel with all its dependencies resolved, rather than a kernel that may wait some time for its dependencies to be deleted. In this way, as much parallelism as possible can be uncovered. To do this, we chose a simple schema rule according to which the next work is planned, in which the fewest edges are traversed before reaching the \"first\" recurring cell. Looking at a recurring network as a 2D cell grid results in a diagonal \"wave\" of starts that spread from the first cell."}, {"heading": "2.5 Performance", "text": "The impact of each of the optimizations on the run-up of a 1000 step, four layers LSTM network with hidden state size 512 and an input minibatch of 64 is shown in Table 1. For this network we achieve a ~ 11x acceleration via a completely naive implementation, and a ~ 6x acceleration via an implementation using the standard GEMM grouping optimization. Given the sufficient recurring steps, there are three variables that significantly affect the performance of an RNN implementation. These are: hidden state size, minibatch size and number of layers. Fixing the number of layers to four, Figure 1 shows the impact of each of these optimizations on a wide range of hidden state sizes and minibatch sizes. In some cases, increasing the number of layers from one to four doubles the throughput."}, {"heading": "2.6 Weight Update", "text": "The above optimizations apply only to propagation steps. By completing the propagation of the weight before the start of the weight update, the weight update becomes very efficient. A single large matrix multiplication can be used to update each matrix without dependencies, and this will usually reach almost the peak performance. Updating the bias weights is very cost-effective compared to updating the matrices."}, {"heading": "3 cuDNN", "text": "The optimizations described in Section 2 were implemented in the fifth version of NVIDIA's cuDNN library for single-gate RNNNs, GRUs, and LSTMs. The performance of this implementation is shown in Figure 2. For this implementation, it was possible to interact with cuBLAS at a lower level than is available through the current interface, and to tailor the heuristics used to determine how cuBLAS works to this application. In particular, cuBLAS will often choose a more parallel but less resource-efficient path when it finds that the GPU is likely to be underutilized by a call. As we know the expected level of parallelism at a higher level than cuBLAS, overcoming this behavior has sometimes led to more favorable, resource-efficient paths in cases of highly streamed parallelism, and it is hoped that an interface to enable this type of manual coordination will be built into future BLAS."}, {"heading": "4 Conclusions", "text": "We have presented a method that allows recurring neural networks to run on GPUs with high efficiency, and while previous implementations have achieved good acceleration on GPUs [19, 20], to our knowledge none have achieved the level of performance we have achieved with the methods described above, the primary strategy has been to show as much parallelism with the GPU as possible in order to maximize the use of hardware resources.The methods are particularly efficient when working on smaller, deeper, recurring networks where individual layers have less inherent parallelism, and one feature of the problem we do not exploit for the benefit of performance is the re-use of parameters between recurring iterations. It is conceivable that these parameters are stored at a lower level of the GPU code and re-used from iteration to iteration, which could greatly improve performance in bandwidth-based systems."}], "references": [{"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Ivo Danihelka Nal Kalchbrenner", "Alex Graves"], "venue": "In Proceedings of the International Conference on Learning Representations, ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence", "author": ["Alex Graves"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Andrew Lavin"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Micha\u00ebl Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan C. Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos", "Erich Elsen", "Jesse Engel", "Linxi Fan", "Christopher Fougner", "Tony Han", "Awni Y. Hannun", "Billy Jun", "Patrick LeGresley", "Libby Lin", "Sharan Narang", "Andrew Y. Ng", "Sherjil Ozair", "Ryan Prenger", "Jonathan Raiman", "Sanjeev Satheesh", "David Seetapun", "Shubho Sengupta", "Yi Wang", "Zhiqian Wang", "Chong Wang", "Bo Xiao", "Dani Yogatama", "Jun Zhan", "Zhenyao Zhu"], "venue": "CoRR, abs/1512.02595,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Introducing currennt: The Munich open-source CUDA recurrent neural network toolkit", "author": ["Felix Weninger"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Torch-rnn. https://github.com/jcjohnson/torch-rnn, 2016", "author": ["Justin Johnson"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 209, "endOffset": 215}, {"referenceID": 1, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 209, "endOffset": 215}, {"referenceID": 2, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 3, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 4, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 5, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 6, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 7, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 8, "context": "Recurrent neural networks have become a standard tool for modelling sequential dependencies in discrete time series and have underpinned many recent advances in deep learning, from generative models of images [1, 2] to natural language processing [3, 4, 5, 6, 7, 8, 9].", "startOffset": 247, "endOffset": 268}, {"referenceID": 9, "context": "However as recurrent networks become deeper [10] and their core units more structured [11, 12], it has become increasingly difficult to maximally utilise the computational capacity of the latest generation of GPUs.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "However as recurrent networks become deeper [10] and their core units more structured [11, 12], it has become increasingly difficult to maximally utilise the computational capacity of the latest generation of GPUs.", "startOffset": 86, "endOffset": 94}, {"referenceID": 11, "context": "However as recurrent networks become deeper [10] and their core units more structured [11, 12], it has become increasingly difficult to maximally utilise the computational capacity of the latest generation of GPUs.", "startOffset": 86, "endOffset": 94}, {"referenceID": 12, "context": "There have been several studies optimizing the implementation of neural networks for GPUs, particularly in the case of convolutional neural networks [13, 14].", "startOffset": 149, "endOffset": 157}, {"referenceID": 13, "context": "There have been several studies optimizing the implementation of neural networks for GPUs, particularly in the case of convolutional neural networks [13, 14].", "startOffset": 149, "endOffset": 157}, {"referenceID": 14, "context": "While GPUs are already widely used to compute RNNs [15, 16, 17, 18], there has been less work on the optimization of RNN runtime.", "startOffset": 51, "endOffset": 67}, {"referenceID": 15, "context": "While GPUs are already widely used to compute RNNs [15, 16, 17, 18], there has been less work on the optimization of RNN runtime.", "startOffset": 51, "endOffset": 67}, {"referenceID": 16, "context": "While GPUs are already widely used to compute RNNs [15, 16, 17, 18], there has been less work on the optimization of RNN runtime.", "startOffset": 51, "endOffset": 67}, {"referenceID": 10, "context": "In this section we will consider the performance of a forward and backward propagation passes through an LSTM network[11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "While previous implementations exist achieving good acceleration on GPUs [19, 20], to our knowledge none achieve the levels of performance we achieve using the methods discussed above.", "startOffset": 73, "endOffset": 81}], "year": 2016, "abstractText": "As recurrent neural networks become larger and deeper, training times for single networks are rising into weeks or even months. As such there is a significant incentive to improve the performance and scalability of these networks. While GPUs have become the hardware of choice for training and deploying recurrent models, the implementations employed often make use of only basic optimizations for these architectures. In this article we demonstrate that by exposing parallelism between operations within the network, an order of magnitude speedup across a range of network sizes can be achieved over a naive implementation. We describe three stages of optimization that have been incorporated into the fifth release of NVIDIA\u2019s cuDNN: firstly optimizing a single cell, secondly a single layer, and thirdly the entire network.", "creator": "gnuplot 5.0 patchlevel 3"}}}