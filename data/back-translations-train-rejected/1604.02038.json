{"id": "1604.02038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "abstract": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation.", "histories": [["v1", "Thu, 7 Apr 2016 15:29:45 GMT  (54kb,D)", "http://arxiv.org/abs/1604.02038v1", null], ["v2", "Fri, 8 Apr 2016 05:45:44 GMT  (54kb,D)", "http://arxiv.org/abs/1604.02038v2", "The submitted version was done in Feb.2016. Still in improvement"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["fei tian", "bin gao", "di he", "tie-yan liu"], "accepted": false, "id": "1604.02038"}, "pdf": {"name": "1604.02038.pdf", "metadata": {"source": "CRF", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "authors": ["Fei Tian", "Bin Gao", "Tie-Yan Liu"], "emails": ["tianfei@mail.ustc.edu.cn", "bingao@microsoft.com", "tyliu@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, we will be able to go in search of a solution that meets the needs of the people, \"he said in an interview with the German Press Agency."}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to play by the rules they have given themselves, and that they will be able to play by the rules that they have played by the rules, and that they will be able to play by the rules that they have played by the rules."}, {"heading": "3 Sentence Level Recurrent Topic Model", "text": "In this section, we describe the proposed Sentence Level Recurrent Topic Model (SLRTM). First, we list three important design factors in SLRTM as follows: \u2022 SLRTM assumes the adoption of a sentence, a topic, as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: All words within the same sentence share the same theme. This assumption guarantees the coherence of the topic within a sentence and allows topic2 sentence. \u2022 To model extensive dependencies between words, SLRTM uses RNN (specifically LSTM) with word embedding vectors as input. The purpose is to use word embedding to improve the semantics of words as it is represented by the previous neural network-based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015] Each RTM topic is represented by the entire RTM."}, {"heading": "3.1 The generative process", "text": "Suppose we have K-themes contained in the dictionary W, and M-documents D = {d1, \u00b7 \u00b7 \u00b7, dM}. For each document, there is a K-dimensional Dirichlet predistribution to you (\u03b1) for the mixing weights of each document. These notations can be used to describe the generative process for document di as follows: 1. Example of the multinomial parameter of you (\u03b1); 2. For the jth sentence of the document di sij = (y1, \u00b7, yTij), j {1, \u00b7, Ni}, where yht \u00b2 W is the tth word for sij: (a) Draw out the subject kij of this sentence."}, {"heading": "3.2 Stochastic Variational Inference and Learning", "text": "Since the calculation of the actual trailing variables in Equation (3) is not comprehensible, we assume the mean variable inference to approximate them. In particular, we use the multinomic distribution parameters for document di as a whole. (1) The variational lower limit of the data probability [Lead et al., 2003] can be called: L (D) the variational parameters for document di as a whole. (1) The variable lower limit of the data probability [Lead et al., 2003] can be called: L (D) the variable distribution parameters for document di as a whole. (D) The variable distribution parameters for document i = 1 Ni = 1 {Eq [logP (sij | kij | kij]] the variational lower limit of the data probability [Lead et al., 2003] can be written as: L (D) the variable distribution parameters for document di as a whole."}, {"heading": "4 Experiments", "text": "In this section we report on our experimental results. Our experiments consist of two parts: (1) quantitative experiments, including a generative task for document evaluation and a task for document classification, on two sets of data; (2) qualitative inspection, including the examination of the records generated under each topic to test whether SLRTM performs well in the topic2 typesetting task."}, {"heading": "4.1 Quantitative Results", "text": "We compare SLRTM with several state-of-the-art theme models on two tasks: generative document evaluation and document classification. The first task is to examine the generational capability of the models, while the second is to represent the representational capability of the models. We base our experiments on two benchmark datasets: \u2022 20Newsgroup, which contains 18,845 emails centered on 20 different thematic groups such as religion, politics and sports. Originally, the dataset is divided into 11,314 training documents and 7,531 test documents. \u2022 Wiki10 + [Zubiaga, 2012] 3, the web-docu2http: / / qwone.com / jason / 20Newsgroups / 20news-bydate.tar.gz 3http: / www.zubiaga.org / datasets / Wiki10 + / datasets / Wikipedia, each of which is associated with several tags such as philosophy, software and music."}, {"heading": "Experimental Setting", "text": "For SLRTM, we implemented it in C + + using eigen6 and Intel MKL. In fairness, we set the word Embedding Size, Topic Embedding Size and LSTM Hidden Layer Size to 128, 128 and 600, respectively. In the experiment, we tested the performance of SLRTM and baselines in terms of the different number of topics K, i.e. K = 128, 256. During initialization (values of BA (0) and BA (0)), the LSTM weight matrices were initialized as orthogonal matrices, the word / theme embedding was randomly sampled from the uniform distribution (\u2212 0.015, 0.015) and refined through the training process, with the results of the LSTM weight matrices set to 0.5 each. The mini-stack size in algorithm 1 was specified as L = 5, and we performed the E-step of the algorithm for optimization in parallel with each of both clips being used for a final conjunction."}, {"heading": "Generative Document Evaluation", "text": "We measure the performance of various topic models based on the perplexity per word on the test set, which is defined as 4http: / / www.nltk.org / api / nltk.tokenize.html 5http: / / gibbslda.sourceforge.net / 6http: / / eigen.tuxfamily.org / perp (D) = exp {\u2212 \u2211 Mi = 1 logP (di) \u2211 M i = 1N \u2032 i}, where N \u2032 i is the number of words in Document di. Experimental results are summarized in Table 1. Based on the table, we have the following discussions: \u2022 Our proposed SLRTM consistently outperforms the base models by significant margins and demonstrates its outstanding ability to model the generative process of documents. In fact, as tested in our further verifications, the perplexity of SLRTM corresponds to that of SLRTM language models, with a small gap of about 100 (higher perplexity) on RTM models, we rate the RTM models within both RTM models significantly higher than the probability of SGMM equals."}, {"heading": "Document Classification", "text": "In this experiment, we fed document vectors (e.g. the \u03b3 values in SLRTM) learned from different subject models to monitored classifiers to compare their display performance. For 20Newsgroup, we used the multi-level logistic regression classifier and used accuracy as an evaluation criterion. Since each document could be associated with several labels (tags), we used for Wiki10 + the logistic regression for each label and the classification result is measured by the Micro F1 Score [Lewis et al., 2004]. For both sets of data, we use 10% of the original training set for validation and the rest for training. All classification results are shown in Table 2. It is clear from the table that SLRTM is the best model among each setting on both datasets. Furthermore, we can find that embedding based methods (Doc-NADE, GMNTM and SLRTM networks) is a more effective document representation than using any RTM on a different document representation model (whereas RTS generates a different force from ADM)."}, {"heading": "4.2 Qualitative Results", "text": "In this subsection, we will show the ability of SLRTM to generate reasonable and comprehensible sentences in the face of specific topics. In the experiment, we trained a larger SLRTM with 128 topics on a randomly sampled 100k Wikipedia documents in 20107 with an average of 275 words per document. The dictionary consists of about 50k common words, including common punctuation marks, with uppercase letters converted to lowercase letters. The size of the word embedding, topic embedding and RNN hidden layer are set to 512, 1024 and 1024, respectable. We used two different mechanisms in sentence generation. The first mechanism is random sampling of new word embedding, topic embedding and RNN hidden layer. The second is dynamic programming based on beam searching [Vinyals et al., 2015] which attempts to generate sentences by globally maximized probability."}, {"heading": "Conclusion", "text": "In this paper, we have proposed a novel topic model called Sentence Level Recurrent Topic Model (SLRTM), which uses recursive neural networks to model the sequential dependence of words and theme coherence within a sentence, demonstrating superior performance in both predictive document modeling and document classification. In addition, topic2satz enables many real-world tasks such as personalized text conversations (STC) to benefit. In the future, we plan to integrate SLRTM into RNN-based STC systems [Shang et al., 2015] to make the dialogue more topic-sensitive."}], "references": [{"title": "The Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "the Journal of machine Learning research", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["David M Blei", "Thomas L Griffiths", "Michael I Jordan", "Joshua B Tenenbaum"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Blei et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A novel neural topic model and its supervised extension", "author": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart V Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the Conference", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "author": ["Rajarshi Das", "Manzil Zaheer", "Chris Dyer. Gaussian lda for topic models with word embeddings"], "venue": "pages 795\u2013 804, July", "citeRegEx": "Das et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223\u20131231,", "citeRegEx": "Dean et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequential latent dirichlet allocation: Discover underlying topic structures within a document", "author": ["Du et al", "2010] Lan Du", "Wray Buntine", "Huidong Jin"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 22nd international conference on World Wide Web", "author": ["Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle. Using topic models for twitter hashtag recommendation"], "venue": "pages 593\u2013596,", "citeRegEx": "Godin et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in neural information processing systems", "author": ["Thomas L Griffiths", "Mark Steyvers", "David M Blei", "Joshua B Tenenbaum. Integrating topics", "syntax"], "venue": "pages 537\u2013544,", "citeRegEx": "Griffiths et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In International Conference on Artificial Intelligence and Statistics", "author": ["Amit Gruber", "Yair Weiss", "Michal Rosen-Zvi. Hidden topic markov models"], "venue": "pages 163\u2013170,", "citeRegEx": "Gruber et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1607\u20131614,", "citeRegEx": "Hinton and Salakhutdinov. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In advances in neural information processing systems", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei. Online learning for latent dirichlet allocation"], "venue": "pages 856\u2013864,", "citeRegEx": "Hoffman et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley. Stochastic variational inference"], "venue": "14(1):1303\u20131347,", "citeRegEx": "Hoffman et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval", "author": ["Thomas Hofmann. Probabilistic latent semantic indexing"], "venue": "pages 50\u201357. ACM,", "citeRegEx": "Hofmann. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Discourse processes", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham. An introduction to latent semantic analysis"], "venue": "25(2-3):259\u2013284,", "citeRegEx": "Landauer et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Hugo Larochelle", "Stanislas Lauly. A neural autoregressive topic model"], "venue": "pages 2708\u2013 2716,", "citeRegEx": "Larochelle and Lauly. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "author": ["Quoc Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rcv1: A new benchmark collection", "author": ["Lewis et al", "2004] David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "In Advances in neural information processing systems", "author": ["Jon D Mcauliffe", "David M Blei. Supervised topic models"], "venue": "pages 121\u2013128,", "citeRegEx": "Mcauliffe and Blei. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Topic sentiment mixture: modeling facets and opinions in weblogs", "author": ["Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai"], "venue": "Proceedings of the 16th international conference on World Wide Web, pages 171\u2013180. ACM,", "citeRegEx": "Mei et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In 11th Annual Conference of the International Speech Communication Association", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur. Recurrent neural network based language model"], "venue": "2010, pages 1045\u20131048,", "citeRegEx": "Mikolov et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "ICWSM 2010", "author": ["Daniel Ramage", "Susan Dumais", "Dan Liebling. Characterizing microblogs with topic models. In Proc"], "venue": "American Association for Artificial Intelligence, May", "citeRegEx": "Ramage et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li. Neural responding machine for short-text conversation"], "venue": "pages 1577\u20131586, July", "citeRegEx": "Shang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "Vinyals et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic modeling: beyond bag-of-words", "author": ["Hanna M Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 977\u2013984. ACM,", "citeRegEx": "Wallach. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "author": ["Hongning Wang", "Duo Zhang", "ChengXiang Zhai. Structural topic model for latent topical structure analysis"], "venue": "pages 1526\u20131535,", "citeRegEx": "Wang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Ordering-sensitive and semantic-aware topic modeling", "author": ["Min Yang", "Tianyi Cui", "Wenting Tu"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhancing navigation on wikipedia with social tags", "author": ["Arkaitz Zubiaga"], "venue": "arXiv preprint arXiv:1202.5469,", "citeRegEx": "Zubiaga. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 16, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 1, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 22, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 14, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 23, "context": "Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification [Mei et al., 2007], social network analysis [Ramage et al.", "startOffset": 185, "endOffset": 203}, {"referenceID": 26, "context": ", 2007], social network analysis [Ramage et al., 2010; Mei et al., 2007], and recommendation systems [Godin et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 23, "context": ", 2007], social network analysis [Ramage et al., 2010; Mei et al., 2007], and recommendation systems [Godin et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 9, "context": ", 2007], and recommendation systems [Godin et al., 2013].", "startOffset": 36, "endOffset": 56}, {"referenceID": 30, "context": "Just list one for illustration [Wallach, 2006]: the department chair couches offers and the chair department offers couches have very different topics, although they have exactly the same bag of words.", "startOffset": 31, "endOffset": 46}, {"referenceID": 11, "context": "For example, several sentence level topic models [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011] tackle the topic coherence problem by assuming all the words in a sentence to share the same topic (i.", "startOffset": 49, "endOffset": 106}, {"referenceID": 31, "context": "For example, several sentence level topic models [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011] tackle the topic coherence problem by assuming all the words in a sentence to share the same topic (i.", "startOffset": 49, "endOffset": 106}, {"referenceID": 32, "context": "For another example, in [Yang et al., 2015], the embedding based neural language model [Bengio et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 25, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 20, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 24, "context": "We use Recurrent Neural Network (RNN) [Mikolov et al., 2010], such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) network [Cho et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 13, "context": ", 2010], such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) network [Cho et al.", "startOffset": 47, "endOffset": 81}, {"referenceID": 29, "context": "With the proposed SLRTM, we can not only model the document generation process more accurately, but also construct new natural sentences that are coherent with a given topic (we call it topic2sentence, similar to image2sentece[Vinyals et al., 2015]).", "startOffset": 226, "endOffset": 248}, {"referenceID": 28, "context": "For example, it can serve as the basis of personalized short text conversation system [Shang et al., 2015; Serban et al., 2015], in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 27, "context": "For example, it can serve as the basis of personalized short text conversation system [Shang et al., 2015; Serban et al., 2015], in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 1, "context": "One of the most representative topic models is Latent Dirichlet Allocation [Blei et al., 2003], in which every word in a document has its topic drawn from document level topic weights.", "startOffset": 75, "endOffset": 94}, {"referenceID": 2, "context": "Several variants of LDA have been developed such as hierarchical topic models [Blei et al., 2004] and supervised topic models [Mcauliffe and Blei, 2008].", "startOffset": 78, "endOffset": 97}, {"referenceID": 22, "context": ", 2004] and supervised topic models [Mcauliffe and Blei, 2008].", "startOffset": 36, "endOffset": 62}, {"referenceID": 12, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 19, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 3, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 5, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 10, "context": "For example, in [Griffiths et al., 2004], both semantic (i.", "startOffset": 16, "endOffset": 40}, {"referenceID": 11, "context": "After that, a hidden Markov transition model for topics was proposed [Gruber et al., 2007], in which all the words in a sentence were regarded as having the same topic.", "startOffset": 69, "endOffset": 90}, {"referenceID": 31, "context": "Such a one sentence, one topic assumption was also used by some other works, including [Du et al., 2010; Wang et al., 2011].", "startOffset": 87, "endOffset": 123}, {"referenceID": 32, "context": "To address this problem, the authors of [Yang et al., 2015] adopted the neural language model technology [Bengio et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 0, "context": ", 2015] adopted the neural language model technology [Bengio et al., 2003] to enhance topic model.", "startOffset": 53, "endOffset": 74}, {"referenceID": 13, "context": "Another line of research related to our model is Recurrent Neural Network (RNN), especially some recently developed effective RNN models such as Long Short Term Memory [Hochreiter and Schmidhuber, 1997] and Gated Recurrent Unit [Cho et al.", "startOffset": 168, "endOffset": 202}, {"referenceID": 28, "context": ", 2014] and short text conversation [Shang et al., 2015].", "startOffset": 36, "endOffset": 56}, {"referenceID": 24, "context": "In particular, for language modeling tasks, it has been shown that RNN (and its variants such as LSTM) is much more effective than simple feedforward neural networks with fixed window size [Mikolov et al., 2010] given that it can model dependencies with nearly arbitrary length.", "startOffset": 189, "endOffset": 211}, {"referenceID": 11, "context": "\u2022 SLRTM takes the one sentence, one topic assumption as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: all words within the same sentence share the same topic.", "startOffset": 59, "endOffset": 116}, {"referenceID": 31, "context": "\u2022 SLRTM takes the one sentence, one topic assumption as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: all words within the same sentence share the same topic.", "startOffset": 59, "endOffset": 116}, {"referenceID": 12, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 19, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 3, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 5, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 29, "context": ", 2014] and the image vector output by Convolutional Neural Network in image captioning [Vinyals et al., 2015].", "startOffset": 88, "endOffset": 110}, {"referenceID": 1, "context": "Then the variational lower bound of the data likelihood [Blei et al., 2003] can be written as: Figure 1: The illustration of the SLRTM generative process.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Considering that mini-batch (containing several sentences) inference and training are necessary to optimize the neural network, we leverage the stochastic variational inference algorithm developed in [Hoffman et al., 2010; Hoffman et al., 2013] to conduct inference and learning in a variational ExpectationMaximization framework.", "startOffset": 200, "endOffset": 244}, {"referenceID": 15, "context": "Considering that mini-batch (containing several sentences) inference and training are necessary to optimize the neural network, we leverage the stochastic variational inference algorithm developed in [Hoffman et al., 2010; Hoffman et al., 2013] to conduct inference and learning in a variational ExpectationMaximization framework.", "startOffset": 200, "endOffset": 244}, {"referenceID": 14, "context": "5, 1], to make sure \u03b3 will converge [Hoffman et al., 2010].", "startOffset": 36, "endOffset": 58}, {"referenceID": 13, "context": "Due to space limit, we omit the derivation details for the updating equations in Algorithm 1, as well as the forward/backward pass details for LSTM [Hochreiter and Schmidhuber, 1997].", "startOffset": 148, "endOffset": 182}, {"referenceID": 17, "context": "We did not use any recently developed algorithms for inference and learning under deep neural networks such as variational autoencoder [Kingma and Welling, 2013] because they are designed for continuous hidden states while our model includes discrete variables.", "startOffset": 135, "endOffset": 161}, {"referenceID": 8, "context": "Use grad to obtain \u0398 by stochastic gradient descent methods such as Adagrad [Duchi et al., 2011].", "startOffset": 76, "endOffset": 96}, {"referenceID": 33, "context": "\u2022 Wiki10+ [Zubiaga, 2012]3, which contains Web docu-", "startOffset": 10, "endOffset": 25}, {"referenceID": 3, "context": "Following [Cao et al., 2015], we kept the most frequent 25 tags and removed those documents without any of these tags, forming a training set and a test set with 11,164 and 6,161 documents, respectively.", "startOffset": 10, "endOffset": 28}, {"referenceID": 1, "context": "\u2022 LDA [Blei et al., 2003].", "startOffset": 6, "endOffset": 25}, {"referenceID": 19, "context": "\u2022 Doc-NADE [Larochelle and Lauly, 2012].", "startOffset": 11, "endOffset": 39}, {"referenceID": 11, "context": "\u2022 HTMM [Gruber et al., 2007].", "startOffset": 7, "endOffset": 28}, {"referenceID": 32, "context": "\u2022 GMNTM [Yang et al., 2015].", "startOffset": 8, "endOffset": 27}, {"referenceID": 32, "context": "For the sake of fairness, similar to [Yang et al., 2015], we set the word embedding size, topic embedding size, and LSTM hidden layer size to be 128, 128, and 600 respectively.", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "Asynchronous stochastic gradient descent [Dean et al., 2012] with Adagrad was used to perform multi-thread parallel training.", "startOffset": 41, "endOffset": 60}, {"referenceID": 29, "context": "The second is dynamic programming based beam search [Vinyals et al., 2015], which seeks to generate sentences by globally maximized likelihood.", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "This is consistent with the observations in [Serban et al., 2015].", "startOffset": 44, "endOffset": 65}, {"referenceID": 28, "context": "In the future, we plan to integrate SLRTM into RNN-based STC systems [Shang et al., 2015] to make the dialogue more topic sensitive.", "startOffset": 69, "endOffset": 89}], "year": 2017, "abstractText": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation.", "creator": "LaTeX with hyperref package"}}}