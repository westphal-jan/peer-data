{"id": "1604.02080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes", "abstract": "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.", "histories": [["v1", "Thu, 7 Apr 2016 17:12:07 GMT  (276kb,D)", "http://arxiv.org/abs/1604.02080v1", "16 pages, 3 figures"]], "COMMENTS": "16 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["jordi grau-moya", "felix leibfried", "tim genewein", "daniel a braun"], "accepted": false, "id": "1604.02080"}, "pdf": {"name": "1604.02080.pdf", "metadata": {"source": "CRF", "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes", "authors": ["Jordi Grau-Moya", "Felix Leibfried", "Tim Genewein", "Daniel A. Braun"], "emails": ["jordi.grau@tuebingen.mpg.de,"], "sections": [{"heading": null, "text": "Keywords: limited rationality, model uncertainty, robustness, planning, Markov decision-making processes"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this is a case of an injury to a person who was able to escape to safety."}, {"heading": "2 Background and Notation", "text": "In the MDP setting, the agent interacts with the environment in due course by taking action during the transition in the state of st-S. Then, the environment updates the state of the agent to st + 1-S according to the transition probabilities T (st + 1 | at, st). After each transition, the agent receives a reward Rst + 1 at which he is limited. For our purposes, we consider A and S finite. The agent's goal is to choose his policy \u03c0 (a-s) to maximize the total discounted expected reward or value function for any s-SV-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s."}, {"heading": "2.1 Information-Theoretic Constraints for Acting", "text": "If we consider a single-step decision problem where the agent is in a state of s and must select a single action a from set A to maximize the reward, a perfectly rational agent selects the optimal action a (s) = argmaxa (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s, a). However, a perfectly rational agent selects the optimal action a (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s) s) s (s) s (s) s (s) s) s (s) s (s) s) s (s) s (s) s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s (s) s) s (s) s) s (s (s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s (s) s (s) s (s (s) s (s) s (s (s) s (s) s) s (s) s (s (s) s (s) s (s) s (s (s) s) s (s (s) s (s (s) s) s (s (s) s) s (s) s (s) s (s) s (s (s) s) s (s ("}, {"heading": "2.2 Information-Theoretic Constraints for Model Uncertainty", "text": "In the following, we assume that the agent has a model of environmental dynamics in which the agent considers a unit vector of transition probabilities in all possible states. As the agent interacts with the environment, the agent can inject new data by forming the subordinate agent (a, a, s, D), where D is the observed data. If the agent has observed an infinite amount of data (and assumes that he interacts with the environment), the agent will include new data by engaging with the subordinate agent (a, s, s, s) in the delta distribution model (a, s, s), and the agent will act optimally according to the true transition probabilities, just as in ordinary optimal selection strategies with known models. If the agent acts under a limited amount of data, the agent cannot determine the value of an action with the true transition model."}, {"heading": "3 Model Uncertainty and Bounded Rationality in MDPs", "text": "In this section, we will consider a limited rational agent with model uncertainty in the infinite horizon setting of an MDP. In this case, the agent must take into account all future rewards and information costs, thus taking into account the following free energy objectiveF * (s) = max \u03c0 ext lim T (\u03b2 \u03b2 | 1) (\u03b2 | 1) (Rst + 1st, at \u2212 1 \u03b2, at, st). (\u2032) \u00b7 1) \u2212 1 \"log\" (at | st). (6), where the most extreme operator is either max for \u03b2 > 0 or min for \u03b2 < 0 < 1 is the discount factor and expectation E across all trajectories (at | st). (a0, a0, a1)."}, {"heading": "3.1 Free Energy Iteration Algorithm", "text": "The solution of the self-consistency equation (8) can be achieved by a generalized version of the value replay, while the optimal solution can be achieved by initializing the free energy to any value F and applying a value replay scheme Bi + 1F = BBiF, in which we use the operator BF (s) = max. (s) x x x x x x x (a) x (a) x (a) x (a) x (a) x (a) x (a) x (a) x (a) x (a) x (a) x (s) x (s) x (s) x x (s) x x (a) x x (b) x (x) x (b) x x x (b) x x x x (b) x x (b) x x (b) x (x) x (b) x (x) x (x) x (b) x (x x x) x x x x (x x x x x x x x) x x x x x x x x x x (b) x x (c) x (c) x (b) x x (b) x (b) x x (b) x x (b) x x (b) x (b) x (c) x (b) x x (b) x (b) x x (b) x (b) x x (b) x (b) x (b) x (b) x (b) x (x (b) x (b) x (c) x (b) x (x (b) x (b) x (x x (b) x x (b) x (b) x (b) x (x x x x x (x) x (x) x x (x x x x x) x (b) x x (x x x x x x x x (b) x) x (x x x x x x x x (b) x (b) x (x x x x x x x x x x x x) x x x) x x x x x (x x x x x x x x x x x x) x x x x x x x x x x x x (x x x x x x x x) x x x x x x x x x x x x x"}, {"heading": "4 Convergence", "text": "In this context, we first show the existence of a unique fixed point (Theorem 1), which follows [17,18] and then proves the convergence of the value titeration scheme (s), provided that a unique fixed point exists (Theorem 2). Theorem 1), provided that an optimal vector F (s) is a unique fixed point of the Bellman equation F (s). (Theorem 2), in which the mapping B: R (S) is defined as an equation. (Theorem 1) is a unique fixed point of the Bellman equation F (s), where the mapping B: R (S) is defined as an equation. (Theorem 1) is demonstrably a vector F (s). (s) is a unique fixed point of the Bellman equation F (s). The mapping F equation (S) is a unique fixed point (Bellman)."}, {"heading": "5 Experiments: Grid World", "text": "This section illustrates the proposed value iteration scheme with an intuitive example in which an agent must navigate through a grid world. An agent starts at position S-S with the goal of reaching the target state G-S, and at each time step can select one of a maximum of four possible actions that are represented as gray tiles (actions that move the agent toward the wall are not possible), holes that are represented as black tiles (movements into the hole cause a negative reward), and random tiles that are represented as white tiles with a question mark (the transition chances of the random tiles are unknown to the agent). Achieving target G results in a reward R = + 1, while entering a hole leads to a negative reward R = \u2212 1. In both cases, the agent is subsequently teleported to starting position S. Achieving target G results in a reward that results in a reward of a reward of i = 0,0."}, {"heading": "5.1 The Role of the Parameters \u03b1 and \u03b2 on Planning", "text": "Figure 1 shows the solution of the variable problem of free energy achieved by iteration to convergence according to algorithm \u03b2 1 at different values of \u03b1 and \u03b2. In particular, the first row shows the free energy function F \u0443 (s) (Eq. (8)). The second, third and fourth rows show thermal images of the position of an active substance following the optimal policy (Eq. (12)), according to the tendency beliefs (plan) of the active substance and the actual transition probabilities in a friendly and unfriendly environment. In random tiles, the most likely transitions in these two environments are indicated by arrows, in which the active substance is teleported with a probability of 0.999 into the tile indicated by the arrow and with a probability of 0.001 to a random other adjacent tile. In the first column of Fig. 1, one can see that a stochastic active substance (\u03b1 = 3.0) with a high degree of uncertainty in the model setting = 400 represents an unsafe transition to the broad (an optimistic preference)."}, {"heading": "5.2 Updating the Bayesian Posterior \u00b5 with Observations from the Environment", "text": "Similar to the model identification of adaptive controllers that perform system identification while the system is running [\u03b2 \u03b2], we can also use the proposed planning algorithm in a reinforcement learning setup by updating Bayean views of the MDP while always performing the first action and rescheduling the next step. However, during the learning phase, exploration is determined by both factors \u03b1 and \u03b2, but each factor has a different influence. In particular, lower \u03b1 values lead to more exploration in the long run due to the inherent stochasticity in agent action selection, similar to a greedy policy. Of course, fixing \u03b1 over time implies a \"suboptimal\" (i.e. limited optimal) policy. In contrast, the parameter \u03b2 regulates the exploration of states with unknown transition probabilities more directly and has no influence on the performance of the agent in the limit where sufficient data has eliminated model uncertainty."}, {"heading": "6 Discussion and Conclusions", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they are able to live, in which they, in which they live, in which they, in which they"}], "references": [{"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1957}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["Emanuel Todorov"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Path integral control and bounded rationality", "author": ["Daniel A Braun", "Pedro A Ortega", "Evangelos Theodorou", "Stefan Schaal"], "venue": "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Risk sensitive path integral control", "author": ["Bart van den Broek", "Wim Wiegerinck", "Hilbert J. Kappen"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Thermodynamics as a theory of decision-making with information-processing costs", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "In Proc. R. Soc. A,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Generalized thompson sampling for sequential decision-making and causal inference", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Information theory of decisions and actions", "author": ["Naftali Tishby", "Daniel Polani"], "venue": "In Perceptionaction cycle,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["Michael O\u2019Gordon Duff"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Bias and variance approximation in value function estimates", "author": ["Shie Mannor", "Duncan Simester", "Peng Sun", "John N Tsitsiklis"], "venue": "Management Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Robust control of markov decision processes with uncertain transition matrices", "author": ["Arnab Nilim", "Laurent El Ghaoui"], "venue": "Operations Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Robust dynamic programming", "author": ["Garud N Iyengar"], "venue": "Mathematics of Operations Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Robust markov decision processes", "author": ["Wolfram Wiesemann", "Daniel Kuhn", "Ber\u00e7 Rustem"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "The many faces of optimism: a unifying approach", "author": ["Istv\u00e1n Szita", "Andr\u00e1s L\u0151rincz"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u00e1n Szita", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Trading value and information in mdps. In Decision Making with Imperfect Decision Makers", "author": ["Jonathan Rubin", "Ohad Shamir", "Naftali Tishby"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "and JN Tsitsiklis", "author": ["DP Bertseka"], "venue": "Neuro-dynamic programming.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Reinforcement learning in finite mdps: Pac analysis", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Adaptive control", "author": ["Karl J \u00c5str\u00f6m", "Bj\u00f6rn Wittenmark"], "venue": "Courier Corporation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Linear theory for control of nonlinear stochastic systems", "author": ["Hilbert J Kappen"], "venue": "Physical review letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Fox D Poole, et al", "author": ["J Peters", "K M\u00fclling", "Y Altun"], "venue": "Relative entropy policy search. In TwentyFourth National Conference on Artificial Intelligence (AAAI-10), pages 1607\u20131612. AAAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Risk-sensitive reinforcement learning", "author": ["Yun Shen", "Michael J Tobia", "Tobias Sommer", "Klaus Obermayer"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Robustness and risk-sensitivity in markov decision processes", "author": ["Takayuki Osogami"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Risk-sensitive and robust decision-making: a cvar optimization approach", "author": ["Yinlam Chow", "Aviv Tamar", "Shie Mannor", "Marco Pavone"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A bayesian approach for learning and planning in partially observable markov decision processes", "author": ["St\u00e9phane Ross", "Joelle Pineau", "Brahim Chaib-draa", "Pierre Kreitmann"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Efficient bayes-adaptive reinforcement learning using sample-based search", "author": ["Arthur Guez", "David Silver", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search", "author": ["Arthur Guez", "David Silver", "Peter Dayan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Monte carlo methods for exact & efficient solution of the generalized optimality equations", "author": ["Pedro A Ortega", "Daniel A Braun", "Naftali Tishby"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "G-learning: Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A minimum relative entropy principle for learning and acting", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "In 3d Conference on Artificial General Intelligence", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Rlpy: A value-function-based reinforcement learning framework for education and research", "author": ["Alborz Geramifard", "Christoph Dann", "Robert H Klein", "William Dabney", "Jonathan P How"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The problem of planning in Markov Decision Processes was famously addressed by Bellman who developed the eponymous principle in 1957 [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.", "startOffset": 8, "endOffset": 13}, {"referenceID": 2, "context": "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.", "startOffset": 8, "endOffset": 13}, {"referenceID": 3, "context": "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].", "startOffset": 101, "endOffset": 106}, {"referenceID": 4, "context": "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].", "startOffset": 101, "endOffset": 106}, {"referenceID": 5, "context": "(measured by the Kullback-Leiber divergence in bits) [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Such a decision-maker can also be instantiated by a sampling process that has restrictions in the number of samples it can afford [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "Disregarding the possibility of a sampling-based interpretation, the Kullback-Leibler divergence introduces a control information cost that is interesting in its own right when formalizing the perception action cycle [8].", "startOffset": 217, "endOffset": 220}, {"referenceID": 8, "context": "In Bayes-Adaptive MDPs [9], for example, the uncertainty over the latent parameters of the MDP is explicitly represented, such that new information can be incorporated with Bayesian inference.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "However, Bayes-Adaptive MDPs are not robust with respect to model misspecification and have no performance guarantees when planning with wrong models [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 11, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 12, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 13, "context": "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration\u2014also referred in the literature as optimism in face of uncertainty [14,15].", "startOffset": 177, "endOffset": 184}, {"referenceID": 14, "context": "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration\u2014also referred in the literature as optimism in face of uncertainty [14,15].", "startOffset": 177, "endOffset": 184}, {"referenceID": 2, "context": "When comparing the literature on information-theoretic control and model uncertainty, it is interesting to see that some notions of model uncertainty follow exactly the same mathematical principles as the principles of relative entropy control [3].", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "Applying variational calculus and following the same rationale as in the previous sections [6], the extremum operators can be eliminated and equation (7) can be reexpressed as", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 88, "endOffset": 95}, {"referenceID": 17, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 234, "endOffset": 238}, {"referenceID": 18, "context": "2 Updating the Bayesian Posterior \u03bc with Observations from the Environment Similar to model identification adaptive controllers that perform system identification while the system is running [20], we can use the proposed planning algorithm also in a reinforcement learning setup by updating the Bayesian beliefs about the MDP while executing always the first action and replanning in the next time step.", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].", "startOffset": 295, "endOffset": 302}, {"referenceID": 11, "context": "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].", "startOffset": 295, "endOffset": 302}, {"referenceID": 12, "context": "Recent extensions of these approaches include more general assumptions regarding the set properties of the permissible models and assumptions regarding the data generation process [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 19, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 20, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 15, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Specifically, the reference distribution in [8] is given", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "by the marginal distributions (which is equivalent to a rate distortion problem) and in [6] is given by fixed priors.", "startOffset": 88, "endOffset": 91}, {"referenceID": 21, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 22, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 23, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 8, "context": "The algorithm presented here and Bayesian models in general [9] are computationally expensive as they have to compute possibly high-dimensional integrals depending on the number of allowed transitions for action-state pairs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 24, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 25, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 26, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 27, "context": "An interesting future direction to extend our methodology would therefore be to develop a sampling-based version of Algorithm 1 to increase the range of applicability and scalability [29].", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 186, "endOffset": 193}, {"referenceID": 30, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 186, "endOffset": 193}, {"referenceID": 18, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "The code was developed on top of the RLPy library [33].", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.", "creator": "LaTeX with hyperref package"}}}