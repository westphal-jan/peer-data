{"id": "1604.02264", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Probabilistic classifiers with low rank indefinite kernels", "abstract": "Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nystr\\\"om approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data.", "histories": [["v1", "Fri, 8 Apr 2016 07:58:36 GMT  (2810kb,D)", "http://arxiv.org/abs/1604.02264v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["frank-michael schleif", "andrej gisbrecht", "peter tino"], "accepted": false, "id": "1604.02264"}, "pdf": {"name": "1604.02264.pdf", "metadata": {"source": "META", "title": "Probabilistic classifiers with low rank indefinite kernels", "authors": ["Frank-Michael Schleif", "Andrej Gisbrecht", "Peter Tino"], "emails": ["schleify@cs.bham.ac.uk", "andrej.gisbrecht@aalto.fi", "pxt@cs.bham.ac.uk"], "sections": [{"heading": null, "text": "Keywords: indeterminate core, Kernel Fisher discriminant, minimum enclosing ball, Nystro \ufffd m approximation, Low Rank approximation, classification"}, {"heading": "1. Introduction", "text": "It is a question of whether it concerns a kind of concepts which only relate to the categories of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts of other concepts and concepts. The concepts of concepts and concepts of and concepts of, and concepts of, and concepts of, and concepts of, and concepts of, and concepts of, and concepts of, and concepts of, and concepts of, and concepts of,"}, {"heading": "2. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notation and basic concepts", "text": "Let X be a collection of N objects xi, i = 1, 2,..., N, in any input space. If the similarity function or the inner product is used to compare two objects xi, xj is metric, proper Mercer cores can be obtained as discussed below. However, a classical similarity function in this context is the Euclidean inner product with the respective Euclidean distance, which is a common core component of various metric core functions, such as the famous radial base function (rbf). Now, let it be: X 7 \u2192 H a mapping of patterns from X to a high-dimensional or infinite dimensional Hilbert product H, equipped with the inner product < \u00b7, \u00b7 H. The transformation is generally a non-linear mapping to a high-dimensional space H and generally must not be given in an explicit form."}, {"heading": "2.2. Krein and Pseudo-Euclidean spaces", "text": "It is indeed the case that it is a matter of a way in which people find themselves in the most diverse areas of life. (...) It is indeed the case that people find themselves in the most diverse areas of life. (...) It is as if people live in the most diverse areas of life. (...) It is as if people live in the most diverse areas of life. (...) It is as if people live in the most diverse areas of life. (...) It is as if they live in the most diverse areas of life. (...) It is as if people live in the most diverse areas of life. (...) It is as if they live in the most diverse areas of life. (...) It is as if they live in the most diverse areas of life. (...) It is as if they live in the most diverse areas of life."}, {"heading": "2.3. Indefinite Fisher and kernel quadratic discriminant", "text": "In Haasdonk and Pekalska Discriminant function f (x) = < Pekalska and Haasdonk (2009), indefinite Kernel Fisher Discriminant Analysis (iKFD) and indefinite Kernel Square Discriminant Analysis (iKQD) have been proposed, which focuses on binary classification problems and was recently extended by a weighting scheme in Yang and Fan (2013).The original idea is to embed the training data in a crein space (see Def. \u2212 Def. \u2212 Ni = 1) and apply a modified Kernel Fisher Discriminant Analysis or Kernel Discrimination Analysis for indefinite nuclei. Consider the binary classification and a dataset of input-target training pairs D = {xi, yi} Ni = 1, where Yi Eui, + 1}. Given the indefinite Kernel MatrixK and the embedded data in a pseudo-euclidean space (E function), the Fisher Discriminant (E) function is represented linear."}, {"heading": "2.4. Probabilistic Classification Vector Learning", "text": "The expectation maximization (EM) is the implementation of the PCVM method N (0, 1). We get: l (x; w, b) = Probit link function (x) = Probit link function (x) \u2212 n (x) \u2212 n (x, 1) dt, where \"x\" is the effective distribution of the normal distribution N (0, 1). We get: l (x; b) = Probit link function (x) = Probit link function N (x) \u2212 n (x, 1) dt, where \"x\" is the effective distribution of the normal distribution N (0, 1). We get: l (x; b) = Probit link function (x, b) = Probit link function (x) = x (x)."}, {"heading": "3. Nystro\u0308m approximated matrix processing", "text": "Nystro-m Approximation Technology was proposed in the context of the Kernel Methods in (Williams and Seeger, 2000). Here we give a brief overview of this technique before it is applied in PCVM and iKFD. A well-known way to approach a N-N-gram matrix is to use a low-rank approximation matrix. This can be done by computing the self-composition of the kernel matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix (Williams and Seeger, 2000)."}, {"heading": "3.1. Pseudo Inverse and Singular Value Decomposition of a Nystro\u0308m approximated matrix", "text": "In the Ny-PCVM approach discussed in Section 5, we need the pseudo-inversion of a Nystro-iKFD approximation matrix, while a Nystro-in-the-approximation decomposition (EVD) is required for the Ny-iKFD. A Nystro-m approximation decomposition can be calculated by a modified singular value decomposition (SVD) with a rank limited by r = min {r, m}, where r is the rank of the pseudo-inversion and m is the number of boundaries. The output is obtained by the rank of reduced singular vectors left and right and the inversion of the singular values. The singular value decomposition is based on a Nystro-m approximation matrix K = KNmK \u2212 1m, mK > Nm > eigenmatrix, calculates the left singular vectors of K as eigenvectors of K and the correct singular matrix."}, {"heading": "3.2. Eigenvalue decomposition of a Nystro\u0308m approximated matrix", "text": "To calculate the eigenvectors and eigenvalues of an indefinite matrix, we must first compute > > the square shape of the approximate kernel matrix."}, {"heading": "4. Supervised landmark selection using minimum enclosing balls", "text": "It is about the question of whether it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way it is about a way and a way in which it is about a way and a way in which it is about a way and a way it is about a way and a way it is about a way and a way and a way it is about a way and a way it is about a way and a way and a way it is about a way and a way it is about a way and a way and a way it is about a way and a way it is about a way and a way and a way it is about a way and a way and a way it is about a way and a way and a way it is about a way and a way it is about a way and a way and a way it is about a way and a way it is a way and a way and a way it is a way and a way and a way and a way it is a way and a way it is a way and a way and a way it is a way and a way"}, {"heading": "4.1. MEB for psd input kernels", "text": "We refer to the set of indices or points of a sub-kernel matrix that relate to a problem of class j of Rj. Assuming that approximately spherical clusters are present, we can approximate this problem by the minimum surrounding sphere: minR2, wj R 2 such that the weighted linear combination of points inRj. Indeed, the assumption of a sphere is not a significant limitation if the provided kernel is sufficiently expressive and wj is a center that can be represented indirectly in the core space. This is also the reason why the core vector data description (CVDD) can be used as a linear time substitute for support vector data classes. Tsang, I. W., Kwok, J. T., Cheung, P., (2005).It has been shown, for example, in Badoiu and Clarkson (2008) that the minimum termination sphere with a sufficient class can be defined by a sufficient quality of the linear data descriptor."}, {"heading": "4.2. MEB for non-psd input kernels", "text": "If the given kernel is not redundant, we can either apply different eigenvalue correction approaches, see Grinding and Tino (2015), or we can use K = K \u00b7 K >, which can be easily approximated for Nystro \ufffd m matrices even without computing a complete matrix (see first part of Equation (15)). This procedure does not change the eigenvectors of K, but takes the square of eigenvalues so that K \u0438\u043d\u0438\u0441\u0442\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0442\u0438\u0441\u0438\u0441\u0442\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0435 is used as an input of a k-mean algorithm, as an input of the classic k-mean at euclidean distance, as proposed in Zhang and Kwok (2010). The proposed supervised selection of boundary stones with MEB not only identifies a good estimate for the number of boundary stones, but also ensures that the boundary stones are sufficient to explain the data space. MEB solutions are not composed of marginal points with MEB, although they are not necessarily related to the MED margins (although MED are closely related to MED)."}, {"heading": "4.3. Small scale experiments - landmark selection scheme", "text": "It is an artificial dataset based on the surface spacing of randomly positioned balls of two classes, each with a slightly different radius. In fact, the datasets are equipped with essential information encrypted in the negative part of the eigenspectrum. We generated the data with 100 characteristics per class, resulting in a high intrinsic dimension of loop and Tino. Furthermore, we analyzed two simulated metric datasets that are not linearly separable using the Euclidean norm: (1) the checker data, which is organized as a two-dimensional dataset with datapoints on a three-dimensional dataset."}, {"heading": "5. Large scale indefinite learning with PCVM and iKFD", "text": "These modifications ensure that all matrices with linear memory complexity are processed and that the underlying algorithms have linear runtime complexity. In both algorithms, the first input is the Nystro \ufffd m-approximated kernel matrix with landmarks selected from one of the previously provided landmark selection schemes."}, {"heading": "5.1. PCVM for large scale proximity data", "text": "The PCVM parameters are optimized with the EM algorithm to select the weight vector w during learning and thus represent the basic functions considered that represent the model. We refer to our method as Ny-PCVM method to keep the matrices K1 = KN, m and K2 = K \u2212 1 linear at all times."}, {"heading": "5.2. Nystro\u0308m based Indefinite Kernel Fisher Discriminant", "text": "Given a kernel matrix approaching Nystro-m, some adjustments need to be made to obtain a valid iKFD formula based solely on the kernel formula approximating Nystro-m, without complete matrix operations.10The number of landmarks m \u00b2 is fixed at 1% of | w | but no more than 500 landmarks. If the length of the w falls below 100 points, we use the original PCVM formulas. \u2212 First, we need to calculate the class size based on the row / column sums of the approximate input kernel matrix."}, {"heading": "6. Complexity analysis", "text": "The original iKFD update rules have costs of O (N3) and memory costs O (N2), where N is the number of points. Ny-iKFD can include the additional Nystro-m approximation of the kernel matrix to obtain KN, m and K \u2212 1m, m, if not already specified. If we have m limits, m N, there are costs of O (mN) for the first matrix and O (m3) for the second, due to the matrix inversion. Furthermore, both matrices are multiplied within the optimization, so we get O (m2N). Likewise, the matrix inversion of the original iKFD without O (N3) is reduced to O (m2N) + O (m3) due to the Nystro-m approximation of the pseudo-inverse. If we assume m N, the total runtime and storage complexity of Ny-iKFD is linear."}, {"heading": "7. Experiments", "text": "We compare iKFD, Ny-iKFD, Ny-PCVM and PCVM on several larger, indefinite sets of data. Unlike many standard kernel approaches, for the iKFD and PCVM, the approximate kernel matrices do not need to be corrected by costly eigenvalue correction. (2009c) The iKFD and PCVM also have direct access to probable classification decisions. First, we show a small simulated experiment for two Gaussians that exist in an intrinsic two-dimensional pseudo-euclidean space. (1,1) The plot in Figure 5 shows a typical result for the obtained decision levels iKFD or Ny-iKFD. The Gaussians are slightly overlapped and both approaches achieve a good separation with 93.50% and 88.50% predictive dimensions."}, {"heading": "8. Conclusions", "text": "Our results suggest that the MEB approach is generally as efficient as K-mean clustering or entropy strategy, but with less effort and almost parameter-free. We found that Ny-iKFD is competitive in predictive accuracy with the original iKFD and alternative approaches, while consuming much less memory and runtime, but less economical than NyPCVM. Ny-iKFD and Ny-PCVM now provide an effective way to obtain a probable classification model for medium to large psd and non-psd datasets, while being less economical than NyPCV.The Ny-iKFD is a problem that can be preferred to Ny-iKFD in batch mode with linear runtime and memory complexity."}], "references": [{"title": "Support vector machines with indefinite kernels", "author": ["I.M. Alabdulmohsin", "X. Gao", "X. Zhang"], "venue": "Proceedings of the Sixth Asian", "citeRegEx": "Alabdulmohsin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alabdulmohsin et al\\.", "year": 2014}, {"title": "Optimal core-sets for balls", "author": ["M. Badoiu", "K.L. Clarkson"], "venue": "Comput. Geom", "citeRegEx": "Badoiu and Clarkson,? \\Q2008\\E", "shortCiteRegEx": "Badoiu and Clarkson", "year": 2008}, {"title": "A theory of learning with similarity functions", "author": ["Balcan", "M.-F", "A. Blum", "N. Srebro"], "venue": "Machine Learning", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation", "citeRegEx": "Belkin and Niyogi,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2003}, {"title": "The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003", "author": ["B. Boeckmann", "A. Bairoch", "R. Apweiler", "Blatter", "M.-C", "A. Estreicher", "E. Gasteiger", "M. Martin", "K. Michoud", "C. O\u2019Donovan", "I. Phan", "S. Pilbout", "M. Schneider"], "venue": "Nucleic Acids Research", "citeRegEx": "Boeckmann et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boeckmann et al\\.", "year": 2003}, {"title": "Optimized fixed-size kernel models for large data sets", "author": ["K.D. Brabanter", "J.D. Brabanter", "J. Suykens", "B.D. Moor"], "venue": "Computational Statistics & Data Analysis", "citeRegEx": "Brabanter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brabanter et al\\.", "year": 2010}, {"title": "Probabilistic classification vector machines", "author": ["H. Chen", "P. Tino", "X. Yao"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Efficient probabilistic classification vector machine with incremental basis function selection", "author": ["H. Chen", "P. Tino", "X. Yao"], "venue": "IEEE TNN-LS", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Families of alpha- beta- and gamma- divergences: Flexible and robust measures of similarities", "author": ["A. Cichocki", "Amari", "S.-I"], "venue": "Entropy", "citeRegEx": "Cichocki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2010}, {"title": "Matching pursuit kernel fisher discriminant analysis", "author": ["T. Diethe", "Z. Hussain", "D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Diethe et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Diethe et al\\.", "year": 2009}, {"title": "A modified hausdorff distance for object matching", "author": ["M. Dubuisson", "A. Jain", "Oct"], "venue": "In: Pattern Recognition, 1994. Vol. 1 - Conference A: Computer Vision amp; Image Processing., Proceedings of the 12th IAPR International Conference on. Vol. 1. pp. 566\u2013568 vol.1.", "citeRegEx": "Dubuisson et al\\.,? 1994", "shortCiteRegEx": "Dubuisson et al\\.", "year": 1994}, {"title": "Non-euclidean dissimilarities: Causes and informativeness", "author": ["R.P.W. Duin", "E. Pekalska"], "venue": "Joint IAPR International Workshop,", "citeRegEx": "Duin and Pekalska,? \\Q2010\\E", "shortCiteRegEx": "Duin and Pekalska", "year": 2010}, {"title": "Metric and non-metric proximity transformations at linear costs", "author": ["A. Gisbrecht", "Schleif", "F.-M"], "venue": "Neurocomputing to appear", "citeRegEx": "Gisbrecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gisbrecht et al\\.", "year": 2015}, {"title": "Revisiting the nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "CoRR abs/1303.1849", "citeRegEx": "Gittens and Mahoney,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney", "year": 2013}, {"title": "A unified approach to pattern recognition", "author": ["L. Goldfarb"], "venue": "Pattern Recognition", "citeRegEx": "Goldfarb,? \\Q1984\\E", "shortCiteRegEx": "Goldfarb", "year": 1984}, {"title": "Feature space interpretation of svms with indefinite kernels", "author": ["B. Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Haasdonk,? \\Q2005\\E", "shortCiteRegEx": "Haasdonk", "year": 2005}, {"title": "Tangent distance kernels for support vector machines", "author": ["B. Haasdonk", "D. Keysers"], "venue": "ICPR", "citeRegEx": "Haasdonk and Keysers,? \\Q2002\\E", "shortCiteRegEx": "Haasdonk and Keysers", "year": 2002}, {"title": "Indefinite kernel fisher discriminant", "author": ["B. Haasdonk", "E. Pekalska"], "venue": "International Conference on Pattern Recognition (ICPR", "citeRegEx": "Haasdonk and Pekalska,? \\Q2008\\E", "shortCiteRegEx": "Haasdonk and Pekalska", "year": 2008}, {"title": "Learning with idealized kernels", "author": ["J.T. Kwok", "I.W. Tsang"], "venue": "Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "Kwok and Tsang,? \\Q2003\\E", "shortCiteRegEx": "Kwok and Tsang", "year": 2003}, {"title": "Shape classification using the inner-distance", "author": ["H. Ling", "D.W. Jacobs"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "Ling and Jacobs,? \\Q2007\\E", "shortCiteRegEx": "Ling and Jacobs", "year": 2007}, {"title": "Learning svm in krein spaces", "author": ["G. Loosli", "S. Canu", "C. Ong"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on PP", "citeRegEx": "Loosli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2015}, {"title": "Graph-based representation of symbolic musical data", "author": ["B. Mokbel", "A. Hasenfuss", "B. Hammer"], "venue": "GbRPR", "citeRegEx": "Mokbel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mokbel et al\\.", "year": 2009}, {"title": "Median variants of learning vector quantization for learning of dissimilarity data", "author": ["D. Nebel", "B. Hammer", "K. Frohberg", "T. Villmann"], "venue": "NeurocomputingCited By", "citeRegEx": "Nebel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nebel et al\\.", "year": 2015}, {"title": "Edit distance based kernel functions for structural pattern classification", "author": ["M. Neuhaus", "H. Bunke"], "venue": "Pattern Recognition", "citeRegEx": "Neuhaus and Bunke,? \\Q2006\\E", "shortCiteRegEx": "Neuhaus and Bunke", "year": 2006}, {"title": "The dissimilarity representation for pattern recognition", "author": ["E. Pekalska", "R. Duin"], "venue": "World Scientific", "citeRegEx": "Pekalska and Duin,? \\Q2005\\E", "shortCiteRegEx": "Pekalska and Duin", "year": 2005}, {"title": "Kernel discriminant analysis for positive definite and indefinite kernels", "author": ["E. Pekalska", "B. Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Pekalska and Haasdonk,? \\Q2009\\E", "shortCiteRegEx": "Pekalska and Haasdonk", "year": 2009}, {"title": "Probabilistic classification vector machine at large scale", "author": ["Schleif", "F.-M", "A.Gisbrecht", "P. Tino"], "venue": "Proceedings of ESANN", "citeRegEx": "Schleif et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schleif et al\\.", "year": 2015}, {"title": "Data analysis of (non-)metric proximities at linear costs", "author": ["Schleif", "F.-M", "A. Gisbrecht"], "venue": "Proceedings of SIMBAD", "citeRegEx": "Schleif et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schleif et al\\.", "year": 2013}, {"title": "2015b. Large scale indefinite kernel fisher discriminant", "author": ["Schleif", "F.-M", "A. Gisbrecht", "P. Tino"], "venue": "Proceedings of Simbad", "citeRegEx": "Schleif et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schleif et al\\.", "year": 2015}, {"title": "Indefinite proximity learning - a review", "author": ["Schleif", "F.-M", "P. Tino"], "venue": "Neural Computation", "citeRegEx": "Schleif et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schleif et al\\.", "year": 2015}, {"title": "Kernel Methods for Pattern Analysis and Discovery", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Memory efficient kernel approximation", "author": ["S. Si", "C. Hsieh", "I.S. Dhillon"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Si et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Si et al\\.", "year": 2014}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S.", "Waterman", "Mar."], "venue": "Journal of molecular biology 147 (1), 195\u2013197.", "citeRegEx": "Smith et al\\.,? 1981", "shortCiteRegEx": "Smith et al\\.", "year": 1981}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P. Cheung"], "venue": "Journal of Machine Learning Research 6,", "citeRegEx": "Tsang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsang et al\\.", "year": 2005}, {"title": "Magnification control in self-organizing maps and neural gas", "author": ["T. Villmann", "J.C. Claussen"], "venue": "Neural Computation", "citeRegEx": "Villmann and Claussen,? \\Q2006\\E", "shortCiteRegEx": "Villmann and Claussen", "year": 2006}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "NIPS", "citeRegEx": "Williams and Seeger,? \\Q2000\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2000}, {"title": "A novel indefinite kernel dimensionality reduction algorithm: Weighted generalized indefinite kernel discriminant analysis", "author": ["J. Yang", "L. Fan"], "venue": "Neural Processing Letters,", "citeRegEx": "Yang and Fan,? \\Q2013\\E", "shortCiteRegEx": "Yang and Fan", "year": 2013}, {"title": "Clustered nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["K. Zhang", "J.T. Kwok"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "Zhang and Kwok,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Kwok", "year": 2010}, {"title": "Improved Nystrom low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Proceedings of the 25th international conference on Machine learning. ICML \u201908. ACM,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 32, "context": "Domain specific proximity measures, like alignment scores in bioinformatics Smith et al. (1981), the modified Hausdorff-distance for structural pattern recog-", "startOffset": 76, "endOffset": 96}, {"referenceID": 13, "context": "nition Dubuisson and Jain (1994), shape retrieval measures like the inner distance Ling and Jacobs (2007) and many other ones generate non-metric or indefinite similarities or dissimilarities.", "startOffset": 83, "endOffset": 106}, {"referenceID": 11, "context": "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al.", "startOffset": 138, "endOffset": 167}, {"referenceID": 11, "context": "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al.", "startOffset": 138, "endOffset": 197}, {"referenceID": 4, "context": "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al. (2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al.", "startOffset": 253, "endOffset": 273}, {"referenceID": 4, "context": "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al. (2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al.", "startOffset": 253, "endOffset": 363}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G.", "startOffset": 99, "endOffset": 127}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied.", "startOffset": 99, "endOffset": 166}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity.", "startOffset": 99, "endOffset": 587}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.", "startOffset": 99, "endOffset": 721}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.", "startOffset": 99, "endOffset": 751}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices.", "startOffset": 99, "endOffset": 829}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nystr\u00f6m approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nystr\u00f6m approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al.", "startOffset": 99, "endOffset": 1694}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nystr\u00f6m approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nystr\u00f6m approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al. (2014); Brabanter et al.", "startOffset": 99, "endOffset": 1712}, {"referenceID": 0, "context": "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nystr\u00f6m based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nystr\u00f6m approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nystr\u00f6m approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al. (2014); Brabanter et al. (2010)).", "startOffset": 99, "endOffset": 1737}, {"referenceID": 26, "context": "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.-M., Gisbrecht, A., Tino, P., (2015b) in a proof of concept setting, to obtain approximate models for the Probabilistic Classification Vector Machine and the Indefinite Fisher Kernel Discriminant analysis using a random landmark selection scheme.", "startOffset": 38, "endOffset": 114}, {"referenceID": 26, "context": "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.-M., Gisbrecht, A., Tino, P., (2015b) in a proof of concept setting, to obtain approximate models for the Probabilistic Classification Vector Machine and the Indefinite Fisher Kernel Discriminant analysis using a random landmark selection scheme. This work is substantially extended and detailed in this article with a specific focus on indefinite kernels, only. A novel landmark selection scheme is proposed. Based on this new landmark selection scheme we provide detailed new experimental results and compare to alternative landmark selection approaches. Structure of the paper: First we give some basic notations necessary in the subsequent derivations. Then we review iKFD and PCVM as well as some approximation concepts proposed by the authors in Schleif et al. (2015a) which are based on the well known Nystr\u00f6m approximation.", "startOffset": 38, "endOffset": 851}, {"referenceID": 30, "context": "Kernelized methods process the embedded data points in a feature space utilizing only the inner products \u3008\u00b7, \u00b7\u3009H (kernel trick) (Shawe-Taylor and Cristianini, 2004), without the need to explicitly calculate \u03c6.", "startOffset": 128, "endOffset": 164}, {"referenceID": 32, "context": "Indefinite kernels are typically observed by means of domain specific nonmetric similarity functions (such as alignment functions used in biology (Smith et al., 1981)), by specific kernel functions - e.", "startOffset": 146, "endOffset": 166}, {"referenceID": 16, "context": "the Manhattan kernel k(x,x\u2032) = \u2212||x\u2212x||1, tangent distance kernel (Haasdonk and Keysers, 2002) or divergence measures plugged into standard kernel functions (Cichocki and Amari, 2010).", "startOffset": 66, "endOffset": 94}, {"referenceID": 15, "context": "Another source of non-psd kernels are noise artifacts on standard kernel functions (Haasdonk, 2005).", "startOffset": 83, "endOffset": 99}, {"referenceID": 14, "context": "Given a symmetric dissimilarity matrix with zero diagonal 1, an embedding of the data in a pseudo-Euclidean vector space determined by the eigenvector decomposition of the associated similarity matrix S is always possible (Goldfarb, 1984) 2 Given the eigendecomposition of S, S = U\u039bU>, we can compute the corresponding vectorial representation V in the pseudo-Euclidean space by V = Up+q+z |\u039bp+q+z| (1)", "startOffset": 222, "endOffset": 238}, {"referenceID": 24, "context": "A detailed presentation of similarity and dissimilarity measures, and mathematical aspects of metric and non-metric spaces is provided in (Pekalska and Duin, 2005).", "startOffset": 138, "endOffset": 163}, {"referenceID": 24, "context": "2 The associated similarity matrix can be obtained by double centering (Pekalska and Duin, 2005) of the (squared) dissimilarity matrix.", "startOffset": 71, "endOffset": 96}, {"referenceID": 15, "context": "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.", "startOffset": 55, "endOffset": 84}, {"referenceID": 15, "context": "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.", "startOffset": 55, "endOffset": 114}, {"referenceID": 15, "context": "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.", "startOffset": 55, "endOffset": 353}, {"referenceID": 15, "context": "In Haasdonk and Pekalska (2008) it is shown that the Fisher Discriminant in the pE space \u2208 R is identical to the Fisher Discriminant in the associated Euclidean space R.", "startOffset": 3, "endOffset": 32}, {"referenceID": 15, "context": "can be used for KQD as well as the indefinite kernel PCA Pekalska and Haasdonk (2009).", "startOffset": 70, "endOffset": 86}, {"referenceID": 6, "context": "The Expectation Maximization (EM) implementation of PCVM Chen et al. (2014) uses the probit link function, i.", "startOffset": 57, "endOffset": 76}, {"referenceID": 6, "context": "The Expectation Maximization (EM) implementation of PCVM Chen et al. (2014) uses the probit link function, i.e. \u03a8(x) = \u222b x \u2212\u221eN (t|0, 1)dt,where \u03a8(x) is the cumulative distribution of the normal distribution N (0, 1). We get: l(x; w, b) = \u03a8 (\u2211N i=1wi\u03c6i(x) + b ) = \u03a8 (\u03a6(x)w + b) In the PCVM formulation Chen et al. (2009a), a truncated Gaussian prior Nt with support on [0,\u221e) and mode at 0 is introduced for each weight wi and a zero-mean Gaussian prior is adopted for the bias b.", "startOffset": 57, "endOffset": 321}, {"referenceID": 6, "context": "For further details can be found in Chen et al. (2009a). Even though kernel machines and their derivatives have shown great promise in practical application, their scope is somehow limited by the fact that the computational complexity grows rapidly with the size of the kernel matrix (number of data items).", "startOffset": 36, "endOffset": 56}, {"referenceID": 35, "context": "The Nystr\u00f6m approximation technique has been proposed in the context of kernel methods in (Williams and Seeger, 2000).", "startOffset": 90, "endOffset": 117}, {"referenceID": 35, "context": "Strategies how to chose the landmarks have recently been addressed in Zhang and Kwok (2010); Zhang et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 35, "context": "Strategies how to chose the landmarks have recently been addressed in Zhang and Kwok (2010); Zhang et al. (2008) and Gittens and Mahoney (2013); Brabanter et al.", "startOffset": 70, "endOffset": 113}, {"referenceID": 12, "context": "(2008) and Gittens and Mahoney (2013); Brabanter et al.", "startOffset": 11, "endOffset": 38}, {"referenceID": 5, "context": "(2008) and Gittens and Mahoney (2013); Brabanter et al. (2010). We denote these rows by Km,N .", "startOffset": 39, "endOffset": 63}, {"referenceID": 36, "context": "In Zhang and Kwok (2010) multiple strategies for landmark selection have been studied and a clustering based approach was suggested to find the specific landmarks.", "startOffset": 3, "endOffset": 25}, {"referenceID": 36, "context": "In Zhang and Kwok (2010) multiple strategies for landmark selection have been studied and a clustering based approach was suggested to find the specific landmarks. Thereby the number of landmarks is a user defined parameter and a classical kmeans algorithm is applied on the kernel matrix to identify characteristic landmark points in the empirical feature space. This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al.", "startOffset": 3, "endOffset": 424}, {"referenceID": 31, "context": "This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al. (2014). We will use it as a baseline for an advanced landmark section approach.", "startOffset": 139, "endOffset": 156}, {"referenceID": 31, "context": "This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al. (2014). We will use it as a baseline for an advanced landmark section approach. Further we will also consider a pure random selection strategy. It should be noted that the formulation given in Zhang and Kwok (2010) takes the full kernel matrix as an input into the k-means clustering.", "startOffset": 139, "endOffset": 364}, {"referenceID": 18, "context": "Kwok and Tsang (2003)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "Kwok and Tsang (2003)). We will however not learn an idealized kernel as proposed in Kwok and Tsang (2003), which by itself is very costly for large scale matrices, but provide a landmark selection strategy leading into a similar direction.", "startOffset": 0, "endOffset": 107}, {"referenceID": 2, "context": "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class.", "startOffset": 14, "endOffset": 35}, {"referenceID": 2, "context": "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class. However the score s(K\u0302,K) is only a rough estimate. It will likely work well for datasets which can easily be modeled by conceptually related classifiers focusing e.g. on exemplar based representations. The median classifier proposed in Nebel et al. (2015) is such a simple classifier.", "startOffset": 14, "endOffset": 478}, {"referenceID": 2, "context": "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class. However the score s(K\u0302,K) is only a rough estimate. It will likely work well for datasets which can easily be modeled by conceptually related classifiers focusing e.g. on exemplar based representations. The median classifier proposed in Nebel et al. (2015) is such a simple classifier. In its simplest form it identifies for each class a single basis function or prototype, showing maximum margin with respect to the other prototypes with different class labels. In Nebel et al. (2015) it was shown that such a classifier can be very efficient also for a variety of classification problems.", "startOffset": 14, "endOffset": 707}, {"referenceID": 1, "context": "in Badoiu and Clarkson (2008) that the minimum enclosing ball can be approximated with quality in (worst case) linear time using an algorithm which requires only a constant subset of the receptive field Rj , the core set.", "startOffset": 3, "endOffset": 30}, {"referenceID": 36, "context": "It should be noted that if we use K\u0302 as an input of a kernel k-means algorithm this is equivalent as using K as the input of the classical k-means with Euclidean distance as suggested in Zhang and Kwok (2010). The proposed supervised landmark selection using MEB does not only identify a good estimate for the number of landmarks but also ensures that the landmarks are sufficient to explain the data space.", "startOffset": 187, "endOffset": 209}, {"referenceID": 1, "context": "Especially only those points are included in the MEB solution which are needed to explain the sphere such that redundancy within this set is avoided Badoiu and Clarkson (2008). Therefore for each class the MEB solutions provides a local span of the underlying eigen-space.", "startOffset": 149, "endOffset": 176}, {"referenceID": 11, "context": "Small scale experiments - landmark selection scheme We use the ball dataset as proposed in Duin and Pekalska (2010). It is an artificial dataset based on the surface distances of randomly positioned balls of two classes having a slightly different radius.", "startOffset": 91, "endOffset": 116}, {"referenceID": 11, "context": "Small scale experiments - landmark selection scheme We use the ball dataset as proposed in Duin and Pekalska (2010). It is an artificial dataset based on the surface distances of randomly positioned balls of two classes having a slightly different radius. The dataset is non-Euclidean with substantial information encoded in the negative part of the eigenspectrum. We generated the data with 100 samples per class leading to an N \u00d7 N dissimilarity matrix D, with N = 200. We also use the protein data (213 pts, 4 classes) set represented by an indefinite similarity matrix, with a high intrinsic dimension Schleif and Tino (2015). Further we analyzed two simulated metric datasets which are not linear separable using the Euclidean norm: (1) the checker board data, generated as a two dimensional dataset with datapoints organized on a 3\u00d73 checkerboard, with alternating labels.", "startOffset": 91, "endOffset": 630}, {"referenceID": 3, "context": "Two dimensional visualizations of the unapproximated K \u00b7 K> similarity matrices obtained by using laplacian eigenmaps Belkin and Niyogi (2003). are shown in Figure 1.", "startOffset": 118, "endOffset": 143}, {"referenceID": 37, "context": "c) the matrix is Nystr\u00f6m approximated using the approach of Zhang and Kwok (2010) where the number of landmarks is taken from the MEB solution (SIM3),", "startOffset": 60, "endOffset": 82}, {"referenceID": 37, "context": "d) using the approach of Zhang and Kwok (2010) but with C landmarks where C is the number of classes (SIM4)", "startOffset": 25, "endOffset": 47}, {"referenceID": 5, "context": "f) using an entropy based selection as proposed in Brabanter et al. (2010) (SIM6) 9 where the number of landmarks is again taken from the MEB solution", "startOffset": 51, "endOffset": 75}, {"referenceID": 35, "context": "Williams and Seeger (2000) ).", "startOffset": 0, "endOffset": 27}, {"referenceID": 37, "context": "shown in Table 1 the approach by Zhang and Kwok (2010) is typically effective for a large variety of datasets also with indefinite kernels, given the number of landmarks is reasonable large and discriminating information is sufficiently provided in the dominating eigenvectors of the cluster solutions.", "startOffset": 33, "endOffset": 55}, {"referenceID": 37, "context": "shown in Table 1 the approach by Zhang and Kwok (2010) is typically effective for a large variety of datasets also with indefinite kernels, given the number of landmarks is reasonable large and discriminating information is sufficiently provided in the dominating eigenvectors of the cluster solutions. For the protein data we observe similar results and the proposed approach, the k-means strategy and the entropy approach are effective. SIM4 and SIM5 is again substantially worse because four landmarks are in general not sufficient to represent these data from a discriminative point of view. For the checker board and Gaussian data SIM2 and SIM3 are again close and SIM4 and SIM5 are substantially worse using only two landmark points. The entropy approach was efficient only for the Gaussian data, but failed for Checker which may be attributed to the strong multi-modality of the data. The runtimes shown in Table 2 show already for the small data examples that the MEB approach is much faster then k-means or the entropy approach if the number of points gets larger which was already expected from the theoretical runtime complexity of these algorithms. In another small experiment we analyzed the effect of the k-means based landmark selection Zhang and Kwok (2010) in more detail.", "startOffset": 33, "endOffset": 1274}, {"referenceID": 6, "context": "Finally, in contrast to the original PCVM formulation Chen et al. (2009a), in our notation we explicitly use the data labels - for example, instead of vector \u03a6\u03b8(x) we write \u039e\u03b8(x) \u25e6 y, where \u039e\u03b8(x) is the kernel vector of x without any label information, y is the label vector and \u25e6 is the element-wise multiplication.", "startOffset": 54, "endOffset": 74}, {"referenceID": 15, "context": "From the derivation in Haasdonk and Pekalska (2008) we know, that only the eigenvector of the Nystr\u00f6m approximated kernel matrix based on K\u0302N,m = K\u0302 N,m+K\u0302 \u2212 N,m are needed.", "startOffset": 23, "endOffset": 52}, {"referenceID": 26, "context": "For the Ny-PCVM we obtain a similar analysis as shown in Schleif et al. (2015a) but with extra costs to calculate the Nystr\u00f6m approximated SVD.", "startOffset": 57, "endOffset": 80}, {"referenceID": 5, "context": "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions.", "startOffset": 155, "endOffset": 175}, {"referenceID": 5, "context": "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions.", "startOffset": 155, "endOffset": 205}, {"referenceID": 5, "context": "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions. First we show a small simulated experiment for two Gaussians which exist in an intrinsically two dimensional pseudo-Euclidean space R. The plot in Figure 5 shows a typical result for the obtained decision planes using the iKFD or Ny-iKFD. The Gaussians are slightly overlapping and both approaches achieve a good separation with 93.50% and 88.50% prediction accuracy, respectively. Subsequently we consider a few public available datasets for some real life experiments. The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al.", "startOffset": 155, "endOffset": 914}, {"referenceID": 5, "context": "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions. First we show a small simulated experiment for two Gaussians which exist in an intrinsically two dimensional pseudo-Euclidean space R. The plot in Figure 5 shows a typical result for the obtained decision planes using the iKFD or Ny-iKFD. The Gaussians are slightly overlapping and both approaches achieve a good separation with 93.50% and 88.50% prediction accuracy, respectively. Subsequently we consider a few public available datasets for some real life experiments. The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al.", "startOffset": 155, "endOffset": 973}, {"referenceID": 4, "context": "The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al. (2003), (version 10/2010, reduced to prosite labeled classes with at least 100 entries ).", "startOffset": 264, "endOffset": 288}, {"referenceID": 4, "context": "The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al. (2003), (version 10/2010, reduced to prosite labeled classes with at least 100 entries ). Further we used the Sonatas data (1068pts, 5 classes) taken from Mokbel et al. (2009). All data are processed as indefinite kernels and the landmarks are selected using the respective landmark selection schemes.", "startOffset": 264, "endOffset": 457}, {"referenceID": 5, "context": "The entropy approach is similar efficient than the k-means strategy but more costly due to the iterative optimization of the landmark set and the respective eigen-decompositions (see Brabanter et al. (2010)).", "startOffset": 183, "endOffset": 207}, {"referenceID": 9, "context": "In future work it could be interesting to incorporate sparsity concepts into iKFD and NyiKFD similar as shown for classical KFD in Diethe et al. (2009).", "startOffset": 131, "endOffset": 152}], "year": 2016, "abstractText": "Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nystr\u00f6m approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data.", "creator": "LaTeX with hyperref package"}}}