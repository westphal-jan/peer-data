{"id": "1604.02354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Bayesian Neighbourhood Component Analysis", "abstract": "Learning a good distance metric in feature space potentially improves the performance of the KNN classifier and is useful in many real-world applications. Many metric learning algorithms are however based on the point estimation of a quadratic optimization problem, which is time-consuming, susceptible to overfitting, and lack a natural mechanism to reason with parameter uncertainty, an important property useful especially when the training set is small and/or noisy. To deal with these issues, we present a novel Bayesian metric learning method, called Bayesian NCA, based on the well-known Neighbourhood Component Analysis method, in which the metric posterior is characterized by the local label consistency constraints of observations, encoded with a similarity graph instead of independent pairwise constraints. For efficient Bayesian optimization, we explore the variational lower bound over the log-likelihood of the original NCA objective. Experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and/or from challenging training set with labels contaminated by errors. The proposed method is also shown to outperform a previous pairwise constrained Bayesian metric learning method.", "histories": [["v1", "Fri, 8 Apr 2016 13:35:03 GMT  (2249kb,D)", "http://arxiv.org/abs/1604.02354v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dong wang", "xiaoyang tan"], "accepted": false, "id": "1604.02354"}, "pdf": {"name": "1604.02354.pdf", "metadata": {"source": "CRF", "title": "Bayesian Neighbourhood Component Analysis", "authors": ["Dong Wang", "Xiaoyang Tan"], "emails": ["(x.tan@nuaa.edu.cn)."], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. PRELIMINARY", "text": "Assuming that we have a data set D with N data points denoted as D = {xi, yi}, i = 1, 2, 3,..., N, where yi is the denomination of the i-th data point xi. In distance metric learning, we aim to learn a Mahalanobis matrix - A using some form of supervision information. Mahalanobis distance metric measures the square distance between two data points xi and xj as follows: d2A (xi, xj) = (xi \u2212 xj) TA (xi \u2212 xj) (1), where A \u2265 0 is a positive semi-defined matrix and xi, xj \u0430Rd is a pair of samples (i, j). For simplicity's sake, we refer to d2A (xi, xj) as d2Aij. With these notations, we give a brief overview of two state-of-the-art works closely related to ours in learning a mahalanobis metric, i.e. [16] and Nyepasian."}, {"heading": "A. Neighbourhood Component Analysis", "text": "The NCA algorithm [16] begins by constructing a complete graph with each data point as its node. Let the weight of each edge between any two nodes called Pij be considered the probability that the data point xi xj selects as its neighbor and can be calculated as follows: pij = exp (\u2212 d2Aij) \u2211 t-Ni exp (\u2212 d 2 Ait) (2), where Ni denotes the set of neighbors of xi. It can be checked whether Pij \u2265 0 and \u2211 j-Ni are pij = 1, and therefore Pij is a valid measure of probability. The goal of the NCA is then to learn a linear transformation A that maximizes the probability that each data point after the transformation selects the points with the same designations as itself as neighbors, i.e. max L (A) = \u2211 i log (\u2211 j-Ni = yj} \u00b7 pij) (3)."}, {"heading": "B. Pairwise Constrained Bayesian Metric Learning", "text": "Yang et al. [15] proposed a Bayesian Method of Metric Learning (BML), which estimates the posterior distribution of distance metrics from the constraints designated as pairs, defining the probability for two data points xi and xj to form an equivalence or inequality condition under a particular distance metric A. P (yij | xi, xj, A, \u00b5) = 11 + exp (yij (d2Aij \u2212 \u00b5))) (4), where yij = {+ 1 (xi, xj) \u0445S \u2212 1 (xi, xj) \u0445D (5) In the definition above, S and D respectively denote the sets of equivalence or inequality constraints. In view of this, the posterior distribution of metric A and the threshold should be estimated by maximizing the following objective: L (A, \u00b5) = (i, j) P (yij, \u00b5) p (\u00b5) p (\u00b5) (6)."}, {"heading": "III. BAYESIAN NEIGHBOURHOOD COMPONENT ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. The Proposed Method", "text": "After, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after, after,"}, {"heading": "B. Distance Estimation", "text": "For conclusions, we are interested in the expectation of point-to-point distance d2\u03b3ij for a new data pair (i, j) according to the posterior distribution of \u03b3, which is a Gaussian distribution, as shown above. In particular, we have, d2\u03b3ij \u0445 N (d 2 \u03b3ij | mij, \u03c3 2 ij) (25), where mij = (wij) TmT\u03c32ij = (wij) TVTwij (26) It is worth mentioning rather that this mechanism of output uncertainty in the remote metric calculation is potentially 4 algorithm 1 Bayesian Neighbourhood Component Analysis. Input: training set {(xi, yi) | i = 1, 2, N}, previous distribution N (g | m0, V0); output: subordinate distribution T (v0); output: subordinate distribution N (g | mT) - training stage1."}, {"heading": "C. Prediction under Parameter Uncertainty", "text": "Under the difficult condition of small training samples or samples with marking noise, a single estimate of parameter A tends to be unreliable, and the traditional DML methods based on it can lead to excessive reliance on future predictions. In other words, they only make predictions, but cannot say whether these predictions make sense. In contrast, this is really not a problem for Bayesian methods, because no errors would be made due to the inaccurate estimate of A. In particular, the prediction for a never-before-seen sample xi can be derived from p (yi | xi, YNi, X). Remember that the variation of the metric parameter \u03b3 is a Gaussian distribution, i.e. q (\u03b3) = N (g | mT, V T) (c.f., eq. (23) & eq. (22)), we have p (yi | xi, XNi, XNi) = quot."}, {"heading": "IV. ANALYSIS OF THE PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Adaptive Sample Selection in Learning", "text": "In the process of metric learning, it is advantageous to use the local property of samples in the entrance room to improve learning efficiency. (NCA) The NCA algorithms are used as an example. (NCA algorithm) The NCA algorithms are calculated as follows: (30) That is, for each dot xi, if and only if all its closest neighbors have the same labels as those of xi, then we will use the Ni algorithms of xi, which means that the gradients to ~ 0. Hence the NCA algorithms would pay more attention to those points whose labels are incompatible with their closest neighbors. In other words, not all pairs (xi, xj) are active in the search space of the transformation matrix in the same way. Similar observations can be made in other NCA models."}, {"heading": "B. Robustness against Label Noise", "text": "In fact, we are going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position where we are."}, {"heading": "C. Computational Cost", "text": "To analyze the calculation costs of the proposed method, first of all, note that the most time-consuming step in the Laplace approximation or a conjugate gradient method is usually related to the calculation of the Hessian matrix. In each iteration, it requires o (N3) computational costs (where N is the number of training data, e.g. if N is 103, the computational costs could be as high as 109). Whereas in our case the Hessian matrix becomes a constant matrix (c.f., eq. (18)) thanks to the Bohning approximation, which is calculated only at once. Furthermore, in NCA and other point-based metric learning methods, its goals are usually optimized by applying some gradient-based methods, such as stochastic gradient departure [28] or conjugated gradient departure [29], etc. These methods usually require a number of iterations to perform local searches (worse, if they do not actually encounter CA-q problems)."}, {"heading": "V. EXPERIMENTS", "text": "To verify the effectiveness of the proposed method, in this section we first compare the robustness performance of our method with several related DML methods on the data sets either with small sample size or with label noise, then we turn to an in-depth study of the behavior of the proposed method."}, {"heading": "A. Experimental Settings", "text": "We compare the performance of the proposed method with several other closely related DML methods, including NCA [16] q, LMNN [17], metric learning for the mean of the next class [1] (NCM), and pairwise limited Bayesian metric learning [15] (BML). Both the NCA and LMNN are metric learning methods with graphical limitations, and the NCA is the method on which our method is based and therefore chosen as the basic algorithm. Like the BML method proposed by Yang et al. [15], it is also a Bayesian method (but with picturesque limitations on learning. Finally, we have adopted an unmonitored latent characteristic learning method, i.e., the main component analysis [30] (PCA) for comparison, as it is completely irrelevant to the issue of labeling noise.1 For all methods except the NCM (which has its own classifier), we have used the comparative method similar to the NK-N method."}, {"heading": "B. Learning from Small Size Training Set", "text": "First, we examine the performance of our small sample size method using three UCI data sets (\"Balance,\" \"Ionosphere,\" and \"Spambase\"), each using an additional subset of 100 data points as a test set. Table I indicates the classification performance. If the training set is small, point-based methods tend to be unreliable; with only 10 training samples, the standard NCA performs even worse than the Basic Approach (PCA) on two of the three data sets tested. In contrast, the proposed BNCA performs best among the comparison methods, in part because of the benefit of the Bayesian framework, which provides potentially reliable estimates even if the size of training data is small. Table I also shows that as the number of training points increases, the performance of all the methods considered here can be significantly improved."}, {"heading": "C. Learning Under Random Label Noise", "text": "To test the performance of our method under the Noise label, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]) and face recognition (on the FRGC-2 [33]). The adopted datasets are popular benchmarks for each of the tasks or: 1) Caltech-10 is a subset sampled by Caltech-256 datasets [34]. The training set contains 300 images (30 from each class) and the test set is another one that is randomly sampled 300 images; 2) The MNIST datasets contain 600 digit images sampled from each class. (60 from each class; Training / Test: 300 / 300) The FRGC datasets we use contain 400 facial images from 20 subjects (20 images per person)."}, {"heading": "D. Predictive Performance under Difficult Conditions", "text": "In recent years, the number of people living in the US has multiplied, both in the US and in Europe."}, {"heading": "E. Discussions", "text": "1) Robustness against overfitting: To examine the behavior of the proposed BNCA method more closely, we record the learning curves of BNCA and NCA as a function of the number of iterations in Fig. 6, using three sets of data (Caltech10, MNIST, and FRGC), with the same experimental setting as before, and injecting 1030.0% random marking noise for each dataset. For NCA training, we use the conjugate gradient method [29], which strives for the steepest gradient direction with the correct step size in each training step. The figure shows that as the iteration continues, NCA training errors tend to decrease, but their test errors tend to increase at the same time, suggesting that the method can be easily overhauled under the condition of marking noise. Although some empirical tricks, such as stopping imports early, can be adopted, the mapping clearly shows that this is no real problem for our expansion."}, {"heading": "VI. CONCLUSION", "text": "We introduce a new Bayesian method of metric learning - the Bayesian Neighbourhood Component Analysis (BNCA), which effectively improves the performance of the KNN classifier under the condition of small sample size and / or noisy data labels11. Based on the classic NCA method of point estimation, it extends it for the first time within the Bayesian framework. BNCA's major advantages over NCA in the field of remote learning are three folds: 1) It is easy to train without worrying about overhauling; 2) it behaves more robustly compared to NCA under difficult conditions; 3) it naturally handles label noise by reducing the influence of data points with possible labeling errors. In addition, we introduce a new variable lower limit on the probability of the object in order to improve the efficiency of Bayesian learning; 3) it naturally handles label noise by reducing the influence of data points with possible labeling errors; and we introduce a new variable lower limit on the probability of the object in order to improve the efficiency of Bayesian learning. Extensive experiments carried out on several challenging applications in the real world, the NCA methodology significantly exceeds the level of accuracy of the NCA's proposed method, and others demonstrate that the BCA methodology meets the state of the NCA's current performance."}], "references": [{"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Computer Vision\u2013ECCV 2012. Springer, 2012, pp. 488\u2013501.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning distance metrics with contextual constraints for image retrieval", "author": ["S.C. Hoi", "W. Liu", "M.R. Lyu", "W.-Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2. IEEE, 2006, pp. 2072\u20132078.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Ordinal distance metric learning for image ranking", "author": ["C. Li", "Q. Liu", "J. Liu", "H. Lu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 7, pp. 1551\u20131559, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple instance metric learning from automatically labeled bags of faces", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision\u2013 ECCV 2010. Springer, 2010, pp. 634\u2013647.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["J. Lu", "X. Zhou", "Y.-P. Tan", "Y. Shang", "J. Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 2, pp. 331\u2013345, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive distance metric learning for clustering", "author": ["J. Ye", "Z. Zhao", "H. Liu"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20137.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Pedestrian recognition with a learned metric", "author": ["M. Dikmen", "E. Akbas", "T.S. Huang", "N. Ahuja"], "venue": "Computer Vision\u2013ACCV 2010. Springer, 2011, pp. 501\u2013512.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 498\u2013505.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 309\u2013316.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2288\u20132295.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Constrained empirical risk minimization framework for distance metric learning", "author": ["W. Bian", "D. Tao"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 8, pp. 1194\u20131205, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating privileged information through metric learning", "author": ["S. Fouad", "P. Tino", "S. Raychaudhury", "P. Schneider"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 7, pp. 1086\u20131098, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. Van Den Hengel"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 2, pp. 394\u2013406, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A distributed approach toward discriminative distance metric learning", "author": ["J. Li", "X. Lin", "X. Rui", "Y. Rui", "D. Tao"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 9, pp. 2111\u20132122, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian active distance metric learning", "author": ["L. Yang", "R. Jin", "R. Sukthankar"], "venue": "Proceedings of the Twenty-Third Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-07). Corvallis, Oregon: AUAI Press, 2007, pp. 442\u2013449.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2004, pp. 513\u2013520.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in neural information processing systems, 2005, pp. 1473\u20131480.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust distance metric learning in the presence of label noise", "author": ["D. Wang", "X. Tan"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward efficient agnostic learning", "author": ["M.J. Kearns", "R.E. Schapire", "L.M. Sellie"], "venue": "Machine Learning, vol. 17, no. 2-3, pp. 115\u2013141, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Sch\u00f6lkopf"], "venue": "ICML. Citeseer, 2001, pp. 306\u2013313.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting parallel perceptrons for label noise reduction in classification problems", "author": ["I. Cantador", "J.R. Dorronsoro"], "venue": "Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach. Springer, 2005, pp. 586\u2013593.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "A novel noise filtering algorithm for imbalanced data", "author": ["J. Van Hulse", "T.M. Khoshgoftaar", "A. Napolitano"], "venue": "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on. IEEE, 2010, pp. 9\u201314.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Handling label noise in video classification via multiple instance learning", "author": ["T. Leung", "Y. Song", "J. Zhang"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2056\u20132063.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Label-noise reduction with support vector machines", "author": ["S. Fefilatyev", "M. Shreve", "K. Kramer", "L. Hall", "D. Goldgof", "R. Kasturi", "K. Daly", "A. Remsen", "H. Bunke"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3504\u20133508.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification in the presence of label noise: a survey", "author": ["B. Fr\u00e9nay", "M. Verleysen"], "venue": "2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "An introduction to mcmc for machine learning", "author": ["C. Andrieu", "N. De Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine learning, vol. 50, no. 1-2, pp. 5\u201343, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer, 2010, pp. 177\u2013 186.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "1994.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Principal component analysis", "author": ["S. Wold", "K. Esbensen", "P. Geladi"], "venue": "Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp. 37\u201352, 1987.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1987}, {"title": "Distance metric learning using dropout: a structured regularization approach", "author": ["Q. Qian", "J. Hu", "R. Jin", "J. Pei", "S. Zhu"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 323\u2013332.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Overview of the face recognition grand challenge", "author": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "J. Chang", "K. Hoffman", "J. Marques", "J. Min", "W. Worek"], "venue": "Computer vision and pattern recognition, 2005. CVPR 2005. IEEE computer society conference on, vol. 1. IEEE, 2005, pp. 947\u2013954.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised feature learning with c-svddnet", "author": ["D. Wang", "X. Tan"], "venue": "Eprint Arxiv, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "The recently proposed pairwise constrained Bayesian metric learning method (BML) [15] overcomes some of the above issues by taking the prior distribution of the transformation matrix into account.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Graph constraints can be regarded as the extension to pairwise constraints ([15], [10]) by simultaneously building the", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Graph constraints can be regarded as the extension to pairwise constraints ([15], [10]) by simultaneously building the", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "One typical metric learning method using graph constraints is the well-known Neighbourhood Component Analysis (NCA) [16], which is conceptually simple and is developed under a well-formulated probabilistic framework.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 135, "endOffset": 138}, {"referenceID": 17, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 163, "endOffset": 167}, {"referenceID": 18, "context": "This problem has been previously studied under the umbrella of agnostic learning [19], in which the relationship between the label and the data is largely relaxed.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": ", NCA [16] and pairwise constrained Bayesian metric learning [15].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": ", NCA [16] and pairwise constrained Bayesian metric learning [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Neighbourhood Component Analysis The NCA algorithm [16] begins by constructing a complete graph with each data point as its node.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "[15] proposed a Bayesian metric learning (BML) method that estimates the posterior distribution for the distance metric from labeled pairwise constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "with an active learning method for data pair selection [15], but the computational cost remains high.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Inspired by [15], we approximate A using the first d eigenvectors, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Using Bohning\u2019s quadratic bound (see [26] page 758, section.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Instead we adopt a MCMC method [27] to approximate this expectation:", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Similar observations can be made in other NCA extensions, such as the Large Margin NN (LMNN [17]) method:", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": ", [15], in the sense that they usually lack an automatic sample selection mechanism in the metric learning objective by itself (c.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "[15] designed an effective active learning method to select the most uncertainty pairs in training process, but the algorithm still needs to compute and store all the pairs\u2019 uncertainty scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Furthermore, in NCA and other point based metric learning methods their objective are commonly optimized using some gradient descent based method, such as stochastic gradient descent [28] or conjugate gradient descend [29], etc.", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Furthermore, in NCA and other point based metric learning methods their objective are commonly optimized using some gradient descent based method, such as stochastic gradient descent [28] or conjugate gradient descend [29], etc.", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 190, "endOffset": 193}, {"referenceID": 14, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 251, "endOffset": 255}, {"referenceID": 14, "context": "[15] is a Bayesian method as well, but with pairwise constraints for learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", Principal Component Analysis [30] (PCA) for comparison, since it is completely irrelevant to the issue of label noise.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 206, "endOffset": 210}, {"referenceID": 31, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 247, "endOffset": 251}, {"referenceID": 32, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 290, "endOffset": 294}, {"referenceID": 33, "context": "The datasets adopted are popular benchmark on each of the task respectively: 1) Caltech-10 is a subset sampled from Caltech-256 image dataset [34] with 10 most popular categories.", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "We conducted this series of experiments using MNIST [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "e, the C-SVDDNet [35], as the expert to choose samples, and those samples close to the decision boundary of C-SVDDNet would be regarded as difficult samples otherwise as normal samples.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Also note that since the pairwise constrained BML method [15] only estimates whether a date pair belongs to the same class or not, while not being able to give the predictive distribution p(yi|xi), it is not included for comparison here.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "For NCA training, we used the conjugate gradient method [29], which seeks the steepest gradient direction with proper step size in each training step.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "methods exploit various low-rank approximation [16] [1] to it without using any prior.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "methods exploit various low-rank approximation [16] [1] to it without using any prior.", "startOffset": 52, "endOffset": 55}], "year": 2016, "abstractText": "Learning a good distance metric in feature space potentially improves the performance of the KNN classifier and is useful in many real-world applications. Many metric learning algorithms are however based on the point estimation of a quadratic optimization problem, which is time-consuming, susceptible to overfitting, and lack a natural mechanism to reason with parameter uncertainty, an important property useful especially when the training set is small and/or noisy. To deal with these issues, we present a novel Bayesian metric learning method, called Bayesian NCA, based on the well-known Neighbourhood Component Analysis method, in which the metric posterior is characterized by the local label consistency constraints of observations, encoded with a similarity graph instead of independent pairwise constraints. For efficient Bayesian optimization, we explore the variational lower bound over the log-likelihood of the original NCA objective. Experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and/or from challenging training set with labels contaminated by errors. The proposed method is also shown to outperform a previous pairwise constrained Bayesian metric learning method.", "creator": "LaTeX with hyperref package"}}}