{"id": "1604.02376", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "abstract": "In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels.", "histories": [["v1", "Fri, 8 Apr 2016 15:33:30 GMT  (451kb,D)", "http://arxiv.org/abs/1604.02376v1", null], ["v2", "Fri, 22 Apr 2016 23:16:20 GMT  (668kb,D)", "http://arxiv.org/abs/1604.02376v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jyothi korra"], "accepted": false, "id": "1604.02376"}, "pdf": {"name": "1604.02376.pdf", "metadata": {"source": "CRF", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "authors": ["Jyothi Korra"], "emails": ["jyothi.korra@christuniversity.in"], "sections": [{"heading": null, "text": "In Computer Vision, the problem of identifying or classifying the objects present in an image is called object categorization. It is a difficult problem, especially when the images have a confusing background, occlusion, or different lighting conditions. Many visual functions have been suggested that support object categorization even under such adverse conditions. Previous research has shown that using multiple features instead of individual features leads to better recognition. Multiple Kernel Learning (MKL) framework was developed to learn an optimal combination of features for object categorization. Existing MKL methods use linear combinations of basic knowledge that may not be optimal for object categorization. Object categorization in the real world may need to consider complex (non-linear) combinations, not just linear combinations."}, {"heading": "1 Introduction", "text": "Face recognition is a less difficult problem than object categorization. There are systems in which computers can automatically identify expressions and morphic faces. In some cases, a higher level of detail in the description of a feature is necessary to solve the problem, but this comes at the cost of handling more data and more sophisticated processing. In this report, an instance of feature representation is considered a critical problem. In other cases, a higher level of detail in the description of a feature is considered necessary."}, {"heading": "2 Organization of this manual", "text": "The summary of the report is as follows: Section 3 discusses the work done so far in this area. Section 7 briefly discusses genetic programming and its benefits. Section 4 describes the methodology for performing object categorization using genetic programming using non-linear nuclei. Section 7 describes implementation details and results on two sets of data. This is followed by the conclusion."}, {"heading": "3 Past Work", "text": "There has been some work in genetic programming for the development of cores to support the vector machine [7, 8, 9]. [9] uses genetic programming for the development of the core for the SVM classifier. The approach presented there combines the two techniques of SVMs and GP to develop a kernel for an SVM. The aim is to eliminate the need to test different kernels and their parameter settings. They claim that the approach may also be possible to discover new kernels that are particularly useful for the type of data under analysis, showing that their method is better than manually selecting the kernel and adjusting parameters. [8] uses a number of standard cores for the development of the kernel that solve a particular problem with genetic programming."}, {"heading": "4 Optimal Combination of Kernels using Genetic Programming", "text": "In order to facilitate object categorization under unfavorable conditions, many descriptors (and examples of character representation) have been proposed in the computer visualization literature. Suppose there are n descriptors d1, d2,..., dn extracted from m-images I1, I2,..., Im. On the basis of these n descriptors, n nuclei K1, K2,..., Kn of size mXm are formed. Steps for producing the hybrid classifier using GP and SVM are described below."}, {"heading": "5 Steps Involved", "text": "\u2022 Create a random population of core functions, represented as trees \u2022 Evaluate the fitness of each individual by creating an SVM from the core tree and testing it against the validation data \u2022 Select the fitter core trees as parents for the recombination \u2022 Perform random crossings and mutations on the newly created offspring \u2022 Replace the old population with the offspring \u2022 Repeat steps 2 to 5 until the population converges \u2022 Create a definitive SVM with the fittest core tree found by GP"}, {"heading": "6 Parameters for GP", "text": "The parameters for the GP include the definition of terminal set, functional set and fitness function. Terminal set = {K1, K2, K3,..., Kn}, where Ki is the kernel formed from one of the descriptors. Function set = {+, \u043a}. Fitness function is the classification error of the respective chromosome on the training set. This is the fitness value for each chromosome in this GP based on the accuracy of the SVM with this chromosome (the non-linear kernel combination given to SVM). An alternative is to base fitness on a cross-validation test (e.g. leave-one-out cross-validation) to give a better assessment of the ability of a core tree to generate a model that generalizes well on invisible data."}, {"heading": "7 Results", "text": "The proposed idea was validated using real object data sets such as Caltech-5 and Caltech-101. Caltech-5 contains five classes of objects cars, airplanes, faces, leopards and bicycles. Caltech-101 contains 101 categories of objects. Each category contains approximately 30-100 images. The accuracy of the proposed method is compared with the best kernel (K1, K2,..., Kn) and the additional kernel, which is K1 + K2 +... + Kn. All experiments follow the 1-Vs-1 SVM classification method. The binary classification mentioned in the following section is performed by occupying two classes simultaneously and finding the accuracy in these two classes."}, {"heading": "8 Results on Caltech-5 dataset", "text": "This section presents results on Caltech-5 using new MKL formulations and descriptors (Csift, Gegensift, Rgsift, Sift, transformedcolorsift) provided by the ColorDescriptor software. Experiments in this section are performed with descriptors provided by the ColorDescriptor software Caltech-5. The dataset contains images of airplanes, cars, faces, leopards and bicycles. We have created cores on 5 descriptors provided with the Gaussian kernel. This experimental procedure has been repeated 10 times with different training test data splits. Table 1 shows that the non-linear kernel method provides better accuracy than the best kernel and the addition of cores. Figure 6 shows the mean accuracy as the number of iterations. Note that in all iterations the proposed non-linear kernel combination method is better than other kernel combinations."}, {"heading": "9 Results on Caltech-101 dataset", "text": "This section presents results for Caltech-101 with new nonlinear kernel combinations, and six cores are taken from the ucsd dataset. We have taken 30 images for each class, 15 of which are taken at random as training, taking 5 for validation data and the rest as test data. This experimental procedure has been repeated 5 times with different training and test data splits. Table 1 shows that the nonlinear kernel method provides better accuracy compared to the best kernel and the addition of cores. Figure 13 shows the mean accuracy as the number of iterations in the Caltech 101 dataset. Note that in all proposed nonlinear iterations, the kernel is the best. Figure 14 shows the nonlinear kernel tree generated from GP for Caltech101 dataset. This kernel tree is nothing more than the additional kernel generated for iteration 4 in Caltech101 (see Figure 13), where non-linear GP kernels provide almost identical additional and non-linear GP kernel accuracy."}, {"heading": "10 Genetic Programming for Similar Images", "text": "Generated cores from the previous sections are used to create a demonstration for similar images. 1530 images from Caltech-101 were taken by us and the cores are generated using the methods discussed above. A matrix M is created which has the dimension 1530 x 1530. M (i, j) refers to the similarity between the ith image and the jth image. Five descriptors are generated for the images and therefore we have five cores. If the addition of cores is used, the cores are combined linearly to find M. SoM = K1 + K2 + K3 + K4 + K5."}, {"heading": "11 Conclusions", "text": "This paper proposed a non-linear nuclear combination using genetic programming, eliminating the need for the user to create non-linear nuclear combinations and applying the proposed framework to object categorization. Experimental results show that the proposed framework for non-linear nuclear combinations with GP performance is better than existing modern nuclear combinations."}], "references": [{"title": "Application of active appearance model to automatic face replacement", "author": ["D. Govindaraj"], "venue": "Journal of Applied Statistics, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Moneybee: Towards enabling a ubiquitous, efficient, and easy-to-use mobile crowdsourcing service in the emerging market", "author": ["D. Govindaraj", "N.K.V.M.", "A. Nandi", "G. Narlikar", "V. Poosala"], "venue": "Bell Labs Technical Journal, vol. 15, no. 4, pp. 79\u201392, 2011. [Online]. Available: http://dx.doi.org/10.1002/bltj.20473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "ICCV, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the discriminative power-invariance trade-off", "author": ["M. Varma", "D. Ray"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["S.N. Jagarlapudi", "D. Govindaraj", "R. S", "C. Bhattacharyya", "A. Ben-tal", "R.K.r."], "venue": "pp. 844\u2013852, 2009. [Online]. Available: http://papers.nips.cc/paper/ 3880-on-the-algorithmics-and-applications-of-a-mixed-norm-based-kernel-learning-formulation.pdf", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Controlled sparsity kernel learning", "author": ["D. Govindaraj", "S. Raman", "S. Menon", "C. Bhattacharyya"], "venue": "CoRR, vol. abs/1401.0116, 2014. [Online]. Available: http://arxiv.org/abs/1401.0116", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Genetically designed multiple-kernels for improving the svm performance", "author": ["L. Diosan", "M. Oltean", "A. Rogozan", "J.P. Pecuchet"], "venue": "GECCO \u201907: Proceedings of the 9th annual conference on Genetic and evolutionary computation. New York, NY, USA: ACM, 2007, pp. 1873\u20131873.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving kernels for support vector machine classification", "author": ["K.M. Sullivan", "S. Luke"], "venue": "GECCO \u201907: Proceedings of the 9th annual conference on Genetic and evolutionary computation. New York, NY, USA: ACM, 2007, pp. 1702\u20131707.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "The genetic evolution of kernels for support vector machine classifiers", "author": ["T. Howley", "M.G. Madden"], "venue": "In 15th Irish Conference on Artificial Intelligence, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse classifier design based on the shapley value", "author": ["P. Ravipally", "D. Govindaraj"], "venue": "Proceedings of the World Congress on Engineering, vol. 1, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling attractiveness and multiple clicks in sponsored search results", "author": ["D. Govindaraj", "T. Wang", "S.V.N. Vishwanathan"], "venue": "CoRR, vol. abs/1401.0255, 2014. [Online]. Available: http://arxiv.org/abs/1401.0255 10", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "There are systems where computer can identify expressions and morph faces automatically [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Labels for classification task are mostly obtained by manual labelling process or through crowd-sourcing [2]", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 3, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 4, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 5, "context": "In [3, 4, 5, 6], the authors employ the Multiple Kernel Learning (MKL) framework to find the optimal combination of feature descriptors (kernels).", "startOffset": 3, "endOffset": 15}, {"referenceID": 6, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 7, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "3 Past Work There have been some work in genetic programming for evolving kernels for Support Vector Machine [7, 8, 9].", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "[9] uses Genetic Programming for evolving the kernel for SVM classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] uses a set of standard kernels for evolving expression for new kernel which performs better for given problem using genetic programming.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] tries to learn a regression function where kernels act as the regression variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This is closely related to [7], where they try to evolve regression function using GP where kernels acts as regression variables.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "[3, 4, 5, 6, 10] considers combining descriptors using multiple kernel learning.", "startOffset": 0, "endOffset": 16}], "year": 2017, "abstractText": "In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels.", "creator": "LaTeX with hyperref package"}}}