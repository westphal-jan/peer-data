{"id": "1604.02416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "How deep is knowledge tracing?", "abstract": "In theoretical cognitive science, there is a tension between highly structured models whose parameters have a direct psychological interpretation and highly complex, general-purpose models whose parameters and representations are difficult to interpret. The former typically provide more insight into cognition but the latter often perform better. This tension has recently surfaced in the realm of educational data mining, where a deep learning approach to predicting students' performance as they work through a series of exercises---termed deep knowledge tracing or DKT---has demonstrated a stunning performance advantage over the mainstay of the field, Bayesian knowledge tracing or BKT. In this article, we attempt to understand the basis for DKT's advantage by considering the sources of statistical regularity in the data that DKT can leverage but which BKT cannot. We hypothesize four forms of regularity that BKT fails to exploit: recency effects, the contextualized trial sequence, inter-skill similarity, and individual variation in ability. We demonstrate that when BKT is extended to allow it more flexibility in modeling statistical regularities---using extensions previously proposed in the literature---BKT achieves a level of performance indistinguishable from that of DKT. We argue that while DKT is a powerful, useful, general-purpose framework for modeling student learning, its gains do not come from the discovery of novel representations---the fundamental advantage of deep learning. To answer the question posed in our title, knowledge tracing may be a domain that does not require `depth'; shallow models like BKT can perform just as well and offer us greater interpretability and explanatory power.", "histories": [["v1", "Mon, 14 Mar 2016 04:20:55 GMT  (273kb,D)", "http://arxiv.org/abs/1604.02416v1", "8 pages, 2 figures"], ["v2", "Tue, 21 Jun 2016 04:51:22 GMT  (55kb,D)", "http://arxiv.org/abs/1604.02416v2", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["mohammad khajah", "robert v lindsey", "michael c mozer"], "accepted": false, "id": "1604.02416"}, "pdf": {"name": "1604.02416.pdf", "metadata": {"source": "CRF", "title": "How Deep is Knowledge Tracing?", "authors": ["Mohammad Khajah", "Robert V. Lindsey", "Michael C. Mozer"], "emails": ["mohammad.khajah@colorado.edu", "robert.lindsey@colorado.edu", "mozer@colorado.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves without being able to survive themselves. In fact, it is the case that they are able to survive themselves, and that they are able to survive themselves. In fact, it is the case that they are not able to survive themselves, and that they are able to survive themselves."}, {"heading": "1.1 Modeling Student Learning", "text": "The area we are looking at is electronic tutoring systems that use cognitive models to track and evaluate students \"knowledge. Belief in what a student knows and what he does not know allows a tutoring system to dynamically adjust its feedback and instructions to optimize the depth and efficiency of learning.Ultimately, the measure of learning is how well students are able to apply skills they have been taught. Consequently, student modeling is often formulated as predicting time series: given the series of exercises a student has previously attempted, and the student's success or failure in each exercise, predicts how the student will fare in a new exercise. Formally, the data consists of a set of binary random variables that indicate whether the student is responding correctly to the study, {Xst}. Data also includes the exercise labels, {Yst} which characterize the exercise."}, {"heading": "1.2 Knowledge Tracing", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "1.3 Where Does BKT Fall Short?", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. EXTENDING BKT", "text": "In the previous section, we described four regularities that appear to be present in the data and which we suspect DKT exploits, but the classic CCT model does not. In this section, we describe three extensions of the CCT that would equate the CCT with the CCT in terms of these regularities."}, {"heading": "2.1 Forgetting", "text": "In order to better understand the effects of relapse, CCTC can be extended to allow the forgetting of skills. Forgetting corresponds to the adaptation of a CCT parameter F \u2261 P (Ks, i + 1 = 0 | Ksi = 1), the probability of the transition from a state of knowledge to a state of ignorance of a skill. In the standard CCT, F = 0.Without forgetting, if CCT suggests that the student has learned, even a long series of poorly executed studies cannot change the derived level of knowledge. However, the level of knowledge can go both ways, allowing the model to be more sensitive to recent studies: a run of unsuccessful studies indicates that the skill is not known, regardless of what preceded the course. Forgetting is not a new idea of CCT, and in fact it has been incorporated into the original psychological theory underlying the notion of a binary state of knowledge [1]."}, {"heading": "2.2 Skill Discovery", "text": "To model interactions between skills, one might assume that each skill has some influence on learning other skills, not unlike the link between the hidden units in CCT. For CCT to allow such interactions between skills, independent CCT models would have to be linked using a skill such as a factorally hidden Markov model [5]. As an alternative to this somewhat complex approach, we explored a simpler scheme in which different practice labels could be broken down to form a single skill. For example, if we look at an exercise technique such as A1-B1-C1-B2-C2-C3, it would be more sensible to treat these skills in a way that brings A and B together into a single skill than to train a single skill, and to train a single CCT institute based on A and B studies."}, {"heading": "2.3 Incorporating Latent Student-Abilities", "text": "In order to take into account the individual variation in students \"abilities, we have expanded CCTC [12, 11] so that the probabilities of slip-ups and guesses are modulated by a latent ability parameter derived from the data, in the spirit of item-response theory [4]. Students with stronger abilities have lower probabilities of slip-ups and higher probabilities of guesswork. When the model is presented to new students, the posterior predictive distribution of abilities is used first, but when the new student's answers are observed, uncertainty about the student's abilities is reduced, resulting in better predictions for the student."}, {"heading": "3. SIMULATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data Sets", "text": "Piech et al. [20] examined three sets of data. One of the sets, from Khan Academy, is not publicly available. Despite our requests and a plea from one of the co-authors of the DKT work, we were unable to obtain permission from the Data Science Team at Khan Academy to use the data set. We examined the other two sets of data in Piech et al., which are considered to be the following. Assistments is an electronic tutor that teaches and evaluates students in elementary math. The 2009-2010 data set \"Skill Builder\" is a large, standard benchmark, available by searching the web for assistance 2009-2010 data. We used the train / test split from Piech et al., and following Piech et al., we discarded all students who had only a single study of data. Synthetically, a synthetic data set created by Piech et al., to perform virtual skills.The virtual exercises are made up of 50 virtual students each, with the ability to model the virtual exercises."}, {"heading": "3.2 Methods", "text": "We evaluated five variants of CCT, each of which contains a different subset of the extensions described in the previous section: a basic version that corresponds to the classic model, and the model against which CCT was evaluated in [20], which we simply call CCT; a version that includes forgetting (CCT + F), a version that includes the discovery of capabilities (CCT + S), a version that includes latent capabilities (CCT + A), and a version that includes all three extensions (CCT + FSA). We also built our own implementation of CCT with recurring units of LSTM. (Piech et al. described the LSTM version as more powerful, but published only the code for the standard version of recurring neural networks.) We confirmed that our implementation produced results that were reported in [20] on assistants and synthetics."}, {"heading": "3.3 Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "4. DISCUSSION", "text": "This year, it will be able to open up a wide range of future perspectives."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "This research was supported by NSF grants SES-1461535, SBE-0542013 and SMA-1041755."}, {"heading": "6. REFERENCES", "text": "[1] R. Atkinson and J. A. Paulson. An approach to thepsychology of instruction. Psychology Bulletin, 78: 49-61, 1972. [2] R. S. Baker, A. T. Corbett, and V. Aleven. [3] A. Corbett and J. Anderson. Knowledge Tracing: Proceedings of the 9th International Conference on Intelligent Tutoring Systems, pp. 406-415, Berlin, Heidelberg, 2008."}], "references": [{"title": "An approach to the psychology of instruction", "author": ["R. Atkinson", "J.A. Paulson"], "venue": "Psychology Bulletin, 78:49\u201361,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1972}, {"title": "More accurate student modeling through contextual estimation of slip and guess probabilities in Bayesian knowledge tracing", "author": ["R.S. Baker", "A.T. Corbett", "V. Aleven"], "venue": "Proceedings of the 9th International Conference on Intelligent Tutoring Systems, pages 406\u2013415, Berlin, Heidelberg,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Knowledge tracing: Modelling the acquisition of procedural knowledge", "author": ["A.T. Corbett", "J.R. Anderson"], "venue": "User Model. User-Adapt. Interact., 4(4):253\u2013278,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Explanatory Item Response Models: a Generalized Linear and Nonlinear Approach", "author": ["P. De Boeck", "M. Wilson"], "venue": "Springer-Verlag, New York, NY,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems 8, pages 472\u2013478. MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "How to optimize student learning using recurrent neural networks (educational technology)", "author": ["R. Golden"], "venue": "Web page, 2016. http://tinyurl.com/GoldenDKT, retrieved February 29,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling skill acquisition over time with sequence and topic modeling", "author": ["J.P. Gonzales-Brenes"], "venue": "S. V. N. V. G. Lebanon, editor, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics. JMLR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 1462\u20131471,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Sequential effects in response time reveal learning mechanisms and event representations", "author": ["M. Jones", "T. Curran", "M.C. Mozer", "M.H. Wilder"], "venue": "Psychological review, 120:628\u2013666,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating knowledge tracing and item response theory: A tale of two frameworks", "author": ["M. Khajah", "Y. Huang", "J.P. Gonzales-Brenes", "M.C. Mozer", "P. Brusilovsky"], "venue": "M. Kravcik, O. C. Santos, and J. G. Boticario, editors, Proceedings of the 4th International Workshop on Personalization Approaches in Learning Environments, pages 7\u201315. CEUR Workshop Proceedings,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating latent factors into knowledge tracing to predict individual differences in learning", "author": ["M. Khajah", "R.M. Wing", "R.V. Lindsey", "M.C. Mozer"], "venue": "J. Stamper, Z. Pardos, M. Mavrikis, and B. M. McLaren, editors, Proceedings of the 7th International Conference on Educational Data Mining, pages 99\u2013106. Educational Data Mining Society Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A data repository for the EDM community: The PSLC DataShop", "author": ["K. Koedinger", "R. Baker", "K. Cunningham", "A. Skogsholm", "B. Leber", "J. Stamper"], "venue": "C. Romero, S. Ventura, M. Pechenizkiy, and R. Baker, editors, Handbook of Educ. Data Mining, http://pslcdatashop.org,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521:436\u2013444,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving student\u2019s long-term knowledge retention with personalized review", "author": ["R. Lindsey", "J. Shroyer", "H. Pashler", "M. Mozer"], "venue": "Psychological Science, 25:639\u201347,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic discovery of cognitive skills to improve the prediction of student learning", "author": ["R.V. Lindsey", "M. Khajah", "M.C. Mozer"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1386\u20131394. Curran Associates, Inc.,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances in Neural Information Processing Systems 4, pages 275\u2013282. Morgan-Kaufmann,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "KT-IDEM: Introducing item difficulty to the knowledge tracing model", "author": ["Z.A. Pardos", "N.T. Heffernan"], "venue": "User Modeling, Adaption and Personalization, pages 243\u2013254. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep knowledge tracing", "author": ["C. Piech", "J. Bassen", "J. Huang", "S. Ganguli", "M. Sahami", "L.J. Guibas", "J. Sohl-Dickstein"], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 505\u2013513. Curran Associates, Inc.,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Does time matter? modeling the effect of time with Bayesian knowledge tracing", "author": ["Y. Qiu", "Y. Qi", "H. Lu", "Z.A. Pardos", "N.T. Heffernan"], "venue": "M. Pechenizkiy, T. Calders, C. Conati, S. Ventura, C. Romero, and J. C. Stamper, editors, Educational Data Mining 2011, pages 139\u2013148. www.educationaldatamining.org,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "The benefit of interleaved mathematics practice is not limited to superficially similar kinds of problems", "author": ["D. Rohrer", "R.F. Dedrick", "K. Burgess"], "venue": "Psychonomic Bulletin and Review, 21:1323\u20131330,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Interleaved practice improves mathematics learning", "author": ["D. Rohrer", "R.F. Dedrick", "S. Stershic"], "venue": "Journal of Educational Psychology, 107:900\u2013908,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural networks, 61:85\u2013117,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Sliver", "A. Huang", "C.J. Maddison", "A. Guez"], "venue": "search. Nature,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "OLI Engineering Statics \u2013 Fall 2011", "author": ["P. Steif", "N. Bier"], "venue": "Feb.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature engineering and classifier ensemble for KDD cup", "author": ["H.-F. Yu", "Others"], "venue": "Technical report, Department of Computer Science and Information Engineering,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, Computer Vision \u2013 ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I, pages 818\u2013833. Springer International Publishing, Cham,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "In the past forty years, machine learning and cognitive science have undergone many paradigm shifts, but few have been as dramatic as the recent surge of interest in deep learning [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "Deep learning underlies state-of-the-art systems in speech recognition, language processing, and image classification [14, 24].", "startOffset": 118, "endOffset": 126}, {"referenceID": 23, "context": "Deep learning underlies state-of-the-art systems in speech recognition, language processing, and image classification [14, 24].", "startOffset": 118, "endOffset": 126}, {"referenceID": 26, "context": "Deep learning also is responsible for systems that can produce captions for images [27], create synthetic images [8], play video games [17] and even Go [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "Deep learning also is responsible for systems that can produce captions for images [27], create synthetic images [8], play video games [17] and even Go [25].", "startOffset": 113, "endOffset": 116}, {"referenceID": 16, "context": "Deep learning also is responsible for systems that can produce captions for images [27], create synthetic images [8], play video games [17] and even Go [25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Deep learning also is responsible for systems that can produce captions for images [27], create synthetic images [8], play video games [17] and even Go [25].", "startOffset": 152, "endOffset": 156}, {"referenceID": 28, "context": ", [29]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "It was inevitable that deep learning would be applied to student-learning data [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "This domain has traditionally been the purview of the educational data mining community, where Bayesian knowledge tracing, or BKT, is the dominant computational approach [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": ", [6]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 19, "context": "[20] reported substantial improvements in prediction performance with DKT over BKT on two realworld data sets (Assistments, Khan Academy) and one synthetic data set which was generated under assumptions that are not tailored to either DKT or BKT.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Secondary data has also been incorporated in models, including the student\u2019s utilization of hints, response time, and characteristics of the specific exercise and the student\u2019s particular history with related exercises [2, 28].", "startOffset": 219, "endOffset": 226}, {"referenceID": 27, "context": "Secondary data has also been incorporated in models, including the student\u2019s utilization of hints, response time, and characteristics of the specific exercise and the student\u2019s particular history with related exercises [2, 28].", "startOffset": 219, "endOffset": 226}, {"referenceID": 0, "context": "BKT is based on a theory of all-or-none human learning [1] which postulates that the knowledge state of student s following the i\u2019th exercise requiring a certain skill, Ksi, is binary: 1 if the skill has been mastered, 0 otherwise.", "startOffset": 55, "endOffset": 58}, {"referenceID": 19, "context": "[20] used a particular type of hidden unit, called an LSTM (long short-term memory) [9], which is interesting because these hidden units behave very much like the BKT latent knowledge state, Ksi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[20] used a particular type of hidden unit, called an LSTM (long short-term memory) [9], which is interesting because these hidden units behave very much like the BKT latent knowledge state, Ksi.", "startOffset": 84, "endOffset": 87}, {"referenceID": 19, "context": "With 200 LSTM hidden units\u2014the number used in simulations reported in [20]\u2014and 50 skills, DKT has roughly 250,000 free parameters (connection strengths).", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "For example, when individuals perform a choice task repeatedly, response latency can be predicted by an exponentially decaying average of recent stimuli [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "Recurrent neural networks tend to be more strongly influenced by recent events in a sequence than more distal events [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 22, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 19, "context": "[20]\u2014the label indicates the skill required to solve the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Forgetting is not a new idea to BKT, and in fact was included in the original psychological theory that underlies the notion of binary knowledge state [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 20, "context": "When it has been included in BKT [21], the motivation was to model forgetting from one day to the next, not forgetting that can occur on a much shorter time scale.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "For BKT to allow for such interactions among skills, the independent BKT models would need to be interconnected, using an architecture such as a factorial hidden Markov model [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 19, "context": "[20] to motivate DKT has exercise-indexed labels).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We recently proposed an inference procedure that automatically discovers the cognitive skills needed to accurately model a given data set [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "(A related procedure was independently proposed in [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "To account for individual variation in student ability, we have extended BKT [12, 11] such that slip and guess probabilities are modulated by a latent ability parameter that is inferred from the data, much in the spirit of item-response theory [4].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "To account for individual variation in student ability, we have extended BKT [12, 11] such that slip and guess probabilities are modulated by a latent ability parameter that is inferred from the data, much in the spirit of item-response theory [4].", "startOffset": 77, "endOffset": 85}, {"referenceID": 3, "context": "To account for individual variation in student ability, we have extended BKT [12, 11] such that slip and guess probabilities are modulated by a latent ability parameter that is inferred from the data, much in the spirit of item-response theory [4].", "startOffset": 244, "endOffset": 247}, {"referenceID": 19, "context": "[20] studied three data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Spanish is a data set of 182 middle-school students practicing 409 Spanish exercises (translations and application of simple skills such as verb conjugation) over the course of a 15-week semester, with a total of 578,726 trials [15].", "startOffset": 228, "endOffset": 232}, {"referenceID": 25, "context": "Statics is from a college-level engineering statics course with 189,297 trials and 333 students and 1,223 exercises [26], available from the PSLC DataShop web site [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Statics is from a college-level engineering statics course with 189,297 trials and 333 students and 1,223 exercises [26], available from the PSLC DataShop web site [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 19, "context": "We evaluated five variants of BKT, each of which incorporates a different subset of the extensions described in the previous section: a base version that corresponds to the classic model and the model against which DKT was evaluated in [20], which we\u2019ll refer to simply as BKT ; a version that incorporates forgetting (BKT+F ), a version that incorporates skill discovery (BKT+S), a version that incorporates latent abilities (BKT+A), and a version that incorporates all three of the extensions (BKT+FSA).", "startOffset": 236, "endOffset": 240}, {"referenceID": 19, "context": ") We verified that our implementation produced results comparable to those reported in [20] on Assistments and Synthetic.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "[20] do not describe the procedure they use to compute AUC for DKT, code they have made available implements the procedure we describe, and not the obvious alternative procedure in which ROC curves are computed on a per-skill basis and then averaged to obtain an overall AUC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "cite Pardos and Heffernan [19] as obtaining BKT\u2019s best reported performance on Assistments\u2014 an AUC of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "In [19], the overall AUC is computed by averaging the per-skill AUCs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20]; on Statics and Spanish DKT results are from our own implementation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "6% of difference in performance reported in [20] appears to be due to the use of a biased procedure for computing the AUC for BTK.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "formance of BKT versus DKT in [20], and the procedure for BKT is biased to yield a lower score.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "75 for DKT in [20].", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "For example, with Statics, performing skill discovery using exercise-indexed labels, [15] obtain an AUC of 0.", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "We found some evidence that different procedures may have been used to evaluate DKT and BKT in [20], leading to a bias against BKT.", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "When we replicated simulations of BKT reported in [20], we obtained significantly better performance: an AUC of 0.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 55, "endOffset": 67}, {"referenceID": 10, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 55, "endOffset": 67}, {"referenceID": 18, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 55, "endOffset": 67}, {"referenceID": 14, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 89, "endOffset": 96}, {"referenceID": 6, "context": "They include forgetting [21], latent student abilities [12, 11, 19], and skill induction [15, 7].", "startOffset": 89, "endOffset": 96}, {"referenceID": 8, "context": "DKT is a generic recurrent neural network model [9], and it has no constructs that are specialized to modeling learning and forgetting, discovering skills, or inferring student abilities.", "startOffset": 48, "endOffset": 51}], "year": 2016, "abstractText": "In theoretical cognitive science, there is a tension between highly structured models whose parameters have a direct psychological interpretation and highly complex, generalpurpose models whose parameters and representations are difficult to interpret. The former typically provide more insight into cognition but the latter often perform better. This tension has recently surfaced in the realm of educational data mining, where a deep learning approach to predicting students\u2019 performance as they work through a series of exercises\u2014termed deep knowledge tracing or DKT\u2014has demonstrated a stunning performance advantage over the mainstay of the field, Bayesian knowledge tracing or BKT. In this article, we attempt to understand the basis for DKT\u2019s advantage by considering the sources of statistical regularity in the data that DKT can leverage but which BKT cannot. We hypothesize four forms of regularity that BKT fails to exploit: recency effects, the contextualized trial sequence, inter-skill similarity, and individual variation in ability. We demonstrate that when BKT is extended to allow it more flexibility in modeling statistical regularities\u2014using extensions previously proposed in the literature\u2014BKT achieves a level of performance indistinguishable from that of DKT. We argue that while DKT is a powerful, useful, generalpurpose framework for modeling student learning, its gains do not come from the discovery of novel representations\u2014 the fundamental advantage of deep learning. To answer the question posed in our title, knowledge tracing may be a domain that does not require \u2018depth\u2019; shallow models like BKT can perform just as well and offer us greater interpretability and explanatory power.", "creator": "LaTeX with hyperref package"}}}