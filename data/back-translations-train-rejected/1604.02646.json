{"id": "1604.02646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2016", "title": "Visualization Regularizers for Neural Network based Image Recognition", "abstract": "The success of deep neural networks is mostly due their ability to learn meaningful features from the data. Features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to mid-level vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L2 norm regularizer and dropout, on benchmark datasets with no change in the training computational complexity.", "histories": [["v1", "Sun, 10 Apr 2016 07:02:40 GMT  (259kb,D)", "http://arxiv.org/abs/1604.02646v1", null], ["v2", "Sun, 15 May 2016 14:38:38 GMT  (259kb,D)", "http://arxiv.org/abs/1604.02646v2", null], ["v3", "Tue, 3 Jan 2017 10:07:22 GMT  (55kb,D)", "http://arxiv.org/abs/1604.02646v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["biswajit paria", "vikas reddy", "anirban santara", "pabitra mitra"], "accepted": false, "id": "1604.02646"}, "pdf": {"name": "1604.02646.pdf", "metadata": {"source": "CRF", "title": "Visualization Regularizers for Neural Network based Image Recognition", "authors": ["Biswajit Paria", "Anirban Santara"], "emails": ["biswajitsc@iitkgp.ac.in", "santara@iitkgp.ac.in", "pabitra@cse.iitkgp.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to erenen.lcihsrc\u00fcehnlcS mentioned above."}, {"heading": "2 Deep neural network architecture", "text": "The investigations presented in this paper are based on a muti class classification setting. The notation presented in this paper is as follows: x denotes the input, Wi or bi denotes the weights and distortions corresponding to the layers, ui denotes the pre-activation of the layers, hi denotes the activation of the layers and y denotes the output of the neural mesh. The following equations denote a deep neural network with l hidden layers. u1 = W1 \u00b7 x + b1, (1) hi = g (ui), \u04451 \u2264 i \u2264 l, (2) ui + 1 = Wi + 1 \u00b7 hi + bi + 1, \u03c61 \u2264 i \u2264 l \u2212 1, (3) y = Softmax (Wl + 1 \u00b7 hl + bl + 1), (4) hi = g, where g is the activation function, a monotonically increasing non-linear function such as sigmoid, tanh or the loss of the network function (Wl + 1) and the fictitious function (Wbl + 1)."}, {"heading": "3 The notion of visualization of a node", "text": "The visualization of a node refers to the visualization of the characteristics learned by the node. Visualization of the nodes of a neural network was by Erhan et. al. [10]. They proposed activation maximization algorithm to visualize characteristics learned from a node. Following the introduction in [10], we define the visualization of a node as follows. The visualization of a node is defined as the input pattern that activates the node maximally under the constraint of the L2 norm of input to be equal to the unit. The L2 norm of input is limited to the unit to prevent the input from becoming unlimited. Formally, the visualization vis (n) of a node n is called, vis (n) = argmax, x = 1 n (x), the note where n (x) activates the node for input."}, {"heading": "4 Proposed visualization based regularizer", "text": "First, we define the concept of smoothness of a visualization."}, {"heading": "4.1 Smoothness of a visualization", "text": "Intuitively, one can determine whether an image is smooth or noisy by looking at the gradients in the image. An image is smooth if it has small gradients; the gradient of an image can be calculated by crossing it with a high-pass filter. The larger the pixel values in the fold, the larger the gradients in the original image and the noisier the image. We use this intuition to give a formal definition of smoothness. Consider a folding I'K of image I with the core K, where K is a high-pass filter like the laplac core. We define the smoothness of an image as the negative sum of squares of the pixel values of I'K. Likewise, we can define the visualization loss VL of image I as I's, I's = I'K, VL (I) = 1, j (I '\u00b7 I) ij, where \"\u00b7\" is the element-wise product, also known as Shear or Hadard, the loss of the image."}, {"heading": "4.2 Visualization loss as a regularizer", "text": "Classification tasks that use deep neural networks benefit from the high level of abstraction achieved in the higher layers of the neural network. Deep neural networks are designed to use low pixels to learn mid-level characteristics and eventually high-level characteristics. We propose the visualization regulator (VR) to restrict the nodes in the first hidden layer to learn features that resemble those of the middle level. This restriction is designed to more effectively facilitate the discovery of high-level abstractions. Informally, we define the VR regulator as a regulator to reduce the loss of visualization of the nodes of the neural network. In other words, the VR regulator ensures that the nodes learn smooth or less noisy characteristics. The following subsections provide a more detailed description of the VR regulator:"}, {"heading": "4.2.1 Regularizer expression", "text": "From Eq.7 we know that the visualization of a node in the first hidden layer is proportional to the weights of the connections coming into the node. Therefore, we can use VL (wn) as a surrogate for the visualization loss of the visualization of the node n. The difficulty to calculate an algebraic expression for the visualization of nodes in higher hidden layers limits the use of the surrounding function for nodes in the first hidden layer. We define the visualization loss VL (wn) as a surrogate for the visualization loss of the node n. The difficulty to calculate an algebraic expression for the visualization of nodes in higher hidden layers limits the use of the surrounding function for nodes in the first hidden layer. We define the visualization loss VL of a neural network M as, VL (M) as valight."}, {"heading": "4.2.2 Gradient of the visualization regularizer", "text": "For the loss of visualization to be used as a regulator, its gradient must be calculated with respect to the parameters q = q (q = q = q). We derive the expression of the derivative for a general kernel of size (2k + 1) \u00b7 (2k + 1). For simplicity in the calculation of the expression of the gradient, we will represent the elements of the core in relation to the central element as shown in Equation 12 i. The element in the middle is indexed (0, 0). All other elements are indexed according to their position in relation to the central element. K = a \u2212 k, \u2212 k. a \u2212 k. a \u2212 k. a \u2212 k. a \u2212 k."}, {"heading": "4.2.3 Regularized training algorithm", "text": "The training requires the calculation of the gradient for the regulated loss function in relation to the parameters of the network. As shown in Eq.10, the gradient of the loss function can be calculated by first calculating the gradients of Lc (M, D), L \u2032 2 (M) and VL (M) and then calculating their sum. The gradients of Lc (M, D) and L \u2032 2 (M) can be calculated by backpropagation and partial derivatives. In other words, the gradient of VL (M) is the sum of the gradients of VL (wn) for n \u0445 U (M). Note that the gradient of w VL (wn) is zero if w / wn. In other words, the gradient of VL (M) is zero if it does not belong to the set of weights that belong to the node. Therefore, we only need to calculate the gradient for each n \u00b2 unit of calculation."}, {"heading": "5 Relationship with Tikhonov regularization", "text": "For example, the L2 regression, a special case of the Tikhonov regression, is used to calculate solutions to regression problems for which insufficient matrices are encountered in the calculation of their solutions.The solution of the regression of the regularized smallest squares results from w = argmin w-y-y-y-y-y-w-2, (20) where the matrix of the Tikhonov matrix is situated.The L2 regularizer corresponds to the sum of all weights in wn for n-U (M).From Eq.16 we can see that VL (wn) is the sum of the squared terms of the form w-ij 2 = (t-ctwt) 2 = (t-ctwt) 2, where the form of the Vtwt-w, ct-w, 0, 1} and w-wn-K."}, {"heading": "6 Experiments and observations", "text": "We experimented with the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm with other regularizers using the VR regularizer."}, {"heading": "6.1 Experimental setting", "text": "We used a neural network with two fully connected hidden layers for both sets of data. For the CIFAR-10 data set, we additionally used a folding layer and a maximum pooling layer in front of the fully connected layers. To simplify the search for optimal parameters, we set the number of fully connected hidden layers to two, with the number of nodes in each hidden layer being 1000. We also set the number of folding layer characteristic cards to 10, and the core size to 3 \u00d7 3. We used the laplactic kernel for the VR regulator, which was defined as follows. K = [\u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1] (22) We used the mean cross-entropy loss via the mini-batch examples as a classification loss, while the mean weight square rate of the weight parameters was defined as follows."}, {"heading": "6.2 Accuracy", "text": "We compared the different neural networks with combinations of L2, VR and dropout regulators. To find the optimal parameters, we performed a randomized hyperparameter search with manual fine-tuning. Accuracy and optimal hyperparameters for different regularization settings are shown in Table 2. The parameters \u00b5, \u03bb and \u03b1 each denote the VR regularization weight, the L2 regularization weight and the learning rate. The table shows that the VR regularizer represents an improvement over the standard L2 regularizer, which is observed for both MNIST and CIFAR-10."}, {"heading": "6.3 Convergence", "text": "Figures 3 and 4 show the deviation of the accuracy of the test sets over the training epochs. Figures 5 and 6 show the deviation of the regularization concepts over the training epochs. Since the loss values for different parameter settings can be of different magnitude, the loss values for visualization were transformed linearly to show 1 after the first epoch and 0 after the last epoch."}, {"heading": "6.4 Visualizations of hidden nodes", "text": "Figure 7 shows the visualizations of the internal nodes of the MNIST classifier. The first two lines of the partial images correspond to nodes in the first hidden layer, and the last two lines correspond to nodes in the second hidden layers. 1The experiment code is available at https: / / github.com / biswajitsc / VisRegDLThe difference between the visualizations of networks trained with the VR regulator and those not trained with it is obvious. Visualizations of architectures using the VR regulator are smoother and less noisy than expected."}, {"heading": "7 Conclusion and future work", "text": "We observe that the VR regulator is an improvement on the popular L2 regulator in terms of classification accuracy. It is also observed that the networks trained with a VR regulator have smoother visualizations. Although the VR regulator imposes the smoothing restriction only on the first hidden layer nodes, our observations from Figure 7 show that the smoothing restriction is transferred to the next layer. The VR regulator can be extended to a new class of training algorithms that use domain knowledge in the form of regulators."}], "references": [{"title": "Simplifying neural networks by soft weight-sharing,", "author": ["S.J. Nowlan", "G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "and S", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent"], "venue": "Bengio, \u201cWhy does unsupervised pre-training help deep learning?,\u201d The Journal of Machine Learning Research, vol. 11, pp. 625\u2013660", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "and R", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever"], "venue": "Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive dropout for training deep neural networks,", "author": ["J. Ba", "B. Frey"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "and G", "author": ["Y. LeCun", "Y. Bengio"], "venue": "Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep architectures for ai,", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Solutions of ill-posed problems", "author": ["A. Tikhonov", "V. Arsenin"], "venue": "VH Winston and Sons", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "The MNIST database of handwritten digits,", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Learning multiple layers of features from tiny images,", "author": ["A. Krizhevsky"], "venue": "Technical Report, Univ. Toronto,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "and P", "author": ["D. Erhan", "Y. Bengio", "A. Courville"], "venue": "Vincent, \u201cVisualizing higher-layer features of a deep network,\u201d Technical Report, Univ. Montreal", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A method of solving a convex programming problem with convergence rate {O}(1/k\u02c62),", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}], "referenceMentions": [{"referenceID": 0, "context": "Other regularizers include soft-weight sharing, introduced by Nowlan and Hinton [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "showed that layer-wise unsupervised pre-training acts as a regularizer for deep neural networks [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Dropout [3] is a more recent regularizer proposed for deep neural networks.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "There has also been recent works on adaptive dropout [4], which is an improvement over the original dropout.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Deep neural networks have been known to learn hierarchical layers of feature representation [5, 6].", "startOffset": 92, "endOffset": 98}, {"referenceID": 5, "context": "Deep neural networks have been known to learn hierarchical layers of feature representation [5, 6].", "startOffset": 92, "endOffset": 98}, {"referenceID": 6, "context": "We show that the VR regularizer is a special case of Tikhonov regularization [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Using our regularizer, we achieve better performance in terms of accuracy than the baselines when compared on benchmark datasets such as MNIST [8] and CIFAR-10 [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "Using our regularizer, we achieve better performance in terms of accuracy than the baselines when compared on benchmark datasets such as MNIST [8] and CIFAR-10 [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Following the notion in [10] we define the visualization of a node as follows.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "The visualization of an internal node of the neural network can be computed using gradient-ascent as described in [10] as activation maximization.", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "Tikhonov regularization was originally developed for solutions to ill-posed problems [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "We experimented on the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm using the VR regularizer with other regularizers.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "We experimented on the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm using the VR regularizer with other regularizers.", "startOffset": 46, "endOffset": 49}, {"referenceID": 10, "context": "We trained the neural network using stochastic gradient descent as described in figure 2, and also used Nesterov\u2019s accelerated gradient (momentum) [11].", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "The success of deep neural networks is mostly due their ability to learn meaningful features from the data. Features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to midlevel vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L2 norm regularizer and dropout, on benchmark datasets with no change in the training computational complexity.", "creator": "LaTeX with hyperref package"}}}