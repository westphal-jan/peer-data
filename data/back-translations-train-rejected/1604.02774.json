{"id": "1604.02774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Reverse Engineering and Symbolic Knowledge Extraction on {\\L}ukasiewicz Fuzzy Logics using Linear Neural Networks", "abstract": "This work describes a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued {\\L}ukasiewicz logic, where we take advantage of fact what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction.", "histories": [["v1", "Mon, 11 Apr 2016 02:05:21 GMT  (28kb)", "http://arxiv.org/abs/1604.02774v1", "24 pages"]], "COMMENTS": "24 pages", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["carlos leandro"], "accepted": false, "id": "1604.02774"}, "pdf": {"name": "1604.02774.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["[miguel.melro.leandro@gmail.com]"], "sections": [{"heading": null, "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "1 Preliminaries", "text": "We begin by presenting the basic concepts that we need from the subjects of many cherished logics, and how formulas can be injected into their language and extracted from a neural network of retransmission."}, {"heading": "1.1 Many valued logic", "text": "The classical propositional logic is one of the earliest formal systems of logic. The algebraic semantics of this logic is given by Boolean algebra. Both logic and algebraic semantics have been generalized in many directions. The generalization of Boolean algebra can be based on the relationship between conjunction and implication given by Boolean algebra."}, {"heading": "1.2 Processing units", "text": "In this work, we present a methodology that uses neural networks to learn formulas from data. < [1] And where neural networks are presented as circular counterparts of (1) formulas that are either easy to implement and tall parallel objects to implement, it is possible to represent the corresponding neural network as a combination of Lukasiewicz and vice versa propositions [1]. <"}, {"heading": "1.3 Similarity between a configuration and a formula", "text": "We called Castro neural network a neural network with an activation function reduced to zero and one, and where its weights are 1, 0 or 1 and each have an integer. \u2212 And a neural network is representable if it is codified as a binary neural network, i.e. a neural network in which each neuron has no more than two inputs. \u2212 A network is designated as unrepresentable if it cannot be codified with a binary neural network of the first order. \u2212 In Figure 4 we present an example of an unrepresentable network configuration, as we will see below. \u2212 Note what binary neural network can be translated in the language of Lukasiewicz, and in this sense they are called Lukasiewicz neural network."}, {"heading": "1.4 A neural network crystallization", "text": "The weights in Castro neural networks are based on the values -1 or 1. However, since the usual learning algorithms process neural networks, weights are defined based on the continuity of the weighting domain. Of course, any neural network with weights in [\u2212 1, 1] can be considered an approximation of a Castro neural network. The process of identifying a neural network with weights in [\u2212 1, 1] with a Lukasiewicz neural network was called crystallization, and essentially consists in rounding each neural sign wi to the nearest integer of smaller or equal wi. In this sense, the crystallization process can be seen as a circumcision of the network structure, in which connections between neurons with weights near 0 and weights near -1 or 1 are consolidated. However, this process is very crisp. We need a smooth procedure to crystallize a network in each learning iteration to avoid drastic reduction in learning performance."}, {"heading": "2 Learning propositions", "text": "We started by examining the neural network generation, which tries to do reverse engineering on a truth table. By this we mean that a truth table from a (n + 1) -rated Lukasiewicz logic, generated by a formula from Lukasiewicz logic, will find its interpretation in the form of a Lukasiewicz neural network."}, {"heading": "2.1 Training the neural network", "text": "This is the direction in which the efficiency decreases fastest. In the EBP algorithm, the sum to be minimized is the sum of the squared grids between the target and the target. This is the direction in which the efficiency decreases fastest."}, {"heading": "3 Applying reverse engineering on truth tables", "text": "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "4 Applying the process on real data", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5 Conclusions", "text": "This method of codifying and extracting symbolic knowledge from a neuron network is very simple and efficient for extracting simple rules from medium-sized data sets. From our experience, the described algorithm is a very good tool for selecting attributes, especially when we have low interference and classification problems, which depend on a few nominal attributes to be selected from a variety of possible attributes. In theory, it is particularly interesting that the limitation of the values assumed by neuron weights restricts the propagation of information in the network. Allows the formation of patterns in the neural network structure. In the case of linear neural networks, these structures are characterized by the appearance of patterns in the neuron configuration with a direct symbolic representation in a Lukasiewicz logic."}], "references": [{"title": "Neural networks and rational lukasiewicz logic", "author": ["P. Amato", "A.D. Nola", "B. Gerla"], "venue": "IEEE Transaction on Neural Networks, vol. 5 no. 6, 506- 510, 2002. ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A modified regression algorithm for fast one layer neural network training", "author": ["T.J. Andersen", "B.M. Wilamowski"], "venue": "World Congress of Neural Networks, Washington DC, USA, Vol. 1 no. 4, CA, (1995)687-690. ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Frist- and second-order methods for learning between steepest descent and newton\u2019s method", "author": ["R. Battiti"], "venue": "Neural Computation, Vol. 4 no. 2, 141-166, 1992. ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Enhanced training algorithms", "author": ["M.G. Bello"], "venue": "and intehrated training/architecture selection for multi layer perceptron network, IEEE Transaction on Neural Networks, vol. 3, 864-875, 1992. ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Massively parallel reasoning", "author": ["S.E. Bornscheuer", "S. H\u00f6lldobler", "Y. Kalinke", "A. Strohmaier"], "venue": "in: Automated Deduction - A Basis foe Applications, Vol. II, Kluwer Academic Publisher, 291-321, 1998. ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "The logic of neural networks", "author": ["J.L. Castro", "E. Trillas"], "venue": "Mathware and Soft Computing, vol. 5, 23-27, 1998. ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Conjugate gradient algorithm for efficient training of artificial neural networks", "author": ["C. Charalambous"], "venue": "IEEE Proceedings, Vol. 139 no. 3, 301-310, 1992. ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Semantics of architectural connectors", "author": ["J. Fiadeiro", "A. Lopes"], "venue": "TAP- SOFT\u201997 LNCS, v.1214, p.505-519, Springer-Verlag, 1997. ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "On the simultaneous associativity of f(x", "author": ["M.J. Frank"], "venue": "y) and x+y\u2212f(x, y), Aequations Math., vol. 19, 194-226, 1979. ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1979}, {"title": "Knowledge-based connectionism from revising domain theories", "author": ["L.M. Fu"], "venue": "IEEE Trans. Syst. Man. Cybern, Vol. 23, 173-182, 1993. ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Connectionist expert systems", "author": ["S.I. Gallant"], "venue": "Commun. ACM, Vol. 31 ,(1988)152-169. ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Functional representation of many-valued logics based on continuous t-norms", "author": ["B. Gerla"], "venue": "PhD thesis, University of Milano, 2000. ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural network design", "author": ["M.T. Hagan", "H.B. Demuth", "M.H. Beal"], "venue": "PWS Publishing Company, Boston.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Training feed-forward networks with marquardt algorithm", "author": ["M.T. Hagan", "M. Menhaj"], "venue": "IEEE Transaction on Neural Networks, vol. 5 no. 6, 989- 993, 1999. ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimal brain surgeon and general network pruning", "author": ["B. Hassibi", "D.G. Stork", "G.J. Wolf"], "venue": "IEEE International Conference on Neural Network, vol. 4 no. 5, 740-747, 1993. ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Logic programs and connectionist networks", "author": ["P. Hitzler", "S. H\u00f6lldobler", "A.K. Seda"], "venue": "Journal of Applied Logic, 2, 245-272, 2004. ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Challenge problems for the integration of logic and connectionist systems", "author": ["S. H\u00f6lldobler"], "venue": "in: F. Bry, U.Geske and D. Seipel, editors, Proceedings 14. Workshop Logische Programmierung, GMD Report 90, 161-171, 2000. ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Towards a new massively parallel computational model for logic programming", "author": ["S. H\u00f6lldobler", "Y. Kalinke"], "venue": "in: Proceedings ECAI94 Workshop on Combining symbolic and Connectionist Processing, 68-77, 1994. ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Approximating the semantics of logic programs by recurrent neural networks", "author": ["S. H\u00f6lldobler", "Y. Kalinke", "H.P. St\u00f6rr"], "venue": "Applied Intelligence 11, 45-58, 1999. ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["R.A. Jacobs"], "venue": "Neural Networks, Vol. 1 no. 4, CA, 295-308, 1988. ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1988}, {"title": "and S", "author": ["K. Mehrotra", "C.K. Mohan"], "venue": "Ranka, Elements of artificial neural networks,, The MIT Press.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Acceleration of back-propagation through learning rate and momentum adaptation", "author": ["A.A. Miniani", "R.D. Williams"], "venue": "Proceedings of International Joint Conference on Neural Networks, San Diego, CA, 676-679, 1990. ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "Back-propagation improvements based on heuristic arguments", "author": ["T. Samad"], "venue": "Proceedings of International Joint Conference on Neural Networks, Washington, 565-568, 1990. ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Accelerated learning in layered neural networks", "author": ["S.A. Solla", "E. Levin", "M. Fleisher"], "venue": "Complex Sustems, 2, 625-639, 1988. ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1988}, {"title": "Extracting refined rules from knowledgebased neural networks", "author": ["G.G. Towell", "J.W. Shavlik"], "venue": "Mach. Learn., Vol. 13 ,71-101, 1993. ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 5, "context": "Our approach uses finite truth valued Lukasiewicz logic, where we take advantage of fact, presented by Castro in [6], what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one.", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.", "startOffset": 87, "endOffset": 90}, {"referenceID": 15, "context": "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "This has been done for some types of neuro networks like Knowledge-based networks [10] [27].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "In the other direction there has been widespread activity aimed at translating neural language in the form of symbolic relations [11] [12] [26].", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "In the other direction there has been widespread activity aimed at translating neural language in the form of symbolic relations [11] [12] [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "one [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "We focused our attention on many-valued logics having [0, 1] as set of truth values.", "startOffset": 54, "endOffset": 60}, {"referenceID": 11, "context": "In [13] it is defined as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments and 1\u2297 x = x and 0\u2297 x = 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In [13] it is defined as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments and 1\u2297 x = x and 0\u2297 x = 0.", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "In [9] all continuous t-norms are characterized as ordinal sums of Lukasiewicz, G\u00f6del and product t-norms.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "2 Processing units As mention in [1] there is a lack of a deep investigation of the relationships between logics and neural networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "In [6] it is shown what, by taking as activation function \u03c8 the identity truncated to zero and one, also named saturating linear transfer function \u03c8(x) = min(1,max(x, 0)) it is possible to represent the corresponding neural network as combination of propositions of Lukasiewicz calculus and viceversa[1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [6] it is shown what, by taking as activation function \u03c8 the identity truncated to zero and one, also named saturating linear transfer function \u03c8(x) = min(1,max(x, 0)) it is possible to represent the corresponding neural network as combination of propositions of Lukasiewicz calculus and viceversa[1].", "startOffset": 300, "endOffset": 303}, {"referenceID": 7, "context": "This task of construct complex structures based on simplest ones can be formalized using generalized programming [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "A truth table f\u03c6 for a formula \u03c6 is a map f\u03c6 : [0, 1] m \u2192 [0, 1], where m is the number of propositional variables used in \u03c6.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "A truth table f\u03c6 for a formula \u03c6 is a map f\u03c6 : [0, 1] m \u2192 [0, 1], where m is the number of propositional variables used in \u03c6.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Since, for every network N and n > 0, \u2206(N) \u2265 \u2206(\u03a5n(N)), we have: Proposition 7 Given a neural networks N with weights in the interval [0, 1].", "startOffset": 133, "endOffset": 139}, {"referenceID": 14, "context": "Stork in [16].", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].", "startOffset": 54, "endOffset": 57}, {"referenceID": 22, "context": "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "It gives a good exchange between the speed of Newton algorithm and the stability of the steepest descent method [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 20, "context": "The Jacobian matrix Jk can be computed through a standard back propagation technique [22] that is much less complex than computing the Hessian matrix.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "In this way, the performance function is always reduced at each iteration of the algorithm [14].", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "This work describes a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued Lukasiewicz logic, where we take advantage of fact, presented by Castro in [6], what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction. Our method trains a neural network using Levenderg-Marquardt algorithm, where we restrict the knowledge dissemination in the network structure. We show how this reduces neural networks plasticity without damage drastically the learning performance. Making the descriptive power of produced neural networks similar to the descriptive power of Lukasiewicz logic language, simplifying the translation between symbolic and connectionist structures. This method is used in the reverse engineering problem of finding the formula used on generation of a truth table for a multi-valued Lukasiewicz logic. For real data sets the method is particulary useful for attribute selection, on binary classification problems defined using nominal attribute. After attribute selection and possible data set completion in the resulting connectionist model: neurons are directly representable using a disjunctive or conjunctive formulas, in the Lukasiewicz logic, or neurons are interpretations which can be approximated by symbolic rules. This fact is exemplified, extracting symbolic knowledge from connectionist models generated for the data set Mushroom from UCI Machine Learning Repository.", "creator": "LaTeX with hyperref package"}}}