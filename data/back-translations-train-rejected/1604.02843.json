{"id": "1604.02843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Method of Tibetan Person Knowledge Extraction", "abstract": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "histories": [["v1", "Mon, 11 Apr 2016 08:56:58 GMT  (3175kb)", "http://arxiv.org/abs/1604.02843v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuan sun", "zhen zhu"], "accepted": false, "id": "1604.02843"}, "pdf": {"name": "1604.02843.pdf", "metadata": {"source": "CRF", "title": "Method of Tibetan Person Knowledge Extraction", "authors": ["Sun Yuan", "Zhu Zhen"], "emails": ["tracy.yuan.sun@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Tibetan, Knowledge Extraction, Training Corpus, Template, SVM."}, {"heading": "1. INTRODUCTION", "text": "The explosion of web content makes studying social networks on the web from structural analysis to content analysis a hot research topic in the field of natural language processing in the age of big data [1]. According to the survey results of the National Language Resource Monitoring and Research Center of Minzu University of China in 2013: The number of ethnic minority websites in China is 1,031, including 655 Uighur websites, 104 Tibetan websites and 102 Mongolian websites. Knowledge graphics with a complete and complete knowledge system for gathering information, answering questions, building knowledge databases and other areas of study provide resources and support. Knowledge graphics nodes represent entities and concepts and connecting edges represent various semantic relationships between entities and concepts, and entity knowledge extraction is one of the most important research contents."}, {"heading": "2. RELATED WORKS", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "3. TRAINING CORPUS AND TEMPLATE CONSTRUCTION", "text": "First, we search data from some websites, such as Wikipedia, Comba Media Net. Through word segmentation, POS tagging and entity recognition, we use templates and SVM to realize the extraction of entity knowledge."}, {"heading": "3.1 Training corpus", "text": "We use the \"entity, attribute and value\" of the existing information field in Chinese from Wikipedia to obtain the Chinese sentences with the entity and the attribute. Based on the correspondence between entities in the parallel theorem of Tibetan and Chinese, we construct a training corpus for the extraction of Tibetan entity knowledge. Meanwhile, based on the shallow analysis of Tibetan syntactics, semantic characteristics and verbs, we obtain the templates and main features of the extraction of Tibetan entity knowledge presented in Fig. (1)."}, {"heading": "3.2 Template building", "text": "Tibetan is a predicate behind the language, and the verb is the core of the sentence. Meanwhile, the Tibetan tool can clearly indicate the semantic structure of the sentence shown in Table 1. Therefore, the feature selection mainly includes Tibetan fallmakers, verbs and other nouns. Example 1: (Zhuo Ma was born in 1988.) Template 1: < Name / nh > < Time / n > () / u / v [born] Example 2: (Zhuo Gas's hometown is Qinghai.) Template 2: < Name / nh > () / u / n [hometown] / r < Place name / np > () / u / vFig. Example 3: (Tsewang's birthday is October 1, 1988.) Template 3: < Name / nh > () / u / n [birthday] / r < Time / t > / vFig. (1)."}, {"heading": "4. HIERARCHICAL CLASSIFICATION BASED ON SVM", "text": "Although the attribute-based method can achieve higher accuracy in certain test corpus, it requires a lot of manual work and cannot extract the uncovered corpus. Therefore, we have introduced the SVM method based on attribute vectors and designed a hierarchical classifier."}, {"heading": "4.1 Feature selection", "text": "Keywords Keywords are a noun or verb with high frequency and strong distinction. Most of these characteristics are extracted from the templates. Although keywords are not many, these words have a strong distinction in a particular attribute class. (2) Tagging combination characteristics Part-of-speech tagging is an important characteristic, but not every tag can be used as a feature vector, as many tags do not constitute discrimination. Therefore, we use tagging combination characteristics for a better classification effect. For example, the combination of time tag \"/ t\" + auxiliary characteristic \"/ k\" + \"/ v\" is a great help in identifying birth attributes. (3) Words around entities The word characteristic around entities includes parts of language tagging and the naming of entities. We consider the characteristics closer to the entity as more important. Therefore, we choose two words forward and one word backwards."}, {"heading": "4.2 Hierarchical classifier construction", "text": "SVM was originally designed to solve the problem of binary classification, but entity attribute extraction is the multi-classification problem. For example, personal attributes can be divided into birth, place of birth, gender and other categories. So the key problem is to design the high-performance classifier. Currently, there are two types of classifier: (1) one to one classifier. If there are k categories, we need to build (1) / 2k classifier, and then calculate (1) / 2k times and get the cumulative weight. The largest cumulative value is the category. (2) One too many classifiers. This method needs to build k-classifiers, if there are k categories, then we need / 2k times to predict each attribute."}, {"heading": "5. THE EXPERIMENTAL RESULTS AND ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Corpus sources", "text": "In this thesis, the corpus data comes from seven Tibetan websites, as shown in Table 2. We mainly process the following four entity relationships: name-date, name-place of birth, name-father, name-mother. We select 2,400 sentences that contain entity attributes from a large number of websites. Among these are the 1,975 sentences about four entity attributes and 425 sentences about other entity attributes. We select 1,600 sentences as training corpus and 800 sentences as test corpus."}, {"heading": "5.2 The experimental analysis and evaluation", "text": "The experimental results are in Table 3.The experimental results show that the performance of the template method used in the open sentence (800 sentences) is not high. The main reason for this is that this method lacks learning ability and must be constructed through artificial participation. Although the performance is gradually increasing through generalization and correction templates, too much human intervention becomes the bottleneck of the method. Second, we use the hierarchical SVM classifier to extract entity relationships, the experimental results are in Table 4. Compared with the template method, the SVM method improves the memory rate of entity attribute extraction, but the accuracy is obviously not improved. The main reason for this is that the results with SVM can achieve good performance in the non-obvious classification by diversifying the feature vectors, but there are some errors in a clear classification as the training corpus method is insufficient."}, {"heading": "6. CONCLUSION", "text": "In this paper, we propose an SVM and template-based approach to gaining Tibetan knowledge, and the experimental results show that the method performs well. In the following work, we will increase the training corpus and improve this method. Subsequently, we will use CRF, a method for embedding text in the knowledge extraction of Tibetan entities, and provide the comparative results."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the National Nature Science Foundation (No. 61501529, No. 61331013), the National Language Committee Project (No. YB125-139, ZDI125-36) and the Minzu University of China Scientific Research Project (No. 2015MDQN11, No. 2015MDTD14C)."}], "references": [{"title": "Linked data-the story so far", "author": ["C Bizer", "T Heath", "T Berners-Lee"], "venue": "International Journal on Semantic Web and Information Systems (IJSWIS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The focus of the next search engines: Knowledge Graph", "author": ["Zhang Jing", "Tang Jie"], "venue": "CCCF, vol. 9,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Natrual language processing based on naturally annotated web resources", "author": ["Sun Maosong"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "DBpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z.G. Ives"], "venue": "Proceedings of the 6th international semantic web conference, pp. 722-735", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "DBpedia - a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: Science\u201d, Services and Agents on the World Wide Web, vol. 7, no. 3, pp. 154-165", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Freebase: a shared database of structured general human knowledge", "author": ["K.D. Bollacker", "R.P. Cook", "P. Tufts"], "venue": "Proceedings of the 22nd national conference on Artificial intelligence, vol. 2, pp. 1962-1963", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "G", "author": ["F.M. Suchanek"], "venue": "Kasneci, and G.Weikum, \u201cYAGO: a core of semantic knowledge\u201d, Proceedings of the 16th international conference on World Wide Web, pp. 697-706", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Wikipedia as multilingual source of comparable corpora", "author": ["Otero", "Pablo Gamallo", "Isaac Gonz\u00e1lez L\u00f3pez"], "venue": "Proceedings of the 3rd Workshop on Building and Using Comparable Corpora,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Large-Scale Named Entity Disambiguation", "author": ["S Cucerzan"], "venue": "Based on Wikipedia Data\u201d,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Discovering missing semantic relations between entities in Wikipedia", "author": ["Mengling Xu", "Zhichun Wang"], "venue": "The 12th International Semantic Web Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A crosslingual annotation projection approach for relation detection", "author": ["Seokhwan Kim", "Minwoo Jeong", "Jonghoon Lee"], "venue": "Proceedings of COLING,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A graph-based cross-lingual projection approach for weakly supervised relation extraction", "author": ["Seokhwan Kim", "Gary Geunbae Lee"], "venue": "Proceedings of ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Cross Lingual Relation Extraction Via Machine Translation", "author": ["Hu Yanan"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid O \u0301. Se \u0301aghdha", "Sebastian Pado", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Three new graphical models for statistical language modeling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "The Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov Tom\u00e1\u0161"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian Joseph", "Lev Ratinov", "Yoshua Bengio"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Extracting relation information from text documents by exploring various types of knowledge", "author": ["Guodong Zhou", "Min Zhang"], "venue": "Information Processing and Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations", "author": ["Nanda Kambhatla"], "venue": "Proceedings of ACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Exploiting constituent dependencies for tree kernel-based semantic relation extraction", "author": ["Longhua Qian", "Gougong Zhou", "Fang Kong"], "venue": "Proceedings of COLING,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Tree kernel-based relation extraction with context-sensitive structured parse tree information", "author": ["Guodong Zhou", "Min Zhang", "Donghong Ji"], "venue": "Proceedings of EMNLP/CONLL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "A entity relation extraction method based on Wikipedia and pattern clustering", "author": ["Zhang Weiru", "Sun Le", "Han Xianpei"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Extracting relations with integrated information using kernel methods", "author": ["B Zhao S", "R Grishman"], "venue": "Proceedings of ACL,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Exploring various knowledge in relation extraction", "author": ["D Zhou G", "J Su", "J Zhang", "M. Zhang"], "venue": "Proceedings of ACL,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Entity relation extraction method using semantic pattern", "author": ["Deng Bo", "Fan Xiaozhong", "Yang Ligong"], "venue": "Computer Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "A bootstrapping method for acquisition of bi-relations and bi-relational patterns", "author": ["Jiang Jifa", "Wang Shuxi"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Kernel methods for relation extraction", "author": ["D Zelenko", "C Aone", "A Richardella"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Dependency tree kernels for relation extraction", "author": ["A Culotta", "J Sorensen"], "venue": "Proceedings of ACL,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["C. Bunescu R", "J Mooney R"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "A compo site kernel to extract relations between entities with both flat and structured features", "author": ["M Zhang", "J Zhang", "J Su"], "venue": "Proceedings of ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Implementation of a kernel-based Chinese relation extraction system", "author": ["Liu Kebin", "LiFang", "Liu Lei"], "venue": "Journal of Computer Research and Development,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Explosive growth of Web content is making the study of social network on Web from the structure analysis to the content analysis, Knowledge Graph is becoming a hot research of Natural Language Processing in the age of big data [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 1, "context": "1 billion relations), Wiki-links (40 million to disambiguate the relations), Wolframalpha (10 trillion), Probase (more than 265 million entities), Baidu, Sogou, etc [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 3, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 6, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 7, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 8, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 9, "context": "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].", "startOffset": 101, "endOffset": 107}, {"referenceID": 10, "context": "Kim [11] put forward a cross-language relation extraction method based on label mapping, which applied the training model to the source language in parallel corpus, and mapped the identified high reliability examples to the target language, finally obtained the training corpus to the entity relation of target language.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "Kim and Lee [12] further used a semisupervised learning algorithm, which mapped more contextual information in the source language into the target language through iterative method, and improved the quantity and quality of training corpus.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Yanan Hu [13] used machine translation to realize the relation example transformation from the source language to target language, and helped the less resource language to extract semantic relations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 14, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 15, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 16, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 17, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 18, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 19, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 20, "context": "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].", "startOffset": 186, "endOffset": 193}, {"referenceID": 21, "context": "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].", "startOffset": 83, "endOffset": 90}, {"referenceID": 22, "context": "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].", "startOffset": 83, "endOffset": 90}, {"referenceID": 23, "context": "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].", "startOffset": 111, "endOffset": 118}, {"referenceID": 24, "context": "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].", "startOffset": 111, "endOffset": 118}, {"referenceID": 22, "context": "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].", "startOffset": 95, "endOffset": 102}, {"referenceID": 27, "context": "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].", "startOffset": 95, "endOffset": 102}, {"referenceID": 28, "context": "Qin Deng [30] introduced the lexical semantic matching technology into Chinese entity relation extraction based on pattern matching technology, and compared the performance between the general pattern matching technology and lexical semantic pattern matching technology.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "Jifa Jiang [31] proposed a bootstrap acquisition method based on binary relation mode.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "Weiru Zhang [27] proposed a method based on Wikipedia and mode clustering, which has highly accuracy in Chinese entity relation extraction.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "Zelenko [32] introduced the kernel function method into the relation extraction, which defined the kernel function on the basis of shallow parsing and designed a dynamic programming algorithm to extract entity semantic relations, finally got good results in 200 news texts.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "Culotta [33] transformed the parse tree into the dependency tree through some transformation rules, and increased features of the part of speech, entity types, phrases, WordNet, used the SVM classifier to relation extraction.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "Bunescu [34] further proposed the kernel function based on the shortest path dependence tree, and F value achieved 52.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "Zhang [35] designed a composite convolution kernel function tree for relation extraction.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "Kebin Liu [36] has realized the automatic extraction system based on the improved semantic sequence kernel function and combined with KNN machine learning algorithm to entity relation.", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "creator": "PScript5.dll Version 5.2.2"}}}