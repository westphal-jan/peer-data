{"id": "1604.03010", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Semi-supervised learning of local structured output predictors", "abstract": "In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both input-output pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method.", "histories": [["v1", "Mon, 11 Apr 2016 15:53:04 GMT  (568kb,D)", "http://arxiv.org/abs/1604.03010v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xin du"], "accepted": false, "id": "1604.03010"}, "pdf": {"name": "1604.03010.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised learning of local structured output predictors", "authors": ["Xin Du"], "emails": ["xindu.njtu@gmail.com"], "sections": [{"heading": null, "text": "In this paper, we examine the problem of semi-supervised structured output prediction, which aims to learn predictors for structured output data such as sequences, tree nodes, vectors, etc. from a set of data points, both from input pairs and from individual inputs without outputs. Traditional methods of solving this problem typically learn a single predictor for all data points at the same time, ignoring the diversity of different data points. Different parts of the data set can have different local distributions and require different optimal local predictors. To overcome this drawback of existing methods, we suggest learning different local predictors for neighborhoods of different data points and the lack of structured output data at the same time. In the neighborhood of each data point, we suggested learning a linear predictor by minimizing both the complexity of the predictor and the upper limit of structured prediction losses. Minimization is accomplished by gradient descending algorithms."}, {"heading": "1. Introduction", "text": "Machine learning refers to the problem of learning a prediction model to predict an outcome from an input data point [1, 2, 3, 5, 6, 7, 8, 9, 10, e-mail address: xindu.njtu @ gmail.com (Xin Du) Form submitted to Neurocomputing April 12, 2016, however, is referred to as classification [13, 14, 15, 16, 17, 19, 20, 21], while the problem of prediction continues to be referred to as regression. Both of these problems have many applications, such as computer vision, natural language processing, bioinformatics and finance."}, {"heading": "1.1. Related works", "text": "There are a number of existing algorithms for this problem, briefly presented as the following. Altun et al. [40] proposed the problem of semi-supervised learning with structured results. It is an inductive algorithm, and it can easily be extended to new test points. Brefeld and Scheffer suggested solving the problem of semi-supervised maximum margin formulation. It is an inductive algorithm, and it can easily be extended to new test data."}, {"heading": "1.2. Our contributions", "text": "All of the above semi-monitored prediction methods learn a single predictor for the entire dataset. However, we observe that a training set, the local distributions of neighborhoods, play important roles in the problem of modelling input and structured outputs. It is extremely important to respect the local distributions when learning structured outputs. This is all the more important because we need to explore the connections between the different datasets so that we have the structured outputs from the unnamed outputs few, and the structured outputs from all the other data points are missing. To learn the missing structured outputs, we need to explore the connections between the different datasets so that we align the structured outputs from the unused outputs with the unused outputs."}, {"heading": "1.3. Paper organization", "text": "In Section 2, we will model the learning problem and present the optimization methods for the problem; in Section 3, both the iterative algorithm for the learning process and the algorithm for the testing process will be presented; in Section 4, the experiments will be presented in four benchmark datasets, including medical DDSM mammography imaging datasets, natural SUN imaging datasets, data sets of Cora research papers and sets of Spanish news broadcasts; in Section 5, the work will be completed; in Section 6, we will discuss future work."}, {"heading": "2. Problem modeling and optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem modeling", "text": "Suppose we have a training set of n data points, X = L \u00b2 U, which consists of a marked subset, L, and an unmarked subset, U. L contains l data points of input-output pairs, L = {xi, yi} li = 1, where xi-l data points of inputs without outputs, U = {xi} ni = l + 1. The structured output problem is to predict the structured outputs of the inputs, Y is the space of structured outputs. U contains u = n \u2212 l data points of inputs without outputs, U = {xi} ni = l + 1. The structured output prediction problem is to predict the structured outputs of the inputs of the inputs. Weproposed to learn a local predictor for the neighborhood of each data point instead of learning a single predictor for all data points. We present the neighborhood of the i-th data points as a set of their nearest neighbors, Nxj \u00b2."}, {"heading": "2.2. Problem optimization", "text": "To solve the problem in (9), we propose to use an iterative problem (wi = wi = wi = wi). In this algorithm, we use an alternative optimization strategy (wi = wi = wi = wi = wi = wi). Each iteration has two steps. In the first step, we solve the problem in (9) we solve the problem in (9) is transferred to (9). In the second step, we solve the problem in (9) we solve the problem in (9). In the third step, we solve the problem in (9). In the third step, we solve the problem in (9)."}, {"heading": "3. Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Iterative learning algorithm", "text": "Based on the optimization problem, we develop an iterative algorithm to jointly learn the local structured output predictor and the outputs. In each iteration, we first repair the local predictor parameters and the outputs to update the upper limit parameters, then we update the local predictor parameters by fixing the outputs and the upper limit parameters, and finally we repair the local predictor parameters to update the outputs. Iterations repeat themselves for T times. The developed iterative algorithm is transformed into algorithm 1. \u2022 Algorithm 1. Iterative training algorithm of semi-supervised learning of the locally structured output predictor. \u2022 Inputs: training set, X. \u2022 Inputs: Maximum iteration number, T. \u2022 Initialization (wi, yi) | ni = 1; \u2022 For t = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, T-updating the upper limit parameters - j \u00b7 j \u00b7 j \u00b7 j, j \u00b7 j \u00b7 j \u00b7 j, j \u00b7 j \u00b7 j \u00b7 j \u00b7 j \u00b7 j, j \u00b7 j \u00b7 j \u00b7 j \u00b7 i \u00b7 \u00b2 (i \u00b7 \u00b2)."}, {"heading": "3.2. Algorithm of predicting structured output of test data point", "text": "In this section, we will discuss how to use these local predictors for the task of predicting the structured output of a new test data point. Suppose the input characteristic vector of the new incoming data point is x, to predict its output, we will first find out what neighborhoods it belongs to. To do this, we will first find its closest neighbors from the training set and call the set of its k closest neighbors Nx. We will assume that x is in the neighbors of the data points in Nx, and use their local predictors to predict the structured results of x. Considering a candidate output, we will use y these local predictors to get k matching values. The average matching score is used as the final matching score that matches x and y."}, {"heading": "4. Experiments", "text": "In this section we evaluate the performance of the proposed semi-supervised structured output prediction method. Experiments are carried out using three benchmark data sets. We first compare them with several modern semi-supervised structured output prediction methods and then experimentally analyze their sensitivity to the parameters k and C."}, {"heading": "4.1. Benchmark data sets", "text": "We used three benchmark datasets in our experiments, which are discussed as follows: \u2022 Dataset I - DDSM mammography image data set: The last dataset is a medical dataset containing mammography images [50]. This dataset contains 2620 pairs of images and they belong to three different classes, normal, cancerous and benign. Thus, this is a three-stage image processing problem. Some examples of images from this dataset are in Fig. 1. To represent the class of each image pair, we encode it as a three-stage binary vector. For the i-th image pair, its structured output is a vector called yi = [yi2, yi3] >, where eyi1 = {1, if the i \u2212 th image pair belongs to the normal class, 0, otherwise.yi2 = {1, if the i \u2212 th image pair belongs to another category, if the i \u2212 th image pair belongs to the cancer class."}, {"heading": "4.2. Experiment setup", "text": "To perform the experiments, we use the 10-fold cross-validation strategy to divide the training and test set, randomly dividing it into ten sets of the same size for a whole set of data, each set being used as a test set, while the remaining nine sets are combined and used as a training set. The proposed algorithm is applied to the training set to learn the local predictors, and then the local predictors are used to predict the structured outputs of the data points of the test set. Average structured loss over the test set is used to evaluate the performance of the proposed algorithm. In the case of a test set, T with nT test data points, the average structured loss is defined as follows: Average structured loss = 1nT value i: xi value T value (yi, y value) (y value) (28% value), while the results of the structured test are structured as follows."}, {"heading": "4.3. Comparison to state-of-the-arts", "text": "We first compare the proposed algorithm with some state-of-the-art semi-supervised prediction algorithms, including those proposed by Altun et al. [40], Brefeld and Scheffer [41], Suzuki et al. [42] and Jiang et al. [43]. All four competing algorithms learn a single global prediction algorithm for all data points, and our algorithm is the only algorithm that investigates the local distribution of the data set and learns the local prediction algorithms for different neighborhoods. Our algorithm is referred to as the semi-supervised Local Structured Prediction Algorithm (SSLSOP). The average losses of the compared algorithms over three different datasets are given in Table 1. Results in Table 1 indicate that the proposed algorithm works better than all competing algorithms. For example, only SSLSOP prediction algorithms can predict an average structured loss below 400 combined problems."}, {"heading": "4.4. Sensitivity to parameters", "text": "In our goal there are two important parameters, k, the size of each neighborhood, and C, the compromise parameters. To investigate the sensitivity of the algorithm to these two parameters, we plot the curves of the results with different values of the parameters. These sensitivity curves to parameters k over four benchmark datasets are shown in Figure 3. From the figures we note that the algorithm is stable over the changes of parameter k. For different datasets, the optimal neighborhood variables differ. For example, the lowest average structural loss is achieved over dataset I at k = 20, while for dataset II it is achieved at k = 100. The sensitivity curves of C are shown in Figure 4. From this figure we can also see that the performance of the proposed algorithm is stable over the changes of parameter C, especially over dataset II. We cannot detect a clear trend of changes in results that correspond to changes in the values of C., which is the selection of the algorithm."}, {"heading": "4.5. Repainting time analysis", "text": "We are also interested in the runtime of the proposed method, SSLSOP, and its competing methods. The runtime of these methods over four benchmark data sets is shown in Fig. 5. The figure shows that the proposed method, SSLSOP, consumes the second shortest runtime over three data sets, the only exception being the results over data set II. The algorithm that takes the least time is that of Altun et al. [40], but its prediction results are not satisfactory. The algorithm that takes the most time is that of Jiang et al. [39], but its prediction results are not as good as SSLSOP."}, {"heading": "5. Conclusion", "text": "To solve the problem of the diversity of local distributions, we propose to learn local structured output predictors for neighborhoods of different data points. Furthermore, we propose to learn the missing outputs of the unlabeled data points. We build a new minimization problem to learn the local structured output predictors and the missing structured outputs at the same time. This problem is modeled as a common minimization of local predictor complexity and locally structured output loss. The problem is optimized by gradient descent and we developed an iterative algorithm to learn the local predictors. Experiments on benchmark datasets of medical image classification, natural image classification, informatic paper classification and sentence division marking."}, {"heading": "6. Future work", "text": "In the future, we will examine how to adapt the proposed algorithm to big data sets by using the Big Data Processing Framework (HDFS) to store the dataset. If the dataset is large, i.e., the number of data points n is large, we can use the Hadoop Distributed Filesystem (HDFS) to store the dataset. The entire dataset is divided into a few small subsets, and different subsets are stored in different clusters. The proposed algorithm has three basic steps, and each of them can be easily paralleled. The three steps are listed as follows: 1. Identify the k closest neighbors of each dataset, 2. Update the locally structured output predictor parameter of each neighborhood, and 3. Update the outputs of each dataset point. To find the k closest neighbors of a dataset from the distributed big data set, we can use the Map-reduction framework to reduce the results of the neighborhood functions to one point at a time by updating the neighboring only the neighboring functions to the neighboring structure."}], "references": [{"title": "Multiple parameter control for ant colony optimization applied to feature selection problem", "author": ["G. Wang", "H. Chu", "Y. Zhang", "H. Chen", "W. Hu", "Y. Li", "X. Peng"], "venue": "Neural Computing and Applications 26 (7) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Margin-based over-sampling method for learning from imbalanced datasets", "author": ["X. Fan", "K. Tang", "T. Weise"], "venue": "in: Advances in Knowledge Discovery and Data Mining, Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved lower bound for bayesian network structure learning", "author": ["X. Fan", "C. Yuan"], "venue": "in: Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Feedforward kernel neural networks", "author": ["S. Wang", "Y. Jiang", "F.-L. Chung", "P. Qian"], "venue": "generalized least learning machine, and its deep learning with application to image classification, Applied Soft Computing Journal 37 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "An intrusion detection system using network traffic profiling and online sequential extreme learning machine", "author": ["R. Singh", "H. Kumar", "R. Singla"], "venue": "Expert Systems with Applications 42 (22) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularized minimum class variance extreme learning machine for language recognition, Eurasip", "author": ["J. Xu", "W.-Q. Zhang", "J. Liu", "S. Xia"], "venue": "Journal on Audio, Speech, and Music Processing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "in: Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on, IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "in: 2014 IEEE 26th International Conference on Tools with Artificial Intelligence ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-kernel learning for multivariate performance measures optimization", "author": ["F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald"], "venue": "Neural Computing and Applications ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Classification of unsteady flow patterns in a rotodynamic blood pump: Introduction of non-dimensional regime map", "author": ["F. Shu", "S. Vandenberghe", "J. Brackett", "J. Antaki"], "venue": "Cardiovascular Engineering and Technology 6 (3) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhanced maximum auc linear classifier", "author": ["X. Fan", "K. Tang"], "venue": "in: Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, Vol. 4, IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding optimal bayesian network structures with constraints learned from data", "author": ["X. Fan", "B. Malone", "C. Yuan"], "venue": "in: Proceedings of the 30th Annual Conference on Uncertainty in Artificial Intelligence (UAI-14)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Tightening bounds for bayesian network structure learning", "author": ["X. Fan", "C. Yuan", "B. Malone"], "venue": "in: Proceedings of the 28th AAAI Conference on Artificial Intelligence", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A bayesian approach for sleep and wake classification based on dynamic time warping method", "author": ["C. Fu", "P. Zhang", "J. Jiang", "K. Yang", "Z. Lv"], "venue": "Multimedia Tools and Applications ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J.J.-Y. Wang", "H. Bensmail"], "venue": "in: Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on, IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Representing data by sparse combination of contextual data points for classification", "author": ["J. Wang", "Y. Zhou", "M. Yin", "S. Chen", "B. Edwards"], "venue": "in: Advances in Neural Networks\u2013ISNN 2015, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Biomarker binding on an antibodyfunctionalized biosensor surface: The influence of surface properties", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "electric field, and coating density, The Journal of Physical Chemistry C 118 (26) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Achieving profitable biological sludge disintegration through phase separation and predicting its anaerobic biodegradability by non linear regression model", "author": ["S. Kavitha", "S. Adish Kumar", "S. Kaliappan", "I. Yeom", "J. Banu"], "venue": "Chemical Engineering Journal 279 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Combined use of milp and multi-linear regression to simplify lca studies", "author": ["J. Pascual-Gonzlez", "C. Pozo", "G. Guilln-Goslbez", "L. Jimnez-Esteller"], "venue": "Computers and Chemical Engineering 82 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Studies of the adaptive network-constrained linear regression and its application", "author": ["H. Yang", "D. Yi"], "venue": "Computational Statistics and Data Analysis 92 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Weibull and lognormal taguchi analysis using multiple linear regression", "author": ["M. Pia-Monarrez", "J. Ortiz-Yaez"], "venue": "Reliability Engineering and System Safety 144 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The relationship of dependency relations and parts of speech in hungarian", "author": ["V. Vincze"], "venue": "Journal of Quantitative Linguistics 22 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating word embeddings and a revised corpus for part-of-speech tagging in portuguese", "author": ["E. Fonseca", "J. G Rosa", "S. Alusio"], "venue": "Journal of the Brazilian Computer Society 21 (1) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Leveraging topic modeling and part-of-speech tagging to support combinational creativity in requirements engineering", "author": ["T. Bhowmik", "N. Niu", "J. Savolainen", "A. Mahmoud"], "venue": "Requirements Engineering ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilingual part-of-speech tagging with weightless neural networks", "author": ["H. Carneiro", "F. Frana", "P. Lima"], "venue": "Neural Networks 66 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Pos-rs: A random subspace method for sentiment classification based on part-of-speech analysis", "author": ["G. Wang", "Z. Zhang", "J. Sun", "S. Yang", "C. Larson"], "venue": "Information Processing and Management 51 (4) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Multifractal characterisation and classification of bread crumb digital images", "author": ["R. Baravalle", "C. Delrieux", "J. Gmez"], "venue": "Eurasip Journal on Image and Video Processing 2015 (1) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel information transferring approach for the classification of remote sensing images", "author": ["J. Gao", "L. Xu", "J. Shen", "F. Huang", "F. Xu"], "venue": "Eurasip Journal on Advances in Signal Processing 2015 (1) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "A computer vision approach for automated analysis and classification of microstructural image data", "author": ["B. Decost", "E. Holm"], "venue": "Computational Materials Science 110 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian sample steered discriminative regression for biometric image classification", "author": ["G. Gao", "J. Yang", "S. Wu", "X. Jing", "D. Yue"], "venue": "Applied Soft Computing Journal 37 ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning distributed representations for structured output prediction", "author": ["V. Srikumar", "C. Manning"], "venue": "Vol. 4", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "The impact of incomplete knowledge on the evaluation of protein function prediction: A structured-output learning perspective", "author": ["Y. Jiang", "W. Clark", "I. Friedberg", "P. Radivojac"], "venue": "Bioinformatics 30 (17) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Augmenting image descriptions using structured prediction output", "author": ["Y. Han", "X. Wei", "X. Cao", "Y. Yang", "X. Zhou"], "venue": "IEEE Transactions on Multimedia 16 (6) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured output prediction with hierarchical loss functions for seafloor imagery taxonomic categorization", "author": ["N. Nourani-Vatani", "R. Lpez-Sastre", "S. Williams"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9117 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization in structured output space for semi-supervised structured output prediction", "author": ["F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux"], "venue": "Neural Computing and Applications ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum margin semisupervised learning for structured variables", "author": ["Y. Altun", "M. Belkin", "D.A. Mcallester"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised learning for structured output variables", "author": ["U. Brefeld", "T. Scheffer"], "venue": "in: Proceedings of the 23rd international conference on Machine learning, ACM", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "H", "author": ["J. Suzuki", "A. Fujino"], "venue": "Isozaki, Semi-supervised structured output learning based on a hybrid generative and discriminative approach., in: EMNLP-CoNLL", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "Manifold regularization in structured output space for semi-supervised structured output prediction", "author": ["F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux"], "venue": "Neural Computing and Applications ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards a probabilistic semisupervised kernel minimum squared error algorithm", "author": ["H. Gan", "R. Huang", "Z. Luo", "Y. Fan", "F. Gao"], "venue": "Neurocomputing 171 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised feature selection based on local discriminative information", "author": ["Z. Zeng", "X. Wang", "J. Zhang", "Q. Wu"], "venue": "Neurocomputing 173 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Enhancing semi-supervised learning through label-aware base kernels", "author": ["Q. Wang", "K. Zhang", "Z. Chen", "D. Wang", "G. Jiang", "I. Marsic"], "venue": "Neurocomputing 171 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Manifold regularization for structured outputs via the joint kernel", "author": ["C. Hu", "J.T. Kwok"], "venue": "in: Neural Networks (IJCNN), The 2010 International Joint Conference on, IEEE", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised distance metric learning based on local linear regression for data clustering", "author": ["H. Zhang", "J. Yu", "M. Wang", "Y. Liu"], "venue": "Neurocomputing 93 ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Local ridge regression for face recognition", "author": ["H. Xue", "Y. Zhu", "S. Chen"], "venue": "Neurocomputing 72 (4) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "The digital database for screening mammography", "author": ["M. Heath", "K. Bowyer", "D. Kopans", "R. Moore", "W.P. Kegelmeyer"], "venue": "in: Proceedings of the 5th international workshop on digital mammography, Citeseer", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "A", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva"], "venue": "Torralba, et al., Sun database: Large-scale scene recognition from abbey to zoo, in: Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, IEEE", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient clustering of highdimensional data sets with application to reference matching", "author": ["A. McCallum", "K. Nigam", "L.H. Ungar"], "venue": "in: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": ["T.K. Sang"], "venue": "in: Proceedings of the 6th conference on natural language learning,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}], "referenceMentions": [{"referenceID": 10, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 11, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 12, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 13, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 14, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 15, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 16, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 17, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 70, "endOffset": 106}, {"referenceID": 18, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 180, "endOffset": 196}, {"referenceID": 19, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 180, "endOffset": 196}, {"referenceID": 20, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 180, "endOffset": 196}, {"referenceID": 21, "context": "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].", "startOffset": 180, "endOffset": 196}, {"referenceID": 22, "context": "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].", "startOffset": 239, "endOffset": 259}, {"referenceID": 23, "context": "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].", "startOffset": 239, "endOffset": 259}, {"referenceID": 24, "context": "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].", "startOffset": 239, "endOffset": 259}, {"referenceID": 25, "context": "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].", "startOffset": 239, "endOffset": 259}, {"referenceID": 26, "context": "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].", "startOffset": 239, "endOffset": 259}, {"referenceID": 27, "context": "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].", "startOffset": 190, "endOffset": 206}, {"referenceID": 28, "context": "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].", "startOffset": 190, "endOffset": 206}, {"referenceID": 29, "context": "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].", "startOffset": 190, "endOffset": 206}, {"referenceID": 30, "context": "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].", "startOffset": 190, "endOffset": 206}, {"referenceID": 31, "context": "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].", "startOffset": 123, "endOffset": 143}, {"referenceID": 32, "context": "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].", "startOffset": 123, "endOffset": 143}, {"referenceID": 33, "context": "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].", "startOffset": 123, "endOffset": 143}, {"referenceID": 34, "context": "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].", "startOffset": 123, "endOffset": 143}, {"referenceID": 35, "context": "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].", "startOffset": 123, "endOffset": 143}, {"referenceID": 36, "context": "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].", "startOffset": 80, "endOffset": 96}, {"referenceID": 37, "context": "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].", "startOffset": 80, "endOffset": 96}, {"referenceID": 38, "context": "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].", "startOffset": 80, "endOffset": 96}, {"referenceID": 39, "context": "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].", "startOffset": 80, "endOffset": 96}, {"referenceID": 40, "context": "Learning from such a training set is call semi-supervised learning [44, 45, 46].", "startOffset": 67, "endOffset": 79}, {"referenceID": 41, "context": "Learning from such a training set is call semi-supervised learning [44, 45, 46].", "startOffset": 67, "endOffset": 79}, {"referenceID": 42, "context": "Learning from such a training set is call semi-supervised learning [44, 45, 46].", "startOffset": 67, "endOffset": 79}, {"referenceID": 36, "context": "[40] proposed the problem of semi-supervised learning with structured outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Brefeld and Scheffer [41] proposed to solve the problem of semi-supervised structured output prediction by learning in the space of input-output space, and using co-training method.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "[42] proposed a hybrid method to solve the problem of semi-supervised structured output learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43] proposed to regularize the structured outputs by the manifold constructed from the input space directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Our work approximate the upper bound of the structured loss, and is inspired by the lower bound approximation of the structure learning of the Bayesian network [16, 4].", "startOffset": 160, "endOffset": 167}, {"referenceID": 2, "context": "Our work approximate the upper bound of the structured loss, and is inspired by the lower bound approximation of the structure learning of the Bayesian network [16, 4].", "startOffset": 160, "endOffset": 167}, {"referenceID": 13, "context": "Thus we also discuss the works of bound approximation technologies of [16, 4].", "startOffset": 70, "endOffset": 77}, {"referenceID": 2, "context": "Thus we also discuss the works of bound approximation technologies of [16, 4].", "startOffset": 70, "endOffset": 77}, {"referenceID": 13, "context": "[16] proposed to tighten the upper and lower bounds of the breadth-first branch and bound algorithm for the learning of Bayesian network structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The work of [16] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "[4] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 39, "context": "It has been shown that using local connections is an effective way to model the connections among different data points [43].", "startOffset": 120, "endOffset": 124}, {"referenceID": 43, "context": "More specifically, with the nearest neighbor graph, we hope that the neighboring data points can obtain similar structured outputs from the predictor [47, 43].", "startOffset": 150, "endOffset": 158}, {"referenceID": 39, "context": "More specifically, with the nearest neighbor graph, we hope that the neighboring data points can obtain similar structured outputs from the predictor [47, 43].", "startOffset": 150, "endOffset": 158}, {"referenceID": 44, "context": "To solve this problem, we propose to learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [48, 49].", "startOffset": 216, "endOffset": 224}, {"referenceID": 45, "context": "To solve this problem, we propose to learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [48, 49].", "startOffset": 216, "endOffset": 224}, {"referenceID": 46, "context": "\u2022 Data set I - DDSM mammography image data set: The last data set is a medical imaging data set which contains Mammography images [50].", "startOffset": 130, "endOffset": 134}, {"referenceID": 47, "context": "\u2022 Data set II - SUN natural image data set: The second data set is a image data set [51].", "startOffset": 84, "endOffset": 88}, {"referenceID": 48, "context": "\u2022 Data set III - Cora research paper data set: This data set is a set of computer science research papers [52].", "startOffset": 106, "endOffset": 110}, {"referenceID": 49, "context": "\u2022 Data set IV - Spanish news wire article sentence data set: This data set is a data set for named entity recognition problem of natural language processing [53].", "startOffset": 157, "endOffset": 161}, {"referenceID": 36, "context": "[40], Brefeld and Scheffer [41], Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40], Brefeld and Scheffer [41], Suzuki et al.", "startOffset": 27, "endOffset": 31}, {"referenceID": 38, "context": "[42], and Jiang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[40] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "504 Brefeld and Scheffer [41] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 38, "context": "[42] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43] is the second best method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[40], however, its prediction results are not satisfying.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[39], but its prediction results are not as good as SSLSOP.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both inputoutput pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method.", "creator": "LaTeX with hyperref package"}}}