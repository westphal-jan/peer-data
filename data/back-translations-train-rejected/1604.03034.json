{"id": "1604.03034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "M3: Scaling Up Machine Learning via Memory Mapping", "abstract": "To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory's strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.", "histories": [["v1", "Mon, 11 Apr 2016 17:12:14 GMT  (1793kb,D)", "http://arxiv.org/abs/1604.03034v1", "2 pages, 1 figure, 1 table"]], "COMMENTS": "2 pages, 1 figure, 1 table", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["dezhi fang", "duen horng chau"], "accepted": false, "id": "1604.03034"}, "pdf": {"name": "1604.03034.pdf", "metadata": {"source": "CRF", "title": "M3: Scaling Up Machine Learning via Memory Mapping", "authors": ["Dezhi Fang", "Duen Horng Chau"], "emails": ["dezhifang@gatech.edu", "polo@gatech.edu"], "sections": [{"heading": null, "text": "CCS concepts \u2022 Software and its Engineering \u2192 Virtual Storage; \u2022 Computer Methods \u2192 Machine Learning;"}, {"heading": "1. INTRODUCTION", "text": "The use of virtual memory to extend algorithms for obsolete data has received increasing attention in data analysis communities. Recent research has shown the strong potential of virtual memory to scale graph algorithms on a single PC [4, 3]. Virtual storage approaches are available on almost all modern platforms, easy to implement and use, and can process graphs with up to 6 billion edges [3]. Some implementations with a single thread on a PC can even surpass popular distributed systems such as Spark (128 cores) [4]. Mapping a data set into the virtual memory of a machine allows the dataset to be treated identically as an in-memory dataset. The algorithm developer no longer needs to explicitly determine how to partition the (large) dataset, nor manage which partitions should be loaded into or unloaded from RAM."}, {"heading": "2. SCALING UP USING M3", "text": "Because existing work has focused on graph algorithms such as PageRank and finding associated components, we are investigating whether a similar virtual memory-based approach can process large amounts of data for machine learning. Inspired by previous work on graph computation, our M31 approach uses memory mappings to expand a single machine's ability to process large amounts of data for machine learning algorithms. Because memory mappings treat a data set identically to an in-memory data set, M3 is a transparent scaling strategy that developers can easily apply and require minimal changes to existing code. Table 1, for example, shows that existing algorithm implementations with minimal code changes and a trivial helper function can easily handle much larger, memory-mapped datasets. Modern 64-bit machines have address spaces large enough to accommodate large datasets (up to 1024B), as the statistical system can continue to integrate such data into a variety of operations."}, {"heading": "3. EXPERIMENTS", "text": "Our current review focuses on: (1) understanding how M3 scales with increasing data size; (2) how M3 compares to distributed systems like Spark, as previous work suggests the possibility that a single machine can outperform a computer cluster [4]. Experiment Setup. All tests with M3 are performed on a desktop computer with Intel i7-4770K 3.50 GHz quad CPU (8 hyperthreads), 4 \u00d7 8GB RAM, 1TB SSD from OCZ RevoDrive 350. We used Amazon EC2 m3.2xlarge instances for spark experiments. Each instance has 8 vCPUs (hyperthreads of an Intel Xeon core) with 30GB of memory and 2 \u00d7 80GB SSDs. Spark clusters are created by Amazon Elastic MapReduce and the data sets are stored on the HDFS.datasets (hyperthreads of an Intel Xeon core) with 30GB and 80GB of memory."}, {"heading": "3.1 Key Findings & Implications", "text": "1. M3 scales linearly when the data fit into the RAM and when it is outside the core for logistical regression (Figure 1a). The dotted vertical line in the figure indicates the RAM size (32 GB). M3 scales linearly both when the data set fits into the RAM (yellow region in Figure 1a) and when it exceeds the RAM (green dotted line), at a higher scaling constant than expected. Considering the resource utilization of M3, we saw that M3 I / O is bound: Disk I / O has been utilized to 100%, while the CPU has only been utilized to about 13%. This points to a strong potential for M3, which achieves even higher speeds when using faster disks, or configurations such as RAID 0. 2. M3's speed (a PC) is comparable to 8-instance and significantly faster than Spark 4-instance."}, {"heading": "4. CONCLUSIONS & ONGOING WORK", "text": "We are taking an important first step in assessing the feasibility of using virtual memory as a basic, alternative method of scaling machine learning algorithms. M3 adds an interesting perspective to existing solutions based mainly on distributed systems. We are contributing to: (1) our latest finding that memory mapping could become a viable technique for scaling generic machine learning algorithms when the dataset exceeds RAM; (2) M3, an easy-to-use approach that allows existing machine learning implementations to work with out-core datasets; (3) our observations that M3 on a PC can achieve a speed significantly faster than a 4-instance spark cluster and is comparable to an 8-instance cluster. We will better understand our M3 approach to a wide range of machine learning (including online learning) and data mining algorithms versus, for example, the scope of access to and algorithms."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "We thank Dr. Ryan Curtin and Minsuk (Brian) Kahng for their feedback. This work was supported by NSF grants IIS-1217559, TWC-1526254, IIS-1563816 and gifts from Google, Symantec, Yahoo, eBay, Intel, Amazon, LogicBlox."}, {"heading": "6. REFERENCES", "text": "[1] S. Boyd-Wickizer, A. T. Clements, Y. Mao, A. Pesterev, M. F. Kaashoek, R. Morris, N. Zeldovich, et al. An Analysis of Linux Scalability on Many Nuclei. In OSDI, Volume 10, pp. 86-93, 2010. [2] R. Curtin, J. R. Cline, N. P. Slagle, W. B. March, P. Ram, N. A. Mehta, and A. G. Gray. mlpack: A scalable C + + machine learning library. Journal of Machine Learning Research, 14: 801-805, 2013. [3] Z. Lin, M. Kahng, K. M. Sabrin, D. H. P. Chau, H. Lee, and U. Kang. Mmap: Fast billion-scale graph computation on a PC via memory mapping. In Big Data (Big Data), 2014 IEEE International Conference on, pp. 159-164. IEEE, 2014. [4] F. McSherry, M. Isard, M. Murray, 15. D. Murray."}], "references": [{"title": "An analysis of linux scalability to many cores", "author": ["S. Boyd-Wickizer", "A.T. Clements", "Y. Mao", "A. Pesterev", "M.F. Kaashoek", "R. Morris", "N. Zeldovich"], "venue": "In OSDI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "mlpack: A scalable C++ machine learning library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Mmap: Fast billion-scale graph computation on a pc via memory mapping", "author": ["Z. Lin", "M. Kahng", "K.M. Sabrin", "D.H.P. Chau", "H. Lee", "U. Kang"], "venue": "In Big Data (Big Data),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Scalability! but at what cost", "author": ["F. McSherry", "M. Isard", "D.G. Murray"], "venue": "In 15th Workshop on Hot Topics in Operating Systems (HotOS XV),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recent research demonstrated virtual memory\u2019s strong potential to scale up graph algorithms on a single PC [4, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Recent research demonstrated virtual memory\u2019s strong potential to scale up graph algorithms on a single PC [4, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Available on almost all modern platforms, virtual memory based approaches are straight forward to implement and to use, and can handle graphs with as many as 6 billion edges [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "Some single-thread implementations on a PC can even outperform popular distributed systems like Spark (128 cores) [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "methods including least recent used caching and read-ahead to achieve efficiency [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "To test the feasibility of M3, we minimally modified mlpack, an efficient machine learning library written in C++ [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Our current evaluation focuses on: (1) understanding of how M3 scales with increasing data sizes; and (2) how M3 compares with distributed systems such as Spark, as prior work suggested the possibility that a single machine can outperform a computer cluster [4].", "startOffset": 258, "endOffset": 261}, {"referenceID": 3, "context": "tion that suggests cluster may not be necessary for moderatelysized datasets [4, 3].", "startOffset": 77, "endOffset": 83}, {"referenceID": 2, "context": "tion that suggests cluster may not be necessary for moderatelysized datasets [4, 3].", "startOffset": 77, "endOffset": 83}], "year": 2016, "abstractText": "To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory\u2019s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.", "creator": "LaTeX with hyperref package"}}}