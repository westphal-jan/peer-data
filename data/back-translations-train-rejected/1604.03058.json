{"id": "1604.03058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Binarized Neural Networks on the ImageNet Classification Task", "abstract": "We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet LSVRC-2102 dataset classification task and achieved a good performance. With a moderate size network of 10 layers, we obtained top-5 classification accuracy rate of 81 percent on validation set which is much better than previous published results. We expect training networks of a much better performance through increase network depth would be straight forward by following our current strategies. A detailed discussion on strategies used in the network training is included as well as preliminary analysis.", "histories": [["v1", "Mon, 11 Apr 2016 18:39:33 GMT  (471kb)", "http://arxiv.org/abs/1604.03058v1", null], ["v2", "Wed, 13 Apr 2016 03:22:37 GMT  (0kb,I)", "http://arxiv.org/abs/1604.03058v2", "More testing and verification is needed"], ["v3", "Mon, 24 Oct 2016 15:25:06 GMT  (430kb)", "http://arxiv.org/abs/1604.03058v3", "Updated with new results"], ["v4", "Tue, 8 Nov 2016 00:38:03 GMT  (512kb)", "http://arxiv.org/abs/1604.03058v4", null], ["v5", "Sat, 19 Nov 2016 01:37:40 GMT  (512kb)", "http://arxiv.org/abs/1604.03058v5", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["xundong wu", "yong wu", "yong zhao"], "accepted": false, "id": "1604.03058"}, "pdf": {"name": "1604.03058.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wuxundong@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.03 058v 1 [cs.C V"}, {"heading": "1 Introduction", "text": "Current implementations of deep neural networks in a wide variety of areas rely heavily on high computing power, energy-hungry hardware such as GPUs, which restricts the implementation of these neural networks in embedded environments such as mobile phones or portable hardware. Recently (Rastegari et al., 2016; Courbariaux et al., 2016, 2015) attempts have been made to reduce the computing need for deep neural networks (Han et al., 2015; Chen et al., 2015; Imaderberg et al., 2014). More recently (Rastegari et al., 2016; Courbariaux et al., 2015) attempts have been made to reduce synapse weight and / or intermedial signal representation to binary form. Binarization of neural networks has been shown to drastically reduce the computing need, as well as the amount of memory required for weight and intermediate result storage. These approaches are particularly useful for memory-limited GA environments."}, {"heading": "2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Softmax Input Rescaling", "text": "A pure BNN layer has an output of large magnitude due to the binary weights and binary inputs from the previous layer. In the initial state of the network for a standard density, fully connected layer, if we assume that the layer input is of 50% density, i.e. half of the inputs have activation of + 1, the rest has activation of -1, and the synapse weights are also initialized to be 50% density. The mean of the output of this layer will be 0, while half of the inputs N, here N is the number of inputs that receives the layer that is used for the last layer of a network that is used for tasks such as ImageNet data. With a typical N = 4096 used in this study, which will give us a real value of 64, this will not be a problem if the output of a BNN layer is followed by the truth that a batch normalization layer like in Couraribaux, 2016, brings the normalization to the batch area, as activation)."}, {"heading": "2.2 First layer architecture", "text": "In the original BNN layer designed for the CIFAR 10 dataset, as in the first layer (Courbariaux et al., 2016), all network layers are binary. However, the filters used in the first layer of this network have a small core size of 3 and a small step of 1. However, such a design is suitable for CIFAR data as it comes with an image size of 32 x 32, which is very small compared to the typical input image size used in ImageNet data of 224x224. In order to reduce the computing requirement for the network, typically a large revolutionary core size and large step sizes (2 or 4) are used on the first layer and immediately thereafter a max pooling layer trained on ImageNet data. Therefore, most networks implemented in this work also start with large core sizes (7 or 11) and large step layers on top of the first or second layer (max)."}, {"heading": "2.3 Weight adaptation behavior analysis and learning rate selection", "text": "In a BNN layer, real synaptic weights are used during the training phase (Courbariaux et al., 2016). This additional resolution is removed at the level of the output of numerical layers. It is assumed that real weights are needed to compensate for the noise in SGD learning (Courbariaux et al., 2016). Here, in fact, we show that this is the case. If a high learning rate is used for training, as in (Rastegari et al., 2016), after an epoch of training, we observed that most weight loss remains about -1 and + 1 (Figure 3a) because the weight is truncated. Under such a condition, weights jump quickly between edge values of -1 and + 1 with low accumulation process. Accordingly, a rather slow loss decline is observed (Figure 3c). If we lower the learning rate to 0.001, we observed a weight distribution that moves more strongly between edge values of -1 and + 1 with low accumulation process."}, {"heading": "3 Discussion", "text": "Our results show that an ImageNet scale of BNN with a good network architecture and training strategies can be trained to perform well. At the moment, the best network we have trained is a 10-layer network with 81 percent top-5 accuracy rate, which is much better than the result presented in (Rastegari et al., 2016). Limited by the computing resources available, we mostly tested networks with real weight gain in the first layer of large filters and large increments. Assessing some of our small experiments and also the results shown by (Courbariaux et al., 2016), it is very possible that a purely binary network with small filter size and smaller step speed can achieve a modern level of performance. A purely binary network is also desirable as it further reduces and simplifies network design in confined environments."}], "references": [{"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Courbariaux", "Matthieu", "Hubara", "Itay", "Soudry", "Daniel", "El-Yanivand", "Ran", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 0, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 4, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 5, "context": "Recently, (Rastegari et al., 2016; Courbariaux et al., 2016, 2015) have been trying to reduce synapse weight and/or intermediate signal representation to binary form.", "startOffset": 10, "endOffset": 66}, {"referenceID": 2, "context": "In this work, we designed a network architecture based on Binarized Neural Networks (BNN), originally proposed by (Courbariaux et al., 2016), and trained networks on the ImageNet classification task.", "startOffset": 114, "endOffset": 140}, {"referenceID": 2, "context": "This won\u2019t be a problem if the output of a BNN layer is followed by a batch normalization layer as in (Courbariaux et al., 2016), since batch normalization bring the activation into right range.", "startOffset": 102, "endOffset": 128}, {"referenceID": 2, "context": "In (Courbariaux et al., 2016) case, the network output has small dimensionality, for example in MNIST case the output dimension is 10.", "startOffset": 3, "endOffset": 29}, {"referenceID": 2, "context": "In the original BNN layer designed for CIFAR-10 dataset as in (Courbariaux et al., 2016), all layers of network are binary.", "startOffset": 62, "endOffset": 88}, {"referenceID": 2, "context": "It is believed that real-valued weights are needed for smooth out the noise in the SGD learning (Courbariaux et al., 2016).", "startOffset": 96, "endOffset": 122}, {"referenceID": 5, "context": "When a high learning rate is used for training as suggested in (Rastegari et al., 2016), after one epoch of training, we observed most weight value stay around -1 and +1 (Figure 3a) because of weight clipping.", "startOffset": 63, "endOffset": 87}, {"referenceID": 5, "context": "At moment the best network we trained is a 10 layers network with 81 percent top 5 accuracy rate, which is much better than the result shown in (Rastegari et al., 2016).", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "Judge from some of our small scale experiments and also results shown by (Courbariaux et al., 2016), it is very possible a purely binary network can achieve state of art level of performance with small filter size and smaller stride step.", "startOffset": 73, "endOffset": 99}], "year": 2016, "abstractText": "We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet LSVRC-2102 dataset classification task and achieved a good performance. With a moderate size network of 10 layers, we obtained top-5 classification accuracy rate of 81 percent on validation set which is much better than previous published results. We expect training networks of a much better performance through increase network depth would be straight forward by following our current strategies. A detailed discussion on strategies used in the network training is included as well as preliminary analysis.", "creator": "LaTeX with hyperref package"}}}