{"id": "1604.03073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Reservoir computing for spatiotemporal signal classification without trained output weights", "abstract": "Reservoir computing is a recently introduced machine learning paradigm that has been shown to be well-suited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the `hidden layer' nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks, one may forgo the weight training step entirely and instead use a simple supervised clustering method. The proposed method is analyzed theoretically and explored through numerical experiments on real-world data. The examples demonstrate that the proposed clustering method outperforms the traditional trained output weight approach in terms of speed, accuracy, and sensitivity to reservoir parameters.", "histories": [["v1", "Mon, 11 Apr 2016 19:14:05 GMT  (100kb,D)", "http://arxiv.org/abs/1604.03073v1", "10 pages, 5 figures"], ["v2", "Tue, 19 Jul 2016 13:28:17 GMT  (547kb,D)", "http://arxiv.org/abs/1604.03073v2", "12 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["ashley prater"], "accepted": false, "id": "1604.03073"}, "pdf": {"name": "1604.03073.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ashley Prater"], "emails": [], "sections": [{"heading": "Introduction", "text": "It is only a matter of time before there will be a real analysis in which the nodes hardly need to be connected to each other in order to adapt specific data. As the weights of the reservoir design are fixed, it requires only a simple initialization step, as a counterpart to more traditional recurrent neural networks, whose weight and connections need to be learned in a tedius backpropagation training step [3]. Strengths of the reservoir design include ease of initialization, along with a reservoir ability to immediately adapt to new data and applications. Reservoirs, like all recurrent neural networks, are based on the premise that the state of the reservoir at a given time should depend on the current values of the input signal and reservoir states."}, {"heading": "Reservoir Computing Models for Classification", "text": "Suppose u (t) is an input signal at the time t, possibly after the application of a multiplex mask. The values of the reservoir nodes at the time t are called reservoir states and are denoted by the vector x (t). The n-th entry of this vector, xn (t) denotes the state of the n-th reservoir node at the time. The dynamics of the ESN and TDR architecture are described by the following models: ESN: x (t) = f (Winu (t) + Wresx (t \u2212 1)))). (1) TDR: xn (t) = {f (t) + \u03b2xN \u2212 1 (t \u2212 1)))) when n = 0, xN \u2212 1 (t \u2212 1), when n = 1, 2,........, N \u2212 nodes model."}, {"heading": "Output Weights Trained using Regularized Least Squares", "text": "Classically, signals are classified in a reservoir computer that uses a linear combination of reservoir states determined by a collection of trained output weights [1, 10, 11, 12]. To train the output weights, input signals of all classes of the training set are processed, with all reservoir states stored in a matrix Xtrain, RTS, and N. (Output weights are selected to map the reservoir states to predefined target functions.) Typically, a unique target vector yk, RT is selected for each input signal, S is the total number of signals in the training set, and N is the number of nodes in the reservoir. Output weights are selected to map the reservoir states to predefined target functions. Typically, a unique target vector yk, RT is selected for each class. All individual target vectors are arranged in a diagonal block of targets, Ytarget matrix are combined with target blocks, Ytarget blocks are arranged to diagonally match the target blocks."}, {"heading": "Classification via Clustering with Principal Components", "text": "The underlying idea for the training method (3) is that inputs similar to those in the reservoir produce similar results, even after applying high-dimensional nonlinear processing. Under this assumption, it is possible to classify data using a cluster method. Therefore, we propose the following method, in which the main components of the reservoir responses are used to perform classifications. For each input u (j) in the lowest class of the training set, we calculate the vector v (j) whose inputs are the norms of the reservoir node state: v (j) (t) = requires input x (j) (t)."}, {"heading": "Analysis of Reservoir Behavior", "text": "The clustering method (5) will be more precise when small variations in the input signals lead to limited differences in the reservoir states, while large discrepancies in the input signals are further apart. (5) To ensure that some combinations of the above factors can be seriously degraded [2, 7, 10, 13], the metrics used in the reservoir literature, however, tend to be experimentally investigated to see how well the reservoir response separates classes, the separation ratio [6, 14], point separation [2, 15] and class separation [16]."}, {"heading": "Numerical Example", "text": "The data used comes from the database of the United States Postal Service (USPS), taken from [24]. A sample of these images is shown in Figure 2. Each image in the data set is a 16 x 16 8-bit grayscale image that has been redesigned as a column vector with 256 lengths. Data is divided into ten classes of 1100 images each, the digits 0 to 9. For each simulation shown below, 400 images in each class are randomly selected to form the training set, while the remaining 700 images are used as a test set. Although the near pixel behavior is not obtained in x-direction by transforming each image into a column vector, the correlations are still present in the simulation shown below."}, {"heading": "Experiment Setup", "text": "Each image vector is multiplexed with a mask of length N \u2212 1, randomly selecting values from the set {\u00b1 1}, generating input vectors of length 256 (N \u2212 1). The parameter \u03b1 varies over the set {0,1, 0,2,.., 0,9}. The non-linear activation function is selected as f (x) = sin (x) for all simulations. If a reservoir with the ESN topology is used, the input weights are selected as Win = [\u03b1 0 \u00b7 0] >, and the internal weight matrix Wres is randomly filled and scaled to 20% density, so that the same data size is used for both ESN and TDR topologies to obtain the dynamic range of the data. The reservoir states are not amplified, but selected values for each step individually (1 each)."}, {"heading": "Results", "text": "The results of these simulations are shown in Figures 3-5. Figure 3 shows the average value of the separation method Sep (t) of Equation (6) over all t for each simulation. Note that the separation quality of ESN-style reservoirs is modest, with a maximum value of 2.25. Furthermore, the separation quality of ESNs depends on parameter \u03b1, but does not appear to depend on the size of reservoir N. On the other hand, the separation quality for TDR-style reservoirs is much greater and strongly depends on both, with larger reservoirs separating the classes better. Figure 4 shows the classification accuracy and the time required to classify all 7,000 images in the test datasets for each reservoir type and the combination of parameters. The clustering approach exceeds the linear output with trained weights in all simulations in terms of accuracy and CPU time. The accuracy results are shown in Figure 4 of both figures."}, {"heading": "Conclusion", "text": "The numerical experiments show that the proposed cluster method (5) exceeds the conventional approach of trained linear initial weights (3) in terms of time and accuracy for both ESNs and TDRs for all studied reservoir parameters. Accuracy achieved by the cluster approach, in contrast to the results obtained by trained initial weights, does not depend greatly on the type, size or parameter selection of the reservoir. Furthermore, the cluster approach has the potential to be several orders of magnitude faster, enabling its use in real-time systems. The cluster method described in this paper is robust and is a promising approach for use in reservoir computers for classification tasks."}, {"heading": "Author contributions statement", "text": "A. P. designed and carried out all the work."}, {"heading": "Additional information", "text": "Disclaimer Recognition: All opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the U.S. Air Force."}], "references": [{"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks - with an Erratum note", "author": ["H. Jaeger"], "venue": "Fraunhofer Institute for Autonomous Intelligent Systems, Technical report: GMD Report", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Real-time computing without stable states: a new framework for neural computational based on perturbations", "author": ["W. Maass", "H. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Comput", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "Proc. IEEE 89,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Real-time compuitation at the edge of chaos in recurrent neural networks", "author": ["N. Bertschinger", "H. Natschl\u00e4ger"], "venue": "Neural Comput", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["H. Jaeger"], "venue": "Jacobs University Bremen Technical Report", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Spatiotemporal pattern recognition via liquid state machines", "author": ["E. Goodman", "D. Ventura"], "venue": "International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Optimal nonlinear information processing capacity in delay-based reservoir computers", "author": ["L. Grigoryeva", "J. Henriques", "L. Larger", "J. Ortega"], "venue": "Sci. Rep", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A unified framework for reservoir computing and extreme learning machines based on a single time-delayed neuron", "author": ["S Ort\u0301\u0131n"], "venue": "Sci. Rep. 5,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Optoelectronic reservoir computing", "author": ["Y Paquot"], "venue": "Sci. Rep. 2", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A comparitive study of reservoir computing for temporal signal processing", "author": ["A. Goudarzi", "P. Banda", "M. Lakin", "C. Teuscher", "D. Stefanovic"], "venue": "Technical Report 11pp,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Comp. Sci. Rev", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A practical guide to applying echo state networks. in NN", "author": ["M. Luko\u0161evi\u010dius"], "venue": "Tricks of the Trade (ed. Montavon, G. et al.) 2nd edn. 650\u2013686 (Springer-Verlag Berlin Heidelberg", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Reservoir computing: a photonic neural network for information processing", "author": ["Y. Paquot", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Proc. SPIE Nonlinear Optics and Applications IV", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unifying quality metrics for reservoir", "author": ["T. Gibbons"], "venue": "networks. 2010 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "An overview of reservoir computing: theory, applications and implementations", "author": ["B. Schrauwen", "D. Verstraeten", "J. Van Campenhout"], "venue": "ESANN 2007 proceedings - European Symposium on Artificial Neural Networks, Bruges Belgium", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "On the correlation between reservoir metrics and performance for time series classification under the influence of synaptic plasticity", "author": ["J. Chrol-Cannon", "Y. Jin"], "venue": "PLOS ONE Vol 9, Iss", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Edge of chaos and prediction of computational performance for neural circuit models", "author": ["R. Legenstein", "W. Maass"], "venue": "Neural Networks", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Minimal approach to neuroinspired information processing", "author": ["M. Soriano", "D. Brunner", "M. Escalona-Mor\u00e1n", "C. Mirasso", "I. Fischer"], "venue": "Front. Comput. Neurosci", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Information processing capacity of dynamical systems", "author": ["J. Dambre", "D. Verstraeten", "B. Schrauwen", "S. Massar"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "An experimental unification of reservoir computing methods", "author": ["D Verstraeten"], "venue": "Neural Networks 20,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "On computational power and the order-chaos phase transition in reservoir computing", "author": ["B Schrauwen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["B. Nils", "H. Natschl\u00e4ger"], "venue": "Neural Comput", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Reservoir computing dynamics for single nonlinear node with delay line structure", "author": ["C. DiMarco"], "venue": "Syracuse University Technical Report", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Data for MATLAB hackers", "author": ["S. Roweis"], "venue": "www.cs.nyu.edu/roweis/data.html. (Accessed:", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Reservoir computing is a recently developed bio-inspired machine learning paradigm for the processing of spatiotemporal data [1, 2].", "startOffset": 125, "endOffset": 131}, {"referenceID": 1, "context": "Reservoir computing is a recently developed bio-inspired machine learning paradigm for the processing of spatiotemporal data [1, 2].", "startOffset": 125, "endOffset": 131}, {"referenceID": 2, "context": "Because the weights are fixed, using a reservoir requires only a simple initialization step, as opposed to more traditional recurrent neural networks whose weights and connections must be learned in a tedius backpropagation training step [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 3, "context": "It is desirable for a reservoir to operate \u2018at the edge of chaos\u2019 [4], so dissimilar inputs are sufficiently separated in the reservoir node states, yet inputs with only small perturbation-like differences do not stray too far apart.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "Reservoir dynamics demonstrate long short-term memory [5], so any individual point-wise errors in a signal will not corrupt the entire reservoir response.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 1, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 5, "context": "An ESN uses randomly, yet sparsely, connected nodes with randomly assigned fixed weights [1, 2, 6].", "startOffset": 89, "endOffset": 98}, {"referenceID": 6, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 7, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 8, "context": "A TDR uses a cyclic topology, where each node is connected to exactly two other nodes with fixed, non-random weights [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 0, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 9, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 10, "context": "The output layer of both ESN and TDR-type reservoirs almost always use linear output weights, trained on a labeled dataset using least squares or ridge regression[1, 10, 11].", "startOffset": 162, "endOffset": 173}, {"referenceID": 0, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 9, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 10, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 11, "context": "Output Weights Trained using Regularized Least Squares Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12].", "startOffset": 220, "endOffset": 235}, {"referenceID": 1, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 6, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 9, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 12, "context": "Several studies of reservoir performance based on the type of reservoir architecture, chosen parameters, as well as the characteristics of the input data have been performed, with evidence that some combinations of the aforementioned factors can seriously degrade performance [2, 7, 10, 13].", "startOffset": 276, "endOffset": 290}, {"referenceID": 5, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 83, "endOffset": 90}, {"referenceID": 1, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 114, "endOffset": 121}, {"referenceID": 14, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 114, "endOffset": 121}, {"referenceID": 15, "context": "To explore how well the reservoir response separates classes, the separation ratio [6, 14], point-wise separation [2, 15], and class separation [16] have been used.", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 16, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 17, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 156, "endOffset": 168}, {"referenceID": 18, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "Similarly, to measure how effectively a reservoir can process a particular dataset, researchers use the universal approximation property [2] kernel quality [16, 17, 18], reservoir capacity [19], and the Echo State Property [1].", "startOffset": 223, "endOffset": 226}, {"referenceID": 17, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 15, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 16, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 19, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 20, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 21, "context": "For robustness to noise, generalization rank [18] or the Lyapunov coefficient [14, 16, 17, 20, 21, 22] are considered.", "startOffset": 78, "endOffset": 102}, {"referenceID": 0, "context": "In Proposition 3 of [1], the distance between two reservoir states at a given time is bounded in terms of the reservoir states at the previous timestep and the spectral radius of the reservoir weights.", "startOffset": 20, "endOffset": 23}, {"referenceID": 22, "context": "5 of [23] bounds the distance between two output vectors of a TDR, determined using linear read-out weights, in terms of the reservoir parameters and the behavior of the input signals.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "In the Theorems below, we prove upper bounds for distances between two reservoir responses to different inputs in terms of reservoir parameters and the behavior of the inputs for both ESNs and TDRs, and in more generality than the results given in [23] and [1].", "startOffset": 248, "endOffset": 252}, {"referenceID": 0, "context": "In the Theorems below, we prove upper bounds for distances between two reservoir responses to different inputs in terms of reservoir parameters and the behavior of the inputs for both ESNs and TDRs, and in more generality than the results given in [23] and [1].", "startOffset": 257, "endOffset": 260}, {"referenceID": 5, "context": "For this, we turn to the separation ratio, introduced in [6] and further explored in [14].", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "For this, we turn to the separation ratio, introduced in [6] and further explored in [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "The data used are from the United States Postal Service (USPS) database, obtained from [24].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "Reservoir computing is a recently introduced machine learning paradigm that has been shown to be wellsuited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the \u2018hidden layer\u2019 nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks, one may forgo the weight training step entirely and instead use a simple supervised clustering method. The proposed method is analyzed theoretically and explored through numerical experiments on real-world data. The examples demonstrate that the proposed clustering method outperforms the traditional trained output weight approach in terms of speed, accuracy, and sensitivity to reservoir parameters.", "creator": "TeX"}}}