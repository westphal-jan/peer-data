{"id": "1604.03200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing", "abstract": "We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams.", "histories": [["v1", "Tue, 12 Apr 2016 01:52:38 GMT  (1073kb,D)", "http://arxiv.org/abs/1604.03200v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["ricardo \\~nanculef", "ilias flaounas", "nello cristianini"], "accepted": false, "id": "1604.03200"}, "pdf": {"name": "1604.03200.pdf", "metadata": {"source": "META", "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing", "authors": ["Ricardo \u00d1anculefa", "Ilias Flaounasb", "Nello Cristianinib"], "emails": ["jnancu@inf.utfsm.cl", "ilias.flaounas@bristol.ac.uk", "nello.cristianini@bristol.ac.uk"], "sections": [{"heading": null, "text": "We present a method for classifying multi-labeled text documents that is explicitly designed for data stream applications that require a virtually infinite sequence of data using constant storage and processing time. Our method consists of an online method that is used to efficiently map text into a low-dimensional attribute space, and a division of that space into a series of regions for which the system extracts and stores statistics that are used to predict multi-labeled text annotations. Documents are fed into the system as word sequences, assigned to a region of the partition, and commented on based on the statistics calculated from the described instances that collide in the same region. This approach is called collision. We illustrate the method in real text data by comparing the results with those of other text classifiers. In addition, we provide an analysis of the effects of the mean performance of the online dimensionality of the system described, which actually show the performance of the representation of the full dimensionality of the system."}, {"heading": "1. INTRODUCTION", "text": "Efficient analysis of massive data sets is one of the greatest challenges of modern machine learning and data mining applications (Rajaraman and Ullman, 2012; Hand, 2013; Wu et al., 2014). Typically, in these scenarios, data is generated continuously, which enters the system in the form of a fast and virtually infinite stream of data (Aggarwal, 2007; Bifet, 2013). Examples are the stream of messages exchanged on social email addresses: jnancu @ inf.utfsm.cl (Ricardo \u00d1anculef), ilias.flaounas @ bristol.ac.uk (Ilias Flaounas), nello.cristianini @ bristol.ac.uk (Nello Cristianini) Preprint submitted to Elsevierar Xiv: 160 4.03 200v 1 [cs.A I] 1 2A pr2 01network or the stream of daily stories generated by various news agencies."}, {"heading": "1.1. Context of this Research", "text": "In this section we will motivate the framework of this research, discuss current work in this field and highlight the novel aspects of our approach."}, {"heading": "1.1.1. Text Representation", "text": "This representation is often achieved by selecting a set of terms suitable for capturing the content of documents (the vocabulary) and a weighting scheme that maps the dimensions of the feature vector. In this model, every possible word in the series of known texts is indexed in relation to a set of words (BOW), which is the most widely used text representation method in current research (Lan et al., 2009; Ren and Sohrab, 2013). Every possible word in the series of known texts corresponds to a dimension of the feature space used for embedd documents. Along with BOW, the TF-IDF weighting scheme is usually applied to obtain the final representation of a document."}, {"heading": "1.1.2. Multi-Label Text Classification", "text": "Most classification methods studied in practice (Tsoumakas et al, 2010) are designed to deal with individual label mappings, i.e. a data element belongs to one and only one class of possible categories. Therefore, multi-line classification methods for problems encountered in areas such as text categorization, image description, and protein function classification have been of increasing research interest in recent years (Tsoumakas et al., 2010; Madjarov et al., 2012; Jiang et al., 2012; Monta\u00f1es et al, 2014). Multi-line classification is usually approached with a problem conversion approach, in which the problem is broken down into several classical classification tasks, or by directly designing a method to predict several classes at once. (Madjarov et al, 2012; Yu et al al al al., 2014) In the first category, the binary relevance model is most widespread in practice (Tsoumakas et al, 2010)."}, {"heading": "1.2. Novelty and Contributions", "text": "The main contribution of this paper is a simple and effective method of document classification, where both components, text rendering and classification adjustment, are explicitly designed for large-scale multi-label scenarios; the combined approach to text rendering and classification is called a collision; this system efficiently extracts and maintains sufficient statistics for online text rendering and multi-label classification, ensuring a truly limited resource solution in which each operation is performed at a constant time. As we explained in Section 1.1.1, our approach to text rendering with multiple labels is new and builds on current ideas for reducing massive data flows. What is new here is the fact that both the TF and IDF approaches are carried out fully online, guaranteeing constant time and space. As we explain in Section 1.1.2, our approach to obtaining annotations with multiple labels is also novel in trying to maintain and utilize the efficiency of our deployment and deployment."}, {"heading": "1.3. Practical Implications of this Research", "text": "The problem of classifying documents with finite resources is a major challenge in the field of breaking down text streams. Fast text streams are generated by online messaging, social media, and endless other applications, and the need to sort them automatically and adaptably into partial streams is a critical one. Insisting on using only limited resources is a consequence of the size of the streams: both time and memory need to be kept under control. We have focused our ongoing efforts in analyzing web messages on this issue, and the algorithm we developed will be included in our Computational Social Sciences pipeline (Flaounas et al., 2011)."}, {"heading": "1.4. Organization of this paper", "text": "The rest of this article is broken down as follows: In Section 2, we briefly present the problem of document classification and the data stream scenario; in this section, we also present the hash technique used to render text in online domains; the conflicting approach is described in Section 3 with two variants to learn a division of this space based on the available names; in Section 4, we present a theoretical analysis of the rendering obtained by hash versus the geometry of the full word count space; other related work not covered in this introduction is presented in Section 5; in Section 6, we present several experiments performed with the Reuters RCV1 corpus and additional scalability tests performed in the New York Times dataset; the main conclusions and contributions of this paper are summarized in Section 7; the appendix at the end of this manuscript provides the evidence for all the theoretical claims presented in Section 5."}, {"heading": "2. PROBLEM STATEMENT AND BACKGROUND", "text": "We use D to denote the set of all possible documents and use x (d) \u0441X to denote a vector representation d (di), which by using a text representation model x: D \u2192 X. We use x as an abbreviation of x (d) and xi as an abbreviation of x (di), if this is clear from the context. A text record or corpus is a collection of documents D containing words w1, w2,... from a sentence W."}, {"heading": "2.1. Document Classification", "text": "The task of text classification is to extract from a data set D a decision-making mechanism f: D \u2192 2T that describes how documents should be classified (Sebastiani, 2002), that is, how documents should be labeled with markups that are relevant to them. Text classification can be regarded as a kind of information retrieval task for each case in which the markups play the role of the documents to be retrieved (Quevedo et al., 2012). Unlike character pattern classification tasks, text classification is a problem with multiple markups, that is, classes are not mutually exclusive: a document can be labeled with multiple markups at the same time. In this essay, we assume that each document di D has been labeled with markup patterns, i.e., we adopt a supervised approach (Joachims, 2002), using different methods known from both the D and the Zotani and the Tani classification models (SVani annulation methods)."}, {"heading": "2.2. The Data Stream Setting", "text": "In this paper, we deal with text categorization in the context of a data stream setting, i.e. we assume that documents arrive as a continuous and virtually infinite sequence of observations d1, d2,..., engl., dD. In contrast to the traditional setting for learning a text classifier, the set of documents and the set of words (characteristics) contained in these documents are both previously unknown, i.e., D and W are continuously updated. In accordance with recent work on the subject (see e.g. Read et al., 2012; Mena Torres and Aguilar Ruiz, 2014), we recognize original requirements for systems that function efficiently in this environment, i.e., D and W are processed at one time and reviewed at most once; (2) are willing to make predictions at any point; (3) data can evolve over time; and (4) expect an infinite stream, but process it under finite resources (time and memory).It is a framework for learning from this data in the context."}, {"heading": "2.3. Text Representation", "text": "Text representation is the task of transforming text documents into elements of formally defined space X suitable for pattern analysis tasks. Due to its simplicity and effectiveness, the vector space model (VSM) is the most commonly used text representation approach in document recovery and word processing applications (Joachims, 2002; Zhang et al., 2011). In this model, text is represented as a vector x = (x1,., xd) whose components correspond to the weights of a set of linguistic units V used to capture the contents of a document. V is most often constructed by extracting the words contained in the documents, and the weights are determined by using counting statistics, ignoring both the sequence and the position in the text. For example, a document d and word wi leads at the time of word selection (d) in the document, the TF representation of a text document can be achieved by setting Txi = wi."}, {"heading": "2.4. The Hashing Trick", "text": "The hash trick is a method for accelerating the internal product calculations required by some learning methods and improving their memory requirements, first presented by Shi et al. (2009b) and then expanded in (Shi et al., 2009a) and (Attenberg et al., 2009). Here we briefly present the essential definitions and results we need to analyze our techniques. Suppose the data were indexed by [d] = {1, 2, in a d-dimensional attribute space X \u2212 \u2212."}, {"heading": "2.5. The Count-Min (CM) Sketch", "text": "The hash trick shown above has its roots in the so-called counting minute sketch proposed by Cormode and Muthukrishnan (2005). We will use this technique to get an online approximation of the IDF weight scheme. Leave f (j) the number of elements in D with a value j (\u00b7) n. A counting minute sketch of approximate f (j) within an accuracy and trust ratio is based on a two-dimensional array C of the L \u00b7 m counter. For each line \"a hash function h '(\u00b7) uniformly forms the input domain N in the range M = {1, 2,.., m}. If at a certain time element j is observed as f, each line of the sketch f'h' is updated."}, {"heading": "3. THE CLASHING SYSTEM", "text": "A colliding system consists of the following main components: (1) a document-wise hash function \u03c6: D \u2192 Rm, which is used to obtain a low-dimensional vector representation of a document from the set of terms (words) contained therein; (2) a partition R = {R1,.., Rnp} of the low-dimensional attribute space \u03c6 (D); (3) a map function, which is used to automatically accommodate a document in a region of the partition; (4) a prediction rule, which is used to output a series of labels corresponding to the region; (5) a learning rule, which is used to update the prediction rule from designated instances. Algorithm 1 describes the general functioning of the system. In short, the underlying principle is that similar documents in the same region are highly likely to collide and therefore classification can be performed by storing a series of simple statistics about the label distribution."}, {"heading": "3.1. Document Hashing", "text": "D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D. (D). (D. (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D. (D). (D). (D). (D"}, {"heading": "3.2. The Mapper", "text": "The classification model on which the colliding system is based can be represented by a partition R = {R1,..., Rnp} of low-dimensional space \u03c6 (D) in which documents are embedded. A function called Mapper aims to actively create similar documents that collide in the same regions. An easy way to achieve this goal is to use a series of prototypes p1, p2,..., pnp \u03c6 (D) and a voronoi rule of formRi: = {\u03c6 (d): i = arg minj \u03c6 (d) \u2212 pj \u0445 2}. (12) By equation. (12) the documents associated with the same regions have the same next prototype. If the maximum distance from a point in \u03c6 (D) to its corresponding prototype (the extreme points of Ri) is given, we have two documents associated with the same region equal to s by at least (d1) probability as (D) (D) (is high (D))."}, {"heading": "3.3. The Prediction Rule", "text": "Each region Ri is associated with a probability distribution on T, which is called Fi = {Fi, 1.,.., Fi, nt} and which models the probability of observing a certain tag \u03c4j in a document associated with that region, i.e. Fi, j = Pr (\u03c4j | Ri) or better, Fi, j = Pr (\u03c4j, T (d) | \u03c6 (d), where T (d) is the set of tags for a document. Formally, the classification hypothesis f: D \u2192 2T is implemented as f (d) = {\u03c4i, T: F \u0441i, j \u0445 > \u03b8}, (13), where F \u0441i, j an estimator of Fi, j and \u03b8 is a parameter that can be used to control the precision / recall of the system. That is, a label is predicted by the system if and only if a fraction of the documents colliding in the region contains that day."}, {"heading": "3.4. The Learning Engine", "text": "The learning machine is the component of the system that determines the manner and manner in which we construct a suitable partition and estimate the distribution of data from data. (dn, Tn). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "3.5. Complexities", "text": "The system must store a fixed set of np prototypes. In this paper, we have np = nt, that is, we have a prototype for every possible label. Classification and training require a search between the prototypes, using a linear similarity function in the compressed spatial dimension. Thanks to the hashing-based embedding, the spatial and runtime complexity is independent of the potential number of attributes (words in the text). The spatial complexity is O (npm) + O (npnt), as we store np m-dimensional prototypes and nt-counters for each region. The test complexity is O (npm) due to the closest prototype search. The update time is O (| Tt | (npm + nt), where | Tt | is the number of labels included in the document. Note that both storage and runtime regardless of the number of documents observed are uncomplicated to search for (with large number of prototypes) applications."}, {"heading": "3.6. Beyond TF-IDF", "text": "In view of the increasing interest in weight methods that have been able to improve the accuracy of text classifiers in recent years (Lertnattee and Theeramunkong, 2004; Lan and Sohrab, 2013), one can ask whether the online method provided here can essentially be extended to other weight schemes. For example, several publications that are able to evaluate information about the different distribution of terms between different classes have studied variants of the TF-IDF that are based on feature scores (filter methods) originally developed to reduce dimensionality (Azam and Yao, 2012; Sebastiani, 2002; Lan et al.)."}, {"heading": "4. THEORETICAL ANALYSIS", "text": "We present an analysis of the online embedding method used by the Clasher to theoretically characterize the effects of dimension reduction on classification performance. We compare the predictive power of the model in hash space \u03c6 (D) with the power of the equivalent model in word count space X. If X denotes the TF representation of the document space, we know from the discussion in the previous section that \u03c6 (X) roughly preserves the geometry (standards and internal products) of room X. This is a direct consequence of Theorem 1. However, if X denotes the TF-IDF representation of the document space, theorem 1 is not sufficient, because we must take into account the effects of the approximate IDF weighting used in Alg. 3."}, {"heading": "4.1. Preservation of the TF-IDF Geometry", "text": "Our first result is that approximate IDF weights calculated in Alg. 3 have essentially the same guarantees as those provided by Theorem 2 for Plan C. That is, the operating protocol (n / \u00b7) does not substantially change the theoretical limits on the difference between the true and estimated quantities, and the proof of this problem can be found in the Appendix. Let IDF (wi) be the exact IDF for a given round for a word calculated from the documents observed so far, and h (wi) = k. Therefore, the IDF estimate (wi) = protocol (n / Ck) calculated by Alg. 3 meets the IDF (wi) \u2264 IDF (wi) and IDF (wi) \u2212 f, for each > 0, provided that m \u00b2 dexp (1) / e."}, {"heading": "4.2. Effect on the Classification Performance", "text": "We analyze the cumulative performance of the system at a given time t compared to the performance of an equivalent system operating directly in the word count space. Thanks to Theorem 1, the word count space can correspond to either the TF-IDF representation or the TF-IDF representation, with slightly different conditions in terms of the dimensionality required to obtain the guarantees. For the sake of simplicity, we analyze the case of the TF representation with Theorem 1. For the TF-IDF, the procedure is analogous to Theorem 1. For the time being, we also assume that the colliding system works with the learning rule of Alg. 5.The following result shows that the chance of achieving the same performance depends on the following notion of marginality (x) = minj 6 = i-p-j j j-x-x-x-x-p-i grounding 2 (18) Lemma 2. Prerequisite for the x position is that the x-x position is classified with at least a distance in the original data space."}, {"heading": "5. ADDITIONAL RELATED WORK", "text": "This year it is more than ever before."}, {"heading": "6. EXPERIMENTS", "text": "In this section, we will present experiments performed to evaluate the performance of the colliding system and other text classification methods explicitly designed or adapted for data stream settings. We will begin our analysis by examining the ability of the online embedding method examined in this paper to effectively maintain the geometry of the word count space. In particular, we will try to determine how the performance of the colliding approach depends on the dimensionality of the low-dimensional representation space. Next, we will conduct experiments to compare the performance of the system with other text classifiers."}, {"heading": "6.1. Performance Measures", "text": "Precision can be defined as the probability that a retrieved instance is relevant to a given query, and recall as the probability to retrieve a relevant instance (Joachims, 2002). When specifying a class name, they are each estimated as the following values: \u0421\u0438j = n j + + / (n j + + n j + \u2212 n), where nj + + n j \u2212 n (n j + + n j \u2212), where nj + \u2212 is the number of documents for which such a label is f (xi), but percj / n Ti (false positive), nj + + is the number of documents for which a label f (xi) and an actual label / n Ti (true positive), nj \u2212 + is the number of documents for which such a label f (xi), n labels for a label f (false negative)."}, {"heading": "6.2. Preservation of Metric Information", "text": "In this experiment, we are investigating the conservation of metric information via the Reuters RCV1 Volume I Corpus (RCV1) scheme. This is an archive of over 800,000 newswire stories, collected and manually categorized by Reuters, which is widely used to evaluate text-mining algorithms (see Lewis et al., 2004 for details), but we are using the Topics category, which consists of 103 tags covering the most important topics of a story. For this corpus, we are calculating two n \u00b7 n graph matrices Mx and M\u03c6, which are composed of the inner products between documents in original vector space X and the low-dimensional representation space HIV (X); that is, Mxij = < xi, xj > and M\u03c6ij = diamond (xi), \u03c6 (xj) >. As we have already discussed, a document classification system based on TF-IDF vector space representation is typically more accurate than a system based on TF-only."}, {"heading": "6.3. Effect of Dimensionality on the Classification Performance", "text": "Continuing the previous experiment, we will examine the predictive power of the colliding models as a function of the dimensionality of the hashed space. To this end, we will use all documents dated on odd days as independent test sets and the rest will be used as training instances. Training instances will be processed in chronological order, according to the setting of the data stream introduced in Section 2.2, i.e. each document will be classified only once and used as a training instance. Figure 2 shows the measurements of Macro a Micro F1 on the test set after processing the training documents and taking into account all 103 tags in the Reuters Corpus (Topics Collection). After m = 210 dimensions, we will observe rapid convergence to stable performance. Note that this is consistent with the results of the previous section, where we will maintain rapid convergence to linear correlations between p = 1 to m = 212 m or 214 dimensions in the space, rather than compressive performance, which suggests that the original space must be primed, but that all of the observed performance is easy to achieve."}, {"heading": "6.4. Batch Sanity Checks", "text": "We present a classic tensile test experiment, which aims to compare the predictive performance of the models analyzed in this paper with some results available in (Lewis et al., 2004), in which the authors thoroughly examine the Reuters corpus. Table 1 shows accuracy, macro- and microaveraged F-measurement, precision and retrieval, calculated on the test set, for different classifiers, after processing the complete marked data stream. For kNN, we have set k = 1 and predicted that all labels are contained in the closest neighbor of a test document. Binary models (Perceptron and SVM) were trained using a binary relevance approach (Sebastiani, 2002; Lewis et al., 2004). Perceptron was trained sequentially on the dataset with a learning rate of 0.1. SVMs were trained using the LIBLINEAR library for large-scale classification (Fan, al., 2008, regulated formulation with a classical L1-loss)."}, {"heading": "6.5. Data Stream Experiments", "text": "In this section, we analyze the cumulative performance of the clashing models and alternative classification methods in the data stream setting described in Section 2.2. At each round, the system is provided with a new document and it is asked to predict the set of labels T-T that should actually be assigned. After providing a prediction T-T = ft \u2212 1 (dt), the system is provided with the set of correct labels Tt and this information is used to update the model. Cumulative performance achieved by the models is monitored by updating for false positives, real positives, false negatives and real negatives, which are then used to calculate precision, and F1 measures at any desired time. As we are interested in a multi-step evaluation of the classifiers, we calculate micro and macro averages among all 103 of the Reuters corpus. That is, for a micro average we use global errors."}, {"heading": "6.6. Partially Labelled Streams", "text": "To confirm this hypothesis, we conducted an experiment with the full Reuters dataset, in which the classifiers are trained on different fractions (p) of marked data. We present the 800,000 examples corresponding to this dataset in a series of rounds t = 1, 2,..... Each document serves as a test instance for calculating cumulative performance measures. However, after the test, a document is used as a training instance with a probability of p.6. Figure 6 shows the results obtained by setting, p = 0.25, p = 0.0625 and p = 0.03125. These results confirm that the colliding approach can learn from some marked instances faster than the other methods, and this advantage becomes more apparent when we focus on the macro-averaged F measurement."}, {"heading": "7. CONCLUSIONS", "text": "In fact, it is the case that most of us are able to survive ourselves, and that they are able to survive themselves, \"he said.\" But it is not that they are able to survive themselves. \"Indeed,\" it is not that they are able to survive themselves. \"Indeed,\" it is not that they are able to survive themselves. \"Indeed,\" it is not that they are able to survive themselves, but that they are able to survive themselves. \"Indeed,\" it is not as if they are able to survive themselves, as if they are able to survive themselves. \""}, {"heading": "Acknowledgments", "text": "Nello Christianini and Ilias Flaounas were supported by the EU project COMPLACS. Ricardo \u00d1anculef was partly financed by the National Commission for Scientific and Technological Research of Chile, Grant Fondecyt 11130122."}, {"heading": "APPENDIX (PROOFS)", "text": "SDF (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF) (SDF)"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams.", "creator": "LaTeX with hyperref package"}}}