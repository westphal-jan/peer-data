{"id": "1604.03247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "abstract": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels. Since essentially a single descriptor is selected, the existing formulations maybe sub- optimal for object categorization. A MKL formulation based on block l-infinity norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alter- native algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.", "histories": [["v1", "Tue, 12 Apr 2016 04:56:24 GMT  (2980kb,D)", "http://arxiv.org/abs/1604.03247v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dinesh govindaraj"], "accepted": false, "id": "1604.03247"}, "pdf": {"name": "1604.03247.pdf", "metadata": {"source": "CRF", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "authors": ["Dinesh Govindaraj"], "emails": [], "sections": [{"heading": null, "text": "In the past, many descriptors have been proposed that support object categorization even under such adverse conditions. Each descriptor has its own advantages and disadvantages; some descriptors are invariant to transformations, while others are more discriminatory [1, 2]. Previous research has shown that the use of multiple descriptors instead of a single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of available descriptors for a particular classification task is being investigated. Multiple Kernel Learning (MKL) framework has been developed to learn an optimal combination of descriptors for object categorization. Existing MKL formulations often use Block-l-1 regularization that corresponds to the selection of a single kernel."}, {"heading": "1 INTRODUCTION", "text": "(n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) ((n) (n) ((n) (n) ((n) (n) ((n) ((n) ((n) (n) ((n) ((n) ((n) (n) ((n) (n) ((n) (n) ((n) (n) (n) (n) (((n) (n) (n) (((n) (n) (((n) (n) ((n) ((n) ("}, {"heading": "2 Related Work", "text": "In this section, some of the machine learning work involved in object categorization is performed. SVM-KNN [13] gets motivation from local learning, which K-Nearest neighbor uses to select local training points, and uses SVM algorithm in these local training points for classifying objects. However, the main problem here is the time for classification. Multiple Kernel Learning looks at the scenario in which multiple descriptors (kernels) are available for a specific classification task. It aims to simultaneously learn the optimal combination of the given kernels and the optimal classification parameters that maximize generalization capability. Most of the work on MKL, since it was first introduced in [5], focuses on the application of Block-1 regulation. The most important features of this are: a) l-1 regulation leads to sparse combination of authors, which leads to a sparse combination of the kernels, and therefore performs function selection automatically."}, {"heading": "3 Multiple kernel learning", "text": "Let Xk represent the matrix whose columns are the training datapoints in the kth attribute space. Let Y also be Kk \u2261 X > kXk, the grammatical matrix of the training datapoints in the kth attribute space. Let Y be the column vector, the diagonal matrix with the entries as labels of the training datapoints or. Let the discriminating hyperplane l = 0 w > k xk \u2212 b = 0 (here xk denotes the kth attribute space representation of the datapoint, x. l is the number of kernel labels of the training datapoints.) Let the discriminating hyperplane l = 0 w > k \u2212 b = 0 (here xk denotes the kth attribute space representation of the datapoint, x. l is the number of kernel labels)."}, {"heading": "4 L-\u221e regularization MKL For-", "text": "I'm not sure what I'm going to do with it, I'm not going to do it, I'm not going to do it, I'm not going to do it, I'm not going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, '.'"}, {"heading": "4.1 Algorithms for solving the Li-MKL Formulation", "text": "The Li-MKL formulation can be solved with standard solvers of the Second Order Cone Program (SOCP) (e.g. SeDuMi1, Mosek2), but the optimization problem would entail l conical square constraints (l, the number of cores, can be large), and the size of the optimization problem (m, the number of training datapoints) can be large. Therefore, generic cone solvers for large l or m cannot be solved. Interestingly, there are more efficient methods for solving the dual formulations (12). The following sections briefly explain the possible methods."}, {"heading": "4.2 Alternating Minimization Algorithm", "text": "The dual (12) can be efficiently solved with an alternating minimization algorithm in the variables \u03b1 and \u03bb. Note that the minimization at a fixed value of \u03bb, (12) is nothing other than the SVM dual (which has very efficient scalable solvers), and the minimization at a fixed value of \u03b1 is the following simple problem: min-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k (8). It is easy to prove that the optimal values for the problem (8) (\u03bbk > 0-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k (9), so the following iterative algorithm can be efficiently applied to solve (12): 1. Initialize with \u03bb (0) k = 1 highl, where the kernel number is 2.mil /.du.http: /."}, {"heading": "5 Composite MKL Formulation", "text": "This section explains the composition of MKL. Let's assume n descriptors have the same priority. Furthermore, for each of these descriptors a selection of kernels (linear, polynomial, Gaussian) is defined. Let's define the number of kernels of the jth descriptor by nj. Let's also specify the number of mapping induced by the kth kernel of the jth descriptor. The goal is to choose the \"best\" combination of these kernels to maximize generalization. The idea is to combine the kernels so that all descriptors get the same priority (weightage) b), select the best of the kernels in each descriptor. In other words, let's regulate the parameters (wjk), giving each descriptor the same priority."}, {"heading": "6 Numerical Experiments", "text": "In this section, the experimental results on standard object categorization are presented. Various experiments, which were also carried out with the Adaboost to combine descriptors based on standard object categorization data sets, are intended to show that the proposed l- \u221e regularization and composite regularization-based MKL formulation leads to a better generalization than the l-1 regularization-based MKL formulations, which are state-of-the-art methods for object identification. Results on synthetic and real data are summarized in sections 6.2.1 and 6.2.3 respectively. In all cases, the parameters for the respective methods were adjusted on the basis of a validation set."}, {"heading": "6.1 Results using Adaboost", "text": "Adaboost, which is mentioned in the result, is performed with the following setup: 1. Set of classifiers for Adaboost is SVM.2. Each SVM in this set is based on different base kernels Ki, which were mentioned in the previous paragraph.3. Here, each Ki consists of descriptors such as pyramid histograms of the gradient, scale invariant feature descriptions. 4. Adaboost puts emphasis on the SVM classifier, which is based on each of the aforementioned kernels. This AdaBoost is inspired by working with multiple kernels, where each kernel is built with different descriptors. Difference between AdaBoost with SVM (with different descriptors) and multiple kernel learning is AdaBoost gives weight to the SVM classifier (each SVM with different descriptors in the kernel), where weights are specified for each kernel in an SVM problem in multiple kernel learning. The following table shows the result."}, {"heading": "6.2 Results of L-\u221e MKL Experiments", "text": "This section contains experimental results for the LiMKL."}, {"heading": "6.2.1 Synthetic Data", "text": "This section presents results on synthetic datasets that demonstrate the usefulness of the proposed methodology. > The most important result is that the Li-MKL formulation achieves a better generalization, especially in cases where the redundancy in the given cores is lower, as in applications such as object categorization. To this end, the experimental strategy of [23] is cited. We repeat the description of the experimental setup presented here for the sake of completeness. We want to create l-cores whose degree of redundancy is controlled by a single parameter \u03c1. First, m-datapoints are sampled from two independent normal distributions with covariance as identity matrix (dimensionality of the data is n). Here, datapoints from different norms are assumed to belong to different classes. Now, the characteristics are summarized in p-disjoint sets (p varies from 1 to l): Xp."}, {"heading": "6.2.2 Results on Caltech4 dataset", "text": "This section presents the results of Caltech-43. The Caltech4 dataset contains images of airplanes, cars, faces and bicycles. We took 80 images for each class, 40 of which were randomly taken as training / validation data and the rest as test data. We used Pyramid Histogram Of Gradient (PHOG) characteristics generated at different levels (1,2,3) and angles (180,360). On these six PHOG characteristics, we generated cores using different parameters for the polynomic and Gaussian core (9 for each characteristic, a total of 54 cores). This experimental procedure was repeated 20 times with different parts of training and test data. Average test accuracies achieved with L1-MKL and Li-MKL were 92.00 \u00b1 2.44% and 93.50 \u00b1 2.14%, respectively. This shows that the Li-MKL dataset achieves a better generalization."}, {"heading": "6.2.3 Results on Oxford dataset", "text": "The task in the Oxford flower dataset is to categorize images of 17 varieties of flowers. This dataset3http: / / www.robots.ox.ac.uk / ~ vgg / data / data-cats.html 4Code is available at http: / / www.robots.ox.ac.uk / ~ vgg / research / caltech / phog.htmlencontains 80 examples for each class. In [11] the authors introduced four different color characteristics, SIFT for foreground region, SIFT for foreground boundary, Histogram of gradients for flowers. We used the distances given in [11, 24] 5 for our experiments on this dataset. We used the same training, validation and test splits as in [11]. Mean test set accuracy achieved by L1-MKL and Li-MKL accuracy are 85.88 \u00b1 1.83% and 87.35 \u00b1 1.72% respectively."}, {"heading": "6.2.4 Results on Caltech-101 dataset", "text": "In this section, the results are presented on Caltech-1016. Caltech-101 dataset contains 101 object categories. We took 30 images for each class, 15 of which are randomly taken as training / validation data and the rest as test data. We used Pyramid Histogram Of Gradient (PHOG) features, which were generated at different levels (1,2,3) and angles (180,360). We generated cores on these six PHOG features using different parameters for polynomic and Gaussian nuclei (9 for each feature, a total of 54 cores). This experimental procedure was repeated three times with different training data. Average test accuracies obtained with L1-MKL and Li-MKL were 31.45% and 27.12%, respectively. This shows that the Li-MKL achieves better generalizations."}, {"heading": "6.3 Results of CKL Experiments", "text": "All experiments in this section are performed with descriptors provided by the ColorDescriptor software8. The general procedure for the experiment is set out in Figure??. All experiments in this section follow: \u2022 Generate for all training images. \u2022 Generate all 14 descriptors provided by the software (RGB histogram, opposing histogram, color histogram, rg histogram, transformed color histogram, color moments, color moment invariants, SIFT, HueSIFT, HSV-SIFT, OpponentSIFT, rgSIFT, C-SIFT, transformed color SIFT). \u2022 Do not use spatial pyramids. \u2022 Cluster all points from all training images to form a code book for each descriptor. \u2022 Generate a histogram of both the training and test images codeboog. \u2022 Train the classifier at this feature desk: http: / abc.scio.results.http: / cresc.com /"}, {"heading": "6.4 Results on Caltech-5 dataset", "text": "This section presents results on Caltech-59 provided by the ColorDescriptor software. Caltech-5 dataset contains images of airplanes, cars, faces, leopards, and bicycles. This section follows the same procedure described in the previous section. We used cluster size of 100 to form a code book. Clusters are found using k-mean algorithm. Note that we did not run k-mean multiple times to find the best cluster or code book. We took 100 images for each class, 15 of which are randomly used as code book. We created cores on 5 descriptors provided using different parameters for Gaussian kernels (10 for each descriptor, a total of 50 cores). This experimental procedure was repeated 5 times with different training test data splits."}, {"heading": "6.5 Results on Oxford dataset", "text": "The task in the Oxford Flower dataset is to categorize images of 17 flower species. This dataset contains 80 examples for each class. In [11], the authors introduced four different features: SIFT for the foreground region, SIFT for the foreground boundary, histogram of gradients for flowers. We used the distances given in [11, 24] 10 for our experiments on this dataset. We used the same training, validation and test splits as in [11]. The mean test set accuracy achieved by L1-MKL, Li-MKL and CKL is 85.3922%, 86.6667% and 86.6667%, respectively."}, {"heading": "6.6 Results on Caltech-101 dataset", "text": "In this section, the results of Caltech-10111 are presented using new MKL formulations and all 14 descriptors provided by the ColorDescriptor software. We have taken 30 images for each class, 15 of which are randomly taken as training / validation data and the rest as test data. We have generated cores on 14 descriptors provided with different parameters for the Gaussian kernel (2 for each descriptor, a total of 28 cores), cluster size 600 is about 24.1% accurate, and cluster size 300 is 23.21% accurate. The main problem in Caltech-101 is clustering for 10http: / / www.robots.ox.ac.uk / ~ vgg / data / flowers / 17 / index.html 11http: / / www.vision.caltech.edu / Image _ Datasets / Caltech101 / forming codebook."}, {"heading": "7 Conclusions and Future Work", "text": "The project also briefly addressed the problem of detecting video changes. Adaboost was designed to combine descriptors, using state-of-the-art methods for object categorization using l-1 regularization based on the MKL formulation, which is better suited for selecting descriptors than combining them. The key idea is to apply an l-\u221e regularization problem and a mixed l-1 regularization to combine the descriptors within an MKL framework. The new MKL formulation is better suited for object categorization and highly efficient algorithms that solve the corresponding convex optimization problem. Empirical results performed on synthetic and real-world benchmark data sets clearly demonstrate the effectiveness of the proposed MKL formulation. In some cases, increasing accuracy compared to the standard 1 regularization overall is lower than the 101% improvement in the proposed ML formulation."}], "references": [{"title": "Distinctive image features from scaleinvariant keypoints", "author": ["David G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Geometric blur for template matching", "author": ["A. Berg", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Learning the discriminative power-invariance trade-off", "author": ["M. Varma", "D. Ray"], "venue": "In ICCV, pages", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "In ICCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S\u00f6ren Sonnenburg", "Gunnar R\u00e4tsch", "Christin Sch\u00e4fer", "Bernhard Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Simple MKL", "author": ["Alain Rakotomamonjy", "Francis R. Bach", "Stephane Canu", "Yves Grandvalet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An Extended Level Method for Efficient Multiple Kernel Learning", "author": ["Z. Xu", "R. Jin", "I. King", "M. Lyu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Representing shape with a spatial pyramid kernel", "author": ["A. Zisserman A. Bosch", "X. Munoz"], "venue": "In CIVR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Automated flower classification over a large number of classes", "author": ["M-E. Nilsback", "A Zisserman"], "venue": "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Local ensemble kernel learning for object category recognition", "author": ["Y.Y. Lin", "T.Y. Liu", "C.S. Fuh"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Svm-knn: Discriminative nearest neighbor classification for visual category recognition", "author": ["Hao Zhang", "Alexander C. Berg", "Michael Maire", "Jitendra Malik"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Local features and kernels for classification of texture and object categories: a comprehensive study", "author": ["S. Lazebnik J. Zhang", "M. Marszalek", "C. Schmid"], "venue": "In IJCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["Saketha N. Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-tal", "Ramakrishnan K.r"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Controlled sparsity kernel learning", "author": ["Dinesh Govindaraj", "Sankaran Raman", "Sreedal Menon", "Chiranjib Bhattacharyya"], "venue": "CoRR, abs/1401.0116,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Sparse classifier design based on the shapley value", "author": ["Prashanth Ravipally", "Dinesh Govindaraj"], "venue": "In Proceedings of the World Congress on Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Moneybee: Towards enabling a ubiquitous, efficient, and easyto-use mobile crowdsourcing service in the emerging market", "author": ["Dinesh Govindaraj", "Naidu K.V.M", "Animesh Nandi", "Girija Narlikar", "Viswanath Poosala"], "venue": "Bell Labs Technical Journal,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Modeling attractiveness and multiple clicks in sponsored search results", "author": ["Dinesh Govindaraj", "Tao Wang", "S.V.N. Vishwanathan"], "venue": "CoRR, abs/1401.0255,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Application of active appearance model to automatic face replacement", "author": ["Dinesh Govindaraj"], "venue": "Journal of Applied Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization", "author": ["J. Platt"], "venue": "In Advances in Kernel Methods\u2014Support Vector Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Non-sparse multiple kernel learning", "author": ["Marius Kloft", "Ulf Brefeld", "Pavel Laskov", "Soren Sonnenburg"], "venue": "In Workshop on Kernel Learning: Automatic Selection of Optimal Kernels,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A visual vocabulary for flower classification", "author": ["M-E. Nilsback", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 5, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 6, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 7, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 8, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 0, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 10, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 11, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 3, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 4, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "One way to circumvent this problem of optimal weights being zero for many of the kernels, was introduced in [3], where an additional constraint to employ prior information is included.", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "SVM-KNN [13] gets motivation from Local learning which uses K-Nearest neighbor to select local training point and uses SVM algorithm in those local training points for classification of object.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "Most of the work on MKL, since it was first introduced in [5], concentrates on the employment of a block l-1 regularization.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 6, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 7, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 8, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 2, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 10, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 9, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 3, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 11, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 13, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 14, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 15, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 16, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 17, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 18, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 19, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 9, "context": "In [10], the authors introduce spatial pyramid kernel and combine shape (pyramid histogram of gradient), appearance descriptors for object classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "In [12], a sample dependent local ensemble kernel machine is learned for object categorization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3], the authors use six descriptors for object categorization and employ a MKL formulation for learning the optimal combination of descriptors.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "This MKL formulation [3] is known to achieve state-of-the-art performance for many object recognition tasks.", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.", "startOffset": 121, "endOffset": 124}, {"referenceID": 16, "context": "The usual soft-margin Support Vector Machine (SVM) [21, 17] formulation with this notation", "startOffset": 51, "endOffset": 59}, {"referenceID": 4, "context": "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Now, suppose that all the gram-matrices Kk are positive-definite (add a small ridge if singular, see also [8]).", "startOffset": 106, "endOffset": 109}, {"referenceID": 20, "context": "Infact, algorithms which exploit this sparsity in solution and outperform standard QP solvers exist [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "For this, the experimental strategy given by [23].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 22, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "We have used same training, validation and test splits as used in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "The accuracy achieved by the proposed formulation is comparable to the best accuracy reported in [11], which is 88.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 22, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "We have used same training, validation and test splits as used in [11].", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-\u221e norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-\u221e and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.", "creator": "LaTeX with hyperref package"}}}