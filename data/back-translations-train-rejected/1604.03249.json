{"id": "1604.03249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Attributes as Semantic Units between Natural Language and Visual Recognition", "abstract": "Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.", "histories": [["v1", "Tue, 12 Apr 2016 05:23:26 GMT  (8730kb,D)", "http://arxiv.org/abs/1604.03249v1", "book chapter"]], "COMMENTS": "book chapter", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["marcus rohrbach"], "accepted": false, "id": "1604.03249"}, "pdf": {"name": "1604.03249.pdf", "metadata": {"source": "CRF", "title": "Attributes as Semantic Units between Natural Language and Visual Recognition", "authors": ["Marcus Rohrbach"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "1.1 Challenges for combining visual and linguistic modalities", "text": "The basic data unit of visual modality is a (photographic) image or video that always shows a specific instance of a category. In contrast, the basic semantic unit of words (which are strings or phonematerials) is very important, but we limit ourselves to written formulations."}, {"heading": "1.2 Overview and outline", "text": "In this chapter, we explain how linguistic knowledge can help identify novel object categories and composite activities (Section 2), how attributes help to describe videos and images with sentences of natural language (Section 3), how to ground phrases in images (Section 4), and how compositional computation allows effective answers to questions about images (Section 5). We conclude with instructions for future work in Section 6. All of these directions have in common that attributes form a layer or composition that is beneficial for the connection between textual and visual representations. In Section 2, for recognizing novel object categories and composite activities, attributes form the layer in which the transfer takes place. Attributes are distributed across known and novel categories, while information from different language resources are able to provide the associations between known categories and attributes in order to learn attributes, classify and between attributes and novel categories in the test period."}, {"heading": "2 Linguistic knowledge for recognition of novel categories", "text": "While supervised training is an integral part of the construction of visual, textual or multimodal category models, more recently knowledge transfer between categories has been recognized as an important component to scale to a large number of categories and enable fine-grained categorization, reflecting the psychological standpoint that humans are able to generalize to novel categories with only a few training samples [56, 6], which has lately increased interest in computer vision and machine learning literature dealing with zero-shot recognition (without training instances for a class) [44, 17, 58, 59, 22, 53, 21] and one- or two-tiered recognition [85, 6, 61]. Knowledge transfer is particularly advantageous when scaled to a large number of classes where training dates are limited [53, 21, 70], fine-grained categories [19, 13, or compositional activities analyzed in 22]."}, {"heading": "2.1 Semantic relatedness mined from language resources for zero-shot recognition", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2.2 Propagated semantic transfer", "text": "Fig. 4 shows the results on the AwA dataset [43]. We note that, unlike the previous section, classifiers are automatically trained on all training examples, not just 92 per class. Fig. 4 (a) shows zero-shot results where no training examples are available for the new or in this case invisible classes. The table compares PST with the propagation on a graph based on the similarity of the attribute classifiers with the image descriptor and shows a clear advantage of the former. This variant also exceeds DAP and IAP [44] and Zero-Shot Learning [22]. Next, we compare PST in the fewshot setting, i.e. we add labeled examples per class. In Fig. 4 (b) we compare PST with two label propagations (LP) baselines that do not match the same classes."}, {"heading": "2.3 Composite activity recognition with attributes and script data", "text": "Understanding activities in visual and textual data is generally considered more difficult than understanding object categories due to the limited training data, the challenges in defining the extent of an activity, and the similarities between activities. However, long-term compound activities can be broken down into shorter fine-grained activities [72]. Consider, for example, compound cooking activities, which can be broken down into attributes of fine-grained activities (e.g. open, brooding), ingredients (e.g. eggs), and tools (e.g. pan, blade). These attributes can then be divided and transferred to compound activities, as in Fig. 6 using the same approaches as for objects and attributes discussed in the previous section. However, representations need to change both on the visual and language side. Fine-grained activities and associated attributes can be characterized by posthumous frequency."}, {"heading": "3 Image and video description using compositional attributes", "text": "In this section, we will discuss how to generate sentences in natural language that describe visual content, rather than just labeling images and videos as in the previous section. This fascinating task has lately received increased attention in computer vision and computer linguistic communities [89, 90, 91] and has a large number of potential applications, including interacting with human robots, retrieving images and videos, and describing visual content for visually impaired people. In this section, we will focus on approaches that decouple visual recognition and sentence generation and introduce a semantic intermediate layer that is recognisable as a layer of attributes (Section 3.1). Introducing such a semantic layer has several advantages. Firstly, this allows to argue sentences at a semantic level, which, as we will see, is beneficial for the multi-sentence description of videos (Section 3.2). Secondly, we can show that learning reliable attributes of a highly modern sentence writing scenario (Section 3.3) enables this to result in a highly diverse set structure (Section 3.3)."}, {"heading": "3.1 Translating image and video content to natural language descriptions", "text": "To address the problem of image and video description, Rohrbach et al. [75] propose a two-step approach to translation, which first predicts a semantic intermediate layer and then learns how to translate from this semantic representation to natural sentences. Figure 7 provides an overview of this two-step approach to video. Secondly, the generation of natural language is formulated as a complex translation problem, using semantic representation as the source language and the generated sentences as the target language. To this end, a CRF models the relationships between different attributes of visual input. Secondly, the generation of natural language is formulated as a machine translation problem, using semantic representation as the source language and the generated sentences as the target language."}, {"heading": "3.2 Coherent multi-sentence video description with variable level of detail", "text": "Most approaches to automatic video description, including the above, focus on generating individual sentence descriptions and are unable to vary the detail description. An advantage of the two-step approach with an explicit intermediate layer of semantic attributes is that it is based on this semantic level. To generate coherent multi-sentence descriptions, we need to expand the two-step translation approach to the level of semantic consistency by enforcing a consistent theme prepared in the cooking scenario. To produce shorter or one-part summaries, we need to select the most important sentences at the semantic level."}, {"heading": "3.3 Describing movies with an intermediate layer of attributes", "text": "Two challenges arise when the above idea is extended to the film description [67], which looks at the problem of how films for the blind are described. Firstly, and perhaps more importantly, there are no semantic attributes that are commented on as in the kitchen data, and secondly, the data is visually more diverse and challenging. To address the second challenge of increased visual difficulty, Rohrbach et al. [66] propose to improve the reliability of these attributes or \"visual labels\" by taking a semantic approach to description, distinguishing, first, three semantic groups of labels (verbs, objects and scenes) and, secondly, the use of appropriate characteristic representations for each: activity detection with dense trajectories [92], the detection of objects with LSDA [31], and the classification of scenes with Places [99]."}, {"heading": "3.4 Describing novel object categories", "text": "In fact, one must be able to manoeuvre oneself into a situation in which one sees oneself as being able to put oneself at the centre."}, {"heading": "4 Grounding text in images", "text": "In this section we discuss the problem of grounding natural language in the images. In this case, this means locating the subset of the image that corresponds to the input."}, {"heading": "4.1 Unsupervised grounding", "text": "Although many data sources contain images that are described with sentences or phrases, they typically do not provide spatial localization of phrases, both for curated datasets such as MSCOCO [47] and for large user-generated content such as the YFCC 100M dataset [84]. Consequently, the ability to learn from these data without grounding would allow for a large amount and variety of training data. This setting is visualized in Fig. 12 (a). For this setting, Rohrbach et al. [65] suggest the GroundeR approach, which is capable of learning grounding by aiming to reconstruct a given phrase using an attention mechanism, as in Fig. 12 (b)."}, {"heading": "4.2 Semi-supervised and fully supervised grounding", "text": "If grounding monitoring (bounding box associations) is present, GroundR [65] can integrate it by adding a loss to the attention mechanism (Fig. 12b, \"Attend\"). Interestingly, this allows monitoring to be provided only for a subset of phrases (semi-monitored) or for all phrases (fully monitored). For monitored grounding, Plummer et al. [60] proposed learning a CCA embedding mechanism [26] between phrases and the visual representation. Spatial Context Recurrent ConvNet (SCRC) [32] and the approach of Mao et al. [51] use a caption generation framework to evaluate phrases on a series of bounding box suggestions, which allows the ranking of Bounding Box suggestions for a particular phrase or a referential expression. Hu et al. [32] show the benefit of the overwriting picture to differentiate between the overwriting set, the overwriting box and the overspace box as well as the context descriptions were trained."}, {"heading": "4.3 Grounding results", "text": "In the following, we will discuss the results of the Flickr 30k entities dataset [60] and the ReferItGame dataset [38], both of which provide ground-level alignment between noun phrases (within sentences) and boundary boxes. In the case of unattended models, the grounding notes are used for evaluation only at test date, not for training. Allapproaches use the activations of the second-to-last layer of the VGG network [79] to encode the image within the boundary boxes. Table 4 (a) compares the approaches quantitatively. The unattended variant of GroundeR achieves nearly the monitored performance of CCA [60] or SCRC [32] on Flickr 30k entities, successful examples are shown in Fig. 13. For the referential expressions of the ItGame dataset, the unattended variant of GroundeR achieves performance at par with previous work (table 4b), if the verified results are quickly added to the verified performance (if some of the verified performance are also presented in the verified data)."}, {"heading": "5 Visual question answering", "text": "The most current approaches to answering the question of \"why\" and \"why?\" are indeed very different: \"Why?,\" \"Why?,\" \"Why?,\" \"Why?,\" \"Why?,\" \"Why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \",\" \"why,\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \",\" \",\" \",\", \"\", \"\", \",\" \",\" \",\", \"\", \"\", \",\" \",\" \"\", \"\" \",\", \"\" \"\" \",\" \"\", \"\" \",\" \"\", \"\", \"\" \",\" \"\", \",\" \"\", \",\" \"\", \"\" \",\" \"\", \"\" \"\" \",\", \"\" \",\" \",\" \"\" \",\" \"\", \"\" \",\" \"\" \",\" \"\" \"\" \",\" \"\", \"\" \"\", \"\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\", \",\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "6 Conclusions", "text": "In this chapter, we presented several tasks and approaches where attributes allow a connection between visual recognition and natural language on a semantic level. In this scenario, we saw that semantic attributes classifiers can additionally build a good metric spacing space that is useful for constructing instance graphs and learning composite activity detection models. In Section 3, we explained how a middle level of attributes can be used to describe multi-sentence, variable-level videos and describe new object categories. In Section 4, we presented approaches to unattended and supervised creation of phrases in images. Different phrases are semantically overlapping and the approaches studied attempt to realize these semantic units by jointly learning representations for visual and linguistic modalities. Section 5 discusses an approach to visual questions that represent the most important attributes of a question in a compositional graph."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1511.02799", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "Journal of Machine Learning Research (JMLR), 3:1107\u20131135", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Single-example learning of novel classes using representation by similarity", "author": ["E. Bart", "S. Ullman"], "venue": "Proceedings of the British Machine Vision Conference (BMVC)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Novel association measures using web search with double checking", "author": ["H.-H. Chen", "M.-S. Lin", "Y.-C. Wei"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "and L", "author": ["J. Deng", "A. Berg", "K. Li"], "venue": "Fei-Fei. What does classifying more than 10,000 image categories tell us? In European Conference on Computer Vision (ECCV)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Measures of the amount of ecologic association between species", "author": ["L.R. Dice"], "venue": "Ecology, 26(3):297\u2013302", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1945}, {"title": "Long-term recurrent convolutional networks for  Attributes as Semantic Units between Natural Language and Visual Recognition  25 visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering Localized Attributes for Fine-grained Recognition", "author": ["K. Duan", "D. Parikh", "D. Crandall", "K. Grauman"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting Structures in Image Collections for Object Recognition", "author": ["S. Ebert", "D. Larlus", "B. Schiele"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), 88(2):303\u2013338", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute-centric recognition for crosscategory generalization", "author": ["A. Farhadi", "I. Endres", "D. Hoiem"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance", "author": ["R. Farrell", "O. Oza", "V. Morariu", "T. Darrell", "L. Davis"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "WordNet: An Electronical Lexical Database", "author": ["C. Fellbaum"], "venue": "The MIT Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multimodal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 36(2):303\u2013316", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Open-vocabulary object retrieval", "author": ["S. Guadarrama", "E. Rodner", "K. Saenko", "N. Zhang", "R. Farrell", "J. Donahue", "T. Darrell"], "venue": "Robotics: science and systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.05284v1", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "K. Saenko", "T. Darrell"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language object retrieval", "author": ["R. Hu", "H. Xu", "M. Rohrbach", "J. Feng", "K. Saenko", "T. Darrell"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation from natural language expressions", "author": ["R. Hu", "M. Rohrbach", "T. Darrell"], "venue": "arXiv preprint arXiv:1603.06180", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "L.-J. Li", "D. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Cambridge University Press", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual  Attributes as Semantic Units between Natural Language and Visual Recognition  27 genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalanditis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 36(3):453\u2013465", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Script-to-movie: A computational framework for story movie composition", "author": ["C. Liang", "C. Xu", "J. Cheng", "W. Min", "H. Lu"], "venue": "Multimedia, IEEE Transactions on, 15(2):401\u2013414", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neuralbased approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A. Yuille", "K. Murphy"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1998}, {"title": "Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "A method for word sense disambiguation of unrestricted text", "author": ["R. Mihalcea", "D.I. Moldovan"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1999}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalization to novel images in upright and inverted faces", "author": ["Y. Moses", "S. Ullman", "S. Edelman"], "venue": "Perception, 25:443\u2013461", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1996}, {"title": "Spatial semantic regularisation for large scale object detection", "author": ["D. Mrowca", "M. Rohrbach", "J. Hoffman", "R. Hu", "K. Saenko", "T. Darrell"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G. Hinton", "T. Mitchell"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B. Plummer", "L. Wang", "C. Cervantes", "J. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-taught learning: Transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A. Ng"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Grounding Action Descriptions in Videos", "author": ["M. Regneri", "M. Rohrbach", "D. Wetzel", "S. Thater", "B. Schiele", "M. Pinkal"], "venue": "Transactions of the Association for Computational Linguistics (TACL)", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "Proceedings of the German Confeence on Pattern Recognition (GCPR)", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "arXiv preprint arXiv:1511.03745", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "The long-short story of movie description", "author": ["A. Rohrbach", "M. Rohrbach", "B. Schiele"], "venue": "Proceedings of the German Confeence on Pattern Recognition (GCPR)", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrbach", "N. Tandon", "B. Schiele"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining visual recognition and computational linguistics: linguistic knowledge for visual recognition and natural language descriptions of visual content", "author": ["M. Rohrbach"], "venue": "PhD thesis, Saarland University", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "What helps Where - and Why? Semantic Relatedness for Knowledge Transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating Knowledge Transfer and Zero-Shot Learning in a Large-Scale Setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2011}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["M. Rohrbach", "S. Amin", "M. Andriluka", "B. Schiele"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "Script data for attribute-based recognition of composite activities", "author": ["M. Rohrbach", "M. Regneri", "M. Andriluka", "S. Amin", "M. Pinkal", "B. Schiele"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining language sources and robust semantic relatedness for attribute-based knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "B. Schiele"], "venue": "Proceedings of the European Conference on Computer Vision Workshops (ECCV Workshops), volume 6553 of LNCS", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer Learning in a Transductive Setting", "author": ["M. Rohrbach", "S. Ebert", "B. Schiele"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2013}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognizing fine-grained and composite activities using hand-centric features and script data", "author": ["M. Rohrbach", "A. Rohrbach", "M. Regneri", "S. Amin", "M. Andriluka", "M. Pinkal", "B. Schiele"], "venue": "International Journal of Computer Vision (IJCV)", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2015}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Senina", "M. Rohrbach", "W. Qiu", "A. Friedrich", "S. Amin", "M. Andriluka", "M. Pinkal", "B. Schiele"], "venue": "arXiv preprint arXiv:1403.6173", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2014}, {"title": "Models of semantic representation with visual attributes", "author": ["C. Silberer", "V. Ferrari", "M. Lapata"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering Object Categories in Image Collections", "author": ["J. Sivic", "B.C. Russell", "A.A. Efros", "A. Zisserman", "W.T. Freeman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2005}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2010}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on danish commons", "author": ["T. S\u00f8rensen"], "venue": "Biol. Skr., 5:1\u201334", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1948}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["B. Thomee", "B. Elizalde", "D.A. Shamma", "K. Ni", "G. Friedland", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, 59(2):64\u201373", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "85", "shortCiteRegEx": null, "year": 1996}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["A. Torabi", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "arXiv preprint arXiv:1503.01070v1", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2015}, {"title": "K", "author": ["J.R. Uijlings"], "venue": "E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. International Journal of Computer Vision (IJCV), 104(2):154\u2013171", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence \u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "arXiv preprint arXiv:1505.00487v2", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards automatic discovery of object categories", "author": ["M. Weber", "M. Welling", "P. Perona"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2000}, {"title": "A", "author": ["Q. Wu", "C. Shen"], "venue": "v. d. Hengel, P. Wang, and A. Dick. Image captioning and visual question answering based on attributes and their related external knowledge. arXiv preprint arXiv:1603.02814", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "arXiv preprint arXiv:1502.08029v4", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL), 2:67\u201378", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2014}, {"title": "Wisdom of crowds versus wisdom of linguists measuring the semantic relatedness of words", "author": ["T. Zesch", "I. Gurevych"], "venue": "Natural Language Engineering, 16(1):25\u201359", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "Jason Weston", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal"], "venue": "and B. Sch\u00f6lkopf. Learning with Local and Global Consistency. In Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2004}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 82, "context": "Computer vision has made impressive progress in recognizing large number of objects categories [83], diverse activities [92], and most recently also in describing images and videos with natural language sentences [91, 89] and answering natural language questions about images [48].", "startOffset": 95, "endOffset": 99}, {"referenceID": 91, "context": "Computer vision has made impressive progress in recognizing large number of objects categories [83], diverse activities [92], and most recently also in describing images and videos with natural language sentences [91, 89] and answering natural language questions about images [48].", "startOffset": 120, "endOffset": 124}, {"referenceID": 90, "context": "Computer vision has made impressive progress in recognizing large number of objects categories [83], diverse activities [92], and most recently also in describing images and videos with natural language sentences [91, 89] and answering natural language questions about images [48].", "startOffset": 213, "endOffset": 221}, {"referenceID": 88, "context": "Computer vision has made impressive progress in recognizing large number of objects categories [83], diverse activities [92], and most recently also in describing images and videos with natural language sentences [91, 89] and answering natural language questions about images [48].", "startOffset": 213, "endOffset": 221}, {"referenceID": 47, "context": "Computer vision has made impressive progress in recognizing large number of objects categories [83], diverse activities [92], and most recently also in describing images and videos with natural language sentences [91, 89] and answering natural language questions about images [48].", "startOffset": 276, "endOffset": 280}, {"referenceID": 27, "context": "Given sufficient training data these approaches can achieve impressive performance, sometimes even on par with humans [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 7, "context": "Image and caption from MS COCO [8].", "startOffset": 31, "endOffset": 34}, {"referenceID": 43, "context": "We also like to note that some datasets such as Animals with Attributes [44] include non-visual attributes, e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 42, "context": "The order of the following sections weakly follows the historic development, where we start with work which appeared at the time when attributes started to become popular in computer vision [43, 18].", "startOffset": 190, "endOffset": 198}, {"referenceID": 17, "context": "The order of the following sections weakly follows the historic development, where we start with work which appeared at the time when attributes started to become popular in computer vision [43, 18].", "startOffset": 190, "endOffset": 198}, {"referenceID": 47, "context": "And the last section on visual question answering, a problem which requires more complex interactions between language and visual recognition, has only recently become a topic in the computer vision community [48, 4].", "startOffset": 209, "endOffset": 216}, {"referenceID": 3, "context": "And the last section on visual question answering, a problem which requires more complex interactions between language and visual recognition, has only recently become a topic in the computer vision community [48, 4].", "startOffset": 209, "endOffset": 216}, {"referenceID": 55, "context": "This development reflects the psychological point of view that humans are able to generalize to novel3 categories with only a few training samples [56, 6].", "startOffset": 147, "endOffset": 154}, {"referenceID": 5, "context": "This development reflects the psychological point of view that humans are able to generalize to novel3 categories with only a few training samples [56, 6].", "startOffset": 147, "endOffset": 154}, {"referenceID": 43, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 16, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 57, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 58, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 21, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 52, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 20, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 177, "endOffset": 205}, {"referenceID": 84, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 240, "endOffset": 251}, {"referenceID": 5, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 240, "endOffset": 251}, {"referenceID": 60, "context": "This has recently gained increased interest in the computer vision and machine learning literature, which look at zero-shot recognition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or few-shot recognition [85, 6, 61].", "startOffset": 240, "endOffset": 251}, {"referenceID": 52, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 118, "endOffset": 130}, {"referenceID": 20, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 118, "endOffset": 130}, {"referenceID": 69, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 118, "endOffset": 130}, {"referenceID": 18, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 171, "endOffset": 179}, {"referenceID": 12, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 171, "endOffset": 179}, {"referenceID": 21, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 229, "endOffset": 237}, {"referenceID": 71, "context": "Knowledge transfer is particularly beneficial when scaling to large numbers of classes where training data is limited [53, 21, 70], distinguishing fine-grained categories [19, 13], or analyzing compositional activities in videos [22, 72].", "startOffset": 229, "endOffset": 237}, {"referenceID": 42, "context": "2 Zero-shot recognition with the Direct Attribute Prediction model [43] allows recognizing unseen classes z using an intermediate layer of attributes a.", "startOffset": 67, "endOffset": 71}, {"referenceID": 68, "context": "[69] reduce supervision by mining object-attribute association from language resources, such as Wikipedia, WordNet, and image or web search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43, 44] propose to use attribute based recognition to allow recognizing unseen categories based on their object-attribute associations.", "startOffset": 0, "endOffset": 8}, {"referenceID": 43, "context": "[43, 44] propose to use attribute based recognition to allow recognizing unseen categories based on their object-attribute associations.", "startOffset": 0, "endOffset": 8}, {"referenceID": 68, "context": "[69, 73, 70] show how these previously manual defined attribute associations am and am can be replaced with associations mined automatically from different language resources.", "startOffset": 0, "endOffset": 12}, {"referenceID": 72, "context": "[69, 73, 70] show how these previously manual defined attribute associations am and am can be replaced with associations mined automatically from different language resources.", "startOffset": 0, "endOffset": 12}, {"referenceID": 69, "context": "[69, 73, 70] show how these previously manual defined attribute associations am and am can be replaced with associations mined automatically from different language resources.", "startOffset": 0, "endOffset": 12}, {"referenceID": 6, "context": "Yahoo Snippets [7, 73], which computes co-occurrence statistics on summary snippets returned by search engines, shows the best performance of all single measures.", "startOffset": 15, "endOffset": 22}, {"referenceID": 72, "context": "Yahoo Snippets [7, 73], which computes co-occurrence statistics on summary snippets returned by search engines, shows the best performance of all single measures.", "startOffset": 15, "endOffset": 22}, {"referenceID": 72, "context": "[73] also discuss several fusion strategies to get more robust measures by expanding the attribute inventory with clustering and combining several measures, which can achieve performance on par with manually defined associations (second last versus last line in Table 1a).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "WordNet [20], path Lin measure [46] [69] 60.", "startOffset": 8, "endOffset": 12}, {"referenceID": 45, "context": "WordNet [20], path Lin measure [46] [69] 60.", "startOffset": 31, "endOffset": 35}, {"referenceID": 68, "context": "WordNet [20], path Lin measure [46] [69] 60.", "startOffset": 36, "endOffset": 40}, {"referenceID": 53, "context": "5 Yahoo Web, hit count [54] Dice coef.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "[11, 82] [69] 60.", "startOffset": 0, "endOffset": 8}, {"referenceID": 81, "context": "[11, 82] [69] 60.", "startOffset": 0, "endOffset": 8}, {"referenceID": 68, "context": "[11, 82] [69] 60.", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "4 Flickr Img, hit count [69] Dice coef.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "[11, 82] [69] 70.", "startOffset": 0, "endOffset": 8}, {"referenceID": 81, "context": "[11, 82] [69] 70.", "startOffset": 0, "endOffset": 8}, {"referenceID": 68, "context": "[11, 82] [69] 70.", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "1 Yahoo Img, hit count [69] Dice coef.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "[11, 82] [69] 71.", "startOffset": 0, "endOffset": 8}, {"referenceID": 81, "context": "[11, 82] [69] 71.", "startOffset": 0, "endOffset": 8}, {"referenceID": 68, "context": "[11, 82] [69] 71.", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "0 Wikipedia [69] ESA [23, 98] [69] 69.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "0 Wikipedia [69] ESA [23, 98] [69] 69.", "startOffset": 21, "endOffset": 29}, {"referenceID": 97, "context": "0 Wikipedia [69] ESA [23, 98] [69] 69.", "startOffset": 21, "endOffset": 29}, {"referenceID": 68, "context": "0 Wikipedia [69] ESA [23, 98] [69] 69.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "7 Yahoo Snippets [7] Dice/Snippets [73] [73] 76.", "startOffset": 17, "endOffset": 20}, {"referenceID": 72, "context": "7 Yahoo Snippets [7] Dice/Snippets [73] [73] 76.", "startOffset": 35, "endOffset": 39}, {"referenceID": 72, "context": "7 Yahoo Snippets [7] Dice/Snippets [73] [73] 76.", "startOffset": 40, "endOffset": 44}, {"referenceID": 72, "context": "[73] 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "2 Combination Classifier fusion [73] 75.", "startOffset": 32, "endOffset": 36}, {"referenceID": 72, "context": "[73] 79.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "manual [43] [73] 79.", "startOffset": 7, "endOffset": 11}, {"referenceID": 72, "context": "manual [43] [73] 79.", "startOffset": 12, "endOffset": 16}, {"referenceID": 72, "context": "(b) Attributes versus direct-similarity, reported in [73].", "startOffset": 53, "endOffset": 57}, {"referenceID": 42, "context": "Table 1 Zero-shot recognition on AwA dataset [43].", "startOffset": 45, "endOffset": 49}, {"referenceID": 68, "context": "[69] also propose to directly transfer information from most similar classes which does not require and intermediate level of attributes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[70] extend zero-shot recognition from the 10 unseen categories in the AwA dataset to a setting of 200 unseen ImageNet [9] categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[70] extend zero-shot recognition from the 10 unseen categories in the AwA dataset to a setting of 200 unseen ImageNet [9] categories.", "startOffset": 119, "endOffset": 122}, {"referenceID": 19, "context": "propose to mine part-attributes from WordNet [20] as ImageNet categories correspond to WordNet synsets.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "An alternative approach is DeViSE [21] which learns an embedding into a semantic skip-gram word-space [55], trained on Wikipedia documents.", "startOffset": 34, "endOffset": 38}, {"referenceID": 54, "context": "An alternative approach is DeViSE [21] which learns an embedding into a semantic skip-gram word-space [55], trained on Wikipedia documents.", "startOffset": 102, "endOffset": 106}, {"referenceID": 69, "context": "The hierarchical variants [70] performs best, also compared to DeViSE [21] which relies on more powerful CNN [42] features.", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "The hierarchical variants [70] performs best, also compared to DeViSE [21] which relies on more powerful CNN [42] features.", "startOffset": 70, "endOffset": 74}, {"referenceID": 41, "context": "The hierarchical variants [70] performs best, also compared to DeViSE [21] which relies on more powerful CNN [42] features.", "startOffset": 109, "endOffset": 113}, {"referenceID": 52, "context": "Further improvements can be achieved by metric learning [53].", "startOffset": 56, "endOffset": 60}, {"referenceID": 56, "context": "[57] show how such hierarchical semantic knowledge allows to improve large scale object detection not just classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "Hierarchy leaf WordNet nodes [69] 72.", "startOffset": 29, "endOffset": 33}, {"referenceID": 68, "context": "8 inner WordNet nodes [69] 66.", "startOffset": 22, "endOffset": 26}, {"referenceID": 68, "context": "7 all WordNet nodes [69] 65.", "startOffset": 20, "endOffset": 24}, {"referenceID": 52, "context": "2 + metric learning [53] 64.", "startOffset": 20, "endOffset": 24}, {"referenceID": 68, "context": "3\u2217 Part Attributes Wikipedia [69] 80.", "startOffset": 29, "endOffset": 33}, {"referenceID": 68, "context": "9 Yahoo Holonyms [69] 77.", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "3 Yahoo Image [69] 81.", "startOffset": 14, "endOffset": 18}, {"referenceID": 68, "context": "4 Yahoo Snippets [69] 76.", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "2 all attributes [69] 70.", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "3 Direct Similarity Wikipedia [69] 75.", "startOffset": 30, "endOffset": 34}, {"referenceID": 68, "context": "6 Yahoo Web [69] 69.", "startOffset": 12, "endOffset": 16}, {"referenceID": 68, "context": "3 Yahoo Image [69] 72.", "startOffset": 14, "endOffset": 18}, {"referenceID": 68, "context": "0 Yahoo Snippets [69] 75.", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "5 all measures [69] 66.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Label embedding DeViSe [21] 68.", "startOffset": 23, "endOffset": 27}, {"referenceID": 52, "context": "\u2217Note that [53, 21] report on a different set of unseen classes than [69].", "startOffset": 11, "endOffset": 19}, {"referenceID": 20, "context": "\u2217Note that [53, 21] report on a different set of unseen classes than [69].", "startOffset": 11, "endOffset": 19}, {"referenceID": 68, "context": "\u2217Note that [53, 21] report on a different set of unseen classes than [69].", "startOffset": 69, "endOffset": 73}, {"referenceID": 73, "context": "The approach Propagated Semantic Transfer (PST) [74] combines four ideas to jointly handle the challenging scenario of recognizing novel categories.", "startOffset": 48, "endOffset": 52}, {"referenceID": 93, "context": "Second, PST exploits the manifold structure of novel classes similar to unsupervised learning approaches [94, 80].", "startOffset": 105, "endOffset": 113}, {"referenceID": 79, "context": "Second, PST exploits the manifold structure of novel classes similar to unsupervised learning approaches [94, 80].", "startOffset": 105, "endOffset": 113}, {"referenceID": 100, "context": "More specifically it adapts the graph-based Label Propagation algorithm [101, 100] \u2013 previously used only for semi-supervised learning [14] \u2013 to zero-shot and few-shot learning.", "startOffset": 72, "endOffset": 82}, {"referenceID": 99, "context": "More specifically it adapts the graph-based Label Propagation algorithm [101, 100] \u2013 previously used only for semi-supervised learning [14] \u2013 to zero-shot and few-shot learning.", "startOffset": 72, "endOffset": 82}, {"referenceID": 13, "context": "More specifically it adapts the graph-based Label Propagation algorithm [101, 100] \u2013 previously used only for semi-supervised learning [14] \u2013 to zero-shot and few-shot learning.", "startOffset": 135, "endOffset": 139}, {"referenceID": 73, "context": "The approach Propagated Semantic Transfer [74] combines knowledge transferred via attributes from known classes (left) with few labeled examples in graph (red lines) which is build according to instance similarity.", "startOffset": 42, "endOffset": 46}, {"referenceID": 42, "context": "4 shows results on the AwA [43] dataset.", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "This variant also outperform DAP and IAP [44] as well as Zero-Shot Learning [22].", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "This variant also outperform DAP and IAP [44] as well as Zero-Shot Learning [22].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "4(b) we compare PST to two label propagation (LP) baselines [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "[14] and uses similarity based image features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "DAP [44] 81.", "startOffset": 4, "endOffset": 8}, {"referenceID": 43, "context": "4 IAP [44] 80.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "2 Zero-Shot Learning [22] n/a 41.", "startOffset": 21, "endOffset": 25}, {"referenceID": 73, "context": "3 PST [74] on image descriptors 81.", "startOffset": 6, "endOffset": 10}, {"referenceID": 73, "context": "Adapted from [74].", "startOffset": 13, "endOffset": 17}, {"referenceID": 73, "context": "Adapted from [74].", "startOffset": 13, "endOffset": 17}, {"referenceID": 61, "context": "Understanding activities in visual and textual data is generally regarded as more challenging than understanding object categories due to the limited training data, challenges in defining the extend of an activity, and the similarities between activities [62].", "startOffset": 255, "endOffset": 259}, {"referenceID": 71, "context": "However, long-term composite activities can be decomposed in shorter fine-grained activities [72].", "startOffset": 93, "endOffset": 97}, {"referenceID": 91, "context": "In addition to holistic features [92], one consequently should exploit human posebased [71] and hand-centric [77] features.", "startOffset": 33, "endOffset": 37}, {"referenceID": 70, "context": "In addition to holistic features [92], one consequently should exploit human posebased [71] and hand-centric [77] features.", "startOffset": 87, "endOffset": 91}, {"referenceID": 76, "context": "In addition to holistic features [92], one consequently should exploit human posebased [71] and hand-centric [77] features.", "startOffset": 109, "endOffset": 113}, {"referenceID": 71, "context": "[72] collected textual description (Script data) of these activities with AMT.", "startOffset": 0, "endOffset": 4}, {"referenceID": 75, "context": "Table 3 shows results on the MPII Cooking 2 dataset [76].", "startOffset": 52, "endOffset": 56}, {"referenceID": 91, "context": "Comparing the first column (holistic Dense Trajectory features [92]) with the second, shows the benefit of adding the more semantic hand-[77] and pose-[71] features.", "startOffset": 63, "endOffset": 67}, {"referenceID": 76, "context": "Comparing the first column (holistic Dense Trajectory features [92]) with the second, shows the benefit of adding the more semantic hand-[77] and pose-[71] features.", "startOffset": 137, "endOffset": 141}, {"referenceID": 70, "context": "Comparing the first column (holistic Dense Trajectory features [92]) with the second, shows the benefit of adding the more semantic hand-[77] and pose-[71] features.", "startOffset": 151, "endOffset": 155}, {"referenceID": 88, "context": "This intriguing task has recently received increased attention in computer vision and computational linguistics communities [89, 90, 91] and has a large number of potential applications including human robot interaction, image and video retrieval, and describing visual content for visually impaired people.", "startOffset": 124, "endOffset": 136}, {"referenceID": 89, "context": "This intriguing task has recently received increased attention in computer vision and computational linguistics communities [89, 90, 91] and has a large number of potential applications including human robot interaction, image and video retrieval, and describing visual content for visually impaired people.", "startOffset": 124, "endOffset": 136}, {"referenceID": 90, "context": "This intriguing task has recently received increased attention in computer vision and computational linguistics communities [89, 90, 91] and has a large number of potential applications including human robot interaction, image and video retrieval, and describing visual content for visually impaired people.", "startOffset": 124, "endOffset": 136}, {"referenceID": 91, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 25, "endOffset": 29}, {"referenceID": 91, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 30, "endOffset": 42}, {"referenceID": 76, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 30, "endOffset": 42}, {"referenceID": 70, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 30, "endOffset": 42}, {"referenceID": 91, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 43, "endOffset": 47}, {"referenceID": 91, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 48, "endOffset": 60}, {"referenceID": 76, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 48, "endOffset": 60}, {"referenceID": 70, "context": "Activity representation: [92] [92, 77, 71] [92] [92, 77, 71]", "startOffset": 48, "endOffset": 60}, {"referenceID": 75, "context": "Table 3 Composite cooking activity classification on MPII Cooking 2 [76], mean AP in %.", "startOffset": 68, "endOffset": 72}, {"referenceID": 75, "context": "Adapted from [76].", "startOffset": 13, "endOffset": 17}, {"referenceID": 74, "context": "[75] propose a two-step translation approach which first predicts an intermediate semantic attribute layer and then learns how to translate from this semantic representation to natural sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "For this a parallel corpus of videos, annotated semantic attributes, and textual descriptions allows to adapt statistical machine translation (SMT) [39]", "startOffset": 148, "endOffset": 152}, {"referenceID": 74, "context": "Overview of the two-step translation approach [75] with an intermediate semantic layer of attributes (SR) for describing videos with natural language.", "startOffset": 46, "endOffset": 50}, {"referenceID": 67, "context": "From [68].", "startOffset": 5, "endOffset": 9}, {"referenceID": 70, "context": "proach on the videos of the MPII Cooking dataset [71, 72] and the aligned descriptions from the TACoS corpus [62].", "startOffset": 49, "endOffset": 57}, {"referenceID": 71, "context": "proach on the videos of the MPII Cooking dataset [71, 72] and the aligned descriptions from the TACoS corpus [62].", "startOffset": 49, "endOffset": 57}, {"referenceID": 61, "context": "proach on the videos of the MPII Cooking dataset [71, 72] and the aligned descriptions from the TACoS corpus [62].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "[16] who mine visual concepts for image description by integrating multiple instance learning [52].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[16] who mine visual concepts for image description by integrating multiple instance learning [52].", "startOffset": 94, "endOffset": 98}, {"referenceID": 94, "context": "[95] learn an intermediate attribute representation from the image descriptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[64] extend", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "For an example output on the TACoS MultiLevel corpus [64] see Figure 8.", "startOffset": 53, "endOffset": 57}, {"referenceID": 63, "context": "From [64].", "startOffset": 5, "endOffset": 9}, {"referenceID": 92, "context": "7) are very well suited for that in contrast to Bag-of-Words dense trajectories [93].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "[12] show that the second step, the SMTbased sentence generation, can be replaced with a deep recurrent network to better model visual uncertainty, but still relying on the multi-sentence reasoning on the semantic level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "9% [64] with SMT and 24.", "startOffset": 3, "endOffset": 7}, {"referenceID": 74, "context": "9% with SMT without multi-sentence reasoning [75].", "startOffset": 45, "endOffset": 49}, {"referenceID": 66, "context": "Two challenges arise, when extending the idea presented above to movie description [67], which looks at the problem how to describe movies for blind people.", "startOffset": 83, "endOffset": 87}, {"referenceID": 66, "context": "[67] propose to extract attribute labels from the description to train visual classifiers to build a semantic intermediate layer by relying on a semantic parsing approach of the description.", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "[66] show how to improve the robustness of these attributes or \u201cVisual Labels\u201d by three steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 91, "context": "by distinguishing three semantic groups of labels (verbs, objects and scenes) and using corresponding feature representations for each: activity recognition with dense trajectories [92], object detection with LSDA [31], and scene classification with", "startOffset": 181, "endOffset": 185}, {"referenceID": 30, "context": "by distinguishing three semantic groups of labels (verbs, objects and scenes) and using corresponding feature representations for each: activity recognition with dense trajectories [92], object detection with LSDA [31], and scene classification with", "startOffset": 214, "endOffset": 218}, {"referenceID": 66, "context": "SMT [67] Someone is a man, someone is a man.", "startOffset": 4, "endOffset": 8}, {"referenceID": 87, "context": "S2VT [88] Someone looks at him, someone turns to someone.", "startOffset": 5, "endOffset": 9}, {"referenceID": 65, "context": "Visual labels [66] Someone is standing in the crowd, a little man with a little smile.", "startOffset": 14, "endOffset": 18}, {"referenceID": 66, "context": "SMT [67] The car is a water of the water.", "startOffset": 4, "endOffset": 8}, {"referenceID": 87, "context": "S2VT [88] On the door, opens the door opens.", "startOffset": 5, "endOffset": 9}, {"referenceID": 65, "context": "Visual labels [66] The fellowship are in the courtyard.", "startOffset": 14, "endOffset": 18}, {"referenceID": 66, "context": "SMT [67] Someone is down the door, someone is a back of the door, and someone is a door.", "startOffset": 4, "endOffset": 8}, {"referenceID": 87, "context": "S2VT [88] Someone shakes his head and looks at someone.", "startOffset": 5, "endOffset": 9}, {"referenceID": 65, "context": "Visual labels [66] Someone takes a drink and pours it into the water.", "startOffset": 14, "endOffset": 18}, {"referenceID": 66, "context": "9 Qualitative results on the MPII Movie Description (MPII-MD) dataset [67].", "startOffset": 70, "endOffset": 74}, {"referenceID": 65, "context": "The \u201cVisual labels\u201d approach [66] which uses an intermediate layer of robust attributes, identifies activities, objects, and places better than related work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 65, "context": "From [66].", "startOffset": 5, "endOffset": 9}, {"referenceID": 98, "context": "Places-CNN [99].", "startOffset": 11, "endOffset": 15}, {"referenceID": 66, "context": "use SMT for sentence generation in [67], they rely on a recurrent network (LSTM) in [66].", "startOffset": 35, "endOffset": 39}, {"referenceID": 65, "context": "use SMT for sentence generation in [67], they rely on a recurrent network (LSTM) in [66].", "startOffset": 84, "endOffset": 88}, {"referenceID": 87, "context": "The Visual Labels approach outperforms prior work [88, 67, 96] on the MPIIMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.", "startOffset": 50, "endOffset": 62}, {"referenceID": 66, "context": "The Visual Labels approach outperforms prior work [88, 67, 96] on the MPIIMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.", "startOffset": 50, "endOffset": 62}, {"referenceID": 95, "context": "The Visual Labels approach outperforms prior work [88, 67, 96] on the MPIIMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.", "startOffset": 50, "endOffset": 62}, {"referenceID": 66, "context": "The Visual Labels approach outperforms prior work [88, 67, 96] on the MPIIMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 85, "context": "The Visual Labels approach outperforms prior work [88, 67, 96] on the MPIIMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.", "startOffset": 92, "endOffset": 96}, {"referenceID": 66, "context": "An interesting characteristic of the compared methods is the size of the output vocabulary, which is 94 for [67], 86 for [88] (which uses an end-to-end LSTM approach without an intermediate semantic representation) and 605 for [66].", "startOffset": 108, "endOffset": 112}, {"referenceID": 87, "context": "An interesting characteristic of the compared methods is the size of the output vocabulary, which is 94 for [67], 86 for [88] (which uses an end-to-end LSTM approach without an intermediate semantic representation) and 605 for [66].", "startOffset": 121, "endOffset": 125}, {"referenceID": 65, "context": "An interesting characteristic of the compared methods is the size of the output vocabulary, which is 94 for [67], 86 for [88] (which uses an end-to-end LSTM approach without an intermediate semantic representation) and 605 for [66].", "startOffset": 227, "endOffset": 231}, {"referenceID": 65, "context": "Although it is far lower than 6,422 for the human reference sentences, it clearly shows a higher diversity of the output for [66].", "startOffset": 125, "endOffset": 129}, {"referenceID": 90, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 0, "endOffset": 20}, {"referenceID": 49, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 0, "endOffset": 20}, {"referenceID": 88, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "[91, 50, 12, 16, 89]) are limited to describe objects which appear in caption corpora such as MS COCO [8] which consist of pairs of images and sentences.", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "ImageNet [10]) or text only corpora (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "[30] propose the Deep Compositional Captioner (DCC) to exploit these vision-only and language-only unpaired data sources to describe novel categories as visualized in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The Lexical Layer is expanded by objects from ImageNet [10].", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "The Deep Compositional Captioner (DCC) [30] uses an intermediate semantic attribute or \u201clexical\u201d layer to connect classifiers learned on unpaired image datasets (ImageNet) with text corpora (e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "Adapted from [29].", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "DCC [30] compared to an ablation without transfer.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "From [29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 54, "context": "To be able to not only recognize but also generate the description about the novel objects, DCC transfers the word prediction model from semantically closest known word in the Lexical Layer, where similarity is computed with Word2Vec [55].", "startOffset": 234, "endOffset": 238}, {"referenceID": 90, "context": "Interesting to note is, that image captioning approaches such as [91, 12] do use ImageNet data to (pre-) train the models (indicated with a dashed arrow in Fig.", "startOffset": 65, "endOffset": 73}, {"referenceID": 11, "context": "Interesting to note is, that image captioning approaches such as [91, 12] do use ImageNet data to (pre-) train the models (indicated with a dashed arrow in Fig.", "startOffset": 65, "endOffset": 73}, {"referenceID": 64, "context": "(a) Without bounding box annotations at training or test time GroundeR [65] learns to ground free-form natural language phrases in images.", "startOffset": 71, "endOffset": 75}, {"referenceID": 64, "context": "(b) GroundeR [65] reconstructs phrases by learning to attend to the right box at training time.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "(c) GroundeR [65] localizes boxes test time.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "From [65].", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "in [40, 34, 5, 81], it is restricted to few categories.", "startOffset": 3, "endOffset": 18}, {"referenceID": 33, "context": "in [40, 34, 5, 81], it is restricted to few categories.", "startOffset": 3, "endOffset": 18}, {"referenceID": 4, "context": "in [40, 34, 5, 81], it is restricted to few categories.", "startOffset": 3, "endOffset": 18}, {"referenceID": 80, "context": "in [40, 34, 5, 81], it is restricted to few categories.", "startOffset": 3, "endOffset": 18}, {"referenceID": 35, "context": "[36, 37] who aim to discover a latent alignment between phrases in text and bounding box proposals in the image.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[36, 37] who aim to discover a latent alignment between phrases in text and bounding box proposals in the image.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[37] ground dependency-tree relations to image regions using multiple instance learning (MIL) and a ranking objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Karpathy and Fei-Fei [36] simplify the MIL objective to just the maximal scoring box and replace the dependency tree with a learned recurrent network.", "startOffset": 21, "endOffset": 25}, {"referenceID": 64, "context": "13 Qualitative results for GroundeR unsupervised [65] on Flickr 30k Entities [60].", "startOffset": 49, "endOffset": 53}, {"referenceID": 59, "context": "13 Qualitative results for GroundeR unsupervised [65] on Flickr 30k Entities [60].", "startOffset": 77, "endOffset": 81}, {"referenceID": 59, "context": "released: Flickr30k Entities [60] augments Flickr30k [97] with bounding boxes for all noun phrases present in textual descriptions and ReferItGame [38] has localized referential expressions in images.", "startOffset": 29, "endOffset": 33}, {"referenceID": 96, "context": "released: Flickr30k Entities [60] augments Flickr30k [97] with bounding boxes for all noun phrases present in textual descriptions and ReferItGame [38] has localized referential expressions in images.", "startOffset": 53, "endOffset": 57}, {"referenceID": 37, "context": "released: Flickr30k Entities [60] augments Flickr30k [97] with bounding boxes for all noun phrases present in textual descriptions and ReferItGame [38] has localized referential expressions in images.", "startOffset": 147, "endOffset": 151}, {"referenceID": 46, "context": "Even more recent, at the time of writing, efforts are being made to also collect grounded referential expressions for the MS COCO [47] dataset, namely the authors of ReferItGame are in progress of extending their annotations as well as longer referential expressions have been collected by Mao et al.", "startOffset": 130, "endOffset": 134}, {"referenceID": 50, "context": "[51].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "Similar efforts are also made in the Visual Genome project [41] which provides densely annotated images with phrases.", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "In the following we focus on how to approach this problem and the first question is, where is the best point of interaction between linguistic elements and visual elements? Following the approaches in [36, 37, 60] a good way to this is to decompose both, sentence and image into concise semantic units or attributes which we can match to each other.", "startOffset": 201, "endOffset": 213}, {"referenceID": 36, "context": "In the following we focus on how to approach this problem and the first question is, where is the best point of interaction between linguistic elements and visual elements? Following the approaches in [36, 37, 60] a good way to this is to decompose both, sentence and image into concise semantic units or attributes which we can match to each other.", "startOffset": 201, "endOffset": 213}, {"referenceID": 59, "context": "In the following we focus on how to approach this problem and the first question is, where is the best point of interaction between linguistic elements and visual elements? Following the approaches in [36, 37, 60] a good way to this is to decompose both, sentence and image into concise semantic units or attributes which we can match to each other.", "startOffset": 201, "endOffset": 213}, {"referenceID": 86, "context": "For the data as shown in Figures 12(a) and 13, sentences can be split into phrases of typically a few words and images are composed into a larger number of bounding box proposals [87].", "startOffset": 179, "endOffset": 183}, {"referenceID": 34, "context": "An alternative is to integrate phrase grounding in a fully-convolutional network, for bounding box prediction [35] or segmentation prediction [33].", "startOffset": 110, "endOffset": 114}, {"referenceID": 32, "context": "An alternative is to integrate phrase grounding in a fully-convolutional network, for bounding box prediction [35] or segmentation prediction [33].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "SCRC [32] GroundeR semi-supervised [65] GroundeR supervised [65] with 3.", "startOffset": 5, "endOffset": 9}, {"referenceID": 64, "context": "SCRC [32] GroundeR semi-supervised [65] GroundeR supervised [65] with 3.", "startOffset": 35, "endOffset": 39}, {"referenceID": 64, "context": "SCRC [32] GroundeR semi-supervised [65] GroundeR supervised [65] with 3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 37, "context": "14 Qualitative grounding results on ReferItGame Dataset [38].", "startOffset": 56, "endOffset": 60}, {"referenceID": 46, "context": "is true for both curated datasets such as MSCOCO [47] or large user generated content as e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 83, "context": "in the YFCC 100M dataset [84].", "startOffset": 25, "endOffset": 29}, {"referenceID": 64, "context": "[65] propose the approach GroundeR, which is able to learn the grounding by aiming to reconstruct a given phrase using an attention mechanism as shown in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "Unsupervised training GroundeR (VGG-CLS) [65] 24.", "startOffset": 41, "endOffset": 45}, {"referenceID": 64, "context": "66 GroundeR (VGG-DET) [65] 32.", "startOffset": 22, "endOffset": 26}, {"referenceID": 64, "context": "Semi-supervised training GroundeR (VGG-CLS) [65] 3.", "startOffset": 44, "endOffset": 48}, {"referenceID": 59, "context": "Supervised training CCA embedding [60] 25.", "startOffset": 34, "endOffset": 38}, {"referenceID": 31, "context": "30 SCRC (VGG+SPAT) [32] 27.", "startOffset": 19, "endOffset": 23}, {"referenceID": 64, "context": "80 GroundeR (VGG-CLS) [65] 41.", "startOffset": 22, "endOffset": 26}, {"referenceID": 64, "context": "56 GroundeR (VGG-DET) [65] 47.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Unsupervised training LRCN [12] (reported in [32]) 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 31, "context": "Unsupervised training LRCN [12] (reported in [32]) 8.", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "59 CAFFE-7K [27] (reported in [32]) 10.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "59 CAFFE-7K [27] (reported in [32]) 10.", "startOffset": 30, "endOffset": 34}, {"referenceID": 64, "context": "38 GroundeR (VGG+SPAT) [65] 10.", "startOffset": 23, "endOffset": 27}, {"referenceID": 64, "context": "Semi-supervised training GroundeR (VGG+SPAT) [65] 3.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "Supervised training SCRC (VGG+SPAT) [32] 17.", "startOffset": 36, "endOffset": 40}, {"referenceID": 64, "context": "93 GroundeR (VGG+SPAT) [65] 26.", "startOffset": 23, "endOffset": 27}, {"referenceID": 59, "context": "(a) Flickr 30k Entities dataset [60] (b) ReferItGame dataset [38]", "startOffset": 32, "endOffset": 36}, {"referenceID": 37, "context": "(a) Flickr 30k Entities dataset [60] (b) ReferItGame dataset [38]", "startOffset": 61, "endOffset": 65}, {"referenceID": 78, "context": "VGG-CLS: Pre-training the VGG network [79] for the visual representation on ImageNet classification data only.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "VGG-DET: VGG further fine-tuned for the object detection task on the PASCAL dataset [15] using Fast R-CNN [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "VGG-DET: VGG further fine-tuned for the object detection task on the PASCAL dataset [15] using Fast R-CNN [25].", "startOffset": 106, "endOffset": 110}, {"referenceID": 64, "context": "If grounding supervision (phrase bounding box associations) is available, GroundeR [65] can integrate it by adding a loss over the attention mechanism (Fig.", "startOffset": 83, "endOffset": 87}, {"referenceID": 59, "context": "[60] proposed to learn a CCA embedding [26] between phrases and the visual representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[60] proposed to learn a CCA embedding [26] between phrases and the visual representation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "The Spatial Context Recurrent ConvNet (SCRC) [32] and the approach of Mao et al.", "startOffset": 45, "endOffset": 49}, {"referenceID": 50, "context": "[51] use a caption generation framework to score phrases on a set of bounding box proposals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] show the benefit of transferring models trained on full-image description datasets as well as spatial (bounding box location and size) and full-image context features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[51] show how to discriminatively train the caption generation framework to better distinguish different referential expression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "In the following we discuss results on the Flickr 30k Entities dataset [60] and the ReferItGame dataset [38], which both provide ground truth alignment between noun phrases (within sentences) and bounding boxes.", "startOffset": 71, "endOffset": 75}, {"referenceID": 37, "context": "In the following we discuss results on the Flickr 30k Entities dataset [60] and the ReferItGame dataset [38], which both provide ground truth alignment between noun phrases (within sentences) and bounding boxes.", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "[2] propose to dynamically create a deep network which is composed of different \u201cmodules\u201d (colored boxes).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Adapted from [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 78, "context": "approaches use the activations of the second last layer of the VGG network [79] to encode the image inside the bounding boxes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 59, "context": "The unsupervised variant of GroundeR reaches nearly the supervised performance of CCA [60] or SCRC[32] on Flickr 30k Entities, successful examples are shown in Fig.", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "The unsupervised variant of GroundeR reaches nearly the supervised performance of CCA [60] or SCRC[32] on Flickr 30k Entities, successful examples are shown in Fig.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "15 +FT: image features fine-tuned on captions [12] NMN: ablation w/o LSTM how many different lights in various different shapes and sizes? four (four) what color is the vase?", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "(a) Results from evaluation server of [4] in %.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "(b) Answers from [1] (ground truth answers in parentheses).", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "16 Results on the VQA dataset [4].", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Adapted from [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 48, "context": "Most recent approaches to visual question answering learn a joint hidden embedding of the question and the image to predict the answer [49, 63, 24, 4] where all computation is shared and identical for all questions.", "startOffset": 135, "endOffset": 150}, {"referenceID": 62, "context": "Most recent approaches to visual question answering learn a joint hidden embedding of the question and the image to predict the answer [49, 63, 24, 4] where all computation is shared and identical for all questions.", "startOffset": 135, "endOffset": 150}, {"referenceID": 23, "context": "Most recent approaches to visual question answering learn a joint hidden embedding of the question and the image to predict the answer [49, 63, 24, 4] where all computation is shared and identical for all questions.", "startOffset": 135, "endOffset": 150}, {"referenceID": 3, "context": "Most recent approaches to visual question answering learn a joint hidden embedding of the question and the image to predict the answer [49, 63, 24, 4] where all computation is shared and identical for all questions.", "startOffset": 135, "endOffset": 150}, {"referenceID": 94, "context": "[95], who learn an intermediate attribute representation from the image descriptions, similar to the work discussed in Sections 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The embedded textual knowledge base information is combined with the attribute representation and the hidden representation of a caption-generation recurrent network (LSTM) and forms the input to an LSTM-based question-answer encoder-decoder [49].", "startOffset": 242, "endOffset": 246}, {"referenceID": 1, "context": "[2] go one step further with respect to compositionality and propose to predict a compositional neural network structure from the questions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 78, "context": "The find[cat] and find[amber] (green) modules take in CNN activations (VGG [79], last convolutional layer) and produce a spatial attention heatmap, while combine[and] (orange) combines two heatmaps to a single one, and describe[where] (blue) takes in a heatmap and CNN features to predict an answer.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "[3] learn not only the modules, but also what the best network structure is from a set of parser proposals, using reinforcement learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2, 3] also incorporate a recurrent network (LSTM) to model common sense knowledge and dataset bias which has been shown to be important for visual question answering [49].", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[2, 3] also incorporate a recurrent network (LSTM) to model common sense knowledge and dataset bias which has been shown to be important for visual question answering [49].", "startOffset": 0, "endOffset": 6}, {"referenceID": 48, "context": "[2, 3] also incorporate a recurrent network (LSTM) to model common sense knowledge and dataset bias which has been shown to be important for visual question answering [49].", "startOffset": 167, "endOffset": 171}, {"referenceID": 61, "context": "[62] and Silberer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "[78] take a step in this direction by looking at joint semantic representation from the textual and visual modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 66, "context": "Section 3 presents compositional models for describing videos, but it is only a first step towards automatically describing a movie to a blind person as humans can do it [67], which will require an even higher degree of semantic understanding, and transfer within and between modalities.", "startOffset": 170, "endOffset": 174}], "year": 2016, "abstractText": "Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.", "creator": "LaTeX with hyperref package"}}}