{"id": "1604.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Optimal Margin Distribution Machine", "abstract": "Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper.", "histories": [["v1", "Tue, 12 Apr 2016 11:39:16 GMT  (58kb)", "http://arxiv.org/abs/1604.03348v1", "arXiv admin note: substantial text overlap witharXiv:1311.0989"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1311.0989", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["teng zhang", "zhi-hua zhou"], "accepted": false, "id": "1604.03348"}, "pdf": {"name": "1604.03348.pdf", "metadata": {"source": "CRF", "title": "Optimal Margin Distribution Machine", "authors": ["Teng Zhang", "Zhi-Hua Zhou"], "emails": ["zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.03 348v 1 [cs.L G] 12 ASupport Vector Machine (SVM) is one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e. the smallest distance from the instances to the classification limit. However, recent theoretical results show that maximizing the minimum margin does not necessarily lead to better generalization performance, but that margin distribution is demonstrably more decisive. On the basis of this idea, we propose a new method called Optimal Margin Distribution Machine (ODM), which seeks to achieve better generalization performance by optimizing margin distribution. We characterize margin distribution using first and second order statistics, i.e. margin average and variance. The proposed method is a general learning approach that can be applied wherever SVM can be applied, and its superiority is verified both theoretically and empirically in this paper."}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are living, in which they are able to live"}, {"heading": "2. Preliminaries", "text": "We designate the margin of the instance (xi, yi) and Y = {+ 1, \u2212 1} the labeling specified. Let D have an unknown (underlying) distribution over X \u00b7 Y. A training set of size mS = {(x1, y1), (x2, y2),..., (xm, ym), is identical and independent (i.i.d.) according to the distribution D. Our goal is to learn a function that is used to predict the labels for future invisible instabilities. For SVMs, f is considered a linear model, i.e. f (x) = w (x), where w is a linear predictor. (x) is a characteristic mapping of the x-induced margin k, i.e. k (xi, xj) = optimization (xi)."}, {"heading": "3. Formulation", "text": "The two simplest statistics for characterizing margin distribution are the first and second order statistics, i.e. the mean and the variance of the margin. Formally, X is the matrix whose i-th column is separated from each other by a column vector (xi), i.e., X = [\u03c6 (x1),.., \u03c6 (xm), y = [y1,.., ym] is a column vector, and Y is a m x-m diagonal margin with y1,..., ym as the diagonal elements. According to the definition in (1), the margin indicator means that i = 1yiw x-ms is evident (xi)."}, {"heading": "3.2. ODM", "text": "The idea of ODML is relatively simple, but the final formulation is somewhat more complex. In this section, we will try to suggest a simpler solution. Note that SVM sets the minimum margin as 1 by scaling the margin variance by similarly setting the margin mean as 1. Then the margin deviation from (xi, yi) to the margin mean | yiw \u03c6 (xi) \u2212 1 |. By minimizing the margin variance, we arrive at the following formulation: min, i, 2 w w w w w + C-millimeter difference i = 1 (xi, yi) millimeter difference."}, {"heading": "4. Optimization", "text": "We first propose a dual coordinate descend method for the kernel ODML and ODM, and then propose a stochastic sequence with variance reduction for the large linear kernel ODML and ODM.4.1. In this section, we show that the dual kernel ODML and ODM both have convex square optimizations with only simple square constraints, and then a dual coordinate descend method ODMdcd to solve these problems. 4.1.1. Kernel ODMLBy substitution (3), (5), w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w."}, {"heading": "4.1.2. Kernel ODM", "text": "If you introduce the Lagrange multipliers for the two Q-markers (7), the Lagrange leads from (7) to L-label (w-label) + \u03b2-label (1-label) + \u03b2-label (1-label) + \u03b2-label (1-label) + \u03b2-label (1-label) + \u03b2-label (1-label). (15) If you set the partial derivatives of w-label (1-label) to zero, you get L-label (XY-label) + XY-label (18-label) and (15-label), you get L-label (2-label) [2-label (XY-label) + XY-label (18-label) to (15-label), so we get label (2-L-label) + XY-label (18-label)."}, {"heading": "4.1.3. Dual Coordinate Descent", "text": "(12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12). (12. (12). (12). (12). (12). (12. (12). (12). (12). (12. (12). (12). (12). (12. (12). (12). (12). (12). (12. (12). (12). (12. (12. (12). (12). (12). (12. (12). (12. (12). (12). (12). (12. (12). (12. (12). (12). (12. (12). (12. (12). (12). (12). (12."}, {"heading": "5. Analysis", "text": "In this section we will examine the statistical property of ODML and ODM. Here we will consider only the linear case for simplicity, but the results are also applicable to all other characteristics. As indicated in section 4.1, the double problem results in each of the following form, min \u03b1f (\u03b1) = 12 \u03b1 H\u03b1 + q \u03b1, s.t. 0 \u2264 i \u2264 i \u2264 ui, i = 1,. (27) Lemma 2. Let us find the optimal solution of (27), and add the solution of (A) = argmin 0 \u2264 u (H), f (H) = argmin 0 \u2264 i,. (H)."}, {"heading": "6. Empirical Study", "text": "In this section, we evaluate empirically the effectiveness of our methods using a wide range of data sets. We first present the experimental settings in Section 6.1 and then compare ODML and ODM with SVM and Linear Discriminant Analysis (LDA) in Section 6.2 and Section 6.3. In addition, we also examine the cumulative margin distribution generated by ODML, ODM and SVM in Section 6.4. Calculation costs are presented in Section 6.5."}, {"heading": "6.1. Experimental Setup", "text": "We evaluate the effectiveness of our proposed methods on thirty-two regular datasets and ten large datasets, including both UCI datasets and real datasets such as KDD20101. Table 1 summarizes the statistics of these datasets, ranging in size from 62 to more than 8,000,000, and dimensionality from 2 to more than 20,000,000, covering a wide range of properties. All characteristics are standardized to the interval [0, 1]. For each dataset, half of the examples are randomly selected as training data, and the remaining examples are used as test data. For regular datasets, both linear and RBF cores are evaluated. Experiments are repeated 30 times with random data partitions, and average accuracies and standard deviations are recorded. For large datasets, linear datasets are evaluated."}, {"heading": "6.2. Results on Regular Scale Data Sets", "text": "Tables 2 and 3 summarize the results on 32 regular datasets. As you can see, the overall performance of our methods is superior or very competitive to SVM. Especially for the linear kernel, ODML / ODM performs significantly better on 17 / 24 over 32 datasets than SVM on 17 / 24 over 32 datasets and achieves the best accuracy on 31 datasets; for the RBF kernel, ODML / ODM performs significantly better on 15 / 25 over 32 datasets than SVM and achieves the best accuracy on 31 datasets. Furthermore, the win / tie / loss counts show that ODML and ODM are always better or comparable, never worse than SVM."}, {"heading": "6.3. Results on Large Scale Data Sets", "text": "Table 4 summarizes the results in ten large datasets. LDA did not provide the results in some datasets due to the high computing costs. As can be seen, the overall performance of our methods is superior to SVM or very competitive. In particular, ODML / ODM performs significantly better than SVM in 6 / 7 over 10 datasets and achieves the best accuracy in almost all datasets.6.4. Margin Distribution Figure 1 plots the cumulative margin distribution of SVM, ODML and ODM in some representative regular datasets. The curves of other datasets are more or less similar, the point at which a curve and the x-axis crosses are the corresponding minimum span. As you can see, our methods generally have a slightly smaller minimum distance than SVM, whereas the curve of ODML and ODM is generally on the right, which shows that the margin distribution of ODML and ODM is generally better than SVM. In other words, most of our methods we have a larger margin."}, {"heading": "6.5. Time Cost", "text": "All experiments are performed with MATLAB 2012b on a computer with 8 x 2.60 GHz CPUs and 32 GB of RAM. Average CPU time (in seconds) for each data set is shown in Figure 2. We refer to SVM implemented by the LIBLINEAR (Fan et al., 2008) package as SVMl or SVM implemented by SGD2 as SVMs. It turns out that both SVMs and our methods are faster than SVMl, due to the use of SGD. ODML and ODM are only slightly slower than SVMs on three data sets (adult-a, w8a and mini-boo-ne), but highly competitive with SVMs on the remaining data sets. Note that both SVML and SVM are very fast implementations of SVMs; this shows that ODML and ODM are also computationally efficientd.2led / / onbots.org /."}, {"heading": "7. Related Work", "text": "There are some studies that consider margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).Garg et al. (Garg and Roth, 2003) proposed the Margin Distribution Optimization (MDO) algorithm, which minimizes the sum of the costs of each instance, with costs being a function that assigns larger values to instances with smaller margins. Furthermore, MDO can only be considered as a method for optimizing margin combinations, where weights are related to margins. However, the objective function optimized by MDO is not convex and can therefore get stuck in local minimums. Furthermore, MDO can only be used for linear margin combinations. Pelckmans et al. (Pelckmans et al., 2008) suggested the maximum average margin of classifiers (MAMC), and it can be considered a special case of the MDO, which is directly comparable to a combination of margins."}, {"heading": "8. Conclusions", "text": "Recent theoretical results suggest that margin distribution is more important for generalization performance than a one-point margin such as the minimum margin. In this paper, we propose a new method called the Optimal Margin Distribution Machine (ODM), which attempts to optimize margin distribution by taking into account the margin mean and margin variance at the same time. Our method is a general learning approach that can be applied wherever SVM can be applied. Comprehensive experiments with thirty-two regular datasets and ten large-scale datasets confirm the superiority of our method over SVM. In the future, it will be interesting to generalize the idea of optimal margin distribution to other learning settings."}], "references": [{"title": "A kernel method for the optimization of the margin distribution", "author": ["F. Aiolli", "G. San Martino", "A. Sperduti"], "venue": "Proceedings of the 18th International Conference on Artificial Neural Networks. Prague,", "citeRegEx": "Aiolli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aiolli et al\\.", "year": 2008}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of the 19th International Conference on Computational Statistics. Paris,", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Prediction games and arcing classifiers", "author": ["L. Breiman"], "venue": "Neural Computation", "citeRegEx": "Breiman,? \\Q1999\\E", "shortCiteRegEx": "Breiman", "year": 1999}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-shwartz", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Cotter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2013}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2000}, {"title": "Convex formulations of radius-margin based support vector machines", "author": ["H. Do", "K. Alexandre"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Do and Alexandre,? \\Q2013\\E", "shortCiteRegEx": "Do and Alexandre", "year": 2013}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "The use of multiple measurements in taxonomic", "author": ["R.A. Fisher"], "venue": null, "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "On the doubt about margin explanation", "author": ["W. pp. 23\u201337. Gao", "Zhou", "Z.-H"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Margin distribution and learning", "author": ["A. Garg", "D. Roth"], "venue": "boosting. Artificial Intelligence", "citeRegEx": "Garg and Roth,? \\Q2003\\E", "shortCiteRegEx": "Garg and Roth", "year": 2003}, {"title": "Accelerating stochastic gradient descent using", "author": ["R. Johnson", "T. Zhang"], "venue": "Helsinki, Finland,", "citeRegEx": "Johnson and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Stochastic approximation and recursive", "author": ["H.J. Kushner", "G.G. Yin"], "venue": "International Conference on Machine Learning. Atlanta,", "citeRegEx": "Kushner and Yin,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "On estimation of characters", "author": ["A. pp. 53\u201361. Luntz", "V. Brailovsky"], "venue": null, "citeRegEx": "Luntz and Brailovsky,? \\Q1969\\E", "shortCiteRegEx": "Luntz and Brailovsky", "year": 1969}, {"title": "A risk minimization principle for a class of parzen estimators", "author": ["K. Pelckmans", "J. Suykens", "B.D. Moor"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pelckmans et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pelckmans et al\\.", "year": 2008}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization", "citeRegEx": "Polyak and Juditsky,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["L. Reyzin", "R.E. Schapire"], "venue": "Proceedings of 23rd International Conference on Machine Learning", "citeRegEx": "Reyzin and Schapire,? \\Q2006\\E", "shortCiteRegEx": "Reyzin and Schapire", "year": 2006}, {"title": "Boosting the margin: a new explanation for the effectives of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P.L. Bartlett", "W.S. Lee"], "venue": "Annuals of Statistics", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2001}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proceedings of the 24th International Conference on Machine Learning. Helsinki,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["O. Shamir", "T. Zhang"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Shamir and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Shamir and Zhang", "year": 2013}, {"title": "Mini-batch primal and dual methods for svms", "author": ["M. Takac", "A. Bijral", "P. Richtarik", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Takac et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Takac et al\\.", "year": 2013}, {"title": "The Nature of Statistical Learning Theory. Springer-Verlag, New York", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "A refined margin analysis for boosting algorithms via equilibrium margin", "author": ["L.W. Wang", "M. Sugiyama", "C. Yang", "Zhou", "Z.-H", "J. Feng"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent. CoRR, abs/1107.2490", "author": ["W. Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2011\\E", "shortCiteRegEx": "Xu", "year": 2011}, {"title": "Recent advances of large-scale linear classification", "author": ["G.X. Yuan", "C.H. Ho", "C.J. Lin"], "venue": "Proceedings of the IEEE", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the 21st International Conference on Machine learning. Banff,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Large margin distribution machine", "author": ["T. Zhang", "Zhou", "Z.-H"], "venue": "Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["Zhao", "P.-L", "T. Zhang"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Zhou", "Z.-H"], "venue": null, "citeRegEx": "Zhou and Z..H.,? \\Q2012\\E", "shortCiteRegEx": "Zhou and Z..H.", "year": 2012}, {"title": "Large margin distribution learning", "author": ["Zhou", "Z.-H"], "venue": "Proceedings of the 6th IAPR International Workshop on Artificial Neural Networks in Pattern Recognition (ANNPR\u201914). Montreal, Canada,", "citeRegEx": "Zhou and Z..H.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Z..H.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "Introduction Support Vector Machine (SVM) (Cortes and Vapnik, 1995; Vapnik, 1995) has always been one of the most successful learning algorithms.", "startOffset": 42, "endOffset": 81}, {"referenceID": 23, "context": ", the smallest distance from the examples to the classification boundary, and the margin theory (Vapnik, 1995) provided a good support to the generalization performance of SVMs.", "startOffset": 96, "endOffset": 110}, {"referenceID": 18, "context": "(Schapire et al., 1998) first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman (Breiman, 1999) indicated that the minimum margin is crucial and developed a boosting-style algorithm, named Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": ", 1998) first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman (Breiman, 1999) indicated that the minimum margin is crucial and developed a boosting-style algorithm, named Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance.", "startOffset": 130, "endOffset": 145}, {"referenceID": 17, "context": "(Reyzin and Schapire, 2006) found that although Arc-gv tends to produce larger minimum margin, it suffers from a poor margin distribution; they conjectured that the margin distribution, rather than the minimum margin, is more crucial to the generalization performance.", "startOffset": 0, "endOffset": 27}, {"referenceID": 24, "context": "Such a conjecture has been theoretically studied (Wang et al., 2011; Gao and Zhou, 2013), and it was recently proven by Gao and Zhou (Gao and Zhou, 2013).", "startOffset": 49, "endOffset": 88}, {"referenceID": 23, "context": "According to (Cortes and Vapnik, 1995; Vapnik, 1995), the margin of instance (xi, yi) is formulated as \u03b3i = yiw \u03c6(xi), \u2200i = 1, .", "startOffset": 13, "endOffset": 52}, {"referenceID": 5, "context": "From (Cristianini and Shawe-Taylor, 2000), it is shown that SVM with hardmargin (or Hard-margin SVM) is regarded as the maximization of the minimum distance, max w \u03b3\u0302", "startOffset": 5, "endOffset": 41}, {"referenceID": 19, "context": "According to the representer theorem (Sch\u00f6lkopf and Smola, 2001), the optimal solution will be spanned by the support vectors.", "startOffset": 37, "endOffset": 64}, {"referenceID": 26, "context": "As suggested by (Yuan et al., 2012), it can be efficiently solved by the dual coordinate descent method.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "To make them more useful, in the following, we present a fast linear kernel ODM and ODM for large scale problems by adopting the stochastic gradient descent with variance reduction (SVRG) (Polyak and Juditsky, 1992; Johnson and Zhang, 2013).", "startOffset": 188, "endOffset": 240}, {"referenceID": 11, "context": "To make them more useful, in the following, we present a fast linear kernel ODM and ODM for large scale problems by adopting the stochastic gradient descent with variance reduction (SVRG) (Polyak and Juditsky, 1992; Johnson and Zhang, 2013).", "startOffset": 188, "endOffset": 240}, {"referenceID": 12, "context": "Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution (Kushner and Yin, 2003; Bottou, 2010).", "startOffset": 127, "endOffset": 164}, {"referenceID": 2, "context": "Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution (Kushner and Yin, 2003; Bottou, 2010).", "startOffset": 127, "endOffset": 164}, {"referenceID": 27, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 20, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 1, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 21, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 11, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 16, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 11, "context": "Since the objective function of ODM is differentiable, in practice we use the stochastic gradient descent with variance reduction (SVRG) which is more robust than SGD (Johnson and Zhang, 2013).", "startOffset": 167, "endOffset": 192}, {"referenceID": 13, "context": "As shown in (Luntz and Brailovsky, 1969),", "startOffset": 12, "endOffset": 40}, {"referenceID": 8, "context": "ODM and ODM are compared with standard SVM which ignores the margin distribution, and Linear Discriminant Analysis (LDA) (Fisher, 1936).", "startOffset": 121, "endOffset": 135}, {"referenceID": 11, "context": "The parameters \u03b7 used for ODMsvrg are set with the same setup in (Johnson and Zhang, 2013).", "startOffset": 65, "endOffset": 90}, {"referenceID": 7, "context": "We denote SVM implemented by the LIBLINEAR (Fan et al., 2008) package as SVMl and SVM implemented by SGD as SVMs, respectively.", "startOffset": 43, "endOffset": 61}, {"referenceID": 10, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 14, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 0, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 10, "context": "(Garg and Roth, 2003) proposed the Margin Distribution Optimization (MDO) algorithm which minimizes the sum of the cost of each instance, where the cost is a function which assigns larger values to instances with smaller margins.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "(Pelckmans et al., 2008) proposed the Maximal Average Margin for Classifiers (MAMC) and it can be viewed as a special case of ODM assuming that the margin variance is zero.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "(Aiolli et al., 2008) proposed a Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) from a game theoretical perspective.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the firstand second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper.", "creator": "LaTeX with hyperref package"}}}