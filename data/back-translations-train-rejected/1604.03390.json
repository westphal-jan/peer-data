{"id": "1604.03390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Video Description using Bidirectional Recurrent Neural Networks", "abstract": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.", "histories": [["v1", "Tue, 12 Apr 2016 13:09:01 GMT  (1322kb,D)", "http://arxiv.org/abs/1604.03390v1", "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)"], ["v2", "Mon, 12 Dec 2016 12:28:07 GMT  (1320kb,D)", "http://arxiv.org/abs/1604.03390v2", "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)"]], "COMMENTS": "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["\\'alvaro peris", "marc bola\\~nos", "petia radeva", "francisco casacuberta"], "accepted": false, "id": "1604.03390"}, "pdf": {"name": "1604.03390.pdf", "metadata": {"source": "CRF", "title": "Video Description using Bidirectional Recurrent Neural Networks", "authors": ["\u00c1lvaro Peris", "Marc Bola\u00f1os", "Petia Radeva", "Francisco Casacuberta"], "emails": ["lvapeab@prhlt.upv.es", "fcn@prhlt.upv.es", "marc.bolanos@ub.edu", "petia.ivanova@ub.edu"], "sections": [{"heading": null, "text": "Keywords: Video Description, Neural Machine Translation, Birectional Recurrent Neural Networks, LSTM, Convolutional Neural Networks."}, {"heading": "1 Introduction", "text": "Auto-generation of image descriptions is a current trend in computer vision, which is an interesting but challenging task due to dramatic advances in Convolutional Neural Network (CNN) models that have allowed advanced algorithms to outperform many computer vision problems: object detection, object detection, activity detection, and so on. Generating video descriptions is an even more difficult task that could lead to multiple applications (such as video indexing and retrieval, movie description for multimedia applications, or for blind or human-robot interaction). We can imagine how much data is generated on YouTube (people watch hundreds of hours of video every day and generate billions of views) and how video descriptions would help categorize them, provide an efficient retrieval mechanism, and serve as a summary tool for the blind."}, {"heading": "2 Related Work", "text": "Although the problem of video labeling has appeared recently thanks to the new learning capabilities of deep learning techniques, the general pipeline used in these works is similar to the traditional encoder decoder methodology used in Machine Translation (MT).The main difference is that the prevailing approach is the compact representation of the source word set, we create a representation of the images that belong to the video. MT aims to automatically translate text or language from a source into a target language. In recent decades, the prevailing approach has been statistical. [5] The application of connectionist models in the field has attracted much attention from researchers in recent years. In addition, a new approach has been proposed for MT: so-called Neural Machine Translation, in which the translation process is carried out by a large number of people."}, {"heading": "3 Methodology", "text": "An overview of our proposal is presented in Fig. 1. We propose an encoder-decoder concept consisting of four stages, each using four stages to extract complementary features for each of the raw images from the video. Second, (red in the scheme) we must describe the actions performed in successive frames, and we apply a BLSTM to capture temporal relationships and complementary information by looking at the action in a forward and backward facing image. Third, (yellow in the scheme) the two output vectors of forward and backward directed LATM models of the previous step are linked to the CNN outputs for each image and fed into a soft attention model in the decoder. This model decides on which parts of the input video should focus on the emission of the next word, taking into account the description thus generated."}, {"heading": "4 Results", "text": "In this section, we describe the data sets and metrics used to evaluate and compare our model with state-of-the-art video captioning."}, {"heading": "4.1 Dataset", "text": "The Microsoft Research Video Description Corpus (MSVD) [2] is a dataset composed of 1970 open domain clips collected by YouTube and commented on via a crowd sourcing platform. Each video has a variable number of captions written by different users. We used the splits of [14,16], with the dataset separated into 1200 videos for training, 100 for validation and the remaining 670 for testing."}, {"heading": "4.2 Evaluation Metrics", "text": "To evaluate and compare the results of the different models, we used the standardized COCO Caption Evaluation Package [3], which provides several metrics for comparing text descriptions. We used three main metrics, all of which are represented from 0 (minimum quality) to 100 (maximum quality): BLEU [8]: This metric compares the ratio of n-gram structures shared between the system hypotheses and the reference sets. METEOR [7]: It was introduced to correct the absence of the recall component in the calculation of BLEU. It calculates the accuracy and recall value of hypotheses and references. CIDER [13]: Similar to BLEU, it calculates the number of matching Ngrams, but penalizes any n-gram that occurs frequently throughout the training set."}, {"heading": "4.3 Experimental Results", "text": "In all tests we performed with a batch size of 64, the learning rate was automatically adjusted using the Adadelta [17] method, and as the authors reported in [16], we performed a frame subsample, selecting only one image per 26 images to reduce the computational load. Network parameters were randomly initialized, the validation set was evaluated every 1000 updates, the learning process was interrupted when the reported error increased after 5 evaluations. For each configuration, we performed 10 experiments. In each of these experiments, we randomly determined the value of the critical model hyperparameters. Such hyperparameters and their tested ranges are m-ig [300, 700], | ht-ig [1000, 3000]. Using the BLSTM encoder, we performed an additional selection on | vj | bay [100, 2100]. For each configuration, the best model in terms of BLEU measurement size was selected to correspond to the object group of the BLSTM, which we reported best in the Fit with the Fit group of characteristics in Table 1."}, {"heading": "5 Discussion and Conclusions", "text": "By analyzing the results obtained, a clear trend of improvement can be derived when using the BLSTM as a temporal inference mechanism. BLSTM addition in the use of Objects features enables an improvement in the result across all metrics, resulting in a benefit of more than 2 BLEU points. Adding Scene-related features also slightly improves the result, although it is not as remarkable as the BLSTM improvement. The combination of Objects + Scenes + BLSTM provides the best CIDEr performance, but this result is slightly below the Objects + BLSTM features on the other metrics. This behavior is probably due to the significant increase in the parameters to be learned. It should be investigated whether the reduction in the number of parameters by reducing CNN features or using larger datasets for further improvements.Finally, we have developed a new methodology for natural language video descriptions, which benefits from an analysis of bidirectional input."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (arXiv:1409.0473),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "The meteor metric for automatic evaluation of machine translation", "author": ["Alon Lavie", "Michael J Denkowski"], "venue": "Machine translation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Within the last decades, the prevailing approach is the statistical one [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Moreover, a new approach to MT has been recently proposed: the so-called Neural Machine Translation, where the translation process is carried out by a means of a large Recurrent Neural Network (RNN) [11].", "startOffset": 199, "endOffset": 203}, {"referenceID": 3, "context": "Both RNNs usually make use of gated units, such as the popular Long Short-term Memory (LSTM) [4], in order to cope with long-term relationships.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "The recent reintroduction of Deep Learning in the Computer Vision field through CNNs [6], has allowed to obtain new and richer image representations compared to the traditional hand-crafted ones.", "startOffset": 85, "endOffset": 88}, {"referenceID": 16, "context": "These networks have demonstrated to be a powerful tool to extract feature representations for several kinds of computer vision problems like on objects [10] or scenes [18] recognition.", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "and generating, word by word, a final description of the image [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Seminal works applied methodologies inspired by classical MT [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "Nevertheless, more recent works following the encoder-decoder approach, obtained state-of-the-art performances [14,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "Nevertheless, more recent works following the encoder-decoder approach, obtained state-of-the-art performances [14,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "We propose to use as a base architecture the one introduced in [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "For this purpose we use a GoogleNet architecture [12] separately trained on two datasets, one for objects (ILSVRC dataset [10]), and the other for scenes (Places 205 [18]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "For this purpose we use a GoogleNet architecture [12] separately trained on two datasets, one for objects (ILSVRC dataset [10]), and the other for scenes (Places 205 [18]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "This network is equipped with an attention mechanism [1,16]: a soft alignment model, implemented as a single-layered perceptron, that helps the decoder to know where to look at for generating each output word.", "startOffset": 53, "endOffset": 59}, {"referenceID": 14, "context": "This network is equipped with an attention mechanism [1,16]: a soft alignment model, implemented as a single-layered perceptron, that helps the decoder to know where to look at for generating each output word.", "startOffset": 53, "endOffset": 59}, {"referenceID": 9, "context": "Following [11], a beam-search method is used to find the caption with highest conditional probability.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "The Microsoft Research Video Description Corpus (MSVD) [2] is a dataset composed of 1970 open domain clips collected from YouTube and annotated using a crowd sourcing platform.", "startOffset": 55, "endOffset": 58}, {"referenceID": 12, "context": "We used the splits made by [14,16], separating the dataset in 1200 videos for training, 100 for validation and the remaining 670 for testing.", "startOffset": 27, "endOffset": 34}, {"referenceID": 14, "context": "We used the splits made by [14,16], separating the dataset in 1200 videos for training, 100 for validation and the remaining 670 for testing.", "startOffset": 27, "endOffset": 34}, {"referenceID": 2, "context": "In order to evaluate and compare the results of the different models we used the standardized COCO-Caption evaluation package [3], which provides several metrics for text description comparison.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "BLEU [8]: this metric compares the ratio of n-gram structures that are shared between the system hypotheses and the reference sentences.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "METEOR [7]: it was introduced to solve the lack of the recall component when computing BLEU.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "CIDEr [13]: similarly to BLEU, it computes the number of matching ngrams, but penalizes any n-gram frequently found in the whole training set.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "On all the tests we used a batch size of 64, the learning rate was automatically set by the Adadelta [17] method and, as the authors in [16] reported, we applied a frame subsampling, picking only one image every 26 frames for reducing the computational load.", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "On all the tests we used a batch size of 64, the learning rate was automatically set by the Adadelta [17] method and, as the authors in [16] reported, we applied a frame subsampling, picking only one image every 26 frames for reducing the computational load.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "*Model from [16] only with Object features evaluated on our system.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "The first row correspond to the result obtained with our system with the object features from [16].", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.", "creator": "LaTeX with hyperref package"}}}