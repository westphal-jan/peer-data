{"id": "1604.03506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization", "abstract": "One of missions for personalization systems and recommender systems is to show content items according to users' personal interests. In order to achieve such goal, these systems are learning user interests over time and trying to present content items tailoring to user profiles. Recommending items according to users' preferences has been investigated extensively in the past few years, mainly thanks for the popularity of Netflix competition. In a real setting, users may be attracted by a subset of those items and interact with them, only leaving partial feedbacks to the system to learn in the next cycle, which leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions. Traditionally, unbiased data collection is done through showing items uniformly sampling from the content pool. However, this simple scheme is not feasible as it risks user engagement metrics and it takes long time to gather user feedbacks. In this paper, we introduce a user-friendly unbiased data collection framework, by utilizing methods developed in the exploitation and exploration literature. We discuss how the framework is different from normal multi-armed bandit problems and why such method is needed. We layout a novel Thompson sampling for Bernoulli ranked-list to effectively balance user experiences and data collection. The proposed method is validated from a real bucket test and we show strong results comparing to old algorithms", "histories": [["v1", "Tue, 12 Apr 2016 18:32:43 GMT  (38kb,D)", "http://arxiv.org/abs/1604.03506v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["liangjie hong", "adnan boz"], "accepted": false, "id": "1604.03506"}, "pdf": {"name": "1604.03506.pdf", "metadata": {"source": "CRF", "title": "An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization", "authors": ["Liangjie Hong", "Adnan Boz"], "emails": ["liangjie@yahoo-inc.com,", "adnanb@yahoo-inc.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it has come to the point where it can only take a year for a solution to be found and a solution to be found."}, {"heading": "2. EXPLOITATION AND EXPLORATION", "text": "In this section, we will discuss a specific type of exploitation and exploration algorithms: context-dependent, multi-armed bandit methods that we will develop for unbiased data acquisition. To simplify the discussion, we will use CTR as a metric for optimization."}, {"heading": "2.1 Contextual Multi-Armed Bandit Problems", "text": "A multi-armed bandit (MAB) problem is a sequential decision-making process. Bandit problems involve a decision in each round. As soon as a decision is made, a corresponding reward is collected and the corresponding reward is calculated. The contextual-bandit-formalistic distribution method becomes this classic setting by introducing contextual information into the interaction loop.Formally, we are defined by A = 1, 2, 2 and a policy that describes a series of actions, a contextual vector, a reward vector rt = {rt, rt, K} where each rt, a reward, a policy, and a policy that describes a contextual bandit (cMAB) describes a round interaction between a learner and the environment. In each round, the problem can be decomposed in the following steps: \u2022 The environment chooses (xt, rt) from some unknown distributions D. Only xt is disclosed to the reward, while the reward cannot be observed."}, {"heading": "3. UNBIASED OFFLINE EVALUATION", "text": "In this section, we present the basic framework for conducting unbiased data collection, and before going into detail, there are two basic design criteria that any approach to such a problem must meet: 1. The data set is therefore collected with impartiality or bias, but the bias could be addressed in later analyses.2. The proposed method must have adequate user participation so that users do not suffer from any data collection strategies, which are usually not the focus of classical exploitation / exploration literature.3.1 Unbiased data collection by cMAB is not only a powerful method for balancing exploration and exploitation, but also a method for constructing unbiased offline evaluation processes, proposed by [5]. The basic idea is that we use a known policy to conduct a cMAB problem for data collection and to select the actions that are likely to be undertaken, as well as the rewards, and the rewards."}, {"heading": "3.2 Unbiased Policy Evaluation", "text": "In this subsection, we show that Eq.1 is an unbiased evaluation procedure for politics. We define the value of a policy \u03c0 as: V (\u03c0) = E (x, r) = D (x, r) = D raP (x, r) dxdrIn such a case, we want to prove that ED (V) dDThe important step in proof is that we need to use the following quantity: E raI (x, a, pa) D raI (x) = a) D raI (x) = a) pa] P (D) dDThe important step in proof is that we need to use the following quantity: Ea raI (x) = a) pa] = raI (x) = a) (x) (x) that we have the quantity (x)."}, {"heading": "4. ONLINE EXPERIMENTS", "text": "This year it is more than ever before."}, {"heading": "6. REFERENCES", "text": "[1] D. Agarwal, B. Chen, and P. Elango. Explore / Exploitscheme for Web Content Optimization. In Proceedings of the Ninth IEEE International Conference on Data Mining, pp. 1-10, 2009. [2] O. Chapelle and L. Li. An empirical evaluation of thomtful sampling. In Proceedings of the 31th International Conference on Machine Processing Systems 24, pp. 2249-2257. 2011. [3] A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex online problems. In Proceedings of the 31th International Conference on Machine Learning, pp. 100-108, 2014. [4] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pp. 661-670, 2010. [5] L. Li."}], "references": [{"title": "Explore/exploit schemes for web content optimization", "author": ["D. Agarwal", "B. Chen", "P. Elango"], "venue": "In Proceedings of the The Ninth IEEE International Conference on Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Toward predicting the outcome of an a/b experiment for search relevance", "author": ["L. Li", "J.Y. Kim", "I. Zitouni"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning from logged implicit exploration data", "author": ["A. Strehl", "J. Langford", "L. Li", "S.M. Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Automatic ad format selection via contextual bandits", "author": ["L. Tang", "R. Rosales", "A. Singh", "D. Agarwal"], "venue": "Proceedings of the 22nd ACM International Conference on Information and Knowledge Management,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Therefore, systems are only learning from partial feedback data from users [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 7, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 0, "context": "Formally, we define by A = {1, 2, \u00b7 \u00b7 \u00b7 ,K}, a set of actions, a contextual vector xt \u2208 X , a reward vector rt = {rt,1, \u00b7 \u00b7 \u00b7 , rt,K}, where each rt,a \u2208 [0, 1] and a policy \u03c0 : X \u2192 A.", "startOffset": 153, "endOffset": 159}, {"referenceID": 1, "context": "Thompson sampling [2] is an effective way to conduct exploitation and exploration through Bayesian posterior sampling, optimizing CTR in long-run.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "With further assumptions, in a recently proposed work [3], the authors proposed such method to tackle the problem of playing subsets of bandit arms.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "cMAB is not only a powerful way to balance exploration and exploitation but also a method to construct unbiased offline evaluation process, suggested by [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "This framework (but not exactly same) stemmed from [7], also discussed in [?, 8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Note that, if p is uniform over all arms, it is essentially uniformly random strategy that has been used frequently in the past, like [4, 2].", "startOffset": 134, "endOffset": 140}, {"referenceID": 1, "context": "Note that, if p is uniform over all arms, it is essentially uniformly random strategy that has been used frequently in the past, like [4, 2].", "startOffset": 134, "endOffset": 140}], "year": 2016, "abstractText": "One of missions for personalization systems and recommender systems is to show content items according to users\u2019 personal interests. In order to achieve such goal, these systems are learning user interests over time and trying to present content items tailoring to user profiles. Recommending items according to users\u2019 preferences has been investigated extensively in the past few years, mainly thanks for the popularity of Netflix competition. In a real setting, users may be attracted by a subset of those items and interact with them, only leaving partial feedbacks to the system to learn in the next cycle, which leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions. Traditionally, unbiased data collection is done through showing items uniformly sampling from the content pool. However, this simple scheme is not feasible as it risks user engagement metrics and it takes long time to gather user feedbacks. In this paper, we introduce a user-friendly unbiased data collection framework, by utilizing methods developed in the exploitation and exploration literature. We discuss how the framework is different from normal multiarmed bandit problems and why such method is needed. We layout a novel Thompson sampling for Bernoulli ranked-list to effectively balance user experiences and data collection. The proposed method is validated from a real bucket test and we show strong results comparing to old algorithms.", "creator": "LaTeX with hyperref package"}}}