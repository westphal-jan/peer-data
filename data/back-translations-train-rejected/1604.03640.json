{"id": "1604.03640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "abstract": "We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 dataset.", "histories": [["v1", "Wed, 13 Apr 2016 02:59:34 GMT  (1036kb,D)", "http://arxiv.org/abs/1604.03640v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["qianli liao", "tomaso poggio"], "accepted": false, "id": "1604.03640"}, "pdf": {"name": "1604.03640.pdf", "metadata": {"source": "CRF", "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "authors": ["Qianli Liao"], "emails": [], "sections": [{"heading": null, "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC award CCF - 1231216.ar Xiv: 160 4."}, {"heading": "1 Introduction", "text": "The recent incarnations of this idea [10] with hundreds of layers show a consistent improvement in performance compared to flatter networks. The 3.57% top-5 errors achieved by residual networks on the ImageNet test are probably comparable to human performance. A notable difference is in the depth [33] that networks of the AlexNetz [16] successfully predict the properties of neurons in the visual cortex, a natural question arises: How similar is an ultra-deep residual network to the primate cortex? A notable difference is that a residual network has as many as 1202 layers. Biological systems seem to have two orders of magnitude fewer if we make the usual assumption that one layer in the NN architecture corresponds to a cortical range."}, {"heading": "2 Equivalence of ResNet and RNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Intuition", "text": "Here we are discussing a very simple observation: a ResNet (ResNet) approaches a specific, standard Recurrent Neural Network (RNN), which implements the discrete dynamic system described byht = K \u0445 (ht \u2212 1) + ht \u2212 1 (1), where ht is the activity of the neural layer at the time and K is a nonlinear operator. Such a dynamic system corresponds to the feedback system of Figure 1 (B). Figure 1 (A) shows that the feedback system, when rolling in (discrete) time, produces a deep residual network with the same (i.e. split) weights between the layers. The number of layers in the unrolled network corresponds to the discrete timelines of the dynamic system. The identity abbreviation mapping, which characterizes the residual learning, appears in the figure. ResNets with divided weights can thus be reformulated in the form of a recursive system. Section 5.2 Resin the Net, we show that most of the weights are divided in the NAx (in most of the weights)."}, {"heading": "2.2 Formulation in terms of Dynamical Systems (and Feedback)", "text": "A neural network (which, for the sake of simplicity, we assume has a single layer of n neurons) can be a dynamic system with dynamics defined asht + 1 = f (ht; wt) + xt (2), where ht-Rn is the activity of n neurons in the layer at the time t and f: Rn \u2192 Rn is a continuous, limited function parameterized by the vector of weights. In a typical neural network f is parameterized by the following relationship between the activity yt of a single neuron and its inputs xt \u2212 1: yt = \u03c3 (< w, xt \u2212 1 > + b), which is a non-linear function such as the linear rectifier."}, {"heading": "3 A Generalized RNN for Multi-stage Fully Recurrent Processing", "text": "As shown in the previous section, the recurrent form of a ResNet is actually flat (if we disregard the possible depth of operator K. In this section, we generalize it into a moderately deep RNN that reflects multi-stage processing in the visual cortex of primates."}, {"heading": "3.1 Multi-state Graph", "text": "Such a hierarchy can be characterized by a directed (cyclic) graph G with vertices V and edges e.g. G = {V, E} (5), in which vertices V are a set containing all processing steps (i.e. we also call them states). Consider the ventral current of the visual cortex, e.g. V = {LGN, V 1, V 2, V 4, IT}. Note that the retina is not listed, as there is no known feedback from the primate cortex to the retina. Edges E are a set containing all connections (i.e. transition functions) between all vertices / states, e.g. V1-V2, V1-V4, V2-IT, etc. An example of such a graph is shown in Figure 2 (A)."}, {"heading": "3.2 Pre-net and Post-net", "text": "We refer to the preprocessor as a \"pre-net,\" as shown in Figure 2 (B). On the other hand, you also need a \"post net\" as a post processor and provide monitoring signals to the recursive system and the pre-net. The pre-net, the recursive system and the post net are trained in an end-to-end way with back propagation. For most models in this paper, unless otherwise specified, we use these terms as a simple 3x3 Convolutionary Layer and the post net as a pipeline of batch normalization, a ReLU, a global average pooling and a fully connected layer (or a 1x1 folding). Take, for example, the primate visual system, the retina is a part of the \"pre-net.\" It receives no feedback from the fully connected layer (or a 1x1 folding that we use interchangeably)."}, {"heading": "3.3 Transition Matrix", "text": "The edge group E can be represented as a 2-D matrix in which each element (i, j) represents the transition function from state i to state j. It is also possible to extend the representation of E to a 3-D matrix in which the third dimension is time and each element (i, j, t) represents the transition function from state i to state j at the time. In this formulation, the transition functions can vary over time (e.g. from time t1 to time t2 are blocked, etc.) The increased expressiveness of this formulation allows us to design a system in which several locally recurring systems are sequentially connected: a downstream recursive system receives input only when its upstream recursive system is finished, similar to recursive convolutionary neural networks (e.g. [19]). This system with undivided weights can also represent exactly the state of the art ResNet (see Figure 3)."}, {"heading": "3.4 Shared vs. Non-shared Weights", "text": "Weight distribution is described at the level of a rolled-out network. Therefore, it is possible to have undivided weights with a 2D transition matrix - even if the transitions are stable over time, their weights may vary over time.In a rolled-out network, a weight distribution can be described as a set of S, the element of which is a set of stocking weights s = {Wi1, j1, t1,..., Wim, jm, tm}, whereby Wim, jm, tm denotes the weight of the transition functions from the state at the time tm. This requires: 1. all weights Wim, jm, tm, tm, s have the same initial values. 2. The actual gradients used for updating each element of s are the sum of the gradients of all elements in s: \u0432 W, (ZW) to be used (ZW), (ZW) where E is the training object. For NR, these weights distribution may be an uncommon but a portion of weights over a frame."}, {"heading": "3.5 Notations: Unrolling Depth vs. Readout Time", "text": "The meaning of \"roll depth\" may vary in different RNN models, because \"roll depth\" of a cyclic graph is not exactly defined. In this paper, we use a biologically plausible definition: We simulate the time after the occurrence of visual stimuli under the assumption that each transition function takes a constant time 1. We use the term \"read time\" to describe the time in which the postal network reads the data from the last state. In principle, this definition allows quantitative comparisons with biological systems. For example, for a model with read time t in this essay, the wall clock time can be estimated at 20t to 50t ms, taking into account the latency of a single layer of biological neurons."}, {"heading": "3.6 Sequential vs. Static Inputs/Outputs", "text": "As an RNN, our model supports sequential data processing and basically all other tasks supported by conventional RNNs. See Figure 5. However, if there is a batch normalization in the model, we must apply the \"time-specific normalization\" described in Section 3.7, which may not be feasible for some tasks."}, {"heading": "3.7 Batch Normalizations for RNNs", "text": "As an additional observation, we found that it is generally detrimental to performance if the normalization statistics (e.g. average, standard deviation, learnable scaling, and shifting of parameters) are divided over time in batch normalization, which may be consistent with the observations in [18]. However, good performance is restored if we apply a procedure we call \"time-specific normalization\": Mean and standard deviation are calculated independently for each t (using a training set). The scaling and shifting parameters that can be learned should be time-specific. However, in most models we do not use the learnable parameters of BN, as they tend not to greatly affect performance. We expect this method to benefit other batch-normalized RNNNNs. However, to use this method, one must have an initial size of t = 0 and enumerate all sorts of ts. This is feasible for visual processing, but requires changes for other tasks."}, {"heading": "4 Related Work", "text": "Deep Recurrent Neural Networks: Our final model is deep and resembles a stacked RNN [27, 5, 7] with several main differences: 1. Our model has feedback transitions between hidden layers and self-transition from each hidden layer to oneself. 2. Our model has identity shortcut mappings inspired by residual learning. 3. Our transition functions are deep and revolutionary. As suggested by [23], the term depth in RNN could also refer to input-to-hidden, hidden-to-hidden or hidden-to-output connections. Our model is deep in all of these senses. See Section 3.2. Recursive Neural Networks and Convolutional Recurrent Neural Networks: When RNN unfolds into a feedback network, the weights of many layers are interconnected, reminiscent of recursive neural networks (recursive neural resistance) first proposed by [29]."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset and training details", "text": "We test all models on the standard dataset CIFAR-10 [15]. All images are 32 x 32 pixels with color. Data expansion is done in the same way as [8]. Momentum was used with the hyperparameter 0.9. All experiments were performed for 60 epochs with lot size 64, unless otherwise specified. Learning rates are 0.01 for the first 40 epochs, 0.001 for epochs 41 to 50, and 0.0001 for the last 10 epochs. All experiments used the cross-entropy loss function and Softmax for classification. Batch normalization (BN) [13] is used for all experiments, but the learnable scaling and shifting parameters are not used (with the exception of the last BN layer in the post-net). Network weights were initialized using the method described in [9]. Although we do not expect the initialization to matter as long as batch normalization is used."}, {"heading": "5.2 Experiment A: ResNet with shared weights", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Sharing Weights Across Time", "text": "We suspect that the effectiveness of ResNet stems mainly from the fact that it efficiently models the recurring calculations required for the detection task. If this is the case, one should be able to reinterpret ResNet as an RNN with weight distribution and achieve a performance comparable to the original version. We show various incarnations of this idea and show that it is indeed the case. We tested the one- and three-stage resnets described in Figure 3 and 4. Results are shown in Figure 6."}, {"heading": "5.2.2 Sharing Weights Across All Convolutional Layers (Less Biologically-plausible)", "text": "For purely technical reasons, the limit of weight distribution could be further shifted by dividing not only over time, but also across states. Here, we show two three-stage renets that use a single set of coil weights across all coil layers and achieve reasonable performance with very few parameters (Figure 7).CIFAR \u2212 10 with Very Few Parameters"}, {"heading": "5.3 Experiment B: Multi-state Fully/Densely Recurrent Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Shared vs. Non-shared Weights", "text": "Although an RNN is usually implemented with common weights over time, it is possible to split the weights and use an independent set of weights at any time. For practical applications, an RNN with non-divided weights should be feasible, similar to the time-specific batch normalization described in 3.7. Results of fully recurrent neural networks with common and non-shared weights are shown in Figure 6."}, {"heading": "5.3.2 The Effect of Readout Time", "text": "In the visual cortex, useful information increases with time from the onset of visual stimuli, suggesting that a relapsing system may have better representational power if more time is available. We have tried to train and test the fully relapsing 2-state network with different readout times (e.g. roll depth, see Section 3.5) and observed similar effects. See Figure 8."}, {"heading": "5.3.3 Larger Models With More States", "text": "We have shown the effectiveness of a fully recursive 2-state network above by comparing it to a 1-state ResNet. Now, we are discussing several observations with respect to 3-state and 4-state networks. First, 3-state models generally seem to be better than 2-state models. This is expected as more parameters are introduced. With a 3-state model with minimal engineering, we were able to achieve a validation error of 7.47% on CIFAR-10.2 \u2212 State Fully Recurrent NN With Different Readout Time tNext. In this case, we were only trying to allow each state transitions to neighboring states and to itself by disabling bypass connections (e.g. V1-V3, V2-IT etc.). In this case, the number of transitions scales linearly as the number of states increases, rather than square. This setting works well with 3-state networks and slightly less well with 4-state networks (perhaps as a result of smaller parameters)."}, {"heading": "5.3.4 Generalization Across Readout Time", "text": "As an RNN, our model supports training and testing with different readout times. Based on our theoretical analysis in Section 2.2, the convergence of representation in the execution of a model with time t \u2192 \u221e is not usually guaranteed. Nevertheless, the model shows a good generalization over time. The results are in Figure 10. As a small detail, the model in this experiment shows only adjacent connections and does not exhibit self-transition, but we do not expect this to affect the conclusion."}, {"heading": "6 Discussion", "text": "In other words, they will be able to put themselves in the position they are in, and they will be able to put themselves in the position they are in. (...) They will be able to put themselves in the position they are in. (...) They will be able to put themselves in the position they are in. (...) They will be able to put themselves in the position they are in. (...) They will be able to put themselves in the position they are in. (...) They will be able to put themselves in the position, to put themselves in the position, to be able to assert themselves. (...)"}, {"heading": "Acknowledgments", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC Award CCF - 1231216."}, {"heading": "A An Illustrative Comparison of a Plain RNN and a ResNet", "text": "B Inhomogeneous, time-invariant ResNet The inhomogeneous, time-invariant version of ResNet is shown in Figure 12.Let K \u2032 = K + I, asymptotically we have: h = Ix \u2212 K \u2032 h (7) (I \u2212 K \u2032) h = x (8) h = (I \u2212 K \u2032) \u2212 1x (9) The power series extension of the above equation corresponds to the standard ResNet # # # # (I \u2212 K \u2032) \u2212 1x = (I + K \u2032 K \u2032 K \u2032 + K \u2032 K \u2032 K \u2032 \"(10) x (10) Inhomogeneous, time-invariant version of ResNet # # # # Recac13 with common weights and links from input to each layer. If the model has only one state, it is experimentally observed that these links are undesirable, that these links add undesirable links to the final representations and degrade performance."}], "references": [{"title": "Modulation of connectivity in visual pathways by attention: cortical interactions evaluated with structural equation modelling and fmri", "author": ["Christian B\u00fcchel", "KJ Friston"], "venue": "Cerebral cortex,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Loopy neural nets: Imitating feedback loops in the human brain. CS231n Report, Stanford, http://cs231n.stanford.edu/reports2016/110_Report.pdf", "author": ["Isaac Caswell", "Chuanqi Shen", "Lisa Wang"], "venue": "Google Scholar time stamp: March", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Understanding deep architectures using a recursive convolutional network", "author": ["David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.1847,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Cortical feedback improves discrimination between figure and background by v1", "author": ["JM Hupe", "AC James", "BR Payne", "SG Lomber", "P Girard", "J Bullier"], "venue": "v2 and v3 neurons. Nature,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Attention modulates contextual influences in the primary visual cortex of alert monkeys", "author": ["Minami Ito", "Charles D Gilbert"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Feedforward, horizontal, and feedback processing in the visual cortex", "author": ["Victor AF Lamme", "Hans Super", "Henk Spekreijse"], "venue": "Current opinion in neurobiology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Batch normalized recurrent neural networks", "author": ["C\u00e9sar Laurent", "Gabriel Pereyra", "Phil\u00e9mon Brakel", "Ying Zhang", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.01378,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Ming Liang", "Xiaolin Hu"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "How important is weight symmetry in backpropagation", "author": ["Qianli Liao", "Joel Z Leibo", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.05067,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["Pedro HO Pinheiro", "Ronan Collobert"], "venue": "arXiv preprint arXiv:1306.2795,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects", "author": ["Rajesh PN Rao", "Dana H Ballard"], "venue": "Nature neuroscience,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M Riesenhuber", "T Poggio"], "venue": "Nature Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Dicarlo. Using goal-driven deep learning models to understand sensory cortex, 2016", "author": ["J.D.D.L.K. Yamins"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Deconvolutional networks", "author": ["Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 dataset. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 60 4. 03 64 0v 1 [ cs .L G ] 1 3 A pr 2 01 6", "creator": "LaTeX with hyperref package"}}}