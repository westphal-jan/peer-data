{"id": "1604.04125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2016", "title": "Filling in the details: Perceiving from low fidelity images", "abstract": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina. In contrast, most deep learning architectures are computationally wasteful in that they consider every part of the input when performing an image processing task. Yet, the human visual system is able to perform visual reasoning despite having only a small fovea of high visual acuity. With this in mind, we wish to understand the extent to which connectionist architectures are able to learn from and reason with low acuity, distorted inputs. Specifically, we train autoencoders to generate full-detail images from low-detail \"foveations\" of those images and then measure their ability to reconstruct the full-detail images from the foveated versions. By varying the type of foveation, we can study how well the architectures can cope with various types of distortion. We find that the autoencoder compensates for lower detail by learning increasingly global feature functions. In many cases, the learnt features are suitable for reconstructing the original full-detail image. For example, we find that the networks accurately perceive color in the periphery, even when 75\\% of the input is achromatic.", "histories": [["v1", "Thu, 14 Apr 2016 12:10:23 GMT  (3517kb,D)", "http://arxiv.org/abs/1604.04125v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["farahnaz ahmed wick", "michael l wick", "marc pomplun"], "accepted": false, "id": "1604.04125"}, "pdf": {"name": "1604.04125.pdf", "metadata": {"source": "CRF", "title": "Filling in the details: Perceiving from low fidelity images", "authors": ["Farahnaz Ahmed Wick", "Michael L. Wick"], "emails": ["fwick@cs.umb.edu", "mwick@cs.umass.edu", "mpomplun@cs.umb.edu", "fwick@cs.umb.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2 Background: Autoencoders", "text": "Autoencoders are a class of unattended algorithms that connect a bottom-up detection network (encoder) to a top-down generative network (decoder). However, the encoder, referred to as function f\u03b8, forms a compressed representation y = f\u03b8 (x) of input x. However, y is the function vector representation or the code calculated from x. As part of our work, we were interested in whether or not we can learn a rich representation of an input image with low fidelity x.The output, referred to as function g\u03b8 \u2032, maps the feature vector back into the input space, which produces a reconstruction z = gasm \u2032 (y) by minimizing a reconstruction error function. Good generalization means reconstruction errors of test examples should be near the reconstruction error for training examples. In order to capture the structure of the underlying data distribution and to prevent the autoencoding function from requiring the identification, we can either learn a dimension or a lower one."}, {"heading": "3 Related work", "text": "This year, the time has come for an agreement to be reached, with only half of it still to be reached."}, {"heading": "4 Framework: Defoveating Autoencoders (DFAE)", "text": "It is not only a matter of time before such a disaster will occur, but also whether such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...) It is a matter of time before such a disaster will occur. (...)"}, {"heading": "4.1 DFAE Architecture and Loss Function", "text": "In our experiments, we investigated DFAEs with fully connected layers. That is, DFAEs of the form: x = \u03c6 (x) (no learnable parameters) (4) h (0) = tanh (W (0) x (5) h (i) = tanh (W (i) h (i \u2212 1))) for i = 1, \u00b7 \u00b7 \u00b7, k \u2212 1 (6) y = \u03c3 (W (k) h (k \u2212 1) (7), where \u03c3 is the logistic function. The sigmoid in the last layer allows us to conveniently compare the pixel intensities between the generated image y and the original image x directly, without having to post-process the output values. We experimented with the number of hidden units per layer as well as the number of layers. For training, we were able to use the traditional mean squared shape (MSE) to determine the error or quadridentropic origin x, but we found that the domain specific image loss ratio of PSE (Pak - signal ratio) was much better."}, {"heading": "4.2 Recurrent DFAEs for Sequences of Foveations", "text": "The above architecture is useful for studying individual foveations, which is the main focus of this work. However, we point out that it is easy to extend DFAEs by recurring connections in order to handle a sequence of foveations similar to what has been done with attention for solving classification tasks [14]. Firstly, to extend the foveation function to include a place where the fovea is centered. Secondly, a saccade function s (ht; Ws) predicts such a place from the current hidden states of the DFAE, and finally, we make the hidden state recurring via a function g (ht \u2212 1, Wg). If we put all this together, the following architecture results: x-t = \u03c6 (xt; Ws) foveate the image at location'ht = fe (g (ht \u2212 1; Wg), x-t; W) encode the new hidden states' s = (Wht)."}, {"heading": "5 Experiments", "text": "Remember that we are interested in the question of whether an artificial neural network can perceive an image from focused input images. In the context of autoencoders, the hidden layers h are responsible for the representation of the focused inputs x. If the network learns a reasonable representation, it should be able to generate a higher resolution output y. We can then measure how similar the output of the network is to the original image to assess how well the network can perceive. In these experiments, we address the architecture of our network to the family described in the previous section and vary the type of foveation, the number of hidden units and the number of layers, and examine the learned characteristics and reconstruction accuracy. We deal with the following questions: 1. Can the network perceive aspects of the image that are not present in the input? What can it perceive and under what conditions? 2. Can the network perceive colors that are not present in the input focus? Does it need to do a lot of 3 to do the input?"}, {"heading": "5.1 Foveation Functions", "text": "In our experiments, we examine several different foveation functions (described in more detail in the relevant sections). In many cases, downampling is used as part of the foveation function for which we use the closest neighbor interpolation algorithm. Interpolation algorithms produce poor quality or block-like images because there is no smoothing function. We selected the closest neighbor to test the worst case scenario by sampling the input factors on our system. Foveation functions include: \u2022 Downsampled factor d (no fovea) because we selected the closest neighbor as the downsampling algorithm."}, {"heading": "5.2 Datasets and pre-processing", "text": "We used two sets of data in our experiments: MNIST and CIFAR100. The MNIST database consists of 28 x 28 handwritten digits and has a training set of 60,000 examples and a test set of 10,000 examples. Each class has 6,000 examples. The training set consists of 32 x 32 color images of 100 classes. Some examples of classes are: flowers, large outdoor scenes, insects, people, vehicles, etc. Each class has 600 examples. The training set consists of 50,000 images and the test set consists of 10,000 images. We trained DFAEs on the MNIST and CIFAR100 datasets (in gray scale and color). We normalized the data sets so that the pixel values are between 0 and 1 and are centered additionally. This step corresponds to local brightness and contrast normalization. Apart from this step, no other pre-processing such as patch extraction or brightening was applied."}, {"heading": "5.3 Baseline: Comparison with standard interpolation functions", "text": "First, to determine the baseline and context for our results, we compare a single-layer DFAE with the usual upsampling algorithms found in image processing software. To measure the quality of the reconstructed images by the interpolation algorithm and the DFAE, we report the mean square error (MSE) between the reconstructed image and the original image. A MSE of zero means that the algorithm or DFAE is able to reconstruct the input with perfect accuracy. Figure 3 shows the MSE of the interpolation algorithms and a DFAE. Unsurprisingly, the closest neighbor performed the worst overall reconstruction, and bilinear interpolation performed best compared to other upsampling algorithms tested. The interpolation algorithms performed poorly when they reconstructed an image sampled beyond factor 2."}, {"heading": "5.4 Feature detectors learnt without any foveations", "text": "This year it is more than ever before."}, {"heading": "5.5 Reconstructing foveated inputs", "text": "In this section, we evaluate DFAEs for foveated inputs, SCT-R, and FOV-R as described in Section 5.1. As described in the introduction, the human visual system is used effectively, and the rationale for scotoma-like regions is desirable to detect or classify images from degraded configurations, which in turn reduces the need for carefully edited and pre-edited datasets during runtime."}, {"heading": "5.6 Reconstructing color from foveated inputs", "text": "It is well known that the human visual system loses its chromatic sensitivity to the periphery of the retina. Recently, there has been interest in how deeply networks, especially Convolutionary Neural Networks (CNNs), can colorize grayscale images [33] and learn an artistic style [34]. Especially in Dahl's [33] reconstructions of grayscale images, numerous cases of colored images produced were damped or sepia-colored. The problem of coloring, which is inherently poorly represented, was treated as a classification task in these studies. Can DFAEs perceive colors when they are missing from the input? We investigated this question using ACH-R and FOVA-R inputs described in Section 5.1. The color regions tested were r = 0% or no color, 6% and 100% or full color. Figure 8a and 8b show examples of color reconstructions of these input types."}, {"heading": "6 Conclusions", "text": "The most important finding in this paper is that current deep architectures are able to learn useful properties of inputs with low accuracy. As in the introduction, the human visual system will use sequential foveations to collect information about its environment. In each foveation, only a fraction of the inputs will be better than existing upsampling algorithms and remarkably, color reconstructions with foveated inputs are as good as complete colored inputs. Usually, these results are reconstructed with a small number of inputs."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The cost of cortical computation", "author": ["Peter Lennie"], "venue": "Current biology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "How much the eye tells the brain", "author": ["Kristin Koch", "Judith McLean", "Ronen Segev", "Michael A Freed", "Michael J Berry", "Vijay Balasubramanian", "Peter Sterling"], "venue": "Current Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Retinal detachment", "author": ["Ronald G Michels", "TA CP Rice"], "venue": "St. Louis: CV Mosby,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Human cortical magnification factor and its relation to visual acuity", "author": ["A Cowey", "ET Rolls"], "venue": "Experimental Brain Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1974}, {"title": "Some characteristics of peripheral visual performance", "author": ["Frank N Low"], "venue": "American Journal of Physiology\u2013 Legacy Content,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1946}, {"title": "Color perception in the intermediate periphery of the visual field", "author": ["Thorsten Hansen", "Lars Pracejus", "Karl R Gegenfurtner"], "venue": "Journal of Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Face recognition by humans: Nineteen results all computer vision researchers should know about", "author": ["Pawan Sinha", "Benjamin Balas", "Yuri Ostrovsky", "Richard Russell"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1962}, {"title": "Recognition memory for a rapid sequence of pictures", "author": ["Mary C Potter", "Ellen I Levy"], "venue": "Journal of experimental psychology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1969}, {"title": "Atoms of recognition in human and computer vision", "author": ["Shimon Ullman", "Liav Assif", "Ethan Fetaya", "Daniel Harari"], "venue": "Proceedings of National Academy of Sciences, page in press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Where to look next? eye movements reduce local uncertainty", "author": ["Laura Walker Renninger", "Preeti Verghese", "James Coughlan"], "venue": "Journal of Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Christopher Poultney", "Sumit Chopra", "Yann L Cun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Pierre Baldi", "Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Nonlinear autoassociation is not equivalent to pca", "author": ["Nathalie Japkowicz", "Stephen Jose Hanson", "Mark Gluck"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Chris M Bishop"], "venue": "Neural computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["Junyuan Xie", "Linli Xu", "Enhong Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Scholkopf. A machine learning approach for non-blind image deconvolution", "author": ["Christian J Schuler", "Harold Christopher Burger", "Stefan Harmeling", "Bernhard"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Natural image denoising with convolutional networks", "author": ["Viren Jain", "Sebastian Seung"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Deep network cascade for image super-resolution", "author": ["Zhen Cui", "Hong Chang", "Shiguang Shan", "Bineng Zhong", "Xilin Chen"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["Chao Dong", "Chen Change Loy", "Kaiming He", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Learning iterative image reconstruction in the neural abstraction pyramid", "author": ["Sven Behnke"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Automatic colorization. http://tinyclouds.org/colorize", "author": ["Ryan Dahl"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A major appeal of deep learning, on which the current dominant approaches for machine vision tasks are based [1], is that they can automatically learn useful feature representations from the data.", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "In contrast, the human visual system has only a small fovea of high resolution chromatic input allowing it to more judiciously budget computational resources [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "The human retina receives 10 million bits per second which exceeds the computational resources available to our visual system to assimilate at any given time [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "To further put this question in perspective, our own fovea takes up only 4% of the entire retina [4] and is solely responsible for sharp central full color vision with maximum acuity.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "The relative visual acuity diminishes rapidly with eccentricity from the fovea [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "As a result, visual performance is best at the fovea and progressively worse towards the periphery [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Indeed, our visual cortex is receiving distorted color-deprived visual input except for the central two degrees of the visual field [7] as seen in Figure 1.", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "For instance, our system can recognize faces and emotions expressed by those faces in resolutions as low as 16 x 16 pixels [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "We can reliably extract contents of a scene from the gist of an image [9] even at low resolutions [10, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 9, "context": "[12] has shown that our visual system is capable of recognizing contents of images from critical feature configurations (called minimal recognizable images or MIRCs) that current deep learning systems cannot utilize for similar tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 11, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 12, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 13, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 12, "context": "Such work is exciting and has lead to improvements in tasks ranging from machine translation [15] to image captioning [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Such work is exciting and has lead to improvements in tasks ranging from machine translation [15] to image captioning [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "Unlike these systems, humans perceive by sequentially directing attention to relevant portions of the data and in turn enables our visual system to reduce computational costs [17, 3].", "startOffset": 175, "endOffset": 182}, {"referenceID": 2, "context": "Unlike these systems, humans perceive by sequentially directing attention to relevant portions of the data and in turn enables our visual system to reduce computational costs [17, 3].", "startOffset": 175, "endOffset": 182}, {"referenceID": 15, "context": "In contrast to traditional or de-noising autoencoders [18], which attempt to reconstruct the original image (or respectively, a salt and pepper corrupted version), our autoencoders", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "To capture the structure of the underlying data distribution and prevent the autoencoder from learning the identity function, we can either require the hidden layer to have lower dimensionality than the input or regularize the weights [19].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "The lower dimension constraint is what the classical autoencoder or PCA does while the higher dimension is used by the sparse autoencoders [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "Recently, denoising autoencoders have been shown to regularize the network by adding noise such as salt-and-pepper (SP) noise to the input, thus forcing the model to learn to predict the true image from its noisy representation [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "As mentioned above, it has been shown that the features learnt by the encoder without any nonlinearity are a subspace of the principal components of the input space [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "However, when a nonlinear activation such as a sigmoid is used in the encoder, an AE can learn more powerful feature detectors than a simple PCA [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Using noisy or jittered inputs to understand feature learning in the framework of AEs or MLPs has been explored before [23, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Using noisy or jittered inputs to understand feature learning in the framework of AEs or MLPs has been explored before [23, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "[19] first proposed training autoencoders with corrupted", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In fact, Bishop [24] has argued that in a linear system training with noise has a similar effect as training with a regularizer, such as an L2 weight decay.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] presented an approach to remove noise from corrupted inputs using sparse coding and deep networks pre-trained with DAEs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Along the same lines, post deblurring denoising [27] and using convolutional neural networks for natural image denoising of patterns such as specks, dirt and rain has been investigated [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Along the same lines, post deblurring denoising [27] and using convolutional neural networks for natural image denoising of patterns such as specks, dirt and rain has been investigated [28].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "[29] used low resolution images interpolated to the size of the output image and AEs in their pipeline to restore resolution of these images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] improves on Cui et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] by using convolutional neural networks and with an end-to-end system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] demonstrated that difficult non-linear image reconstruction from low resolution inputs can be learnt by hiearchical recurrent networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as \u03c6 : [0, 1] \u2192 [0, 1] s.", "startOffset": 180, "endOffset": 186}, {"referenceID": 0, "context": "For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as \u03c6 : [0, 1] \u2192 [0, 1] s.", "startOffset": 189, "endOffset": 195}, {"referenceID": 0, "context": "We then employ the autoencoder to defoveate x\u0302 by generating a high-quality output image y = f(x\u0302;W ) in which, for example, y \u2208 [0, 1].", "startOffset": 129, "endOffset": 135}, {"referenceID": 15, "context": "Thus, as an homage to denoising autoencoders [18], we have termed these models defoveating autoencoders or DFAEs.", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "1] and loss was minimized by stochastic gradient descent with adagrad updates [32].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "However, we remark that it is straightforward to augment DFAEs with recurrent connections to handle a sequence of foveations similar to what has has been done for solving classification tasks with attention [14].", "startOffset": 207, "endOffset": 211}, {"referenceID": 16, "context": "[19], where their denoising autoencoder learnt global structures when it was trained on corrupted inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images [33] and learn artistic style [34].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images [33] and learn artistic style [34].", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "Specifically in Dahl\u2019s [33] reconstructions from grayscale images, numerous cases of the colorized images produced were muted or sepia colored.", "startOffset": 23, "endOffset": 27}], "year": 2016, "abstractText": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina. In contrast, most deep learning architectures are computationally wasteful in that they consider every part of the input when performing an image processing task. Yet, the human visual system is able to perform visual reasoning despite having only a small fovea of high visual acuity. With this in mind, we wish to understand the extent to which connectionist architectures are able to learn from and reason with low acuity, distorted inputs. Specifically, we train autoencoders to generate full-detail images from low-detail \u201cfoveations\u201d of those images and then measure their ability to reconstruct the full-detail images from the foveated versions. By varying the type of foveation, we can study how well the architectures can cope with various types of distortion. We find that the autoencoder compensates for lower detail by learning increasingly global feature functions. In many cases, the learnt features are suitable for reconstructing the original fulldetail image. For example, we find that the networks accurately perceive color in the periphery, even when 75% of the input is achromatic.", "creator": "LaTeX with hyperref package"}}}