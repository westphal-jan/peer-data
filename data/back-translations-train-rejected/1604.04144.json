{"id": "1604.04144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2016", "title": "Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle", "abstract": "Visual representation is crucial for a visual tracking method's performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target's motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.", "histories": [["v1", "Thu, 14 Apr 2016 13:12:07 GMT  (2968kb,D)", "http://arxiv.org/abs/1604.04144v1", "Pattern Recognition (Elsevier), 2015"]], "COMMENTS": "Pattern Recognition (Elsevier), 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jason kuen", "kian ming lim", "chin poo lee"], "accepted": false, "id": "1604.04144"}, "pdf": {"name": "1604.04144.pdf", "metadata": {"source": "CRF", "title": "Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle", "authors": ["Jason Kuen", "Kian Ming Lima", "Chin Poo Lee"], "emails": ["jason7fd@gmail.com", "kmlim@mmu.edu.my", "cplee@mmu.edu.my"], "sections": [{"heading": null, "text": "These descriptors were developed without taking tracking-specific information into account. In this paper, we propose to learn complex evaluated invariant representations from tracked sequential image fields, strong time constraints, and stacked revolutionary autoencoders; the deep slow local representations are learned offline from unlabeled data and transferred to the observation model of our proposed tracker; the proposed observation model retains old training samples to mitigate drift, and collects negative samples that match the movement pattern of the target for better discriminatory tracking; with the learned presentation and online training samples, a logistic regression classifier is used to distinguish target from background, and re-trained online to adapt to changes in appearance; and the observation model is then integrated into a particle filter framework to allow visual tracking; and experimental results on challenging benchmarks show that proposed follow-up forms are favorable:"}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Related work", "text": "In fact, most of them are able to survive on their own, without having to orient themselves in a different direction."}, {"heading": "3. Learning deep invariant representations using temporal slowness", "text": "In this paper, we aim to exploit the temporal slowness to learn generic invariant representations in an unattended manner and to use it for visual tracking. Temporal slowness is one of the most important priorities for representation learning [14] and it has been successfully used in object recognition tasks (e.g. Mobahi et al. [21]; Zou et al. [22]. The main motivation for learning such representation is that it is difficult to develop or create an exact feature extraction algorithm by hand that is robust against object transformations found in videos. To the best of knowledge, this paper is the first attempt to use the principle of temporal slowness for visual tracking. Zou et al. [22] showed that temporal slowness limitations can simply be added to the conventional cost function of the autoencoder in order to learn invariant features, we also choose the autoencoding that will be our autocoding model to pre-learn the biographical nature of the representation."}, {"heading": "3.1. Autoencoder with temporal slowness constraint", "text": "An autoencoder with mere reconstruction costs can hardly learn a meaningful representation of the data that would allow a better reconstruction of the input data. (...) An autoencoder is the only way in which we are able to reconstruct the data. (...) An autoencoder is the only function in which we have a square error function. (...) An autoencoder function is the activation function, \u03b2 is the network biase, and subscriptions e and d point to the associations of the components with the encoder or decoder. (Wex) + \u00dfe learned in the hidden layer of an autoencoder is considered useful for classification purposes. Conventional autoencoders with mere reconstruction costs can hardly learn a useful representation of the data."}, {"heading": "3.2. Stacked convolutional autoencoders", "text": "In this essay, we stack and train a second autoencoder (the so-called second layer) on the nested features of the first autoencoder (known as the first layer) in greedy layer-by-layer training. [31] The Convolutionary Learning Architecture enables the reuse of features of the first layer to learn higher features from larger image fields. Prior to training the second layer, the features of the first layer are extracted from larger d2-dimensionally tracked image fields (where d2 > d1) with a predefined spatial increment or increment k1, where k1 \u2265 1. Experimentally, we choose k1 in such a way that the overlap between the dense image fields is small. A small k1 is sophisticated in computing terms, while a large k1 without overlap impairs performance."}, {"heading": "3.3. Feature extraction and visualization", "text": "As explained in Section 3.1, there are two types of information that can be obtained from the sub-ranges of summarized units of the proposed autoencoder: the amplitude or magnitude of the complex representation A + iB indicates the degree of presence of the feature while it reacts invariably to its phase changes and transformations; it is calculated as the Euclidean norm of the complex number: \u221a A2 + B2 (4), which is exactly what is equivalent. (3) The amplitudes are a good invariant representation for its phase changes and transformations; the second information obtained from the sub-ranges is the phase. Phase of the complex representation A + iB is defined as tan \u2212 1 (BA) with temporary slowness."}, {"heading": "3.4. Sequential training data collection", "text": "In fact, most of them are able to go in search of a new home, and they are able to go in search of a new home in which to find themselves."}, {"heading": "4. Adaptive observational model", "text": "The observation model we use in this paper is both discriminatory and adaptive. It is discriminatory in the sense that it uses a monitored binary classifier to divide the tracking observations into positive (target) class and negative (background) class. In addition, we regularly train the classifier with new training samples to adapt the observation model to changes in the appearance of the target and the background over time. Observations are presented on the basis of representations we have learned from the previously introduced stacked autoencoders."}, {"heading": "4.1. Online training samples collection", "text": "In the following frames, each predicted target observation is added to the sample, replacing the oldest positive sample of the observed fact that it is likely to be a positive training target. In this case, a number of positive samples are collected from regions only a few pixels away from the target. Let (xt, yt) denotes the coordinate of the target on horizontal and vertical axes or in a positive target. In the first frame, in which t = 1, a positive sample is collected at the location (x1 +, y 1 +): j 1 + DU (\u2212 v, v), as well as negative samples on horizontal and vertical axes (6), in which v \u00b2 Z determines the maximum pixel translation on one of the axes, DU (\u2212 v, v) is a random integer number sampled from a discrete uniform distribution. In the following frames, in which t > 1, each predicted target observation is added to the sample."}, {"heading": "4.2. Feature extraction for visual tracking", "text": "After being trained offline in Section 3, the stacked autoencoders are transferred for use in visual tracking. Both the first and second layers of stacked autoencoders are used to extract dense local characteristics (amplitude and phase characteristics) from tracking observations and training specimens. As in Section 3.2, all tracking observations are extracted densely from tracking observations with k1 steps and transferred to the first layer to obtain first layer characteristics. The image size of 32 x 32 is a good balance between computing power and image details. As in Section 3.2, local d1-dimensional spots are densely extracted from tracking observations with k1 steps and transferred to the first layer to obtain first layer characteristics. Subsequently, characteristics of the second layer are densely extracted from the first layer with a step of k2. Finally, the intertwined representations from both layers are strongly extracted to form a final representation for visual tracking fallacy 41. [Due to the relatively large use of spatial pyramid 41]"}, {"heading": "4.3. Supervised binary classification", "text": "\"For the first time in my life, I have been able to do this for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the second time. I have been able to do it for the first time."}, {"heading": "5. Proposed tracker", "text": "Before visual tracking can be carried out, the adaptive observation model in Section 4 should be integrated into a method for estimating the object state. Particle filters [42] [43] are preferred to other methods (e.g. Kalman filters) due to their non-linearity, non-Gaussian assumption and ability to sustain multiple hypotheses. In the last part of this section, we compare our proposed tracking method to the tested modern imaging learning trackers."}, {"heading": "5.1. Particle filter", "text": "In fact, the fact is that most of us will be able to be in a position to be able to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to move, to be able to fight, to be able to fight, to be able to fight, to be able to fight, to be able to fight, to be able to fight, to be able to be able to move, to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to move to be able to be able to be able to be able to be able to move to be able to be able to be able to be able to be able to be able to be able to be able to move to be able to be able to be able to be able to be able to be able to be able to be able to be able to move to be able to be able to be able to be able to be able to be able to be able to be able to be able to"}, {"heading": "5.2. Comparison with other representation learning trackers", "text": "In fact, it is a very successful solution, which will be able to find a solution that is also able to find a solution."}, {"heading": "6. Experiments", "text": "In this section we describe the implementation details and parameter settings of daylight saving time, together with the setup for the tracking experiments. Daylight saving time is tested on several challenging sequences against 7 state-of-the-art trackers. Afterwards, we present the results of the experiments in quantitative and qualitative means."}, {"heading": "6.1. Implementation details", "text": "In this section we provide the parameter settings for the parameters described in the previous section. All the parameter settings of the proposed method are collected empirically. One of the most important aspects of the DST is the offline training of stacked autoencoders (Section 3). In terms of sequential training data sets, 8 x 8 tracked patches are used for the training of the first layer and the second layer on 14 x 14 tracked patches. The number of trial sessions NT is 15000 and the number of frames per track NF is 5. Next, the weights of the first layer and the second layer are applied to the cost function."}, {"heading": "6.2. Experimental setups", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "6.3. Quantitative evaluation", "text": "In this subsection, the trackers are evaluated quantitatively with respect to success rates (SR) and center-oflocation (COL) errors. Starting from a Ground Truth Boundingbox RG and a tracking result RT, SR is calculated as Area (RT-RG) Area (RT-RG) > 0.5. The COL error is obtained by calculating the Euclidean distance in pixels between the center of RG and RT. Since all evaluated trackers perform random samples, we run the trackers five times for each sequence and obtain the median results. The median result is obtained by adding max-min normalized SR and inverse COL errors from the five experiments. The average SRand COL errors are shown in Table 1 and 2, respectively."}, {"heading": "6.4. Qualitative evaluation", "text": "This year, it has come to the point where it can only take one year to move on to the next round."}, {"heading": "6.5. Discussions", "text": "The main contribution in this paper is to take an alternative approach to visual tracking learning characteristics. We get to know slow, invariant features offline via stacked autocoders and transfer them for online visual clocking. To this end, we isolate the merits of our projected image description by replacing them with dense SIFT, HOG and LBP local descriptors. All other components of our projected trackers remain the same. To obtain the handmade descriptors for tracking, we use a reputable educational package known as VLFeat. For SIFT, we use the same spatial steps as our first layer and find the best width (in pixels) of the SIFT spatial representation, from some possible values close to the input size of the first layer."}, {"heading": "7. Conclusion", "text": "This paper uses the principle of time slowness to learn invariant representations for visual tracking. Time slowness limitations are built into an autoencoder algorithm to facilitate imaging learning. In order for the learned representations to be specific to visual tracking tasks, a large number of tracked image fields are collected via an existing tracker to form the training set. A deep learning model is created by stacking the autoencoders to learn higher-value inventories with time slowness. We then transfer the offline learned representations to an observation model for online visual tracking. The adaptive observation model is formulated to collect more relevant negative samples and use accumulative training to alleviate tracking drift. Tracking is performed in a particle filter frame to assess the target state sequentially. Compared to several state-of-the-art trackers, the proposed tracker shows favorable follow-up performances in various benchmarks."}], "references": [{"title": "A Survey of Appearance Models in Visual Object Tracking", "author": ["X. Li", "W. Hu", "C. Shen", "Z. Zhang", "A. Dick", "A.V.D. Hengel"], "venue": "ACM Transactions on Intelligent Systems and Technology 4 (4) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "in: Proceedings of International Conference on Machine Learning, 759\u2013766", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "International Journal of Computer Vision 77 (1-3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient Online Subspace Learning With an Indefinite Kernel for Visual Tracking and Recognition", "author": ["S. Liwicki", "S. Zafeiriou", "G. Tzimiropoulos", "M. Pantic"], "venue": "IEEE Transactions on Neural Networks and Learning Systems 23 (10) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Correlation-based incremental visual tracking", "author": ["M. Kim"], "venue": "Pattern Recognition 45 (3) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust online appearance models for visual tracking", "author": ["A.D. Jepson", "D.J. Fleet", "T.F. El-Maraghi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (10) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive object tracking based on an effective appearance filter", "author": ["H. Wang", "D. Suter", "K. Schindler", "C. Shen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 29 (9) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Online selection of discriminative tracking features", "author": ["R.T. Collins", "Y. Liu", "M. Leordeanu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (10) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-Time Tracking via On-line Boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "in: Proceedings of the British Machine Vision Conference, vol. 1, BMVA Press, 47\u201356", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Boosting scalable gradient features for adaptive real-time tracking", "author": ["D.A. Klein", "A.B. Cremers"], "venue": "in: Proceedings of IEEE International Conference on Robotics and Automation, IEEE, 4411\u20134416", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking via online weighted multiple instance learning", "author": ["K. Zhang", "H. Song"], "venue": "Pattern Recognition 46 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Co-tracking using semisupervised support vector machines", "author": ["F. Tang", "S. Brennan", "Q. Zhao", "H. Tao"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, IEEE, 1\u20138", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H. Torr"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, IEEE, 263\u2013270", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (8) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "in: NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 1713\u20131720", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Transferring visual prior for online object tracking", "author": ["Q. Wang", "F. Chen", "J. Yang", "W. Xu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 21 (7) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Tracking with deep neural networks", "author": ["J. Jonghoon", "A. Dundar", "J. Bates", "C. Farabet", "E. Culurciello"], "venue": "in: Proceedings of Conference on Information Sciences and Systems, 1\u20135", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a Deep Compact Image Representation for Visual Tracking", "author": ["N. Wang", "D.-Y. Yeung"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 809\u2013817", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "in: Proceedings of International Conference on Machine Learning, 737\u2013744", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W. Zou", "A. Ng", "S. Zhu", "K. Yu"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 3212\u20133220", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "in: Proceedings of Iinternational Conference on Machine Learning, 1096\u20131103", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "in: Proceedings of International Conference on Machine Learning, 833\u2013840", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, 17  IEEE, 3361\u20133368", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 1017\u20131025", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in: Proceedings of International Conference on Artificial Intelligence and Statistics, vol. 15, 315\u2013323", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Slow", "author": ["Y. Bengio", "J.S. Bergstra"], "venue": "Decorrelated Features for Pretraining Complex Cell-like Networks, in: Proceedings of Conference on Neural Information Processing Systems, 99\u2013107", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Bilinear models of natural images", "author": ["B.A. Olshausen", "C. Cadieu", "J. Culpepper", "D.K. Warland"], "venue": "in: Proceedings of SPIE, vol. 6492, 649206\u2013649206\u201310", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "in: Proceedings of International Conference on Machine Learning, 265\u2013272", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research 10 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "in: Proceedings of International Conference on Artificial Intelligence and Statistics, 215\u2013223", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of visual invariance with temporal coherence", "author": ["W.Y. Zou", "A.Y. Ng", "K. Yu"], "venue": "in: NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing higher-layer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Tech. Rep. 1341, University of Montreal", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2411\u20132418", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive real-time video-tracking for arbitrary objects", "author": ["D.A. Klein", "D. Schulz", "S. Frintrop", "A.B. Cremers"], "venue": "in: Proceedings of International Conference on Intelligent Robots and Systems, IEEE, 772\u2013777", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 1822\u20131829", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "On the analysis of accumulative difference pictures from image sequences of real world scenes", "author": ["R. Jain", "H.-H. Nagel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 1 (2) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1979}, {"title": "On Space-Time Interest Points", "author": ["I. Laptev"], "venue": "International Journal of Computer Vision 64 (2-3) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2169\u20132178", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Human tracking using convolutional neural networks", "author": ["J. Fan", "W. Xu", "Y. Wu", "Y. Gong"], "venue": "IEEE Transactions on Neural Networks 21 (10) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A.F.M. Smith"], "venue": "IEE Proceedings F Radar and Signal Processing 140 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1993}, {"title": "A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking", "author": ["M. Arulampalam", "S. Maskell", "N. Gordon", "T. Clapp"], "venue": "IEEE Transactions on Signal Processing 50 (2) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2002}, {"title": "Superpixel tracking", "author": ["S. Wang", "H. Lu", "F. Yang", "M.-H. Yang"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, ISSN 1550-5499, 1323\u20131330", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 983\u2013990", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "in: Proceedings of European Conference on Computer Vi-  sion, Springer, 864\u2013877", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-Time Object Tracking Via Online Discriminative Feature Selection", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 22 (12) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Object tracking via partial least squares analysis", "author": ["Q. Wang", "F. Chen", "W. Xu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 21 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Online object tracking with sparse prototypes", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 22 (1) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Tracking-Learning- Detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (7) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "in: Proceedings of the International Conference on Multimedia, ACM, 1469\u20131472", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "A typical visual tracking method is dependent on its two major components [1], namely dynamic model (motion estimation) and observational model.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "The stacked autoencoders are then transferred for use in visual tracking, based on self-taught learning paradigm [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "[3] used Principal Component Analysis (PCA) to construct and incrementally update a subspace model of the target object.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] formulated an incremental kernel PCA in Krein space to learn a nonlinear subspace representation for tracking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To account for partial occlusion during tracking, Kim [5] proposed a Canonical Correlation Analysis (CCA)-based tracker which considers the correlations among sub-patches of tracking observations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "[6] learned a mixture model to model appearance changes of target object via an online Expectation-Maximisation (EM) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] develop an adaptive observational model in the joint spatial-color space using Gaussian mixture model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] selected color features online which best discriminates target object from current background.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed an online boosting classifier that adaptively selects discriminative features for tracking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Klein and Cremers [10] introduced a novel scaleinvariant gradient feature and used boosting to track target object efficiently.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "To alleviate visual drift, Zhang and Song [11] proposed a tracking method based on online multiple instance boosting that handles ambiguously labeled samples and weights each positive sample differently for update.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "[12] trained multiple SVMs, each on an independent feature and locates the target object by combining confidence scores from all SVM classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] presented a structured output SVM to directly predict the trajectory of the target object between frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Representation learning [14] is an emerging field aims to learn good representations from raw input or low-level representations.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "[15]; Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16]) have proven to be superior to conventional hand-engineered representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] performed sparse coding on SIFT features extracted from labeled object recognition datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] trained a convolutional neural network offline in an unsupervised manner for tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "To account for object appearance changes, Wang and Yeung [20] pre-trained a stacked denoising autoencoders and finetuned the deep neural network online during tracking.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Temporal slowness is one of the major priors for representation learning [14] and it has been successfully used in object recognition tasks (e.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "[21]; Zou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] showed that temporal slowness constraint can be simply added to conventional autoencoder cost function to learn invariant features, we also choose autoencoder to be our representation learning model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "To allow autoencoder to discover better representations, many autoencoder regularization schemes such as denoising autoencoders [23] and contractive autoencoders [24] have been proposed.", "startOffset": 128, "endOffset": 132}, {"referenceID": 22, "context": "To allow autoencoder to discover better representations, many autoencoder regularization schemes such as denoising autoencoders [23] and contractive autoencoders [24] have been proposed.", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": ", Wang and Yeung [20]), we contend that image representation for visual tracking should be learned in a way more specific to the task.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "Our autoencoder model draws inspiration from Independent Subspace Analysis (ISA) [25] which was proposed for learning motion invariance.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "In the proposed autoencoder, the reconstruction cost replaces hard orthonormality constraint in ISA to prevent feature degeneracy [26] and the sparsity cost helps to discover interesting features [27] (e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "In the proposed autoencoder, the reconstruction cost replaces hard orthonormality constraint in ISA to prevent feature degeneracy [26] and the sparsity cost helps to discover interesting features [27] (e.", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "the second cost term (temporal slowness constraint), as in [21] and [22], we minimize the temporal representation differences in L-norm to allow invariance to be sparsely represented, that is a kind of motion invariance is represented by only a small number of features and thus they become specialized for different invariances.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "the second cost term (temporal slowness constraint), as in [21] and [22], we minimize the temporal representation differences in L-norm to allow invariance to be sparsely represented, that is a kind of motion invariance is represented by only a small number of features and thus they become specialized for different invariances.", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": ", Bengio and Bergstra [28]; Zou et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "[22]) to group similar features in each of the pooled units, therefore achieving invariance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "It pairs every 2 adjacent hidden units to form a complexvalued representation and each pair can be decomposed into amplitude (degree of presence of the features) and phase (transformations of the features over time) variables [29].", "startOffset": 226, "endOffset": 230}, {"referenceID": 28, "context": "(2) can be optimized efficiently using any of the unconstrained optimization methods [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "In this paper, we stack and train a second autoencoder (known as the second layer) on the convolved features of the first autoencoder (known as the first layer) in a greedy layer-wise training fashion [31].", "startOffset": 201, "endOffset": 205}, {"referenceID": 30, "context": "Unlike in object recognition tasks whereby a small k1 is always recommended to achieve good performances [32], there is a great need to balance the feature and run-time performances in visual tracking.", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29]; Zou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]), to visualize what transformations the features are invariant to.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] and use linear combination of filters [34] to visualize the optimal stimuli for the features and invariances learned in the stacked autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] and use linear combination of filters [34] to visualize the optimal stimuli for the features and invariances learned in the stacked autoencoders.", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "In self-taught learning [2], features are learned from unlabeled data and transferred for use in supervised learning tasks, whereby the generating distribution of the unlabeled data is different from the labeled data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": "This setting is analogous to the tracking algorithm [17] which exploits patch-level similarity and transfers visual prior from unlabeled dataset to tracking tasks.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "The deep learning-based tracking algorithm [20] which transfer features learned from unlabeled datasets, employ a very large dataset with great amount of visual diversity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "[35] and Klein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36], in which the objects and scenes are diverse in terms of their appearances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37], tracked image patches are collected randomly from the sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The first approach is about identifying motional pixels via binary-thresholded accumulative difference pictures [38], and a size filter is used to remove trivially small connected components among the identified pixels.", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "contrast, the second approach involves a space-time Harris interest point detection algorithm [39] (referred to as STIP) that identifies regions which are \u2018interesting\u2019 spatially and temporally.", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "[22], we use a larger number of track sessions to encourage diversity in the training dataset, and use a smaller number of frames per track session to minimize the occurences of tracking drift in the dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Due to the use of relatively large convolution strides, spatial pyramid pooling [40] is not performed on the convolved representations.", "startOffset": 80, "endOffset": 84}, {"referenceID": 39, "context": "Doing so would greatly encourage translational invariance, which is not favourable in visual tracking [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "For this, particle filter [42][43] is chosen over other methods (e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "For this, particle filter [42][43] is chosen over other methods (e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 152, "endOffset": 155}, {"referenceID": 18, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 207, "endOffset": 211}, {"referenceID": 17, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 229, "endOffset": 233}, {"referenceID": 16, "context": "All of the trackers use raw image patches to learn representations from, except [17] which extracts SIFT features from the patches as bases to build a sparse coded dictionary.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "In terms of observational models adaptivity, [19] is the only tracker that uses a non-adaptive offline classifier to distinguish between target object and object.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "[17] and [18] including DST employ linear classifiers which are independent of representation learning models, to build adaptive observational models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Wang and Yeung [20] uses a more sophisticated way for classification during tracking, by fine-tuning the deep neural network (unrolled from pre-trained stacked autoencoders) using classification error cost function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "(2) are set as [100, 20] and [300, 20] for first and second layer respectively.", "startOffset": 15, "endOffset": 24}, {"referenceID": 18, "context": "(2) are set as [100, 20] and [300, 20] for first and second layer respectively.", "startOffset": 29, "endOffset": 38}, {"referenceID": 28, "context": "For unconstrained optimization of the autoencoders, we employ off-the-shelf Limited-memory BroydenFletcher-Goldfarb-Shanno (BFGS) algorithm that is relatively memory-efficient and fast-converging [30].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "[35], Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44], and Babenko et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 190, "endOffset": 193}, {"referenceID": 45, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 250, "endOffset": 254}, {"referenceID": 46, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 292, "endOffset": 296}, {"referenceID": 47, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 330, "endOffset": 334}, {"referenceID": 48, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 374, "endOffset": 378}, {"referenceID": 49, "context": "To obtain the hand-crafted descriptors for tracking, we use a reputable computer vision library package known as VLFeat [51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 46, "context": "PLS draws negative samples in an annular region [48] defined by inner and outer radius, where the inner radius is the radius of circle minimally enclosing the target and outer radius is a fixed parameter.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "In future work, we will explore the possibility of online learning of representations using temporal slowness for visual tracking, without relying on selftaught learning paradigm [2].", "startOffset": 179, "endOffset": 182}], "year": 2016, "abstractText": "Visual representation is crucial for a visual tracking method\u2019s performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target\u2019s motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.", "creator": "LaTeX with hyperref package"}}}