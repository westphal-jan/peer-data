{"id": "1604.04383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2016", "title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding", "abstract": "Most current very low bit rate (VLBR) speech coding systems use hidden Markov model (HMM) based speech recognition/synthesis techniques. This allows transmission of information (such as phonemes) segment by segment that decreases the bit rate. However, the encoder based on a phoneme speech recognition may create bursts of segmental errors. Segmental errors are further propagated to optional suprasegmental (such as syllable) information coding. Together with the errors of voicing detection in pitch parametrization, HMM-based speech coding creates speech discontinuities and unnatural speech sound artefacts.", "histories": [["v1", "Fri, 15 Apr 2016 07:35:00 GMT  (242kb,D)", "http://arxiv.org/abs/1604.04383v1", null], ["v2", "Mon, 11 Jul 2016 20:38:29 GMT  (863kb,D)", "http://arxiv.org/abs/1604.04383v2", null], ["v3", "Mon, 29 Aug 2016 07:56:01 GMT  (1002kb,D)", "http://arxiv.org/abs/1604.04383v3", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["milos cernak", "alexandros lazaridis", "afsaneh asaei", "philip n garner"], "accepted": false, "id": "1604.04383"}, "pdf": {"name": "1604.04383.pdf", "metadata": {"source": "CRF", "title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding", "authors": ["Milos Cernak", "Alexandros Lazaridis", "Afsaneh Asaei", "Philip N. Garner"], "emails": [], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the city."}, {"heading": "II. NEURAL NETWORKS FOR SPEECH CODING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Background", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves in the lead."}, {"heading": "B. Coding of phonetic and phonological information", "text": "The encoding of segment information begins with the analysis by converting a segment of speech samples into a sequence of acoustic characteristics (X = {~ x1,.., ~ xn,.., where N denotes the number of segments in the utterance. Conventional receiver coefficients can be used as acoustic characteristics. Encoding can be done at the phonetic or phonological (subphonetic) level. In the former case, the acoustic characteristic observation sequence X is converted into a parameter sequence ~ zn = [z1n,.,., z vec n,.,., p.,., where the n-th frame consists of posterior probabilities zpn = p (cp | xn) of P classes (phonemes), and. > stands for the transpose operative operator."}, {"heading": "C. Coding of prosodic information", "text": "In fact, it is that we are able to hide ourselves, and that we are able to assert ourselves, that we are able, that we are able, that we are able, that we are able, that we are able, \"he said."}, {"heading": "D. Transmission scheme", "text": "Segmental characteristics - phonological side lines - have values that are usually very close to 1 or 0, and these binary patterns allow a very efficient use of 1-bit quantization: the probabilities above 0.5 are normalized to 1 and the probabilities below 0.5 must be set to zero. Figure 1 illustrates a demonstration example of the transmission scheme. Figure 1a shows the speech signal. Figure 1b shows binary values of the three basic resonance phonological prime numbers of the GP system, commonly referred to as A, U, I, and designates the peripheral vowel qualities [a], [u], and [i] respectively. Other vowels are defined by a composition of the basic parameters; for example, [e] is derived from the fusion of the I and A primes. In addition to these \"vocal\" prime numbers, GP also suggests the \"vocal\" qualities [a], [u], and [i]. Other vowels are defined by the composition of the basic A and the composition of the primary 1."}, {"heading": "III. OPEN-SOURCE EXPERIMENTAL FRAMEWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Composition of neural networks", "text": "Figure 2 shows the design of the NN codec; the encoder shown in Figure 2a is based on a bank of DNNs that perform segmental speech analysis of conventional acoustic characteristics, 4 and a parallel spiking NN that detects the syllable boundaries of the continuous F0 signal; the decoder shown in Figure 2b is based on a DNN that synthesizes speech cepstral parameters from transferred segmental and prosodic information; the results of segmental speech analysis are phonological posteriors in which all the unique patterns of the training data generate the segmental code book; the number of unique binary patterns, the size of the segmental code book, is a small fraction of the total allowed pattern (for example, for the phonological system eSPE, it is about 0.5%); the binary patterns are often repeated frame by frame; the segmental code thus consists of an index of the code book together with the duration of the code book."}, {"heading": "B. Data", "text": "The DNN encoder is trained on the 284 sentences of the Wall Street Journal (WSJ0 and WSJ1), and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1-100). The DNN decoder is trained on the Nancy database of Blizzard Challenge4. Known as \"Nancy,\" the speaker is a US native speaker. The database consisted of 16.6 hours of high-quality recordings of natural, expressive human language recorded in an echo chamber with a sampling rate of 96 kHz in 2007 and 2008. The database comprised about 12k of utterances, and the following breakdown was used: \u2022 the training kit, utterances from 1 to 10k, \u2022 the cross-validation kit, utterances from 10k to 11k, \u2022 the test kit, the remaining 1095 sentences. The text was processed by a conventional and freely available system used in the training for the TTS data."}, {"heading": "C. Training", "text": "In fact, it is such that it is a matter of a way in which people move in the most diverse areas of life, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "IV. EVALUATION", "text": "In this section, we present an evaluation of the phonetic and phonological NN speech encoder VLBR and a comparison with the HMM-based base system. We encode and decode 1095 expressions of the Nancy database test set. In the following sections, we present results focusing on: 1) Phonetic NN speech encoders, comparison of phonetic and phonological encoding in Section IV-A. 2) Phonological NN speech encoders, quantification of the influence of different phonological schemes and image shifts on speech quality and transmission rates in Section IV-B. 3) HMM vesus NN speech encoders, comparison of VLBR-HMM-based and NN speech encoding in Section IV-C."}, {"heading": "A. Phonetic NN speech coder", "text": "The segmental information of the NN speech encoder can consist of either phonetic or phonological posteriors (see section II-B), which are transmitted frame by frame. Therefore, we started by comparing the speech quality of the phonetic and phonological speech encoding. To measure the effects of the phonetic and phonological posteriors, the F0 encoding was bypassed with the original F0 signals. To achieve VLBR, binary posteriors must be normalized to 1, so we were interested in which binary phonetic or phonological posteriors perform better. We normalized the phonetic and phonological posteriors to the binary values (the probabilities above 0.5 are normalized to 1 and the probabilities are pushed to zero) and used phonological distortion (MCD) between the original and encoded speech samples as an objective metric to compare the general speech quality."}, {"heading": "B. Phonological NN speech coder", "text": "We have used the binary phonological features of this paper and created a codebook of unique patterns for each phonological scheme. Linear 3-bit quantified syllable-based F0 parameterization is used in this experiment. 3-bit quantization degrades speech quality by only 0.1 dB rates that can be achieved by using a lower phonological scheme. Figure 3 shows a linear dependence of codebook size on the number of phonological classes. Increasing the fragment shift reduces the number of unique phonological patterns. We speculate that the number of unique phonological imitators is related to the number of clustered senders."}, {"heading": "C. HMM versus NN coding", "text": "Finally, we were interested in a qualitative comparison of the VLBR HMM and NN speech encoders. We looked at a 5-point scale from ABX to a strong preference for A, B or both samples. Material for the test consisted of 20 pairs of sentences, so one member of the pair used the GPbased 16 ms frame shift (System A) and the other member generated using the HMM-based voice encoding (System B). Random expressions from the Nancy database were used to generate the encrypted language."}, {"heading": "V. BIT ALLOCATION", "text": "The transmission code contained the index of the binary pattern along with its duration, the segment code, and two indexes of the quantified middle and syllable formation F0 code books together with the syllable duration. Both segment and suprasegment information are transmitted asynchronously, i.e. the segment blocks and syllables have different start, end and duration. As an example of bit allocation, Table V shows the details for the GP system used in the ABX test. Closer analysis of the repeated pattern of the segment code shows that the number of blocks is less than 46% of the total number of frames and 2 bits to determine the number of codes to be transmitted."}, {"heading": "VI. CONCLUSIONS", "text": "VLBR speech encoding based on a recognition / synthesis paradigm 63 is either corpus-based (with unit selection approach) or HMM-based (with HMM-based ASR or TTS).We have designed and presented an NN-based speech encoding consisting of deep NNs and spiking NNNs; the solution is an end-to-end neural network based on VLBR speech encoding. We have compared phonetic and phonological NN encoding; given the binary nature of phonological posteriors, they outperform binary phonetic posteriors. In addition, we have compared three different phonological systems, and we have come to the conclusion that an optimal NN speech encoding can be developed by using the phonological posteriors defined by the phonology classes. By selecting an image shift of 16, the coder works with a latency of 150 NN-3ms."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was carried out with the support of the Swiss NSF under funding number CRSII2 141903: Spoken Interaction with Interpretation in Switzerland (SIWIS) and SP2: the SCOPES Project on Speech Prosody. Partly, the work was also supported within the framework of the RECOD project by armasuisse, the procurement and technology centre of the Federal Department of Defence, Civil Protection and Sport. Afsaneh Asaei was supported by the SNF project \"Parsimonious Hierarchical Automatic Speech Recognition (PHASER)\" under funding number 200021-153507."}], "references": [{"title": "A very low bit rate speech coder based on a recognition/synthesis paradigm,", "author": ["K.-S. Lee", "R. Cox"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Corpus based very low bit rate speech coding,", "author": ["G.V. Baudoin", "F. El Chami"], "venue": "in Proc. of ICASSP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A phonetic vocoder,", "author": ["J. Picone", "G.R. Doddington"], "venue": "Proc. of ICASSP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "A very low bit rate speech coder using HMM-based speech recognition/synthesis techniques,", "author": ["K. Tokuda", "T. Masuko", "J. Hiroi", "T. Kobayashi", "T. Kitamura"], "venue": "in Proc. of ICASSP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Multisensor very lowbit rate speech coding using segment quantization,", "author": ["A. McCree", "K. Brady", "T.F. Quatieri"], "venue": "in Proc. of ICASSP", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Linguistic dissection of switchboardcorpus automatic speech recognition systems,", "author": ["G. Greenberg", "S. Chang"], "venue": "Proc. of ITRW on Automatic Speech Recognition: Challenges for the new Millenium, Paris, France,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Speech vocoding for laboratory phonology,", "author": ["M. Cernak", "S. Benus", "A. Lazaridis"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "The elements of phonological representation", "author": ["J. Harris", "G. Lindsey"], "venue": "Harlow, Essex: Longman,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Boosting attribute and phone estimation accuracies with deep neural networks for detectionbased speech recognition,", "author": ["D. Yu", "S. Siniscalchi", "L. Deng", "C.-H. Lee"], "venue": "in Proc. of ICASSP. IEEE SPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data,", "author": ["S.M. Siniscalchi", "D.-C. Lyu", "T. Svendsen", "C.-H. Lee"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "On Compressibility of Neural Network Phonological Features for Low Bit Rate Speech Coding,", "author": ["A. Asaei", "M. Cernak", "H. Bourlard"], "venue": "in Proc. of Interspeech,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning deep architectures for AI,", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H. Bourlard", "N. Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Deep belief networks for phone recognition,", "author": ["A. Mohamed", "G.E. Dahl", "G.E. Hinton"], "venue": "in NIPS\u201922 workshop on deep learning for speech recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition,", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing (receiving 2013 IEEE SPS Best Paper Award),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Statistical parametric speech synthesis using Deep Neural Networks,", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Anytime learning of decision trees,", "author": ["S. Esmeir", "S. Markovitch", "C. Sammut"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "On the training aspects of deep neural network (DNN) for parametric tts synthesis,", "author": ["Y. Qian", "Y. Fan", "W. Hu", "F. Soong"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "DNN-based speech synthesis: Importance of input features and training data,", "author": ["A. Lazaridis", "B. Potard", "P.N. Garner"], "venue": "in International Conference on Speech and Computer, SPECOM 2015, ser. Lecture Notes in Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A new model of LPC excitation for producing natural-sounding speech at low bit rates,", "author": ["B.S. Atal", "J.R. Remde"], "venue": "in Proc. of ICASSP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates,", "author": ["M. Schroeder", "B. Atal"], "venue": "in Proc. of ICASSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1985}, {"title": "Speech coding based on a multi-layer neural network,", "author": ["S. Morishima", "H. Harashima", "Y. Katayama"], "venue": "in Communications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1990}, {"title": "Prediction in speech coding: the modification of the coding of LPC parameters and nonlinear estimation technique by using ANN,", "author": ["Y. Zhen"], "venue": "in Signal Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "A nonlinear adaptive predictor for speech compression,", "author": ["S. Hunt"], "venue": "Neural Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Discriminative coding with predictive neural networks,", "author": ["C. Chavy", "B. Gas", "J.L. Zarader"], "venue": "Artificial Neural Networks,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Engineering Applications of Bio-Inspired Artificial Neural Networks: International Work-Conference on Artificial and Natural Neural Networks, IWANN\u201999", "author": ["M. Fa\u00fandez-Zanuy"], "venue": "Proceedings, Volume II. Berlin,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Packet Loss Concealment Based on Deep Neural Networks for Digital Speech Transmission,", "author": ["B.-K. Lee", "J.-H. Chang"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Garoucy, \u201cComplexity Reduction of LD-CELP Speech Coding in Prediction of Gain Using Neural Networks,", "author": ["M. Sheikhan", "V.T. Vakili"], "venue": "World Applied Sciences Journal,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "A CELP codebook and search technique using a Hopfield net,", "author": ["M.G. Easton", "C.C. Goodyear"], "venue": "in Proc. of ICASSP. IEEE,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1991}, {"title": "Fully vector-quantized neural network-based code-excited nonlinear predictive speech coding,", "author": ["L. Wu", "M. Niranjan", "F. Fallside"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}, {"title": "Multiband excitation vocoder,", "author": ["D.W. Griffin", "J.S. Lim"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1988}, {"title": "A robust 800 bps MBE coder with VQ and MLP,", "author": ["H. Cui", "H. Jiang"], "venue": "Communication Technology Proceedings,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1998}, {"title": "Phonological vocoding using artificial neural networks,", "author": ["M. Cernak", "B. Potard", "P.N. Garner"], "venue": "in Proc. of ICASSP", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Demodulation as Probabilistic Inference,", "author": ["R.E. Turner", "M. Sahani"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "A role for amplitude modulation phase relationships in speech rhythm perception.", "author": ["V. Leong", "M.A. Stone", "R.E. Turner", "U. Goswami"], "venue": "J. Acoust. Soc. Am.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex.", "author": ["P. Lakatos", "A.S. Shah", "K.H. Knuth", "I. Ulbert", "G. Karmos", "C.E. Schroeder"], "venue": "Journal of neurophysiology,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "From Discontinuous To Continuous F0 Modelling In HMM-based Speech Synthesis,", "author": ["K. Yu", "B. Thomson", "S. Young", "T. Street"], "venue": "in Proc. ISCA SSW7. Kyoto, Japan: ISCA,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Continuous F0 Modeling for HMM Based Statistical Parametric Speech Synthesis,", "author": ["K. Yu", "S. Young"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Using Noisy Speech to Study the Robustness of a Continuous F0 Modelling Method in HMM-based Speech Synthesis,", "author": ["K.U. Ogbureke", "J.P. Cabral", "J. Carson-Berndsen"], "venue": "in Proc. Speech Prosody, Shanghai,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Neuromorphic Based Oscillatory Device for Incremental Syllable Boundary Detection,", "author": ["A. Hyafil", "M. Cernak"], "venue": "in Proc. of Interspeech,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Syllable-Based Pitch Encoding for Low Bit Rate Speech Coding with Recognition/Synthesis Architecture,", "author": ["M. Cernak", "X. Na", "P.N. Garner"], "venue": "in Proc. of Interspeech,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "The design for the wall street journalbased CSR corpus,", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language, ser. HLT \u201991. Stroudsburg, PA, USA: Association for Computational Linguistics,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1992}, {"title": "TIMIT Acoustic-Phonetic Continuous Speech Corpus,", "author": ["L.D. Consortium"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1993}, {"title": "The Festival Speech Synthesis System,", "author": ["A. Black", "P. Taylor", "R. Caley"], "venue": "Human Communication Research Centre,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1997}, {"title": "The HMM-based Speech Synthesis System Version 2.0,", "author": ["H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black", "K. Tokuda"], "venue": "in Proc. of ISCA SSW6,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Speaker adaptation and the evaluation of speaker similarity in the EMIME speech-to-speech translation project,", "author": ["M. Wester", "J. Dines", "M. Gibson", "H. Liang", "Y.-J. Wu", "L. Saheer", "S. King", "K. Oura", "P.N. Garner", "W. Byrne", "Y. Guan", "T. Hirsim\u00e4ki", "R. Karhila", "M. Kurimo", "M. Shannon", "S. Shiota", "J. Tian", "K. Tokuda", "J. Yamagishi"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "A Fast Learning Algorithm for Deep Belief Nets,", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}, {"title": "The kaldi speech recognition toolkit,", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "in Proc. of ASRU. IEEE SPS,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "Speech encoding by coupled cortical theta and gamma oscillations,", "author": ["A. Hyafil", "L. Fontolan", "C. Kabdebon", "B. Gutkin", "A.-L. Giraud", "H. Brownell"], "venue": "eLife,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "A simple continuous excitation model for parametric vocoding,", "author": ["P.N. Garner", "M. Cernak", "B. Potard"], "venue": "in Proc. of ICASSP", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Mel-cepstral distance measure for objective speech quality assessment,", "author": ["R.F. Kubichek"], "venue": "Proc. of ICASSP,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1993}, {"title": "Speech Quality Assessment,", "author": ["V. Grancharov", "W. . B. Kleijn"], "venue": "Handbook of Speech Processing,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2008}, {"title": "Distilling the Knowledge in a Neural Network,", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2015}, {"title": "Compressing Deep Neural Networks using a Rank-Constrained Topology,", "author": ["P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada"], "venue": "in Proc. of Interspeech,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2015}, {"title": "Structured Transforms for Small-Footprint Deep Learning,", "author": ["V. Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", [2], [3], and hidden Markov model (HMM) based, e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": ", [2], [3], and hidden Markov model (HMM) based, e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": ", [4]\u2013[6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [4]\u2013[6].", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "This is well known; phoneme ASR misrecognition rates tend to increase with longer words [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "defined in Appendix A of [9]: (i) the Government Phonology", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "There are only very few phonological classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector, and allow very few combinations that can be stored in a codebook for VLBR speech coding [15].", "startOffset": 226, "endOffset": 230}, {"referenceID": 11, "context": "Recently, in the speech recognition/analysis field, a shift has been observed from HMM-based approaches towards the use of deep neural networks (DNNs) [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Even though the concept of using neural networks in recognition is not new [17], the increase of available speech resources and of computational power, and the use of graphics processing units, have led to a major research interest in DNNs.", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "Additionally, deep architectures with multiple layers can overcome the limitations in the representational capability of HMMs, which are incapable of modelling multiple interacting source streams [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 14, "context": "Furthermore, DNNs are able to create non-linear mappings between the input and output features which cannot be achieved by using Gaussian mixture models (GMMs) in HMM-based approaches, making them more appropriate for modelling the speech signal [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 14, "context": "As a result, DNN-based approaches have managed to show superior performance in comparison to HMM-based ones in recognition tasks [19], [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "As a result, DNN-based approaches have managed to show superior performance in comparison to HMM-based ones in recognition tasks [19], [20].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "occur in HMM-based TTS [21], e.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": ", inefficiency to express complex dependencies in the feature space [22], which leads to decision trees becoming exceedingly large, hence inefficient and data hungry.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "approximating coders, a family of coders originated in [26], [27], to address either improving quality or reducing computational complexity.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "approximating coders, a family of coders originated in [26], [27], to address either improving quality or reducing computational complexity.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "The former usage aims to improve linear prediction (of speech samples or parameters of the excitation signal) with a non-linear prediction based usually on multilayer perceptrons [28]\u2013[32].", "startOffset": 179, "endOffset": 183}, {"referenceID": 26, "context": "The former usage aims to improve linear prediction (of speech samples or parameters of the excitation signal) with a non-linear prediction based usually on multilayer perceptrons [28]\u2013[32].", "startOffset": 184, "endOffset": 188}, {"referenceID": 27, "context": "Recently, regression-based packet loss concealment was proposed using DNNs [33].", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].", "startOffset": 136, "endOffset": 140}, {"referenceID": 30, "context": "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "Speech coding based on speech modelling is known as parametric coding, where the parameters of the speech models are transmitted, such as in a multiband excitation vocoder [37].", "startOffset": 172, "endOffset": 176}, {"referenceID": 32, "context": "Similarly, as in waveform coding, a multilayer perceptron was proposed to decrease the computation complexity of the codebook of line spectral frequencies in the 800 bps multiband excitation speech coding [38].", "startOffset": 205, "endOffset": 209}, {"referenceID": 33, "context": "sequence Z to the speech parameters [39].", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "For example, Probabilistic Amplitude Demodulation method proposed in [40] may robustly estimate the syllable and stress amplitude modulations as a representation of electrophysiological recordings of auditory cortex.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "The work of [41] proves that phase relations of the amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity [42], is a good indication of the syllable stress.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "The work of [41] proves that phase relations of the amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity [42], is a good indication of the syllable stress.", "startOffset": 180, "endOffset": 184}, {"referenceID": 37, "context": "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].", "startOffset": 100, "endOffset": 104}, {"referenceID": 38, "context": "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].", "startOffset": 106, "endOffset": 110}, {"referenceID": 39, "context": "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].", "startOffset": 158, "endOffset": 162}, {"referenceID": 40, "context": "To estimate syllable boundaries from the speech signal, a neuromorphic oscillatory device is used based on modelling brain neural oscillations at syllable frequency, resulting in highly noise robust incremental syllable boundary detection [47].", "startOffset": 239, "endOffset": 243}, {"referenceID": 41, "context": "In the original proposal of syllable-based F0 parametrization for speech coding [48], unvoiced syllables were not parametrized (and not transmitted), and the pitch coding oper-", "startOffset": 80, "endOffset": 84}, {"referenceID": 42, "context": "The DNN encoder is trained on the si tr s 284 set of the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora [49], and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1\u2013100).", "startOffset": 131, "endOffset": 135}, {"referenceID": 43, "context": "The DNN encoder is trained on the si tr s 284 set of the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora [49], and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1\u2013100).", "startOffset": 192, "endOffset": 196}, {"referenceID": 44, "context": "The text was processed by a conventional and freely available TTS front-end [51], and the resulting phonetic labels were used for training of the synthesis DNN.", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "For HMM analysis models, we trained three-state, cross-word triphone models with the HTS variant [52] of the HTK toolkit on the WSJ training set.", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "For building the HMM synthesis models, the implementation of training from the EMIME project [53] was used.", "startOffset": 93, "endOffset": 97}, {"referenceID": 47, "context": "The training was initialized using deep belief network pretraining done by the single-step contrastive divergence (CD-1) procedure of [54].", "startOffset": 134, "endOffset": 138}, {"referenceID": 48, "context": "The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the Kaldi toolkit [55].", "startOffset": 181, "endOffset": 185}, {"referenceID": 49, "context": "Its principles are based on findings on the role of slow neural oscillations in the auditory cortex for natural speech parsing [56].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "Syllable boundaries are characterised by local minima of the weighted signal, that can be generalised to a convolution of the temporal kernel and the weighted signal [47].", "startOffset": 166, "endOffset": 170}, {"referenceID": 50, "context": "The output features, the LPC speech parameters, were extracted by the SSP: ~ pn Line Spectral Pairs (LSPs) of 24th order plus gain, log(~rn) - a Harmonic-To-Noise (HNR) ratio, and ~tn, log(~ mn) - two glottal model parameters [58], angle t and magnitude log(m) of a glottal pole, respectively.", "startOffset": 226, "endOffset": 230}, {"referenceID": 51, "context": "5 are forced to zero) and used Mel Cepstral Distortion (MCD) [59] between original and encoded speech samples as an objective metric to compare overall speech quality.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "We discuss their meaning and exact definition in [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 52, "context": "We employed a 5-point scale ABX subjective evaluation listening test [61], suitable for comparing two different systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 53, "context": "Following recent research on complexity reduction of NNs (for example [63]\u2013[65]), we believe that this coding approach becomes more feasible for a broad range of computation platforms that may be used in telecommunication networks.", "startOffset": 70, "endOffset": 74}, {"referenceID": 55, "context": "Following recent research on complexity reduction of NNs (for example [63]\u2013[65]), we believe that this coding approach becomes more feasible for a broad range of computation platforms that may be used in telecommunication networks.", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "Most current very low bit rate (VLBR) speech coding systems use hidden Markov model (HMM) based speech recognition/synthesis techniques. This allows transmission of information (such as phonemes) segment by segment that decreases the bit rate. However, the encoder based on a phoneme speech recognition may create bursts of segmental errors. Segmental errors are further propagated to optional suprasegmental (such as syllable) information coding. Together with the errors of voicing detection in pitch parametrization, HMM-based speech coding creates speech discontinuities and unnatural speech sound artefacts. In this paper, we propose a novel VLBR speech coding framework based on neural networks (NNs) for end-to-end speech analysis and synthesis without HMMs. The speech coding framework relies on phonological (sub-phonetic) representation of speech, and it is designed as a composition of deep and spiking NNs: a bank of phonological analysers at the transmitter, and a phonological synthesizer at the receiver, both realised as deep NNs, and a spiking NN as an incremental and robust encoder of syllable boundaries for coding of continuous fundamental frequency (F0). A combination of phonological features defines much more sound patterns than phonetic features defined by HMM-based speech coders, and the finer analysis/synthesis code contributes into smoother encoded speech. Listeners significantly prefer the NN-based approach due to fewer discontinuities and speech artefacts of the encoded speech. A single forward pass is required during the speech encoding and decoding. The proposed VLBR speech coding operates at bit rate about 360 bits/sec.", "creator": "TeX"}}}