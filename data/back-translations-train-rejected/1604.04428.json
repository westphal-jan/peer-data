{"id": "1604.04428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2016", "title": "The Artificial Mind's Eye: Resisting Adversarials for Convolutional Neural Networks using Internal Projection", "abstract": "We introduce a novel type of artificial neural network structure and training procedure that results in networks that are provably, quantitatively more robust to adversarial samples than classical, end-to-end trained classifiers. The main idea of our approach is to force the network to make predictions on what the given instance of the class under consideration would look like and subsequently test those predictions. By forcing the network to redraw the relevant parts of the image and subsequently comparing this new image to the original, we are having the network give a 'proof' of the presence of the object.", "histories": [["v1", "Fri, 15 Apr 2016 11:07:45 GMT  (1969kb,D)", "https://arxiv.org/abs/1604.04428v1", "Under review as a conference paper at ECML PKDD 2016"], ["v2", "Thu, 14 Jul 2016 15:18:56 GMT  (1967kb,D)", "http://arxiv.org/abs/1604.04428v2", null]], "COMMENTS": "Under review as a conference paper at ECML PKDD 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["harm berntsen", "wouter kuijper", "tom heskes"], "accepted": false, "id": "1604.04428"}, "pdf": {"name": "1604.04428.pdf", "metadata": {"source": "CRF", "title": "Resisting Adversarials for Convolutional Neural Networks using Internal Projection", "authors": ["Harm Berntsen", "Wouter Kuijper", "Tom Heskes"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, we present a novel network structure and training method that is imperceptible to the naked eye. The main idea of our approach is to force the network to make predictions about what the given class is considering under the circumstances."}, {"heading": "1.1 Related Work", "text": "Neural networks recognize objects in a different way than humans. As Ullman et al. [26] point out,.. \"the human recognition system uses features and learning processes that are critical for recognition but not used by current models.\" They show that where humans can recognize internal components of the objects in the image, current neural networks do not. Knowledge of the internal representation of the objects can reject false detections if they are not consistent with the internal representation of the object, corresponding to sensitivity to hostile images with an imperceptible change, which has been shown in [23] and various works since [16]. They show that the smoothness assumption does not apply to neural networks; an imperceptible change in the quadrant image can reverse the classification. Goodfellow et al. argue that the primary cause for this is the linear behavior of networks in high-dimensional spaces. [7] As opposed to the nonlinearity class assumed."}, {"heading": "2 Network Architectures", "text": "In this section, we will describe the network architectures we use to test the robustness of our approach. Each network's task is the same: classify the image. Our data consists of grayscale ImageNet images, where a portion of the image is overlaid with an alpha-blended instance of a 3D model. We will use three 3D models that are parameterized by their Euler rotation. The neural network must recognize and emit the 3D models in all of these rotations, which 3D model, if any, is visible in the quadrant image. We will compare the robustness with adverse images based on three concrete network structures. We will call the three 3D models positive classes and the \"no\" class a negative class."}, {"heading": "2.1 Networks", "text": "In fact, most of them will be able to play by the rules that they have adopted in recent years."}, {"heading": "2.2 Rationale", "text": "In order for the neural network to produce a false positive classification, an adversary must disrupt the image in such a way that it ultimately deceives the comparison level of the respective network, but this requires it to pass through both the estimator and the projector stage. Any attempt to generate a false positive will begin to draw a different 3D model over the existing one because the comparer compares the image in question with the stable internal projection of the class in question. Ultimately, the \"false positive\" class will then obviously be visible in the query image. As the comparison network consumes the query image directly, this network may still be vulnerable to adversarial interference of the query image."}, {"heading": "3 Experiment Set-up", "text": "In this section, we describe how to test the robustness of the network architectures from the previous section."}, {"heading": "3.1 Training method", "text": "We used parameterized 3D models as recognition objects. We rendered 64 x 64 pixel grayscale images of three 3D models: a monkey (the Suzanne model of [3]), a penguin [13], and an airplane [6] with an aperture [3]. We used the three-axis rotation of the 3D model as a parameter space, although our method is not limited to this. Rotations were consistently sampled from the [\u2212 0.5, 0.5] radiant range. To give the 3D models a \"natural\" background, we used the phase composition to merge the 3D model with randomly sampled images from the ImageNet dataset [19], which reduces the overmatch of the network to the otherwise black background. We generated 4 x 104 samples for each class. The No class simply consists of random ImageNet images. We used caffe [11] for network implementations. Direct network jjjacking was prepared for use 104 times, using all three of the three ImageNet images."}, {"heading": "3.2 Adversarial Image Generation", "text": "If we want to create an enemy query image x, we look for a minimum disturbance of the original image x, which is sufficient to rotate the classifier in the direction of a selected hostile target class value y. To do this, we use the method of fast gradient signs of [7]. The method of fast gradient signs can efficiently produce contrary images by backpropagation. Our goal is to generate hostile samples that rotate the classification to another positive class value y. Specifically, we disrupt an image by calculating x = clip (x characters (\u0432 xJ (\u03b8, x, y), 0, 255), with J vibrating the loss function over the query image x and the network parameters. Since we use 8-bit grayscale images with a range of [0, 255], each pixel of the image is minimally disturbed. This function is used as often as necessary to change the classification of the network to the target y."}, {"heading": "4 Results", "text": "In fact, it is such that the majority of them are in a position to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process in which there is a process, to a process, to a process in which there is a process, to a process, to a process in which there is a process, to a process, to a process in which there is a process, to a process, to a process, to a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process,"}, {"heading": "5 Discussion", "text": "We have adapted the classical network structure for classification tasks and shown significant improvements in robustness to enemy samples. To avoid contaminating the results, we have not considered other types of solutions to enemy samples, such as enemy training, but this does not mean that these techniques are not useful in our environment. Therefore, our future work plans to include hostile training in our approach. Future work could also apply our technique to include other parameters such as internal deformations, the use of joints, etc. We have tested only three 3D models with limited parameter space. [15,21] has already shown that it is possible to estimate views of 3D models in real-world images. Dosovitskiy et al. [5] have shown that a deconvolutionary neural image can generate on the basis of many classes and viewpoints, opening up possibilities to extend our work to a real-world situation."}, {"heading": "Acknowledgements", "text": "We would like to thank Daan van Beek for the lively discussions during the pre-conceptual phase of this work and also for his detailed comments on a draft of this paper."}], "references": [{"title": "Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models", "author": ["M. Aubry", "D. Maturana", "A. Efros", "B. Russell", "J. Sivic"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Adversarial background augmentation improves object localisation using convolutional neural networks. Master\u2019s thesis, Radboud", "author": ["H. Berntsen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Visual causal feature learning", "author": ["K. Chalupka", "P. Perona", "F. Eberhardt"], "venue": "UAI (2015),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning to generate chairs, tables and cars with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Tatarchenko", "T. Brox"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Antonov an - 71 - 3d model - .obj, .mb", "author": ["P. Fraga"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "CoRR (2015),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "ICANN", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C.S. ri"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "K.Q. (eds.) Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Tux (armatured) - 3d models - kator legaz", "author": ["K. Legaz"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A unified gradient regularization family for adversarial examples", "author": ["C. Lyu", "K. Huang", "H.N. Liang"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Deep exemplar 2d-3d detection by adapting from real to rendered views", "author": ["F. Massa", "B.C. Russell", "M. Aubry"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "images. In: Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G. Corrado", "J. Dean"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Learning deep object detectors from 3d models", "author": ["X. Peng", "B. Sun", "K. Ali", "K. Saenko"], "venue": "http://www.cv-foundation.org/openaccess/content_", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "Workshop at International Conference on Learning Representations", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views", "author": ["H. Su", "C.R. Qi", "Y. Li", "L.J. Guibas"], "venue": "The IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains", "author": ["B. Sun", "K. Saenko"], "venue": "Proceedings of the British Machine Vision Conference. BMVA Press", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["P. Tabacof", "E. Valle"], "venue": "CoRR (2015),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Improving image classification with location context", "author": ["K.D. Tang", "M. Paluri", "L. Fei-Fei", "R. Fergus", "L.D. Bourdev"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Atoms of recognition in human and computer vision", "author": ["S. Ullman", "L. Assif", "E. Fetaya", "D. Harari"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "However, CNNs are vulnerable to adversarial images [16,23].", "startOffset": 51, "endOffset": 58}, {"referenceID": 20, "context": "However, CNNs are vulnerable to adversarial images [16,23].", "startOffset": 51, "endOffset": 58}, {"referenceID": 5, "context": "Adversarial images can be found by perturbing a normal image in such a subtle way that the change is usually imperceptible by the naked eye [7,23].", "startOffset": 140, "endOffset": 146}, {"referenceID": 20, "context": "Adversarial images can be found by perturbing a normal image in such a subtle way that the change is usually imperceptible by the naked eye [7,23].", "startOffset": 140, "endOffset": 146}, {"referenceID": 23, "context": "[26] point out: \u201c.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "This corresponds with the sensitivity to adversarial images with an imperceptible change that have been shown in [23] and various work since [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "This corresponds with the sensitivity to adversarial images with an imperceptible change that have been shown in [23] and various work since [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "argue that the primary cause for this is the linear behaviour of the networks in high-dimensional spaces [7] as opposite to the nonlinearity suspected in [23].", "startOffset": 105, "endOffset": 108}, {"referenceID": 20, "context": "argue that the primary cause for this is the linear behaviour of the networks in high-dimensional spaces [7] as opposite to the nonlinearity suspected in [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 21, "context": "The adversarial images are not isolated, spurious points in the pixel space but appear in large regions of the space [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "Moreover, adversarial images can be efficiently computed using gradient ascent, starting from any input [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "Though the existence of adversarial examples is universal [23], neural networks can be made more robust against them.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "One way is to include adversarial examples in the training data [7,10,14,23], e.", "startOffset": 64, "endOffset": 76}, {"referenceID": 8, "context": "One way is to include adversarial examples in the training data [7,10,14,23], e.", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "One way is to include adversarial examples in the training data [7,10,14,23], e.", "startOffset": 64, "endOffset": 76}, {"referenceID": 20, "context": "One way is to include adversarial examples in the training data [7,10,14,23], e.", "startOffset": 64, "endOffset": 76}, {"referenceID": 2, "context": "Another approach is to adapt the model of the network to improve robustness [4,8].", "startOffset": 76, "endOffset": 81}, {"referenceID": 6, "context": "Another approach is to adapt the model of the network to improve robustness [4,8].", "startOffset": 76, "endOffset": 81}, {"referenceID": 2, "context": "In [4] the authors identify features that are causally related with the classes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [8] the authors test several denoising architectures to reduce the effects of", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].", "startOffset": 21, "endOffset": 28}, {"referenceID": 19, "context": "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].", "startOffset": 75, "endOffset": 84}, {"referenceID": 13, "context": "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].", "startOffset": 75, "endOffset": 84}, {"referenceID": 18, "context": "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].", "startOffset": 75, "endOffset": 84}, {"referenceID": 0, "context": "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.", "startOffset": 55, "endOffset": 62}, {"referenceID": 18, "context": "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.", "startOffset": 55, "endOffset": 62}, {"referenceID": 10, "context": "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].", "startOffset": 95, "endOffset": 108}, {"referenceID": 17, "context": "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].", "startOffset": 95, "endOffset": 108}, {"referenceID": 18, "context": "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].", "startOffset": 95, "endOffset": 108}, {"referenceID": 22, "context": "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].", "startOffset": 95, "endOffset": 108}, {"referenceID": 1, "context": "To adapt AlexNet to a reduced set of classes and smaller query image, we use a reduced version of AlexNet from [2] which uses smaller layers.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "In [5], it was shown that a deep, deconvolutional neural network can be trained to generate images that are parametrised by a broad set of classes and viewpoints.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Due to our smaller set of classes and parameters, we use a downscaled variant of the 1s-S network from [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "This concept was already applied in the context of transforming autoencoders [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 24, "context": "Here we follow [27], which shows how to compare image patches using CNNs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "We rendered 64 \u00d7 64 pixel greyscale images of three 3D models: a Monkey (the Suzanne model from [3]), Penguin [13] and an Aeroplane [6] using Blender [3].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "We rendered 64 \u00d7 64 pixel greyscale images of three 3D models: a Monkey (the Suzanne model from [3]), Penguin [13] and an Aeroplane [6] using Blender [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 9, "context": "We used Caffe [11] for the network implementations.", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "To do this we adopt the fast gradient sign method of [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 20, "context": "Here we follow [23] who measure the amount of perturbation in adversarial sample for original sample as distortion which is defined as:", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "In [15,21] it was already shown that it is possible to estimate viewpoints of 3D models in real-world images.", "startOffset": 3, "endOffset": 10}, {"referenceID": 18, "context": "In [15,21] it was already shown that it is possible to estimate viewpoints of 3D models in real-world images.", "startOffset": 3, "endOffset": 10}, {"referenceID": 3, "context": "[5] have shown that a deconvolutional neural can generate images based on many classes and viewpoints.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "We introduce a novel artificial neural network architecture that integrates robustness to adversarial input in the network structure. The main idea of our approach is to force the network to make predictions on what the given instance of the class under consideration would look like and subsequently test those predictions. By forcing the network to redraw the relevant parts of the image and subsequently comparing this new image to the original, we are having the network give a \u201cproof\u201d of the presence of the object.", "creator": "LaTeX with hyperref package"}}}