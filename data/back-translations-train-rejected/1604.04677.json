{"id": "1604.04677", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2016", "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "abstract": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.", "histories": [["v1", "Sat, 16 Apr 2016 01:49:09 GMT  (143kb,D)", "http://arxiv.org/abs/1604.04677v1", "To appear at BEA11, as part of the AESW 2016 Shared Task"]], "COMMENTS": "To appear at BEA11, as part of the AESW 2016 Shared Task", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["allen schmaltz", "yoon kim", "alexander m rush", "stuart m shieber"], "accepted": false, "id": "1604.04677"}, "pdf": {"name": "1604.04677.pdf", "metadata": {"source": "CRF", "title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart M. Shieber"], "emails": [], "sections": [{"heading": null, "text": "We show that an attention-based encoder decoder model for grammatical error detection at the sentence level can be used for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. In addition to error detection, which is of interest to certain end-user applications, the attention-based encoder decoder models can be used to make corrections. We show that a character-based encoder decoder model is particularly effective, outperforms other results of the AESW Shared Task alone, and has gains over a word-based counterpart. Our final model - a combination of three character-based encoder decoder models, a word-based encoder decoder model, and a sentence-based CNN - is the most powerful system in the AESW 2016 Shared Task Binary Prediction."}, {"heading": "1 Introduction", "text": "The recent confluence of data availability and strong sequence-to-sequence learning algorithms has the potential to lead to practical tools for writing support. Grammar error detection is such an application of potential usefulness as a component of a writing tool. Much of the recent work on grammatical error identification and correction has used hand-crafted rules and features that complement data-driven approaches or individual classifiers for human-defined subsets of errors. Given a large annotated dataset of scientific journal articles, we propose a fully data-driven approach to this problem, inspired by recent work in neural machine translation and general sequence sequence sequencing (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al.).The Automated Review of Scientific Writing (AESW) 2016 is a collection of nearly 10,000 scientific journal articles (over 1 million professional sentences) published between 2006 and 2013 with a corridor correction."}, {"heading": "2 Background", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green so\" rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc for the green for the green for the green for the green for the green"}, {"heading": "3 Related Work", "text": "While this is the first year for a common task focused on detecting binary errors at the sentence level, previous work and common tasks have focused on related tasks of intra-sentence identification and error correction. Until recently, hand-annotated grammatical error records were not available by default, making comparisons more difficult and limiting the choice of methods used. In the absence of a large, hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a sound model with error data without corrections, conducting evaluations on a much smaller group of sentences mechanically corrected by Amazon, from workers in Turkey. Recent work has emerged as a result of a series of common tasks, starting with the Helping Our Own (HOO) Pilot Shared Task Run in 2011, which focused on a series of errors in a small data set."}, {"heading": "4 Models", "text": "We use an end-to-end approach that does not have separate components for candidate generation or reranking that use hand-tuned rules or explicit syntax, and we do not use separate classifiers for humanly differentiated subsets of errors, unlike previous work on the related task of grammatical error correction. Next, we present two approaches to the task of grammatical error identification at the sentence level: a binary classifier and a sequence-to-sequence model that is trained for correction but can also be used as a side effect for identification."}, {"heading": "4.1 Baseline Convolutional Neural Net", "text": "To establish a baseline, we are following previous work that has shown strong performance with Convolutionary Neural Networks (CNNs) in various areas of sentence classification (Kim, 2014; Zhang and Wallace, 2015). We are using the single-layer CNN architecture of Kim (2014) with the publicly available 5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013). We are experimenting with keeping the word vectors static (CNN-STATIC) and fine-tuning the vectors (CNN-NONSTATIC)."}, {"heading": "4.2 Encoder-Decoder", "text": "It may seem natural that the models trained for binary predictions are at the level of the word rather than the level of the word. (So we use the shared task as a means of assessing the usefulness of a complete generation model for binary prediction. (We also suspect that the generation of corrections is sufficient for this task.) Our word-based architecture (WORD) is similar to that of Luong et al. (2015) Our character-based models (CHAR) still make predictions at the word level."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "The AESW task data differs from previous grammatical error records in scale and genre. To the best of our knowledge, the AESW task data set is the first large-format, publicly accessible, professionally edited dataset of academic, scientific writing. The training record consists of 466,672 sentences of editing and 722,742 sentences of unediting, and the development record contains 57,340 sentences of editing and 90,106 sentences of unediting. The raw training and development records are provided as annotated records from which the sequences can be derived. There are 143,802 sentences in the Shared Task Test Set with hidden gold marks that serve directly as sequences. As part of pre-processing, we treat each sentence independently, discard paragraph contexts (the sentences, if any, were in the same paragraphs) and domain information grouped by the field."}, {"heading": "5.2 Training", "text": "Our models were based on the Torch7 Framework.CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015).A limited grid search on the development set out our use of filter windows of width 3, 4, and 5 and 1000 feature maps. We trained for 10 epochs. The training followed the approach of appropriately named CNN-STATIC and CNN-NONSTATIC models of Kim (2014).6Note that the number of sentences set out in the final development without labels on CodaLab (http: / / codalab. org) differed from that originally available on the AESW 2016 Shared Task Website with labels. 7http: / / torch.chencoder-decoder Initial parameters settings 1."}, {"heading": "5.3 Tuning", "text": "In order to maximize the F1 score, this post-hoc tuning was important for these models, without which precision was high and memory low. We will leave it up to future work to find alternative approaches for this type of post-hoc tuning. In the CNN models, we adjusted the decision limit after training to maximize the F1 score on the pre-set tuning set. Similarly, in the encoder models, we adjusted the bias weights (given as input into the last Softmax layer to generate the word / tag distribution) associated with the four comment labels via a simple grid search using iterative bar search on the do-ing set. Due to the relatively high decoding effort, we used a rough grid search, in which the bias weights of the four comment labels were uniformly varied."}, {"heading": "6 Results", "text": "The results on the development list, excluding the 10k Tuning Set, appear in Table 1 (and elsewhere) RANDOM is the result of assigning a set to one of the binary classes. For CNN classifiers, the fine-tuning of the word-based models is better. Encoder decoder models are able to provide themselves with additional data (via word2vec). Character-based models achieve tangible improvements in the word-based models. For consistency, we keep the beam size in 10, but subsequent analyses show that the beam size from 5 to 10 is higher the impact on overall performance."}, {"heading": "7 Discussion", "text": "Of particular interest is that the CHAR + SAMPLE model performs well, both in terms of the performance of the test set compared to other submissions and in terms of the evolution of the development set compared to the WORD models and CNN classifiers. It is possible that this is due to the ability of the CHAR models to detect some types of orthographic errors. Empirical results suggest that the mere addition of additional, already correct source-target pairs in the training of the encoder decoder models may not increase performance, ceteris paribus, as seen when comparing the performance of CHAR + SAMPLE vs WORD + SAMPLE and CHAR + ALL vs WORD + ALL. We leave to future work alternative approaches for the introduction of additional correct (target) sets as investigated for neural translation models (Sennrich et al., 2015; Gu + ALL, 2015)."}, {"heading": "8 Conclusion", "text": "We have submitted our submission to the AESW 2016 Shared Task, in which we propose in particular the usefulness of a neural, attention-based model for detecting grammatical errors at the sentence level. Our models do not use hand-tuned rules, are not trained with explicit syntactical annotations, and do not use individual classifiers designed for human subsets of errors. Subword-level modeling has been advantageous for the encoder decoder models, although predictions are still made at the word level. It would be interesting to push this further to eliminate the need for an initial tokenization step to generalize the approach to other languages such as Chinese and Japanese. We plan to explore alternative approaches to training with additional correct (target) sentences. Creating artificial errors to generate more erroneous (source) sentences is also a direction we want to pursue."}, {"heading": "Acknowledgments", "text": "We would like to thank the organizers of the Shared Task for coordinating the task and providing the unique AESW dataset for research purposes. Institute for Quantitative Social Science (IQSS) and Harvard Initiative for Learning and Teaching (HILT) supported previous related research that led to our participation in the Shared Task. Jeffrey Ling thankfully contributed a facet-based CNN implementation of Kim (2014)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "CoRR, abs/1511.01432.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Helping our own: The hoo 2011 pilot shared task", "author": ["Robert Dale", "Adam Kilgarriff."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation, ENLG \u201911, pages 242\u2013249, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Dale and Kilgarriff.,? 2011", "shortCiteRegEx": "Dale and Kilgarriff.", "year": 2011}, {"title": "Hoo 2012: A report on the preposition and determiner error correction shared task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway."], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54\u201362, Stroudsburg, PA,", "citeRegEx": "Dale et al\\.,? 2012", "shortCiteRegEx": "Dale et al\\.", "year": 2012}, {"title": "A report on the automatic evaluation of scientific writing shared task", "author": ["Vidas Daudaravicius", "Rafael E. Banchs", "Elena Volodina", "Courtney Napoles."], "venue": "Proceedings of the Eleventh Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Daudaravicius et al\\.,? 2016", "shortCiteRegEx": "Daudaravicius et al\\.", "year": 2016}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Lan-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421, Lisbon, Portugal, September. Association", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neu-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ground truth for grammatical error correction metrics", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Napoles et al\\.,? 2015", "shortCiteRegEx": "Napoles et al\\.", "year": 2015}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Automated whole sentence grammar correction using a noisy channel model", "author": ["Y. Albert Park", "Roger Levy."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT \u201911,", "citeRegEx": "Park and Levy.,? 2011", "shortCiteRegEx": "Park and Levy.", "year": 2011}, {"title": "The Illinois-Columbia system in the CoNLL-2014 shared task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34\u2013", "citeRegEx": "Rozovskaya et al\\.,? 2014", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR, abs/1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "CoRR, abs/1507.06228.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace."], "venue": "CoRR, abs/1510.03820.", "citeRegEx": "Zhang and Wallace.,? 2015", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 0, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 1, "context": "Given a large, annotated dataset of scientific journal articles, we propose a fully data-driven approach for this problem, inspired by recent work in neural machine translation and more generally, sequence-tosequence learning (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 226, "endOffset": 291}, {"referenceID": 9, "context": "We also provide evidence of tangible performance gains using a character-aware version of the model, building on the characteraware language modeling work of Kim et al. (2016). In addition to sentence-level classification, the models are capable of intra-sentence error identification and the generation of possible corrections.", "startOffset": 158, "endOffset": 176}, {"referenceID": 1, "context": "formations using recurrent neural network models (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 49, "endOffset": 91}, {"referenceID": 20, "context": "formations using recurrent neural network models (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 49, "endOffset": 91}, {"referenceID": 15, "context": "Unlike the related CoNLL-2014 Shared Task (Ng et al., 2014) data, errors are not labeled with fine-grained types (article or determiner error, verb tense error, etc.", "startOffset": 42, "endOffset": 59}, {"referenceID": 5, "context": "Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016).", "startOffset": 74, "endOffset": 102}, {"referenceID": 16, "context": "Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers.", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al.", "startOffset": 201, "endOffset": 228}, {"referenceID": 4, "context": "More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012).", "startOffset": 364, "endOffset": 383}, {"referenceID": 14, "context": "The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore.", "startOffset": 27, "endOffset": 44}, {"referenceID": 15, "context": "The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners.", "startOffset": 37, "endOffset": 54}, {"referenceID": 13, "context": "the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015).", "startOffset": 149, "endOffset": 171}, {"referenceID": 17, "context": "Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014).", "startOffset": 151, "endOffset": 176}, {"referenceID": 6, "context": "The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 174, "endOffset": 210}, {"referenceID": 22, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 174, "endOffset": 210}, {"referenceID": 12, "context": "We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013).", "startOffset": 173, "endOffset": 195}, {"referenceID": 10, "context": "To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015). We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al.", "startOffset": 175, "endOffset": 268}, {"referenceID": 20, "context": "We follow past work (Sutskever et al., 2014; Luong et al., 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 20, "endOffset": 64}, {"referenceID": 11, "context": "We follow past work (Sutskever et al., 2014; Luong et al., 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 20, "endOffset": 64}, {"referenceID": 8, "context": ", 2015) in stacking multiple recurrent neural networks (RNNs), specifically Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, in both the encoder and decoder.", "startOffset": 106, "endOffset": 140}, {"referenceID": 2, "context": "word2vec/ approach of Dai and Le (2015), we hypothesize that training at the lowest granularity of annotations may be useful for the task.", "startOffset": 22, "endOffset": 40}, {"referenceID": 2, "context": "word2vec/ approach of Dai and Le (2015), we hypothesize that training at the lowest granularity of annotations may be useful for the task. We also suspect that the generation of corrections is of sufficient utility for endusers to further justify exploring models that produce corrections in addition to identification. We thus use the Shared Task as a means of assessing the utility of a full generation model for the binary prediction task. We propose two encoder-decoder architectures for this task. Our word-based architecture (WORD) is similar to that of Luong et al. (2015). Our character-based models (CHAR) still make predictions at the word-level, but use a CNN and a highway network over characters instead of word embeddings as the input to the encoder and decoder, as depicted in Figure 1.", "startOffset": 22, "endOffset": 580}, {"referenceID": 11, "context": "Finally, as in Luong et al. (2015), we feed cj as additional input to the decoder for the next time step by concatenating it with xj , so the decoder equation is modified to,", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "Our character model closely follows that of Kim et al. (2016). Suppose word si is composed of characters [p1, .", "startOffset": 44, "endOffset": 62}, {"referenceID": 19, "context": "Highway Network Instead of replacing the word embedding xi with zi, we feed zi through a highway network (Srivastava et al., 2015).", "startOffset": 105, "endOffset": 130}, {"referenceID": 10, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 112, "endOffset": 148}, {"referenceID": 22, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 112, "endOffset": 148}, {"referenceID": 10, "context": "CNN Architecture and training approaches were informed by past work in sentence-level classification using CNNs (Kim, 2014; Zhang and Wallace, 2015). A limited grid search on the development set determined our use of filter windows of width 3, 4, and 5 and 1000 feature maps. We trained for 10 epochs. Training otherwise followed the approach of the correspondingly named CNN-STATIC and CNN-NONSTATIC models of Kim (2014).", "startOffset": 113, "endOffset": 422}, {"referenceID": 17, "context": "ch encoder-decoder Initial parameter settings (including architecture decisions such as the number of layers and embedding and hidden state sizes) were informed by concurrent work in neural machine translation and existing work such as that of Sutskever et al. (2014) and Luong et al.", "startOffset": 244, "endOffset": 268}, {"referenceID": 11, "context": "(2014) and Luong et al. (2015). We used 4-layer LSTMs with 1000 hidden units in each layer.", "startOffset": 11, "endOffset": 31}, {"referenceID": 11, "context": "(2014) and Luong et al. (2015). We used 4-layer LSTMs with 1000 hidden units in each layer. We trained for 14 epochs with a batch size of 64 and a maximum sequence length of 50. The parameters for the WORD model were uniformly initialized in [\u22120.1, 0.1], and those of the CHAR model were uniformly initialized in [\u22120.05, 0.05]. The L2normalized gradients were constrained to be \u2264 5. Our learning rate schedule started the learning rate at 1 and halved the learning rate after each epoch beyond epoch 10, or once the validation set perplexity no longer improved. The WORD model used 1000-dimensional word embeddings. For CHAR, the character embeddings were 25-dimensional, the filter width was 6, the number of feature maps was 1000, and 2 highway layers were used. The maximum word length was 35 characters for training CHAR. Note that we do not reverse the source (s) sequences, unlike some previous NMT work. Following the work of Zaremba et al. (2014), we employed dropout with a probability of 0.", "startOffset": 11, "endOffset": 955}, {"referenceID": 18, "context": "We leave to future work alternative approaches for introducing additional correct (target) sentences, as has been examined for neural machine translation models (Sennrich et al., 2015; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 161, "endOffset": 207}, {"referenceID": 7, "context": "We leave to future work alternative approaches for introducing additional correct (target) sentences, as has been examined for neural machine translation models (Sennrich et al., 2015; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 161, "endOffset": 207}, {"referenceID": 2, "context": "In future work, we plan to compare against sentence classification using LSTMs (Dai and Le, 2015) and convolutional models that use correction-level annotations.", "startOffset": 79, "endOffset": 97}, {"referenceID": 10, "context": "Jeffrey Ling graciously contributed a torch-based CNN implementation of Kim (2014).", "startOffset": 72, "endOffset": 83}], "year": 2016, "abstractText": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model\u2014a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN\u2014is the highest performing system on the AESW 2016 binary prediction Shared Task.", "creator": "LaTeX with hyperref package"}}}