{"id": "1604.04767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2016", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "abstract": "Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection's result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.", "histories": [["v1", "Sat, 16 Apr 2016 15:42:12 GMT  (485kb,D)", "http://arxiv.org/abs/1604.04767v1", "The final publication is available at Springer viathis http URL"]], "COMMENTS": "The final publication is available at Springer viathis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["markus thom", "matthias rapp", "g\\\"unther palm"], "accepted": false, "id": "1604.04767"}, "pdf": {"name": "1604.04767.pdf", "metadata": {"source": "CRF", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "authors": ["Markus Thom", "Matthias Rapp"], "emails": ["markus.thom@uni-ulm.de", "matthias.rapp@uni-ulm.de", "guenther.palm@uni-ulm.de"], "sections": [{"heading": null, "text": "Communicated by Julien Mairal, Francis Bach, Michael Elad.M. Thom (B) driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany E-Mail: markus.thom @ uni-ulm.deM. Rapp driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany E-Mail: matthias.rapp @ uni-ulm.deG. Palm Institute of Neural Information Processing Ulm University, 89081 Ulm, Germany E-Mail: guenther.palm @ uni-ulm.detherefore come into question for a quick analysis of data sets and problems with enormous amounts of learning examples. Keywords Economical coding \u00b7 Scanty representations \u00b7 Dictionary learning \u00b7 Explicit economical limitations \u00b7 Economical enforcement of predictions"}, {"heading": "1 Introduction", "text": "There is overwhelming evidence that the brains of mammals have respected the barren principle (Laughlin and Sejnowski 2003), which applies primarily to mammalian visual cortex processing (Hubel and Wiesel 1959; Mairal et al. 2009b). It suggests that the barren and biologically plausible representation of signal processing tasks can be fundamental, especially low-level image processing, since natural images can be successfully represented with structural primitives (Olshausen and Field 1997; Mairal et al. 2009b). Interesting and biologically plausible representations have been discovered by computer simulations on natural images (Olshausen and Field 1996, 1997). Similar representations can be achieved by analyzing temporary image sequences (van Hateren and Ruderman 1998; Olshausen 2003), stereo image pairs, and images with chromatic information (Hoyer and Hy\u00e4rinen 2000)."}, {"heading": "2 Efficient Sparseness-Enforcing Projections", "text": "This section suggests a linear time and a constant space algorithm for calculating projections on the level q before (2001). Formally considered, if it is an objective representation (0, 1), then it is an arbitrary point, the point set from the level S: = {s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "3 Explicitly Sparseness-Constrained Dictionary Learning", "text": "This section proposes the Easy Dictionary Learning (EZDL) algorithm for dictionary learning, which is then developed further with simple models. (This section proposes the Easy Dictionary Learning (EZDL) algorithm for dictionary learning. (This section includes a simple formulation of the learning algorithm.) First, we discuss extensions that allow us to focus on various aspects of the dataset, such as the topographic organization of atoms and atomic economy. Little implementation effort is required for these extensions and their computational requirements are low. A description of the comprehensive EZDL learning algorithm is accompanied by several strategies that improve optimization performance. (EZDL) Explicit learning algorithms operate in an alternative manner, in which code words with an acostic optimization process are used before updating the dictionary."}, {"heading": "4 Experimental Results", "text": "This section reports experimentally on the methods proposed in this publication. We first evaluate alternative approaches to the sparse projection methods and show that our algorithm for the sparse-penetrating projection operations is significantly faster than previously known methods. We then turn to the application of the projection method in the Easy Dictionary Learning Algorithm. We achieve a reproduction quality that corresponds to the dictionaries trained with alternative, significantly slower algorithms. Finally, we analyze the performance of the dictionaries when used for the image denoising task and find that no performance deterioration can be observed analogous to the reproduction experiments."}, {"heading": "4.2.1 Raw Pixel Values", "text": "Using the ordinary follower model, we trained two times supercomplete dictionaries with n: = 256 atoms on the normalized pixel values, varying the degree of dictionary thrift from 0.700 to 0.999, resulting in the familiar appearance of dictionaries with Gabor-like filters that resemble the optimal stimulus of visual neurons (Olshausen and Field 1996, 1997) While the filters for small values of \u03c3H were largely small and sharply limited, high thrift resulted in holistic and blurred filters. We used the methods developed by Jones and Palmer for a more detailed analysis of Ringach (1987) and Ringach (2002) for a more detailed analysis, defining a two-dimensional function of gabor as a definition in the equation (1) of Ringach (2002), which is a Gaussian envelope multiplied by a cosmic carrier wave that matches each dictionary with the algorithm of Nelder and Mead (1965)."}, {"heading": "4.2.2 Whitened Image Patches", "text": "This also changes the intuition of similarity measurement in the objective function of EZDL, in which each feature captures a multitude of pixels in the raw patches, resulting in differences in the filter structure, especially in the formation of more low frequency filters.The dictionary shown in the figure was learned from the topographic inference model with an average composition of 3 x 3 neighborhoods. We assign the dictatorial thrift in terms of the number of atoms to n: = 256, arranged on a 16 x 16 grid. The dictionary is very similar to those obtained by topography (Hyv\u00e4rinen et al. 2001, 2009) and the invariant thrift in terms of composition."}, {"heading": "4.2.3 Rank-1 Filters on Raw Pixel Values", "text": "This year is the highest in the history of the country."}, {"heading": "4.3.1 EZDL Results", "text": "Figure 10 illustrates the results obtained from dictionaries produced by Easy Dictionary Learning using dictionary sparseness grade \u03c3H (0.75, 0.85, 0.95, 0.99). A thousand learning epochs were performed, with 30,000 samples presented in each epoch and an initial step size of \u03b70: = 1. During dictionary learning, only the first trivial iteration of a land weaver procedure for sparse code word conclusions was performed by calculating h: = Higgs (W T x) for the ordinary torture model. We first analyzed the effects of performing more land weaver iterations during the image reproduction phase and the effect of variation of the sparse code conclusion grade H by computing. A huge increase in performance can be achieved by using more than one iteration for torture (Fig. 10a). Almost optimal performance is achieved after ten iterations, and one hundred iterations by the method is configured."}, {"heading": "4.3.2 Comparison with Alternative Dictionary Learning Algorithms", "text": "For comparison, we conducted experiments with the Online Dictionary Learning (ODL) algorithm by Maiveau et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010). To deduce sparse code words, ODL minimizes reproduction errors under implicit thriftiness constraints. RLS-DLA uses an external vector selection algorithm for inference, hence explicit constraints such as a target L0 pseudonym can be easily demanded. Both algorithms update the dictionary after each sample presentation. ODL does not require increments to be customized. The key parameter for dictionary thrift here is a number of R > 0 that controls the trade-off between reproduction capabilities and code words. We trained five word economy."}, {"heading": "4.3.3 Comparison of Learning Speed", "text": "In fact, most of us are able to follow the rules that they have imposed on themselves. (...) In fact, it is so that they are able to outdo themselves. (...) In fact, it is so that they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. \"(...)"}, {"heading": "5 Conclusions", "text": "This paper proposed the EZDL algorithm, which measures explicit parsimony constraints in relation to Hoyer's slick parsimony. Predefined parsimony levels are always achieved with an economically enforceable projection operator. Building on a concise representation of the projection, we proved that the projection problem can be formulated as a root-finding problem. We presented a linear time and constant space projection algorithm that is superior to previous approaches to theoretical computational complexity and execution time in real computing machines. EZDL matches dictionaries to measurement data by simple ranking-1 updates. The economical projection serves as the basis for sparse codeword conclusions. Due to the projection efficiency and the absence of complicated gradients, our proposed learning algorithm is significantly faster than the optimized ODL algorithm."}, {"heading": "Appendix: Technical Details and Proofs for Section 2", "text": "This means that we only get involved in situations in which we consider only situations in which the projection (x) is a singleton, in which we also consider y = projM (x).Without loss of generality, we can consider projections from x to M (Deutsch 2001).Since we only consider situations in which projM (x) is a singleton, we can also consider y = projM (x).The projection (x) for a vector x-Rn within the non-negative orthant instead of projS (x) for an arbitrary point x."}], "references": [{"title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Aharon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "Feature Grouping from Spatially Constrained Multiplicative Interaction", "author": ["Felix Bauer", "Roland Memisevic"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Bauer and Memisevic.,? \\Q2013\\E", "shortCiteRegEx": "Bauer and Memisevic.", "year": 2013}, {"title": "The \"Independent Components\" of Natural Scenes are Edge Filters", "author": ["Anthony J. Bell", "Terrence J. Sejnowski"], "venue": "Vision Research,", "citeRegEx": "Bell and Sejnowski.,? \\Q1997\\E", "shortCiteRegEx": "Bell and Sejnowski.", "year": 1997}, {"title": "Nonlinear Programming", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific, Belmont, 2nd edition,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Neural Networks for Pattern Recognition", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Large Scale Online Learning", "author": ["L\u00e9on Bottou", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou and LeCun.,? \\Q2004\\E", "shortCiteRegEx": "Bottou and LeCun.", "year": 2004}, {"title": "Linear Convergence of Iterative Soft-Thresholding", "author": ["Kristian Bredies", "Dirk A. Lorenz"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Bredies and Lorenz.,? \\Q2008\\E", "shortCiteRegEx": "Bredies and Lorenz.", "year": 2008}, {"title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "Best Approximation in Inner Product Spaces", "author": ["Frank Deutsch"], "venue": null, "citeRegEx": "Deutsch.,? \\Q2001\\E", "shortCiteRegEx": "Deutsch.", "year": 2001}, {"title": "Image Deblurring and Super-Resolution by Adaptive Sparse Domain Selection and Adaptive Regularization", "author": ["Weisheng Dong", "Lei Zhang", "Guangming Shi", "Xiaolin Wu"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Dong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2011}, {"title": "De-Noising by Soft-Thresholding", "author": ["David L. Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Donoho.,? \\Q1995\\E", "shortCiteRegEx": "Donoho.", "year": 1995}, {"title": "For Most Large Underdetermined Systems of Linear Equations the Minimal `1-norm Solution Is Also the Sparsest Solution", "author": ["David L. Donoho"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Learning to Sense Sparse Signals: Simultaneous Sensing Matrix and Sparsifying Dictionary Optimization", "author": ["Julio Martin Duarte-Carvajalino", "Guillermo Sapiro"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Duarte.Carvajalino and Sapiro.,? \\Q2009\\E", "shortCiteRegEx": "Duarte.Carvajalino and Sapiro.", "year": 2009}, {"title": "The Approximation of One Matrix by", "author": ["Carl Eckart", "Gale Young"], "venue": "Another of Lower Rank. Psychometrika,", "citeRegEx": "Eckart and Young.,? \\Q1936\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1936}, {"title": "Why Simple Shrinkage Is Still Relevant for Redundant Representations", "author": ["Michael Elad"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Elad.,? \\Q2006\\E", "shortCiteRegEx": "Elad.", "year": 2006}, {"title": "A Mathematical Introduction to Compressive Sensing", "author": ["Simon Foucart", "Holger Rauhut"], "venue": null, "citeRegEx": "Foucart and Rauhut.,? \\Q2013\\E", "shortCiteRegEx": "Foucart and Rauhut.", "year": 2013}, {"title": "A Fast Orthogonal Matching Pursuit Algorithm", "author": ["Mohammad Gharavi-Alkhansari", "Thomas S. Huang"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gharavi.Alkhansari and Huang.,? \\Q1998\\E", "shortCiteRegEx": "Gharavi.Alkhansari and Huang.", "year": 1998}, {"title": "What Every Computer Scientist Should Know About Floating-Point Arithmetic", "author": ["David Goldberg"], "venue": "ACM Computing Surveys,", "citeRegEx": "Goldberg.,? \\Q1991\\E", "shortCiteRegEx": "Goldberg.", "year": 1991}, {"title": "Separable Dictionary Learning", "author": ["Simon Hawe", "Matthias Seibert", "Martin Kleinsteuber"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Hawe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hawe et al\\.", "year": 2013}, {"title": "Mathematics of Digital Images: Creation, Compression, Restoration, Recognition", "author": ["Stuart G. Hoggar"], "venue": null, "citeRegEx": "Hoggar.,? \\Q2006\\E", "shortCiteRegEx": "Hoggar.", "year": 2006}, {"title": "Adaptive Image Compression using Sparse Dictionaries", "author": ["Inbal Horev", "Ori Bryt", "Ron Rubinstein"], "venue": "In Proceedings of the International Conference on Systems, Signals and Image Processing,", "citeRegEx": "Horev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Horev et al\\.", "year": 2012}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints", "author": ["Patrik O. Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoyer.,? \\Q2004\\E", "shortCiteRegEx": "Hoyer.", "year": 2004}, {"title": "Independent Component Analysis Applied to Feature Extraction from Colour and Stereo Images. Network: Computation", "author": ["Patrik O. Hoyer", "Aapo Hyv\u00e4rinen"], "venue": "Neural Systems,", "citeRegEx": "Hoyer and Hyv\u00e4rinen.,? \\Q2000\\E", "shortCiteRegEx": "Hoyer and Hyv\u00e4rinen.", "year": 2000}, {"title": "Receptive Fields of Single Neurones in the Cat\u2019s Striate Cortex", "author": ["David H. Hubel", "Torsten N. Wiesel"], "venue": "Journal of Physiology,", "citeRegEx": "Hubel and Wiesel.,? \\Q1959\\E", "shortCiteRegEx": "Hubel and Wiesel.", "year": 1959}, {"title": "Comparing Measures of Sparsity", "author": ["Niall Hurley", "Scott Rickard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hurley and Rickard.,? \\Q2009\\E", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum Likelihood Estimation", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q1999\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 1999}, {"title": "Emergence of Phase- and ShiftInvariant Features by Decomposition of Natural Images into Independent Feature Subspaces", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen and Hoyer.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Hoyer.", "year": 2000}, {"title": "Topographic Independent Component Analysis", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer", "Mika Inki"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2001}, {"title": "Natural Image Statistics \u2013 A Probabilistic Approach to Early Computational Vision", "author": ["Aapo Hyv\u00e4rinen", "Jarmo Hurri", "Patrik O. Hoyer"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "An Evaluation of the TwoDimensional Gabor Filter Model of Simple Receptive Fields in Cat Striate Cortex", "author": ["Judson P. Jones", "Larry A. Palmer"], "venue": "Journal of Neurophysiology,", "citeRegEx": "Jones and Palmer.,? \\Q1987\\E", "shortCiteRegEx": "Jones and Palmer.", "year": 1987}, {"title": "Learning Invariant Features through Topographic Filter Maps", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Rob Fergus", "Yann LeCun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "The Self-Organizing Map", "author": ["Teuvo Kohonen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Kohonen.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen.", "year": 1990}, {"title": "Dictionary Learning Algorithms for Sparse Representation", "author": ["Kenneth Kreutz-Delgado", "Joseph F. Murray", "Bhaskar D. Rao", "Kjersti Engan", "Te-Won Lee", "Terrence J. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "Kreutz.Delgado et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kreutz.Delgado et al\\.", "year": 2003}, {"title": "Efficient Euclidean Projections in Linear Time", "author": ["Jun Liu", "Jieping Ye"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liu and Ye.,? \\Q2009\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2009}, {"title": "Estimating Unknown Sparsity in Compressed Sensing", "author": ["Miles E. Lopes"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Lopes.,? \\Q2013\\E", "shortCiteRegEx": "Lopes.", "year": 2013}, {"title": "Online Dictionary Learning for Sparse Coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Non-local Sparse Models for Image Restoration", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro", "Andrew Zisserman"], "venue": "In Proceedings of the International Conference on Computer Vision,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "A Simplex Method for Function Minimization", "author": ["John Ashworth Nelder", "Roger Mead"], "venue": "The Computer Journal,", "citeRegEx": "Nelder and Mead.,? \\Q1965\\E", "shortCiteRegEx": "Nelder and Mead.", "year": 1965}, {"title": "Some Theorems on Matrix Differentiation with Special Reference to Kronecker Matrix Products", "author": ["Heinz Neudecker"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Neudecker.,? \\Q1969\\E", "shortCiteRegEx": "Neudecker.", "year": 1969}, {"title": "A Biologically Inspired Algorithm for the Recovery of Shading and Reflectance", "author": ["Adriana Olmos", "Frederick A.A. Kingdom"], "venue": "Images. Perception,", "citeRegEx": "Olmos and Kingdom.,? \\Q2004\\E", "shortCiteRegEx": "Olmos and Kingdom.", "year": 2004}, {"title": "Learning Sparse, Overcomplete Representations of Time-Varying Natural Images", "author": ["Bruno A. Olshausen"], "venue": "In Proceedings of the International Conference on Image Processing,", "citeRegEx": "Olshausen.,? \\Q2003\\E", "shortCiteRegEx": "Olshausen.", "year": 2003}, {"title": "Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": null, "citeRegEx": "Olshausen and Field.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Block Coordinate Descent for Sparse NMF", "author": ["Vamsi K. Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Potluru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Potluru et al\\.", "year": 2013}, {"title": "Numerical Recipes: The Art of Scientific Computing", "author": ["William H. Press", "Saul A. Teukolsky", "William T. Vetterling", "Brian P. Flannery"], "venue": null, "citeRegEx": "Press et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Press et al\\.", "year": 2007}, {"title": "Learning Separable Filters", "author": ["Roberto Rigamonti", "Amos Sironi", "Vincent Lepetit", "Pascal Fua"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex", "author": ["Dario L. Ringach"], "venue": "Journal of Neurophysiology,", "citeRegEx": "Ringach.,? \\Q2002\\E", "shortCiteRegEx": "Ringach.", "year": 2002}, {"title": "Thirteen Ways to Look at the Correlation Coefficient", "author": ["Joseph Lee Rodgers", "W. Alan Nicewander"], "venue": "The American Statistician,", "citeRegEx": "Rodgers and Nicewander.,? \\Q1988\\E", "shortCiteRegEx": "Rodgers and Nicewander.", "year": 1988}, {"title": "Sparse Coding via Thresholding and Local Competition in Neural Circuits", "author": ["Christopher J. Rozell", "Don H. Johnson", "Richard G. Baraniuk", "Bruno A. Olshausen"], "venue": "Neural Computation,", "citeRegEx": "Rozell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rozell et al\\.", "year": 2008}, {"title": "Recursive Least Squares Dictionary Learning Algorithm", "author": ["Karl Skretting", "Kjersti Engan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Skretting and Engan.,? \\Q2010\\E", "shortCiteRegEx": "Skretting and Engan.", "year": 2010}, {"title": "Image Compression Using Learned Dictionaries by RLS-DLA and Compared with K-SVD", "author": ["Karl Skretting", "Kjersti Engan"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Skretting and Engan.,? \\Q2011\\E", "shortCiteRegEx": "Skretting and Engan.", "year": 2011}, {"title": "First Results on Uniqueness of Sparse Non-Negative Matrix Factorization", "author": ["Fabian J. Theis", "Kurt Stadlthanner", "Toshihisa Tanaka"], "venue": "In Proceedings of the European Signal Processing Conference,", "citeRegEx": "Theis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2005}, {"title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "author": ["Markus Thom", "G\u00fcnther Palm"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Thom and Palm.,? \\Q2013\\E", "shortCiteRegEx": "Thom and Palm.", "year": 2013}, {"title": "Learning Sparse Representations of Depth", "author": ["Ivana To\u0161i\u0107", "Bruno A. Olshausen", "Benjamin J. Culpepper"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "To\u0161i\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "To\u0161i\u0107 et al\\.", "year": 2011}, {"title": "Iterative Methods for the Solution of Equations", "author": ["Joe Fred Traub"], "venue": "Prentice-Hall, Englewood Cliffs,", "citeRegEx": "Traub.,? \\Q1964\\E", "shortCiteRegEx": "Traub.", "year": 1964}, {"title": "Independent Component Analysis of Natural Image Sequences Yields Spatio-Temporal Filters Similar to Simple Cells in Primary Visual Cortex", "author": ["J. Hans van Hateren", "Daniel L. Ruderman"], "venue": "Proceedings of the Royal Society B,", "citeRegEx": "Hateren and Ruderman.,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman.", "year": 1998}, {"title": "Mean Squared Error: Love It or Leave It? A New Look at Signal Fidelity Measures", "author": ["Zhou Wang", "Alan C. Bovik"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Wang and Bovik.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Bovik.", "year": 2009}, {"title": "Image Compression Using the Discrete Cosine Transform", "author": ["Andrew B. Watson"], "venue": "The Mathematica Journal,", "citeRegEx": "Watson.,? \\Q1994\\E", "shortCiteRegEx": "Watson.", "year": 1994}, {"title": "Characterizing the Sparseness of Neural Codes. Network: Computation", "author": ["Ben Willmore", "David J. Tolhurst"], "venue": "Neural Systems,", "citeRegEx": "Willmore and Tolhurst.,? \\Q2001\\E", "shortCiteRegEx": "Willmore and Tolhurst.", "year": 2001}, {"title": "The General Inefficiency of Batch Training for Gradient Descent Learning", "author": ["D. Randall Wilson", "Tony R. Martinez"], "venue": "Neural Networks,", "citeRegEx": "Wilson and Martinez.,? \\Q2003\\E", "shortCiteRegEx": "Wilson and Martinez.", "year": 2003}, {"title": "Image Super-Resolution via Sparse Representation", "author": ["Jianchao Yang", "John Wright", "Thomas S. Huang", "Yi Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Coupled Dictionary Training for Image Super-Resolution", "author": ["Jianchao Yang", "Zhaowen Wang", "Zhe Lin", "Scott Cohen", "Thomas Huang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Dictionary Optimization for Block-Sparse Representations", "author": ["Lihi Zelnik-Manor", "Kevin Rosenblum", "Yonina C. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Zelnik.Manor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zelnik.Manor et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 27, "context": "zation (Hyv\u00e4rinen et al. 2001; Kavukcuoglu et al. 2009).", "startOffset": 7, "endOffset": 55}, {"referenceID": 30, "context": "zation (Hyv\u00e4rinen et al. 2001; Kavukcuoglu et al. 2009).", "startOffset": 7, "endOffset": 55}, {"referenceID": 9, "context": "plications that benefit from the efficiency gained through sparseness are as diverse as deblurring (Dong et al. 2011), super-resolution (Yang et al.", "startOffset": 99, "endOffset": 117}, {"referenceID": 9, "context": "2011), super-resolution (Yang et al. 2010, 2012; Dong et al. 2011),", "startOffset": 24, "endOffset": 66}, {"referenceID": 20, "context": "compression (Skretting and Engan 2011; Horev et al. 2012), and depth estimation (To\u0161i\u0107 et al.", "startOffset": 12, "endOffset": 57}, {"referenceID": 53, "context": "2012), and depth estimation (To\u0161i\u0107 et al. 2011).", "startOffset": 28, "endOffset": 47}, {"referenceID": 21, "context": "Throughout this paper, we will use the smooth, normalized sparseness measure \u03c3 proposed by Hoyer (2004):", "startOffset": 91, "endOffset": 104}, {"referenceID": 43, "context": "It has been employed successfully for dictionary learning (Hoyer 2004; Potluru et al. 2013), and its smoothness results in improved generalization capabilities in classification tasks compared to when the L0 pseudo-norm is used (Thom and Palm 2013).", "startOffset": 58, "endOffset": 91}, {"referenceID": 32, "context": "A common approach to dictionary learning is the minimization of the reproduction error between the original samples from a learning set and their approximations provided by a linear generative model under sparseness constraints (Olshausen and Field 1996, 1997; Kreutz-Delgado et al. 2003; Mairal et al. 2009a).", "startOffset": 228, "endOffset": 309}, {"referenceID": 51, "context": "Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013).", "startOffset": 77, "endOffset": 149}, {"referenceID": 43, "context": "Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013).", "startOffset": 77, "endOffset": 149}, {"referenceID": 21, "context": "This paper studies dictionary learning under explicit sparseness constraints with respect to Hoyer\u2019s sparseness measure \u03c3 . A major part of this work is devoted to the efficient algorithmic computation of the sparseness-enforcing projection operator, which is an integral part in efficient dictionary learning. Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013). Only Thom and Palm (2013) provided a complete and mathematically satisfactory proof of correctness for their algorithm.", "startOffset": 93, "endOffset": 488}, {"referenceID": 21, "context": "Existing approaches to dictionary learning that feature explicit sparseness constraints can be categorized into ones that use Hoyer\u2019s \u03c3 and ones that employ the L0 pseudonorm. Hoyer (2004) and Potluru et al.", "startOffset": 126, "endOffset": 189}, {"referenceID": 21, "context": "Existing approaches to dictionary learning that feature explicit sparseness constraints can be categorized into ones that use Hoyer\u2019s \u03c3 and ones that employ the L0 pseudonorm. Hoyer (2004) and Potluru et al. (2013) considered matrix factorization frameworks subject to \u03c3 constraints, with", "startOffset": 126, "endOffset": 215}, {"referenceID": 52, "context": "Thom and Palm (2013) designed sparse code words as the result of", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 91}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 119}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 144}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time. Zelnik-Manor et al. (2012) consider block-sparse representations, here the signals are assumed to reside in the union of few subspaces.", "startOffset": 70, "endOffset": 362}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time. Zelnik-Manor et al. (2012) consider block-sparse representations, here the signals are assumed to reside in the union of few subspaces. Duarte-Carvajalino and Sapiro (2009) propose to simultaneously learn the dictionary and the sensing matrix from example image data, which results in improved reconstruction results in compressed sensing scenarios.", "startOffset": 70, "endOffset": 508}, {"referenceID": 21, "context": "Our technique aims at dictionary learning under explicit sparseness constraints in terms of Hoyer\u2019s sparseness measure \u03c3 using a simple, fast-tocompute and biologically plausible Hebbian-like learning rule. For each presented learning sample, the sparsenessenforcing projection operator has to be carried out. The ability to perform projections efficiently makes the proposed learning algorithm particularly efficient: 30 % less training time is required in comparison to the optimized Online Dictionary Learning method of Mairal et al. (2009a).", "startOffset": 92, "endOffset": 545}, {"referenceID": 32, "context": "The required number of bisection iterations is then less than dlog2(x2nd-max/\u03b4 )e, see Liu and Ye (2009). This number is bounded from above regardless of the dimensionality of the input vector since x2nd-max is upper-bounded and \u03b4 is lower-bounded due to finite machine precision (Goldberg 1991).", "startOffset": 87, "endOffset": 105}, {"referenceID": 28, "context": "and biologically plausible Hebbian-like learning rule (Hyv\u00e4rinen et al. 2009) results:", "startOffset": 54, "endOffset": 77}, {"referenceID": 27, "context": "Topographic organization of the dictionary\u2019s atoms similar to Self-organizing Maps (Kohonen 1990) or Topographic Independent Component Analysis (Hyv\u00e4rinen et al. 2001) can be achieved in a straightforward way with EZDL using alternative inference models proposed in this section.", "startOffset": 144, "endOffset": 167}, {"referenceID": 28, "context": "arbitrarily small (Hyv\u00e4rinen et al. 2009).", "startOffset": 18, "endOffset": 41}, {"referenceID": 32, "context": "This normalization step is also common in a multitude of alternative dictionary learning algorithms (Kreutz-Delgado et al. 2003; Hoyer 2004; Mairal et al. 2009a; Skretting and Engan 2010).", "startOffset": 100, "endOffset": 187}, {"referenceID": 44, "context": "new positions are always located within well-defined intervals (Press et al. 2007).", "startOffset": 63, "endOffset": 82}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al.", "startOffset": 23, "endOffset": 36}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al. (2013), and Thom and Palm (2013) were implemented using the", "startOffset": 23, "endOffset": 59}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al. (2013), and Thom and Palm (2013) were implemented using the", "startOffset": 23, "endOffset": 85}, {"referenceID": 21, "context": "5 Speed-ups relative to the original algorithm of Hoyer (2004) obtained using alternative algorithms.", "startOffset": 50, "endOffset": 63}, {"referenceID": 21, "context": "50 with respect to Hoyer\u2019s \u03c3 . This initial projection better reflects the situation in practice where not completely random vectors have to be processed. Next, all four algorithms were used to compute projections with a target sparseness degree of \u03c3\u2217 := 0.90 and their run time was measured. The original algorithm of Hoyer (2004) was the slowest, so by taking the ratio of the run times of the other algorithms to the run time of that slowest algorithm the relative speed-up was obtained.", "startOffset": 19, "endOffset": 332}, {"referenceID": 42, "context": "While the speed-up of the algorithms of Potluru et al. (2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 40, "endOffset": 62}, {"referenceID": 42, "context": "While the speed-up of the algorithms of Potluru et al. (2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 40, "endOffset": 87}, {"referenceID": 21, "context": "(2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 70, "endOffset": 83}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al.", "startOffset": 58, "endOffset": 71}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al. (2013) and Thom and Palm (2013).", "startOffset": 58, "endOffset": 94}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al. (2013) and Thom and Palm (2013). Because of this appealing asymptotic behavior, there", "startOffset": 58, "endOffset": 119}, {"referenceID": 25, "context": "4 of Hyv\u00e4rinen et al. (2009) with 128 principal components.", "startOffset": 5, "endOffset": 29}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis.", "startOffset": 33, "endOffset": 57}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis.", "startOffset": 33, "endOffset": 76}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis. In doing so, a two-dimensional Gabor function as defined in Equation (1) of Ringach (2002), that is a Gaussian envelope multiplied with a cosine carrier wave, was fit to each dictionary atom using the algorithm of Nelder and Mead (1965).", "startOffset": 33, "endOffset": 197}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis. In doing so, a two-dimensional Gabor function as defined in Equation (1) of Ringach (2002), that is a Gaussian envelope multiplied with a cosine carrier wave, was fit to each dictionary atom using the algorithm of Nelder and Mead (1965). We verified that these Gabor fits accurately described the atoms, which confirmed the Gabor-like nature of the filters.", "startOffset": 33, "endOffset": 343}, {"referenceID": 28, "context": "Principal Component Analysis, low-frequency atoms here minimize the reproduction error best when only very few effective atoms are allowed (Hyv\u00e4rinen et al. 2009; Olshausen and Field 1996).", "startOffset": 139, "endOffset": 188}, {"referenceID": 48, "context": "A major component in the learning algorithm is the sparseness projection, enforcing local competition among the atoms (Rozell et al. 2008) due to its absolute order-preservation property (Thom and Palm 2013, Lemma 12).", "startOffset": 118, "endOffset": 138}, {"referenceID": 28, "context": "Whitening as a pre-processing step helps to reduce sampling artifacts and decorrelates the input data (Hyv\u00e4rinen et al. 2009).", "startOffset": 102, "endOffset": 125}, {"referenceID": 30, "context": "Sparse Decomposition (Kavukcuoglu et al. 2009).", "startOffset": 21, "endOffset": 46}, {"referenceID": 28, "context": "Overcomplete representations are not inherently possible with plain Independent Component Analysis (Bell and Sejnowski 1997; Hyv\u00e4rinen et al. 2009) which limits its expressiveness, a restriction that does not hold for EZDL.", "startOffset": 99, "endOffset": 147}, {"referenceID": 1, "context": "This stands in contrast to the discussion from Bauer and Memisevic (2013) where the necessity of a nonlinearity such as multiplicative interaction or pooling with respect to the Euclidean norm was assumed.", "startOffset": 47, "endOffset": 74}, {"referenceID": 18, "context": "This was also observed recently by Hawe et al. (2013) who considered a tensor decomposition", "startOffset": 35, "endOffset": 54}, {"referenceID": 45, "context": "of the dictionary, and by Rigamonti et al. (2013) who minimized the Schatten 1-norm of the atoms.", "startOffset": 26, "endOffset": 50}, {"referenceID": 35, "context": "Further, the EZDL dictionaries are compared with the results of the Online Dictionary Learning algorithm by Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algo-", "startOffset": 108, "endOffset": 130}, {"referenceID": 49, "context": "rithm by Skretting and Engan (2010) in terms of reproduction quality and learning speed.", "startOffset": 9, "endOffset": 36}, {"referenceID": 35, "context": "(a) Results of the Online Dictionary Learning (ODL) algorithm by Mairal et al. (2009a), where \u03bb trades off between sparseness and reproduction capabilities in a model with implicit sparseness constraints.", "startOffset": 65, "endOffset": 87}, {"referenceID": 49, "context": "(b) Results of the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010). Here, \u03b6 denotes the target L0 pseudo-norm of the code words for inference.", "startOffset": 86, "endOffset": 113}, {"referenceID": 35, "context": "line Dictionary Learning (ODL) algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010).", "startOffset": 44, "endOffset": 66}, {"referenceID": 35, "context": "line Dictionary Learning (ODL) algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010).", "startOffset": 44, "endOffset": 168}, {"referenceID": 35, "context": "Inference Sparseness \u03c3 I ODL by Mairal et al. (2009a)", "startOffset": 32, "endOffset": 54}, {"referenceID": 49, "context": "RLS-DLA by Skretting and Engan (2010)", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images.", "startOffset": 137, "endOffset": 159}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images.", "startOffset": 137, "endOffset": 251}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images. In a final experiment, we investigated the suitability of EZDL dictionaries for image denoising using the image enhancement procedure proposed by Mairal et al. (2009b).", "startOffset": 137, "endOffset": 474}, {"referenceID": 32, "context": "This procedure is quite robust if the input data is noisy, since sparseness provides a strong prior which well regularizes this ill-posed inverse problem (Kreutz-Delgado et al. 2003; Foucart and Rauhut 2013).", "startOffset": 154, "endOffset": 207}, {"referenceID": 35, "context": "The denoising approach of Mairal et al. (2009b) also provides the possibility of dictionary adaptation while denoising concrete input data.", "startOffset": 26, "endOffset": 48}, {"referenceID": 35, "context": "Standard Deviation of Gaussian Noise ODL by Mairal et al. (2009a)", "startOffset": 44, "endOffset": 66}, {"referenceID": 49, "context": "RLS-DLA by Skretting and Engan (2010)", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "14 Denoising performance in terms of the peak signal-to-noise ratio (PSNR) using the denoising method proposed by Mairal et al. (2009b). There is only a small performance difference between dictionaries trained with ODL, RLS-DLA and EZDL.", "startOffset": 114, "endOffset": 136}, {"referenceID": 33, "context": "The denoising procedure of Mairal et al. (2009b) aims at reproduction capabilities as well, with the modification of employing noisy samples as input.", "startOffset": 27, "endOffset": 49}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al.", "startOffset": 14, "endOffset": 61}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al. (2012) which also use problem formulations based on the reproduction error can hence be", "startOffset": 14, "endOffset": 85}, {"referenceID": 8, "context": "the set of Euclidean projections of x onto M (Deutsch 2001). Since we only consider situations in which projM(x) = {y} is a singleton, we may also write y = projM(x). Without loss of generality, we can compute projT (x) for a vector x \u2208 R\u22650 within the non-negative orthant instead of projS(x) for an arbitrary point x \u2208 Rn to yield sparseness-enforcing projections, where T and S are as defined in Sect. 2. First, the actual scale is irrelevant as we can simply re-scale the result of the projection (Thom and Palm 2013, Remark 5). Second, the constraint that the projection lies in the non-negative orthant R\u22650 can easily be handled by flipping the signs of certain coordinates (Thom and Palm 2013, Lemma 11). Finally, all entries of x can be assumed non-negative with Corollary 19 from Thom and Palm (2013). We note that T is non-convex because of the \u2016s\u20162 = \u03bb2 constraint.", "startOffset": 46, "endOffset": 809}, {"referenceID": 8, "context": "the set of Euclidean projections of x onto M (Deutsch 2001). Since we only consider situations in which projM(x) = {y} is a singleton, we may also write y = projM(x). Without loss of generality, we can compute projT (x) for a vector x \u2208 R\u22650 within the non-negative orthant instead of projS(x) for an arbitrary point x \u2208 Rn to yield sparseness-enforcing projections, where T and S are as defined in Sect. 2. First, the actual scale is irrelevant as we can simply re-scale the result of the projection (Thom and Palm 2013, Remark 5). Second, the constraint that the projection lies in the non-negative orthant R\u22650 can easily be handled by flipping the signs of certain coordinates (Thom and Palm 2013, Lemma 11). Finally, all entries of x can be assumed non-negative with Corollary 19 from Thom and Palm (2013). We note that T is non-convex because of the \u2016s\u20162 = \u03bb2 constraint. Moreover, T 6= / 0 for all target sparseness degrees \u03c3\u2217 \u2208 (0, 1) which we show here by construction (see also Remark 18 in Thom and Palm (2013) for further details): Let \u03c8 := ( \u03bb1\u2212 \u221a n\u03bb 2 2\u2212\u03bb 2 1/ \u221a n\u22121 ) /n > 0 and", "startOffset": 46, "endOffset": 1020}, {"referenceID": 51, "context": "We first present a constructive proof based on a geometric analysis conducted in Thom and Palm (2013), which contributes to deepening our insight into the involved computations.", "startOffset": 81, "endOffset": 102}, {"referenceID": 52, "context": "With Theorem 2 and Appendix D from Thom and Palm (2013) there exists a finite sequence of index sets I1, .", "startOffset": 35, "endOffset": 56}, {"referenceID": 52, "context": "The expressions for the individual projections are given in Lemma 13, Lemma 17, Proposition 24, and Lemma 30, respectively, in Thom and Palm (2013). Let I0 := {1, .", "startOffset": 127, "endOffset": 148}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162.", "startOffset": 66, "endOffset": 87}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162.", "startOffset": 66, "endOffset": 133}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant.", "startOffset": 66, "endOffset": 550}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b).", "startOffset": 66, "endOffset": 864}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied.", "startOffset": 66, "endOffset": 1529}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, .", "startOffset": 66, "endOffset": 1883}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, . . . ,h}. Let us now turn to (b). From the last paragraph, we know that \u03b4 = \u03b2 j+1/\u03b2 j for all j \u2208 {0, . . . ,h\u22121} for the projections onto LI j+1 . On the other hand, we have that \u2016r( j+1)\u2212mI j+1\u20162 = \u2016r( j+1)\u20162\u2212\u03bb 2 1/d j+1 from the proof of Lemma 30(a) from Thom and Palm (2013), and \u2016r( j+ 1)\u20162 \u2264 \u03bb 2 2 holds from the proof of Lemma 28(f) in Thom and Palm (2013), so \u03b4 \u2265 1 which implies \u03b20\u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h.", "startOffset": 66, "endOffset": 2200}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, . . . ,h}. Let us now turn to (b). From the last paragraph, we know that \u03b4 = \u03b2 j+1/\u03b2 j for all j \u2208 {0, . . . ,h\u22121} for the projections onto LI j+1 . On the other hand, we have that \u2016r( j+1)\u2212mI j+1\u20162 = \u2016r( j+1)\u20162\u2212\u03bb 2 1/d j+1 from the proof of Lemma 30(a) from Thom and Palm (2013), and \u2016r( j+ 1)\u20162 \u2264 \u03bb 2 2 holds from the proof of Lemma 28(f) in Thom and Palm (2013), so \u03b4 \u2265 1 which implies \u03b20\u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h.", "startOffset": 66, "endOffset": 2285}, {"referenceID": 3, "context": "1 from Bertsekas (1999) guarantees the existence of Lagrange multipliers \u03b1\u0303, \u03b2\u0303 \u2208 R and \u03b3\u0303 \u2208Rn with L\u2032(p, \u03b1\u0303, \u03b2\u0303 , \u03b3\u0303) = 0, \u03b3\u0303i \u2265 0 for all i \u2208 {1, .", "startOffset": 7, "endOffset": 24}, {"referenceID": 3, "context": "1 from Bertsekas (1999) guarantees the existence of Lagrange multipliers \u03b1\u0303, \u03b2\u0303 \u2208 R and \u03b3\u0303 \u2208Rn with L\u2032(p, \u03b1\u0303, \u03b2\u0303 , \u03b3\u0303) = 0, \u03b3\u0303i \u2265 0 for all i \u2208 {1, . . . ,n} and \u03b3\u0303i = 0 for i \u2208 I. Assume \u03b2\u0303 = \u22121, then 2x = \u03b1\u0303 \u00b7 e\u2212 \u03b3\u0303 since the derivative of L must vanish. Hence xi = \u03b1\u0303/2 for all i \u2208 I, and therefore { pi | i \u2208 I } is a singleton with Remark 10 from Thom and Palm (2013) as p was assumed unique and T is permutation-invariant.", "startOffset": 7, "endOffset": 373}, {"referenceID": 43, "context": "An alternative is the method proposed by Potluru et al. (2013), where the input vector is sorted and then each possible candidate for I is checked.", "startOffset": 41, "endOffset": 63}, {"referenceID": 53, "context": "The concrete variant is chosen by the parameter \"solver\" of Algorithm 3, implementation details can be found in Traub (1964) and Press et al.", "startOffset": 112, "endOffset": 125}, {"referenceID": 44, "context": "The concrete variant is chosen by the parameter \"solver\" of Algorithm 3, implementation details can be found in Traub (1964) and Press et al. (2007). Here, the root-finding loop starting at Line 5 is terminated once Algorithm 2 indicates that the correct interval for exact computation of the zero \u03b1\u2217 has been identified.", "startOffset": 129, "endOffset": 149}], "year": 2016, "abstractText": "Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection\u2019s result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is Communicated by Julien Mairal, Francis Bach, Michael Elad. M. Thom (B) driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: markus.thom@uni-ulm.de M. Rapp driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: matthias.rapp@uni-ulm.de G. Palm Institute of Neural Information Processing Ulm University, 89081 Ulm, Germany e-mail: guenther.palm@uni-ulm.de therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.", "creator": "LaTeX with hyperref package"}}}