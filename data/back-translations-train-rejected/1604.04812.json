{"id": "1604.04812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2016", "title": "Structured Sparse Convolutional Autoencoder", "abstract": "This paper aims to improve the feature learning in Convolutional Networks (Convnet) by capturing the structure of objects. A new sparsity function is imposed on the extracted featuremap to capture the structure and shape of the learned object, extracting interpretable features to improve the prediction performance. The proposed algorithm is based on organizing the activation within and across featuremap by constraining the node activities through $\\ell_{2}$ and $\\ell_{1}$ normalization in a structured form.", "histories": [["v1", "Sun, 17 Apr 2016 00:26:57 GMT  (7747kb,D)", "http://arxiv.org/abs/1604.04812v1", null], ["v2", "Thu, 1 Dec 2016 17:40:29 GMT  (0kb,I)", "http://arxiv.org/abs/1604.04812v2", "The paper need some improvements"], ["v3", "Mon, 2 Jan 2017 18:33:43 GMT  (7747kb,D)", "http://arxiv.org/abs/1604.04812v3", "The paper need some improvements"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["ehsan hosseini-asl"], "accepted": false, "id": "1604.04812"}, "pdf": {"name": "1604.04812.pdf", "metadata": {"source": "CRF", "title": "STRUCTURED SPARSE CONVOLUTIONAL AUTOEN-", "authors": ["Ehsan Hosseini-Asl", "Jacek M. Zurada"], "emails": ["ehsan.hosseiniasl@louisville.edu", "jacek.zurada@louisville.edu"], "sections": [{"heading": null, "text": "This work aims to improve the Learning in Convolutional Networks (Convnet) feature by capturing the structure of objects. A new sparsity function is imposed on the extracted feature reap to capture the structure and shape of the learned object, and to extract interpretable features to improve prediction capability.The proposed algorithm is based on organizing activation within and between feature remap by limiting node activities through a structured '2' and '1 normalization."}, {"heading": "1 INTRODUCTION", "text": "They employ hierarchical layers of combined folding and pooling to extract compressed features that capture the intra-class variations between images acquired by Convnet. The purpose of using pooling via neighborhood activations in Convnet's features is to break the spatial correlation of adjacent pixels and improve the scale and translation invariant features learned by Convnet. This also helps in applying learning filters for generic features based on a low level of concepts, such as edge detectors, geometric shapes, and object class Krizhevsky et al. (2012); Donahue et al. (2013); Zeiler et al. (2014).Several regulatory techniques have been proposed to improve functional extraction in Counvnet."}, {"heading": "2 MODEL", "text": "In this case, it is only a matter of time before a decision is made, whether a decision is made, whether a decision is made or not. (...) In this case, it is only a matter of time before a decision is made. (...) We refer to hkij assingle neurons, how a decision is made. (...) We refer to hkij assingle neurons, how a decision is made. (...) We refer to hkij assingle neurons, how a decision is made. (...) We refer to hkij assingle neuron, how a decision is made. (...) We refer to hkij assingle neurons, how a decision is made. (...)"}, {"heading": "3 EXPERIMENTS", "text": "We used Theano Bastien et al. (2012) and Pylearn Goodfellow et al. (2013a), on Amazon EC2 g2.8xlarge instances with GPU GRID K520 for our experiments."}, {"heading": "3.1 REDUCING DEAD FILTERS", "text": "To compare the performance of our model in minimizing dead filters by learning sparse and local filters, the trained filters of the MNIST data are compared between CAE and SSCAE with and without pooling layer in Fig. 4. Fig. 4 (a) (c) shows that CAE with and without pooling layer learn some delta filters that only provide an identity function. However, the sparsity function used in SSCAE attempts to reduce delta filter extraction by controlling activation across feature maps, as shown in Fig. (d)."}, {"heading": "3.2 IMPROVING LEARNING OF RECONSTRUCTION", "text": "To investigate the effects of structured sparseness on learning filters by reconstruction, the performance of CAE and SSCAE on SVHN datasets is compared, as shown in Figure 6. To show the performance of structured sparseness on reconstruction, a small CAE with 8 filters is trained on SVHN datasets. Figure 6 (a) shows the performance of CAE after training, which does not extract edge-like filters and leads to poor reconstruction. Figure 8 also shows the learned 16 encoding and decryption filters on small NORB datasets, where structured sparseness improves the extraction of localized and edge-like filters. However, SSCAE exceeds CAE in reconstruction based on learned edge-like filters. The selected feature remake of both models is shown in Figure 7 (a) (b). The convergence rate of reconstruction optimization for CAE and SAS (SAS) (SA5) (SMAR) is also shown in Figure 7 (a)."}], "references": [{"title": "Structured sparsity through convex optimization", "author": ["Bach", "Francis", "Jenatton", "Rodolphe", "Mairal", "Julien", "Obozinski", "Guillaume"], "venue": "Statistical Science,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research library", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "In Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing,", "citeRegEx": "Hoyer,? \\Q2002\\E", "shortCiteRegEx": "Hoyer", "year": 2002}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Hoyer,? \\Q2004\\E", "shortCiteRegEx": "Hoyer", "year": 2004}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A winner-take-all method for training sparse convolutional autoencoders", "author": ["Makhzani", "Alireza", "Frey", "Brendan"], "venue": "arXiv preprint arXiv:1409.2752,", "citeRegEx": "Makhzani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Makhzani et al\\.", "year": 2014}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "In CS294A Lecture notes, URL https://web.stanford.edu/ class/cs294a/sparseAutoencoder_2011new.pdf,", "citeRegEx": "Ng,? \\Q2011\\E", "shortCiteRegEx": "Ng", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Olshausen", "B. A"], "venue": null, "citeRegEx": "Olshausen and A,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and A", "year": 1996}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Deconvolutional networks", "author": ["Zeiler", "Matthew D", "Krishnan", "Dilip", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Convolutional net (Convnet) LeCun et al. (1998) have shown to be powerful models in extracting rich features from high-dimensional images.", "startOffset": 28, "endOffset": 48}, {"referenceID": 1, "context": "This also helps in learning filters for generic feature extraction of lowmid-high level of concepts, such as edge detectors, geometric shapes, and object class Krizhevsky et al. (2012); Donahue et al.", "startOffset": 160, "endOffset": 185}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014).", "startOffset": 8, "endOffset": 52}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters.", "startOffset": 8, "endOffset": 76}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training.", "startOffset": 8, "endOffset": 293}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets.", "startOffset": 8, "endOffset": 607}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step.", "startOffset": 8, "endOffset": 929}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step. Sparse feature learning is a common method for compressed feature extraction in shallow encoderdecoder-based networks, i.e. in sparse coding Hoyer (2002; 2004); Olshausen et al. (1996); Olshausen & Field (1997), in Autoencoders (AE) Ng (2011), and in Restricted Boltzmann Machines (RBM) Poultney et al.", "startOffset": 8, "endOffset": 1308}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step. Sparse feature learning is a common method for compressed feature extraction in shallow encoderdecoder-based networks, i.e. in sparse coding Hoyer (2002; 2004); Olshausen et al. (1996); Olshausen & Field (1997), in Autoencoders (AE) Ng (2011), and in Restricted Boltzmann Machines (RBM) Poultney et al.", "startOffset": 8, "endOffset": 1334}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step. Sparse feature learning is a common method for compressed feature extraction in shallow encoderdecoder-based networks, i.e. in sparse coding Hoyer (2002; 2004); Olshausen et al. (1996); Olshausen & Field (1997), in Autoencoders (AE) Ng (2011), and in Restricted Boltzmann Machines (RBM) Poultney et al.", "startOffset": 8, "endOffset": 1366}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step. Sparse feature learning is a common method for compressed feature extraction in shallow encoderdecoder-based networks, i.e. in sparse coding Hoyer (2002; 2004); Olshausen et al. (1996); Olshausen & Field (1997), in Autoencoders (AE) Ng (2011), and in Restricted Boltzmann Machines (RBM) Poultney et al. (2006); Ranzato et al.", "startOffset": 8, "endOffset": 1433}, {"referenceID": 1, "context": "(2012); Donahue et al. (2013); Zeiler et al. (2010); Zeiler & Fergus (2014). Several regularization techniques have been proposed to improve feature extraction in Counvnet and to overcome overfitting in large deep networks with many parameters. A dropout technique in Srivastava et al. (2014) is based on randomly dropping hidden units with its connnection during training to avoid co-adaptaion or redundant filter training. This method resemble averaging over ensemble of sub-models, where each sub-model is trained based on a subset of parameters. A maxout neuron is proposed in Goodfellow et al. (2013b) while a maxout neuron, with the maximum of activity across featuremaps is computed in Counvnets. Maxout networks have shown to improve the classification performance by building a convex an unbounded activation function, which prevents learning dead filters. A winner-take-all method is employed in Makhzani & Frey (2014) to reduce or eliminate redundant and delta type filters in pretraining of Counvnet using Convolutional AutoEncoder (CAE), by taking the maximum activity inside featuremap in each training step. Sparse feature learning is a common method for compressed feature extraction in shallow encoderdecoder-based networks, i.e. in sparse coding Hoyer (2002; 2004); Olshausen et al. (1996); Olshausen & Field (1997), in Autoencoders (AE) Ng (2011), and in Restricted Boltzmann Machines (RBM) Poultney et al. (2006); Ranzato et al. (2007). Bach et al.", "startOffset": 8, "endOffset": 1456}, {"referenceID": 0, "context": "Bach et al. Bach et al. (2012) organize `1 sparsity in a structured form to capture interpretable features and improve prediction performance of the model.", "startOffset": 0, "endOffset": 31}, {"referenceID": 0, "context": "Bach et al. Bach et al. (2012) organize `1 sparsity in a structured form to capture interpretable features and improve prediction performance of the model. In this paper, we present a novel Structured Model of sparse feature extraction in CAE that improves the performance of feature extraction by regularizing the distribution of activities inside and across featuremaps. We employ the idea of sparse filtering Ngiam et al. (2011) to regularize the activity across featuremaps and to improve sparsity within and across featuremaps.", "startOffset": 0, "endOffset": 432}, {"referenceID": 9, "context": "Notice that the sparseness of the features (in the `1 sense) is maximized when the examples are on the axes Ngiam et al. (2011). In CAE with n encoding filters, the featuremaps hk\u2208n are computed based on a convolution/pooling/nonlinearity layer, with nonlinear function applied on the pooled activation of convolution layer, as in Eq.", "startOffset": 108, "endOffset": 128}, {"referenceID": 9, "context": "1, as proposed in Ngiam et al. (2011), and as shown in Fig.", "startOffset": 18, "endOffset": 38}, {"referenceID": 1, "context": "3 EXPERIMENTS We used Theano Bastien et al. (2012) and Pylearn Goodfellow et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 1, "context": "3 EXPERIMENTS We used Theano Bastien et al. (2012) and Pylearn Goodfellow et al. (2013a), on Amazon EC2 g2.", "startOffset": 29, "endOffset": 89}], "year": 2017, "abstractText": "This paper aims to improve the feature learning in Convolutional Networks (Convnet) by capturing the structure of objects. A new sparsity function is imposed on the extracted featuremap to capture the structure and shape of the learned object, extracting interpretable features to improve the prediction performance. The proposed algorithm is based on organizing the activation within and across featuremap by constraining the node activities through `2 and `1 normalization in a structured form.", "creator": "LaTeX with hyperref package"}}}