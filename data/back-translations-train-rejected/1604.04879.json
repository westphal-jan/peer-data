{"id": "1604.04879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2016", "title": "Mahalanobis Distance Metric Learning Algorithm for Instance-based Data Stream Classification", "abstract": "With the massive data challenges nowadays and the rapid growing of technology, stream mining has recently received considerable attention. To address the large number of scenarios in which this phenomenon manifests itself suitable tools are required in various research fields. Instance-based data stream algorithms generally employ the Euclidean distance for the classification task underlying this problem. A novel way to look into this issue is to take advantage of a more flexible metric due to the increased requirements imposed by the data stream scenario. In this paper we present a new algorithm that learns a Mahalanobis metric using similarity and dissimilarity constraints in an online manner. This approach hybridizes a Mahalanobis distance metric learning algorithm and a k-NN data stream classification algorithm with concept drift detection. First, some basic aspects of Mahalanobis distance metric learning are described taking into account key properties as well as online distance metric learning algorithms. Second, we implement specific evaluation methodologies and comparative metrics such as Q statistic for data stream classification algorithms. Finally, our algorithm is evaluated on different datasets by comparing its results with one of the best instance-based data stream classification algorithm of the state of the art. The results demonstrate that our proposal is better", "histories": [["v1", "Sun, 17 Apr 2016 15:01:51 GMT  (985kb,D)", "http://arxiv.org/abs/1604.04879v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jorge luis rivero perez", "bernardete ribeiro", "carlos morell perez"], "accepted": false, "id": "1604.04879"}, "pdf": {"name": "1604.04879.pdf", "metadata": {"source": "CRF", "title": "Mahalanobis Distance Metric Learning Algorithm for Instance-based Data Stream Classification", "authors": ["Jorge Luis Rivero Perez", "Bernardete Ribeiro", "Carlos Morell Perez"], "emails": ["jlrivero@dei.uc.pt", "bribeiro@dei.uc.pt", "cmorellp@uclv.edu.cu"], "sections": [{"heading": null, "text": "In recent years, it has become clear that most of them are people who are not able to integrate, and that they are able to integrate."}, {"heading": "II. RELATED WORK", "text": "The essence of metric learning is that given a distance function d (xi, xj) between the data points xi, xj lies in some attribute space X IRd, for example the Euclidean, along with page information as supervision, it should learn a mapping function, so that the original distance function that is applied to the mapped data is better. Methods of this approach are called global, since they are a mappingar Xiv: 160 4.04 879v 1 [cs.L G] 17 April 2 function that is applied to the entire data. Depending on the transformation, this approach is divided into two subclasses: linear and non-linear matrix. In the linear case, the goal is to learn a linear mapping transformation from page information that can encode the matrix G, so that the learned distance | Gxi \u2212 Gxj | 2. This approach is called Mahalanobis metric learning."}, {"heading": "A. Online Mahalanobis Metric Learning Algorithms", "text": "POLA (pseudo-metric online learning algorithm) [10], was the first algorithm to focus on online Mahalanobis metric learning, which has a matrix M and a threshold b \u2265 1. POLA receives a triplet in each step t: (xi, xj, yij), where yij = 1 (if (xi, xj) and yij = \u2212 1) if (xi, xj). The loss function used by POLA is ci (XTMX) = [1 + yi (dM (xi, xj) \u2212 and it uses a square Frobenius norm for regulation. \u2212 POLA performs two consecutive orthogonal projections \u2212 [3]. Another algorithm based on POLA is LEGO (Lego Exact Gradient Online) [11], but it is based on divergence regulation of M \u2212 xx. Another algorithm based on LELA is LEGO (Lego Exact Online)."}, {"heading": "III. PROPOSED APPROACH", "text": "In our proposal, we implemented the KISS Metric Learning Algorithm [8] in an online environment and hybridized it with a k-NN data stream classification algorithm to calculate the distance in each query. We compare its performance with IBLStream [14], one of the best instance-based data stream classification algorithms."}, {"heading": "A. KISSME Algorithm", "text": "The KISSME algorithm [8] is a simple proposal from a statistical point of view, which takes into account two independent generation processes for observed similarities of similar and unequal pairings, which define the dissimilarity by the plausibility of belonging to either one or the other. Then, from a statistical point of view, the optimal statistical decision is made whether a pair (i, j) is unequal or not is determined by a probability test. Thus, the authors test the hypothesis H0 that a pair is unequal and, on the other hand, the alternative H1: [8], xj) = protocol (p (xi, xj | H0) p (xi, xj) p (H1) p (H1) p (H1))) (3) Then H0 xxip (1) is validated with a high value of p (xi, xj)."}, {"heading": "B. Online-KISSME-Stream", "text": "For our online variations1, we initialize the k-NN classification algorithms with a Euclidean distance function = 32 lines to calculate the distances between the instances that set a diagonal d \u00b7 d Mahalanobis matrix, where d is then the number of attributes. Then, we define the maximum number of instances that can be stored in the base. While the instance base does not store the maximum number of each incoming instance, it is added to it (algorithm 1, line: 5, 8). If the classes are the same, then it is a similarity constant that is needed to update the similar matrix, it is provided by the outer vector product (eq. 6) (algorithm 1, lines: 5-7). But if the classes are not the same, then it is a dissimilarity and we update the similarity matrix (7)."}, {"heading": "IV. EXPERIMENTAL EVALUATIONS", "text": "In the case of streaming classification algorithms, there are some suggestions [17], [18], [19], [20] based on evaluation methods. In particular, there is some work on which metrics are appropriate to evaluate the performance of classification techniques. There are essentially two data stream evaluation methods, which are called: (i) holdout (ii) prequential (ii) prequential (ii) prequential (ii) prequential (ii) prequential (ii) prequential (ii) prequential (ii) prequential) errors, both of which should be used with forgetting mechanisms to provide reliable error estimators, proving that the use of prequential error mechanisms and blending factors is beneficial in evaluating performance and comparing learning algorithms."}, {"heading": "A. Evaluation Results on Synthetic Datasets", "text": "On the 100,000-instance random tree generator, the prequential accuracy (see Figure 1) of online KISSMEStream is higher than that of IBLStream. This results in a slight prequential error than IBLStream. Q statistics (see Figure 1) show a higher range under the curve for values below zero, which means that online KISSME streams had fewer losses on that dataset. In the instances of the Waveform dataset evaluated by the online KISSME stream algorithm, we find that the prequential accuracy (see Figure 2) is higher than IBLStream one, especially for the earlier instances. Likewise, the prequential error is smaller than IBLStream one; and the range below the curve for values below zero in Q statistics (see Figure 2) is larger, meaning fewer losses in that dataset."}, {"heading": "B. Evaluation Results on KDD Cup 99 Dataset", "text": "Two experiments were performed on 111 000 KDD Cup 99 instances. In the first case, the prequential accuracy of OnlineKISS stream was better than the IBLStream (Figure 4). In the second, we applied the McNemar's test. This test is non-parametric to nominal data widely used for comparing batch learning classification algorithms. In [17], the applicability of this test to classification problems in data stream environments is highlighted; this test shows an acceptable Type I error. To implement both quantities ni, j: n0.1, the number of examples that were incorrectly classified by OnlineKISSME stream and which were not by IBLStream is highlighted; while ni, j: n1.0 denotes the number of examples that were incorrectly classified by IBLStream and which were not classified by Online KISSME stream. Then, two hypotheses were defined as a difference between the classification class H1 and H1 if there is a greater statistical difference."}, {"heading": "V. CONCLUSIONS", "text": "Most approaches define an optimization model that combines the limitations of similarity with dissimilarity as loss functions with regulators. There are five properties that these algorithms eject for certain environments. An important feature that has led us to think about the development of this work is the scalability that is indispensable to ensure incremental learning in streaming scenarios. In this paper, we have proposed Online-KISS-Stream, a new instance-based data stream classification algorithm that learns a Mahalanobis metric based on the KISSME algorithm. The online optimization proposed by KISSME calculates the similarity and dissimilarity of matrices from the outer vector product and then computes the own decomposition of the difference in the inversion of the two matrices, making it possible to obtain the Mahalanobis matrices matrices."}, {"heading": "ACKNOWLEDGMENT", "text": "Erasmus Mundus Action 2 Programme, Eureka SD Project is appreciated for its funding."}], "references": [{"title": "A survey on learning from data streams: current and future trends", "author": ["J. Gama"], "venue": "Progress in Artificial Intelligence, vol. 1, no. 1, pp. 45\u201355, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["G. Ditzler", "R. Polikar"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 25, no. 10, pp. 2283\u20132301, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning: a survey", "author": ["B. Kulis"], "venue": "Found. and Trends in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning an image manifold for retrieval", "author": ["X. He", "W.-Y. Ma", "H.-J. Zhang"], "venue": "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, 2004, pp. 17\u201323.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifoldranking based image retrieval", "author": ["J. He", "M. Li", "H.-J. Zhang", "H. Tong", "C. Zhang"], "venue": "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, 2004, pp. 9\u201316.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "Advances in neural information processing systems, vol. 18, p. 1473, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2288\u20132295.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Aprendizaje supervisado de funciones de distancia: estado del arte.", "author": ["B. Nguyen Cong", "J.L. Rivero P\u00e9rez", "C. Morell"], "venue": "Revista Cubana de Ciencias Informa\u0301ticas,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning. ACM, 2004, p. 94.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Online metric learning and fast similarity search.", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "in NIPS, vol", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Regularized distance metric learning: Theory and algorithm.", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "in NIPS, vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Iblstreams: a system for instance-based classification and regression on data streams", "author": ["A. Shaker", "E. H\u00fcllermeier"], "venue": "Evolving Systems, vol. 3, no. 4, pp. 235\u2013249, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning with drift detection", "author": ["J. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "Advances in Artificial Intelligence\u2013SBIA 2004. Springer, 2004, pp. 286\u2013295.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Moa: Massive online analysis", "author": ["A. Bifet", "G. Holmes", "R. Kirkby", "B. Pfahringer"], "venue": "The Journal of Machine Learning Research, vol. 99, pp. 1601\u20131604, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "On evaluating stream learning algorithms", "author": ["J. Gama", "R. Sebasti\u00e3o", "P.P. Rodrigues"], "venue": "Machine Learning, vol. 90, no. 3, pp. 317\u2013346, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Issues in evaluation of stream learning algorithms", "author": ["J. Gama", "R. Sebastiao", "P.P. Rodrigues"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009, pp. 329\u2013338.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Recovery analysis for adaptive learning from non-stationary data streams", "author": ["A. Shaker", "E. H\u00fcllermeier"], "venue": "Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013. Springer, 2013, pp. 289\u2013298.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Pitfalls in benchmarking data stream classification and how to avoid them", "author": ["A. Bifet", "J. Read", "I. \u017dliobait\u0117", "B. Pfahringer", "G. Holmes"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 465\u2013479.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "T\u00e9cnicas de aprendizaje autom\u00e1tico para la detecci\u00f3n de intrusos en redes de computadoras", "author": ["J.L. Rivero P\u00e9rez"], "venue": "Revista Cubana de Ciencias Inform\u00e1ticas, vol. 8, no. 4, pp. 52\u201373, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent years there has been a great development in the information technology and communications, which has changed the data collection and processing methods [1], [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "In recent years there has been a great development in the information technology and communications, which has changed the data collection and processing methods [1], [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "Likewise, it should also identify as dissimilar those that are semantically different [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "However, the best results are obtained when the metric is designed specifically for the task at hand, issue that has received much interest from researchers in the last decade [3], [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "However, the best results are obtained when the metric is designed specifically for the task at hand, issue that has received much interest from researchers in the last decade [3], [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 2, "context": "Most of methods learn the metric in a supervised manner from similarity, dissimilarity and/or relative distance constraints, being formulated as an optimization problem [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "Some studies [5], [6], [7] have shown that good design metric learning can significantly improve the k-NN classification accuracy in batch learning.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "Some studies [5], [6], [7] have shown that good design metric learning can significantly improve the k-NN classification accuracy in batch learning.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "Some studies [5], [6], [7] have shown that good design metric learning can significantly improve the k-NN classification accuracy in batch learning.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "For that reason we choose KISS Metric Learning algorithm [8], a simple statistical proposal of distance metric learning.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Then, Mahalanobis distance corresponds to a generalized Euclidean distance using the inverse of the variance-covariance matrix [4], [9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": "Then, Mahalanobis distance corresponds to a generalized Euclidean distance using the inverse of the variance-covariance matrix [4], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "A general regularized model that captures most of the metric learning existing techniques is proposed in [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Quite a few online metric learning algorithms have been proposed, under an approach that learns a Mahalanobis matrix [10], [11], [12], [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Quite a few online metric learning algorithms have been proposed, under an approach that learns a Mahalanobis matrix [10], [11], [12], [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "Quite a few online metric learning algorithms have been proposed, under an approach that learns a Mahalanobis matrix [10], [11], [12], [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "POLA (Pseudo-metric Online Learning Algorithm) [10], was the first algorithm focused on online Mahalanobis metric learning that learns a matrix M and a threshold b \u2265 1.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "POLA performs two successive orthogonal projections [4], [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "POLA performs two successive orthogonal projections [4], [3].", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "Another algorithm based on POLA is LEGO (Lego Exact Gradient Online) [11] but it is based on LogDet divergence regularization, which gives it a better performance than POLA.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "Unlike POLA, the authors perform the update solving a convex quadratic program [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "It is based on composite mirror descent and is focused on regularization with the nuclear norm [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "In our proposal, we implemented KISS Metric Learning algorithm [8] while in an online setting and hybridized it with a k-NN data stream classification algorithm to compute the distance in each query.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "We compare its performance with IBLStream [14], which is one of the best instance-based data stream classification algorithms.", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "The KISSME algorithm [8] is a simple proposal from a statistical inference point of view, making some assumptions about the distributions to obtain a Mahalanobis matrix from similar and dissimilar constraints.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Thus, the authors test the hypothesis H0 that a pair is dissimilar and on the other hand the alternative H1: [8]", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "3 to: [8]", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "4 to: [8]", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "The concept drift detection is performed by means of Drift Detection Method [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "In the next section we present the experimental results obtained from comparing our proposed approach with IBLStream [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "IBLStream is implemented in MOA (Massive Online Analysis) [16] which is an extensible framework, that apart from allowing to implement algorithms also permits running experiments for online learning.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "In the case of streaming classification algorithms, there have been some proposals [17], [18], [19], [20] on evaluation methodologies.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "In the case of streaming classification algorithms, there have been some proposals [17], [18], [19], [20] on evaluation methodologies.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "In the case of streaming classification algorithms, there have been some proposals [17], [18], [19], [20] on evaluation methodologies.", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "In the case of streaming classification algorithms, there have been some proposals [17], [18], [19], [20] on evaluation methodologies.", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "In [18], [17] it is argued that prequential error with forgetting mechanisms should be used to provide reliable error estimators.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [18], [17] it is argued that prequential error with forgetting mechanisms should be used to provide reliable error estimators.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "We also implemented and computed the McNemar\u2019s Test to compare paired proportions of both algorithms classification results, and the statistic Q proposed in [17] to comparative assessment between any two algorithms.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "Both algorithms were evaluated with setups corresponding to its default values [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "In [21] different preprocessing techniques that have been applied to this problem, prior to evaluate machine learning algorithms, have been reviewed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17] the applicability of this test to classification problems in data stream environments is emphasized.", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "With the massive data challenges nowadays and the rapid growing of technology, stream mining has recently received considerable attention. To address the large number of scenarios in which this phenomenon manifests itself suitable tools are required in various research fields. Instance-based data stream algorithms generally employ the Euclidean distance for the classification task underlying this problem. A novel way to look into this issue is to take advantage of a more flexible metric due to the increased requirements imposed by the data stream scenario. In this paper we present a new algorithm that learns a Mahalanobis metric using similarity and dissimilarity constraints in an online manner. This approach hybridizes a Mahalanobis distance metric learning algorithm and a k-NN data stream classification algorithm with concept drift detection. First, some basic aspects of Mahalanobis distance metric learning are described taking into account key properties as well as online distance metric learning algorithms. Second, we implement specific evaluation methodologies and comparative metrics such as Q statistic for data stream classification algorithms. Finally, our algorithm is evaluated on different datasets by comparing its results with one of the best instance-based data stream classification algorithm of the state of the art. The results demonstrate that our proposal is better in some scenarios and has shown to be competitive in others.", "creator": "TeX"}}}