{"id": "1605.01288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Fast rates with high probability in exp-concave statistical learning", "abstract": "We present an algorithm for the statistical learning setting with a bounded exp-concave loss in $d$ dimensions that obtains excess risk $O(d / n)$ with high probability: the dependence on the confidence parameter $\\delta$ is polylogarithmic in $1/\\delta$. The core technique is to boost the confidence of recent $O(d / n)$ bounds, without sacrificing the rate, by leveraging a Bernstein-type condition which holds due to exp-concavity. This Bernstein-type condition implies that the variance of excess loss random variables are controlled in terms of their excess risk. Using this variance control, we further show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner.", "histories": [["v1", "Wed, 4 May 2016 14:22:59 GMT  (15kb,D)", "https://arxiv.org/abs/1605.01288v1", "13 pages"], ["v2", "Mon, 23 May 2016 09:15:28 GMT  (18kb,D)", "http://arxiv.org/abs/1605.01288v2", "13 pages, wider margins than previous version"], ["v3", "Thu, 18 Aug 2016 13:52:47 GMT  (18kb,D)", "http://arxiv.org/abs/1605.01288v3", "improved log(1/delta)^2 to log(1/delta) in main result"], ["v4", "Fri, 14 Oct 2016 15:24:08 GMT  (23kb,D)", "http://arxiv.org/abs/1605.01288v4", "added results on model selection aggregation (Section 7)"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nishant a mehta"], "accepted": false, "id": "1605.01288"}, "pdf": {"name": "1605.01288.pdf", "metadata": {"source": "CRF", "title": "Fast rates with high probability in exp-concave statistical learning", "authors": ["Nishant A. Mehta"], "emails": ["mehta@cwi.nl"], "sections": [{"heading": "1 Introduction", "text": "In the statistical learning effect problem, a learning expert observes an excess of n pointsZ1,.., Zn drawn i.i.d. from an unknown distribution P over an outcome space Z. \"The agent then searches for an action f in an action space F that minimizes their excess of confidence, and it is a subset of Rd, and F is convex. (2015) were the first to show that there is a learner for which the probability is 1 \u2212 3, the excess risk decreases at rate d, and F is convex. (1 / 3) It is the first to show that there is a learning model for which the probability is 1 \u2212 3."}, {"heading": "2 A history of exp-concave learning", "text": "In fact, it is a matter of being able to trump yourself. (...) In fact, it is a matter of being able to trump yourself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...) It is as if one is able to trump oneself. (...)"}, {"heading": "3 Rate-optimal in-expectation bounds", "text": "\"We assume that the loss of two billion euros per year is to be expected over the next five years.\" (\"We assume that the loss of two billion euros per year is to be expected.\" (\"We assume that the loss of two billion euros per year is to be expected.\") \"We assume that the loss of two billion euros per year is to be expected.\" (\"We assume that the loss of two billion euros per year is to be expected.\" (\"We assume that the loss of two billion euros per year is not to be expected.\") \"We assume that the loss of two billion euros per year is to be expected.\" (\"F, z).\" (\"We assume that an example of n points is to be drawn i.i.d.)\" We assume that our goal is to choose a hypothesis that minimizes the excess risk of EZ. \"(\" F, z). (\"We assume that an example of n points is to be drawn i.i.e.)."}, {"heading": "4 A high probability bound for ERM", "text": "The first two assumptions already imply the last two assumptions. \"All these assumptions were made by us.\" \"All these assumptions.\" \"All these assumptions.\" \"All these assumptions.\" \"All these assumptions.\" \"All these assumptions.\" \"All these assumptions.\" \"\" \".\" \"\". \"\" \".\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"..\" \"\" \"\".. \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \".\" \"\" \"\" \"\". \"\". \"\" \"\" \"\". \"\" \"\". \"\" \"\". \"\" \".\" \"\" \"\". \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\" \"\". \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \".\" \"\" \"\". \"\". \"\" \"\" \"\" \"\" \"\". \".\" \"\" \".\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\". \"\" \"\" \".\" \"\" \"\" \".\" \"\". \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\". \"\" \""}, {"heading": "5 Boosting the confidence for high probability bounds", "text": "The two existing excess risk limits mentioned in Section 3 are a problem (1 / n). A na\u00efve application of Markov's inequality proves unsatisfactory and leads to excess risk of order (s). \u2212 Z \u2212 Z \u2212 Z \u2212 Z, with a probability of at least 1 \u2212 Z \u2212 Z (1 / n). This method is essentially Schapire's \"confidence boosting\" trick (1990); 2 the novelty lies in a sophisticated analysis that exploits an amber-type condition to improve the rate in the final high probability tied to the desired O (1 / n). In fact, our analysis of CONFIDENCEBOOST is generally regarded as the exp-concave learning situation, which requires only satisfaction in anticipation of the form."}, {"heading": "6 Online-to-batch-conversion", "text": "The current objective is to show that if one is willing to accept the excess risk by a high probability, then it is sufficient to apply an excess risk in order to accept an additional excess risk. (...) The key difficulty is that one is highly likely to enter into such a limit. (...) This result provides an alternative to the high probabilities. (...) The result for ERM in Section 4.Mahdavi et al. (...) Prior to this, one considers an online conversion of ONS and establishes the first explicit high probability O (...) excess risks in the exp-concave statistical learning environment. (...) Their analysis is elegant but seems to be intuitively coupled with ONS. (...) It is unclear whether their analysis can be used to capture excess risks in the eccentric statistical learning environment."}, {"heading": "7 Model selection aggregation", "text": "The goal is the same as in the stochastic exp-concave optimization problem, but it fails because of the exp-concavity assumption. (...) The goal is the same as in the stochastic exp-concave optimization problem, but the exp-concave premise is not achieved. (...) The goal is the same as in the stochastic exp-concave optimization problem, but it fails because of the convex-concavity premise. (...) The goal is the same as in the stochastic exp-concave-concave optimization problem, but nowF fails because of the convex-concavity premise. (...) The goal is the same as in the stochastic exp-concave optimization problem, but it fails because of the convex-concavity premise. (...)"}, {"heading": "8 Discussion and Open Problems", "text": "The key to proving this limit was the link between Exp concavity and the central condition, a link that suggests that Exp concavity implies a low state of noise. Here, low noise can be interpreted either in relation to the central condition, by the exponential decay of the negative tail of the random excess variable, or in relation to the amber condition, by the variance of the excessive loss of a hypothesis f, which is controlled by its excess risk. All our results for stochastic Exp conception optimization are based on this low noise interpretation of Exp concavity. In contrast, the previous in-expectation is O (d / n) results from Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016), we used the geometric / convex exact interpretation of Exp concavity, which we further increased to a high probability."}, {"heading": "A Proofs for Stochastic Exp-Concave Optimization", "text": "PROOF (OF LEMMA 1) The Exp concavity of f 7 (f, z) for each z-Z (f, z) implies that, for all z-Z and all distributions Q over F-Z (f, z) -D (f, z) -D (f, z) -D (f, z) -D (f, z) -D (f, z) -D (f, z) -D (f) -D (f, z) -D (f, z) -D (f) -f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (D) -D (f) -D (D) -D (D) (D) (D) -D (D) (D) (D) -F (f) -f (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (f) -f (f) -D (f) -D (f) -D (f) -D (f) -D (f) -D (D (f) -D (f) -D (D (f) -D (f) -D (D (f) -D (f) -D (D (f) -D (f) -D (D (f) -D (f (f) -D (f (f) -D (f) -D (D (f) -D (f) -D (f) -D (f (f) -D (f) -D (f) -D (f -D (f) -D (f (f) -D (f -D (f) -D (f) -D (f) -D (f (f) -D (f (f) -D (f) -D (f (f (f) -D (f) -D (f) -D (f) -D (f (f) -D (f (f) -D (f (f) -D (f (f) -D (f)"}, {"heading": "B Proofs for Model Selection Aggregation (Section 7)", "text": "PROOF (OF THEOREMS 4 AND 5) The starting point is the following limit for the progressive mixing rule, if it is executed with previous \u03c0 and parameter \u03b7, due to Audibert (see theorem 1 of Audibert (2008), but the result has already been proven in an earlier version of the technical report by Audibert (2009) (see Corollary 4.1 and Lemma 3.3 therein). If it is executed on an n sample, it results in a hypothesis f (EZ) [EZ [\"Y, f) [Y, f))), f), f), f), [S), [S), [S, S], [S], f), [S), [S), [S), [S), [S), [S), [S), S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}], "references": [{"title": "Progressive mixture rules are deviation suboptimal", "author": ["Jean-Yves Audibert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Audibert.,? \\Q2008\\E", "shortCiteRegEx": "Audibert.", "year": 2008}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["Jean-Yves Audibert"], "venue": "The Annals of Statistics,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Entropy, compactness, and the approximation of operators, volume 98", "author": ["Bernd Carl", "Irmtraud Stephani"], "venue": null, "citeRegEx": "Carl and Stephani.,? \\Q1990\\E", "shortCiteRegEx": "Carl and Stephani.", "year": 1990}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicol\u00f2 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "On tail probabilities for martingales", "author": ["David A. Freedman"], "venue": "The Annals of Probability,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Tightening the sample complexity of empirical risk minimization via preconditioned stability", "author": ["Alon Gonen", "Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1601.04011,", "citeRegEx": "Gonen and Shalev.Shwartz.,? \\Q2016\\E", "shortCiteRegEx": "Gonen and Shalev.Shwartz.", "year": 2016}, {"title": "The safe bayesian", "author": ["Peter Gr\u00fcnwald"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Learning by mirror averaging", "author": ["Anatoli Juditsky", "Philippe Rigollet", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Juditsky et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2008}, {"title": "On the generalization ability of online strongly convex programming algorithms", "author": ["Sham M. Kakade", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade and Tewari.,? \\Q2009\\E", "shortCiteRegEx": "Kakade and Tewari.", "year": 2009}, {"title": "An introduction to computational learning theory", "author": ["Michael J. Kearns", "Umesh Vazirani"], "venue": "MIT press,", "citeRegEx": "Kearns and Vazirani.,? \\Q1994\\E", "shortCiteRegEx": "Kearns and Vazirani.", "year": 1994}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "In Computational Learning Theory,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1999\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1999}, {"title": "Open problem: Fast stochastic exp-concave optimization", "author": ["Tomer Koren"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Koren.,? \\Q2013\\E", "shortCiteRegEx": "Koren.", "year": 2013}, {"title": "Fast rates for exp-concave empirical risk minimization", "author": ["Tomer Koren", "Kfir Levy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koren and Levy.,? \\Q2015\\E", "shortCiteRegEx": "Koren and Levy.", "year": 2015}, {"title": "Aggregation via empirical risk minimization", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": "Probability theory and related fields,", "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2009\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2009}, {"title": "Optimal learning with q-aggregation", "author": ["Guillaume Lecu\u00e9", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Lecu\u00e9 and Rigollet.,? \\Q2014\\E", "shortCiteRegEx": "Lecu\u00e9 and Rigollet.", "year": 2014}, {"title": "Excess risk bounds for exponentially concave losses", "author": ["Mehrdad Mahdavi", "Rong Jin"], "venue": "arXiv preprint arXiv:1401.4566,", "citeRegEx": "Mahdavi and Jin.,? \\Q2014\\E", "shortCiteRegEx": "Mahdavi and Jin.", "year": 2014}, {"title": "Lower and upper bounds on the generalization of stochastic exponentially concave optimization", "author": ["Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Mahdavi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2015}, {"title": "From stochastic mixability to fast rates", "author": ["Nishant A. Mehta", "Robert C. Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mehta and Williamson.,? \\Q2014\\E", "shortCiteRegEx": "Mehta and Williamson.", "year": 2014}, {"title": "Beyond logarithmic bounds in online learning", "author": ["Francesco Orabona", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Orabona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2012}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Machine learning,", "citeRegEx": "Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Schapire.", "year": 1990}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "Robustly minimax codes for universal data compression\u2019, the 21\u2019st", "author": ["Jun-ichi Takeuchi", "Andrew R Barron"], "venue": "In Proceedings of the Twenty-First Symposium on Information Theory and Its Applications,", "citeRegEx": "Takeuchi and Barron.,? \\Q1998\\E", "shortCiteRegEx": "Takeuchi and Barron.", "year": 1998}, {"title": "Mixability in statistical learning", "author": ["Tim van Erven", "Peter Gr\u00fcnwald", "Mark D. Reid", "Robert C. Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erven et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2012}, {"title": "Fast rates in statistical and online learning", "author": ["Tim van Erven", "Peter D. Gr\u00fcnwald", "Nishant A. Mehta", "Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erven et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2015}, {"title": "The nature of statistical learning theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1995\\E", "shortCiteRegEx": "Vapnik.", "year": 1995}, {"title": "Learning and Generalization with Applications to Neural Networks", "author": ["Mathukumalli Vidyasagar"], "venue": null, "citeRegEx": "Vidyasagar.,? \\Q2002\\E", "shortCiteRegEx": "Vidyasagar.", "year": 2002}, {"title": "Aggregating strategies. In Proceedings of the third annual workshop on Computational learning theory, pages 371\u2013383", "author": ["Volodimir G Vovk"], "venue": null, "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "This condition is equivalent to stochastic mixability as well as the pseudoprobability convexity (PPC) condition, both defined by Van Erven et al. (2015). To be precise, for stochastic mixability", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "2015) states that the PPC condition implies the (strong) central condition", "author": ["Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "\u03b3n,\u03b5. Since we removed the empirical inadmissible functions, there exists some \u03b7f \u2265 \u03b7 for which E[e\u2212\u03b7fLf", "author": [], "venue": "Theorem 3 and Lemma", "citeRegEx": "\u2208,? \\Q2014\\E", "shortCiteRegEx": "\u2208", "year": 2014}, {"title": "First, as per the proof of Lemma 1, note that the central condition as defined in the present work is equivalent to the strong PPC condition", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "2015), we may take \u03b5 = 0, improving their constant c2 by a factor of 3; moreover, their result actually holds for the second moment, not just the variance, yielding", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Mahdavi et al. (2015) were the first to show that there exists a learner for which, with probability at least 1 \u2212 \u03b4, the excess risk decays at the rate d(log n + log(1/\u03b4))/n.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Via new algorithmic stability arguments applied to empirical risk minimization (ERM), Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) discarded the log n factor to obtain a rate of d/n, but their bounds only hold in expectation.", "startOffset": 86, "endOffset": 108}, {"referenceID": 4, "context": "Via new algorithmic stability arguments applied to empirical risk minimization (ERM), Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) discarded the log n factor to obtain a rate of d/n, but their bounds only hold in expectation.", "startOffset": 112, "endOffset": 144}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n.", "startOffset": 80, "endOffset": 96}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition.", "startOffset": 80, "endOffset": 971}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition. We then exploit the variance control of the excess loss random variables afforded by the Bernstein condition to boost the boosting the confidence trick of Schapire (1990). In the next section, we discuss a brief history of the work in this area.", "startOffset": 80, "endOffset": 1188}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition. We then exploit the variance control of the excess loss random variables afforded by the Bernstein condition to boost the boosting the confidence trick of Schapire (1990). In the next section, we discuss a brief history of the work in this area. In Section 3, we formally define the setting and describe the previous O(d/n) in-expectation bounds. We present the results for standard ERM and our confidence-boosted ERM method in Sections 4 and 5 respectively. Section 6 extends the results of Kakade and Tewari (2009) to exp-concave losses, showing that under a bounded loss assumption a regret bound for any", "startOffset": 80, "endOffset": 1534}, {"referenceID": 4, "context": "This result continues the line of work of Cesa-Bianchi et al. (2001) and Kakade and Tewari (2009) and accordingly is about the generalization ability of online exp-concave learning algorithms.", "startOffset": 42, "endOffset": 69}, {"referenceID": 4, "context": "This result continues the line of work of Cesa-Bianchi et al. (2001) and Kakade and Tewari (2009) and accordingly is about the generalization ability of online exp-concave learning algorithms.", "startOffset": 42, "endOffset": 98}, {"referenceID": 9, "context": "(Juditsky et al., 2008).", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013).", "startOffset": 277, "endOffset": 310}, {"referenceID": 13, "context": "While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013).", "startOffset": 277, "endOffset": 310}, {"referenceID": 18, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999).", "startOffset": 88, "endOffset": 100}, {"referenceID": 7, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999). Vovk (1990) showed that if a game is \u03b7-mixable (which is implied by \u03b7-exp-concavity), one can guarantee that the worst-case individual sequence regret against the best of K experts is at most logK \u03b7 .", "startOffset": 211, "endOffset": 238}, {"referenceID": 7, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999). Vovk (1990) showed that if a game is \u03b7-mixable (which is implied by \u03b7-exp-concavity), one can guarantee that the worst-case individual sequence regret against the best of K experts is at most logK \u03b7 .", "startOffset": 211, "endOffset": 251}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n.", "startOffset": 0, "endOffset": 16}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM.", "startOffset": 0, "endOffset": 311}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution.", "startOffset": 0, "endOffset": 905}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms.", "startOffset": 0, "endOffset": 1096}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability.", "startOffset": 0, "endOffset": 1454}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability. While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013). If one hopes to eliminate the log n factor, the additional hardness of the online setting makes it unlikely that one can proceed via an online-to-batch conversion approach. Moreover, computational considerations suggest circumventing ONS anyways. In this vein, as we discuss in the next section both Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) recently established in-expectation excess risk bounds for a lightly penalized ERM algorithm and ERM itself respectively, without resorting to an online-to-batch conversion.", "startOffset": 0, "endOffset": 2204}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability. While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013). If one hopes to eliminate the log n factor, the additional hardness of the online setting makes it unlikely that one can proceed via an online-to-batch conversion approach. Moreover, computational considerations suggest circumventing ONS anyways. In this vein, as we discuss in the next section both Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) recently established in-expectation excess risk bounds for a lightly penalized ERM algorithm and ERM itself respectively, without resorting to an online-to-batch conversion.", "startOffset": 0, "endOffset": 2240}, {"referenceID": 14, "context": "\u201cERM\u201d is either penalized ERM (Koren and Levy, 2015) or ERM (Gonen and Shalev-Shwartz, 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 6, "context": "\u201cERM\u201d is either penalized ERM (Koren and Levy, 2015) or ERM (Gonen and Shalev-Shwartz, 2016).", "startOffset": 60, "endOffset": 92}, {"referenceID": 6, "context": "We assume that there exists f\u2217 \u2208 F satisfying E[`f\u2217(Z)] = inff\u2208F EZ\u223cP [`f (Z)]; this assumption also was made by Gonen and Shalev-Shwartz (2016) and Kakade and Tewari (2009).", "startOffset": 113, "endOffset": 145}, {"referenceID": 6, "context": "We assume that there exists f\u2217 \u2208 F satisfying E[`f\u2217(Z)] = inff\u2208F EZ\u223cP [`f (Z)]; this assumption also was made by Gonen and Shalev-Shwartz (2016) and Kakade and Tewari (2009).1 Let AF be an algorithm, defined for a function class F as a mapping AF : \u22c3 n\u22650Z \u2192 F ; we drop the subscript F when it is clear from the context.", "startOffset": 113, "endOffset": 174}, {"referenceID": 6, "context": "Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) both established in-expectation bounds of the form (1) that obtain a rate of O(d/n) in the case when F \u2282 R, each in a slightly different setting.", "startOffset": 26, "endOffset": 58}, {"referenceID": 6, "context": "Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) both established in-expectation bounds of the form (1) that obtain a rate of O(d/n) in the case when F \u2282 R, each in a slightly different setting. Koren and Levy (2015) assume, for each outcome z \u2208 Z , that the loss `(\u00b7, z) has diameter B and is \u03b2-smooth for some \u03b2 \u2265 1, i.", "startOffset": 26, "endOffset": 226}, {"referenceID": 6, "context": "Gonen and Shalev-Shwartz (2016) work in a slightly different setting that captures all known exp-concave losses.", "startOffset": 0, "endOffset": 32}, {"referenceID": 12, "context": "1This assumption is not explicit from Koren and Levy (2015), but their other assumptions might imply it.", "startOffset": 38, "endOffset": 60}, {"referenceID": 6, "context": "Regardless, if their results and those of Gonen and Shalev-Shwartz (2016) hold, our analysis in Section 5 can be adapted to work if the infimal risk is not achieved, i.", "startOffset": 42, "endOffset": 74}, {"referenceID": 14, "context": "The closest such result, Theorem 1 of Mahdavi and Jin (2014), does not apply as it relies on an additional assumption (see their Assumption (I)).", "startOffset": 38, "endOffset": 61}, {"referenceID": 14, "context": "The closest such result, Theorem 1 of Mahdavi and Jin (2014), does not apply as it relies on an additional assumption (see their Assumption (I)). Our assumptions subtly differ from elsewhere in this work. We assume that F \u2282 R satisfies supf,f \u2032\u2208F \u2016f \u2212 f \u20162 \u2264 R and that, for each outcome z \u2208 Z , the loss `(\u00b7, z) is L-Lipschitz and |`f (z)\u2212`f\u2217(z)| \u2264 B. The first two assumptions already imply the last forB = LR. All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 38, "endOffset": 471}, {"referenceID": 12, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 117, "endOffset": 149}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R). The first, critical observation is that exp-concavity implies good concentration properties of the excess loss random variable. This is easiest to see by way of the \u03b7-central condition, which the excess loss satisfies. This concept, studied by Van Erven et al. (2015) and first introduced by Van Erven et al.", "startOffset": 117, "endOffset": 559}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R). The first, critical observation is that exp-concavity implies good concentration properties of the excess loss random variable. This is easiest to see by way of the \u03b7-central condition, which the excess loss satisfies. This concept, studied by Van Erven et al. (2015) and first introduced by Van Erven et al. (2012) as \u201cstochastic mixability\u201d, is defined as follows.", "startOffset": 117, "endOffset": 607}, {"referenceID": 19, "context": "With the central condition in our grip, Theorem 7 of Mehta and Williamson (2014) directly implies an O(d log(n)/n) bound for ERM; however, a far simpler version of that result yields much smaller constants.", "startOffset": 53, "endOffset": 81}, {"referenceID": 20, "context": "This method is essentially the \u201cboosting the confidence\u201d trick of Schapire (1990);2 the novelty lies in a refined analysis that exploits a Bernstein-type condition to improve the rate in the final high probability bound from the typical O(1/ \u221a n) to the desired O(1/n).", "startOffset": 66, "endOffset": 82}, {"referenceID": 11, "context": "2 of Kearns and Vazirani (1994).", "startOffset": 5, "endOffset": 32}, {"referenceID": 24, "context": "The next lemma, which adapts a result of Van Erven et al. (2015), shows that the \u03b7-central condition, together with boundedness of the loss, implies that a Bernstein condition holds.", "startOffset": 45, "endOffset": 65}, {"referenceID": 13, "context": "Corollary 1 Applying Theorem 2 with AF the algorithm of Koren and Levy (2015) and their assumptions (with \u03b2 \u2265 1), the bound in Theorem 2 specializes to", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "Similarly taking AF the algorithm of Gonen and Shalev-Shwartz (2016) and their assumptions yields", "startOffset": 37, "endOffset": 69}, {"referenceID": 18, "context": "Indeed, under strong convexity (which is strictly stronger than exp-concavity), Sridharan et al. (2009) show that a similar bound for ERM is possible; however, they used strong convexity to bound a localized complexity.", "startOffset": 80, "endOffset": 104}, {"referenceID": 11, "context": "It is unclear if exp-concavity can be used to bound a localized complexity, and the Bernstein condition alone seems insufficient; such a bound may be possible via ideas from the local norm analysis of Koren and Levy (2015). While we think controlling a localized complexity from exp-concavity is a very interesting and worthwhile direction, we leave this to future work, and for now only conjecture that ERM also enjoys excess risk bounded by O((d+ log(1/\u03b4))/n) with high probability.", "startOffset": 201, "endOffset": 223}, {"referenceID": 0, "context": "This conjecture is from analogy to the empirical star algorithm of Audibert (2008), which for convex F reduces to ERM itself; note that the conjectured effect of log(1/\u03b4) is additive rather than multiplicative.", "startOffset": 67, "endOffset": 83}, {"referenceID": 17, "context": "Mahdavi et al. (2015) previously considered an online-to-batch conversion of ONS and established the first explicit high probability O(log n/n) excess risk bound in the exp-concave statistical learning setting.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Our analysis builds strongly on the analysis of Kakade and Tewari (2009) in the strongly convex setting.", "startOffset": 48, "endOffset": 73}, {"referenceID": 10, "context": "When the losses are bounded, strongly convex, and Lipschitz, Kakade and Tewari (2009) showed that if an online algorithm has regret Rn on an i.", "startOffset": 61, "endOffset": 86}, {"referenceID": 10, "context": "When the losses are bounded, strongly convex, and Lipschitz, Kakade and Tewari (2009) showed that if an online algorithm has regret Rn on an i.i.d. sequence Z1, . . . , Zn \u223c P , online-to-batch conversion by simple averaging of the iterates f\u0304n := 1 n \u2211n t=1 ft admits the following guarantee. Theorem 3 (Cor. 5, Kakade and Tewari (2009)) For all z \u2208 Z , assume that `(\u00b7, z) is bounded by B, \u03bdstrongly convex, and L-Lipschitz.", "startOffset": 61, "endOffset": 338}, {"referenceID": 8, "context": "For instance, Online Gradient Descent (Hazan et al., 2007) admits the regret boundRn \u2264 G 2 2\u03bd (1 + log n), where G is an upper bound on the gradient.", "startOffset": 38, "endOffset": 58}, {"referenceID": 9, "context": "What if we relax strong convexity to exp-concavity? As we will see, it is possible to extend the analysis of Kakade and Tewari (2009) to \u03b7-exp-concave losses.", "startOffset": 109, "endOffset": 134}, {"referenceID": 8, "context": "Indeed, at least two such algorithms and bounds exist, due to Hazan et al. (2007):", "startOffset": 62, "endOffset": 82}, {"referenceID": 5, "context": "The key insight is that exp-concavity implies a variance inequality similar to Lemma 1 of Kakade and Tewari (2009), a pivotal result of that work that unlocks Freedman\u2019s inequality for martingales (Freedman, 1975).", "startOffset": 197, "endOffset": 213}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses.", "startOffset": 42, "endOffset": 67}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses. While similar results can be obtained from the work of Mahdavi et al. (2015) for the specific case of ONS, our analysis is agnostic of the base algorithm.", "startOffset": 42, "endOffset": 167}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses. While similar results can be obtained from the work of Mahdavi et al. (2015) for the specific case of ONS, our analysis is agnostic of the base algorithm. A particular consequence is that our analysis also applies to EWOO, which, although highly impractical, offers a better regret bound. Moreover, our analysis applies to any future online learning algorithms which may have improved guarantees and computational complexities. The key insight is that exp-concavity implies a variance inequality similar to Lemma 1 of Kakade and Tewari (2009), a pivotal result of that work that unlocks Freedman\u2019s inequality for martingales (Freedman, 1975).", "startOffset": 42, "endOffset": 633}, {"referenceID": 10, "context": "The next corollary is from a retrace of the proof of Theorem 2 of Kakade and Tewari (2009). Corollary 2 For all z \u2208 Z , let `(\u00b7, z) be bounded by B and \u03b7-exp-concave with respect to the action f \u2208 F .", "startOffset": 66, "endOffset": 91}, {"referenceID": 26, "context": "The loss is a supervised loss, as in supervised classification and regression, unlike the more general loss functions used in the rest of the paper which fit into Vapnik\u2019s general setting of the learning problem (Vapnik, 1995).", "startOffset": 212, "endOffset": 226}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 22}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 297}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 351}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass. Lecu\u00e9 and Rigollet (2014) extended these results to take into account a prior on the class using their Q-aggregation procedure.", "startOffset": 6, "endOffset": 443}, {"referenceID": 0, "context": "3Audibert (2008) only proved the case of bounded squared loss with a suggestion for how to handle the case of exp-concave losses; because of the techniques used, it is likely that Lipschitz continuity would come into play.", "startOffset": 1, "endOffset": 17}, {"referenceID": 23, "context": "Here, BAYESRED\u03b7 (n, \u03c0) is the \u03b7-generalized Expected Bayesian Redundancy (Takeuchi and Barron, 1998; Gr\u00fcnwald, 2012), defined as", "startOffset": 73, "endOffset": 116}, {"referenceID": 7, "context": "Here, BAYESRED\u03b7 (n, \u03c0) is the \u03b7-generalized Expected Bayesian Redundancy (Takeuchi and Barron, 1998; Gr\u00fcnwald, 2012), defined as", "startOffset": 73, "endOffset": 116}, {"referenceID": 16, "context": "For instance, if there is a set F \u2032 of large prior measure which has excess risk close to f\u2217, then Theorem 4 pays log(1/\u03c0(F \u2032)) for the complexity; in contrast, Theorem A of Lecu\u00e9 and Rigollet (2014) pays a higher complexity price of log(1/\u03c0(f\u2217)).", "startOffset": 174, "endOffset": 200}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002).", "startOffset": 94, "endOffset": 124}, {"referenceID": 11, "context": "In contrast, The previous in-expectation O(d/n) results of Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) used the geometric/convexity-interpretation of exp-concavity, which we further boosted to high probability results using the low noise interpretation.", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": "In contrast, The previous in-expectation O(d/n) results of Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) used the geometric/convexity-interpretation of exp-concavity, which we further boosted to high probability results using the low noise interpretation.", "startOffset": 85, "endOffset": 117}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002). Is it a limitation of algorithmic stability techniques that high probability O(d/n) fast rates seem to be out of reach without a posthoc confidence boosting procedure, or are we simply missing the right perspective? One reason to avoid a confidence boosting procedure is that the resulting bounds suffer from a multiplicative log(1/\u03b4) factor rather than the lighter effect of an additive log(1/\u03b4) factor in bounds like Theorem 1. As we mentioned earlier, we conjecture that the basic ERM method obtains a high probability O(d/n) rate, and a potential path to show this rate would be to control a localized complexity as done by Sridharan et al. (2009) but using a more involved argument based on exp-concavity rather than strong convexity.", "startOffset": 95, "endOffset": 778}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002). Is it a limitation of algorithmic stability techniques that high probability O(d/n) fast rates seem to be out of reach without a posthoc confidence boosting procedure, or are we simply missing the right perspective? One reason to avoid a confidence boosting procedure is that the resulting bounds suffer from a multiplicative log(1/\u03b4) factor rather than the lighter effect of an additive log(1/\u03b4) factor in bounds like Theorem 1. As we mentioned earlier, we conjecture that the basic ERM method obtains a high probability O(d/n) rate, and a potential path to show this rate would be to control a localized complexity as done by Sridharan et al. (2009) but using a more involved argument based on exp-concavity rather than strong convexity. We also developed high probability quantile-like risk bounds for model selection aggregation, one with an optimal rate and another with a slightly suboptimal rate but no explicit dependence on the Lipschitz continuity of the loss. However, our bound form is not yet a full quantile-type bound; it degrades when the GAP term is large, while the bound of Lecu\u00e9 and Rigollet (2014) does not have this problem.", "startOffset": 95, "endOffset": 1245}], "year": 2016, "abstractText": "We present an algorithm for the statistical learning setting with a bounded exp-concave loss in d dimensions that obtains excess riskO(d log(1/\u03b4)/n) with probability at least 1\u2212\u03b4. The core technique is to boost the confidence of recent in-expectation O(d/n) excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity. We also show that with probability 1\u2212\u03b4 the standard ERM method obtains excess riskO(d(log(n)+ log(1/\u03b4))/n). We further show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner. Lastly, we present two high probability bounds for the exp-concave model selection aggregation problem that are quantile-adaptive in a certain sense. The first bound is a purely exponential weights type algorithm, obtains a nearly optimal rate, and has no explicit dependence on the Lipschitz continuity of the loss. The second bound requires Lipschitz continuity but obtains the optimal rate.", "creator": "LaTeX with hyperref package"}}}