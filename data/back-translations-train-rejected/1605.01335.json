{"id": "1605.01335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Learning from the memory of Atari 2600", "abstract": "We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in NIPS and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.", "histories": [["v1", "Wed, 4 May 2016 16:23:34 GMT  (147kb,D)", "http://arxiv.org/abs/1605.01335v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jakub sygnowski", "henryk michalewski"], "accepted": false, "id": "1605.01335"}, "pdf": {"name": "1605.01335.pdf", "metadata": {"source": "CRF", "title": "Learning from the memory of Atari 2600", "authors": ["Jakub Sygnowski", "Henryk Michalewski"], "emails": ["J.Sygnowski@students.mimuw.edu.pl", "H.Michalewski@mimuw.edu.pl"], "sections": [{"heading": null, "text": "As a benchmark, we used the folding model proposed in [12] and achieved comparable results in all the games we looked at. In the case of Seaquest, surprisingly, we could only train RAM agents that behaved better than the benchmark-only agent. Mixing screen and RAM did not result in improved performance compared to screen-only and RAM-only agents."}, {"heading": "1 Introduction", "text": "An Atari 2600 controller can perform one of 18 actions, and in this thesis we will learn which of these 18 actions is most profitable given the state of the screen or memory. Our work is based on deep Q-Learning [12] - a reinforcement method that can learn to play Atari games. The number of actions available is 4 for Breakout, 18 for Seaquest and 6 for Bowling.ar Xiv: 160 5.01 335v 1 [cs.L G] 4only input from the screen. The number of actions available is 4 for Seaquest and 6 for Bowling.ar Xiv: 160 5.01 335v 1 for learning. The deep Q-Learning algorithms build on Q-Learning, which values Q in its simplest form (see [14, Figure 21.8])."}, {"heading": "2 The setting of the experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Games", "text": "Bowling - Simulation of the game Bowling; the player aims the ball at the pins and then steers the ball; the goal is to hit the pins [4, 17].Breakout - the player bounces the ball with the bat onto the layer of stones; the task is to destroy all the stones; a stone is destroyed when the ball hits it [6, 18].Seaquest - the player commands a submarine that can shoot enemies and rescue divers by bringing them over the water level; the player dies if he fails to bring a diver up before the air mirror of the submarine disappears [15, 20].We have selected these games because each of them offers a unique challenge. Breakout is a relatively simple game where the actions of the player are limited to movements along the horizontal axis. We chose Breakout because catastrophic learning results indicate a fundamental problem that the game would be based on RAM quest of the network 12, which is very interesting for this level of the network learning RAM, the basic RAM is very basic for this level of the network."}, {"heading": "2.2 Technical architecture", "text": "In this thesis, we cite numerical results from 30 experiments we carried out 4. For our experiments, we used Nathan Sprague's implementation of the Deep Q Learning algorithm [13] in Theano [3] and Lasagne [8]. The code uses the Arcade Learning Environment [2] - the standard framework for evaluating agents playing Atari games. Our code with instructions on how to execute it can be found on github [19]. All the experiments were carried out on a Linux computer with a graphics card Nvidia GTX 480. Each of the experiments lasted 1-3 days. A single epoch of RAM-pure training took about half the time of pure screen training for an architecture with the same number of layers."}, {"heading": "2.3 Network architectures", "text": "We conducted experiments with four neural network architectures that accept the RAM state as (part) of the input, scaling RAM input by 256, so that all input is between 0 and 1. All hyperparameters of the network we are looking at are the same as in [12], unless otherwise specified (see Appendix A). We just changed the size of the playback memory to \u2248 105 items, so that it fits in 1.5 GB of Nvidia GTX 480 memory5."}, {"heading": "3 Plain approach", "text": "Here we present the results of the formation of RAM-only networks only RAM and large RAM as well as the benchmark model nip.Neural Network 1: RAM only (outputDim) Input: RAM Output: A vector of length outputDim1 hiddenLayer1 \u2190 DenseLayer (RAM, 128, correct) 2 hiddenLayer2 \u2190 DenseLayer (hiddenLayer1, 128, correct) 3 Output \u2190 DenseLayer (hiddenLayer2, outputDim, no activation) 4 Return out4The total number of experiments exceeded 100, but this excludes experiments with other models and repetitions of the experiments described in this paper.5We have no statistically significant change in the results when switching between playback memory size of 105 and 5 \u00b7 105. The next architecture considered consists of the above network with two additional dense layers: Neural Network 2: large RAM (outputDim) Output: A vector of the length of layer1 Dimer1 (D1)."}, {"heading": "4 Regularization", "text": "To address this problem, we have used dropout [16], a standard regulation technique for neural networks. Dropout is a simple but effective regulation method. It consists in switching off each neuron in training with the probability of p, i.e. setting the output of each neuron to 0, regardless of its input. In reverse propagation, the parameters of switched-off nodes are not updated. During the test, all neurons are then set to \"on\" - they work as in normal training, except that the output of each neuron is multiplied by p to compensate for the distorted training. The intuition behind the dropout method is that it forces each node to learn in the absence of other nodes. [23] The work shows experimental evidence that the drop-out method actually reduces the results of the Q2 test."}, {"heading": "5 Decreasing learning rate", "text": "The learning rate is a parameter of the rmsprop algorithm that determines how the parameters are changed in each step. Greater learning rates correspond to a faster movement in the parameter space, making learning faster but noisier. We expected that the drastic performance changes between successive eras, as shown in Figures 2 and 3, could be caused by exceeding optimal values for too large steps. If so, the reduction in step size should lead to slower learning, coupled with greater precision in determining minimal loss functions. The results of these experiments can be found in Table 3. Compared with training without regulation, the values only improved in the case of breakout and the Just-ram network, but not by a large distance."}, {"heading": "6 Frame skip", "text": "Atari 2600 is designed to use an analog TV as an output device with 60 new frames appearing on the screen every second. To simplify the search space, we impose a rule that an action is repeated over a fixed number of frames every second. This fixed number is referred to as frame skip. The default frame skip used in [12] is 4. For this frame skip, the agent makes a decision on the next step every 4 \u00b7 160 = 1 15 of a second. Once the decision is made, it remains unchanged for the next 4 frames. Low frame skip allows the network to learn strategies based on a superhuman reflex. High frame skip will limit the number of strategies, therefore learning can be faster and more successful. In the benchmark agent nips that are trained with the frame skip 4, all 4 frames for training along with the sum of rewards that come after them, this is sometimes tarded by the fact that few objects are used."}, {"heading": "7 Mixing screen and memory", "text": "One of the hopes of the future work is to integrate the information from the RAM and the information from the screen in order to train an ultimate Atari 2600 agent. In this work we have taken some first steps towards this goal. We are looking at two mixed network architectures. The first is mixed RAM, in which we simply link the output of the last hidden layer of the Convolutionary Network to the RAM input and then apply a linear transformation in the last layer, without any subsequent non-linearity. Neural Network 3: mixed RAM (outDim) input: RAM, screen output: A vector of the length outputDim1 conv2DLayer (screen, correct) 2 conv2DLayer (convex, correct) 3 hidden layers (convective layer 2, correct) 3 hidden \u2190 DenseLayer (convective layer 2, correct) 256, we use the vector layer (convex, correct), 4 (RAM) non-verbal layer."}, {"heading": "8 RAM visualization", "text": "Each column in Figure 7 corresponds to one of 128 nodes in the first layer of the trained Big Aries network, and each row corresponds to one of 128 memory cells. The color of a cell in a particular column describes whether the high value in that RAM cell affects the activation level for that neuron negatively (blue) or positively (red). Figure 7 suggests that the RAM cells with the numbers 95-105 in Breakout and 90-105 in Seaquest are important for gameplay - the behavior of large Aries networks depends greatly on the state of those cells."}, {"heading": "9 Conclusions", "text": "We trained a number of neural networks that are able to play Atari 2600 games: bowling, breakout and Seaquest. The novel aspect of this work is that the networks use information stored in the console memory. In all games, the RAM agents are on a par with the agent nips that are only on the screen. RAMagents trained using the methods described in this work, without being aware of the more abstract features of the games, such as counters that control the amount of oxygen or the number of divers in seawater. In the case of Seaquest, even a simple ram architecture with an appropriately chosen FRAME SKIP parameter, as well as the large ram agent with standard parameters, performs better than the benchmark nips agent. In the case of Breakout, the performance is below the agent nips that are only on the screen, but still reasonable. In the case of bowling methods that are presented in [12], the agents are not included in this set, as are very helpful."}, {"heading": "10 Future work", "text": "10.1 Refined logic games Since the performance of RAM-only networks is quite good in the case of Seaquest, games like Pacman or Space Invaders would be a natural next target, offering interesting tactical challenges similar to Seaquest."}, {"heading": "10.2 More sophisticated architecture and better hy-", "text": "Recent work [11, 21, 9, 25] presents more complex ideas for improving deep Q networks. We would like to see if these improvements can be applied to RAM models as well. It would also be interesting to set hyperparameters so that they are specifically tailored to the needs of RAM-based neural networks. In particular, we are interested in: \u2022 better understanding of what the deep Q network learns about specific memory cells; can critical cells in memory be identified? \u2022 Improving learning stability and reducing variance and overfitting \u2022 more effective linking of information from screen and memory \u2022 Attempting to develop more complex, deeper architectures for RAM."}, {"heading": "10.3 Recurrent neural networks and patterns of mem-", "text": "By reading the RAM state while executing the Deep Q Learning algorithm, we gain access to a virtually unlimited stream of Atari 2600 memory states. We can use this current to build a recursive neural network that takes into account previous RAM states. In our opinion, it would also be interesting to train an autoencoder. It could help to identify RAM patterns used by Atari 2600 programmers, and to find better output parameters of the neural network [22]."}, {"heading": "10.4 Patterns as finite state machines", "text": "The work of D. Angluin [1] introduced the concept of learning the structure of a finite state machine by means of queries and counter-examples. A game for the Atari 2600 can be identified with a finite state machine that takes the memory state and action as input and outputs a different memory state. We are interested in developing a neural network that would learn the structure of this finite state machine. Successive layers of the network would get to know sub-automata responsible for certain memory cells, and later layers would connect the automata to an automaton that would act on the entire memory state."}, {"heading": "11 Acknowledgements", "text": "This research was carried out with the support of the GG63-11 funding programme of the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) of the University of Warsaw. We would like to thank Marc G. Bellemare for his contribution to this research topic."}, {"heading": "A Parameters", "text": "The list of hyperparameters and their descriptions. Most descriptions come from [11]. Hyperparameter Value Description Minibatch Size 32 Number of training cases over which each stochastic gradient (SGD) update is calculated. Memory size 100,000 SGD updates are randomly sampled from this number of recent frames. 4 The number of recent frames experienced by the agent as input to the Q Learning network in the case of networks, the screens as the Input.update rule rmsprop Name of the algorithm for optimizing the objective functional learning rate of the neural network 0.0002 The learning rate for rmspropdiscount 0.95 discount factor used in the Q Learning update. Measurements how much less do we value our expectation of the state compared to observed reward."}], "references": [{"title": "Learning Regular Sets from Queries and Counterexamples", "author": ["Dana Angluin"], "venue": "Inf. Comput", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Theano: a CPU and GPU Math Expression Compiler", "author": ["James Bergstra"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Frame Skip Is a Powerful Parameter for Learning to Play Atari", "author": ["Alex Braylan"], "venue": "Workshop on Learning for General Competency in Video Games", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A Comparison of learning algorithms on the Arcade Learning Environment", "author": ["Aaron Defazio", "Thore Graepel"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning", "author": ["Yitao Liang"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Classical Planning with Simulators: Results on the Atari Video Games", "author": ["Nir Lipovetzky", "Miquel Ramirez", "Hector Geffner"], "venue": "In: International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih"], "venue": "Nature", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Playing Atari With Deep Reinforcement Learning", "author": ["Volodymyr Mnih"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Artificial Intelligence - A Modern Approach (3", "author": ["Stuart J. Russell", "Peter Norvig"], "venue": "internat. ed.) Pearson Education,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava"], "venue": "Journal of Machine Learning Research", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["Pascal Vincent"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908. Helsinki, Finland: ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "An empirical analysis of dropout in piecewise linear networks", "author": ["David Warde-Farley"], "venue": "url: http://arxiv.org/abs/1312", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Technical Note Q- Learning", "author": ["Christopher J.C.H. Watkins", "Peter Dayan"], "venue": "Machine Learning", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1992}, {"title": "Dueling Network Architectures Learning", "author": [], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "Our work is based on deep Q-learning [12] \u2013 a reinforcement learning algorithm that can learn to play Atari games using \u2217J.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "The deep Q-learning algorithm builds on the Q-learning [24] algorithm, which in its simplest form (see [14, Figure 21.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "One can arbitrarily restrict the number of features which can be learned, but instead of using manually devised features, the deep Qlearning algorithm presented in [12] builds them in the process of training of the neural network.", "startOffset": 164, "endOffset": 168}, {"referenceID": 8, "context": "In this work we use the same computational infrastructure as in [12], including the above decisions (1)-(3).", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].", "startOffset": 67, "endOffset": 78}, {"referenceID": 7, "context": "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].", "startOffset": 67, "endOffset": 78}, {"referenceID": 5, "context": "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].", "startOffset": 67, "endOffset": 78}, {"referenceID": 6, "context": "The work [10] implemented a classical planning algorithm on the RAM state.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Nevertheless, the learning in [10] happens during the gameplay, so it depends on the time limit for a single move.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "In contrast, in [12] the learning process happens before the gameplay - in particular the agent can play in the real-time.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "To the best of our knowledge the only RAM-based agent not depending on search was presented in [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "The work [12] as the main benchmark", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "The changes to the deep Q-learning algorithm proposed in [11] came at a cost of making computations more involved comparing to [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "The changes to the deep Q-learning algorithm proposed in [11] came at a cost of making computations more involved comparing to [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "In this work we decided to use as the reference result only the basic work [12], which is not the state of the art, but a single training of a neural network can be contained in roughly 48 hours using the experimental setup we describe below.", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "From this perspective the basic results in [12] seem to be a perfect benchmark to verify feasibility of learning from RAM.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "The deep Q-network for Seaquest constructed in [12] plays at an amateur human level and for this reason we consider this game as a tempting target for improvements.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "For our experiments we made use of Nathan Sprague\u2019s implementation of the deep Q-learning algorithm [13] in Theano [3] and Lasagne [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "The code uses the Arcade Learning Environment [2] \u2013 the standard framework for evaluating agents playing Atari games.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "All the hyperparameters of the network we consider are the same as in [12], if not mentioned otherwise (see Appendix A).", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "To tackle this problem we\u2019ve applied dropout [16], a standard regularization technique for neural networks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "The work [23] shows an experimental evidence that the dropout method indeed reduces the variance of the learning process.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "The standard frame skip used in [12] is 4.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "The work [5] suggests that choosing the right frame skip can have a big influence on the performance of learning algorithms (see also [7]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "The work [5] suggests that choosing the right frame skip can have a big influence on the performance of learning algorithms (see also [7]).", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "As noticed in [5], in the case of Breakout high frame skips, such as FRAME SKIP = 30, lead to a disastrous performance.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "In the case of of Bowling methods presented in [12] as well as those in this paper are not very helpful \u2013 the agents play at a rudimentary level.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.", "startOffset": 18, "endOffset": 33}, {"referenceID": 11, "context": "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.", "startOffset": 18, "endOffset": 33}, {"referenceID": 5, "context": "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.", "startOffset": 18, "endOffset": 33}, {"referenceID": 15, "context": "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.", "startOffset": 18, "endOffset": 33}, {"referenceID": 12, "context": "It may help to identify RAM patterns used by Atari 2600 programmers and to find better initial parameters of the neural network [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "Angluin [1] introduced the concept of learning the structure of a finite state machine through queries and counter-examples.", "startOffset": 8, "endOffset": 11}], "year": 2016, "abstractText": "We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.", "creator": "LaTeX with hyperref package"}}}