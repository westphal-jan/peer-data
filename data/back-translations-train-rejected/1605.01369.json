{"id": "1605.01369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Accelerating Deep Learning with Shrinkage and Recall", "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.", "histories": [["v1", "Wed, 4 May 2016 18:17:37 GMT  (276kb,D)", "https://arxiv.org/abs/1605.01369v1", null], ["v2", "Mon, 19 Sep 2016 19:27:39 GMT  (4119kb)", "http://arxiv.org/abs/1605.01369v2", "The 22nd IEEE International Conference on Parallel and Distributed Systems (ICPADS 2016)"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["shuai zheng", "abhinav vishnu", "chris ding"], "accepted": false, "id": "1605.01369"}, "pdf": {"name": "1605.01369.pdf", "metadata": {"source": "CRF", "title": "Accelerating Deep Learning with Shrinkage and Recall", "authors": ["Shuai Zheng", "Abhinav Vishnu", "Chris Ding"], "emails": ["abhinav.vishnu@pnnl.gov", "zhengs123@gmail.com,", "chqding@uta.edu"], "sections": [{"heading": null, "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "II. MOTIVATION", "text": "The amount of data in our world has exploded. [14] A lot of big data technologies, including cloud computing, have been proposed [15], [16], [17], [18], [19]. Analyzing large data using machine learning algorithms requires special hardware implementations and large amounts of runtime. SVM [20] solves the following optimization problem: min w, [16], [17], [17], [18], [19]. Analyzing large data using screening algorithms requires special hardware implementations and large amounts of optimization time."}, {"heading": "III. SHRINKING DEEP LEARNING", "text": "Given the checkpoint xi-p \u00b7 1, i = 1, 2,..., n, let the class indicator vector be y (0) i-p \u00b7 1 \u00b7 c, where n is the number of test samples, c is the number of classes, yi has all 0s except one 1 to indicate the class of that test point. Let the output of a neural mesh for the checkpoint xi-1 \u00b7 c-yi contain continuous values and the ith series of Y is."}, {"heading": "A. Standard Deep Learning", "text": "(1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 0 (1): 1 (1): 1 (1) (1): 1 (1): 1 (1): 1 (1): 1 (1): 1 (1): 1 (1): 1 (1) (1) (1) (1) (1) (1) (1) (1): 1) (1) (1) (1) (1): 1 (1) (1) (1): 1 (1) (1): 1 (1) (1) (1): 1 (1) (1) (1): 1 (1) (1) (1): 1 (1) (1) (1): 1 (1) (1) (1) (1) (1): 1 (1) (1) (1): 1 (1) (1) (1) (1): 1 (1) (1) (1): 1 (1) (1) (1) (1): 1 (1) (1) (1) (1): 1 (1) (1): 1 (1) (1): 1 (1) (1) (1) (1) (1: 1) (1) (1: 1) (1) (1) (1: 1) (1) (1) (1: 1) (1: 1) (1: 1) (1) (1) (1) (1) (1: 1) (1: 1) (1: 1) (1) (1) (1) (1: 1) (1: 1) (1) (1) (1: 1) (1 (1: 1) (1) (1) (1: 1) (1) (1) (1) (1: 1) (1) (1) (1) (1: 1) (1) (1) (1: 1) (1 (1) (1) (1"}, {"heading": "B. Shrinking Deep Learning", "text": "In order to accelerate the calculation and inspired by techniques of shrinking in SVM and screening of LASSO, we propose to eliminate the shrinkage of deep learning in algorithm 2 by eliminating samples with small errors (Eq. (4)) from training data and to use less data for the training threshold. algorithm 2 specifies the outline of shrinkage of deep learning (sDL). Compared with standard deep learning samples in algorithm 1, sDL requires two additional inputs, elimination rate s and stop thresholds. s is a percentage indicating the amount of training data to be eliminated during an epoch, t is a number indicator to stop the elimination of training data when nepoch < t, where nepoch is current number of training data. We maintain an index vector A. In algorithm 1, both forward and backward algorithm 2, all of the training data is applied to a training set."}, {"heading": "IV. SHRINKING WITH RECALL", "text": "As the training data in sDL is dwindling, the weight parameter w exercises is based on the subset of training data. It is not optimized for the entire training data set. To deal with this situation, we are now introducing shrinking Deep Learning with Recall (Algorithm 3). To use all training data when the number of active training samples nepoch < t increases, we start using all training samples, as shown in Algorithm 3, A = A0. Algorithm 3 ensures that the trained model is optimized for the entire training data. Shrinking with recall of Algorithm 3 results in competitive classification performance with standard Deep Learning of Algorithm 1. In the experiment, we are also investigating the impact of the threshold t on the classification results (see Figure 7)."}, {"heading": "V. EXPERIMENTS", "text": "In the experiment we test our algorithms using datasets of different domains using 5 different random initializations. The datasets we use are listed in Table I. MNIST is a standard dataset of handwritten digits; CIFAR-10Table II: MNIST-Classification Error Improvement (IMP) and Training Period Speedup.Method DNN sDNNr IMP / Speedup DBN sDBNr IMP / Speedup CNN sCNNr IMP / Speedup Test Error 0.0387 0.0324 16.3% 0.0192 0.0182 5.21% 0.0072 0.0073 \u2212 1.39% Training Period (s) 1653 805 2.05 1627 700 2.32 1431 2.130 50 1000.040.0450.0550.06IterationC IterationC IterationC Iterationation test erro r784 \u2212 100 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 200.000.000.000.000.000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "A. Results on MNIST", "text": "It is a question of whether and in what form people are able to surpass themselves, and of whether and how they are able to surpass themselves. (...) It is a question of whether and in which form they can surpass themselves. (...) It is a question of whether and in which form they must surpass themselves. (...) It is a question of whether and in which form they can surpass themselves. (...) It is a question of whether and in which form they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...)"}, {"heading": "B. Results on CIFAR-10", "text": "CIFAR-10 [25] data contains 60,000 32 \u00d7 32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. CIFAR-10 is an object data set that includes airplane, car, bird, cat, etc., and classes are completely mutually exclusive. In our experiment, we use the CNN network to evaluate performance in terms of classification errors. The network architecture uses 5 revolutionary layers: each revolutionary layer is followed by a maximum pooling layer for the first three layers; the fourth revolutionary layer is followed by a ReLU layer; the fifth layer is followed by a softmax loss output layer. Table III shows the classification error and training time. Top-1 classification error in Table III means that predictive identification is determined with maximum probability only by considering the class."}, {"heading": "C. Results on Higgs Boson", "text": "Each sample is a signal process that produces either Higgs bosons or not. We use 7 high-level features derived from physicists to distinguish particles between the two classes. Both activation and output functions were sigmoid functions. DNN batch size is 100 and the recall threshold t = 20% x 50 000. We test on different network settings and select the best ones. Table IV shows the experimental results using different networks."}, {"heading": "D. Results on Alternative Splicing", "text": "Alternative splicing [27] is a set of RNA sequences used in bioinfomatics. It contains 3446 cassette-like mouse exons with 1389 characteristics per exon. We randomly select 2500 exons for training and use the rest for testing. For each exon, the dataset contains three real-rated positive predictive targets yi = [qinc qexc qnc], depending on the probability that the exon is more likely to be absorbed into the given tissue, is more likely to be excluded or shows no change compared to other tissues. To demonstrate the effectiveness of the proposed shrinkage approach \"Deep Learning with Recall,\" we use a simple DNN network with different numbers of layers and neurons with optimal tangential activation function and sigmoid output function. We use the following average sum of squared error criteria to evaluate the model performance error."}, {"heading": "VI. CONCLUSION", "text": "Finally, we proposed a shrinking approach to Deep Learning with Recall (sDLr), and the most important contribution of sDLr is that it can significantly shorten runtime. Extensive experiments with 4 data sets show that shrinking deep learning with Recall can significantly shorten training time, while still offering competitive classification performance."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Three classes of deep learning architectures and their applications: a tutorial survey", "author": ["L. Deng"], "venue": "APSIPA transactions on signal and information processing, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Universit\u00e4t Dortmund, Tech. Rep., 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast support vector machines using parallel adaptive shrinking on distributed systems", "author": ["J. Narasimhan", "A. Vishnu", "L. Holder", "A. Hoisie"], "venue": "arXiv preprint arXiv:1406.5161, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 1070\u20131078.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A dynamic screening principle for the lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European. IEEE, 2014, pp. 6\u201310.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data: The next frontier for innovation, competition, and productivity", "author": ["J. Manyika", "M. Chui", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers"], "venue": "2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A closed form solution to multi-view low-rank regression.", "author": ["S. Zheng", "X. Cai", "C.H. Ding", "F. Nie", "H. Huang"], "venue": "in AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Kernel alignment inspired linear discriminant analysis", "author": ["S. Zheng", "C. Ding"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2014, pp. 401\u2013416.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Tidewatch: Fingerprinting the cyclicality of big data workloads", "author": ["D. Williams", "S. Zheng", "X. Zhang", "H. Jamjoom"], "venue": "IEEE INFOCOM 2014-IEEE Conference on Computer Communications. IEEE, 2014, pp. 2031\u20132039.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Virtual machine migration in an over-committed cloud", "author": ["X. Zhang", "Z.-Y. Shae", "S. Zheng", "H. Jamjoom"], "venue": "2012 IEEE Network Operations and Management Symposium. IEEE, 2012, pp. 196\u2013203.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis and modeling of social influence in high performance computing workloads", "author": ["S. Zheng", "Z.-Y. Shae", "X. Zhang", "H. Jamjoom", "L. Fong"], "venue": "European Conference on Parallel Processing. Springer Berlin Heidelberg, 2011, pp. 193\u2013204.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, vol. 9, no. 3, pp. 293\u2013300, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Exponential smoothing: The state of the art", "author": ["E.S. Gardner"], "venue": "Journal of forecasting, vol. 4, no. 1, pp. 1\u201328, 1985.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1985}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": "Technical University of Denmark, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Matconvnet-convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "arXiv preprint arXiv:1412.4564, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature communications, vol. 5, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian prediction of tissue-regulated splicing using rna sequence and cellular context", "author": ["H.Y. Xiong", "Y. Barash", "B.J. Frey"], "venue": "Bioinformatics, vol. 27, no. 18, pp. 2554\u20132562, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep Learning [1] has become a powerful machine learning model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Deep architectures using multiple layers outperform shadow models [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Secondly, there is no need to extract human design features [3], which can reduce the dependence of the quality of human extracted features.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 3, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 212, "endOffset": 215}, {"referenceID": 4, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 250, "endOffset": 253}, {"referenceID": 5, "context": "Deep Belief Network (DBN) is a generative unsupervised pretraining network which uses stacked RBMs [6] during pretraining.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "DBN has undirected connections between its first two layers and directed connections between all its lower layers[7] [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "DBN has undirected connections between its first two layers and directed connections between all its lower layers[7] [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "CNN forces the extraction of local features by restricting the receptive fields of hidden neurons to be local [9].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "Inspired from the shrinking technique [10] [11] used in accelerating computation of Support Vector Machines (SVM) algorithm", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "Inspired from the shrinking technique [10] [11] used in accelerating computation of Support Vector Machines (SVM) algorithm", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "and screening [12] [13] technique used in LASSO, we propose an accelerating algorithm shrinking Deep Learning with Recall (sDLr).", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "and screening [12] [13] technique used in LASSO, we propose an accelerating algorithm shrinking Deep Learning with Recall (sDLr).", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Analyzing large data sets, so-called big data, will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer interest [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "SVM [20] solves the following optimization problem:", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "Since SVM learning problem has much less support vectors than training examples, shrinking [10] [11] was proposed to eliminate training samples for large learning tasks where the fraction of support vectors is small compared to the training sample size or when many support vectors are at the upper bound of Lagrange multipliers.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Since SVM learning problem has much less support vectors than training examples, shrinking [10] [11] was proposed to eliminate training samples for large learning tasks where the fraction of support vectors is small compared to the training sample size or when many support vectors are at the upper bound of Lagrange multipliers.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "LASSO [21] is an optimization problem to find sparse representation of some signals with respect to a predefined dictionary.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Screening [12] [13] is a technique used to reduce the size of dictionary using some rules in order to accelerate the computation of LASSO.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Screening [12] [13] is a technique used to reduce the size of dictionary using some rules in order to accelerate the computation of LASSO.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "We use exponential smoothing [22] to adjust the threshold used in batch i+1: instead of using ti+1 as the threshold to eliminate samples, we use the following t\u2032i+1: t\u2032i+1 = \u03b1t \u2032 i + (1\u2212 \u03b1)ti+1, (9)", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We use DNN and DBN implementation from [23] and CNN implementation from [24].", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "We use DNN and DBN implementation from [23] and CNN implementation from [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "CIFAR-10 [25] data contains 60,000 32\u00d7 32 color image in 10 classes, with 6,000 images per class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Higgs Boson is a subset of data from [26] with 50, 000 training and 20, 000 testing.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Alternative Splicing [27] is a set of RNA sequences used in bioinfomatics.", "startOffset": 21, "endOffset": 25}], "year": 2016, "abstractText": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance. Keywords-Deep Learning; Deep Neural Network (DNN); Deep Belief Network (DBN); Convolution Neural Network (CNN)", "creator": "LaTeX with hyperref package"}}}