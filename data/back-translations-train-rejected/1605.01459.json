{"id": "1605.01459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach", "abstract": "In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real-time, anticipate future actions, and synthesize their own motion accordingly. We explore this within the context of joint action, where humans and robots move together synchronously. In this paper, we present an anticipation method which takes high-level group behavior into account. We validate the method within a human-robot interaction scenario, where an autonomous mobile robot observes a team of human dancers, and then successfully and contingently coordinates its movements to \"join the dance\". We compared the results of our anticipation method to move the robot with another method which did not rely on high-level group behavior, and found our method performed better both in terms of more closely synchronizing the robot's motion to the team, and also exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high-level group behavior than when it does not. This work will help enable others in the robotics community to build more fluent and adaptable robots in the future.", "histories": [["v1", "Wed, 4 May 2016 23:48:16 GMT  (1307kb,D)", "http://arxiv.org/abs/1605.01459v1", "11 pages, 7 figures, IEEE Transactions on Robotics 2016 preprint"]], "COMMENTS": "11 pages, 7 figures, IEEE Transactions on Robotics 2016 preprint", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["tariq iqbal", "samantha rack", "laurel d riek"], "accepted": false, "id": "1605.01459"}, "pdf": {"name": "1605.01459.pdf", "metadata": {"source": "CRF", "title": "Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach", "authors": ["Tariq Iqbal", "Samantha Rack", "Laurel D. Riek"], "emails": ["lriek}@nd.edu."], "sections": [{"heading": null, "text": "It is indeed the case that we will be able to manoeuvre ourselves into a situation in which we see ourselves in a position in which we are able to change the world in which we find ourselves."}, {"heading": "II. SYSTEM ARCHITECTURE AND EXPERIMENTAL TESTBED", "text": "To explore how a robot could use human group dynamics to synthesize SJA with a mixed team, we needed an experimental test field where a robot could perform tasks in sync with humans. We also needed a group activity where each member's actions had an impact on the actions of others as well as on the dynamics of the group as a whole. So we designed a motion task where a team of humans and a robot could coordinate their movement in real time. Specifically, we examined SJA in the context of synchronous dance. Together with an experienced dancer, we choreographed a routine to Michael Jackson's song Smooth Criminal, performed in 4 / 4 time. Dance is iterative and is performed cyclically counterclockwise (see Figure 1-A.) There are four iterations in a dance session that correspond to each of the cardinal directions (North, West, South and East). Each iteration includes dancers performing the following steps backwards and forwards (2)."}, {"heading": "A. Data Acquisition Process", "text": "Figure 1-A shows the structure of the data acquisition. Three human participants and a Turtlebot v.2 robot were arranged in two rows. Four Microsoft Kinect v.2 sensors were located about three meters above the ground in each direction. Each sensor was connected to a computer (client) to capture and process Kinect depth, infrared and skeletal data. All four clients and the server were running Windows 8 on an Intel Core i5 processor with 1.70Hz and 12GB of RAM. Since we study synchronous activities, it was critical for all clients that the robot maintained a consistent time reference. Thus, we created a server to manage communication and global time synchronization. Details of the synchronization architecture can be found in Iqbal et al. [43]. Each client performed real-time processing of the raw data to detect dance events (e.g. forward motion, stop, etc.) that were received on a server with a time sensor."}, {"heading": "B. Client-side data processing", "text": "We extracted five high-level events from participants \"movements during the dance: begin to move forward, stop moving forward, begin to move backwards, stop moving backwards, and clap. The initial motion event is detected when a participant approaches the Kinect, and stop moving forward when he stops moving. Similarly, we detected participants\" gossip events that occurred at the end of each iteration. See Figure 2. To detect these events from participants \"body movements, the clients used the skeletal positions provided by the Kinect. Clients calculated forward and backward motion set along the z-axis primarily based on the position of the spinal base joints, as it is the most stable and reliable joint position when the participants are moving. However, there were times when participants did not move their spine, clips were placed around the shoulders, or the four additional clips were placed in the early movement, too."}, {"heading": "C. Robot Command Generation and Execution", "text": "After determining which movement the robot should perform, which it does using a method of anticipation described in III, the server sends a motion command to the robot. These commands include: forward, backward, stop and rotate. The server has translated the clapping commands into rotation commands while sending them to the robot because the robot cannot clap. The robot, which under Ubuntu version 12.04 operated the version of the robot operating system (ROS) Hydro, accepted commands from the server, analyzed the commands and used a ROS publisher to send motion commands to the controller. The robot is able to move forward and backward and can rotate in both directions on its vertical axis."}, {"heading": "III. EVENT ANTICIPATION METHODS", "text": "For this work, we have developed two anticipation methods to move the robot: The first method, the Synchronization Index Based Anticipation (SIA), is inspired by our previous SJA detection work [39]. It calculates the synchronicity of the group in real time, determines who the synchronous dancer is, and uses this information to move the robot. The second method, Event Cluster Based Anticipation (ECA), we have developed to establish a reasonable comparison method for SIA that is not based on group dynamics. ECA is a simple method that takes into account the average times of participants during an earlier iteration of the dance. Figure 3 provides a visual comparison of how the two methods work in practice, and they are described in the following text."}, {"heading": "A. Synchronization Index Based Anticipation (SIA)", "text": "The SIA method takes into account the internal dynamics of a group in the creation of robot movements. The main idea is that for a given iteration, the participant who moves most in sync with the other dancers is always a good model that the robot must follow in order to be well coordinated with the team. In addition, the method will adjust their identification of the synchronous dancer after each iteration. Figure 3-B explains this method visually. To generate future actions for the robot, we measured the synchronous person of the group at the beginning of each iteration using the non-linear dynamic method we described in Iqbal and Riek. We will briefly describe the method in sections III-A1 and III-A2 and then discuss in section III how we use the method of evaluating the synchronous dancers to move."}, {"heading": "B. Event Cluster-Based Anticipation Method (ECA)", "text": "We developed the ECA method to establish a reasonable comparison method for SIA that is not based on group dynamics. ECA is theoretically simple but powerful in nature. Because dance is rhythmic and iterative in nature, the motion events for an iteration are similar to events that occurred in the previous iteration. Therefore, we averaged the timing of events during an iteration to predict the timing of the same events for the next iteration. Figure 3-A explains this method visually. First, for an iteration, we represented all events associated with the dancers by a time series. Thus, this time series represented all events of that iteration. Then, we grouped together all similar types of events that occurred within a time threshold. For example, for a single event, we calculated the timing of the event executed by three human participants, i.e. t (dancer1), e), (dancer2), danceri (), (i)."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Pilot Studies", "text": "Before conducting the main experiment to compare the performance between the two anticipation methods, we conducted a series of pilot studies to test the structure of the system and determined various parameters for the two anticipation methods. We conducted two groups of pilot studies with a total of seven participants (three women, four men), the participants were opportunistically recruited and rewarded with a $5 voucher for participation [3]. During the first group of pilot studies, a single participant danced with the robot. Here, we tried to measure two things: how fast the robot received action messages and how accurately the robot worked with the human participant. During the second group of pilot studies, a group of three participants danced with the robot. Here, we tried to determine appropriate parameters for the anticipation methods. In order to obtain these measurements, we recorded events generated from server logs as well as from odometry data from the robot. We compared the two and determined differences in distance, and event times."}, {"heading": "B. Main experiment", "text": "We recruited a total of nine groups (a total of 27 participants, 3 persons per group) for our main experiment. 14 participants were women, 13 were men. Their average age was 22.93 years (s.d. = 3.98 years), and the majority were students and doctoral students. Only 3 participants had previous dance experience, 24 did not. Participants were recruited via mailing list and campus advertisements. In scheduling a time window, participants were randomly assigned to join a group with two others. Each participant was compensated for their time with an $8 voucher. After informed consent, participants watched a video of the choreographed dance and the experimenters explained the various movements. Subsequently, participants had time to practice the dance movements as often as a group as they wished. During this practice session, the robot did not dance with them. Following the practice session, the group participated in three dance sessions. During the first session, only people participated in the dance."}, {"heading": "V. ROBOT DATA PRE-PROCESSING", "text": "The server provided the logs of human movement and the raw data of the clients during the experiment, as detailed in Sections II and IV-B. However, in order to make a complete comparison between the two anticipation methods, it is also necessary to determine how and when the robot actually moved during the two experimental sessions using the time-stamped odometric data of the robot (X and Y pose and angle alignment \u2212 z) as well as the server-side robot command logs. We calculated the same events for the robot as for the human (forward, backward, stop, rotate). Based on the changes in two consecutive X or Y pose values and the robot motion, we calculated whether the robot moved forward or backward. For example, if the robot faced the first Kinect sensor and moved forward, the changes in two consecutive pose values were positive, if it moved backward, negative."}, {"heading": "VI. DATA ANALYSIS AND RESULTS", "text": "To compare the performance and accuracy of the two anticipation methods, we first measured how synchronously the entire group, including the robot, coordinated their movements during both sessions, and then measured how timely the movement of the robot was with its human counterparts."}, {"heading": "A. Measuring Synchronization of the Group", "text": "Using the method described in [39] and discussed in Section III-A, the degree of synchronization of the group was measured for each individual iteration of the dance. First, we created individual time series for each of the dancers and the robot. Events in the time series began to move forward, begin to move backwards, stop moving backwards, and gossip and gossip. Then, we calculated the paired synchronization index for each pair using the method used in Section III-A. From the paired synchronization index humans, we built a group of topology graphs (GTG) and calculated the individual synchronization index for each human dancer, as described in Section III-A. as people were physically very close to each other, we assumed that each of the group members was influenced by all the other members of the group over the course of an entire dance session. (Each iteration, each participant rotated their position, so one person in the itri front was connected by each group's itr + 2 analysis)."}, {"heading": "B. Measuring Robot Timing Appropriateness", "text": "For both, it is important that they are able to hold their own, and that they are able to hold their own, \"he said in an interview with The New York Times."}, {"heading": "VII. DISCUSSION AND FUTURE WORK", "text": "The fact is that we will be able to manoeuvre ourselves into a situation in which we will be able, in which we will be able, in which we will be able to change the world."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Afzal Hossain, Olivia Choudhury, James Delaney, Cory Hayes and Michael Gonzales for their support."}], "references": [{"title": "Optimization of temporal dynamics for adaptive human-robot interaction in assembly manufacturing,", "author": ["R. Wilcox", "S. Nikolaidis", "J.A. Shah"], "venue": "Proceedings of Robotics: Science and Systems (RSS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The social co-robotics problem space: Six key challenges,", "author": ["L.D. Riek"], "venue": "Proceedings of Robotics: Science, and Systems (RSS), Robotics Challenge and Visions,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Enabling synchronous joint action in human-robot teams,", "author": ["S. Rack", "T. Iqbal", "L.D. Riek"], "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Adaptive human-robot interaction in sensorimotor task instruction: From human to robot dance tutors,", "author": ["R. Ros", "I. Baroni", "Y. Demiris"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A Socially Assistive Robot Exercise Coach for the Elderly,", "author": ["J. Fasola", "M. Mataric"], "venue": "Journal of Human-Robot Interaction,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A dancing robot for rhythmic social interaction,", "author": ["M.P. Michalowski", "S. Sabanovic", "H. Kozima"], "venue": "Proceedings of the ACM/IEEE International Conference on Human-robot interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Understanding a child\u2019s play for robot interaction by sequencing play primitives using hidden markov models,", "author": ["H.W. Park", "A.M. Howard"], "venue": "in Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Real-time imitation of human whole-body motions by humanoids,", "author": ["J. Koenemann", "F. Burget", "M. Bennewitz"], "venue": "in ICRA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Cooperative gestures: effective signaling for humanoid robots,", "author": ["L.D. Riek", "T.-C. Rabinowitch", "P. Bremner", "A.G. Pipe", "M. Fraser", "P. Robinson"], "venue": "Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Joint action perception to enable fluent human-robot teamwork,", "author": ["T. Iqbal", "M.J. Gonzales", "L.D. Riek"], "venue": "Robot and Human Interactive Communication (RO-MAN),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Cost-based anticipatory action selection for human\u2013robot fluency,", "author": ["G. Hoffman", "C. Breazeal"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Using spatial and temporal contrast for fluent robot-human handovers,", "author": ["M. Cakmak", "S.S. Srinivasa", "M.K. Lee", "S. Kiesler", "J. Forlizzi"], "venue": "Proceedings of the 6th International conference on Human-robot interaction", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A Self-Training Approach for Visual Tracking and Recognition of Complex Human Activity Patterns,", "author": ["J. Bandouch", "O.C. Jenkins", "M. Beetz"], "venue": "International Journal of Computer Vision, vol. 99,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "4-Dimensional Local Spatio-Temporal Features for Human Activity Recognition,", "author": ["H. Zhang", "L.E. Parker"], "venue": "Intelligent robots and systems (IROS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation,", "author": ["H. Koppula", "A. Saxena"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Berkeley MHAD: A comprehensive Multimodal Human Action Database,", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Applications of Computer Vision (WACV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Experiences with an interactive museum tour-guide robot,", "author": ["W. Burgard", "A.B. Cremers", "D. Fox", "D. H\u00e4hnel", "G. Lakemeyer", "D. Schulz", "W. Steiner", "S. Thrun"], "venue": "Artificial intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Social context perception for mobile robots,", "author": ["A. Nigam", "L. Riek"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Hierarchical group dynamics in pigeon flocks,", "author": ["M. Nagy", "Z. \u00c1kos", "D. Biro", "T. Vicsek"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "In the dance studio: Analysis of human flocking,", "author": ["N.E. Leonard", "G. Young", "K. Hochgraf", "D. Swain", "A. Trippe", "W. Chen", "S. Marshall"], "venue": "American Control Conference (ACC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "In the Dance Studio: An Art and Engineering Exploration of Human Flocking,", "author": ["N.E. Leonard", "G.F. Young", "K. Hochgraf", "D.T. Swain", "A. Trippe", "W. Chen", "K. Fitch", "S. Marshall"], "venue": "Controls and Art,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "In time with the music: the concept of entrainment and its significance for ethnomusicology.", "author": ["M. Clayton", "R. Sager", "U. Will"], "venue": "European meetings in ethnomusicology.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Dynamics of interpersonal coordination,", "author": ["R.C. Schmidt", "M.J. Richardson"], "venue": "Coordination: Neural, behavioral and social dynamics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Nonlinear multivariate analysis of neurophysiological signals,", "author": ["E. Pereda", "R.Q. Quiroga", "J. Bhattacharya"], "venue": "Progress in neurobiology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Role of tempo entrainment in psychophysiological differentiation of happy and sad music?", "author": ["S. Khalfa", "M. Roy", "P. Rainville", "S. Dalla Bella", "I. Peretz"], "venue": "International Journal of Psychophysiology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning and synchronising dance movements in south african songs: Cross-cultural motioncapture study,", "author": ["T. Himberg", "M.R. Thompson"], "venue": "Dance Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Movement synchrony and perceived entitativity,", "author": ["D. Lakens"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Synchronization in human musical rhythms and mutually interacting complex systems,", "author": ["H. Hennig"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Joint action: bodies and minds moving together.", "author": ["N. Sebanz", "H. Bekkering", "G. Knoblich"], "venue": "Trends in cognitive sciences,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Measuring group synchrony: a cluster-phase method for analyzing multivariate movement time-series,", "author": ["M.J. Richardson", "R.L. Garcia", "T.D. Frank", "M. Gergor", "K.L. Marsh"], "venue": "Frontiers in Physiology,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A System for Real-Time Multimodal Analysis of Nonverbal Affective Social Interaction in User-Centric Media,", "author": ["G. Varni", "G. Volpe", "A. Camurri"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Sensorimotor synchronization with tempo-changing auditory sequences: Modeling temporal adaptation and anticipation,", "author": ["M.M. van der Steen", "N. Jacoby", "M.T. Fairhurst", "P.E. Keller"], "venue": "Brain research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Recurrence plots for the analysis of complex systems,", "author": ["N. Marwan", "M. Carmenromano", "M. Thiel", "J. Kurths"], "venue": "Physics Reports, vol. 438,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Event synchronization: a simple and fast method to measure synchronicity and time delay patterns,", "author": ["R.Q. Quiroga", "T. Kreuz", "P. Grassberger"], "venue": "Physical review E,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "A Method for Automatic Detection of Psychomotor Entrainment,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Anticipating human actions for collaboration in the presence of task and sensor uncertainty,", "author": ["K.P. Hawkins", "S. Bansal", "N.N. Vo", "A.F. Bobick"], "venue": "in Robotics and Automation (ICRA),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Fast target prediction of human reaching motion for cooperative human-robot manipulation tasks using time series classification,", "author": ["C. P\u00e9rez-DArpino", "J. Shah"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Human-robot co-navigation using anticipatory indicators of human walking motion,", "author": ["V.V. Unhelkar", "C. P\u00e9rez-DArpino", "L. Stirling", "J. Shah"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "A Model for Time- Synchronized Sensing and Motion to Support Human-Robot Fluency,", "author": ["T. Iqbal", "M.J. Gonzales", "L.D. Riek"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), Workshop on Timing in HRI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Commentary: P values and statistical practice,", "author": ["A. Gelman"], "venue": "Epidemiology, vol. 24,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Synchronization in human-robot Musicianship,", "author": ["G. Hoffman", "G. Weinberg"], "venue": "19th International Symposium in Robot and Human Interactive Communication,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Detecting and Synthesizing Synchronous Joint Action in Human-Robot Teams,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Coordination Dynamics in Multi-human Multi-robot Teams,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "Under Review,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Detecting social context: A method for social event classification using naturalistic multimodal data,", "author": ["M. O\u2019Connor", "L. Riek"], "venue": "Automatic Face and Gesture Recognition (FG),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "First-Person Activity Recognition: What Are They Doing to Me?", "author": ["M.S. Ryoo", "L. Matthies"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "The ADaptation and Anticipation Model (ADAM) of sensorimotor synchronization,", "author": ["M.C.M. van der Steen", "P.E. Keller"], "venue": "Frontiers in Human Neuroscience,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Ant groups optimally amplify the effect of transiently informed individuals,", "author": ["A. Gelblum", "I. Pinkoviezky", "E. Fonio", "A. Ghosh", "N. Gov", "O. Feinerman"], "venue": "Nature communications,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Robotic systems have long been involved in assembly lines automating and increasing efficiency of monotonous or dangerous factory procedures [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "appropriately [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "counter people performing various social actions, such as engaging in social activities, or performing synchronous movements [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "[4] used a humanoid robot to play the role of a dance instructor with children, and Fasola et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] designed a socially assistive robot to engage older adults in physical exercise.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "have used robots to dance and play cooperatively with children in therapeutic settings [6], [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "have used robots to dance and play cooperatively with children in therapeutic settings [6], [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "demonstrated a system which enabled humanoid robots to imitate complex human whole-body motion [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "If the robot has this understanding of its environment, then its interactions within the team might reach to a higher-level of coordination, resulting in a fluent meshing of actions [9]\u2013[12].", "startOffset": 182, "endOffset": 185}, {"referenceID": 11, "context": "If the robot has this understanding of its environment, then its interactions within the team might reach to a higher-level of coordination, resulting in a fluent meshing of actions [9]\u2013[12].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Human activity recognition from body movement is an active area of research across many fields [13]\u2013[17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "This understanding is critical in humanrobot interaction scenarios, as the \u201cone human, one robot\u201d paradigm is rarely seen in ecological settings [18], [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "This understanding is critical in humanrobot interaction scenarios, as the \u201cone human, one robot\u201d paradigm is rarely seen in ecological settings [18], [19].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "To make informed decisions, robots need to understand this context [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "Many disciplines have investigated interaction dynamics within groups, which include sociology, psychology, biology, music and dance [20]\u2013[31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "Many disciplines have investigated interaction dynamics within groups, which include sociology, psychology, biology, music and dance [20]\u2013[31].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "[20], [21] investigated collective behavior on animals, and developed automated methods for assessing social dominance and leadership in domestic pigeons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22], [23] investigated how collective group motion emerges when basic animal flocking rules (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22], [23] investigated how collective group motion emerges when basic animal flocking rules (i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "that occurs when two or more participants coordinate their actions in space and time to make changes to the environment [32].", "startOffset": 120, "endOffset": 124}, {"referenceID": 29, "context": "Understanding synchronous joint action is important, as it helps to accurately understand the affective behavior of a team, and also provides information regarding the group level cohesiveness [33], [34].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "Understanding synchronous joint action is important, as it helps to accurately understand the affective behavior of a team, and also provides information regarding the group level cohesiveness [33], [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "It also might learn advanced adaptive coordination techniques the human teams use, such as tempo adaptation or cross-training [1], [35].", "startOffset": 126, "endOffset": 129}, {"referenceID": 31, "context": "It also might learn advanced adaptive coordination techniques the human teams use, such as tempo adaptation or cross-training [1], [35].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "Many approaches have been taken by researchers across different fields to measure the degree of synchronization in continuous time series data, including recurrence analysis [36], correlation [37], and phase difference approaches [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": "Many approaches have been taken by researchers across different fields to measure the degree of synchronization in continuous time series data, including recurrence analysis [36], correlation [37], and phase difference approaches [33].", "startOffset": 230, "endOffset": 234}, {"referenceID": 33, "context": "Other sets of methods work across categorical time series data, which may define discrete events [38].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "To address this gap, we created an event-based method which can successfully take multiple types of discrete, task-level events into consideration while measuring the degree of synchronization of a system [39].", "startOffset": 205, "endOffset": 209}, {"referenceID": 35, "context": "[40] developed a method that determines an appropriate action for an assistive robot to take when providing parts during an assembly activity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed an adaptive action selection mechanism for a robot, which could make anticipatory decisions based on confidence of their validity and their relative risks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41] proposed a data-driven approach to synthesize anticipatory knowledge of human motion, which they used to predict targets during reaching motions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] extended this concept for a human-robot co-navigation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "method which performed more accurately and robustly than existing methods [10], [39].", "startOffset": 74, "endOffset": 78}, {"referenceID": 34, "context": "method which performed more accurately and robustly than existing methods [10], [39].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Each iteration includes the dancers taking the following steps in order: move forward and backward twice, then, clap, and turn 90-degrees (see Figure 2) [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 38, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The first method, synchronization-index based anticipation (SIA), is inspired by our prior SJA detection work [39].", "startOffset": 110, "endOffset": 114}, {"referenceID": 34, "context": "Thus, to generate future actions for the robot using this method, at the beginning of each iteration we measured the most synchronous person of the group using our the non-linear dynamical method we described in Iqbal and Riek [39].", "startOffset": 227, "endOffset": 231}, {"referenceID": 34, "context": "Suppose, mx and my are the number of events occuring in time series x and y respectively, and E is the set of all events [39].", "startOffset": 121, "endOffset": 125}, {"referenceID": 34, "context": "my) respectively [39].", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "In the case of synchronous events in both time series, the same event should appear roughly at the same time, or within a time lag \u00b1\u03c4 [39].", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "On the other hand, Q\u03c4 (e) = 0 shows us that the events are asynchronous [39].", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "While calculating Q\u03c4 (ei), we will not consider any other type of event, except ei [39].", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "We will call this the synchronization index of that pair [39].", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "If no synchronous are synchronous, the value of Q \u03c4 will be 0 [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": ", Q s(H\u22121)sH \u03c4 [39].", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "We modified our process slightly from the description in Iqbal and Riek [39].", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "However, in [39], after calculating the pairwise synchronization index, an undirected weighted graph was built.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "We will refer to this graph as the group topology graph (GTG) [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "We assumed that during this dance performance, each human participant may have some direct or indirect influences on the other human participants of the group [39].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "Participants were opportunistically recruited, and compensated with a $5 gift card for participating [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "We compared the two, noting differences in velocity, distance, and event timings [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 34, "context": "Following the experiment, participants completed a short questionnaire asking them to rate which of the two dance sessions they felt was more synchronous, a measure we have used in prior work [39].", "startOffset": 192, "endOffset": 196}, {"referenceID": 34, "context": "Using the method described in [39] and discussed in Section III-A, we measured the degree of synchronization", "startOffset": 30, "endOffset": 34}, {"referenceID": 34, "context": "we calculated the group synchronization index for each group using the method described in [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "Then, the overall group synchronization index is computed by taking the average of this product [39].", "startOffset": 96, "endOffset": 100}, {"referenceID": 39, "context": "1Note, due to a small sample size (n = 36), it would be dubious to run statistical means comparisons, and one should not accept a p-value with certainty [44].", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": "Instead, we agree with Gelman [44] that reliable patterns can be found by averaging, as reported here.", "startOffset": 30, "endOffset": 34}, {"referenceID": 40, "context": "This measure is similar to the absolute offset measure used in [45], however, the timing appropriateness measure used here is within the context of a group.", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "We are currently exploring the effect of different anticipation methods in multi-human multi-robot scenarios [46], [47].", "startOffset": 109, "endOffset": 113}, {"referenceID": 42, "context": "We are currently exploring the effect of different anticipation methods in multi-human multi-robot scenarios [46], [47].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Moreover, we are also planning to incorporate a decision module for robots, which will use the perceived knowledge to select the best decision from a set of options, based on the context [19], [48].", "startOffset": 187, "endOffset": 191}, {"referenceID": 43, "context": "Moreover, we are also planning to incorporate a decision module for robots, which will use the perceived knowledge to select the best decision from a set of options, based on the context [19], [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "However, we will build on our prior multimodal fusion and others\u2019 robot-centric perception work to overcome this challenge [19], [49].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "However, we will build on our prior multimodal fusion and others\u2019 robot-centric perception work to overcome this challenge [19], [49].", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "Models like ADAM (ADaptation and Anticipation Model) have been proposed in the literature to computationally model this behavior in humans by combining adaptation and anticipation during an activity [35], [50].", "startOffset": 199, "endOffset": 203}, {"referenceID": 45, "context": "Models like ADAM (ADaptation and Anticipation Model) have been proposed in the literature to computationally model this behavior in humans by combining adaptation and anticipation during an activity [35], [50].", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "Movement coordination is an important, emerging research area in robotics, neuroscience, biology, and many other fields [13]\u2013[17], [51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 46, "context": "Movement coordination is an important, emerging research area in robotics, neuroscience, biology, and many other fields [13]\u2013[17], [51].", "startOffset": 131, "endOffset": 135}], "year": 2016, "abstractText": "In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real-time, anticipate future actions, and synthesize their own motion accordingly. We explore this within the context of joint action, where humans and robots move together synchronously. In this paper, we present an anticipation method which takes high-level group behavior into account. We validate the method within a human-robot interaction scenario, where an autonomous mobile robot observes a team of human dancers, and then successfully and contingently coordinates its movements to \u201cjoin the dance\u201d. We compared the results of our anticipation method to move the robot with another method which did not rely on high-level group behavior, and found our method performed better both in terms of more closely synchronizing the robot\u2019s motion to the team, and also exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high-level group behavior than when it does not. This work will help enable others in the robotics community to build more fluent and adaptable robots in the future.", "creator": "LaTeX with hyperref package"}}}