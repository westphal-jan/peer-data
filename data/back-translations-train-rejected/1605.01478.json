{"id": "1605.01478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "abstract": "Sentiment analysis on social media data such as tweets and weibo has become a very important and challenging task. Due to the intrinsic properties of such data, tweets are short, noisy, and of divergent topics, and sentiment classification on these data requires to modeling various contexts such as the retweet/reply history of a tweet, and the social context about authors and relationships. While few prior study has approached the issue of modeling contexts in tweet, this paper proposes to use a hierarchical LSTM to model rich contexts in tweet, particularly long-range context. Experimental results show that contexts can help us to perform sentiment classification remarkably better.", "histories": [["v1", "Thu, 5 May 2016 03:06:47 GMT  (620kb,D)", "http://arxiv.org/abs/1605.01478v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["minlie huang", "yujie cao", "chao dong"], "accepted": false, "id": "1605.01478"}, "pdf": {"name": "1605.01478.pdf", "metadata": {"source": "CRF", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "authors": ["Minlie Huang", "Yujie Cao", "Chao Dong"], "emails": ["aihuang@tsinghua.edu.cn,", "caoyujieboy@163.com,", "neutronest@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "\"We have an important source for people to express their opinions online.\" Sentiment analysis on social media as Twitter and Weibo has been attracting more and more attention lately, but due to the intrinsic properties of social media data, tweets and tweets, it is possible to contextualize different tweets, to model the contexts for a current tweet, for example, to take into account the social context (such as the relationships between followers and followers), discourse relationships (such as the relationships and conditionalities between individual tweets), theme-based and dialogue-based contexts for a current tweet, it is even more difficult to look at context contexts, such as the entire retweet history for a tweet that has been largely ignored so far. Indeed, the long-term context influences the classification of a tweet into a tweet. By analyzing a tweet consisting of more than 14,000 tweets over 1,600 threads in one episode of 1,600 tweets, it is an episode of 1,600."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Twitter Sentiment Analysis", "text": "Following Pang et al. (2002), many papers attempted to design more effective features such as emotional signals, dictionaries, n-grams, etc. (Hu et al., 2013a; Basile and Novielli, 2015; Taboada et al., 2011; Feldman et al. (2013) built up their system in SemEval2013 with a series of features such as POS tags, hashtags, capital letters, punctuations, etc. Recently, many methods have used contextual information. Based on the characteristics of the social network, there are social relationships that can be used to predict the polarities of tweets (Tan et al., 2011; Deng et al., 2013; Hu et al., 2013b)."}, {"heading": "2.2 Deep Learning Approaches for Sentiment Analysis", "text": "One class of deep-learning approaches is the revolutionary neural network. Many papers have developed CNN architectures to model semantic and sentimental information of sentences (Kalchbrenner et al., 2014; Kim, 2014). In Semeval-2015, Severyn et al. (2015) achieved excellence in predicting polarities at the message and phrase level through a deep CNN. Another class of deep-learning approaches to sentiment analysis is a recursive neural network introduced by Socher et al. (2011; 2013) that composes a sentence recursively from its infantile phrases. Dong et al. (2014) proposed an adaptive RNN that combines a number of compositional functions whose weights are adaptively learned. A deep RNN that stacks several recursive layers fared well in the fine-grained sentence analysis (Irsoy and carquence analysis)."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Long Short-Term Memory (LSTM)", "text": "Some sequence models, such as the recursive neural network, are very suitable for sentiment analysis because they can model long-term dependence and thus use the contextual information. However, the recursive neural network has a crucial problem of disappearing and exploding gradients (Bengio et al., 1994). Long-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) can address this problem in order to model long-term dependencies. Like other recursive neural networks, LSTM also has a recursive layer consisting of memory blocks. In each memory block there is a memory cell unit that can store memory state information and several gates that can control the change in memory state. Formally, we can haveft = \u03c3 (fzzt \u2212 1 + Wfxxt + bf) (1) it = cell latte (Wizzt \u2212 1 + Wixxt + 2) (cxt + txt-1) is the type of xxt."}, {"heading": "3.2 Hierarchical LSTM (HLSTM) for Twitter Sentiment Analysis", "text": "Since many tweets are short and informal in grammar, there is very limited information available in a single tweet. However, the tweets that have the relation of response or retweet to the tweet in question can be used as a long-term context, which inspires us to design another level of LSTM above the word level LSTM. Figure 2 shows that the word level LSTM is a single word. The hidden state of the last word is assumed to represent the tweet. Each tweet in a thread passes through the word level LSTM, and the tweet in the thread generates a tweet representation of zt, which is an input of the tweet level LSTM. The inputs for the tweet level LSTM are the representations of tweets in a propagation thread. The first tweet is the original text, which represents the basis of the entire thread, while the other retweets or responses of the tweet are the function of the origin (and the last one of these is the STm)."}, {"heading": "3.3 HLSTM with Additional Contexts", "text": "In addition to the long-term context, tweets have many additional contexts, such as social context, conversation-based context and topic-based context. This contextual information is very easy to obtain and can be used appropriately to expand our model. This additional contextual information is extracted from the parent tweet or root tweet of a targeted tweet in the thread, which is detailed in the next subsection. For any kind of context, we take it as a binary value property and encrypt it into a 0 or 1. These contextual properties can be inserted into the tweet level LSTM, as follows: ft = \u03c3 (Wfhht \u2212 1 + Wfzzt + Wfddt + bf) (7) it = \u03c3 (Wihht \u2212 1 + Wizzt + Widdt + bi) (8) C-t = \u03c6 (Wchht \u2212 1 + Wcddt + bc)."}, {"heading": "3.4 Context Features", "text": "We consider three types of contexts, social context, conversation-based context, and topic-based context, and these contexts are constructed as binary characteristics. We use Tweet j to denote the current tweet, and Tweet i to denote a particular tweet or its root tweet. The first context is a social context. SameAuthor (i, j) The same person usually takes a consistent stance on a specific topic, especially in a tweet thread. If two tweets are posted in the thread by the same person, the two tweets tend to have the same emotional polarity. This context can be formulated as follows: SameAuthor (i, j) = {1, ai = aj0, otherwise (13) ai and aj are the authors of Tweet i and Tweet j, respectively. We should note that Tweet i is the parent tweet or root tweet, so that we get two attributes from the current Tweet context for the authentication."}, {"heading": "3.5 Training", "text": "Here we use P g (xi) to denote the standard golden sentiment distribution of Tweet i, and P (xi) to denote the sentiment distribution that our model predicts. Loss function is shown below. Loss = \u2212 1 N-i = 1 C-j = 1 P gj (xi) log (Pj (xi))) (17) P (xi) = Softmax (hi) (18) We use N to denote the total number of training examples and C the number of sentiment classes. Here, hi is the hidden state of the tweet level LSTM of the tweet i. We use AdaDelta (Liners, 2012) to optimize the parameters."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We have collected tweets from a Chinese microblog site, Weibo.com. On Weibo.com, the retweets and replies always retain their previous tweets during the dissemination process, and therefore wide-ranging contexts are visible for a current tweet. We have collected about 15k tweets, consisting of over 1.6k tweet threads on 51 topics. We should note that a tweet thread forms a tree structure starting with an original text, and the other nodes in the tree are retweets or replies. The average number of tweets in all threads is 8.93, and the average depth is 3.75. The mood polarity of each tweet is divided by two independent commentators into three classes, positive, neutral and negative. The consistency of the annotations in the data set is 63.4%, and these tweets with contradictory labels are then verified by a third judge. Finally, there are 36%, 39.6% and 24.6% of the positive, neutral tweets we have the same negative and neutral analysis of the data set, with 44.7% having the same contextuality of the data."}, {"heading": "4.2 Baseline Methods", "text": "We compare our approach to the following baselines. \u2022 SVMmulticlass: a classic method for classifying multiclass that takes no context information into account. \u2022 SVMHMM (Vanzo et al., 2014): an SVM variant that combines HMM. This method follows the Markov first-order hypothesis. In SVMHMM, we use the same characteristics as in SVMmulticlass, and we also use several other characteristics between the two adjacent tweets in the Markov chain, such as whether they have the same author, whether they represent a conversation, whether they are in the same thread, whether they contain overlapping emoticons and hashtags, and the similarity of their bag-of-words vectors."}, {"heading": "4.3 Experiment Settings", "text": "Implementation We implement our model of Theano. AdaDelta (Lines, 2012) is used to optimize the parameters. We add a fail layer before the softmax layer to prevent overadjustment. The fail rate is 0.5. We use mini-batch to accelerate convergence. And the batch size is 5, which means that we update our parameters after each 5 tweet thread. Parameter Settings For CNN, we set the size of the folding filters to 2, 3 and 4, while the number of feature maps is set to 100. The hidden states of the LSTM baseline, the LSTM layer of LSTMRNN, the first LSTM layer of HLSTM all have a dimension of 128, while the hidden states of the second LSTM layer of HLSTM have 64 dimensions. Evaluation methods We divide our sets of random results according to the test principle of 1. To validate the validation principles used in the 121 partition:"}, {"heading": "4.4 Results and Analysis", "text": "The experimental results are shown in Table 2. We can find that conventional SVM methods perform relatively poorly because SVM multiclass does not take into account contextual information, and this justifies the effectiveness of using the contextual information. Compared to LSTM, our hierarchical LSTM (HLSTM) model performs much better. This result confirms our assumption that the use of the contextual information is helpful for classifying mood in such short and loud texts. In the LSTM RNN model, we replace the second LSTM layer in HLSTM with a recursive neural network layer. The result shows that the LSTM RNN model has a slightly worse effect than our HLSTM model. The reason for this is that RNN suffers from the additional LSTM layer in HLSTM layers that we do not use."}, {"heading": "4.5 Case Study", "text": "An important case is that the polarities of some sentiment words are changed in a particular context, as shown in Table 3. In fact, this example is a conversation between two authors. We can find that in this tweet thread, the HLSTM model correctly predicts all tweets, while the LSTM model is correct only on the root tweet. Looking at author B's second tweet, we can see that it contains some positive sentiment words, such as \"welfare\" and \"pleasure.\" Then, LSTM predicts these words as positive, without taking the broad context into account. Nevertheless, we can clearly see that the second tweet is negative because it is an obvious sarcasm of Beijing haze. In the third tweet, there is an ambiguous sentiment word in Chinese: \"(severe, powdery).\" Without any context, the LSTM may not be able to determine the correct polarity. Another common case, however, is that some of the tweets do not match or exactly match the context."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a hierarchical LSTM model with two layers of LSTM networks to model long-term dependencies. In addition, we use extensive additional contexts such as social context and text-based context to help our model encode more contextual information. Experimental results have shown that our method is superior to baselines. Case studies also show that our proposed model can capture the polarity drift with contexts."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Sentiment analysis on social media data<lb>such as tweets and weibo has become<lb>a very important and challenging task.<lb>Due to the intrinsic properties of such data, tweets are short, noisy, and of diver-<lb>gent topics, and sentiment classification<lb>on these data requires to modeling various<lb>contexts such as the retweet/reply history<lb>of a tweet, and the social context about authors and relationships. While few prior<lb>study has approached the issue of model-<lb>ing contexts in tweet, this paper proposes<lb>to use a hierarchical LSTM to model rich<lb>contexts in tweet, particularly long-range context. Experimental results show that<lb>contexts can help us to perform sentiment<lb>classification remarkably better.", "creator": "TeX"}}}