{"id": "1605.01652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues", "abstract": "We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP.", "histories": [["v1", "Thu, 5 May 2016 17:00:44 GMT  (502kb,D)", "http://arxiv.org/abs/1605.01652v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["phong le", "marc dymetman", "jean-michel renders"], "accepted": false, "id": "1605.01652"}, "pdf": {"name": "1605.01652.pdf", "metadata": {"source": "CRF", "title": "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues", "authors": ["Phong Le", "Marc Dymetman", "Jean-Michel Renders"], "emails": ["p.le@uva.nl", "firstname.lastname@xrce.xerox.com"], "sections": [{"heading": "1 Introduction", "text": "The traditional architecture of dialog systems (Jokinen and McTear, 2009) involves a combination of several components that require a lot of expertise in the various technologies, considerable development and implementation efforts to integrate each component into a new domain, and is only partially feasible (Hochreiter and Schmidhuber, 1997), with the agent's response to the dialog story reaching the point at which it is to be produced: this network can be considered a form of conditional neutrality (LSTM)."}, {"heading": "2 LSTM-based Mixture of Experts", "text": "The method is shown in Figure 1. Let wt1 = w1... wt be a story about words. Let's assume that we have K models, each of which can compute a distribution over its own vocabulary Vk: pk (w, Vk | wt1), for k (k, K). We use an LSTM to encode the story word by word into a vector ht, which will step the hidden state of the LSTM in due course. We then use a Softmax layer to calculate the probabilities p (k, wt1) = eu (k, ht).K \u2032 K \u2032 s (k, ht), where [u (1, ht), u (K, ht)] T = Wht + b, W, RK \u00d7 dim (ht), b (ht).RK \u00b7 dim (h, ht).The final probability of the next word is then: p (w, wt1)."}, {"heading": "3 Data", "text": "Our corpus consists of 165k dialogs from a \"technology company\" in the domain of mobile phone support. We divide them into train, development and test sets of 145k, 10k and 10k. We then tokenise, lowercase each dialog and remove unused information such as head, tail, chat time (Figure 2). For each answer statement found in a dialog, we create a context pair whose context consists of all sentences appearing before the answer. This process gives us 973k / 74k / 75k pairs for training / development / testing.Knowledge Base The KB we use in this work consists of 1,745k device attribute value triples, e.g. (Apple iPhone 5; camera Megapixels; 8.0). There are 4729 devices and 608 attributes. Since we only look at numerical values, only triples that have numerical attributes are selected, resulting in a set of 65k attributes (camera 8.0) ixels."}, {"heading": "4 KB-aware Chat Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Neural Chat Model", "text": "Ouur corpus is similar to the first experiment described in Vinyals and Le (2015), and we are using a similar neural chat model here. Without going into detail for lack of space, this model uses an LSTM to encode the sequence of words observed up to a certain point in a dialogue into a vector, and then this vector is used by another LSTM to also generate the next utterance word by word. The approach is reminiscent of seq2seq machine translation models such as (Sutskever et al., 2014), where the role of the \"source sentence\" is played by the dialogue prefix and that of the \"target sentence\" by the response utterance."}, {"heading": "4.2 Neural Question Answering Model", "text": "By default, a question to query a KB must be formal (e.g., SQL), but because a human QA system should include natural questions as input, we build a neural model to translate natural questions into formal queries. This model employs an LSTM to encode a natural question into a vector, and then uses two softmax layers to predict the device name and attribute. This model is appropriate here as we focus on the QA situation where the client asks for device specifications. In more complex cases, advanced QA models should be considered (e.g., Bordes et al.), Yih et al. (2015).Given the question wl1, the two softmax layers give us a distribution over devices pd (\u2022 wl1) and a distribution over attributes pa (\u2022 wl1) and a distribution over attributes pa (\u2022 wl1)."}, {"heading": "4.3 Integration", "text": "We will now show how to integrate the chat model with the QA model, using the LSTM-based mix-of-experts method. Intuition is the following: the chat model is responsible for generating smooth responses, into which the QA model \"inserts\" the values retrieved from the chat. Ideally, we should use an independent LSTM for the purpose of calculating the weight distribution, as in Section 2. However, due to the lack of training data, our integration model uses the hidden state of the chat model to calculate these weights. As this hidden state captures the uncertainty of generating the next word, it is also able to detect whether the next word should be generated by the chat model or not. It is easy to see that the chat model is the backbone model, because most of the tokens should be generated by it. The QA model, on the other hand, is crucial, because we want the future system to generate the chues alone, as we often do not want it to be valid."}, {"heading": "5 Experiments", "text": "We implement our models in C + + with the CUDA toolkit. As the evaluation of a conversation system is still difficult, we use only word confusion according to Vinyals and Le (2015). In our experiments, each LSTM has 1024 hidden units and 1024 memory cells. The vocabulary of the chat model includes 19.3k words, that of the QA model 12.7k words. First, we train the chat model on all chat data with the learning rate 0.01 and continue it on the device-specific training data with a lower learning rate, 0.001. Based on this lower learning rate, we expect the model not to forget what it has learned on the entire chat corpus. Next, we train the QA model on the data generated in Section 4.2 with the learning rate 0.01. Finally, we train the integration model on the device-specification training model with a lower learning rate, 0.001. Based on this lower learning rate, we expect the model to forget what it has learned on the whole learning corpus, too."}, {"heading": "6 Conclusions", "text": "While the experimental results are limited to measurements of helplessness, they show that the integration model is capable of handling chats within which the user can ask for device specifications; a more thorough and convincing evaluation would require human assessments of the quality of the answers produced. We believe that the proposed integration method has potential for a wide range of applications, allowing a number of different language models to be bundled, with each expert trained in a specific area or problem class (possibly independently based on the most appropriate data), and generating the next word based on a competition between these4Perplexity is a weak proxy for what a human assessment of usefulness would offer. In terms of helplessness, the gain over value tokens overall does not help, because value tokens are rare, only about 6.7% of the models under the supervision of an STM mechanism."}], "references": [{"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni"], "venue": "ACL", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Dynamic nonlocal language modeling via hierarchical topic-based adaptation", "author": ["Florian", "Yarowsky1999] Radu Florian", "David Yarowsky"], "venue": "In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Compu-", "citeRegEx": "Florian et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Florian et al\\.", "year": 1999}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Task decomposition through competition in a modular connectionist architecture: The what and where vision", "author": ["Michael I Jordan", "Andrew G Barto"], "venue": null, "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Spoken Dialogue Systems. Synthesis Lectures on Human Language Technologies", "author": ["Jokinen", "McTear2009] Kristiina Jokinen", "Michael F. McTear"], "venue": null, "citeRegEx": "Jokinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jokinen et al\\.", "year": 2009}, {"title": "Artificial Intelligence: A Modern Approach. Pearson Education, 2 edition", "author": ["Russell", "Norvig2003] Stuart J. Russell", "Peter Norvig"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Recently, Vinyals and Le (2015), Serban et al. (2015), Shang et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 7, "context": "Recently, Vinyals and Le (2015), Serban et al. (2015), Shang et al. (2015) proposed to replace this complex architecture by a single network (such as a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)) that predicts the agent\u2019s response from the dialogue history up to the point where it should be produced: this network can be seen as a form of conditional neural language model (LM), where the dialogue history provides the context for the production of the next agent\u2019s utterance.", "startOffset": 33, "endOffset": 75}, {"referenceID": 4, "context": "Similar to the mixture-of-experts technique of Jacobs et al. (1991), we predict a label by using a \u201cgating\u201d neural network to mix the predictions of different experts based on the current situation, and similar to the approach of Florian and Yarowsky (1999), we dynamically combine distributions on words to produce an integrated LM.", "startOffset": 47, "endOffset": 68}, {"referenceID": 4, "context": "Similar to the mixture-of-experts technique of Jacobs et al. (1991), we predict a label by using a \u201cgating\u201d neural network to mix the predictions of different experts based on the current situation, and similar to the approach of Florian and Yarowsky (1999), we dynamically combine distributions on words to produce an integrated LM.", "startOffset": 47, "endOffset": 258}, {"referenceID": 9, "context": "The approach is reminiscent of seq2seq models for machine translation such as (Sutskever et al., 2014), where the role of \u201csource sentence\u201d is played by the dialogue prefix, and that of \u201ctarget sentence\u201d", "startOffset": 78, "endOffset": 102}, {"referenceID": 0, "context": ", Bordes et al. (2014), Yih et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": ", Bordes et al. (2014), Yih et al. (2015)).", "startOffset": 2, "endOffset": 42}, {"referenceID": 1, "context": ", Fader et al. (2013)) hardly meet our need here.", "startOffset": 2, "endOffset": 22}], "year": 2016, "abstractText": "We introduce an LSTM-based method for dynamically integrating several wordprediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP. 1", "creator": "LaTeX with hyperref package"}}}