{"id": "1605.01713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences", "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.", "histories": [["v1", "Thu, 5 May 2016 19:52:32 GMT  (1441kb,D)", "http://arxiv.org/abs/1605.01713v1", "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning"], ["v2", "Sun, 8 May 2016 21:34:42 GMT  (1441kb,D)", "http://arxiv.org/abs/1605.01713v2", "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning"], ["v3", "Tue, 11 Apr 2017 15:58:48 GMT  (1624kb,D)", "http://arxiv.org/abs/1605.01713v3", "6 pages, 3 figures, this is an older version; seethis https URLfor the newer version"]], "COMMENTS": "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["avanti shrikumar", "peyton greenside", "anna shcherbina", "anshul kundaje"], "accepted": false, "id": "1605.01713"}, "pdf": {"name": "1605.01713.pdf", "metadata": {"source": "META", "title": "Not Just A Black Box:  Interpretable Deep Learning by Propagating Activation Differences", "authors": ["Avanti Shrikumar", "Anna Y. Shcherbina", "Anshul Kundaje"], "emails": ["(avanti@stanford.edu),", "(pgreens@stanford.edu)", "(annashch@stanford.edu),", "(akundaje@stanford.edu)"], "sections": [{"heading": "1. Introduction", "text": "As neural networks become increasingly popular, their reputation as \"black boxes\" is an obstacle to adoption when interpretation options are at the forefront. Understanding the characteristics that lead to a particular output builds trust among users and can lead to novel scientific discoveries. Simonyan et al. (2013) suggested using gradients to generate saliency maps and showed that this is a generalization of the deconvolutionary networks by Zeiler et al. (2013). Guided back propagation (Springenberg et al. 2014) is another variant that only takes into account gradients that have a positive error signal. As shown in Figure 2, saliency maps can be substantially improved simply by multiplying the gradient by the input signal, which corresponds to a first-order Taylor approach of how the output would change if the input were set to zero; the layered relevance of networks contained in Samek et al (2015) are reduced to these omatorial approaches, with the omatorial approach being incorporated in these approaches."}, {"heading": "2. DeepLIFT Method", "text": "We refer to the contribution of x to y as Cxy. Let the activation of a neuron n be referred to as An. Further, let the reference activation of the neuron n be referred to as A0n and let the An \u2212 A0n be referred to as \u03b4n. We define our contributions Cxy to satisfy the following characteristics.ar Xiv: 160 5.01 713v 1 [cs.L G] 5M ay"}, {"heading": "2.1. Summation to \u03b4", "text": "For each group of neurons S, whose activation is minimal enough to calculate the activation of y (that is, if we know the activation of S, we can calculate the activation of y, and there is no set of S \u2032 \u0442 S such that S \u00b2 is sufficient to calculate the activation of y - in layman's language, S is a complete set of non-redundant inputs to y), the following property applies: \u0445 s-S Csy = \u03b4y (1) That is, the sum of all contributions of neurons in S to y corresponds to the difference of y."}, {"heading": "2.2. Linear composition", "text": "Let Ox represent the initial neurons of x. The following property applies: Cxy = \u2211 o \u0445 Ox Cxo \u03b4o Coy (2) In layman's terms, each neuron \"inherits\" a contribution through its outputs in relation to how much this neuron contributes to the difference from the reference of the output."}, {"heading": "2.3. Backpropagation Rules", "text": "We show that the contributions defined above can be calculated on the basis of the following rules (which can be implemented to run on a GPU), the calculation being reminiscent of the chain rule used for gradient repropagation, since Equation 2 makes it possible to start with contribution values of later layers and use these to find the contribution values of previous layers. To avoid problems of numerical stability if the number of contribution values for a particular neuron is small, rather than calculating the contribution values explicitly, we instead calculate multipliers mxy, which, multiplied by the difference-out-reference, provide the contribution: mxy\u03b4x = ICT ICT ICT ICT ICT = ICT ICT ICT ICT ICT = ICT ICT ICT ICT = ICT ICT ICT ICT = ICT ICT ICT ICT ICT = ICT ICT ICT ICT = ICT ICT ICT = ICT ICT ICT = ICT ICT ICT = ICT ICT ICT = ICT ICT ICT = ICT ICT ICT = ICT ICT = ICT ICT ICT = ICT ICT = (3) Do not represent the target neuron to which we intend to calculate contributions, and let Ox represent the amount of expenses of x. We show that: mxt = ICT = ICT = ICT ICT ICT ICT ICT = ICT ICT ICT ICT ICT ICT = ICT ICT ICT ICT ICT = ICT ICT ICT ICT = ICT ICT ICT = ICT ICT ICT ICT ICT = ICT ICT = ICT ICT ICT = ICT = ICT ICT ICT = ICT = ICT ICT ICT = ICT = ICT = ICT ICT = ICT ICT = ICT = ICT = ICT = ICT ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT = ICT ="}, {"heading": "2.3.1. AFFINE FUNCTIONS", "text": "LetAy = \u2211 x-Iy wxyAx + b (6) Then mxy = wxyProof. We show that \u03b4y = \u2211 x-Iy mxy\u03b4x.Using the fact that An = A0n + \u03b4n, we have: (A0y + \u03b4y) = \u2211 x-Iy wxy (A 0 x + \u03b4x) + b = \u2211 x-Iy wxyA 0 x + b + \u2211 x-Iy wxy\u03b4x (7) We also note that the reference activation A0y can be found as follows: A0y = \u2211 x-Iy wxyA 0 x + b (8) Thus, the cancellation of A0y yields: \u03b2y = \u0441y wxy\u0441x = \u0441\u0442\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438A 0 0 0 x) (8)"}, {"heading": "2.3.2. MAX OPERATION", "text": "We consider the case of the max operation as a maxpool: Ay = max x-Iy-Ax (10) Then we have: mxy = 1 {Ax = Ay} \u03b4y \u03b4x (11) Where 1 {} is the indicator function. If a symbolic calculation package is used, then the gradient of y can be used with respect to x instead of 1 {Ax = Ay}. Proof."}, {"heading": "2.3.3. MAXOUT UNITS", "text": "A maxout function has the formAy = nmax i = 1 (\u2211 x wixyAx) + bi (13) i.e. it is the max over n affine functions of the input vector ~ x. For a given activation vector A ~ x of the inputs, we divide A ~ x \u2212 A0 ~ x into segments so that over each segment s a unique affinity function dominates the maxout and the coefficient of a single input x is over that segment w (s) xy. Let l (s) denote the proportion of A ~ x \u2212 A0 ~ x in segments s. We have: mxy = x s l (s) w (s) xy (14) Intuitively, we simply divide the piecewise linear maxout function in regions where it is linear, and make a weighted sum of the x coefficients in each region according to the proportion of A ~ x \u2212 A ~ x in that region."}, {"heading": "2.3.4. OTHER ACTIVATIONS", "text": "The following choice for mxy, which is the same for all inputs to y, is sufficient summation by triangle: mxy = \u03b4y \u2211 x \u00b2 and vice versa. (15) This rule can be used for nonlinearities such as ReLUs, PReLUs, sigmoid and tanh (where y has only one input). Situations where the denominator is close to zero can be handled by applying the L'hopital rule, because by definition: \u03b4y \u2192 0 as \u2211 x \u0445 Iy \u03b4x \u2192 0 (16)."}, {"heading": "2.3.5. ELEMENT-WISE PRODUCTS", "text": "Consider the function: Ay = A 0 y + \u03b4y = (A 0 x1 + \u03b4x1) (A 0 x2 + \u03b4x2) (17) We have: \u03b4y = (A 0 x1 + \u03b4x1) (A 0 x2 + \u03b4x2) \u2212 (A 0 x1A 0 x2) = A0x1\u03b4x2 + A 0 x2\u03b4x1 + \u03b4x1\u03b4x2 = \u03b4x1 (A0x2 + \u03b4x2 2) + \u03b4x2 (A0x1 + \u03b4x1 2) (18) Therefore, useful decisions for the multipliers are mx1y = A 0 x2 + 0,5\u03b4x2 and mx2y = A 0 x1 + 0,5\u03b4x1"}, {"heading": "2.4. A note on final activation layers", "text": "Activation functions such as a Softmax or a Sigmoid have a maximum \u03b4 of 1.0. Due to the summation to the \u03b4 property, the contribution values for individual characteristics are lower if several redundant characteristics are present. Let's take as an example At = \u03c3 (Ay) (where Sigma is the Sigmoid transformation) and Ay = Ax1 + Ax2. Let's leave the default input activations A0x1 = A 0 x2 = 0. If x1 = 100 and x2 = 0, we have Cx1t = 0.5. However, if both are x1 = 100 and x2 = 100, we have Cx1t = Cx2t = 0.25. To avoid this damping of the contribution for redundant inputs, we can use the contributions on y instead of t; in both cases Cx1y = 100."}, {"heading": "2.5. A note on Softmax activation", "text": "Let t1, t2... tn represent the output of a softmax transformation to the node y1, y2... yn, so that: Ati = eAyi = 1 e Ay \u2032 i (19) Here are Ay1... Ayn affine functions of their inputs. Let x represent a neuron that is an input to Ay1... Ayn, and let wxyi represent the coefficient of Ax in Ayi. Since Ay1... Ayn is followed by a softmax transformation, if wxyi is equal to all yi (that is, x contributes equally to all yi), then x effectively has no contribution to Ati. This can be observed by replacing Ayi = wxyiAx + ryi in the expression for Ati and replacing all ewxyiAx (here ryi is the sum of all remaining terms in the affine expression for Ayi) when all wxyy classes are equal to Yi, as y-yy class."}, {"heading": "2.6. Weight normalization for constrained inputs", "text": "Let y be a neuron with a subset of Sy inputs limited in such a way that \u2211 x-Sy-Ax = c (for example, a uniform-coded input meets the constraint \u2211 x-Sy-Ax = 1, and a revolutionary neuron operating on uniform-coded rows has a constraint per column it sees). Let the weights be from x to y wxy and let it be characterized by the inclination of y. It is advisable to use normalized weights w-xy = wxy \u2212 \u00b5 and bias b-y = by + c\u00b5, where \u00b5 is the mean value over all wxy. We note that this maintains the output of the neural network, because for each constant \u00b5: Ay = (\u2211 Ax (w-xy \u2212 \u00b5) + (um + c\u00b5) and bias b-y = um + c\u00b5 (um + c\u00b5) = (um + c\u00b5)."}, {"heading": "3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Tiny ImageNet", "text": "A model with the architecture VGG16 (Long et al., 2015) was trained using the Keras framework (Chollet, 2015) on a scaled-down version of the Imagenet dataset, called \"Tiny Imagenet.\" The images were 64 x 64 cm in size and belonged to one of 200 output classes. Results in Figure 2; the reference input was an input of all zeros after pre-processing."}, {"heading": "3.2. Genomics", "text": "The positive class requires that the DNA patterns \"GATA\" and \"CAGATG\" appear together in the length 200 sequence; the negative class has only one of the two patterns that occur once or twice; outside the core pattern (taken from a generative model), the four bases A, C, G, and T. A CNN was trained using the Keras framework (Chollet, 2015) on uniformly coded sequences with 20 revolutionary filters of length 15 and step 1 and a maximum pool position of width and step 50, followed by two completely interconnected layers of size 200. PReLU nonlinearity was used for the hidden layers; this model works well with auROC of 0.907. The incorrectly classified examples occur especially when one of the patterns in the randomly sampled layer of size 200.Then we run DeepLIFT through to assign a meaning to each point correctly predicted in the sequence after the results of the first LIv."}, {"heading": "3.3. Discussion", "text": "As shown in Figure 1, this can give misleading results if the local gradient is zero. Instead, DeepLIFT takes into account the deviation from the reference activity of a neuron. Thus, it is able to handle RNN memory units blocked by activations with vanishing gradients (e.g. sigmoid, tanh). Layered relevance propagation (LRP), described in Samek et al. and first proposed by Bach et al. (2015), it is obviously not dependent on gradients; however, it can be shown that if the reference terms for bias are included in the relevance propagation and all activations are piecemeal, LRP is reduced to gradients (a first-order alignment by Taylor) if the problems for entering LIFT values are zero (if all results are zero for all references) and all references are equal (if all references are zero)."}, {"heading": "3.4. Equivalence of grad*input to Layer-wise Relevance Propagation", "text": "We show, when all activations are piecemeal linear and bi-specific terms are included in the calculation, the layer-by-layer relevance propagation (LRP) of Bach et al., reduced to gradient * input. We refer to Samek et al. (2015) for the succinct description of LRP: Unpooling: \"The back signal is redirected proportionally to the place for which the activation was recorded in forward.\" This is trivially the same as gradient * input, because the gradient * input will be zero for all places that do not activate the pooling level, and equal to the output for the place where the activation takes place: We consider the first rule described in Samek et al., where zij = a (l) i w (l + 1) ij is the weighted activation of neuron i in the next layer, and l is the index of the layer: R (l)."}, {"heading": "4. Author contributions", "text": "AS & PG designed DeepLIFT, AS implemented DeepLIFT in the software, PG introduced the application to genomics, AYS introduced the application to Tiny Imagenet, AK gave guidance and feedback, AS, PG, AYS & AK prepared the manuscript."}], "references": [{"title": "On Pixel-Wise explanations for Non-Linear classifier decisions by Layer-Wise relevance propagation", "author": ["Bach", "Sebastian", "Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Klauschen", "Frederick", "M\u00fcller", "Klaus-Robert", "Samek", "Wojciech"], "venue": "PLoS One, 10(7):e0130140,", "citeRegEx": "Bach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["S Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., 9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Samek", "Wojciech", "Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Bach", "Sebastian", "M\u00fcller", "Klaus-Robert"], "venue": null, "citeRegEx": "Samek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": null, "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer vision\u2013", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Deconvolutional networks", "author": ["Zeiler", "Matthew D", "Krishnan", "Dilip", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Guided backpropagation (Springenberg et al. 2014) is another variant which only considers gradients that have positive error signal.", "startOffset": 23, "endOffset": 49}, {"referenceID": 4, "context": "Simonyan et al. (2013) proposed using gradients to generate saliency maps and showed that this is a generalization of the deconvolutional nets of Zeiler et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Simonyan et al. (2013) proposed using gradients to generate saliency maps and showed that this is a generalization of the deconvolutional nets of Zeiler et al. (2013). Guided backpropagation (Springenberg et al.", "startOffset": 0, "endOffset": 167}, {"referenceID": 4, "context": "As shown in Figure 2, saliency maps can be substantially improved by simply multiplying the gradient with the input signal, which corresponds to a first-order Taylor approximation of how the output would change if the input were set to zero; the layer-wise relevance propagation rules described in Samek et al. (2015) reduce to this approach, assuming bias terms are included in the denominators.", "startOffset": 298, "endOffset": 318}, {"referenceID": 1, "context": "Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs (Chung et al. 2014; Hochreiter and Schmidhuber 1997), but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant.", "startOffset": 165, "endOffset": 217}, {"referenceID": 2, "context": "Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs (Chung et al. 2014; Hochreiter and Schmidhuber 1997), but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant.", "startOffset": 165, "endOffset": 217}, {"referenceID": 3, "context": "Tiny ImageNet A model with the VGG16 (Long et al., 2015) architecture was trained using the Keras framework (Chollet, 2015) on a scaled-down version of the Imagenet dataset, dubbed \u2018Tiny Imagenet\u2019.", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "and first proposed by Bach et al. (2015), does not obviously rely on gradients; however, it can be shown that if bias terms are included in the relevance propaga-", "startOffset": 22, "endOffset": 41}, {"referenceID": 0, "context": "Equivalence of grad*input to Layer-wise Relevance Propagation We show when all activations are piecewise linear and bias terms are included in the calculation, the Layer-wise Relevance Propagation (LRP) of Bach et al., reduces to gradient*input. We refer to Samek et al. (2015) for the concise description of LRP: Unpooling: \u201cThe backwards signal is redirected proportionally onto the location for which the activation was recorded in the forward pass\u201d: This is trivially the same as gradient*input, because the gradient*input will be zero for all locations which do not activation the pooling layer, and equal to the output for the location that does.", "startOffset": 206, "endOffset": 278}], "year": 2016, "abstractText": "The purported \u201cblack box\u201d nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its \u2018reference activation\u2019 and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.", "creator": "LaTeX with hyperref package"}}}