{"id": "1605.02019", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "abstract": "Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.", "histories": [["v1", "Fri, 6 May 2016 18:13:18 GMT  (175kb,D)", "http://arxiv.org/abs/1605.02019v1", "Submitted to CoNLL 2016"]], "COMMENTS": "Submitted to CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christopher e moody"], "accepted": false, "id": "1605.02019"}, "pdf": {"name": "1605.02019.pdf", "metadata": {"source": "CRF", "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "authors": ["Christopher Moody"], "emails": ["chrisemoody@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to survive themselves by blaming themselves and others. (...) It is not so that they feel able to survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.).). (. (.).). (. (.). (.).). (.). (. (.).). (. (.).). (.). (. (.). (.). (.). (. (.). (.).). (.). (.).). (.). (.). (.).). (.). (.). (. (.).). (.).). (.).). (.). (.).).). (. (.).). (.). (.). (.).). (.). (.)."}, {"heading": "2 Model", "text": "This section describes the model for lda2vec. We are interested in modifying the Skipgram Negative Sampling (SGNS) target in (Mikolov et al., 2013) to use document-wide feature vectors while learning continuous document weights to be loaded onto topic vectors.The network architecture is in Figure 1. The total loss term L in (1) is the sum of Skipgram Negative Sampling Lnegij (SGNS) Lnegij with the addition of a Dirichlet probability term about document weights, Ld, which will be discussed later. Lnegij = log \u03c3 (~ cj \u00b7 ~ wi) + n l = 0 log (cj \u00b7 wl)"}, {"heading": "2.1 Word Representation", "text": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they occur in a moving window that scans the entire corpus. In our experiments, the window contains five symbols before and after the pivot symbol. For each pivot and target pair, the pivot word is used to predict the nearest target word. Each word is represented with a dense, distributed vector, but, unlike Mikolov et al. (2013), the same word vectors are used in both pivot and target representations. SGNS loss as shown in (2) attempts to distinguish context word pairs appearing in the corpus from words randomly sampled from a \"negative\" word pool. This loss is minimized when the observed words are completely separated from the marginal distribution. The distribution from which tokens are drawn is u\u00df, where u normalizes the totality by normalizing the totality by fixing the corpus size."}, {"heading": "2.2 Document Representations", "text": "lda2vec embeds both words and document vectors in the same space and forms both representations at the same time. If you add the rotary and document vectors together, both spaces are effectively connected to each other. Mikolov et al. (2013) convey the intuition that word vectors can be combined to form a semantically meaningful combination of the two words. For example, the vector representation for Germany + airline is similar to the vector for Lufthansa. We would like to use the additive property of word vectors to construct a meaningful sum of word and document vectors. For example, if a document is scanned as lda2vec, the j. word Germany, then it is predicted that adjacent words will be similar, such as France, Spain and Austria. But if the document deals specifically with airlines, then we would like to construct a document vector similar to the word vector for the airline."}, {"heading": "2.2.1 Document Mixtures", "text": "If we only included the structure up to this point, the model ~ ~ ~ ~ would generate a dense vector for each document. However, lda2vec strives to create interpretable representations, and to do so, an additional constraint is imposed so that the document representations are similar to those in traditional LDA models. We aim to create a document vector from a mixture of topic vectors, and to do this, we start by projecting the document vector ~ dj onto a series of latent topic topics,..., ~ tk: ~ dj = pj0 \u00b7 t0 \u00b7 pj1 \u00b7 ~ t1 +... + pjk \u00b7 tk \u00b7 pjn \u00b7 pjn \u00b7 tn \u00b7 tn \u00b7 tn (4) Every topic that deals with a latent topic, ~ t1, is a fraction indicating the affiliation to document j in the topic k. For example, the Twenty Newsgroups model described later has 11313 documents and n = 20 topics, so j = 0... 1k = 13k."}, {"heading": "2.2.2 Sparse Memberships", "text": "Finally, document weights are saved by optimizing document weights for a Dirichlet probability with a low concentration parameter \u03b1: Ld = \u03bb\u044bjk (\u03b1 \u2212 1) log pjk (5) The overall goal in (5) measures the probability of document j in topic k, which is summarized across all available documents; the strength of this term is modulated by the tuning parameter \u03bb. This simple probability encourages document-proportional coupling in each topic sparsely when \u03b1 < 1 and homogeneously when \u03b1 > 1. To advance interpretability, we are interested in finding sparse affiliations and thus determining where n is the number of topics. We also find that setting the overall strength of the dirichlet optimization to \u03bb = 200 works well. Document proportions are initialized to be relatively homogeneous, but over time, the Ld encourages document vectors to become more concentrated (e.g., more sparse over time)."}, {"heading": "2.3 Preprocessing and Training", "text": "The target in (1) is trained in individual minibatches at the same time, while we use the Adam Optimizer (Kingma and Ba, 2014) for two hundred epochs in the entire dataset. Dirichlet probability term Ld is typically calculated across all documents, so when modifying the target to minibatches, we adjust the loss of the term proportional to the minibatch size divided by the size of the entire corpus. Our software is open source, available online, documented and tested1. Finally, the ten most likely words of a given topic are submitted to the online measuring tool Palmetto2 on quality and the coherence measurement Cv is recorded. After evaluating several alternatives, Cv is the recommended coherence metric in Ro \u00bc der et al. (2015). This metric represents the average of normalized Pointwise Mutual Information (NPMI) for each word pair within a size 110 sliding window on an external corpus."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Twenty Newsgroups", "text": "In this section, experiments are conducted to discover the most important topics in the Twenty Newsgroups Dataset, a popular corpus for machine learning on text. Each document in the corpus has been posted to one of twenty possible newsgroups. While the text of each post is available in Scikit Learning, each of the newsgroup partitions is not disclosed to the algorithm, but is nevertheless useful for post-hoc qualitative assessments of the discovered topics. The corpus is pre-processed using the topic available in Scikit Learning. (Pedregosa et al., 2012) and tokens are identified with the SpaCy parser. (Honnibal and Johnson, 2015) Words are processed into group infections into individual tokens. Tokens that occur less than ten times in the corpus are removed as tokens."}, {"heading": "3.2 Hacker News Comments corpus", "text": "In fact, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves, most of them are able to trump themselves."}, {"heading": "3.3 Conclusion", "text": "This paper demonstrates a simple model, lda2vec, which expands SGNS (Mikolov et al., 2013) to create unattended document representations that provide coherent themes. Word, theme and document vectors are jointly trained and embedded in a common representation space that maintains semantic regularities between the learned word vectors while at the same time yielding sparse and interpretable Documentto-Topic proportions in the style of LDA (Lead et al., 2003). Topics formed in the corpus of twenty newsgroups exhibit high mean topic coherences that correlate with human assessments of themes (Ro \u00a4der et al., 2015)."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "The Journal of Machine Learning Research, 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic Topic Models", "author": ["David Blei", "Lawrence Carin", "David Dunson."], "venue": "IEEE Signal Processing Magazine, 27(6):55\u201365.", "citeRegEx": "Blei et al\\.,? 2010", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J Chang", "S Gerrish", "C Wang."], "venue": "Advances in . . . .", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Dynamic Poisson Factorization", "author": ["Laurent Charlin", "Rajesh Ranganath", "James McInerney", "David M Blei."], "venue": "ACM, September.", "citeRegEx": "Charlin et al\\.,? 2015", "shortCiteRegEx": "Charlin et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv.org, July.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An Improved Non-monotonic Transition System for Dependency Parsing", "author": ["Matthew Honnibal", "Mark Johnson."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373\u20131378, Stroudsburg,", "citeRegEx": "Honnibal and Johnson.,? 2015", "shortCiteRegEx": "Honnibal and Johnson.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv.org, December.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-Thought Vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural . . . , pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Stroudsburg, PA, USA. Association for", "citeRegEx": "Levy and Goldberg.,? 2014a", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "CoNLL, pages 171\u2013180.", "citeRegEx": "Levy and Goldberg.,? 2014b", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in Neural Information Processing . . . , pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014c", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Schutze."], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Manning et al\\.,? 2009", "shortCiteRegEx": "Manning et al\\.", "year": 2009}, {"title": "Inferring Networks of Substitutable and Complementary Products", "author": ["Julian McAuley", "Rahul Pandey", "Jure Leskovec."], "venue": "ACM, New York, New York, USA, August.", "citeRegEx": "McAuley et al\\.,? 2015", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["Brucher", "Matthieu Perrot", "\u00c9douard Duchesnay."], "venue": "arXiv.org, January.", "citeRegEx": "Brucher et al\\.,? 2012", "shortCiteRegEx": "Brucher et al\\.", "year": 2012}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Strouds-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Exploring the Space of Topic Coherence Measures", "author": ["Michael R\u00f6der", "Andreas Both", "Alexander Hinneburg."], "venue": "ACM, February.", "citeRegEx": "R\u00f6der et al\\.,? 2015", "shortCiteRegEx": "R\u00f6der et al\\.", "year": 2015}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["S Tokui", "K Oono", "S Hido", "CA San Mateo", "J Clayton."], "venue": "learningsys.org.", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al.", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": ", 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 13, "context": ", 2015), and infer networks of complementary products (McAuley et al., 2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 0, "context": "For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in domains as diverse as computer vision, genetic markers, survey data, and social network data.", "startOffset": 112, "endOffset": 274}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al.", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features.", "startOffset": 177, "endOffset": 310}, {"referenceID": 2, "context": "By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect and more immediately yield high level intuitions about the underlying system (although not without hazards, see Chang et al. (2009)).", "startOffset": 224, "endOffset": 244}, {"referenceID": 18, "context": "(Tokui et al., 2015) framework to quickly develop models while also enabling us to utilize GPUs to dramatically improve computational speed.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Once created, these representations are then useful for information retrieval (Manning et al., 2009) and parsing tasks (Levy and Goldberg, 2014a).", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": ", 2009) and parsing tasks (Levy and Goldberg, 2014a).", "startOffset": 26, "endOffset": 52}, {"referenceID": 10, "context": "For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus.", "startOffset": 13, "endOffset": 35}, {"referenceID": 9, "context": "In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones.", "startOffset": 9, "endOffset": 35}, {"referenceID": 9, "context": "In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones. Closely related to the PMI matrix, Pennington et al. (2014) factorize a large global word count co-occurrence matrix to yield more efficient and slightly more performant computed embeddings than SGNS.", "startOffset": 9, "endOffset": 269}, {"referenceID": 14, "context": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus.", "startOffset": 6, "endOffset": 28}, {"referenceID": 14, "context": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations.", "startOffset": 6, "endOffset": 446}, {"referenceID": 14, "context": "Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013). Note that a distribution of u0.", "startOffset": 132, "endOffset": 154}, {"referenceID": 14, "context": "Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "In order to prevent co-adaptation, we also perform dropout on both the unnormalized document vector ~ dj and the pivot word vector ~ wj (Hinton et al., 2012).", "startOffset": 136, "endOffset": 157}, {"referenceID": 17, "context": "The topic coherence has been demonstrated to correlate with human evaluations of topic models (R\u00f6der et al., 2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "The objective in (1) is trained in individual minibatches at a time while using the Adam optimizer (Kingma and Ba, 2014) for two hundred epochs", "startOffset": 99, "endOffset": 120}, {"referenceID": 10, "context": "Token-toword similarity is evaluated using the 3COSMUL measure (Levy and Goldberg, 2014b).", "startOffset": 63, "endOffset": 89}, {"referenceID": 14, "context": "After evaluating multiple alternatives, Cv is the recommended coherence metric in R\u00f6der et al. (2015). This measure averages the Normalized Pointwise Mutual Information (NPMI) for every pair of words within a sliding window of size 110 on an external corpus and returns mean of the NPMI for the submitted set of words.", "startOffset": 82, "endOffset": 102}, {"referenceID": 5, "context": ", 2012) and tokens are identified using the SpaCy parser (Honnibal and Johnson, 2015).", "startOffset": 57, "endOffset": 85}, {"referenceID": 14, "context": "Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "To take advantage of this rich corpus, we use the SpaCy to tokenize whole noun phrases and entities at once (Honnibal and Johnson, 2015).", "startOffset": 108, "endOffset": 136}, {"referenceID": 0, "context": "These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003).", "startOffset": 148, "endOffset": 167}, {"referenceID": 14, "context": "Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013) but specialized to the Hacker News corpus.", "startOffset": 90, "endOffset": 112}, {"referenceID": 10, "context": "The subtractions and additions of vectors are evaluated literally, but instead take advantage of the 3COSMUL objective (Levy and Goldberg, 2014b).", "startOffset": 119, "endOffset": 145}, {"referenceID": 14, "context": "This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics.", "startOffset": 66, "endOffset": 88}, {"referenceID": 0, "context": "representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003).", "startOffset": 186, "endOffset": 205}, {"referenceID": 17, "context": "groups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (R\u00f6der et al., 2015).", "startOffset": 115, "endOffset": 135}], "year": 2016, "abstractText": "Distributed dense word vectors have been shown to be effective at capturing tokenlevel semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.", "creator": "TeX"}}}