{"id": "1605.02097", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning", "abstract": "The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.", "histories": [["v1", "Fri, 6 May 2016 20:46:34 GMT  (3644kb,D)", "http://arxiv.org/abs/1605.02097v1", null], ["v2", "Tue, 20 Sep 2016 19:12:49 GMT  (3592kb,D)", "http://arxiv.org/abs/1605.02097v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["micha{\\l} kempka", "marek wydmuch", "grzegorz runc", "jakub toczek", "wojciech ja\\'skowski"], "accepted": false, "id": "1605.02097"}, "pdf": {"name": "1605.02097.pdf", "metadata": {"source": "CRF", "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning", "authors": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek"], "emails": ["wjaskowski@cs.put.poznan.pl"], "sections": [{"heading": null, "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "II. RELATED WORK", "text": "One of the earliest works on visual enhancement learning comes from Asada et al. [3], [2] who trained robots in various elementary football skills. Further work in this area includes teaching mobile robots with visual-based Qlearning [9], learning strategies with deep auto-encoders and batch-mode algorithms [18], neuroevolution for a vision-based version of the mountain car problem [5] and compressed neuroevolution with recurring neural networks for vision-based auto-simulators [16]. Recently, Mnih et al. have demonstrated a deep Q-learning method for learning Atari 2600 games using visual inputs [21]. Various first-person shooter (FPS) video games have already been used either as AI research platforms or as application techniques."}, {"heading": "A. Why Doom?", "text": "In fact, it is a purely mental game, in which the world is placed at the centre of attention."}, {"heading": "B. Application Programming Interface (API)", "text": "ViZDoom API is flexible and easy to use. Designed with reinforcement and apprenticeship in mind, it provides full control over the underlying doom process. Specifically, it allows you to retrieve the game's screen buffer and perform actions that correspond to keyboard keys (or their combinations) and mouse movements. Some game state variables, such as player health or ammo, are directly available. ViZDom's API is written in C + +. The API offers a host of configuration options such as control modes and rendering options. In addition to C + + support, bindings for Python and Java have been provided. An example of Python API is shown in Fig. 2."}, {"heading": "C. Features", "text": "In fact, most of them are able to survive themselves, most of them are able to survive themselves, but most of them are not able to save themselves, most of them are able to save themselves, most of them are able to save themselves, most of them are able to save themselves, most of them are able to save themselves, and most of them are able to save themselves, most of them are not able to save themselves, most of them are able to save themselves, and most of them are able to save themselves."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Basic Experiment", "text": "The primary purpose of the experiment was to show that learning from visual input is a problem introduced in ViZDoom 2600. Furthermore, the experiment investigates how the number of skipped frames (see section III-C4) influenced the number of skipped frames (see section III-C4). Scenario: This simple scenario takes place in a rectangular chamber (see fig. 5). An agent is impaled in the middle of the longer wall of the room. A stationary monster is washed at a random position along the opposite wall. A single hit is enough to kill the monster. The episode ends when the monster is eliminated or after 300 frames, whichever comes first. The agent scores 101 points for killing the monster, \u2212 1 in addition to each action. Scores motivate the learning agents as quickly as possible, preferably with a single shooting 3.2."}, {"heading": "B. Medikit Collecting Experiment", "text": "The previous experiment was conducted on a simple scenario that was closer to a 2D arcade game than a real 3D virtual world. Therefore, we decided to test whether similar deep reinforcement learning methods would work in a more involved scenario that requires significant spatial reasoning. Scenario: In this scenario, however, the agent fails at a random location in a labyrinth with an acid surface that slowly but steadily continues the agent's life (see fig. 8). To survive, the agent must collect medication and avoid blue vials with poison. However, items of both types appear at random locations during the episode. The agent is allowed to move (forward / backward) and rotate (left / right). He scores 1 point for each tick, and is punished with \u2212 100 points to become dead. Thus, he is motivated to survive as long as possible."}, {"heading": "V. CONCLUSIONS", "text": "ViZDoom is a doom-based platform for research into visionary reinforcement learning. It is easy to use, highly flexible, multi-platform, lightweight and efficient. Unlike the other popular visual learning environments such as Atari 2600, ViZDoom offers a 3D, semi-realistic, first-person perspective virtual world. ViZDom's API gives the user full control over the environment. Multiple operating modes facilitate experimenting with different learning paradigms such as reinforcement learning, apprentice learning, learning by demonstration and even the \"ordinary,\" supervised learning. The strength and versatility of the environment lie in4https: / / youtu.be / re6hkcTWVUYis customizability via the mechanism of scenarios that can be conveniently programmed with open source tools. We have also shown that visual enhancement learning is possible in the 3D virtual environment of ViZDoom by conducting deep Q-learning on two scenarios that involve shifting the speed of the scenarios to 10."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by the grant number DEC-2013 / 09 / D / ST6 / 03932 of the Polish National Science Centre."}], "references": [{"title": "Purposive behavior acquisition for a real robot by vision-based reinforcement learning", "author": ["Minoru Asada", "Shoichi Noda", "Sukoya Tawaratsumida", "Koh Hosoda"], "venue": "In Recent Advances in Robot Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "A vision-based reinforcement learning for coordination of soccer playing behaviors", "author": ["Minoru Asada", "Eiji Uchibe", "Shoichi Noda", "Sukoya Tawaratsumida", "Koh Hosoda"], "venue": "In Proceedings of AAAI-94 Workshop on AI and A-life and Entertainment,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Using a genetic algorithm to tune first-person shooter bots", "author": ["Nicholas Cole", "Sushil J Louis", "Chris Miles"], "venue": "In Evolutionary Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Intrinsically motivated neuroevolution for vision-based reinforcement learning", "author": ["Giuseppe Cuccu", "Matthew Luciw", "J\u00fcrgen Schmidhuber", "Faustino Gomez"], "venue": "In Development and Learning (ICDL),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Towards using first-person shooter computer games as an artificial intelligence testbed", "author": ["Mark Dawes", "Richard Hall"], "venue": "In Knowledge- Based Intelligent Information and Engineering Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "A hybrid fuzzy ANN system for agent adaptation in a first person shooter", "author": ["Abdennour El Rhalibi", "Madjid Merabti"], "venue": "International Journal of Computer Games Technology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Controlling bots in a First Person Shooter game using genetic algorithms", "author": ["A I Esparcia-Alcazar", "A Martinez-Garcia", "A Mora", "J J Merelo", "P Garcia-Sanchez"], "venue": "In Evolutionary Computation (CEC),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Reinforcement learning for a vision based mobile robot", "author": ["Chris Gaskett", "Luke Fletcher", "Alexander Zelinsky"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "An empirical study of machine learning algorithms applied to modeling player behavior in a first person shooter video game", "author": ["Benjamin Geisler"], "venue": "PhD thesis, University of Wisconsin-Madison,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "DRE-Bot: A hierarchical First Person Shooter bot using multiple Sarsa(\u03bb) reinforcement learners", "author": ["F G Glavin", "M G Madden"], "venue": "In Computer Games (CGAMES),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Adaptive Shooting for Bots in First Person Shooter Games Using Reinforcement Learning", "author": ["F G Glavin", "M G Madden"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "An evaluation of models for predicting opponent positions in first-person shooter video games", "author": ["S Hladky", "V Bulitko"], "venue": "Computational Intelligence and Games", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Believable Bot Navigation via Playback of Human Traces, pages 151\u2013170", "author": ["Igor V. Karpov", "Jacob Schrum", "Risto Miikkulainen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning", "author": ["Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber", "Faustino Gomez"], "venue": "In Proceedings of the 2014 conference on Genetic and evolutionary computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["Sascha Lange", "Martin Riedmiller"], "venue": "In IJCNN,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Reinforcement Learning in First Person Shooter Games", "author": ["M McPartland", "M Gallagher"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "TALIATE: learning winning policies in first-person shooter games", "author": ["Megan Smith", "Stephen Lee-Urban", "H\u00e9ctor Mu\u00f1oz-Avila. RE"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Continuous and Reinforcement Learning Methods for First-Person Shooter Games", "author": ["Tony C Smith", "Jonathan Miles"], "venue": "Journal on Computing (JoC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "The Evolution of Gamebots for 3D First Person Shooter (FPS)", "author": ["Chang Kee Tong", "Ong Jia Hui", "J Teo", "Chin Kim On"], "venue": "In Bio- Inspired Computing: Theories and Applications (BIC-TA),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Computer game engines for developing first-person virtual environments", "author": ["David Trenholme", "Shamus P Smith"], "venue": "Virtual reality,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3):279\u2013292", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 15, "context": "Deep learning has been applied to many supervised machine learning tasks and performed spectacularly well especially in the field of image classification [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "Recently, deep architectures have also been successfully employed in the reinforcement learning domain to train human-level agents to play a set of Atari 2600 games from raw pixel information [21].", "startOffset": 192, "endOffset": 196}, {"referenceID": 19, "context": "In order to demonstrate the usability of the platform, we perform two ViZDoom experiments with deep Q-learning [21].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "FPS games, especially the most popular ones such as Unreal Tournament [11] [12], Counter-Strike [14] or Quake III Arena [7], have already been used in AI research.", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "FPS games, especially the most popular ones such as Unreal Tournament [11] [12], Counter-Strike [14] or Quake III Arena [7], have already been used in AI research.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "FPS games, especially the most popular ones such as Unreal Tournament [11] [12], Counter-Strike [14] or Quake III Arena [7], have already been used in AI research.", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "FPS games, especially the most popular ones such as Unreal Tournament [11] [12], Counter-Strike [14] or Quake III Arena [7], have already been used in AI research.", "startOffset": 120, "endOffset": 123}, {"referenceID": 13, "context": "behave more believable [15].", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "[3], [2], who trained robots various elementary soccer-playing skills.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[3], [2], who trained robots various elementary soccer-playing skills.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "Other works in this area include teaching mobile robots with visual-based Qlearning [9], learning policies with deep auto-encoders and batch-mode algorithms [18], neuroevolution for a vision-based version of the mountain car problem [5], and compressed neuroevolution with recurrent neural networks for vision-based car simulator [16].", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "Other works in this area include teaching mobile robots with visual-based Qlearning [9], learning policies with deep auto-encoders and batch-mode algorithms [18], neuroevolution for a vision-based version of the mountain car problem [5], and compressed neuroevolution with recurrent neural networks for vision-based car simulator [16].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Other works in this area include teaching mobile robots with visual-based Qlearning [9], learning policies with deep auto-encoders and batch-mode algorithms [18], neuroevolution for a vision-based version of the mountain car problem [5], and compressed neuroevolution with recurrent neural networks for vision-based car simulator [16].", "startOffset": 233, "endOffset": 236}, {"referenceID": 14, "context": "Other works in this area include teaching mobile robots with visual-based Qlearning [9], learning policies with deep auto-encoders and batch-mode algorithms [18], neuroevolution for a vision-based version of the mountain car problem [5], and compressed neuroevolution with recurrent neural networks for vision-based car simulator [16].", "startOffset": 330, "endOffset": 334}, {"referenceID": 19, "context": "have shown a deep Q-learning method for learning Atari 2600 games from visual input [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "The first work on AI in FPS games is due to Geisler [10].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Cole used genetic algorithms to tune bots in Counter Strike [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "Dawes [6] identified Unreal Tournament 2004 as a potential AI research test-bed.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "El Rhalib studied weapon selection in Quake III Arena [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 20, "context": "Smith devised a RETALIATE reinforcement learning algorithm for optimizing team tactics in Unreal Tournament [22].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "SARSA(\u03bb), another reinforcement learning method, was the subject of research in FPS games [20], [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "SARSA(\u03bb), another reinforcement learning method, was the subject of research in FPS games [20], [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "Recently, continuous and reinforcement learning techniques were applied to learn the behavior of tanks in the game BZFlag [23].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "Creating a 3D first-person perspective environment from scratch solely for research purposes would be wasteful [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "Unreal Tournament, however popular, is not as recognizable as Doom or Quake but it has been a primary research platform for FPS games [8], [25].", "startOffset": 134, "endOffset": 137}, {"referenceID": 23, "context": "Unreal Tournament, however popular, is not as recognizable as Doom or Quake but it has been a primary research platform for FPS games [8], [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "2) Deep Q-Learning: The learning procedure is similar to the Deep Q-Learning introduced for Atari 2600 [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "The problem is modeled as a Markov Decision Process and Qlearning [27] is used to learn the policy.", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "We also used experience replay but no target network freezing (see [21]).", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Each convolution layer is followed by a max-pooling layer with max pooling of size 2 and rectified linear units for activation [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Next, there is a fully-connected layer with 800 leaky rectified linear units [19] and an output layer with 8 linear units corresponding to the 8 combinations of the 3 available actions (left, right and shot).", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "2) Experimental Setup: The learning procedure was the same as described in Section IV-A2 with the difference that for updating the weights RMSProp [24] this time.", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the firstperson perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.", "creator": "LaTeX with hyperref package"}}}