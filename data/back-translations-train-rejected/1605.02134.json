{"id": "1605.02134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Neural Recovery Machine for Chinese Dropped Pronoun", "abstract": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.", "histories": [["v1", "Sat, 7 May 2016 02:41:54 GMT  (1885kb,D)", "http://arxiv.org/abs/1605.02134v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wei-nan zhang", "ting liu", "qingyu yin", "yu zhang"], "accepted": false, "id": "1605.02134"}, "pdf": {"name": "1605.02134.pdf", "metadata": {"source": "CRF", "title": "Neural Recovery Machine for Chinese Dropped Pronoun", "authors": ["Wei-Nan Zhang", "Ting Liu", "Qingyu Yin", "Yu Zhang"], "emails": ["wnzhang@ir.hit.edu.cn"], "sections": [{"heading": null, "text": "In this paper, we propose a Neural Recovery Machine (NRM) to model and restore DPs in Chinese, avoiding the non-trivial feature engineering process. Experimental results show that the proposed NRM significantly exceeds current approaches in both heterogeneous datasets. Further experimental results of Chinese zero pronoun resolution (ZP) show that the performance of ZP resolution can also be improved by recovering the ZPs from DPs. Keywords: neural network, drop pronoun recovery, Chinese ceropron resolution."}, {"heading": "1. Introduction", "text": "One of the most important challenges in understanding natural language is to effectively model missing elements that are pragmatically incomprehensible in a sense, such as sunken pronouns. A segment of Chinese sentences observed in real text between humans and humans is shown in Table 1. Pronouns in square brackets are dropped in Chinese sentences. People can easily understand the meaning of the dialogue text due to the \"coherent model.\" However, it is not trivial for the computer to understand the correct meaning of the incomplete email address: wnzhang @ ir.hit.edu.cn (Wei-Nan Zhang) Preprint, the arXiv May 10, 2016ar Xiv: 160 5.02 134v 1 [cs.C] 7M ay2 01sentences, without restoring the sunken pronouns, it is one of the most important steps to recover the sunken pronouns."}, {"heading": "2. Preliminary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Data Description", "text": "First, we used an interview dialog from the \"bc\" section of OntoNotes Release 4.01, which contains 2,501 sentences. Second, we collected a question-and-answer dialog2 from Baidu Zhidao3, which contains 11,160 sentences. We use both heterogeneous records for the experiment of Chinese dropped pronoun restoration. Table 2 shows the statistics of the records for annotation.1http: / / catalog.ldc.upenn.edu / LDC2011T03, the files are: phoenix 0000-phoenix 0011, msnbc 0000, cnn 0000-cnn 0004, cctv 0000-cctv 0007, p2.5 cmn 0001-p2.5 cmn 0061 2A question-and-answer dialog is constructed by multiple questions and answers between the questioner and the answer. http: / dazhio.du.com /"}, {"heading": "2.2. Dropped Pronoun Annotation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1. Actual Pronouns", "text": "In this study, we follow the annotation scheme in [2]. The annotation framework proposed by [2] includes 14 types of pronouns. Of these 14 pronouns, 10 are actual pronouns commonly used in the Chinese language and writing. The details of these pronouns, by following the description of [2], are listed below: Table 3: Actual pronouns and their descriptions. Actual pronoun description in the first person singular. (we) First Person singular. (You) Second Person singular. (You) Second Person plural. (You) Third Person singular. (You) Third Person masculine singular. (You) Third Person singular. (You) Third Person singular. (You) Third Person singular. (You) Third Person singular."}, {"heading": "2.2.2. Abstract Pronouns", "text": "The remaining 4 types of pronouns are called abstract pronouns that exist in Chinese but do not match certain Chinese words, but can be expressed in non-proprietary languages such as English. We describe them as: existential: An existential subject usually appears before a small number of \"existence\" verbs, such as \"Hab\" and \"Hab,\" etc. Example (2) shows an existential subject that precedes the verb \"Hab.\" (2) This type of subject can sometimes be translated as \"Hab\" or \"Hab.\" Example (3) shows such an event that is not specified: an unspecified subject occurs when there is no one (or no one) to interpret. This type of subject can sometimes be translated as \"an\" or \"someone.\" Example (3) shows an unspecified subject that precedes the verb \"Hab.\""}, {"heading": "2.3. Annotation Statistics", "text": "In order to comment on the notes on the sentences of OntoNotes 4.0, two Chinese native speakers who are not involved in our experimental design are asked to comment on the DPs. To facilitate the comments and avoid ambiguity, we asked the two annotators to reference the English translations of the Chinese corpus, which are also offered in the OntoNotes Release 4.0 data. This is because English is a non-case language, while the Chinese DPs normally exist in their respective English translations. In order to comment on the Baidu Zhidao sentences, three Chinese native speakers who are not involved in our experimental design are asked to comment on the DPs. The final labels are obtained by voting. The annotation agreement measured by Cohen's Kappa [4] is equal to 0.71, meaning a \"good\" match. The distribution of these types of dropped pronouns in our data set is shown in Table 4."}, {"heading": "3. Our Approach", "text": "In this section, we describe the proposed NRM for recovering dropped pronouns. Figure 1 shows the general framework of the NRM for recovering dropped pronouns using an example. Here, the position recognition process focuses on selecting the position where a dropped pronoun should exist. The process of generating dropped pronouns then generates an explicit pronoun for the dropped position."}, {"heading": "3.1. Dropped Position Identification", "text": "To generate the dropped hypothesis (\u03c6), we assume that there may be a dropped position between each of the adjacent words and the beginning and end of an input sentence, as in Figure 2. Each of the dropped positions is a dropped hypothesis. To represent the dropped hypothesis, we use context embedding, which is constructed by concatenating the word embedding in a specific window. Afterwards, we use a neural network to identify the dropped position in a sentence, using a multi-layered perceptron to realize the dropped position identification of neural networks. Note that the dropped position identification of neural networks can also be realized through the Convolutionary Neural Network (CNN), the Recurring Neural Network (RNN), etc. We leave the exploration of these neural networks to future work."}, {"heading": "3.2. Dropped Pronoun Generation", "text": "Figure 3 shows the frame of the drop pronoun generation. As shown in Figure 3, we represent the dropped position in a sentence by using its context embedding, which is constructed by concatenating the word embedding in a specific window. We then use a neural network to generate the explicitly dropped pronoun for the dropped position. The dropped pronoun also creates a neural network by using multi-layered perceptronics.Dropped Pronoun Generation"}, {"heading": "3.3. Multi-Layer Perceptron Model", "text": "[5] proposed a unified framework for natural language processing (NLP), such as part-of-speech tagging, chunking, named entity recognition, semantic role labeling, language modeling, semantically related words, etc. In this work, we focus on another NLP task, called drop pronoun recovery in the pro-drop language as Chinese, Japanese, etc. Inspired by [5], to specify the dropped pronoun recovery task, we proposed a multi-layered perceptron model to realize the dropped position identification of neural networks (Section 3.1) and the dropped pronoun generation of neural networks (Section 3.2). We use x, Wi, and bi to represent the continuous representation of input, the weight matrix and bias of the i-th layer are realized. Here, the continuous representation of the equation in the equation is achieved by concatenation of the word insertion of section 4.1, as described in Section 4.1."}, {"heading": "4. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data Preprocessing", "text": "To get the input of the NRM, there are two pre-processing steps: the first is Chinese word segmentation, which segments the entered raw sentences into Chinese word sequences. In this work, we use the LTP Cloud 4 service, which is a state-of-the-art Chinese word segmenter, to get the segmentation result. Note that the OntoNotes 4.0 data has the manual word segmentation result, so the LTP cloud is applied only to the Baidu Zhidao data. The second is to convert the raw words into continuous word embedding, using the Word2Vec toolkit5 to get the Chinese word embedding. [6]"}, {"heading": "4.2. Parameter Tuning", "text": "For the experiments, we separate the data in Table 2 into training data, development data and test data at a ratio of 3: 1: 1 in the two datasets OntoNotes 4.0 and Baidu Zhidao. We use the development data to adjust the parameters of the proposed NRM. First, we look at the accuracy variation using three parameters: 1) the dimension of word embedding, 2) the window size of the dropped hypothesis, and 3) the number of layers of the multilayer perceptor for dropped position detection or dropped pronoun generation. Figures 4 and 5 show the accuracy of dropped position detection via the dimension of word embedding, the window size of the dropped hypothesis, and the number of layers of the multi-layer perceptron the OntoNotes 4.0 and Baidu Zhidao datasets. Figure 6 and 7 show the accuracy of the fall data identification of Baidu Zhidao: http: / / cloudde.http: / www.dgle.com"}, {"heading": "4.3. Dropped Pronoun Recovery Results", "text": "In our experiments, we use the accuracy to evaluate the proposed approach. Four comparison systems are selected as baselines: SVM [7] with linear kernels and sparse vector input (SVMSL), SVM with linear kernels andDropout Tuning for Dropped Pronoun GenerationEpoch Tuning for Dropped Position IdentificationEpoch Tuning for Dropped Pronoun Generationse input (SVMDL), SVM with sigmoid kernel and sparse vector input (SVMSS) and SVM with sigmoid kernel and dense vector input (SVMDS). Meanwhile, we also re-establish a state of art (SOTA) approach to reactivate discarded pronoons (Yang et al."}, {"heading": "4.4. Error Analysis on Dropped Pronoun Recovery", "text": "It is a challenge for the DPs that the pronouns for the DPs are clear, but the pronouns they represent are not. (This often occurs in the nouns that represent organizations.) An organization can be a speaker designated by a singular or a plural. (This can be considered ambiguity, which is complicated to model. (6) The following two sentences are examples to show the cases mentioned above. (6) The CN pronouns-pronouns-pronos-pronos-pronos-pronos-pronos-pronos-pronos-pronos-pronos-pronos-pronos-pronos-prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- pronos-prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono- prono-"}, {"heading": "4.5. Zero Pronoun Resolution Results", "text": "Since the zero pronouns have no indicative information, such as the open pronouns, we are therefore motivated to restore the lowered pronouns for the zero pronoun resolution task. To integrate, we first run our NRM on the OntoNotes 5.0 data, which is a standard and authoritative dataset for the zero pronoun resolution task. [8] For each of the anaphorical zero pronouns predicted by the zero pronoun-specific neural network (ZPSNN), the NRM generates a pronoun whose representation is an embedding vector. ZPSNN then expands its input with the restored pronoun and generates the zero pronoun resolution. To verify the effectiveness of the NRM for the zero pronoun resolution, we compare the results of the ZPSNN and the ZPSNN + NRM with the results of the state-of-the-the-TA (use)."}, {"heading": "4.6. Case Study on Zero Pronoun Resolution", "text": "We compare the various cases produced by the ZPSNN and the ZPSNN + NRM. We note that the NRM can improve the performance of the ZPSNN in the following cases. \u2022 The expected content for restoring a null pronoun (precursor) is an open pronoun. (9) The \"AZP\" is an anaphorical null pronoun that should be called \"we.\" The \"NRM\" generates a dropped pronoun of \"we\" for the \"AZP.\" Therefore, the \"AZP\" is solved correctly. While the \"AZP\" is an anaphorical null pronoun that should be called \"we.\" The \"NRM\" generates a detached pronoun of \"we\" for the \"AZP.\" Therefore, the \"AZP\" is solved correctly without integrating the \"NRM,\" AZP \"becomes the\" precursor of. \""}, {"heading": "5. Related Work", "text": "The related studies with declining pronoun restoration focused mainly on restoring the Empty Category (EC) and zero anaphora resolution. Restoring the EC can be seen as an earlier step towards DP recovery, while zero anaphora resolution can be used either as a feature or as an application for the DP recovery task."}, {"heading": "5.1. Empty Category Recovery", "text": "Motivated by government-binding theory, empty categories (ECs) are artificially commented to explain specific linguistic phenomena in Penn Treebanks. [10] They proposed a log-linear model for restoring ECs in Chinese treebanks, using lexical and syntactic characteristics; for training and testing, they encoded the surface node in two categories, EC and NEC (non-EC). Therefore, restoration is actually a binary classification process. However, the biggest drawback of this study is overlooking structural and position information. [11] They presented a simple and highly effective EC recovery approach that can be fully integrated into the state-of-the-art parser. The basic assumption is that the state splitting of the parsing model will allow it to learn where to expect ECs to be inserted into the test sets."}, {"heading": "5.2. Zero Pronoun Resolution", "text": "In this context, it has to be noted that the cases mentioned in the study are a \"miscalculation\" that affects the way in which it has taken place in recent years, namely the way in which it has taken place in recent years. (...) In this context, it has to be established that it is a miscalculation. (...) In this context, it has to be established that it is a miscalculation. (...) It is not only a miscalculation, but also a supposition that it is a miscalculation. (...) It is also a supposition that it is a miscalculation. (...). (...) It is a miscalculation. (...) It is. (...) It is a miscalculation. (...) It is a miscalculation. (...). (...) It is. (...). (... It is. (...) It is. (... It is. (...) It is. \"It is.\" It is. \"It is.\" It is. (... It is.) It is. \"It is. (... It is.\" It is. \"It is.\" It is........... \"It is......\" It is.... \"It is...\" It is.. \"It is..\" It is.. \"It is.......\" It is.. \"It is.....\" it is. \"it is.\" it is. \"it is. (... it is.\" it is. \"it is. (... it is.\" it is. it is. it is. \"it is. it is.\" it is. it is. \"it is. it is. (... it is. (... it is. it is. (... it is. it is. it is. it is. it is. it is. it is. it is. it is. it is. it is. it is. it is. it is.\" it is. it is. (... it is. \"it is. it is. it is. it is. it is. it is. it is. it is.\" it is. it is"}, {"heading": "6. Conclusion and Future Work", "text": "In this study, we present a novel neural network framework, the NRM, for the restoration of sunken pronouns in China. Experimental results show that the proposed NRM significantly exceeds the current approach in statistics. Furthermore, we integrate the recovered sunken pronouns into the task of resolving zeros pronouns. Experimental results show that the performance of the resolution of zeros pronouns can be improved by restoring sunken pronouns. Future work will focus on the following: First, we plan to explore more compatible neural networks to capture the context information and compositional semantics of words. Second, since one of the obstacles to the monitored learning approach is the lack of commented training data, we automatically plan to create or generate training data for the restoration of sunken pronouns."}], "references": [{"title": "A novel approach for dropped pronoun translation", "author": ["W. Longyue", "T. Zhaopeng", "Z. Xiaojun", "L. Hang", "W. Andy", "L. Qun"], "venue": "in: Proceedings of NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Annotating dropped pronouns in chinese newswire text", "author": ["E. Baran", "Y. Yang", "N. Xue"], "venue": "in: LREC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Recovering dropped pronouns from chinese text messages, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)", "author": ["Y. Yang", "Y. Liu", "N. Xue"], "venue": "Association for Computational Linguistics, Beijing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A coefficient of agreement for nominal scales, Educational and psychological measurement", "author": ["J. Cohen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1960}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "in: Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Eprint Arxiv", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A deep neural network for chinese zero pronoun resolution, in: arXiv:1604.05800v1, 2016", "author": ["Q. Yin", "W.-N. Zhang", "Y. Zhang", "T. Liu"], "venue": "URL http://arxiv.org/abs/1604.05800v1", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Chinese zero pronoun resolution: A joint unsupervised discourse-aware model rivaling state-of-the-art resolvers", "author": ["C. Chen", "V. Ng"], "venue": "in: Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Chasing the ghost: recovering empty categories in the chinese treebank", "author": ["Y. Yang", "N. Xue"], "venue": "in: Proceedings of the 23rd International Conference on Computational Linguistics: Posters, Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Language-independent parsing with empty elements", "author": ["S. Cai", "D. Chiang", "Y. Goldberg"], "venue": "in: ACL (Short Papers),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Dependency-based empty category detection via phrase structure trees", "author": ["N. Xue", "Y. Yang"], "venue": "in: Proceedings of NAACL-HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A clause-level hybrid approach to chinese empty element recovery, in: Proceedings of the Twenty-Third international joint conference on Artificial Intelligence", "author": ["F. Kong", "G. Zhou"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Enlisting the ghost: Modeling empty categories for machine translation", "author": ["B. Xiang", "X. Luo", "B. Zhou"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Identification and resolution of chinese zero pronouns: A machine learning approach", "author": ["S. Zhao", "H.T. Ng"], "venue": "in: EMNLP-CoNLL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A tree kernel-based unified framework for chinese zero anaphora resolution", "author": ["F. Kong", "G. Zhou"], "venue": "in: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Zero-anaphora resolution by learning rich syntactic pattern features, ACM Transactions on Asian Language Information Processing", "author": ["R. Iida", "K. Inui", "Y. Matsumoto"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames", "author": ["R. Sasano", "S. Kurohashi"], "venue": "in: IJCNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Subject/object drop in the acquisition of korean: A crosslinguistic comparison", "author": ["Y.-J. Kim"], "venue": "Journal of East Asian Linguistics", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming", "author": ["C. Chen", "V. Ng"], "venue": "in: Twenty- Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently, recovering the dropped pronouns has been verified to be effective for statistical machine translation [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "To address the dropped pronoun recovery problem, [2] manually annotated dropped pronouns in Chinese newswire text.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "[3] utilized a maximum entropy (ME) classifier to recover dropped pronoun from Chinese text messages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Actual Pronouns In this study, we follow the annotation scheme in [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "The annotation framework which is proposed by [2] includes 14 types of pronouns.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "The details of these pronouns by following the description of [2] are listed below:", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "For data annotation, we follow the annotation scheme proposed by [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "The annotating agreement measured by Cohen\u2019s Kappa [4] equals to 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Multi-Layer Perceptron Model [5] proposed a unified framework for natural language processing (NLP), such as part-of-speech tagging, chunking, named entity recognition, semantic role labeling, language modeling, semantically related words, etc.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Inspired by [5], to the specification of the dropped pronoun recovery task, we proposed a multi-layer perceptron model to realize the dropped position identification neural network (Section 3.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "The learning theory of the Word2Vec is described in [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "They are SVM [7] with linear kernel and sparse vector input (SVMSL), SVM with linear kernel and", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "2015 [3]) from Chinese text message for empirical comparison.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "predicted by the zero pronoun specific neural network (ZPSNN) [8], the NRM generates a pronoun, of which the representation is an embedding vector.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "To verify the effectiveness of the NRM for zero pronoun resolution, we compare the results of the ZPSNN and the ZPSNN+NRM with the results of the state-of-the-art (SOTA) approach [9] on zero pronoun resolution task.", "startOffset": 179, "endOffset": 182}, {"referenceID": 8, "context": "As we use the same dataset for training, development and test with [9], we directly show the original experimental results of [9] in Table 7.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "As we use the same dataset for training, development and test with [9], we directly show the original experimental results of [9] in Table 7.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "[10] proposed a log-linear based model to recover the ECs in Chinese Treebanks, by utilizing the lexical and syntactic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] presented a simple and highly effective EC recovery approach, which can fully integrate with state-of-the-art parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] described a novel approach to detecting ECs that represented in dependency parse trees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] explored a clause-level hybrid approach, which integrated the linear tagging and structure parsing information, to recovering ECs in Chinese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed a structure learning based approach to recovering ECs for machine translation (MT) task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] first performed the identification and resolution of Chinese anaphoric zero pronouns by using a machine learning approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a unified tree kernel based framework for Chinese zero pronoun resolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] further extended [15]\u2019s work by involving more features and exploiting the co-reference links between zero pronoun and antecedent into a SVM classifier.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Experimental results indicated that the extended approach outperformed the two state-of-the-art approaches [16, 15] significantly.", "startOffset": 107, "endOffset": 115}, {"referenceID": 14, "context": "Experimental results indicated that the extended approach outperformed the two state-of-the-art approaches [16, 15] significantly.", "startOffset": 107, "endOffset": 115}, {"referenceID": 16, "context": "[18] implemented the zero pronoun resolution task in a semantic role labeling framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] exploited the generated lexicalized case frames from large scale Web sentences as external knowledge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] discussed the subjectobject drop phenomenon and the pattern found in the spoken and written text of child Korean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] proposed an unsupervised approach for Chinese zero pronouns resolution with language independent features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "They [9] further proposed an end-to-end unsupervised probabilistic model for Chinese zero pronoun resolution task.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.", "creator": "TeX"}}}