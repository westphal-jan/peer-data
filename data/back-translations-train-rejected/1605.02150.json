{"id": "1605.02150", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "On Improving Informativity and Grammaticality for Multi-Sentence Compression", "abstract": "Multi Sentence Compression (MSC) is of great value to many real world applications, such as guided microblog summarization, opinion summarization and newswire summarization. Recently, word graph-based approaches have been proposed and become popular in MSC. Their key assumption is that redundancy among a set of related sentences provides a reliable way to generate informative and grammatical sentences. In this paper, we propose an effective approach to enhance the word graph-based MSC and tackle the issue that most of the state-of-the-art MSC approaches are confronted with: i.e., improving both informativity and grammaticality at the same time. Our approach consists of three main components: (1) a merging method based on Multiword Expressions (MWE); (2) a mapping strategy based on synonymy between words; (3) a re-ranking step to identify the best compression candidates generated using a POS-based language model (POS-LM). We demonstrate the effectiveness of this novel approach using a dataset made of clusters of English newswire sentences. The observed improvements on informativity and grammaticality of the generated compressions show that our approach is superior to state-of-the-art MSC methods.", "histories": [["v1", "Sat, 7 May 2016 06:39:57 GMT  (473kb,D)", "http://arxiv.org/abs/1605.02150v1", "19 pages"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["elaheh shafieibavani", "mohammad ebrahimi", "raymond wong", "fang chen"], "accepted": false, "id": "1605.02150"}, "pdf": {"name": "1605.02150.pdf", "metadata": {"source": "CRF", "title": "On Improving Informativity and Grammaticality for Multi-Sentence Compression", "authors": ["Elahe Shafiei", "Mohammad Ebrahimi", "Raymond Wong", "Fang Chen"], "emails": ["elahehs@cse.unsw.edu.au", "mohammade@cse.unsw.edu.au", "wong@cse.unsw.edu.au", "fang@cse.unsw.edu.au"], "sections": [{"heading": null, "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-Sentence Compression", "text": "State-of-the-art approaches to MSC are usually divided into monitored [21, 11] and unattended groups [6]. MSC methods traditionally use a syntactic parser to generate grammatical compressions and fall into two categories (based on their implementations): (1) tree-based approaches that generate a compressed sentence by editing the syntactic tree of the original approach [21, 11, 10, 8]; (2) sentence-based approaches that directly generate strings [6]. Alternatively, word-based approaches that require only a POS tagger have recently been used in various tasks, such as guided microblog summaries [12] and message wire summaries [9, 4, 30]. In these approaches, a directional word curve is constructed in which words are represented, while edges between two nodes adjacent relationships between words are in a sentence."}, {"heading": "2.2 Multiword Expressions", "text": "An MWE is a combination of words with lexical, syntactic or semantic idiosyncrasy [26, 3]. It is estimated that the number of MWEs in a native speaker's lexicon is of the same order of magnitude as the number of individual words [15]. Therefore, the explicit identification of MWEs in various NLP applications has proven useful. Components of an MWE can be treated as one unit to improve the effectiveness of steps for reclassification in IR systems [1]. In this paper, we identify MWEs, merge their components and replace them with their available one-word synonyms if necessary. These strategies help to create an improved word graph and improve the informativity of the compression candidates."}, {"heading": "2.3 POS-based Language Model (POS-LM)", "text": "A language model assigns a probability to a sequence of m words P (w1,..., wm) by means of a probability distribution. Language models are an essential element of natural language processing for tasks from spell checking to machine translation. In view of the increasing need to ensure grammatical sentences in various applications, POS-LM comes into play as a remedy. POS-LM describes the probability of a sequence of m POS tags P (t1,..., tm). POS-LMs are traditionally used for speech recognition problems [14] and statistical machine translation systems [17, 23, 25] for collecting syntactic information. In this essay, we benefit from POS-LMs to collect syntactical information from sentences and improve the grammaticality of compression candidates."}, {"heading": "3 Proposed Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Word Graph Construction for MSC", "text": "Consider a set of contiguous sentences S = {s1, s2,..., sn}, a traditional word graph is constructed by adding iterative sentences to it. This directed graph is an ordered pair G = (V, E), which consists of a series of vertices or words together with a series of directed edges that show the adaptance between the corresponding nodes [9, 4]. The graph is constructed first by the first sentence and shows words in a sentence as a sequence of connected nodes. The first node is the start node and the last is the end node. Words are added to the graph in three steps in the following order: (1) non-stop words for which there is no candidate in the graph; or for which a unique mapping is possible (i.e. there is only one node in the graph that refers to the same word / POS pair); (2) non-stop words for which there are either several possible candidates in the graph that relate to the same pair (i.e. there are more than one pair in the OS; there are only one in the pair)."}, {"heading": "3.2 Merging and Mapping Strategies", "text": "In fact, it is the case that most people who live in the USA also live in the USA, and that most of them in the USA are not able to live in the USA. In the USA, it is the case that they live in the USA. In the USA, it is the case that most of them live in the USA. In the USA, it is the case that they live in the USA. In the USA, it is the case that they live in the USA. In the USA, it is the case that most of them live in the USA. In the USA, it is the case that they live in the USA. In the USA, it is the case that they live in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA and in the USA."}, {"heading": "3.3 Re-ranking Strategy (POS-LM)", "text": "In fact, it will be able to stir the aforementioned lcihsrc\u00fcehncS until it is able to mention itself."}, {"heading": "4 Data Preparation", "text": "Many attempts have been made to release various types of data sets and rating corpora for sentence compression and automatic summary, as introduced in [5], but to our knowledge there is no data set to automatically evaluate MSC [4]. Since the prepared data set in Boudin and Morin (2013) is also available in French, we have followed the instructions below to construct a standard newswire dataset: We have gathered news articles in clusters on Australia1 and the US-2 edition of Google News over a five-month period (January 2015 - May 2015). Clusters composed of at least 15 news articles on a single news event have been manually extracted from different categories (i.e. Top Stories, World, Business, Technology, Science, Health, etc.). Leading sentences in news articles are known to provide a good summary of the article and are used as the basis for the summary [7]."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Evaluation Metrics", "text": "We evaluate the proposed method on the basis of our constructed data set (normal and diverse clusters) using automatic and manual assessments.The quality of the compressions generated was automatically assessed on the basis of version 2.0 1 of Rouge [20] and version 13a 2 of Bleu [24].These sets of metrics are typically used to evaluate automatic summary and machine translation.They compare an automatically generated summary with a reference or a series of man-made summary.1http: / / kavita-ganesan.com / content / rouge-2.0 2ftp: / / jaguar.ncsl.gov / mt / resources / mteval-v13a.plFor the manual examination of the quality of the compressions generated, three native English speakers were asked to rate the gramaticality and informaticality of the compression (which is based on the dot scale defined in Filippova (2010). Grammaticality: (if the compression is grammatical) (if i) (if the dot is (i); if the compression is slight; if i)."}, {"heading": "5.2 Experiment Results", "text": "Two existing approaches, i.e. Filippova (2010) and Boudin and Morin (2013), are used as foundations of informatics and basic research in our experiments. In order to better understand the behavior of our system, we examined our test data sets and made the following observations. For manual evaluation (Table 5.2), we observed a significant improvement in the average grammar and informatics scores along with the compression rate. Baseline1 \"s informativeness is negatively impacted by the lack of important information about the set of related sentences [4]. However, Baseline2 has decreased the informatics and informatics quality scores along with the compression rates achieved by the outputs of longer compressions. In our approach, the remarkable improvement of grammaticality scores is due to the addition of the syntactic order-step-step-step-step-step-step-step-step-step-step-step-step-step-the."}, {"heading": "6 Conclusions", "text": "In short, we presented our attempt to use MWEs, synonyms and POS-based language modeling to address one of the weaknesses of MSC, which simultaneously improves informativeness and grammaticality. Through manual and automatic (Rouge and Bleu) evaluations, experiments with a constructed English Newswire dataset show that our approach outperforms competitive baselines. In particular, the proposed merge and mapping strategies, together with the grammatically improved POS-LM revaluation method, improve both informaticity and grammaticality of the compressions with an improved compression ratio."}], "references": [{"title": "Identification and treatment of multiword expressions applied to information retrieval. In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World, pages 101\u2013109", "author": ["Otavio Costa Acosta", "Aline Villavicencio", "Viviane P Moreira"], "venue": "Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio"], "venue": "Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Multiword expressions. Handbook of Natural Language Processing, second edition", "author": ["Timothy Baldwin", "Su Nam Kim"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Keyphrase extraction for n-best reranking in multi-sentence compression", "author": ["Florian Boudin", "Emmanuel Morin"], "venue": "In North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Models for sentence compression: A comparison across domains, training requirements and evaluation measures", "author": ["James Clarke", "Mirella Lapata"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Modelling compression with discourse constraints", "author": ["James Clarke", "Mirella Lapata"], "venue": "In EMNLP-CoNLL, pages", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Overview of duc 2005", "author": ["Hoa Trang Dang"], "venue": "In Proceedings of the document understanding conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Learning to fuse disparate sentences", "author": ["Micha Elsner", "Deepak Santhanam"], "venue": "In Proceedings of the Workshop on Monolingual Text-To-Text Generation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Multi-sentence compression: finding shortest paths in word graphs", "author": ["Katja Filippova"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Sentence fusion via dependency graph compression", "author": ["Katja Filippova", "Michael Strube"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Lexicalized markov grammars for sentence compression", "author": ["Michel Galley", "Kathleen McKeown"], "venue": "In HLT-NAACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Opinosis: a graphbased approach to abstractive summarization of highly redundant opinions", "author": ["Kavita Ganesan", "ChengXiang Zhai", "Jiawei Han"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A bit of progress in language modeling", "author": ["Joshua T Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Pos tagging versus classes in language modeling", "author": ["Peter A Heeman"], "venue": "In Proceedings of the 6th Workshop on Very Large Corpora,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "The architecture of the language faculty", "author": ["Ray Jackendoff"], "venue": "Number 28. MIT Press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Sentence reduction for automatic text summarization", "author": ["Hongyan Jing"], "venue": "In Proceedings of the sixth conference on Applied natural language processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Towards better machine translation quality for the german\u2013english language pairs", "author": ["Philipp Koehn", "Abhishek Arun", "Hieu Hoang"], "venue": "In Proceedings of the Third Workshop on Statistical Machine Translation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "jmwe: A java toolkit for detecting multi-word expressions", "author": ["Nidhi Kulkarni", "Mark Alan Finlayson"], "venue": "In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Discriminative sentence compression with soft syntactic evidence", "author": ["Ryan T McDonald"], "venue": "In EACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}, {"title": "Statistical machine translation with local language models", "author": ["Christof Monz"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Morpheme-and pos-based ibm1 scores and language model scores for translation quality estimation", "author": ["Maja Popovi\u0107"], "venue": "In Proceedings of the Seventh Workshop on Statistical Machine Translation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Multiword expressions: A pain in the neck for nlp", "author": ["Ivan A Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Experiments in microblog summarization", "author": ["Beaux Sharifi", "Mark-Anthony Hutton", "Jugal K Kalita"], "venue": "In Social Computing (SocialCom),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In INTERSPEECH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Learning to summarise related sentences", "author": ["Emmanouil Tzouridis", "Jamal Abdul Nasir", "LUMS Lahore", "Ulf Brefeld"], "venue": "In The 25th International Conference on Computational Linguistics (COLING14),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization", "author": ["Dingding Wang", "Tao Li", "Shenghuo Zhu", "Chris Ding"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "Multi-Sentence Compression (MSC) refers to the method of mapping a collection of related sentences to a sentence shorter than the average length of the input sentences, while retaining the most important information that conveys the gist of the content, and still remain grammatically correct [16, 4].", "startOffset": 293, "endOffset": 300}, {"referenceID": 3, "context": "Multi-Sentence Compression (MSC) refers to the method of mapping a collection of related sentences to a sentence shorter than the average length of the input sentences, while retaining the most important information that conveys the gist of the content, and still remain grammatically correct [16, 4].", "startOffset": 293, "endOffset": 300}, {"referenceID": 3, "context": "MSC is one of the challenging tasks in natural language processing that has recently attracted increasing interest [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 30, "context": "A standard way to generate summaries usually consists of the following steps: ranking sentences by their importance, clustering them by similarity, and selecting a sentence from the top ranked clusters [31].", "startOffset": 202, "endOffset": 206}, {"referenceID": 9, "context": "[10, 8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[10, 8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "As an alternative, some recent works in this field [9, 4] are based on word graphs, which only require a Part-Of-Speech (POS) tagger and a list of stopwords.", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "As an alternative, some recent works in this field [9, 4] are based on word graphs, which only require a Part-Of-Speech (POS) tagger and a list of stopwords.", "startOffset": 51, "endOffset": 57}, {"referenceID": 8, "context": "Although the proposed approach in [9] introduces an elegant word graph to MSC, approximately half of their generated sentences are missing important information about the set of related sentences [4].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "Although the proposed approach in [9] introduces an elegant word graph to MSC, approximately half of their generated sentences are missing important information about the set of related sentences [4].", "startOffset": 196, "endOffset": 199}, {"referenceID": 20, "context": "State-of-the-art approaches in the field of MSC are generally divided into supervised [21, 11] and unsupervised groups [6].", "startOffset": 86, "endOffset": 94}, {"referenceID": 10, "context": "State-of-the-art approaches in the field of MSC are generally divided into supervised [21, 11] and unsupervised groups [6].", "startOffset": 86, "endOffset": 94}, {"referenceID": 5, "context": "State-of-the-art approaches in the field of MSC are generally divided into supervised [21, 11] and unsupervised groups [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 20, "context": "tence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings directly [6].", "startOffset": 6, "endOffset": 21}, {"referenceID": 10, "context": "tence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings directly [6].", "startOffset": 6, "endOffset": 21}, {"referenceID": 9, "context": "tence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings directly [6].", "startOffset": 6, "endOffset": 21}, {"referenceID": 7, "context": "tence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings directly [6].", "startOffset": 6, "endOffset": 21}, {"referenceID": 5, "context": "tence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings directly [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 26, "context": "As an alternative, word graph-based approaches that only require a POS tagger have recently been used in different tasks, such as guided microblog summarization [27], opinion summarization [12] and newswire summarization [9, 4, 30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "As an alternative, word graph-based approaches that only require a POS tagger have recently been used in different tasks, such as guided microblog summarization [27], opinion summarization [12] and newswire summarization [9, 4, 30].", "startOffset": 189, "endOffset": 193}, {"referenceID": 8, "context": "As an alternative, word graph-based approaches that only require a POS tagger have recently been used in different tasks, such as guided microblog summarization [27], opinion summarization [12] and newswire summarization [9, 4, 30].", "startOffset": 221, "endOffset": 231}, {"referenceID": 3, "context": "As an alternative, word graph-based approaches that only require a POS tagger have recently been used in different tasks, such as guided microblog summarization [27], opinion summarization [12] and newswire summarization [9, 4, 30].", "startOffset": 221, "endOffset": 231}, {"referenceID": 29, "context": "As an alternative, word graph-based approaches that only require a POS tagger have recently been used in different tasks, such as guided microblog summarization [27], opinion summarization [12] and newswire summarization [9, 4, 30].", "startOffset": 221, "endOffset": 231}, {"referenceID": 3, "context": "However, some important information are missed from 48% to 60% of the generated sentences in their approach [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 25, "context": "An MWE is a combination of words with lexical, syntactic or semantic idiosyncrasy [26, 3].", "startOffset": 82, "endOffset": 89}, {"referenceID": 2, "context": "An MWE is a combination of words with lexical, syntactic or semantic idiosyncrasy [26, 3].", "startOffset": 82, "endOffset": 89}, {"referenceID": 14, "context": "It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Components of an MWE can be treated as a single unit to improve the effectiveness of re-ranking steps in IR systems [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 13, "context": "POS-LMs are traditionally used for speech recognition problems [14] and statistical machine translation systems [17, 23, 25] to capture syntactic information.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "POS-LMs are traditionally used for speech recognition problems [14] and statistical machine translation systems [17, 23, 25] to capture syntactic information.", "startOffset": 112, "endOffset": 124}, {"referenceID": 22, "context": "POS-LMs are traditionally used for speech recognition problems [14] and statistical machine translation systems [17, 23, 25] to capture syntactic information.", "startOffset": 112, "endOffset": 124}, {"referenceID": 24, "context": "POS-LMs are traditionally used for speech recognition problems [14] and statistical machine translation systems [17, 23, 25] to capture syntactic information.", "startOffset": 112, "endOffset": 124}, {"referenceID": 8, "context": "This directed graph is an ordered pair G = (V,E) comprising of a set of vertices or words together with a set of directed edges which shows the adjacency between corresponding nodes [9, 4].", "startOffset": 182, "endOffset": 188}, {"referenceID": 3, "context": "This directed graph is an ordered pair G = (V,E) comprising of a set of vertices or words together with a set of directed edges which shows the adjacency between corresponding nodes [9, 4].", "startOffset": 182, "endOffset": 188}, {"referenceID": 18, "context": "To detect MWEs, we use the jMWE toolkit [19], which is a Java-based library for constructing and testing MWE token detectors.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "0 of WordNet [22] to obtain its available one-word synonym with an appropriate POS and replace the n-words MWE with a shorter synonym word.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "6 [23].", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "To build a POS-LM, we use the SRILM toolkit with modified Kneser-Ney smoothing [28], and train the language model on our POS annotated corpus.", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "In this regard, we make use of the Stanford POS tagger [29] to annotate the AFE sections of LDCs Gigaword corpus (LDC2003T05) as a large newswire corpus (\u223c170 M-words).", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Since modified Kneser-Ney discounting appears to be the most efficient method in a systematic description and comparison of the usual smoothing methods [13], we use this type of smoothing to help our language model.", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "This may encourage more syntactically correct output [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "As the estimated scores for each cluster of sentences fall into different ranges, we make use of unity-based normalization to bring the values of score(c) in Equation 4, and the scoreLM into the range [0, 1].", "startOffset": 201, "endOffset": 207}, {"referenceID": 4, "context": "Many attempts have been made to release various kinds of datasets and evaluation corpora for sentence compression and automatic summarization, such as the one introduced in [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "However, to our knowledge, there is no dataset available to evaluate MSC in an automatic way [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Leading sentences in news articles are known to provide a good summary of the article content and are used as a baseline in summarization [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 29, "context": "This results in low lexical and syntactical diversity, and vice versa [30].", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "0 1 of Rouge [20] and the version 13a 2 of Bleu [24].", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "0 1 of Rouge [20] and the version 13a 2 of Bleu [24].", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "6) through Kappa\u2019s evaluation metrics, which indicates that the strength of this agreement is moderate [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "The informativity of Baseline1 is adversely influenced by missing important information about the set of related sentences [4].", "startOffset": 123, "endOffset": 126}], "year": 2016, "abstractText": "Multi Sentence Compression (MSC) is of great value to many real world applications, such as guided microblog summarization, opinion summarization and newswire summarization. Recently, word graph-based approaches have been proposed and become popular in MSC. Their key assumption is that redundancy among a set of related sentences provides a reliable way to generate informative and grammatical sentences. In this paper, we propose an effective approach to enhance the word graph-based MSC and tackle the issue that most of the state-of-the-art MSC approaches are confronted with: i.e., improving both informativity and grammaticality at the same time. Our approach consists of three main components: (1) a merging method based on Multiword Expressions (MWE); (2) a mapping strategy based on synonymy between words; (3) a re-ranking step to identify the best compression candidates generated using a POS-based language model (POS-LM). We demonstrate the effectiveness of this novel approach using a dataset made of clusters of English newswire sentences. The observed improvements on informativity and grammaticality of the generated compressions show that our approach is superior to state-of-the-art MSC methods.", "creator": "LaTeX with hyperref package"}}}