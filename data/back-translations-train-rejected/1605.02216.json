{"id": "1605.02216", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Distributed stochastic optimization for deep learning (thesis)", "abstract": "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method.", "histories": [["v1", "Sat, 7 May 2016 16:55:22 GMT  (4615kb,D)", "http://arxiv.org/abs/1605.02216v1", "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun"]], "COMMENTS": "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sixin zhang"], "accepted": false, "id": "1605.02216"}, "pdf": {"name": "1605.02216.pdf", "metadata": {"source": "CRF", "title": "Distributed stochastic optimization for deep learning", "authors": ["Yann LeCun", "Minjie Wang", "Zhaoguo Wang"], "emails": [], "sections": [{"heading": null, "text": "Distributed stochastic optimization for deep learning by Sixin ZhangA dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of PhilosophyDepartment of Computer ScienceNew York University May 2016 - - - - - - - - -Yann LeCunar Xiv: 160 5.02 216v 1 [cs.L G] 7M ay2 016"}, {"heading": "Dedication", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "To my parents and grandparents", "text": ""}, {"heading": "Acknowledgements", "text": "The time you spend most on it during your doctoral thesis is also determined by the way we interact with each other. My consultant, Yann LeCun, is not a doppelganger who has a great influence on me. To some extent, the way we interact with each other is like the Elastic Averaging SGD (EASGD) method we have jointly named. Throughout my doctoral thesis, I am very grateful that I have the freedom to explore various research topics. He always has patience to wait, even if sometimes there are no interesting results. Sometimes it is an experiment, the good result is just one step away, and he has a good feeling of it. Sometimes he is also very strict. Remember when I was preparing the proposal for the work, but everyone was worried as the deadline approached."}, {"heading": "Abstract", "text": "We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its state of stability with the existing ADMM method in the round robin scheme. An asynchronous and dynamic variant of the EASGD method is used to train deep Convolutionary Neural Networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. In addition, it requires a much lower level of communication than other common baseline approaches such as the DOWNPOUR method.We then investigate the limit in the speed of the initial and asymptotic phases of the mini-batch SGD, the Dynamic SGD method and the EASGD optimization. We find that the diffusion of the Dynamic GASD method represents a surprising convergence between its initial stability and the asymptotic phase of the SGD-EGD method."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dedication ii", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Acknowledgements iv", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Abstract v", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "List of Figures viii", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "List of Tables xvi", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "1.1 What is the problem?...................................................................................................................."}, {"heading": "2 Elastic Averaging SGD (EASGD) 9", "text": ".........................................................................................................................."}, {"heading": "3 Convergence Analysis of EASGD 17", "text": "3.1 Square case................................................ 173.1.1 One-dimensional case.......................... 18 3.1.2 Generalization to multidimensional case................ 263.2. Strong convex case.........................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Performance in Deep Learning 43", "text": "4.1. Experimental setup......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 The Limit in Speedup 71", "text": "5.1. Additional noise............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Scaling up Elastic Averaging SGD 112", "text": "6.1 EASGD tree..................................................... 1126.1.1 The algorithm................................ 113.............................................."}, {"heading": "7 Conclusion 134", "text": "Bibliography 138vii"}, {"heading": "List of Figures", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "List of Tables", "text": "4.1 The learning rates studied in Figures 4.1, 4.2, 4.3 and 4.4 (CIFAR experiment) for each method shown in Figures 4.5, 4.6 and 4.7 (CIFAR experiment).........................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 What is the problem", "text": "This year it has come to the point where it is only a matter of time before it will happen, until it does."}, {"heading": "1.2 Formalizing the problem", "text": "Consider the minimization of a function F (x) in a parallel computer environment [7] with p-N workers and a master. In this thesis, we focus on the stochastic optimization problem of the following form x (x): = E [f (x)], (1,1) where x is the model parameter to be estimated and that x is a random variable that follows the probability distribution P over x, so that F (x) = x-f (x) P (d) is smooth. However, the optimization problem in Equation 1.1 can be reformulated as a follow-up parameter in x1,..., xp, x-p [f (xi, i)] + x-f (1,2), each following the same distribution P (hence, we assume that each worker can scan scan scan the entire dataset).In this thesis, we refer to xs as local variables and point to x-x-x-b as the central distractable variable.The problem of the consensus of these two is that the problem of equality and the 11 is the problem of consensus."}, {"heading": "1.3 An overview", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "Elastic Averaging SGD (EASGD)", "text": "In this chapter we will discuss how to extend the synchronous EASGD method to the asynchronous scenario. (EASGD) We will first discuss the synchronous EASGD method with two different approaches. (EASGD) We will first discuss the synchronous EASGD method with two different approaches. (EASGD) We will first discuss the synchronous EASGD method with two different approaches. (EASGD) We will then discuss the synchronous EASGD method with two different approaches. (EASGD) We will first discuss the synchronous EASGD method with two different approaches. (EASGD) We will first discuss the synchronous EASGD method with two different approaches. (EASGD) We will then discuss the synchronous EASGD method with two different approaches. (EASGD) We will discuss the synchronous EASGD method with two different approaches."}, {"heading": "Convergence Analysis of EASGD", "text": "In this chapter we provide the convergence analysis of the synchronous EASGD algorithm with constant learning rate. The analysis focuses on the convergence of the mean variables to the optimum. First, we discuss a one-dimensional square case (Lemma 3.1.1), then we introduce a two-dimensional mean sequence and prove that it is asymptotically optimal. We provide two clear proofs for this (one in Lemma 3.1.2 for the one-dimensional case and the other in Lemma 3.1.3 for the multidimensional case). We extend the analysis to the strongly convex case as described in Theorem 3.2.1. Finally, we offer a stability analysis of the asynchronous EASGD and ADMM methods in the round-robin scheme in Section 3.3."}, {"heading": "3.1 Quadratic case", "text": "Our analysis in the quadratic case extends the analysis of the ASGD in [45]. Suppose each of the p local workers observes a noise gradient at a point in time t \u2265 0 of the linear form defined in Equation 3.1.git (x i t) = Ax i \u2212 b \u2212 \u043fit, i \u0441it {1,..., p}, (3.1) 17 where the matrix A is positively defined (each eigenvalue is strictly positive) and the random variables are i.e., with a matrix of zero mean and positively defined covariance. Let us let x \u0445 denote the optimal solution where x \u0445 = A \u2212 1b-Rn."}, {"heading": "3.1.1 One-dimensional case", "text": "In this section, we will analyze the behavior of the mean squared error (MSE) of the mean variable x \u00b2, where this error is called E [\u00b2 x \u00b2 t \u2212 x \u00b2 2], as a function of t, p \u2212 p \u2212 p, p \u2212 p, and \u03b2, where \u03b2 = p\u0430. Note that the MSE error can be dissected as (squared) bias and variance. For a one-dimensional case (n = 1), we will assume A = h > 0 and sp = 2 > 0. Lemma 3.1.1. Let x and {xi0} i = 1, p be arbitrary constants, thenE [x \u00b2) = quadrified cases (n = 1) assume that the squared order is not equal."}, {"heading": "Visualizing Lemma 3.1.1", "text": "Figure 3.1 illustrates the dependence of MSE on \u03b2, \u03b7 and the number of processors p over time. We look at the setting with high noise, where x-0 = x i = 1, h = 1 and \u03c3 = 10. The MSE error is colour coded in such a way that the deep blue colour corresponds to the MSE equals 10 \u2212 3, the green colour to the MSE equals 1, the red colour to the MSE equals 103 and the dark red colour corresponds to the divergence of the EASGD algorithm (condition in Eq.3.4 is then violated)."}, {"heading": "Condition in Equation 3.4", "text": "We will show that the number of workers working in a country, compared to the number of workers in relation to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to a country, compared to other countries, compared to a country, compared to other countries, compared to one country, compared to other countries, we will show that the number of workers is the same in relation to one country, compared to another, we will show that the number of workers is the same in relation to one country, compared to another."}, {"heading": "3.1.2 Generalization to multidimensional case", "text": "The asymptotic variance in the lemmas 3.1.2 is optimal with each fixed setpoint and each individual case, for which equation 3.4 applies = = details. The next problem (lemmas 3.1.3) extends the result in lemmas 3.1.2 to the multidimensional situation. (Lt h denotes the largest eigenvalue of A. If (2 \u2212 g) > 2\u03b2 / p, (2 \u2212 g) > 2xp, (2 \u2212 g) > \u03b2 / p, (2 \u2212 g) > 0 and \u03b2 > 0, then the normalized double average sequence is switched weakly to the normal distribution with the mean and the covariance. (A \u2212 1) T, p (zt \u2212 x) N (0, V) > 0 and \u03b2 > 0, then the normalized average sequence is switched weakly to the normal distribution. (3.17) Since A is symmetrical, one can use the evidence technology of lemmas 3.1.2 to prove lemmas by diagramming the matrix."}, {"heading": "3.2 Strongly convex case", "text": "We are now extending the above evidence ideas to analyze the strongly convex case in which the rust gradient Git (x) \u00b2 \u00b2 \u00b2 = equal F (x) \u2212 x) \u2212 x \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "Performance in Deep Learning", "text": "In this chapter we compare empirically the performance in deep learning of asynchronous EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their mean and impulse variants. \u2022 All parallel comparison methods are listed below: \u2022 DOWNPOUR [16], the detail and the pseudo-code of the implementation are described in Section 4.4 (Algorithm 3). \u2022 Momentum DOWNPOUR (MDOWNPOUR), where the current scheme of Nesterov is applied to updating the master (note: it is unclear how it should be applied to the local workers or to the case when it is > 1). \u2022 The pseudo-code is described in Section 4.4 (Algorithms 4 and 5). \u2022 A method we call ADOWNPOUR, where we average over the time of the central variables x as follows."}, {"heading": "4.1 Experimental setup", "text": "For all our experiments, we use a GPU cluster interconnected with InfiniBand. \u2192 \u2192 Each node has 4 Titan GPU processors in which each local worker corresponds to a GPU processor. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 0,5 \u2212 Each node is stored on the centralized parameter server and updated [16]. Our implementation is available at https: / / github.com / sixin-zh / mpiT.To describe the architecture of the conventional neural network, we will first introduce a notation (c, x, y) indicating the size of the input image on each layer, with c indicating the number of color channels and (x, y) the horizontal and vertical dimensions of usage of 2.2."}, {"heading": "Data preprocessing", "text": "For the ImageNet experiment, each RGB image is re-sorted so that the smallest dimension is 256 pixels. We also scale each pixel value to the interval [0, 1]. We then extract random harvests (and their horizontal rotations) of size 3 x 221 x 221 pixels and present them to the network in mini-batches of size 128. For the CIFAR experiment, we use the original RGB image of size 3 x 32 x 32. As before, we scale each pixel value to the interval [0, 1]. We then extract random harvests (and their horizontal flips) of size 3 x 28 pixels and present them to the network in mini-batches of size 128. Formation and test loss and test error are calculated only from the central patch (3 x 28 x 28 cm)."}, {"heading": "4.2 Experimental results", "text": "For all experiments in this section we use EASGD with \u03b2 = 0.9 and \u03b1 = \u03b2 / p, for all impulse-based methods we set the impulse term \u03b4 = 0.99, and finally for MVADOWNPOUR we set the moving rate to \u03b1 = 0.001. We will start the experiment on CIFAR dataset with p = 4 local workers working on a single computing power. For all methods we have studied for the communication periods of the following set of results, we will examine a wide range of learning rates that will be examined in all experiments, summarized in Table 4.1, 4.2 and 4.3. The CIFAR experiment has been performed 3 times independently of the same method and for each method we report its best performance measured by the smallest achievable test errors.From the results in Figure 4.1, 4.2, 4.4 and 4.4, we conclude that all DOWNPOUR-based methods achieve their best performance."}, {"heading": "4.3 Further discussion and understanding", "text": "4.3.1 Comparing SGD, ASGD, MVASGD and MSGDFor comparison, we also report on the performance of MSGD, which exceeds SGD, ASGD and MVASGD on the test dataset. Remember that the way we compare performance between different methods is based on the smallest possible test error. As the test dataset is fixed on a previous one, we may have a tendency to exceed that test dataset. In fact, as we might see in Figure 4.10, we emphasize that the point here is not the best possible test accuracy, but the possibilities we can find, i.e. the range of dynamics emerging from the neural network to fix this."}, {"heading": "4.3.2 Dependence of the learning rate", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of EAMSGD and EASGD for different learning rates \u03b7 when p = 16 and \u03c4 = 10 are applied to the CIFAR experiment. In Figure 4.12, we observe that higher learning rates \u03b7 lead to better test performance for the EAMSGD algorithm, which can potentially be justified by maintaining higher fluctuations of the local workforce. We suspect that higher fluctuations lead to more exploration and at the same time force higher regulation. However, this image appears to be opposite for the EASSGD algorithm, where higher learning rates impair the performance of the method and lead to overadjustment. Interestingly, in this experiment, for both the EASGD and the EAMSGD algorithm, the learning rate at which the best training performance was achieved resulted in the worst test performance."}, {"heading": "4.3.3 Dependence of the communication period", "text": "In this section, the dependence of the trade-off between exploration and exploitation on the communication period is discussed. We observe in Figure 4.13 that the EASGD algorithm exhibits a very similar convergence behavior when \u03c4 = 1 to even \u03c4 = 1000 for the CIFAR experiment, while the EAMSGD can remain trapped at quite a high energy level (of the object) when \u03c4 = 100. This catching behavior is due to the non-convexity of the objective function. It can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty period, as shown in Figure 4.13. In contrast, the EASGD algorithm does not seem to be caught at all by any saddle point along its orbit.6263The performance capability7 of the EASGD, which is less sensitive to the communication period than EAMSGD, is another noteworthy observation."}, {"heading": "4.3.4 The tradeoff between data and parameter communication", "text": "In addition, we report in Table 4.4 the breakdown of the total runtime for EASGD if \u03c4 = 1 = 124.10 (the time allocation for EAMSGD is almost identical) and DOWNPOUR if \u03c4 = 1 in computing time, data loading time and parameter communication time. For the CIFAR experiment, the reported time is equivalent to the processing of 400 x 128 data samples, whereas for the ImageNet experiment it is equivalent to the processing of 1024 x 128 data samples. For \u03c4 = 1 and p {8, 16} we observe that the communication time is responsible for a significant part of the total runtime, whereas the communication time is negligible compared to the total runtime. (Remember that based on previous results, EASGD and EAMSGD achieve the best performance with larger volume, which is ideal in the environment when communication is time consuming)."}, {"heading": "4.3.5 Time speed-up", "text": "In Figure 4.14 and 4.15 we summarize the time it takes to achieve the same error rate for all methods of the CIFAR and ImageNet experiments; for the CIFAR experiment (Figure 4.14) we investigated the following values: {21%, 20%, 19%, 18%}; and for the ImageNet experiment (Figure 4.15) we investigated: {49%, 47%, 45%, 43%}. If a method does not appear on the figure for a certain error level, it indicates that this method has never reached that level. For the CIFAR experiment, we note that the EASGD method takes less time to reach a certain error level among the EASGD, DOWNPOUR- and MDOWNPOUR methods."}, {"heading": "4.4 Additional pseudo-codes of the algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "DOWNPOUR pseudo-code", "text": "Similar to the asynchronous behavior of EASGD, which we have described in Chapter 2 (Section 2.2), the DOWNPOUR method also performs several steps of local gradient updates by each worker i before pushing the accumulated gradient vi back to the mean variable. Specifically, the i-th worker reads a new mean variable x from the parameter server at the beginning of each period, then performs local SGD steps from the new x. All gradients are accumulated (added), and at the end of this period the total vi is pushed back to the parameter server (added). The mean variable is updated by adding the accumulated gradients of all local workers. Note that we do not use an adaptive learning scheme as has been done in [16]."}, {"heading": "MDOWNPOUR pseudo-code", "text": "Algorithms 4 and 5 capture the pseudo-codes of the dynamics used in this work DOWNPOUR (MDOWNPOUR). Algorithm 4 describes the behavior of each local worker and algorithm 5 describes the behavior of the master. Note: 67.68.69Algorithm 3: DOWNPOUR: processing by worker i and the masterInput: learning rate, communication period \u03c4 N Initialize: x = x is randomly initialized, xi = x, vi = 0, ti = 0 We repeat when we do not use the communication period."}, {"heading": "The Limit in Speedup", "text": "This chapter examines the limitations of the acceleration of several stochastic optimization methods: the mini-batch SGD, the impulse SGD and the EASGD method. In Section 5.1, we first examine the asymptotic phase of these methods using an additive noise model. The continuous SDE approximation (stochastic differential equation) of its SGD update is an Ornstein-Uhlenbeck process. We then examine the initial phase of these methods using a multiplicative noise model in Section 5.2. The continuous SDE approximation of its SGD update is a geometric Brownian movement. In Section 5.3, we examine the stability of the critical points of a simple non-convex problem and discuss when the EASGD method can get stuck at a saddle point."}, {"heading": "5.1 Additive noise", "text": "We (re-) examine the simple additive noise model: a one-dimensional square target with Gaussian noise (as in Section 3.1.1). The objective function evaluated in state x + 2 is defined as the average loss of the square shape (hx \u2212) 2, i.e. we assume it is zero mean and a constant deviation of 2 > 0.5.1.1 SGD with mini-batchThe update rule for the SGD method for solving the problem in Equation 5.1 isxt + xt = xt (hxt \u2212), where x0 is the starting point."}, {"heading": "5.2 Multiplicative noise", "text": "In this section, we will examine a multiplicative noise model that attempts to capture the initial behavior of the stochastic optimization method. It is complementary to the additive noise model that captures the asymptotic behavior. Our starting point is the following linear regression problem in a given case [(v \u2212 au) 2], in which the expectation E is taken about the common distribution of (u, v). However, if the input data (u, v) follow a procedure v = a), we can reduce the above problem to the following case, min x. RE [xu) 2], in which the common distribution of (u, v) is an interesting perspective of this problem, that we can assume that the u2 follow the input data of a gamma distribution (\u03bb u), with the middle east / west distribution."}, {"heading": "5.3 A non-convex case", "text": "Here is an amusing, non-convex case that sheds some light on when EASGD will work and when it will not. (Let's remember our formalization of the problem in Chapter 1, Equation 1.2: min x1,..., xp, x-p \u2211 i = 1 E [f (xi, \u0432i)] + \u03c1 2 + xi \u2212 x 2, where we refer to xi's as a local variable and we refer to x-p as a mean variable. If f (xi, xi) = 14 (1 \u2212 (xi) 2) 2, which is deterministic (independent of i) and one-dimensional, we see that f has two minimal xi = 1 and xi = \u2212 1 \u00b0. It also has a saddle point at xi = 0. We wonder how big it should be that the xi's will have a common minimum. Let's say that p = 2, let's see if we can x1 = \u2212 1 = \u2212 x2 = a stable solution."}, {"heading": "Scaling up Elastic Averaging SGD", "text": "This chapter describes how the EASGD method can be extended to hundreds and thousands of processors. In Section 6.1, we first propose a tree-structured extension of the EASGD method called the EASGD Tree. The basic idea and construction principle are discussed in Section 6.1.1, and the numerical results are presented in Section 6.1.2. We present two different communication programs for the EASGD method, seeing the advantage of the EAMSGD reformulation, we also accelerate the EASGD tree using Nesterov's dynamics method. In Section 6.2, we combine the DONWPOUR method and the EASGD method by looking at a Gauss-Seidel reformulation of the EASGD rules in the synchronous scenario. This unification suggests that both the use of the DONWPOUR and the EASGD structure are different."}, {"heading": "6.1.1 The algorithm", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6.1.2 The result", "text": "In this section, we demonstrate empirically the advantage and challenge of expanding the EASGD tree to a few hundred CPU cores. As we are limited by the number of available 117GPUs \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 all results are executed on CPUs. In addition, the setup is different from our previous results in Section 4.1. We can examine the results within 12 hours using a Convolutionary Neural Network Model (1.10) based on the CIFAR-10 dataset, simulating the long-term behavior of the algorithm within 12 hours. We do not use data augmentation as we have done in Chapter 4 because it requires additional CPUs to process the input data. Each data point is scanned with substitutes (unlike Section 4.1)."}, {"heading": "Conclusion", "text": "This year, it has come to the point where it is only a matter of time before it is ready, until it is ready, until it is ready."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["A. Agarwal", "O. Chapelle", "M. Dud\u0301\u0131k", "J. Langford"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In NIPS, pages 873\u2013881,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Katyusha: Accelerated variance reduction for faster sgd", "author": ["Z. Allen-Zhu"], "venue": "arXiv preprint arXiv:1603.05953,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Towards an optimal stochastic alternating direction method of multipliers", "author": ["S. Azadi", "S. Sra"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Scaling up machine learning: Parallel and distributed approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A new class of incremental gradient methods for least squares problems", "author": ["D.P. Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Parallel and Distributed Computation", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Asynchronous stochastic approximations", "author": ["V. Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Curiously fast convergence of some stochastic gradient descent algorithms. Unpublished open problem offered to the attendance of the SLDS", "author": ["L. Bottou"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M.B. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed deep learning using synchronous stochastic gradient descent", "author": ["D. Das", "S. Avancha", "D. Mudigere", "K. Vaidynathan", "S. Sridharan", "D. Kalamkar", "B. Kaul", "P. Dubey"], "venue": "arXiv preprint arXiv:1602.06709,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "First-order methods of smooth convex optimization with inexact oracle", "author": ["O. Devolder", "F. Glineur", "Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "Automatic control, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Distributed deep learning for answer selection", "author": ["M. Feng", "B. Xiang", "B. Zhou"], "venue": "CoRR, abs/1511.01158,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Global convergence of the heavy-ball method for convex optimization", "author": ["E. Ghadimi", "H.R. Feyzmahdavian", "M. Johansson"], "venue": "In Control Conference (ECC), 2015 European,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Optimization theory: the finite dimensional case", "author": ["M.R. Hestenes"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1975}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Deepspark: Spark-based deep learning supporting asynchronous updates and caffe compatibility", "author": ["H. Kim", "J. Park", "J. Jang", "S. Yoon"], "venue": "arXiv preprint arXiv:1602.08191,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "An optimal method for stochastic composite optimization", "author": ["G. Lan"], "venue": "Mathematical Programming,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Slow learners are fast", "author": ["J. Langford", "A. Smola", "M. Zinkevich"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Dynamics of stochastic gradient algorithms", "author": ["Q. Li", "C. Tai", "W. E"], "venue": "CoRR, abs/1511.06251,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A universal catalyst for first-order optimization", "author": ["H. Lin", "J. Mairal", "Z. Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Sparknet: Training deep networks in spark", "author": ["P. Moritz", "R. Nishihara", "I. Stoica", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1511.06051,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Distributed asynchronous incremental subgradient methods", "author": ["A. Nedi\u0107", "D. Bertsekas", "V. Borkar"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Cooperative distributed multi-agent optimization", "author": ["A. Nedi", "A. Ozdaglar"], "venue": "Convex Optimization in Signal Processing and Communications,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Math. Program.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Numerical Optimization, Second Edition", "author": ["J. Nocedal", "S. Wright"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}, {"title": "Convergence speed in distributed consensus and averaging", "author": ["A. Olshevsky", "J.N. Tsitsiklis"], "venue": "SIAM review,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Gpu asynchronous stochastic gradient descent to speed up neural network training", "author": ["T. Paine", "H. Jin", "J. Yang", "Z. Lin", "T. Huang"], "venue": "arXiv preprint arXiv:1312.6186,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Introduction to optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software New York,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1987}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1992}, {"title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Iterative methods for sparse linear systems", "author": ["Y. Saad"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2003}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "In Interspeech", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR2014),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "author": ["O. Shamir"], "venue": "In NIPS,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Oscillation helps to get division right", "author": ["D.J. Sherratt"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "An efficient implementation of vector clocks", "author": ["M. Singhal", "A. Kshemkalyani"], "venue": "Information Processing Letters,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1992}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["C. Tai", "T. Xiao", "X. Wang", "W. E"], "venue": "In ICLR,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans"], "venue": "American Control Conference,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1984}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "Distributed average consensus with least-meansquare deviation", "author": ["L. Xiao", "S. Boyd", "S.-J. Kim"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2007}, {"title": "Multi-gpu training of convnets", "author": ["O. Yadan", "K. Adams", "Y. Taigman", "M. Ranzato"], "venue": "In Arxiv,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Asynchronous distributed admm for consensus optimization", "author": ["R. Zhang", "J. Kwok"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Staleness-aware async-sgd for distributed deep learning", "author": ["W. Zhang", "S. Gupta", "X. Lian", "J. Liu"], "venue": "arXiv preprint arXiv:1511.05950,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A. Smola", "L. Li"], "venue": "In NIPS,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 2) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 8, "context": "The subject of this thesis is on how to parallelize the training of large deep learning models that use a form of stochastic gradient descent (SGD) [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 29, "context": "As an optimization method, it often exhibits fast initial convergence toward the local optimum as compared to the batch gradient method [30].", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 21, "context": "However, as the size of the dataset explodes [22], the amount of time taken to go through each data point sequentially becomes prohibitive.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "There have been attempts to parallelize the training for large-scale deep learning models on thousands of CPUs, including the Google\u2019s Distbelief system [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "neural networks trained on a few GPU cards sitting in a single computer [27, 49].", "startOffset": 72, "endOffset": 80}, {"referenceID": 48, "context": "neural networks trained on a few GPU cards sitting in a single computer [27, 49].", "startOffset": 72, "endOffset": 80}, {"referenceID": 51, "context": "To date, the AlphaGo system is trained using 50 GPUs for a few weeks [52].", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "To solve such large-scale optimization problem, one line of research is to fill-in the gap between the stochastic gradient descent method (SGD) and the batch gradient method [40].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 16, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 24, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 30, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 24, "context": "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 14, "context": "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 61, "context": "It is even observed that in deep learning problems, using too large mini-batch size may lead to solutions of very poor test accuracy [62].", "startOffset": 133, "endOffset": 137}, {"referenceID": 7, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 28, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 62, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 1, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 45, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 15, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 61, "context": "The tradeoff is that asynchronous behavior results in large communication delay, which can in turn slow down the convergence rate [62].", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "The DOWNPOUR method belongs to the above class of asynchronous SGD methods, and is proposed for training deep learning models [16] .", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "This idea resembles the incremental gradient method [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 33, "context": "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].", "startOffset": 140, "endOffset": 148}, {"referenceID": 25, "context": "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].", "startOffset": 140, "endOffset": 148}, {"referenceID": 56, "context": "It is based on the idea of consensus averaging [57].", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "clock synchronization) [41].", "startOffset": 23, "endOffset": 27}, {"referenceID": 58, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 36, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 176, "endOffset": 184}, {"referenceID": 18, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 176, "endOffset": 184}, {"referenceID": 10, "context": "The ADMM (Alternating Direction Method of Multipliers) [11] method can also be used to solve the consensus averaging problem above.", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].", "startOffset": 122, "endOffset": 130}, {"referenceID": 60, "context": "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].", "startOffset": 122, "endOffset": 130}, {"referenceID": 6, "context": "2 Formalizing the problem Consider minimizing a function F (x) in a parallel computing environment [7] with p \u2208 N workers and a master.", "startOffset": 99, "endOffset": 102}, {"referenceID": 23, "context": "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].", "startOffset": 160, "endOffset": 168}, {"referenceID": 49, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 15, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 59, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 42, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 47, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 6, "context": "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.", "startOffset": 47, "endOffset": 53}, {"referenceID": 4, "context": "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.", "startOffset": 47, "endOffset": 53}, {"referenceID": 12, "context": "We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 32, "context": "2 and the Moreau-Yosida regularization [33] in convex optimization used to smooth the non-smooth part of the objective function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "When using the rectified linear units [35] in deep learning models, the objective function also becomes non-smooth.", "startOffset": 38, "endOffset": 42}, {"referenceID": 39, "context": "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].", "startOffset": 51, "endOffset": 55}, {"referenceID": 44, "context": "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Thus in order to minimize the staleness [25] of the difference xt\u2212x\u0303t between the center and the local variable for the asynchronous EASGD (described in Algorithm 1), the update for the master in Equation 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "4) as a Jacobi method [47].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 3, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 7, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 35, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 28, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 1, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 45, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 62, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 6, "context": "Note that the asynchronous behavior described above is partially asynchronous [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 38, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 27, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 54, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 43, "context": "In literature, there\u2019s another well-known momentum variant called heavy-ball method (aka Polyak \u2019s method) [44].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "The analysis of its global convergence property is still a very challenging problem in convex optimization literature [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "One reason is that the momentum method has an error accumulation effect [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 31, "context": "Due to the stochastic noise in the gradient, using momentum can actually result in higher asymptotic variance (see [32] and our discussion in Section 5.", "startOffset": 115, "endOffset": 119}, {"referenceID": 44, "context": "Our analysis in the quadratic case extends the analysis of ASGD in [45].", "startOffset": 67, "endOffset": 71}, {"referenceID": 44, "context": "2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [45]) {z1, z2, .", "startOffset": 141, "endOffset": 145}, {"referenceID": 44, "context": "Note that this linear system has a degenerate noise \u039et which prevents us from directly applying results of [45].", "startOffset": 107, "endOffset": 111}, {"referenceID": 37, "context": "We start from the following estimate for the strongly convex function [38], \u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2265 \u03bcL \u03bc+ L \u2016x\u2212 y\u2016 + 1 \u03bc+ L \u2016\u2207F (x)\u2212\u2207F (y)\u2016 .", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "In this section we study the stability of the asynchronous EASGD and ADMM methods in the round-robin scheme [29].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 60, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 41, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 41, "context": "At each t, we linearize the function F (xi) with F (xt)+ \u3008 \u2207F (xt), xi \u2212 xt \u3009 + 1 2\u03b7 \u2225\u2225xi \u2212 xit\u2225\u22252 as in [42].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 60, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 41, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 60, "context": "The convergence analysis in [61] is based on the assumption that \u201cAt any master iteration, updates from the workers have the same probability of arriving at the master.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "All the parallel comparator methods are listed below1: \u2022 DOWNPOUR [16], the detail and the pseudo-code of the implementation are described in Section 4.", "startOffset": 66, "endOffset": 70}, {"referenceID": 60, "context": "\u2022 A method that we call MVADOWNPOUR, where we compute the moving average We have compared asynchronous ADMM [61] with EASGD in our setting as well, the performance is nearly the same.", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "All the sequential comparator methods (p = 1) are listed below: \u2022 SGD [9] with constant learning rate \u03b7.", "startOffset": 70, "endOffset": 73}, {"referenceID": 54, "context": "\u2022 Momentum SGD (MSGD) [55] with constant (Nesterov\u2019s) momentum rate \u03b4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "\u2022 ASGD [45] with moving rate \u03b1t+1 = 1 t+1 .", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "\u2022 MVASGD [45] with moving rate \u03b1 set to a constant.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "The center variable of the master is stored and updated on the centralized parameter server [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 34, "context": "[35]), P denotes the max pooling operator, L denotes the linear operator and D denotes the dropout operator with rate equal to 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "For the ImageNet experiment we use the similar approach to [49] with the following 11-layer convolutional neural network: (3, 221, 221) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (7,7,2,2) (96, 108, 108) P \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,3,3) (96, 36, 36) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (256, 32, 32) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 16, 16) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (384, 14, 14) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,1,1) (384, 13, 13) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,1,1) (256, 12, 12) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 6, 6) L,R,D \u2212\u2212\u2212\u2212\u2192 0.", "startOffset": 59, "endOffset": 63}, {"referenceID": 57, "context": "For the CIFAR experiment we use the similar approach to [58] with the following 7-layer convolutional neural network: (3, 28, 28) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (64, 24, 24) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (64, 12, 12) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (128, 8, 8) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (128, 4, 4) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (64, 2, 2) L,R,D \u2212\u2212\u2212\u2212\u2192 0.", "startOffset": 56, "endOffset": 60}, {"referenceID": 53, "context": "5 [54].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "We also re-scale each pixel value to the interval [0, 1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "As before, we re-scale each pixel value to the interval [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 9, "context": "Notice that each data loader cycles5 through the Its advantage is observed in [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "We remark that if the stochastic gradient is sparse, DOWNPOUR empirically performs well with large communication period [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Thus our initial learning rate is decreased twice over time, by a factor of 5 and then 2, when we observe that the online predictive loss [12] stagnates.", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Notice that we do not use any adaptive learning scheme as having been done in [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "2 by an Ornstein Uhlenbeck process [32] as follows,", "startOffset": 35, "endOffset": 39}, {"referenceID": 43, "context": "In case that \u03b4h is chosen independently of h, the update is no different to the heavy ball method [44].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "Moreover, as in [32], we can try to minimize |z3| with respect to \u03b4 such that the rate of convergence of the second order moment is maximized for a given \u03b7.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "\u03b4 \u2192 1 [32].", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "This is consistent with the choice of the learning rate and the momentum rate scheduling in the Nesterov\u2019s optimal methods in literature [28].", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "24 by a Geometric Brownian motion [32] as follows,", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 2) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].", "startOffset": 89, "endOffset": 96}, {"referenceID": 61, "context": "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].", "startOffset": 89, "endOffset": 96}, {"referenceID": 55, "context": "We report the results based on CIFAR-10 dataset using a low-rank convolutional neural-network model [56].", "startOffset": 100, "endOffset": 104}, {"referenceID": 55, "context": "As in [56], we center the input pixel values by the mean and the standard deviation for each of the three channels using the training data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "\u2022 The connection between EAMSGD and Katyusha [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 56, "context": "The basic tool [57] based on the mean-field method (i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "\u2022 The description of the asynchronous behavior using vector-clock [53].", "startOffset": 66, "endOffset": 70}, {"referenceID": 53, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 28, "endOffset": 32}, {"referenceID": 57, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 49, "endOffset": 53}, {"referenceID": 50, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods.", "creator": "LaTeX with hyperref package"}}}