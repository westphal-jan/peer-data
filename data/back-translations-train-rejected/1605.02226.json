{"id": "1605.02226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Neural Autoregressive Distribution Estimation", "abstract": "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neu- ral network architectures applied to the problem of unsupervised distribution and density esitmation. They leverage the probability product rule and a weight sharing scheme in- spired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive per- formance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.", "histories": [["v1", "Sat, 7 May 2016 18:13:25 GMT  (3523kb,D)", "https://arxiv.org/abs/1605.02226v1", null], ["v2", "Wed, 11 May 2016 12:00:17 GMT  (3523kb,D)", "http://arxiv.org/abs/1605.02226v2", null], ["v3", "Fri, 27 May 2016 14:25:41 GMT  (3518kb,D)", "http://arxiv.org/abs/1605.02226v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benigno uria", "marc-alexandre c\\^ot\\'e", "karol gregor", "iain murray", "hugo larochelle"], "accepted": false, "id": "1605.02226"}, "pdf": {"name": "1605.02226.pdf", "metadata": {"source": "CRF", "title": "Neural Autoregressive Distribution Estimation", "authors": ["Benigno Uria", "Iain Murray"], "emails": ["benigno.uria@gmail.com", "marc-alexandre.cote@usherbrooke.ca", "karol.gregor@gmail.com", "i.murray@ed.ac.uk", "hlarochelle@twitter.com"], "sections": [{"heading": null, "text": "Keywords: deep learning, neural networks, density modelling, unsupervised learning"}, {"heading": "1. Introduction", "text": "From a good and flexible distribution estimator, it is in principle possible to solve a variety of types of consequential problems, such as classification, regression, lack of value imputation and many other predictive tasks. Currently, one of the most common forms of distribution estimation is based on directed graphical models. In general, these models describe the process of data generation as sampling a latent state h from some previous p (h), followed by sampling the observed data x from some \u00a9 2000 Benigno Uria, Marc-Alexandre Co., Carol Gregor, Iain Murray, Hugo Larochelle.ar Xiv: 160 5.02 22conditional p (x | h), followed by sampling the observed data x from some \u00a9 2000 Benigno Uria, Marc-Alexandre Co., whose authority models h, Karol Gregor, Iain Murray, Hugo Larochelle.ar Xiv: 160 5.02 22conditional p (x | h). Unfortunately, this approach becomes intractable and requires increasing complexity."}, {"heading": "2. NADE", "text": "We consider the problem of modeling the distribution p (x) of the input vector observations x. < p (1) p (0, 1) p (0, 1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (2) p (2) p (1) p) p (xo < d) p (1) p (2) p) p (2) p (1) p) p (1) p (1) p) p (xo < d) d) p (1) p (1) p) p (2) p (2) p (2) p (2) p) p (1) p)."}, {"heading": "2.1 Relationship with the RBM", "text": "The proposed weight for NADE is not simply motivated by mathematical reasons; it also reflects the calculations of the approximation conclusion in the RBM.The calculation of the energy function and distribution under an RBM asE (x, h) = - h > Wx \u2212 b > x \u2212 c > h (7) p (x, h) = exp {\u2212 E (x, h) / Z, (8) the calculation of all conditions p (xo < d) = - x < d) = - xo > d (0,1} D \u2212 d \u00b2 - d (0,1) H exp {\u2212 E (x, h)} / Z (xo < d) - (xo < d) = - xo < D \u2212 d + 1} D \u2212 d + 1 - d (0,1} H exp {lt."}, {"heading": "3. NADE for non-binary observations", "text": "So far, we have considered only the case of binary observations xi. However, the framework of NADE naturally extends to distributions over other types of observations. In the next section, we will discuss the case of real observations, which is one of the most general cases of non-binary observations and is a vivid example of the technical considerations that one faces when extending NADE to new observations."}, {"heading": "3.1 RNADE: Real-valued NADE", "text": "The resulting neural network would mean a Gaussian fixed variance mesh for each of the models in Equation 1. Such a model is not competitive with mixing models (e.g. on perceptual datasets (Uria, 2015). However, we can explore alternative models by outputting the neural network for each conditional distribution as parameters of a distribution that does not have a fixed Gaussian variance. Specifically, a mix of one-dimensional Gaussians can provide a flexible model for each autoregressive condition by outputting the parameters of a distribution that does not have a fixed Gaussian variance. Specifically, a mix of one-dimensional Gaussians can provide a flexible model for each autoregressive condition.Given sufficient components, a mix of Gaussians can model any continuous distribution to arbitrary precision, and the resulting model can be interpreted as a result of mixing density networks (Bishop, 1994) with common parameters."}, {"heading": "4. Orderless and Deep NADE", "text": "The fixed order of variables in a NADE model performs the exact calculation of arbitrary conditional probabilities = number of complex units. < < # # # # # # # # # # # # # # 160; # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4.1 Ensembles of NADE models", "text": "As already mentioned, the DeepNADE method of parameter adjustment effectively generates a factorial number of different NADE models, one for each arrangement of variables, and these models will generally not assign the same probability to a particular data point. However, this discrepancy is undesirable when we need consistent conclusions for different derivative problems, as they exclude the use of the most convenient arrangement of variables for each inference task. However, it is possible to use this variability to our advantage across the various arrangements by combining several models. A common approach to improving a particular estimator is to construct an interaction of several strong but different estimators, e.g. by bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999). The DeepNADE training method suggests a method for generating ensembles of NADE models: Make a series of evenly distributed arrangements {o)."}, {"heading": "5. ConvNADE: Convolutional NADE", "text": "One disadvantage of the NADE (and its variants until now) is the lack of a mechanism for fully exploiting the high-dimensional structure of the data. (W) \"We need to flatten out the 2D images before we make them available as a vector.\" (W) Spatial topology is not integrated into the network, it cannot use this information to divide parameters and can learn less quickly. (W) \"We have the ability to focus on many higher-level tasks related to the images Krizhevsky et al.\" (2012) In short, CNNs are composed of evolutionary layers, each with multiple learnable filters. The outputs of a revolutionary layer are feature maps and are achieved by the evolution on the input image (or previous feature maps) of a linear filter."}, {"heading": "6. Related Work", "text": "As we have already mentioned, the development of the NADE and its extensions was motivated by the question of whether a tractable distribution estimator could be developed to provide a powerful but unruly model such as the restricted Boltzmann machine. The original inspiration came from the auto-regressive approach taken by fully visible networks of sigmoid faith (FVSBN), which Frey et al. (1996) described as surprisingly competitive, despite the simplicity of the distribution family for their conditions. Bengio and Bengio (2000) later proposed to use more powerful conditional networks modelled as single-layer neural networks. Furthermore, they proposed to connect the output of each dth model to all the hidden layers of d \u2212 1 neural networks for their conditions."}, {"heading": "7. Results", "text": "In this section, we evaluate the performance of our various NADE models on a variety of datasets."}, {"heading": "7.1 Binary vectors datasets", "text": "We begin by evaluating the performance of NADE models on a set of benchmark datasets in which the observations correspond only to binary vectors. These datasets were mostly taken from the LIBSVM datasets of the website1, with the exception of OCR-letters2 and NIPS-12 3. Code to download these datasets is available here: http: / / info.usherbrooke.ca / hlarochelle / code / nade.gz. Table 1 summarizes the most important statistics for these datasets.For these experiments, we consider only tractable distribution estimators, where we can accurately evaluate p (x) for test elements. We consider the following baselines: A mixture of multivariate Bernoullis, trained using EM algorithms. The number of mixing components was selected from {32, 64, 128, 256} based on performance validation, and early stop was used to determine the number of algorithms used."}, {"heading": "7.2 Binary image dataset", "text": "We consider the case of an image data set constructed by binarizing MNIST digit data sets = < < < < < < < p (2008) This yardstick is a popular choice for evaluating generative neural network models. Here we examine two questions: 1. How does NADE compare with other intractable generative models? 2. Does the use of a revolutionary architecture improve the performance of NADE? In addition to the baselines already described in Section 7.1, we consider the following: \u2022 DARN (Gregor et al., 2014): This deep generative autoencoder has two hidden layers, one deterministic and one binary stochastic. Both layers have 500 units (referred to as nh = 500). Adaptive weight noise (adaNoise) was either used or not used to avoid the need for an early stop (Graves, 2011)."}, {"heading": "7.3.1 Low-dimensional data", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7.3.2 Natural image patches", "text": "In fact, most of us are able to play by the rules we set ourselves, \"he said in an interview with The New York Times."}, {"heading": "7.3.3 Speech acoustics", "text": "We have measured the ability of RNADE to model small patches of speech spectrograms extracted from the TIMIT dataset (Garofolo et al., 1993). The patches contained 11 frames of 20 filter banks plus energy; a total of 231 dimensions per datapoint. A good generative model of speech acoustics could be used, for example, in denociation or speech recognition. We have equipped the models with the standard TIMIT training environment, which includes recordings of 605 speakers of American English. We compare RNADE with a mixture of Gaussians by measuring their log probability on the full TIMIT Core test dataset. A held set of 25 speakers contains recordings. The RNADE models have 512 hidden units, ReLU activations and a mixture of 20 one-dimensional Gaussian components per output."}, {"heading": "8. Conclusion", "text": "We have described the Neural Autoregressive Distribution Estimator, a tractable, flexible, and competitive alternative to directed and undirected graphical models for estimating unattended distribution. Since the publication of the first formulation of NADE (Larochelle and Murray, 2011), it has been extended to many more settings than described in this paper. Larochelle and Lauly (2012); Zheng et al. (2015b) adapted NADE for thematic modeling of documents and images, while Boulanger-Lewandowski et al. (2012) used NADE for modeling of music sequence data. Theis and Bethge (2015) and Oord et al. (2016) suggested different NADE models for images than the one we presented, applied to natural images and based on evolutionary and LSTM-hidden units. Zheng et al. (2015a) used a NADE model to provide an attention-effectiveness mechanism for images that can be incorporated into a number of benchmark and flexible models."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Yoshua Bengio", "Samy Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio and Bengio.,? \\Q2000\\E", "shortCiteRegEx": "Bengio and Bengio.", "year": 2000}, {"title": "Statistical analysis of non-lattice data", "author": ["Julian Besag"], "venue": "The Statistician,", "citeRegEx": "Besag.,? \\Q1975\\E", "shortCiteRegEx": "Besag.", "year": 1975}, {"title": "Mixture density networks", "author": ["Christopher M. Bishop"], "venue": "Technical Report NCRG 4288,", "citeRegEx": "Bishop.,? \\Q1994\\E", "shortCiteRegEx": "Bishop.", "year": 1994}, {"title": "Reweighted wake-sleep", "author": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bornschein and Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bornschein and Bengio.", "year": 2015}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Parallel tempering is efficient for learning restricted Boltzmann machines", "author": ["KyungHyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "In Proceedings of the International Joint Conference on Neural Networks", "citeRegEx": "Cho et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "Enhanced gradient for training restricted Boltzmann machines", "author": ["KyungHyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "Neural Computation,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow and Liu.,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu.", "year": 1968}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "The Helmholtz machine", "author": ["Peter Dayan", "Geoffrey E. Hinton", "Radford M. Neal", "Richard S. Zemel"], "venue": "Neural Computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Tempered Markov chain Monte Carlo for training of restricted Boltzmann machine", "author": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio", "Pascal Vincent", "Olivier Delalleau"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Yoav Freund", "David Haussler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Freund and Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1992}, {"title": "Does the wake-sleep algorithm learn good density estimators", "author": ["Brendan J. Frey", "Geoffrey E. Hinton", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frey et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Frey et al\\.", "year": 1996}, {"title": "DARPA TIMIT acoustic-phonetic continuous speech corpus", "author": ["J. Garofolo", "L. Lamel", "W. Fisher", "J. Fiscus", "D. Pallett", "N. Dahlgren", "V. Zue"], "venue": "CD-ROM. NIST,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "MADE: Masked autoencoder for distribution estimation", "author": ["Mathieu Germain", "Karol Gregor", "Iain Murray", "Hugo Larochelle"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "The EM algorithm for mixtures of factor analyzers", "author": ["Zoubin Ghahramani", "Geoffrey E. Hinton"], "venue": "Technical Report CRG-TR-96-1, University of Toronto,", "citeRegEx": "Ghahramani and Hinton.,? \\Q1996\\E", "shortCiteRegEx": "Ghahramani and Hinton.", "year": 1996}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Andriy Mnih", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "DRAW: a recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A kernel method for the two-sample-problem", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte Rasch", "Bernhard Sch\u00f6lkopf", "Alex J. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Greedy learning of binary latent trees", "author": ["Stefan Harmeling", "Christopher K.I. Williams"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Harmeling and Williams.,? \\Q2011\\E", "shortCiteRegEx": "Harmeling and Williams.", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["Geoffrey E. Hinton", "Peter Dayan", "Brendan J. Frey", "Radford M. Neal"], "venue": "Science, 268:1161\u20131558,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2005}, {"title": "Some extensions of score matching", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables", "author": ["Aapo Hyv\u00e4rinen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "Adam: a method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard S. Zemel"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Inductive principles for restricted Boltzmann machine learning", "author": ["Benjamin Marlin", "Kevin Swersky", "Bo Chen", "Nando de Freitas"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Deep Boltzmann machines and the centering trick", "author": ["Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, Second Edition,", "citeRegEx": "Montavon and M\u00fcller.,? \\Q2012\\E", "shortCiteRegEx": "Montavon and M\u00fcller.", "year": 2012}, {"title": "Connectionist learning of belief networks", "author": ["Radford M. Neal"], "venue": "Artificial Intelligence,", "citeRegEx": "Neal.,? \\Q1992\\E", "shortCiteRegEx": "Neal.", "year": 1992}, {"title": "Learning deep energy models", "author": ["Jiquan Ngiam", "Zhenghao Chen", "Pang Wei Koh", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron Van Den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "Proceedings of the 33rd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Improved Gaussian mixture density estimates using Bayesian penalty terms and network averaging", "author": ["Dirk Ormoneit", "Volker Tresp"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ormoneit and Tresp.,? \\Q1995\\E", "shortCiteRegEx": "Ormoneit and Tresp.", "year": 1995}, {"title": "Iterative neural autoregressive distribution estimator (NADE-k)", "author": ["Tapani Raiko", "Li Yao", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2009}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["Ruslan Salakhutdinov"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2010}, {"title": "Deep Boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Efficient learning of deep Boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Salakhutdinov and Larochelle.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Larochelle.", "year": 2010}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Mixed cumulative distribution networks", "author": ["Ricardo Silva", "Charles Blundell", "Yee Whye Teh"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "Parallel Distributed Processing: Volume 1: Foundations,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Linearly combining density estimators via stacking", "author": ["Padhraic Smyth", "David Wolpert"], "venue": "Machine Learning,", "citeRegEx": "Smyth and Wolpert.,? \\Q1999\\E", "shortCiteRegEx": "Smyth and Wolpert.", "year": 1999}, {"title": "Minimum probability flow learning", "author": ["Jascha Sohl-Dickstein", "Peter Battaglino", "Michael R. DeWeese"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2011}, {"title": "Striving for simplicity: the all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Deep mixtures of factor analysers", "author": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Tang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2012}, {"title": "Generative image modeling using spatial lstms", "author": ["Lucas Theis", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Theis and Bethge.,? \\Q2015\\E", "shortCiteRegEx": "Theis and Bethge.", "year": 2015}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["Tijmen Tieleman"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Tieleman.,? \\Q2008\\E", "shortCiteRegEx": "Tieleman.", "year": 2008}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["Tijmen Tieleman", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2009}, {"title": "Connectionist multivariate density-estimation and its application to speech synthesis", "author": ["Benigno Uria"], "venue": "PhD thesis, The University of Edinburgh,", "citeRegEx": "Uria.,? \\Q2015\\E", "shortCiteRegEx": "Uria.", "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Mixture of factor analyzers", "author": ["Jakob Verbeek"], "venue": "Matlab implementation,", "citeRegEx": "Verbeek.,? \\Q2005\\E", "shortCiteRegEx": "Verbeek.", "year": 2005}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["Max Welling", "Michal Rosen-Zvi", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Welling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2005}, {"title": "Parameter inference for imperfectly observed Gibbsian fields", "author": ["Laurent Younes"], "venue": "Probability Theory Related Fields,", "citeRegEx": "Younes.,? \\Q1989\\E", "shortCiteRegEx": "Younes.", "year": 1989}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Yin Zheng", "Richard S. Zemel", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A deep and autoregressive approach for topic modeling of multimodal data", "author": ["Yin Zheng", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["Daniel Zoran", "Yair Weiss"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Zoran and Weiss.,? \\Q2011\\E", "shortCiteRegEx": "Zoran and Weiss.", "year": 2011}, {"title": "Natural images, Gaussian mixtures and dead leaves", "author": ["Daniel Zoran", "Yair Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zoran and Weiss.,? \\Q2012\\E", "shortCiteRegEx": "Zoran and Weiss.", "year": 2012}], "referenceMentions": [{"referenceID": 51, "context": "A popular choice for such a model is the restricted Boltzmann machine (RBM), which substantially out-performs mixture models on a variety of binary datasets (Salakhutdinov and Murray, 2008).", "startOffset": 157, "endOffset": 189}, {"referenceID": 13, "context": "Frey et al. (1996) followed this approach and proposed using simple (log-)linear logistic regression models for these conditionals.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Bengio and Bengio (2000) proposed a more flexible approach, with a single-layer feed-forward neural network for each conditional.", "startOffset": 0, "endOffset": 25}, {"referenceID": 45, "context": "A generalization of NADE based on this connection to mean field inference has been further explored by Raiko et al. (2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 65, "context": "1 to the Gaussian-RBM (Welling et al., 2005).", "startOffset": 22, "endOffset": 44}, {"referenceID": 61, "context": "Such a model is not competitive with mixture models, for example on perceptual datasets (Uria, 2015).", "startOffset": 88, "endOffset": 100}, {"referenceID": 3, "context": "The resulting model can be interpreted as a sequence of mixture density networks (Bishop, 1994) with shared parameters.", "startOffset": 81, "endOffset": 95}, {"referenceID": 62, "context": "It was found empirically (Uria et al., 2013) that stochastic gradient descent leads to better parameter configurations when the gradient of the mean ( \u2202J \u2202\u03bcod,c ) was multiplied by the standard deviation (\u03c3od,c).", "startOffset": 25, "endOffset": 44}, {"referenceID": 61, "context": "Other choices, like single variable-variance Gaussians, sinh-arcsinh distributions, and mixtures of Laplace distributions, have been examined by Uria (2015). Training an RNADE can still be done by stochastic gradient descent on the parameters of the model with respect to the negative log-density of the training set.", "startOffset": 145, "endOffset": 157}, {"referenceID": 0, "context": "Deep neural networks (Bengio, 2009; LeCun et al., 2015) are at the core of state-of-the-art models for supervised tasks like image recognition (Krizhevsky et al.", "startOffset": 21, "endOffset": 55}, {"referenceID": 34, "context": ", 2015) are at the core of state-of-the-art models for supervised tasks like image recognition (Krizhevsky et al., 2012) and speech recognition (Dahl et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 10, "context": ", 2012) and speech recognition (Dahl et al., 2013).", "startOffset": 31, "endOffset": 50}, {"referenceID": 64, "context": "The resulting training procedure resembles that of a denoising autoencoder (Vincent et al., 2008).", "startOffset": 75, "endOffset": 97}, {"referenceID": 44, "context": "using bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999).", "startOffset": 14, "endOffset": 40}, {"referenceID": 54, "context": "using bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999).", "startOffset": 53, "endOffset": 78}, {"referenceID": 34, "context": "Recently, convolutional neural networks (CNN) have achieved state-of-the-art performance on many supervised tasks related to images Krizhevsky et al. (2012). Briefly, CNNs are composed of convolutional layers, each one having multiple learnable filters.", "startOffset": 132, "endOffset": 157}, {"referenceID": 56, "context": "In fact, recent research suggests that these types of layers are not essential to obtain state-of-the-art results (Springenberg et al., 2015).", "startOffset": 114, "endOffset": 141}, {"referenceID": 13, "context": "The original inspiration came from the autoregressive approach taken by fully visible sigmoid belief networks (FVSBN), which were shown by Frey et al. (1996) to be surprisingly competitive, despite the simplicity of the distribution family for its conditionals.", "startOffset": 139, "endOffset": 158}, {"referenceID": 0, "context": "Bengio and Bengio (2000) later proposed using more powerful conditionals, modeled as single layer neural networks.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Bengio and Bengio (2000) later proposed using more powerful conditionals, modeled as single layer neural networks. Moreover, they proposed connecting the output of each dth conditional to all of the hidden layers of the d\u2212 1 neural networks for the preceding conditionals. More recently, Germain et al. (2015) generalized this model by deriving a simple procedure for making it deep and orderless (akin to DeepNADE, in Section 4).", "startOffset": 0, "endOffset": 310}, {"referenceID": 9, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968).", "startOffset": 154, "endOffset": 174}, {"referenceID": 41, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al.", "startOffset": 96, "endOffset": 108}, {"referenceID": 27, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995).", "startOffset": 135, "endOffset": 176}, {"referenceID": 11, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995).", "startOffset": 135, "endOffset": 176}, {"referenceID": 41, "context": "Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al.", "startOffset": 71, "endOffset": 83}, {"referenceID": 27, "context": "Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995).", "startOffset": 128, "endOffset": 149}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016).", "startOffset": 220, "endOffset": 240}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al.", "startOffset": 155, "endOffset": 1079}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al. (2014) proposed the variational autoencoder (VAE), where the model is the same as the Helmholtz machine, but with real-valued (usually Gaussian) stochastic units.", "startOffset": 155, "endOffset": 1102}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al. (2014) proposed the variational autoencoder (VAE), where the model is the same as the Helmholtz machine, but with real-valued (usually Gaussian) stochastic units. Importantly, Kingma and Welling (2014) identified a reparameterization trick making it possible to train the VAE in a way that resembles the training of an autoencoder.", "startOffset": 155, "endOffset": 1297}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016). Gregor et al. (2015) later exploited the VAE approach to develop DRAW, a directed generative model for images based on a read-write attentional mechanism.", "startOffset": 221, "endOffset": 263}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016). Gregor et al. (2015) later exploited the VAE approach to develop DRAW, a directed generative model for images based on a read-write attentional mechanism. Goodfellow et al. (2014) also proposed an adversarial approach to training directed generative networks, that relies on a discriminator network simultaneously trained to distinguish between data and model samples.", "startOffset": 221, "endOffset": 422}, {"referenceID": 23, "context": "(2015) instead propose a training objective derived from Maximum Mean Discrepancy (MMD; Gretton et al., 2007).", "startOffset": 82, "endOffset": 109}, {"referenceID": 12, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011).", "startOffset": 100, "endOffset": 150}, {"referenceID": 55, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011).", "startOffset": 100, "endOffset": 150}, {"referenceID": 53, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 47, "endOffset": 91}, {"referenceID": 14, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 47, "endOffset": 91}, {"referenceID": 49, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 149, "endOffset": 181}, {"referenceID": 36, "context": "Salakhutdinov and Murray (2008) provided one of the first quantitative evidence of the generative modeling power of RBMs, which motivated the original parameterization for NADE (Larochelle and Murray, 2011).", "startOffset": 177, "endOffset": 206}, {"referenceID": 26, "context": "The proposal of Contrastive Divergence (CD; Hinton, 2002) was instrumental in the popularization of the RBM.", "startOffset": 39, "endOffset": 57}, {"referenceID": 2, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 38, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 24, "context": ", 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 92, "endOffset": 121}, {"referenceID": 55, "context": ", 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al., 2011).", "startOffset": 156, "endOffset": 185}, {"referenceID": 66, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 59, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al.", "startOffset": 148, "endOffset": 164}, {"referenceID": 13, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 224, "endOffset": 294}, {"referenceID": 7, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 224, "endOffset": 294}, {"referenceID": 60, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 8, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 40, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 50, "context": ", 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 98, "endOffset": 134}, {"referenceID": 28, "context": "The most notable case is the Deep Belief Network (DBN; Hinton et al., 2006), which corresponds to a sigmoid belief network for which the prior over its top hidden layer is an RBM (whose hidden layer counts as an additional hidden layer).", "startOffset": 49, "endOffset": 75}, {"referenceID": 22, "context": "Li et al. (2015) instead propose a training objective derived from Maximum Mean Discrepancy (MMD; Gretton et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 9, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011). The undirected paradigm has also been explored extensively for developing powerful generative networks. These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks. Salakhutdinov and Murray (2008) provided one of the first quantitative evidence of the generative modeling power of RBMs, which motivated the original parameterization for NADE (Larochelle and Murray, 2011).", "startOffset": 101, "endOffset": 532}, {"referenceID": 2, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al., 2011). Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010). Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010). The work of Ngiam et al. (2011) also proposed an undirected model that distinguishes itself from deep Boltzmann machines by having deterministic hidden units, instead of stochastic.", "startOffset": 43, "endOffset": 877}, {"referenceID": 9, "context": "is an O(D2) fitting algorithm to find the maximum likelihood tree and conditional distributions (Chow and Liu, 1968).", "startOffset": 96, "endOffset": 116}, {"referenceID": 9, "context": "is an O(D2) fitting algorithm to find the maximum likelihood tree and conditional distributions (Chow and Liu, 1968). We adapted an implementation provided by Harmeling and Williams (2011), who found Chow\u2013Liu to be a strong baseline.", "startOffset": 97, "endOffset": 189}, {"referenceID": 17, "context": "\u2022 MADE (Germain et al., 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers.", "startOffset": 7, "endOffset": 29}, {"referenceID": 0, "context": ", 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers.", "startOffset": 58, "endOffset": 83}, {"referenceID": 0, "context": ", 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers. We consider a version using a single (fixed) input ordering and another trained on multiple orderings from which an ensemble was constructed (which was inspired from the order-agnostic approach of Section 4) that we refer to as MADE-E. See Germain et al. (2015) for more details.", "startOffset": 58, "endOffset": 366}, {"referenceID": 47, "context": "2 Binary image dataset We now consider the case of an image dataset, constructed by binarizing the MNIST digit dataset, as generated by Salakhutdinov and Murray (2008). This benchmark has been a popular choice for the evaluation of generative neural network models.", "startOffset": 136, "endOffset": 168}, {"referenceID": 21, "context": "1, we consider the following: \u2022 DARN (Gregor et al., 2014): This deep generative autoencoder has two hidden layers, one deterministic and one with binary stochastic units.", "startOffset": 37, "endOffset": 58}, {"referenceID": 20, "context": "Adaptive weight noise (adaNoise) was either used or not to avoid the need for early stopping (Graves, 2011).", "startOffset": 93, "endOffset": 107}, {"referenceID": 22, "context": "\u2022 DRAW (Gregor et al., 2015): Similar to a variational autoencoder where both the encoder and the decoder are LSTMs, guided (or not) by an attention mechanism.", "startOffset": 7, "endOffset": 28}, {"referenceID": 43, "context": "\u2022 Pixel RNN (Oord et al., 2016): NADE-like model for natural images that is based on convolutional and LSTM hidden units.", "startOffset": 12, "endOffset": 31}], "year": 2016, "abstractText": "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.", "creator": "LaTeX with hyperref package"}}}