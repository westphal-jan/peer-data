{"id": "1605.02276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2016", "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "abstract": "Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.", "histories": [["v1", "Sun, 8 May 2016 05:09:28 GMT  (23kb)", "https://arxiv.org/abs/1605.02276v1", null], ["v2", "Tue, 21 Jun 2016 04:48:34 GMT  (23kb)", "http://arxiv.org/abs/1605.02276v2", "The First Workshop on Evaluating Vector Space Representations for NLP"], ["v3", "Wed, 22 Jun 2016 02:41:04 GMT  (23kb)", "http://arxiv.org/abs/1605.02276v3", "The First Workshop on Evaluating Vector Space Representations for NLP"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "yulia tsvetkov", "pushpendre rastogi", "chris dyer"], "accepted": false, "id": "1605.02276"}, "pdf": {"name": "1605.02276.pdf", "metadata": {"source": "CRF", "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer"], "emails": ["mfaruqui@cs.cmu.edu,", "ytsvetko@cs.cmu.edu,", "cdyer@cs.cmu.edu,", "pushpendre@jhu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.02 276v 3 [cs.C L] 22 Jun 2016In the absence of standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors.The evaluation of word similarity, which correlates the distance between vectors and human judgments of \"semantic similarity,\" is attractive because it is mathematically inexpensive and fast. In this paper, we present several problems related to the evaluation of word vectors on word similarity records and summarize existing solutions. Our study suggests that the use of word similarity tasks for the evaluation of word vectors is unsustainable and requires further research on evaluation methods."}, {"heading": "1 Introduction", "text": "Despite the ubiquity of word vector representations in NLP, there is no consensus in the community as to what is the best way to evaluate word vectors. The most popular intrinsic evaluation task is word similarity evaluation. In the word similarity evaluation, a list of word similarity problems is provided along with their similarity evaluation (as judged by human commentators) to measure how well the concept of word similarity is grasped by humans through the word similarity problems. Table 1 shows some word pairs along with their similarity assessments from WS353 (Finkelstein et al., 2002), a popular word similarity data association of Dataset.Let a, b be two words, and a, b, RD, their corresponding word vectors in a D-dimensional vector space. Word similarity in the vector space variant is achieved by calculating cosmic similarity."}, {"heading": "2 Problems", "text": "We will now discuss the main issues by evaluating word vectors using word similarity tasks and present existing solutions (if available) to address them."}, {"heading": "2.1 Subjectivity of the task", "text": "The concept of word similarity is subjective and is often confused with kinship. For example, cup and coffee are related, but not comparable. Coffee refers to a plant (a living organism) or a hot brown drink, while cup is an artificial object containing liquids, often coffee. Nevertheless, cup and coffee are rated in WS-353 (Finkelstein et al., 2002) as more similar than pairs like cars and trains. Such anomalies are also found in recently constructed datasets like MEN (Bruni et al., 2012). Such datasets are therefore unfairly categorized 1. An alternative to correlation-based word similarity evaluation is the word analogy task, in which the task is to find the missing word b in the relationship: a is related to a like b, where a, a, a, a, a, and a, a, a dataset have the same relationship as a, a, a, a, and so on."}, {"heading": "2.2 Semantic or task-specific similarity?", "text": "Distributional word vector models capture one aspect of word equality statistics for words in a language (Levy and Goldberg, 2014b; Levy et al., 2015) and can therefore be considered evidence of Harris's (1954) distribution hypothesis to the extent that these models produce semantically coherent representations. Word embedding such as Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014), trained on word equality censorship, can therefore capture semantic word similarity and can therefore be evaluated on word similarity tasks. Word vector representations trained as part of a neural network to solve a specific task (apart from word similarity censorship) should be designated as such word similarity predictions (Collobert and Weston, 2008), and they are considered as specific word similarities, although they are not two specific word similarities."}, {"heading": "2.3 No standardized splits & overfitting", "text": "In order to obtain generalizable machine learning models, it is necessary to ensure that they do not fit too closely to a particular set of data (Manning and Sch\u00fctze, 1999). Existing word similarity data sets are therefore not divided into training, development and test sets. Therefore, the optimization of word vectors to better perform a word similarity task may be implicitly aligned with the test set and exceed the vectors of the task. However, if researchers decide to conduct their own data splits, the results obtained in different studies may be incomparable. In addition, the average number of word pairs in the word similarity data sets is small (about 781, see Table 2), and the subdivision of the data sets into smaller subgroups.We now present some of the solutions proposed by previous work to avoid word similarity datasets matching. These data sets are small (see Table 2)."}, {"heading": "2.4 Low correlation with extrinsic evaluation", "text": "The evaluation of word similarity measures how well the notion of human word similarity is captured in the word representations of the vector space. Word vectors that can capture word similarity may perform well in tasks that require an idea of explicit semantic similarity between words such as paraphrasing and dissociation. However, no strong correlation has been found between the performance of word vectors on word similarity and external evaluation tasks such as text classification, parsing, and mood analysis (Tsvetkov et al., 2015; 2Baseline vectors can be any standard vector models.Schnabel et al., 2015).3 The lack of a strong correlation between the evaluation of word similarity and downstream tasks requires alternative approaches to evaluation."}, {"heading": "2.5 Absence of statistical significance", "text": "The problem of statistical significance in the evaluation of word similarity was initially systematically addressed by Shalaby and Zadrozny (2015), with the results often leading to erroneous conclusions due to the instability of the optimizer (Clark et al., 2011).The problem of statistical significance in the evaluation of word similarity was initially systematically addressed by Shalaby and Zadrozny (2015), who used Steiger's test (Steiger, 1980) 4 to calculate how significant the difference between the rankings produced by two different models is against the gold ranking. However, their method must have explicit ranking of the words produced by the models, and cannot work if they are provided only with the correlation of each model with the gold ranking. This problem was solved by Rastogi et al (2015), which we nex.The significance of the small significance of Rastgi models was shown."}, {"heading": "2.6 Frequency effects in cosine similarity", "text": "The most common method of measuring the similarity between two words in vector space is to calculate the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unit length vectors (equivalent. 1). This prevents any distortion in favor of common words that are longer, since they are updated more frequently during training (Turian et al., 2010). Ideally, if the geometry of embedding space is driven primarily by semantics, the relatively small number of common words should be distributed evenly across the space, while a large number of rare words should group around related but more common words. However, vector spaces have been shown to contain hubs that are vectors close to a large number of other vectors in space (Radovanovic \u00e9t al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al., 2010). This problem manifests itself in word spaces that have a high degree of similarity with other words in terms of cosmic space."}, {"heading": "2.7 Inability to account for polysemy", "text": "Many words have more than one meaning in a language. For example, the word bank can correspond to either a financial institution or the country near a river. However, in WS-353, the bank receives an 8.5 / 10 similarity value to money, meaning that the bank is a financial institution. Such an assumption of meaning per word is widely used in many of the existing word similarity tasks, and it can erroneously punish a word vector model for capturing a particular meaning of the word that is missing from the word similarity task. To take into account the meaning-specific word similarity, Huang et al. (2012) introduced the Contextual Word similarity Data Set (SCWS), which is the task of calculating similarity between two words in which they occur. For example, the words bank and money should have little similarity."}, {"heading": "3 Conclusion", "text": "In this paper, we have identified problems related to the evaluation of word similarity in word vector models and reviewed existing solutions wherever possible. Our study suggests that the use of word similarity tasks to evaluate word vectors can lead to incorrect conclusions and requires further research into evaluation methods. Until a better solution is found for the intrinsic evaluation of word vectors, we propose a task-specific evaluation: Word vector models should be compared to how well they can function in a downstream NLP task. Although task-specific evaluation produces different rankings of word vector models for different tasks (Schnabel et al., 2015), this is not necessarily a problem because different vector models capture different types of information that may be more or less useful for a particular task."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proc. of NAACL", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "An unsupervised model for instance level subcategorization acquisition", "author": ["Baker et al.2014] Simon Baker", "Roi Reichart", "Anna Korhonen"], "venue": "In Proc. of EMNLP", "citeRegEx": "Baker et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "An empirical investigation of statistical significance in nlp", "author": ["David Burkett", "Dan Klein"], "venue": "In Proc. of EMNLP", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2012}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proc. of ACL", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Chris Dyer", "Alon Lavie", "Noah A Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Improving zero-shot learning by mitigating the hubness problem. arXiv preprint arXiv:1412.6568", "author": ["Dinu et al.2014] Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "venue": null, "citeRegEx": "Dinu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2014}, {"title": "Community evaluation and exchange of word vectors at wordvectors.org", "author": ["Faruqui", "Dyer2014a] Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. of ACL: System Demo", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014b] Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. of EACL", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Halawi et al.2012] Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren"], "venue": "In Proc. of SIGKDD", "citeRegEx": "Halawi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Halawi et al\\.", "year": 2012}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proc. of ACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Chris Dyer", "Eduard Hovy"], "venue": "In Proc. NAACL", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "In Proc. of CoNLL", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014b] Omer Levy", "Yoav Goldberg"], "venue": "In Proc. of NIPS", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of NAACL", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of CoNLL", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proc. of NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991] George Miller", "Walter Charles"], "venue": "In Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proc. of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In Proc. of WWW", "citeRegEx": "Radinsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Hubs in space: Popular nearest neighbors in highdimensional data", "author": ["Alexandros Nanopoulos", "Mirjana Ivanovi\u0107"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Radovanovi\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Radovanovi\u0107 et al\\.", "year": 2010}, {"title": "Multiview lsa: Representation learning via generalized cca", "author": ["Benjamin Van Durme", "Raman Arora"], "venue": "In Proc. of NAACL", "citeRegEx": "Rastogi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Proc. of NAACL", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proc. of ACL", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proc. of EMNLP", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Measuring semantic relatedness using mined semantic analysis", "author": ["Shalaby", "Zadrozny2015] Walid Shalaby", "Wlodek Zadrozny"], "venue": null, "citeRegEx": "Shalaby et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shalaby et al\\.", "year": 2015}, {"title": "What\u2019s in a p-value in nlp", "author": ["Anders Johannsen", "Barbara Plank", "Dirk Hovy", "H\u00e9ctor Mart\u00ednez Alonso"], "venue": "In Proc. of CoNLL", "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2014}, {"title": "Tests for comparing elements of a correlation matrix", "author": ["James H Steiger"], "venue": "Psychological bulletin,", "citeRegEx": "Steiger.,? \\Q1980\\E", "shortCiteRegEx": "Steiger.", "year": 1980}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Stratos et al.2015] Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proc. of ACL", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "A probabilistic approach to nearest-neighbor classification: Naive hubness bayesian knn", "author": ["Milo\u0161 Radovanovic", "Dunja Mladenic", "Mirjana Ivanovic"], "venue": "In Proc. of CIKM", "citeRegEx": "Toma\u0161ev et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Toma\u0161ev et al\\.", "year": 2011}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning : Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Verb similarity on the taxonomy of wordnet", "author": ["Yang", "Powers2006] Dongqiang Yang", "David M.W. Powers"], "venue": "In 3rd International WordNet Conference", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "(2002) YP-130 130 Yang and Powers (2006) MTurk-287 287 Radinsky et al. (2011) MTurk-771 771 Halawi et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "(2011) MTurk-771 771 Halawi et al. (2012) MEN 3000 Bruni et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 2, "context": "(2012) MEN 3000 Bruni et al. (2012) RW 2034 Luong et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "(2012) MEN 3000 Bruni et al. (2012) RW 2034 Luong et al. (2013) Verb 144 Baker et al.", "startOffset": 16, "endOffset": 64}, {"referenceID": 1, "context": "(2013) Verb 144 Baker et al. (2014) SimLex 999 Hill et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 1, "context": "(2013) Verb 144 Baker et al. (2014) SimLex 999 Hill et al. (2014)", "startOffset": 16, "endOffset": 66}, {"referenceID": 3, "context": "(Bruni et al., 2012).", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "Mikolov et al. (2013b) showed that this problem can be solved using the vector offset method: b\u2217 \u2248 b \u2212 a + a\u2217.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Mikolov et al. (2013b) showed that this problem can be solved using the vector offset method: b\u2217 \u2248 b \u2212 a + a\u2217. Levy and Goldberg (2014a) show that solving this equation is equivalent to computing a linear combination of word similarities between the query word b\u2217, with the given words a, b, and b\u2217.", "startOffset": 0, "endOffset": 137}, {"referenceID": 0, "context": "In an attempt to address this limitation, Agirre et al. (2009) divided WS-353 into two sets containing word pairs exhibiting only either similarity or relatedness.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "In an attempt to address this limitation, Agirre et al. (2009) divided WS-353 into two sets containing word pairs exhibiting only either similarity or relatedness. Recently, Hill et al. (2014) constructed a new word similarity dataset (SimLex), which captures the degree of similarity between words, and related words are considered dissimilar.", "startOffset": 42, "endOffset": 193}, {"referenceID": 15, "context": "Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015).", "startOffset": 114, "endOffset": 159}, {"referenceID": 23, "context": "Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks.", "startOffset": 55, "endOffset": 128}, {"referenceID": 13, "context": "Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al.", "startOffset": 141, "endOffset": 323}, {"referenceID": 16, "context": "Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks.", "startOffset": 30, "endOffset": 47}, {"referenceID": 16, "context": "Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks.", "startOffset": 30, "endOffset": 156}, {"referenceID": 16, "context": "Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks.", "startOffset": 30, "endOffset": 334}, {"referenceID": 16, "context": "Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the baseline vectors on most tasks.2 By selecting the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set.", "startOffset": 30, "endOffset": 844}, {"referenceID": 2, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al.", "startOffset": 81, "endOffset": 134}, {"referenceID": 32, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al.", "startOffset": 81, "endOffset": 134}, {"referenceID": 4, "context": ", 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011).", "startOffset": 149, "endOffset": 169}, {"referenceID": 33, "context": "The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger\u2019s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking.", "startOffset": 161, "endOffset": 176}, {"referenceID": 2, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger\u2019s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking.", "startOffset": 82, "endOffset": 434}, {"referenceID": 2, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger\u2019s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next.", "startOffset": 82, "endOffset": 826}, {"referenceID": 2, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger\u2019s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next. Rastogi et al. (2015) observed that the improvements shown on small word similarity task datasets by previous work were insignificant.", "startOffset": 82, "endOffset": 873}, {"referenceID": 2, "context": "Statistical significance testing is important for validating metric gains in NLP (Berg-Kirkpatrick et al., 2012; S\u00f8gaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger\u2019s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next. Rastogi et al. (2015) observed that the improvements shown on small word similarity task datasets by previous work were insignificant. We now briefly describe the method presented by them to compute statistical significance for word similarity evaluation. Let A and B be the rankings produced by two word vector models over a list of words pairs, and T be the human annotated ranking. Let rAT , rBT and rAB denote the Spearman\u2019s correlation between A : T , B : T and A : B resp. and r\u0302AT , r\u0302BT and r\u0302AB be their empirical estimates. Rastogi et al. (2015) introduce \u03c3 p0 as the minimum required difference for significance (MRDS) which satisfies the following:", "startOffset": 82, "endOffset": 1407}, {"referenceID": 26, "context": "\u03c3 p0 depends on the size of the dataset, p0 and r and Rastogi et al. (2015) present its values for common word similarity datasets.", "startOffset": 54, "endOffset": 76}, {"referenceID": 37, "context": "This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010).", "startOffset": 116, "endOffset": 137}, {"referenceID": 25, "context": "However, it has been shown that vector-spaces contain hubs, which are vectors that are close to a large number of other vectors in the space (Radovanovi\u0107 et al., 2010).", "startOffset": 141, "endOffset": 167}, {"referenceID": 6, "context": "This problem manifests in word vector-spaces in the form of words that have high cosine similarity with a large number of other words (Dinu et al., 2014).", "startOffset": 134, "endOffset": 153}, {"referenceID": 6, "context": "This problem manifests in word vector-spaces in the form of words that have high cosine similarity with a large number of other words (Dinu et al., 2014). Schnabel et al. (2015) further refine this hubness problem to show that there exists a power-law relationship between the frequency-rank5 of a word and the frequency-rank of its neighbors.", "startOffset": 135, "endOffset": 178}, {"referenceID": 17, "context": "(Luong et al., 2013; Hill et al., 2014), this still does not solve the problem that cosine similarity in the vector-space gets polluted by frequencybased effects.", "startOffset": 0, "endOffset": 39}, {"referenceID": 10, "context": "(Luong et al., 2013; Hill et al., 2014), this still does not solve the problem that cosine similarity in the vector-space gets polluted by frequencybased effects.", "startOffset": 0, "endOffset": 39}, {"referenceID": 6, "context": "Different distance normalization schemes have been proposed to downplay the frequency/hubness effect when computing nearest neighbors in the vector space (Dinu et al., 2014; Toma\u0161ev et al., 2011), but their applicability as an absolute measure of distance for word similarity tasks still needs to investigated.", "startOffset": 154, "endOffset": 195}, {"referenceID": 35, "context": "Different distance normalization schemes have been proposed to downplay the frequency/hubness effect when computing nearest neighbors in the vector space (Dinu et al., 2014; Toma\u0161ev et al., 2011), but their applicability as an absolute measure of distance for word similarity tasks still needs to investigated.", "startOffset": 154, "endOffset": 195}, {"referenceID": 11, "context": "To account for sense-specific word similarity, Huang et al. (2012) introduced the Stanford contextual word similarity dataset (SCWS), in which the task is to compute similarity between two words given the contexts they occur in.", "startOffset": 47, "endOffset": 67}, {"referenceID": 11, "context": "multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015).", "startOffset": 68, "endOffset": 188}, {"referenceID": 22, "context": "multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015).", "startOffset": 68, "endOffset": 188}, {"referenceID": 12, "context": "multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015).", "startOffset": 68, "endOffset": 188}, {"referenceID": 30, "context": "Although task-specific evaluation produces different rankings of word vector models for different tasks (Schnabel et al., 2015), this is not necessarily a problem because different vector models capture different types of information which can be more or less useful for a particular task.", "startOffset": 104, "endOffset": 127}], "year": 2016, "abstractText": "Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of \u201csemantic similarity\u201d is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.", "creator": "LaTeX with hyperref package"}}}