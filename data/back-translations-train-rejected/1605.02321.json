{"id": "1605.02321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2016", "title": "Asymmetric Move Selection Strategies in Monte-Carlo Tree Search: Minimizing the Simple Regret at Max Nodes", "abstract": "The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree search (MCTS) has made a significant impact in various research fields. The UCT algorithm, which combines the UCB bandit algorithm with MCTS, is a good example of the success of this combination. The recent breakthrough made by AlphaGo, which incorporates convolutional neural networks with bandit algorithms in MCTS, also highlights the necessity of bandit algorithms in MCTS. However, despite the various investigations carried out on MCTS, nearly all of them still follow the paradigm of treating every node as an independent instance of the MAB problem, and applying the same bandit algorithm and heuristics on every node. As a result, this paradigm may leave some properties of the game tree unexploited. In this work, we propose that max nodes and min nodes have different concerns regarding their value estimation, and different bandit algorithms should be applied accordingly. We develop the Asymmetric-MCTS algorithm, which is an MCTS variant that applies a simple regret algorithm on max nodes, and the UCB algorithm on min nodes. We will demonstrate the performance of the Asymmetric-MCTS algorithm on the game of $9\\times 9$ Go, $9\\times 9$ NoGo, and Othello.", "histories": [["v1", "Sun, 8 May 2016 13:52:41 GMT  (100kb)", "http://arxiv.org/abs/1605.02321v1", "submitted to the 2016 IEEE Computational Intelligence and Games Conference"]], "COMMENTS": "submitted to the 2016 IEEE Computational Intelligence and Games Conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yun-ching liu", "yoshimasa tsuruoka"], "accepted": false, "id": "1605.02321"}, "pdf": {"name": "1605.02321.pdf", "metadata": {"source": "CRF", "title": "Asymmetric Move Selection Strategies in Monte-Carlo Tree Search: Minimizing the Simple Regret at Max Nodes", "authors": ["Yun-Ching Liu", "Yoshimasa Tsuruoka"], "emails": ["cipherman@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "II. PRELIMINARIES", "text": "Bandit algorithms are algorithms that solve the MAB problem [1]. In the MAB problem, an Agent K slot machine or \"one-armed bandit\" is faced, and the agent can choose to draw one of the slot machines on each play. The selected slot machine will then produce a reward r [0, 1]. The distribution of the reward of each slot machine is unknown to the agent. There are two possible targets in the MAB problem, and different types of bandit algorithms are required to achieve each goal."}, {"heading": "A. Cumulative Regret Minimization", "text": "The goal of the conventional MAB problem is to accumulate as much reward as possible over a total number of T games, where r-it is the expected reward of the optimal arm, and rIt is the reward that the agent receives by pulling the arm. A bandit algorithm is considered optimal if it can limit the increase in cumulative regret to O (Log-T) [1]. The UCB algorithm [5] used in the UCT algorithm [4] is an optimal bandit algorithm that limits the growth of cumulative regret to O (K-Log-T), where it is the difference of the expected reward between a suboptimal arm and the optimal arm of the arm. The UCB algorithm [5] is an optimal bandit algorithm that limits the growth of cumulative regret to O (suboptimal log-T) between the expecting arm and the optimal arm."}, {"heading": "B. Simple Regret Minimization", "text": "The goal of the pure exploration problem MAB is to identify the arm that has the highest expected reward after a given total of T games. \u00b7 This task can be formally stated as minimizing the simple regret, which is defined as SRT = r-rT, where r-rT is the expected reward of the optimal arm, and rT is the mean reward of the arm that is identified as optimal by the agent after the T games. Since the goal is to determine which arm is the optimal arm, it is more critical to gather as much information as possible about each arm, and therefore the amount of accumulated reward during these T games is irrelevant. It has been shown that minimizing the cumulative regret CRT and minimizing the simple regret SRT are two conflicting goals, i.e. as the UCRT decreases, the SRT will increase at the same time, and vice versa the reward increases during these T games [6]."}, {"heading": "III. ASYMMETRIC MONTE-CARLO TREE SEARCH", "text": "MCTS consists of four main steps: selection, expansion, simulation and backpropagation. Bandit algorithms are mainly used in the selection phase, in which each node is considered an independent instance of the MAB problem, with each child node being a single candidate arm. Currently, the most popular variant of MCTS is the UCT algorithm, where the UCB algorithm is the bandit algorithm applied. Although this general MCTS paradigm allows its application across a wide range of domains, it leaves a number of features of the game tree unused."}, {"heading": "A. Concerns on Value Estimation of Different Node Types", "text": "The role of the bandit algorithm on each node of the MCTS, for example, is to estimate the value of the node and make a selection according to the estimated value. As the search progresses, the estimated value of the nodes is also converged. Although the general goal is to get a good reward as quickly as possible, it can be observed that different types of nodes in the game tree have different requirements for their estimated values: \u2022 Min node: Since the least nodes represent the reaction of the opponent, it is not necessary to determine the best possible response of the current decider. Only a good enough response is sufficient to refute a decision of the decider and not to be overly optimistic. \u2022 Min node: Since the least nodes represent the reaction of the opponent, it is not necessary to determine the best possible reaction of the opponent."}, {"heading": "B. Different Bandit Algorithms for Different Node Types", "text": "Since simple repentance algorithms and cumulative repentance bandit algorithms have different properties, they can be applied accordingly to different types of nodes to meet the requirements for the estimated value of each node type: The Asymmetric MCTS algorithm presented in Algorithm 3 maintains the four steps of conventional MCTS, namely selection, expansion, simulation and backpropagation. The main feature of the Asymmetric MCTS is that it applies the UCB algorithm, which is a simple repentance bandit algorithm, to maximum nodes, and the UCB algorithm, which is a cumulative repentance bandit algorithm, to minimum nodes as shown in Figure 2."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we will first demonstrate the effect of the distorted reward on the UCB and UCB \u221a \u00b7 algorithms, and then demonstrate the performance of the asymmetric MCTS algorithm on the game of 9 \u00d7 9 Go, 9 \u00d7 9 Nogo and Othello. Starting point of all experiments is the simple UCT algorithm. To directly compare the effect of the bandit algorithms, all MCTS algorithms used purely random simulations, and no performance-enhancing heuristics were used. Each experimental result corresponds to the average of 2300 games, and each algorithm alternated with black and white in the game."}, {"heading": "A. Effect of Biased Reward in the MAB problem", "text": "To enhance the effect of the biased reward, we will first examine two extreme cases: The rewards tend to be produced in ascending order and descending order. The MAB dynamometer essentially follows the settings given in Sutton et al. [2]. The results are the average of 2000 randomly generated K-armed bandit problems with K = 20. A total of 5000 moves were given for each problem. Each bandit's rewards are initially generated from a normal (Gaussian) distribution, with the mean wi, i-K and the deviation from 1. The mean of the bandits was randomly observed from a normal distribution with mean 0 and deviation 1. To simulate the biased reward, the rewards are then sorted in ascending order and the descending order of the algorithms tended.It can be observed from Figure 3a and Figure 3b, regardless of the order in which UCB is generated."}, {"heading": "B. Performance of the Asymmetric-MCTS on 9\u00d7 9 Go", "text": "We will first look at the performance of Asymmetric MCTS on the game of Go played on the 9 \u00b7 9 board, with the Komi of 6.5.1) Performance of SR + CR scheme: For comparison, we will show the performance of SR + CR scheme on the game of 9 \u00b7 9 Go. The SR + CR scheme applies the UCB \u221a \u00b7 Bandit algorithm given only on the root node, and the UCB bandit algorithm on all other nodes win [8]. Table I shows the win rate of the various settings for the constant cs in the UCB \u221a \u00b7 algorithm in the SR + CR scheme algorithms. The best constant setting for the UCB algorithm is cr = 0.4 in the UCT + CR scheme Asyetric and the level UCT algorithm is c = 0.4. A total of 5000 playouts are given to the two algorithms for the constant MCR nodes. It can be observed that the SR + CR algorithm with its 54% constant is the best CR + CR scheme."}, {"heading": "C. Performance of the Asymmetric-MCTS on 9\u00d7 9 NoGo", "text": "We will now show the performance of AsymmetricMCTS on the game of Nogo \u00b7 \u00b7 This is a lousy variation of the game of Go, in which the first player who loses no legal moves other than capturing the opponent's checkers. 1) The performance of the SR + CR scheme: As in 9 \u00b7 9 Go, we will first show the performance of the SR + CR scheme on the game of 9 \u00b7 9 NoGo for comparison. Table IV shows the win rate of the various settings for the constant cs in the SR + CR scheme. The constant setting for the level UCT algorithm is c = 0.3. A total of 5000 plays are given for both algorithms for each turn. We can observe that SR + CR scheme is extremely good against the level UCT algorithm, reaching a win rate of almost 68% against the level UCT algorithm. 2) The tuning of the C constants: We will now continue to observe the best settings for the constants UCT \u00b7 CR algorithm and optimum UCT-B = 0.0"}, {"heading": "D. Performance of the Asymmetric-MCTS on Othello", "text": "Finally, we proceed to demonstrate the performance of the asymmetric MCTS algorithm at the game of Othello.1) Performance of the SR + CR scheme: We will first examine the performance of the SR + CR scheme on Othello for comparison. Table VII shows the win rate of the various settings for the constant cs in the UCB \u221a \u00b7 algorithm of the SR + CR scheme. The constant setting for the UCB algorithm in the SR + CR scheme is cr = 0.6 and the level UCT algorithm isc = 0.6. A total of 5000 outputs are for both algorithms for each move. It can be observed that the SR + CR scheme can generate a best win rate of about 53%, but that is still around the same level of the level of the UCT algorithm isc = 0.2) Tuning of the C constants: We will now continue to find the best settings for the constants cr \u00b7 UCB \u00b7 Asymmetry in the UCB \u00b7 RTS."}, {"heading": "V. CONCLUSION", "text": "In most MCTS variants, the same bandit algorithm and heuristics are applied to each node in the game tree, treating each node as an independent instance of the MAB problem. Currently, the most dominant variant of the MCTS is the UCT algorithm, which applies the UCB bandit algorithm to each node. Although this paradigm has the advantage that MCTS can be applied across a wide range of fields, it leaves a number of features of the game tree unused. In this work, we have suggested that maximum nodes and minimum nodes should be treated differently by applying different UCB bandit algorithms according to its intrinsic nature, rather than using the same bandit algorithm throughout the game tree. We have observed that different types of nodes have different concerns about their estimated value, and the simple regret bandit algorithms seem to be interesting."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics, vol. 6, issue 1, pp. 4-22, 1985.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "A survey of monte-carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and Al in Games, vol. 4, issue 1, pp.1-43, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 17th European Conference on Machine Learning (ECML\u201906), pp. 282-293, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, vol. 47, issue 2, pp. 235 - 256, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "MCTS based on simple regret", "author": ["D. Tolpin", "S.E. Shimony"], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI), pp. 570-576, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups", "author": ["M. Lanctot", "M.H.M. Winands", "T. Pepels", "N.R. Sturtevant"], "venue": "Proceedings of the 2014 IEEE Conference on Computational Intelligence and Games (CIG", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential halving applied to trees", "author": ["T. Cazenave"], "venue": "IEEE Transactions on Computational Intelligence and Al in Games, vol. 7, issue 1, pp. 102-105, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimizing simple and cumulative regret in monte-carlo tree Search", "author": ["T. Pepels", "T. Cazenave", "M.H.M.M.H.M. Winands", "M. Lanctot"], "venue": "Proceedings of Computer Games Workshop at the 21st European Conference on Artificial Intelligence, pp. 1-15, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Tusruoka ,\u201cRegulation of Exploration for Simple Regret Minimization in Monte-Carlo Tree Search", "author": ["Y.Y.-C. Liu"], "venue": "Proceedings of the 2015 IEEE Conference on Computational Intelligence and Games (CIG 2015),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Monte-Carlo Simulation Balancing", "author": ["D. Silver", "G. Tesauro"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (ICML\u201909), pp. 945-952, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Computing \u201celo ratings\u201d of move patterns in the game of go", "author": ["R. Coulom"], "venue": "ICGA Journal, vol. 30, issue 4, pp. 198-208, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Almost optimal exploration in multiarmed bandits", "author": ["Z. Karnin", "T.Koren", "S. Oren"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML\u201913), pp. 1238-1246, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-armed bandits with episode context", "author": ["C. Rosin"], "venue": "Annals of Mathematics and Artificial Intelligence, vol. 61, issue 3, pp. 203-230, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Better Computer Go Player with Neural Network and Long-term Prediction", "author": ["Y. Tian", "Y. Zhu"], "venue": "Proceedings of International Conference on Learning Representations (ICLR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "Nature, 529, pp. 484-489, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Monte-Carlo Tree Search (MCTS) has made a significant impact on various fields in AI, especially on the field of computer Go [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "The key factor to the success of MCTS lies in its combination with bandit algorithms, which solves the multiarmed bandit problem (MAB) [4].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "The MAB problem is a problem where the agent needs to decide whether it should act optimally based on current available information (exploitation), or gather more information at the risk of suffering losses incurred by performing suboptimal actions (exploration) [1].", "startOffset": 263, "endOffset": 266}, {"referenceID": 4, "context": "One of the most widely used MCTS variants is the UCT algorithm, which simply applies the UCB algorithm to every node in the tree [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": "The integration of offline knowledge was mainly focused on using logistic models to improve the quality of the simulations [13][14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "The integration of offline knowledge was mainly focused on using logistic models to improve the quality of the simulations [13][14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "Recently, a lot of effort has been put into the training of convolutional neural networks and combining them with MCTS in computer Go [17][18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "Recently, a lot of effort has been put into the training of convolutional neural networks and combining them with MCTS in computer Go [17][18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "A breakthrough was made by the program AlphaGo, which essentially combines convolutional neural networks with the PUCB bandit algorithm [16], and has beaten a top human professional player Lee Sedol in a five-game challenge match [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "A breakthrough was made by the program AlphaGo, which essentially combines convolutional neural networks with the PUCB bandit algorithm [16], and has beaten a top human professional player Lee Sedol in a five-game challenge match [18].", "startOffset": 230, "endOffset": 234}, {"referenceID": 5, "context": "One of them is using various bandit algorithms with MCTS, especially the bandit algorithms that solve the pure exploration MAB problem [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "It has been argued that simple regret bandit algorithms might be better suited to the task of game tree search, since the ultimate goal of game tree search is to find the best possible action [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "The SR+CR scheme [8] is an MCTS algorithm that applies a simple regret bandit algorithm on the root node, and the UCB algorithm on all other nodes.", "startOffset": 17, "endOffset": 20}, {"referenceID": 13, "context": "The sequential halving on trees (SHOT) algorithm combines the sequential halving algorithm [15], which is a near optimal simple regret bandit algorithm, with MCTS.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The Hybrid MCTS (H-MCTS) algorithm [11] first applies the UCB algorithm on each node, and then switches to the sequential halving algorithm if the number of times a node has been visited has exceeded a predetermined threshold.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "The CCB-MCTS algorithm [12] uses the improved UCB algorithm to regulate the amount of exploration performed by simple regret bandit algorithms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "Some methods have been proposed to reflect the min-max property of game trees in MCTS, but still essentially treat max nodes and min nodes symmetrically, and apply the same heuristic on every node [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "The SR+CR scheme differs only the root node from other nodes, rather than between max nodes and min nodes [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Bandit algorithms are algorithms that solve the MAB problem [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "The chosen slot machine will then produce a reward r \u2208 [0, 1].", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "A bandit algorithm is considered optimal if it can restrict the increase of cumulative regret to O(log T ) [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "The UCB algorithm [5], which is applied in the UCT algorithm [4], is an optimal bandit algorithm which restricts the growth of cumulative regret to O( log T \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The UCB algorithm [5], which is applied in the UCT algorithm [4], is an optimal bandit algorithm which restricts the growth of cumulative regret to O( log T \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Algorithm 1 The UCB algorithm [5]", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "The objective of the pure exploration MAB problem is to identify the arm that has the highest expected reward after a given total amount of T plays [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": ", as CRT decreases, SRT will increase at the same time, and vice versa [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Algorithm 2 The UCB\u00b7 algorithm [8]", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "O((\u2206 exp(\u2212 \u221a T ))) [8].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "TABLE I: Win Rate of SR+CR scheme [8] against plain UCT algorithm in 9\u00d7 9 Go.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "The SR+CR scheme applies the UCB\u00b7 bandit algorithm only on the root node, and the UCB bandit algorithm on all other nodes [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "TABLE IV: Win Rate of UCB\u00b7 MCTS and SR+CR scheme [8] against plain UCT algorithm in 9\u00d7 9 NoGo.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "6 and the plain UCT algorithm is TABLE VII: Win Rate of the SR+CR scheme [8] against plain UCT algorithm on Othello.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 147, "endOffset": 151}], "year": 2016, "abstractText": "The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree search (MCTS) has made a significant impact in various research fields. The UCT algorithm, which combines the UCB bandit algorithm with MCTS, is a good example of the success of this combination. The recent breakthrough made by AlphaGo, which incorporates convolutional neural networks with bandit algorithms in MCTS, also highlights the necessity of bandit algorithms in MCTS. However, despite the various investigations carried out on MCTS, nearly all of them still follow the paradigm of treating every node as an independent instance of the MAB problem, and applying the same bandit algorithm and heuristics on every node. As a result, this paradigm may leave some properties of the game tree unexploited. In this work, we propose that max nodes and min nodes have different concerns regarding their value estimation, and different bandit algorithms should be applied accordingly. We develop the Asymmetric-MCTS algorithm, which is an MCTS variant that applies a simple regret algorithm on max nodes, and the UCB algorithm on min nodes. We will demonstrate the performance of the Asymmetric-MCTS algorithm on the game of 9 \u00d7 9 Go, 9\u00d7 9 NoGo, and Othello.", "creator": "gnuplot 4.6 patchlevel 4"}}}