{"id": "1605.02401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Audio Event Detection using Weakly Labeled Data", "abstract": "Acoustic event detection is essential for content analysis and description of multimedia recordings. The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data. However, the labels available for online multimedia data are generally weak and do not provide sufficient detail for such methods to be employed. In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data based on a Multiple Instance Learning (MIL) framework. We first show that audio event detection using weak data can be formulated as an MIL problem. We then suggest two frameworks for solving multiple-instance learning, one based on neural networks, and the second on support vector machines. The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning. Our proposed framework can not only successfully detect events in a recording but can also provide temporal locations of events in the recording. This is interesting as these information were never known in the first place for weakly labeled data.", "histories": [["v1", "Mon, 9 May 2016 02:17:12 GMT  (255kb)", "http://arxiv.org/abs/1605.02401v1", "1st version on arXiv"], ["v2", "Thu, 9 Jun 2016 03:33:13 GMT  (255kb)", "http://arxiv.org/abs/1605.02401v2", "updated version on arXiv"], ["v3", "Wed, 6 Jul 2016 05:46:56 GMT  (256kb)", "http://arxiv.org/abs/1605.02401v3", "ACM Multimedia 2016"]], "COMMENTS": "1st version on arXiv", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.MM", "authors": ["anurag kumar", "bhiksha raj"], "accepted": false, "id": "1605.02401"}, "pdf": {"name": "1605.02401.pdf", "metadata": {"source": "CRF", "title": "Audio Event Detection using Weakly Labeled Data", "authors": ["Anurag Kumar", "Bhiksha Raj"], "emails": ["alnu@andrew.cmu.edu", "bhiksha@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.02 401v 1 [cs.S D] 9M ay2 01Keywords Audio Event Recognition, Multiple Instance Learning, Neural Networks, Temporal Localization"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they are able to live."}, {"heading": "2. RELATED WORK", "text": "This year is the highest in the history of the country."}, {"heading": "3. AED USING WEAKLY LABELED DATA", "text": "The problem with audio event detection is that the occurrence of events (e.g. clapping, barking, cheering, etc.) is detected in a given audio recording. To be able to do this, we need models of these events that can be used to detect their occurrence. Training such models, in turn, requires training instances of these sounds. Such instances can be presented either as explicit recordings of these sounds, or as segments of longer audio recordings within which they occur. Our goal is to train the models, instead, contains poorly labeled data that includes recordings in which only the presence or absence of these events is identified, without specifying the exact location of the event or even the number of times it occurred."}, {"heading": "4. PROPOSED FRAMEWORK", "text": "Our formulation of event detection using weak labels of the kind described above is based on Multiple Instance Learning (MIL) [10], a generalized version of supervised label learning available for a number of instances. We suggest that audio event detection using weak labels is essentially a MIL problem. In the following subsections, we first describe the multi-level learning system, Section 4.1, and two popular algorithmic approaches to MIL in sections 4.3 and 4.2. Then, in sections 4.4, 4.6 and 4.5, we describe how we apply MIL to the problem of learning acoustic event detectors."}, {"heading": "4.1 Multiple Instance Learning", "text": "The term Multiple Instance Learning was first correctly developed in 1997 by Dietterich et al. and includes the problem of detecting drug activity. (1) MIL is described in terms of bags; a bag is simply a collection of instances. Labels are attached to the pockets rather than to the individual instances within them. A positive bag is one that has at least one positive instance (an instance in the target class that needs to be classified). A negative bag contains only negative instances. Thus, a negative bag is pure, while a positive bag is impure. This creates an asymmetry from a learning perspective, as all instances in a negative bag can be clearly assigned to a negative label, whereas for a positive bag this cannot be done; an instance in a positive bag can be either positive or negative."}, {"heading": "4.2 MIL for SVM (mi-SVM)", "text": "The first, so-called mi-SVM, operates at the level of the instance and maximizes the scope of the individual instances of a linear discriminant. The second, described as MI-SVM, can also be represented in the form of the linear delimitations of the instances, rather than the individual instances. In this thesis, we use mi-SVM to understand that the relationship between the label Yi and the instances yij can also be represented in the form of linear delimitations."}, {"heading": "4.3 MIL of Neural Networks (BP-MIL)", "text": "The conventional approach to training neural networks is to provide instance-specific labels for a collection of training instances. Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output. To do this, we need to modify the way in which the divergence to be minimized is calculated to represent only the output output output output Output Output Output Output Output Output Output.n where only the output output output Output Output Output Output Output Output Output.n to use the output output Output Output Output Output Output.n to use the output output Output Output Output Output.n to use the output output Output Output Output Output Output.n where only the output output output output Output Output Output Output.39 is proposed"}, {"heading": "4.4 MIL for AED using weakly labeled data", "text": "To apply the MIL framework to this scenario, we must first present the weakly labeled recordings in the form of bag label representations. To convert the recording Ri into a pocket, it is segmented into a number of short audio segments. Segmentation must not overlap. So, let's leave the segments derived from Ri [IRi1 IRi2... IRiK]. Each of these smaller segments is now treated as a single instance within the pocket Ri. Formally, a pocket Ri is Ri = {IRi1, IRi2,.... IRiK}, where K depends on the duration of the recording Ri, the length of the individual segments and the overlap between adjacent segments. If the weak labels for Ri markings and events are present as in Ri, then it may be present at least at a moment of recording Ri when the individual segments are negative and the overlap between adjacent segments is uniquely marked for the event."}, {"heading": "4.5 Temporal Localization of Events", "text": "The MIL frameworks we use in this work learn from baglevel labels, but once learned, they can detect the presence of events in individual cases. In the context of audio analysis, this means that we can detect not only the presence of an event in a test recording (bag), but also in individual segments of the test recording. Formally, if Rx is a test recording that we can more explicitly represent as Rx (t), where \"t\" indexes the time, the individual segments in the recording IRx1, IRx2,.... IRxK by IRxk = Rx (t), (k \u2212 1) l \u00b2 \u2264 t < (k \u2212 1) l \u00b2 l \u00b2 l, where l \u00b2 is the length of the segment in seconds and l \u00b2 denotes the amount by which the segment window is moved. In the particular case of non-overlapping segments l \u00b2 = l \u00b2, the event egg is detected as present, this means that this event can be localized in the form of time \u00b2."}, {"heading": "4.6 GMM based features for audio segments", "text": "Before we can apply the MIL framework, each segment of the audio segment must first be converted into an appropriate feature segment that requires this detail. First, we use the M-Frequency Audio Coefficient (MFCC) vector system, which tends to be ineffective for the purposes of audio classification; other secondary representations derived from these segments are required. As described in Section 2, one of the simple yet very successful approaches is the Bag of Audio Words feature representation, which quantifies the individual MFCC vectors into a series of code words and uses segments of audio as histograms across these codewords.However, the bag of word representation has proven to be very effective for classifying long segments of the audio segment for which we represent weak long-term segments like the one in our case, effectively characterizing several problems."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "5.1 miSVM Results", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "5.2 BP-MIL Results", "text": "For the BP-MIL neural network, three parameters must be defined, namely the number of hidden layers, the number of nodes in each hidden layer (nno) and the learning rate (\u03b7). We used a network with a hidden layer in all experiments. The network is trained for a total of 60 epochs. The learning rate is set to either 0.1 for the entire training or 0.1 for the first 30 epochs and then reduced in each epoch until it reaches 0.01. Larger values of nno are used for a greater dimensionality of input characteristics. For ~ F64 and ~ F128, three different values of nno are used, these are 16, 50 and 100 for ~ F64 and 50, 100 and 150 for ~ F128. If both ~ F and ~ M are used, the values of nno used in experiments are 256 and 512."}, {"heading": "5.3 Temporal Localization of Events", "text": "We now show the performance of the MIL-based framework on the temporal localization of audio events. To evaluate the performance of this task, we need the soil truth markers of all instances in all pockets. The instances in the pockets were obtained through uniform segmentation in our work. Each instance is a second window segment of the recording, which is moved in an overlapping manner to segment the recording. However, the annotations that contain timestamps of events in the recording do not adhere to this uniform segmentation. Thus, an event could begin and end within a segment and it may also begin or end at any point in the recording. Therefore, assigning soil truth markers of instances to a valid analysis is not easy. We use simple heuristics to obtain soil truth markers. As described in Section 4.5, each segment represents a specific time period of the recording. Considering the actual annotations that are available to the segment, the total event may be present as an AUD-length of at least 50% of the event."}, {"heading": "6. DISCUSSIONS AND CONCLUSION", "text": "A framework for learning acoustic event detectors from poorly labeled data was presented in this paper. In addition, the learned detectors can both detect and time-locate events in a record. We show that we achieve reasonable performance for both tasks. In particular, events such as clanking, scraping, children voices are easy to detect by using both SVM and neural mesh approaches. On the other hand, events such as drumming, hammering and laughing are hard to detect using both methods. Although specific differentiations can be seen here, a large-scale application of both methods to a larger number of events could be possible to gain better insights into both methods. Overall performance of both methods is almost identical. Given the limited amount of training data, the success of the proposed approach is evident."}, {"heading": "7. REFERENCES", "text": "[1] Multimedia event detection.http: / / www.nist.gov / itl / iad / mig / med11.cfm. [2] Multimedia event detection.http: / / www.youtube.com / yt / press / statistics.html. [3] E. Amid, A. Mesaros, K. J. Palomaki, J. Laaksonen, and M. Kurimo. Unsupervised feature extraction for multimedia event detection and ranking using audio content. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, p. 5943. IEEE, 2014. [4] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple instance learning. Advances in neural information processing systems, 15: 561-568, 2002. [5] K. Ashraf, B. Elizalde, F. Iandola, M. Moskewicz."}], "references": [{"title": "Unsupervised feature extraction for multimedia event detection and ranking using audio content", "author": ["E. Amid", "A. Mesaros", "K.J. Palomaki", "J. Laaksonen", "M. Kurimo"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 5939\u20135943. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in neural information processing systems, 15:561\u2013568,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Audio-based multimedia event detection with DNNs and sparse sampling", "author": ["K. Ashraf", "B. Elizalde", "F. Iandola", "M. Moskewicz", "J. Bernd", "G. Friedland", "K. Keutzer"], "venue": "Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pages 611\u2013614. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on text-independent speaker verification", "author": ["F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin-Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Gar\u0107\u0131a", "D. Petrovska-Delacr\u00e9taz", "D.A. Reynolds"], "venue": "EURASIP journal on applied signal processing, 2004:430\u2013451,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "The use of the area under the roc curve in the evaluation of machine learning algorithms", "author": ["A.P. Bradley"], "venue": "Pattern recognition, 30(7):1145\u20131159,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach", "author": ["F. Briggs", "B. Lakshminarayanan", "L. Neal", "X.Z. Fern", "R. Raich", "S.J. Hadley", "A.S. Hadley", "M.G. Betts"], "venue": "The Journal of the Acoustical Society of America, 131(6):4640\u20134650,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-visual event recognition in surveillance video sequences", "author": ["M. Cristani", "M. Bicego", "V. Murino"], "venue": "Multimedia, IEEE Transactions on, 9(2):257\u2013267,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence, 89(1):31\u201371,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple instance learning of real valued data", "author": ["D.R. Dooly", "Q. Zhang", "S.A. Goldman", "R.A. Amar"], "venue": "The Journal of Machine Learning Research, 3:651\u2013678,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W.-H. Tsang", "J. Luo"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(9):1667\u20131680,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-based context recognition", "author": ["A.J. Eronen", "V.T. Peltonen", "J.T. Tuomi", "A.P. Klapuri", "S. Fagerlund", "T. Sorsa", "G. Lorho", "J. Huopaniemi"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 14(1):321\u2013329,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European, pages 506\u2013510. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical language modeling for audio events detection in a sports game", "author": ["Q. Huang", "S. Cox"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pages 2286\u20132289. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Audio event classification using deep neural networks", "author": ["Z. Kons", "O. Toledo-Ronen"], "venue": "INTERSPEECH, pages 1482\u20131486,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio event detection from acoustic unit occurrence patterns", "author": ["A. Kumar", "P. Dighe", "R. Singh", "S. Chaudhuri", "B. Raj"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 489\u2013492. IEEE,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Event detection in short duration audio using gaussian mixture model and random forest classifier", "author": ["A. Kumar", "R.M. Hegde", "R. Singh", "B. Raj"], "venue": "21st European Signal Processing Conference 2013 (EUSIPCO 2013), Marrakech, Morocco, Sept.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse representation based on a bag of spectral exemplars for acoustic event detection", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6255\u20136259. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M.I. Mandel", "D.P. Ellis"], "venue": "ISMIR 2008: Proceedings of the 9th International Conference of Music Information Retrieval, pages 577\u2013582. Drexel University,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Advances in neural information processing systems, pages 570\u2013576,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Bag-of-audio-words approach for multimedia event classification", "author": ["S. Pancoast", "M. Akbacak"], "venue": "Proc. of Interspeech,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Gunshot detection in audio streams from movies by means of dynamic programming and bayesian networks", "author": ["A. Pikrakis", "T. Giannakopoulos", "S. Theodoridis"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, pages 21\u201324. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple instance learning-based birdsong classification using unsupervised recording segmentation", "author": ["J.F. Ruiz-Mu\u00f1oz", "M. Orozco-Alzate", "G. Castellanos-Dominguez"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 2632\u20132638. AAAI Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling, 5:3,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based information fusion for semi-supervised music genre classification", "author": ["Y. Song", "C. Zhang"], "venue": "Multimedia, IEEE Transactions on, 10(1):145\u2013152,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Detection and classification of acoustic scenes and events. Multimedia", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M. Plumbley"], "venue": "IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Scream and gunshot detection and localization for audio-surveillance systems", "author": ["G. Valenzise", "L. Gerosa", "M. Tagliasacchi", "F. Antonacci", "A. Sarti"], "venue": "Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on, pages 21\u201326. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Visual word ambiguity", "author": ["J.C. Van Gemert", "C.J. Veenman", "A.W. Smeulders", "J.-M. Geusebroek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(7):1271\u20131283,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Video event detection using motion relativity and feature selection", "author": ["F. Wang", "Z. Sun", "Y.-G. Jiang", "C.-W. Ngo"], "venue": "Multimedia, IEEE Transactions on, 16(5):1303\u20131315,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900, pages 1119\u20131126, San Francisco, CA, USA,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Applications of advances in nonlinear sensitivity analysis", "author": ["P.J. Werbos"], "venue": "System modeling and optimization, pages 762\u2013770. Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1982}, {"title": "Joint audio-visual bi-modal codewords for video event detection", "author": ["G. Ye", "I. Jhuo", "D. Liu", "Y.-G. Jiang", "D. Lee", "S.-F. Chang"], "venue": "In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A generic framework for event detection in various video domains", "author": ["T. Zhang", "C. Xu", "G. Zhu", "S. Liu", "H. Lu"], "venue": "Proceedings of the international conference on Multimedia, pages 103\u2013112. ACM,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "A generic framework for video annotation via semi-supervised learning", "author": ["T. Zhang", "C. Xu", "G. Zhu", "S. Liu", "H. Lu"], "venue": "Multimedia, IEEE Transactions on, 14(4):1206\u20131219,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural networks for multi-instance learning", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Proceedings of the International Conference on Intelligent Information Technology, Beijing, China, pages 455\u2013459,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Real-world acoustic event detection", "author": ["X. Zhuang", "X. Zhou", "M.A. Hasegawa-Johnson", "T.S. Huang"], "venue": "Pattern Recognition Letters, 31(12):1543\u20131551,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Automatic sound event detection also finds application in other scenarios, such as monitoring traffic for sounds of accidents or impact, surveillance, where one may \u201clisten\u201d for sounds of gunshots [31], screams [25] etc.", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "Automatic sound event detection also finds application in other scenarios, such as monitoring traffic for sounds of accidents or impact, surveillance, where one may \u201clisten\u201d for sounds of gunshots [31], screams [25] etc.", "startOffset": 211, "endOffset": 215}, {"referenceID": 5, "context": "It is also useful in cases such as wildlife monitoring [8], context recognition [13] and several health and life style monitoring system.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "It is also useful in cases such as wildlife monitoring [8], context recognition [13] and several health and life style monitoring system.", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "A nice survey on the audio event detection challenges, works and state of art can be found in [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "Audioevent detection has also found its way into consumer devices, particularly for automatic indexing of multimedia recordings of games [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "The authors of [40] model various sound events with Gaussian-mixture models in order to detect them in real-world recordings.", "startOffset": 15, "endOffset": 19}, {"referenceID": 37, "context": "The approach followed in [40] is similar to GMM-HMM architecture used for automatic speech recognition.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 21, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 167, "endOffset": 171}, {"referenceID": 0, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 200, "endOffset": 203}, {"referenceID": 18, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 237, "endOffset": 241}, {"referenceID": 15, "context": "[18] uses an alternate approach to obtaining bags of words \u2013 sound recordings are first decomposed into sequence of basic sound units called \u201cAcoustic Unit Descriptors\u201d (AUDS), which are themselves learned in an unsupervised manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The actual classification may be performed through classifiers such as SVM or random forests [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 34, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Some audio related works include semi-supervised music genre classification are [22] [29].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "Some audio related works include semi-supervised music genre classification are [22] [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "Two other audio related works are [26] and [8] where authors try to exploit weak labels in bird song and bird species classification.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Two other audio related works are [26] and [8] where authors try to exploit weak labels in bird song and bird species classification.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Several well known learning methods such as Support Vector Machines (SVMs)[4], K-NNs [34] have been modified to learn in this paradigm.", "startOffset": 74, "endOffset": 77}, {"referenceID": 31, "context": "Several well known learning methods such as Support Vector Machines (SVMs)[4], K-NNs [34] have been modified to learn in this paradigm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 7, "context": "Our formulation of event detection using weak labels of the kind described above is based onMultiple-Instance Learning (MIL) [10] which is a generalized version of supervised learning in labels are available for a collection of instances.", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "in 1997 for drug activity detection [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 121, "endOffset": 125}, {"referenceID": 31, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "In [4] Andrews et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "[4] proposed an optimization heuristic to solve this integer problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Neural networks have become increasingly popular for classification tasks [28] [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Neural networks have become increasingly popular for classification tasks [28] [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output, typically some representation of their assigned labels [27][35].", "startOffset": 236, "endOffset": 240}, {"referenceID": 32, "context": "Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output, typically some representation of their assigned labels [27][35].", "startOffset": 240, "endOffset": 244}, {"referenceID": 36, "context": "For this, we employ an adaptation of neural networks for multiple instance learning (BP-MIL) proposed in [39].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "in [11] where they showed that irrespective of the number of instances (positive or negative) in a bag, the bag can be fully described by the instance with maximal output.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "However direct characterization of audio as sequences of MFCC vectors tends to be ineffective for the purposes of audio classification [40]; other secondary representations derived from these are required.", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "[19] demonstrated that for short audio segments characterization of audio using Gaussian Mixture Model (GMM) can provide robust representations for performance of short audio segments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "They suggested Gaussian Mixture based characterization of audio events is a combination of two features: the first, which we represent as ~ F , is similar to bag-ofwords characterizations such as [32], and the second, which we represent as ~ M is a characterization of the modes of the distribution of vectors in the segment.", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Area Under ROC curve (AUC) [7] is a well known metric used to characterize ROC curves.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "We use LIBLINEAR [14] in our implementation of miSVM framework.", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "Acoustic event detection is essential for content analysis and description of multimedia recordings. The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data. However, the labels available for online multimedia data are generally weak and do not provide sufficient detail for such methods to be employed. In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data based on a Multiple Instance Learning (MIL) framework. We first show that audio event detection using weak data can be formulated as an MIL problem. We then suggest two frameworks for solving multiple-instance learning, one based on neural networks, and the second on support vector machines. The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning. Our proposed framework can not only successfully detect events in a recording but can also provide temporal locations of events in the recording. This is interesting as these information were never known in the first place for weakly labeled data.", "creator": "LaTeX with hyperref package"}}}