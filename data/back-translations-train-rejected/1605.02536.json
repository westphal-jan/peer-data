{"id": "1605.02536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Random Fourier Features for Operator-Valued Kernels", "abstract": "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operator-valued kernels. We propose a general principle for Operator-valued Random Fourier Feature construction relying on a generalization of Bochner's theorem for translation-invariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets.", "histories": [["v1", "Mon, 9 May 2016 11:36:40 GMT  (383kb,D)", "https://arxiv.org/abs/1605.02536v1", "32 pages, 6 figures"], ["v2", "Mon, 23 May 2016 16:35:41 GMT  (383kb,D)", "http://arxiv.org/abs/1605.02536v2", "32 pages, 6 figures"], ["v3", "Tue, 24 May 2016 12:59:58 GMT  (383kb,D)", "http://arxiv.org/abs/1605.02536v3", "32 pages, 6 figures"]], "COMMENTS": "32 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["romain brault", "florence d'alch\\'e-buc", "markus heinonen"], "accepted": false, "id": "1605.02536"}, "pdf": {"name": "1605.02536.pdf", "metadata": {"source": "CRF", "title": "Random Fourier Features for Operator-Valued Kernels", "authors": ["Romain Brault", "Florence d\u2019Alch\u00e9-Buc", "Markus Heinonen"], "emails": ["ro.brault@telecom-paristech.fr", "florence.dalche@telecom-paristech.fr", "markus.o.heinonen@aalto.fi"], "sections": [{"heading": null, "text": "To extend these methods, we extend the celebrated method of random Fourier features to obtain an approximation of the cores to the value of the operator. We propose a general principle for random Fourier features with the value of the operator: ro.brault @ telecom-paristech.fr \u2020 florence.dalche @ telecom-paristech.fr \u2021 markus.o.heinonen @ aalto.fiar Xiv: 1 feature construction based on a generalization of Bochner's theorem for translational invariant mercer cores with the value of the operator. We demonstrate the uniform convergence of kernel approximation for limited and unlimited operator random Fourier features using appropriate amber matrix concentration imbalance. An experimental proof-of-concept demonstrates the quality of the approximation and efficiency of the corresponding linear models."}, {"heading": "1 Introduction", "text": "In this paper, we are interested in a general and flexible approach to efficiently implement and learn vector-weighted functions, while at the same time taking into account the outputs between the outputs of the outputs for improved performance compared to independent scalar-weighted models. In this paper, we are interested in a general and flexible approach to efficiently implement and learn vector-weighted functions while extending the outputs between outputs. To achieve this goal, we are turning to flat architectures, namely the product of a (non-linear) matrix function (x) and a parameter vector function that we thus use."}, {"heading": "1.1 Notations", "text": "Genumengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengengensingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsing"}, {"heading": "1.2 Background", "text": "This implies that any positive, continuous and deferred conversion of a non-negative measure (x). We first consider scalar-rated functions. Think k: Rd \u00b7 Rd \u2192 R a positive defined kernel on Rd. < R the function that k (x, x). Rd \u00b7 Rd \u00b7 Rd, k (x, z \u2212 a) = k (x, z). Then we define the function that k (x, z) = k0 (x \u2212 z). k0 is referred to as the signature of kernel k. Bochner theorem is the theoretical result leading to the Random Fourier Features.Theorem 1.1 (Bochner's theorem1). Any positively defined complex function is the Fourier transform of a non-negative measurement. This implies that any positive, continuous and displaced invariant measurement is a non-negative measurement."}, {"heading": "W \u2217W is the orthogonal projection onto", "text": "In this paper, we are interested in finding characteristics maps of this form for displacement-invariant Rp-Mercer kernels using the following definitions. A reproducing K kernel to Rd is an Rp-Mercer provided that HK is a subspace of C (Rd; Rp). It is characterized as a displacement-invariant kernel or translation-invariant kernel for the addition of K (x + a, z + a) = K (x, z), K (x, z, a), and X 3. It is characterized by a function K0: X \u2192 L (Y) of completely positive type, so that K (x, z) = K0 (3) and vice versa = \u2212 x."}, {"heading": "2 Operator-valued Random Fourier Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Spectral representation of shift-invariant vector-valued Mercer kernels", "text": "The goal of this work is to build an approximate matrix in which the results are generally defined. (Look at the results). (Look at the results.) The goal of this work is to build an approximate matrix. (Look at the results.) The goal of this work is to create a generic matrix. (Look at the results.) The goal of this work is to create an approximate matrix. (Look at the results.) The goal of this work. (Look at the results.) The goal of this work. (Look at the results.) The goal of this work. (Look at the results.) The goal of this work is to create an approximate matrix. (Look at the results.) The goal of this work is to create an approximate matrix. (Look at the results.)"}, {"heading": "2.2 Construction of Operator Random Fourier Feature", "text": "Considering a shift-invariant Rp-Mercer kernel K on Rd, we create an Operator-Valued Random Fourier Feature (ORFF) map in three steps: 1) calculate C: Rd \u2192 L (Rp) from Gl. (6) using the inverse Fourier transformation (in the sense of sentence 2.4) of K0, the signature of K; 2) find A (\u03c9), p\u00b5 (\u03c9) and calculate B (\u03c9) so that A (\u03c9) = B (\u03c9) B (\u03c9) \u0445; 3) create a randomized characteristic map using the Monte Carlo sample from the probability measurement \u00b5 and application B."}, {"heading": "2.3 Monte-Carlo approximation", "text": "s get Dj = 1 Xj for the block matrix of size rD \u00b7 s by stacking the D matrices X1,. < XD of size r \u00b7 s. Starting from steps 1 and 2, for all j = 1,.., n, we found a decomposition A (Phenomenj) = B (Phenomenj) = B (Phenomenj). We then propose a randomized matrix diagram in which we either have a general analytical closed form or use a numerical decomposition. Denote p \u00b7 p \u00b2 the dimensioning of the matrix B (Phenomena diagram). We then propose a randomized matrix diagram in which x x x x x value is represented."}, {"heading": "3 Uniform error bound on ORFF approximation", "text": "We are interested in an increasing number of examples that show that all three kernels have the same rate as us. (...) We are interested in how close the approach K (x, z) is. (...) We are interested in how close the approach K (x, z) is. (...) We have a real matrix (x, z). (...) We have a real matrix (x, z). (...) We have a real matrix (x, z). (...) We have a study, like the uniform standard F (x, z). (...) We have a real matrix (x, z). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...)."}, {"heading": "3.1 Application to some operator-valued kernel", "text": "In the following, we first show how to bind the constant term bD in these three cases. \u2212 Then, we show the upper limits for the three cores evaluated by the operator, which we have used as examples. \u2212 Then, we make sure that the random variable \"A\" (\"A\") is defined as follows: for all, \"m\" 2, \"p,\" V\u00b5 \"[A (\" A \") m = p\" r, \"A\" rm \"for the matrix\" V\u00b5 \"[A (\" A \")] as follows: for all,\" m \"II\" {1, \"p,\" p, \"V\u00b5\" [A \"] m = p\" r \"= 1 Cov\u00b5\" [A \"r,\" A \"rm\" For a given, \"we define: bD (\" s \"z\"), we define the properties. \""}, {"heading": "4 Learning with ORFF", "text": "In practice, however, the existing limits are too great to find a safe value for D. In the following, numerical examples of ORFF approximations are presented."}, {"heading": "4.1 Penalized regression with ORFF", "text": "Once we have an approximate function board, we can use it to represent a function matrix of size p \u00b7 p with the matrix B \u00b7 p \u00b2 of size p \u00b7 p \u00b2 so that A (\u03c9) = B (\u043d) *. A function f \u00b2 HK is then represented by a linear model p \u00b2 (x) = \u03a6 (x). If there is a local loss function L: S \u00b2 R + and a \"2 penalty,\" we minimize the size L (xi, yi) \u00b2 Rd \u00b7 Rp, i = 1,.., N \u00b2 be a collection of i.i.d training examples. Given a local loss function L: R + and a \"2 penalty,\" we can have a minimize L (LS) = 1 N \u00b2 i \u00b2 i = 1 L (xi)."}, {"heading": "4.2 Numerical illustration", "text": "We present a few experiments to complete the theoretical contribution and to illustrate the behavior of ORFF regression. (FF) Other experimental results with intoxicating output data are shown in Appendix D.2.Datasets: the first dataset is the handwritten digit recognition dataset MNIST3 We select a series of 12,000 images and a test set of 10,000 images, the inputs are as vector xi [0, 255] 784 and the targets are integral between 0 and 9. First we scale the inputs so that they take values in [\u2212 1] 784. Then we include the targets represented by a unique binary vector of length 10. To predict classes, we use simplex coding method presented in Mroueh et al. (2012) The intuition behind simplex coding is to project the binary labels of the dimension p onto the most separated dimensions."}, {"heading": "5 Conclusion", "text": "We have introduced a general and versatile framework for approximating the operator-estimated kernel with operator random Fourier features. We have shown the uniform convergence of these approximations by demonstrating an imbalance of matrix concentration for limited and unlimited ORFFs. The temporal complexity of these approximations together with the linear learning algorithm make this implementation scalable with the number of data and therefore interesting compared to OVK regression. The numerical mapping shows the behavior expected by the theory. ORFFs are in particular a very promising approach in vector field learning or on loud data sets. Another appealing direction is the use of this archi-4 http: / / ta.twi.tudelft.nl / nw / users / gijzen / IDR.htmltecture to learn operator-estimated cores automatically by learning a mixture of ORFFs to select suitable kernel fast carang (a method closely related to the Yelang method of 2013)."}, {"heading": "A Reminder on Random Fourier Feature in the scalar case", "text": "Rahimi and Recht (2007) proved the uniform convergence of Random Fourier Feature (RFF) for a scalar shift of invariant kernel. Theorem A.1 (Uniform error bound for RFF, Rahimi and Recht (2007). Let C be a compact of the Rd of diameter l. Let k a shift invariant kernel, differentiable with a bounded first derivative and \u00b5 its normalized inverse Fourier transform. Let D the dimension of the Fourier feature vectors. Then, for the mapping \u03c6, described in section 2, we have: P {sup x, z, c) improved first derivative."}, {"heading": "B Proof of the uniform error bound for ORFF approxima-", "text": "In this section we establish a proof for the case of bounded ORFF as well as for the case of unbounded. We note that the FJ (x, z) -FJ (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x, z) -F (x) -Z) -F (x) -Z (x) -F (x, z) -F (x) -F (x) -Z) -F (x) -Z) -F (x) -Z) -Z (x) -Z) -F (x) -Z (x) -Z (-Z) -F (x) -F (x) -Z) -F (x)."}, {"heading": "C Application of the bounds to decomposable, curl-free, divergence-", "text": "Define the random matrix V\u00b5 [A (\u03c9)] as follows: \", m \u00b2,\" \".,.,.,.,.,.,\"., \".,.,\"., \".,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,\".,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,"}, {"heading": "D Additional information and results", "text": "D.1 Implementation detail For each \u03c9j \u0445 \u00b5, B (\u03c9j) should be a p-by-p \u2032 matrix, so that B (\u03c9j) B (\u03c9j) \u0445 = A (\u03c9j).In practice, it is prohibitively expensive to make a prediction y = h (x) using the formula h (x) = \u03a6 (x) \u043a directly. Indeed, it would cost O (Dp \u2032 p) to make a prediction, since \u0438 (x) is a p \u00b2 matrix.D 100 101 102 103 105Empirical upper limit variance 10-4 10-3 10-1 100 101 102 E rr orDecomposable kernel kernel D100 101 102 103 104Empirical upper limit variance 10-4 10-2 10-100-1 100 102 E rr orDecomposable kernel kernel kernel D100 101 102 102 103 104Empirical upper limit variance 10-4 10-2 10-100-101 102"}], "references": [{"title": "Kernels for vector-valued functions: a review", "author": ["M.A. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "\u00c1lvarez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2012}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F. Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2015\\E", "shortCiteRegEx": "Bach.", "year": 2015}, {"title": "Vector field learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "editors, ECML/PKDD,", "citeRegEx": "Baldassarre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2010}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning,", "citeRegEx": "Baldassarre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2012}, {"title": "Vector valued reproducing kernel hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanit\u00e0"], "venue": "Analysis and Applications,", "citeRegEx": "Carmeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carmeli et al\\.", "year": 2010}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C. Ong", "P. Gehler", "G. Pillonetto"], "venue": "In Proc. of the 28th Int. Conf. on Machine Learning,", "citeRegEx": "Dinuzzo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dinuzzo et al\\.", "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "A course in abstract harmonic analysis", "author": ["G.B. Folland"], "venue": "CRC press,", "citeRegEx": "Folland.,? \\Q1994\\E", "shortCiteRegEx": "Folland.", "year": 1994}, {"title": "Lsmr: An iterative algorithm for sparse least-squares problems", "author": ["D.C.-L. Fong", "M. Saunders"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fong and Saunders.,? \\Q2011\\E", "shortCiteRegEx": "Fong and Saunders.", "year": 2011}, {"title": "Refined Error Estimates for Matrix-Valued Radial Basis Functions", "author": ["E. Fuselier"], "venue": "PhD thesis,", "citeRegEx": "Fuselier.,? \\Q2006\\E", "shortCiteRegEx": "Fuselier.", "year": 2006}, {"title": "Random features for kernel deep convex network", "author": ["P.-S. Huang", "L. Deng", "M. Hasegawa-Johnson", "X. He"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Using the fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "In ISMB,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "A remark on low rank matrix recovery and noncommutative bernstein type inequalities. In From Probability to Statistics and Back: High-Dimensional Models and Processes\u2013A Festschrift in Honor of Jon A", "author": ["V. Koltchinskii"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Koltchinskii,? \\Q2013\\E", "shortCiteRegEx": "Koltchinskii", "year": 2013}, {"title": "Fastfood - computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarl\u00f3s", "A.J. Smola"], "venue": "In Proceedings of ICML 2013, Atlanta,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Operator-valued kernel-based vector autoregressive models for network inference", "author": ["N. Lim", "F. d\u2019Alch\u00e9-Buc", "C. Auliac", "G. Michailidis"], "venue": "Machine Learning,", "citeRegEx": "Lim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2015}, {"title": "Learning div-free and curl-free vector fields by matrix-valued kernels", "author": ["Y. Macedo", "R. Castro"], "venue": "Technical report,", "citeRegEx": "Macedo and Castro.,? \\Q2008\\E", "shortCiteRegEx": "Macedo and Castro.", "year": 2008}, {"title": "Matrix concentration inequalities via the method of exchangeable pairs", "author": ["L. Mackey", "M.I. Jordan", "R. Chen", "B. Farrel", "J. Tropp"], "venue": "The Annals of Probability,", "citeRegEx": "Mackey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2014}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M.A. Pontil"], "venue": "Neural Computation,", "citeRegEx": "Micchelli and Pontil.,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil.", "year": 2005}, {"title": "Matrix-valued kernels for shape deformation analysis", "author": ["M. Micheli", "J. Glaunes"], "venue": "Technical report, Arxiv report,", "citeRegEx": "Micheli and Glaunes.,? \\Q2013\\E", "shortCiteRegEx": "Micheli and Glaunes.", "year": 2013}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-j. Slotine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mroueh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mroueh et al\\.", "year": 2012}, {"title": "The matrix cookbook", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": "Technical University of Denmark,", "citeRegEx": "Petersen and Pedersen,? \\Q2008\\E", "shortCiteRegEx": "Petersen and Pedersen", "year": 2008}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS 20,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality", "author": ["V. Sindhwani", "H.Q. Minh", "A. Lozano"], "venue": "In Proc. of UAI\u201913,", "citeRegEx": "Sindhwani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2013}, {"title": "Idr (s): A family of simple and fast algorithms for solving large nonsymmetric systems of linear equations", "author": ["P. Sonneveld", "M.B. van Gijzen"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Sonneveld and Gijzen.,? \\Q2008\\E", "shortCiteRegEx": "Sonneveld and Gijzen.", "year": 2008}, {"title": "Optimal rates for random fourier features", "author": ["B. Sriperumbudur", "Z. Szabo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sriperumbudur and Szabo.,? \\Q2015\\E", "shortCiteRegEx": "Sriperumbudur and Szabo.", "year": 2015}, {"title": "On the error of random fourier features", "author": ["D.J. Sutherland", "J.G. Schneider"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Sutherland and Schneider.,? \\Q2015\\E", "shortCiteRegEx": "Sutherland and Schneider.", "year": 2015}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.-A. Tropp"], "venue": "Comput. Math.,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Modeling magnetic fields using gaussian processes", "author": ["N. Wahlstr\u00f6m", "M. Kok", "T. Sch\u00f6n", "F. Gustafsson"], "venue": "In in Proc. of the 38th ICASSP,", "citeRegEx": "Wahlstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wahlstr\u00f6m et al\\.", "year": 2013}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z. Zhou"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "A la carte - learning fast kernels", "author": ["Z. Yang", "A.J. Smola", "L. Song", "A.G. Wilson"], "venue": "CoRR, abs/1412.6493,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "A la carte - learning fast kernels", "author": ["Z. Yang", "A.G. Wilson", "A.J. Smola", "L. Song"], "venue": "In Proc. of AISTATS", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Refinement of operator-valued reproducing kernels", "author": ["H. Zhang", "Y. Xu", "Q. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Compared to the scalar case, the proof follows the same scheme as the one described in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015) but requires to consider matrix norms and appropriate matrix concentration inequality. The main feature of theorem 3.1 is that it covers the case of bounded ORFF as well as unbounded ORFF: in the case of bounded ORFF, a Bernstein inequality for matrix concentration such that the one proved in Mackey et al", "author": ["Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii,? \\Q2012\\E", "shortCiteRegEx": "Koltchinskii", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al.", "startOffset": 37, "endOffset": 65}, {"referenceID": 5, "context": "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al., 2011), vector field learning (Baldassarre et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 3, "context": ", 2011), vector field learning (Baldassarre et al., 2012) and vector autoregression (Sindhwani et al.", "startOffset": 31, "endOffset": 57}, {"referenceID": 22, "context": ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.", "startOffset": 34, "endOffset": 76}, {"referenceID": 14, "context": ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.", "startOffset": 34, "endOffset": 76}, {"referenceID": 17, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 4, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 0, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 21, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 13, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 29, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 24, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 1, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 25, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 13, "context": "RFFs has been shown to be efficient on large datasets and further improved by efficient matrix computations of FastFood (Le et al., 2013), and is considered as one of the best large scale implementations of kernel methods, along with N\u00ffstrom approaches (Yang et al.", "startOffset": 120, "endOffset": 137}, {"referenceID": 28, "context": ", 2013), and is considered as one of the best large scale implementations of kernel methods, along with N\u00ffstrom approaches (Yang et al., 2012).", "startOffset": 123, "endOffset": 142}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).", "startOffset": 128, "endOffset": 152}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).", "startOffset": 128, "endOffset": 232}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).", "startOffset": 128, "endOffset": 264}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).", "startOffset": 128, "endOffset": 355}, {"referenceID": 21, "context": "For the Gaussian kernel k(x \u2212 z) = exp(\u2212\u03b3\u2016x\u2212 z\u2016), the spectral distribution \u03bc(\u03c9) is Gaussian Rahimi and Recht (2007).", "startOffset": 93, "endOffset": 117}, {"referenceID": 4, "context": "We will define operator-valued kernel as reproducing kernels following the presentation of Carmeli et al. (2010). Given X and Y , a map K : X \u00d7 X \u2192 L(Y) is called a Y-reproducing kernel if", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K\u2217 xKz \u2200x, z \u2208 X HK = span {Kxy | \u2200x \u2208 X , \u2200y \u2208 Y} Another way to describe functions ofHK consists in using a suitable feature map.", "startOffset": 17, "endOffset": 39}, {"referenceID": 4, "context": "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K\u2217 xKz \u2200x, z \u2208 X HK = span {Kxy | \u2200x \u2208 X , \u2200y \u2208 Y} Another way to describe functions ofHK consists in using a suitable feature map. Proposition 1.1 (Carmeli et al. (2010)).", "startOffset": 18, "endOffset": 230}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).", "startOffset": 44, "endOffset": 278}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al.", "startOffset": 44, "endOffset": 523}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al. (2012) for our case of interest of X = R and Y = R.", "startOffset": 44, "endOffset": 544}, {"referenceID": 31, "context": "2Equation (36) in Zhang et al. (2012).", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "13 in Carmeli et al. (2010) to matrix-valued operators.", "startOffset": 6, "endOffset": 28}, {"referenceID": 4, "context": "13 in Carmeli et al. (2010) to matrix-valued operators. Proposition 2.2 (Carmeli et al. (2010)).", "startOffset": 6, "endOffset": 95}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1).", "startOffset": 26, "endOffset": 48}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. Then the function \u03a6 : R \u2192 L(R, \u03bc;R) defined for all x \u2208 R by \u2200y \u2208 R, (\u03a6xy) (\u03c9) = ei\u3008x,\u03c9\u3009B(\u03c9)\u2217y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, \u03a6x\u03a6z = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(\u03c9), \u03bc(\u03c9) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution.", "startOffset": 26, "endOffset": 868}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. Then the function \u03a6 : R \u2192 L(R, \u03bc;R) defined for all x \u2208 R by \u2200y \u2208 R, (\u03a6xy) (\u03c9) = ei\u3008x,\u03c9\u3009B(\u03c9)\u2217y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, \u03a6x\u03a6z = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(\u03c9), \u03bc(\u03c9) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution. Proposition 2.4 (Carmeli et al. (2010)).", "startOffset": 26, "endOffset": 960}, {"referenceID": 14, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.", "startOffset": 156, "endOffset": 184}, {"referenceID": 13, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.", "startOffset": 271, "endOffset": 296}, {"referenceID": 2, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al. (2012) for vector field learning.", "startOffset": 297, "endOffset": 323}, {"referenceID": 10, "context": "A usual choice is to choose k as a Gaussian kernel with k0(\u03b4) = exp ( \u2212 2 2\u03c32 ) , which gives \u03bc = N (0, \u03c3\u22122I) (Huang et al., 2013) as its inverse Fourier transform.", "startOffset": 110, "endOffset": 130}, {"referenceID": 5, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 22, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 14, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 3, "context": "If a graph coding for the proximity between tasks is known, then it is shown in Evgeniou et al. (2005); Baldassarre et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": "(2005); Baldassarre et al. (2010) that A can be chosen equal to the pseudo inverse L\u2020 of the graph Laplacian, and then the `2 norm in HK is a graph-regularizing penalty for the outputs (tasks).", "startOffset": 8, "endOffset": 34}, {"referenceID": 15, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 3, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 18, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 18, "context": "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al.", "startOffset": 46, "endOffset": 73}, {"referenceID": 27, "context": "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al., 2013).", "startOffset": 109, "endOffset": 133}, {"referenceID": 2, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p). Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al., 2013). These kernels discussed in Fuselier (2006) allow encoding input-dependent similarities between vector-fields.", "startOffset": 167, "endOffset": 492}, {"referenceID": 15, "context": "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).", "startOffset": 209, "endOffset": 260}, {"referenceID": 3, "context": "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).", "startOffset": 209, "endOffset": 260}, {"referenceID": 25, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 218, "endOffset": 281}, {"referenceID": 24, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 218, "endOffset": 281}, {"referenceID": 16, "context": "It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al., 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.", "startOffset": 298, "endOffset": 319}, {"referenceID": 26, "context": ", 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).", "startOffset": 76, "endOffset": 128}, {"referenceID": 25, "context": "; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).", "startOffset": 49, "endOffset": 81}, {"referenceID": 19, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 176, "endOffset": 200}, {"referenceID": 19, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.1.1. More interestingly, we propose a new bound for Operator Random Fourier Feature approximation in the general case. It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al.", "startOffset": 176, "endOffset": 801}, {"referenceID": 12, "context": "For the latter, we notice that their norms behave as subexponential random variables (Koltchinskii et al., 2013). Before introducing the new theorem, we give the definition of the Orlicz norm and subexponential random variables. Definition 3.1 (Orlicz norm). We follow the definition given by Koltchinskii et al. (2013). Let \u03c8 : R+ \u2192 R+ be a non-decreasing convex function with \u03c8(0) = 0.", "startOffset": 86, "endOffset": 320}, {"referenceID": 21, "context": "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.", "startOffset": 39, "endOffset": 63}, {"referenceID": 21, "context": "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.", "startOffset": 39, "endOffset": 99}, {"referenceID": 19, "context": "To predict classes, we use simplex coding method presented in Mroueh et al. (2012). The intuition behind simplex coding is to project the binarized labels of dimension p onto the most separated vectors on the hypersphere of dimension p \u2212 1.", "startOffset": 62, "endOffset": 83}], "year": 2016, "abstractText": "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operatorvalued kernels. We propose a general principle for Operator-valued Random Fourier \u2217ro.brault@telecom-paristech.fr \u2020florence.dalche@telecom-paristech.fr \u2021markus.o.heinonen@aalto.fi 1 ar X iv :1 60 5. 02 53 6v 3 [ cs .L G ] 2 4 M ay 2 01 6 Feature construction relying on a generalization of Bochner\u2019s theorem for translationinvariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets.", "creator": "LaTeX with hyperref package"}}}