{"id": "1605.02592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "GLEU Without Tuning", "abstract": "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.", "histories": [["v1", "Mon, 9 May 2016 14:05:57 GMT  (13kb,D)", "http://arxiv.org/abs/1605.02592v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["courtney napoles", "keisuke sakaguchi", "matt post", "joel tetreault"], "accepted": false, "id": "1605.02592"}, "pdf": {"name": "1605.02592.pdf", "metadata": {"source": "CRF", "title": "GLEU Without Tuning", "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "GLEU (Generalized Language Understanding Evaluation) 2 was designed and developed by using two sets of annotations as references, using a tunable weight to punish n-grams that should have been changed in the system output but remained unchanged (Napoles et al., 2015). Upon publication, it was found that the weight had to be retuned as the number of references changed. With more references, more variations of the sentence are seen, resulting in a larger set of reference n-grams. Larger sets of reference grams tend to have higher overlaps with the source programs, reducing the number of n-grams that were seen in the source but not in the reference. Therefore, the penalty expression is reduced and a large weight is needed for the penalty expression to have the same magnitude as the penalty if there are fewer references.Since a resetting of the weight for different sized reference sets is undesirable in 2007, we do not have a tunable to simplify GL1b and thus:"}, {"heading": "2 Modifications to GLEU", "text": "Our GLEU implementation differs from that of Napoles et al. (2015). As originally shown, GLEU doubles in calculating the accuracy of n-grams in the reference that does not appear in the source, and subtracts a weighted number of n-grams that occur in the source (S) and not in the reference (R). We use a modified version, GLEU +, which simplifies this. Precision is simply the number of reference n-grams matches that should have been changed in S but not punished.The precision term in Equation 1 is then used in the standard equation BLEU et al., 2002 Reference to get the GLEU + second, since the possible number of grams exceeds 1,000."}, {"heading": "3 Results", "text": "Using this revised version of GLEU, we have calculated the values for each system submitted to the CoNLL 2014-Shared Task on Grammatical Error Correction3 to update the results in Tables 4 and 5 of Napoles et al. (2015). GLEU +'s system ranking is compared with the originally reported GLEU (GLEU0), M2 and the human ranking (Table 1). 3http: / / www.comp.nus.edu.sg / \u02dc nlp / conll14st.htmlar Xiv: 160 5.02 592v 1 [cs.C L] 9M ay2 016On average, M2 systems rank within 3.4 places of the human ranking. Both GLEU values have narrower rankings on average: GLEU0 within 2.6 and GLEU + within 2.9 places of the human ranking. The correlation between the system values and the human ranking is shown in Table 2. GEU + GLEU is shown as the human correlation, with the human correlation being within 2.2."}, {"heading": "4 Conclusion", "text": "We recommend that you stop using the original GLEU because of the problems we identified in Section 1. Instead, you should use the updated version of GLEU that does not require tuning (GLEU +), which is available at https: / / github.com / cnap / gec-ranking."}], "references": [{"title": "Gleu: Automatic evaluation of sentence-level fluency", "author": ["Andrew Mutton", "Mark Dras", "Stephen Wan", "Robert Dale."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344\u2013351, Prague, Czech Repub-", "citeRegEx": "Mutton et al\\.,? 2007", "shortCiteRegEx": "Mutton et al\\.", "year": 2007}, {"title": "Ground truth for grammatical error correction metrics", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Napoles et al\\.,? 2015", "shortCiteRegEx": "Napoles et al\\.", "year": 2015}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 1, "context": "specific annotated errors (Napoles et al., 2015).", "startOffset": 26, "endOffset": 48}, {"referenceID": 1, "context": "unchanged (Napoles et al., 2015).", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Not to be confused with the method of the same name presented in Mutton et al. (2007). portable across comparisons against any number of references.", "startOffset": 65, "endOffset": 86}, {"referenceID": 1, "context": "Our GLEU implementation differs from that of Napoles et al. (2015). As originally presented, in computing n-gram precision, GLEU doublecounts n-grams in the reference that do not appear", "startOffset": 45, "endOffset": 67}, {"referenceID": 2, "context": "The precision term in Equation 1 is then used in the standard BLEU equation (Papineni et al., 2002) to get the GLEU+ score.", "startOffset": 76, "endOffset": 99}, {"referenceID": 1, "context": "Using this revised version of GLEU, we calculated the scores for each system submitted to the CoNLL 2014\u2013Shared Task on Grammatical Error Correction3 to update the results reported in Tables 4 and 5 of Napoles et al. (2015). The system ranking by GLEU+ is compared to the originally reported GLEU (GLEU0), M2, and the human ranking (Table 1).", "startOffset": 202, "endOffset": 224}], "year": 2016, "abstractText": "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.1", "creator": "LaTeX with hyperref package"}}}