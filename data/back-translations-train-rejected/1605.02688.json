{"id": "1605.02688", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Theano: A Python framework for fast computation of mathematical expressions", "abstract": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models.", "histories": [["v1", "Mon, 9 May 2016 18:32:34 GMT  (118kb,D)", "http://arxiv.org/abs/1605.02688v1", "19 pages, 5 figures"]], "COMMENTS": "19 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.SC cs.LG cs.MS", "authors": ["the theano development team: rami al-rfou", "guillaume alain", "amjad almahairi", "christof angermueller", "dzmitry bahdanau", "nicolas ballas", "fr\\'ed\\'eric bastien", "justin bayer", "anatoly belikov", "alexander belopolsky", "yoshua bengio", "arnaud bergeron", "james bergstra", "valentin bisson", "josh bleecher snyder", "nicolas bouchard", "nicolas boulanger-lewandowski", "xavier bouthillier", "alexandre de br\\'ebisson", "olivier breuleux", "pierre-luc carrier", "kyunghyun cho", "jan chorowski", "paul christiano", "tim cooijmans", "marc-alexandre c\\^ot\\'e", "myriam c\\^ot\\'e", "aaron courville", "yann n dauphin", "olivier delalleau", "julien demouth", "guillaume desjardins", "sander dieleman", "laurent dinh", "m\\'elanie ducoffe", "vincent dumoulin", "samira ebrahimi kahou", "dumitru erhan", "ziye fan", "orhan firat", "mathieu germain", "xavier glorot", "ian goodfellow", "matt graham", "caglar gulcehre", "philippe hamel", "iban harlouchet", "jean-philippe heng", "bal\\'azs hidasi", "sina honari", "arjun jain", "s\\'ebastien jean", "kai jia", "mikhail korobov", "vivek kulkarni", "alex lamb", "pascal lamblin", "eric larsen", "c\\'esar laurent", "sean lee", "simon lefrancois", "simon lemieux", "nicholas l\\'eonard", "zhouhan lin", "jesse a livezey", "cory lorenz", "jeremiah lowin", "qianli ma", "pierre-antoine manzagol", "olivier mastropietro", "robert t mcgibbon", "roland memisevic", "bart van merri\\\"enboer", "vincent michalski", "mehdi mirza", "alberto orlandi", "christopher pal", "razvan pascanu", "mohammad pezeshki", "colin raffel", "daniel renshaw", "matthew rocklin", "adriana romero", "markus roth", "peter sadowski", "john salvatier", "fran\\c{c}ois savard", "jan schl\\\"uter", "john schulman", "gabriel schwartz", "iulian vlad serban", "dmitriy serdyuk", "samira shabanian", "\\'etienne simon", "sigurd spieckermann", "s ramana subramanyam", "jakub sygnowski", "j\\'er\\'emie tanguay", "gijs van tulder", "joseph turian", "sebastian urban", "pascal vincent", "francesco visin", "harm de vries", "david warde-farley", "dustin j webb", "matthew willson", "kelvin xu", "lijun xue", "li yao", "saizheng zhang", "ying zhang"], "accepted": false, "id": "1605.02688"}, "pdf": {"name": "1605.02688.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov", "Alexander Belopolsky", "Yoshua Bengio", "Arnaud Bergeron", "James Bergstra", "Valentin Bisson", "Josh Bleecher Snyder", "Nicolas Bouchard", "Nicolas Boulanger-Lewandowski", "Xavier Bouthillier", "Alexandre de Br\u00e9bisson", "Olivier Breuleux", "Pierre-Luc Carrier", "Kyunghyun Cho", "Jan Chorowski", "Paul Christiano", "Tim Cooijmans", "Marc-Alexandre C\u00f4t\u00e9", "Myriam C\u00f4t\u00e9", "Aaron Courville", "Yann N. Dauphin", "Olivier Delalleau", "Julien Demouth", "Guillaume Desjardins", "Sander Dieleman", "Laurent Dinh", "M\u00e9lanie Ducoffe", "Vincent Dumoulin", "Samira Ebrahimi Kahou", "Dumitru Erhan", "Ziye Fan", "Orhan Firat", "Mathieu Germain", "Xavier Glorot", "Ian Goodfellow", "Matt Graham", "Caglar Gulcehre", "Philippe Hamel", "Iban Harlouchet", "Jean-Philippe Heng", "Bal\u00e1zs Hidasi", "Sina Honari", "Arjun Jain", "S\u00e9bastien Jean", "Kai Jia", "Mikhail Korobov", "Vivek Kulkarni", "Alex Lamb", "Pascal Lamblin", "Eric Larsen", "C\u00e9sar Laurent", "Sean Lee", "Simon Lefrancois", "Simon Lemieux", "Nicholas L\u00e9onard", "Zhouhan Lin", "Jesse A. Livezey", "Cory Lorenz", "Jeremiah Lowin", "Qianli Ma", "Pierre-Antoine Manzagol", "Olivier Mastropietro", "Robert T. McGibbon", "Roland Memisevic", "Bart van Merri\u00ebnboer", "Vincent Michalski", "Mehdi Mirza", "Alberto Orlandi", "Christopher Pal", "Razvan Pascanu", "Mohammad Pezeshki", "Colin Raffel", "Daniel Renshaw", "Matthew Rocklin", "Adriana Romero", "Markus Roth", "Peter Sadowski", "John Salvatier", "Fran\u00e7ois Savard", "Jan Schl\u00fcter", "John Schulman", "Gabriel Schwartz", "Iulian Vlad Serban", "Dmitriy Serdyuk", "Samira Shabanian", "\u00c9tienne Simon", "Sigurd Spieckermann", "S. Ramana Subramanyam", "Jakub Sygnowski", "J\u00e9r\u00e9mie Tanguay", "Gijs van Tulder", "Joseph Turian", "Sebastian Urban", "Pascal Vincent", "Francesco Visin", "Harm de Vries", "David Warde-Farley", "Dustin J. Webb", "Matthew Willson", "Kelvin Xu", "Lijun Xue", "Li Yao", "Saizheng Zhang", "Ying Zhang"], "emails": [], "sections": [{"heading": null, "text": "Theano: A Python framework for Fast Computation of Mathematical Expressions (The Theano Development Team) - Rami Al-Rfou, 6 Guillaume Alain, 1 Amjad Almahairi, 1 Christof Angermueller, 7, 8 Dzmitry Bahdanau, 1 Nicolas Ballas, 1 Fre \u0301 de \u0301 ric Bastien, 1 Justin Bayer, Anatoly Belikov, 9 Alexander Belopolsky, 10 Yoshua Bengio, 1, 3 Arnaud Bergeron, 1 James Bergstra, 1 Valentin Bisson, 1 Josh Bleecher Snyder, Nicolas Bouchard, 1 Nicolas Boulanger-Lewandowski, 1Xavier Bouthillier, 1 Alexandre de Bre Chibisson, 1 Olivier Breuleux, 1 Pierre-Luc Carrier, 1 Kyunghyun Cho, 1, 11Jan Chorowski, 1, 1 Paul Christiano, 13 Tim Cooijmans, 1, 14 Marc-Alexandre Co, 15 Myriville Daulain, Isalain 1 Isalain, Isaleson, Isaleson, Isaleson 1, Isaleson, Isaleson, 11 York, 1York, 111York, 1York, 1York, 1York, 1York."}, {"heading": "I. OVERVIEW", "text": "A. VisionTheano allows a user to symbolically define mathematical expressions and compile them in a highly optimized way, either on CPUs or GPUs (the latter with CUDA) 1, simply by changing a configuration flag. In addition, Theano can automatically compute symbolic differentiation of complex expressions, ignore the variables that are not required to compute final output, reuse partial results to avoid redundant calculations, apply mathematical simplifications, perform in-place operations if possible to minimize memory usage, and apply numerical stability optimization to overcome or minimize error due to hardware approximations. To achieve this, the mathematical expressions defined by the user are stored as a graph of variables and operations that are truncated and optimized during compilation time."}, {"heading": "B. Community", "text": "Theano is free open source software licensed under the new (3-clause) BSD license, and relies on a broad and very active community of developers and users around the world. Main channels of communication with developers are the project's GitHub page 2 for bug reports, feature requests and pull requests, and the theano-dev mailing list with 675 subscribers. Support for users is provided by the community at theano-users4 (more than 3,000 members) and on StackOverflow5 (more than 1,000 questions asked). PyPI6 has counted 38,000 downloads of theano packages in recent months. Since the migration of project development to GitHub in 2011, Theano has been redirected 1280 times. About 250 developers have actively contributed to the code base, and numerous others have played a role in the community, asking questions, answering or curating, helping to discuss development needs and documentation, tutorials, or even writing full-fledged software projects based on Theano."}, {"heading": "C. Software based on Theano", "text": "Several software packages have been developed to build on the strengths of Theano, with a superior user interface more suited to specific goals. For example, machine learning and deep learning packages such as Pylearn2 [8], Blocks [9], Lasagne [10] and Keras [11] have been developed with the aim of making the architecture of deep learning models easier to express and evaluating algorithms as mathematical expressions evaluated by Theano. Another example is PyMC3 [12], a probabilistic programming framework that Theano uses to automatically derive expressions for gradients and generate C code for quick execution."}, {"heading": "II. MAIN FEATURES", "text": "Theano defines a language to render and manipulate mathematical expressions (Section II A), a compiler to create functions that can calculate values for these expressions (Section II B), and a library that performs these functions1. Some OpenCL support is available in the new GPU backend, but it is still limited and experimental. 2 https: / / github.com / Theano / Theano / 3 https: / / groups.google.com / group / theano-dev / 4 https: / / groups.google.com / group / theano-users / 5 http: / / stackoverflow.com / questions / tagged / theano 6 https: / / pypi.python.org / pypi 7 For example, the deep learning tutorials at http: / / deeplearning.net / tutorial / 4 are evaluated using numerical values (Section II C)."}, {"heading": "A. Mathematical expressions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1. Graph structure", "text": "Theano represents symbolic mathematical expressions as directed, acyclic graphs. These graphs are also split into two and contain two types of nodes: \u2022 Variable nodes (or variables) that represent data, usually tensors; \u2022 Applying nodes that represent the application of mathematical operations. In practice, variables are used for graph inputs and outputs, as well as intermediate values. During the execution phase, values for input variables are provided and calculated for intermediate and output nodes. An application node has inputs and output nodes that are variable nodes; it represents the application of a mathematical operation (or op) on its input variables. A variable node can be input to multiple application nodes, but can be the output of at most one (graph inputs are not the result of any calculation).This corresponds to the individual static mapping (SA) form compiler design, which is a result of a variable that is just one."}, {"heading": "2. Building a graph", "text": "Because variables are heavily entered in Theano, the type of these variables must be specified when creating them, and by calling Python functions on variables, the user can then interact with them directly and naturally, as reflected under the hood by creating Apply nodes and new Variable nodes that expand the graph. The Tensor module exposes many of the functions NumPy provides for tensor operations to present the user with a familiar user interface, some of which insert a single Apply node and its output into the graph and return the output variable node, while others combine more complex graphs with Apply nodes that correspond to different ops so that the returned variable represents the expected result. It is also possible to clone existing graph or a part of it. In this case, an intermediate variable in the original graph could become a free input node or join another to a variable."}, {"heading": "3. Symbolic differentiation", "text": "A useful method for deriving gradients is to apply the chain rule backwards through the graph, from scalar costs to the inputs (or parameters). This procedure is called a gradient backspread or a backward or backward differentiation method. If, for example, we have three functions f: RM \u2192 R, g: RN \u2192 RM and C: RN \u2192 R, so that C (x) = f (g (x))), then: \"C\" x \"x\" x \"x\" \"x\" \"x\" \"x\" x \"x\" x \"x\" x \"x\" x \"x\" \"\" x \"\" \"x\" \"\" \"x\" \"x\" \"x\" x \"\" x \"\" \"x\" \"\" x \"\" \"x\" \"\" \"x\" \"\" \"x\" \"x\" x \"x\" x \"x\" x \"x x x x\" x x x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"\" x \"x\" x \"\" \"x\" x \"x\" x \"\" x \"x\" x \"\" x \"x\" \"\" x \"\" \"x\" x \"x\" \"x\" x \"x\" \"\" x \"\" x \"x\" \"x\" x \"x\" \"x\" x \"x\" \"x\" x \"\" x \"x\" x \"x\" x \"x\" \"\" x \"x\" x \"x\" x \"x\" \"x\" x \"x\" x \"x\" x \"\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \"x\" x \""}, {"heading": "4. Scan: Symbolic loops", "text": "Since the calculation graph is acyclic and its structure is fixed and independent of the actual data, it can be a challenge to express loops symbolically. An option, if the number of steps in the loop is set, is to explicitly unroll the loop and add to the calculation graph the calculation of each of the iterations several times. Unfortunately, this makes it impossible to iterate over a sequence of unknown lengths or iterate a variable number of times depending on the value of the data. To circumvent these problems, Theano implements a special op called Scan that abstracts the entire loop into a single Apply node in the diagram. This single node contains a complete calculation graph isolated from the main diagram that represents the calculation performed during each iteration of the loop. The scan node handles the communication between the external or outer calculation graph to which it belongs and the internal or inner diagram. It is also responsible for managing the loops between each one."}, {"heading": "B. The compilation phase", "text": "The compilation phase generates a theano function (a callable Python object) that is able to calculate values for certain symbolic output variables, given values for input variables. The set of input and output variables must be provided when the function is compiled, but the inputs do not have to be inputs for the full calculation graph, and the output variables do not have to be ultimate output variables. It is possible to compile a function that goes from some intermediate variables in the diagram to other intermediate variables, as long as the set of inputs contains all the information needed to compute the output graph. Multiple theano functions can be compiled, calculating different parts of the same calculation graph. During the compilation of a theano function, the relevant part of the calculation graph is cloned first, then it is rewritten by applying graph optimizations, and an optimized user graph is created next, and an optimized + C or C object is returned."}, {"heading": "1. Graph optimizations", "text": "The structure of the computation diagram makes it possible to replace parts of the graph. For example, a variation node, which is the output of a particular Apply node, could be replaced by the output of another Apply node, as long as they have the same type. Optimizations specify how the replacement of variables by other variables, which represent an equivalent computation, can be performed. Some of them are local, which means that they can only consider one Apply node and replace its output, some of them are global, and can examine the entire computation graph and perform arbitrary substitutions. Optimizations are mostly organized in the stages described below, even if there is some overlap. \u2022 Canonicalize put the graph into a canonical form to facilitate the task of subsequent optimizations (for example, X-components x-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-component-compon"}, {"heading": "2. Shared variables", "text": "Common variables are symbolic variables associated with persistent values that are shared between theano functions. They can only be input variables (not intermediate variables) because their value is not the result of calculating an Apply node. Common variables are implicit inputs to all theano functions that use them. When compiling a theano function, it is possible to specify update expressions for common variables. These expressions are symbolic variables that represent the new value to assign the common variables at the end of each function execution. They are implicit outputs of the function and are computed along with the other outputs before the value is updated. In some cases, such update rules allow the array to be updated in place rather than returning another array. It is also possible to explicitly assign a new value to an existing common variable, outside a theano variable, as long as it is compatible with its type. As the form of the type cannot be shared (if this is only a U at the end of the type)."}, {"heading": "3. C code compilation and caching", "text": "The code for calculating the output values for each op can be implemented either in Python or in C + + (or CUDA for GPU ops), using the C-API of Python and NumPy (and of CudaNdarray or GpuArray for GPU). After optimizing the function graph, each op generates the C + + or CUDA code for a Python module that implements this calculation (including reading and writing from the correct memory card), which is then compiled and imported. A persistent cache on disk allows to avoid generating code twice for the same op, and to avoid compiling when different ops generate the same code (for example, for the same operation applied to different data types or different dimensions)."}, {"heading": "C. Function execution", "text": "Theano includes a runtime machine that, when called, determines the calculation to be performed, on which data and in what order, and orchestrates its evaluation, originally done by forwarding graphs from input to output, requiring all branches to be evaluated before the output could be returned. Default runtime now uses a virtual machine (VM) system, which uses a data structure that contains pointers to store each variable (inputs and outputs of each Apply node), by ordering constraints, pointers to the functions that perform the calculations, and information about what has been calculated and needs to be calculated in the current call. If the execution speed is more important than memory usage, it is possible to hold on to results with references."}, {"heading": "D. Extending Theano", "text": "If the existing Theano library does not contain the operations required for a particular model, the framework has been designed for easy expandability; new ops can be written by specifying the type of their input and output variables and providing Python code to perform the evaluation; this Python code can use bindings to high-performance external libraries, such as Cython; methods can also be added to specify gradient expressions and the R operator (see Section II A 3) and form inferences; Theano's self-test functions can be used to validate output and test symbolic gradients against numerical evaluations among others; as mentioned above, operators can also be implemented directly in C + + or CUDA; the raw code can be delivered as a string that the Python code uses to generate the code used by the graph compiler."}, {"heading": "E. Related software", "text": "In fact, most of them are able to play by the rules that they need for their work, and they are able to play by the rules that they need for their work."}, {"heading": "III. NEW FEATURES", "text": "Several improvements have been made to Theano in recent years, especially with regard to faster execution, including support for more GPU operations and support for multiple GPUs (Section III A), faster graph optimization, especially for larger graphs (Section III B), and ease of use, with better error messages and tools for introspection, visualization, and debugging (Section III C)."}, {"heading": "1. Abstract Ops and 2D convolutions", "text": "A more detailed description of convolution operations can be found in [19]. Multiplying the available implementations for convolution (CPU-GEMM, GPU-cuDNN, GPU-GEMM, FFT,...) in Theano has increased the need for a flexible convolution interface that allows easy switching between these implementations, with each implementation having a different speed and memory trade-off, as well as different software dependencies. To meet this need, Theano 0.8 introduces abstract ops that unravel the interface of an op to its actual implementation. An abstract op introduces a placeholder application node into the graph that corresponds to a given operation that does not provide actual implementation."}, {"heading": "2. Using cuDNN", "text": "Efficient CUDA primitives for neural networks are implemented in the cuDNN library [20], in particular convolutions, pooling and their gradients. Multiple implementations of convolutions (and gradients) are provided with the same interface, with performance and memory usage depending on the actual shape of the data and filters. Since the best implementation can be different for different convolutions within the same model (depending on size) and on different hardware (depending on available memory), cuDNN also offers heuristics to guess the best given forms of the algorithm and select the different implementations (feasible given the available free memory) and the fastest. Theano includes cuDNN 2D and 3D convolutions and their gradients and provides options for selecting the algorithm to be used, either explicitly or using one of the following special values: \"guess _ on _ shape _ time,\" \"\" change _ pott. \""}, {"heading": "3. CNMeM integration", "text": "Another improvement in GPU performance is the integration of the CNMeM library, 11 and the use of the allocator and deallocator provided by it. The main problem was that cudaFree is called synchronously, forcing the synchronization of all streams on the device and waiting for their termination, severely limiting the potential for parallel execution of different kernels. An earlier option was to leave the memory for intermediate values between calls, as mentioned in Section II C, but the amount of memory normally available on GPU devices is limited.11 The original code is available at https: / / github.com / NVIDIA / cnmem, Theano contains a copy of it. 9 CNMeM works by allocating large memory pools using cudaMalloc, returning pieces of it when the allocator is called, and tracking which are released by its deallocator, allowing Theano to reserve a portion of memory from the GPU to begin using it."}, {"heading": "4. Improvements in Scan", "text": "The time to optimize and compile graphs containing Scan Apply nodes has been significantly shortened, and the execution time of the resulting function has been improved.The optimizations related to scan (displacement of the calculation from the loop, elimination of useless calculation) have been improved so that they can be applied faster. Additional optimizations have been added so that at each execution step more calculation can be shifted out of the loop to increase the execution speed.The scan execution backside has also been made more efficient by removing some of the accounting effort and the internal function is written directly into the correct output buffer at each execution step, instead of having to copy the intermediate results every time.The scan method has been rewritten so that it can be scaled more efficiently when a large number of input and output variables are present, and to generate a cleaner graph so that this cleaner optimization can be performed faster than the internal one."}, {"heading": "5. New gpuarray-based back-end", "text": "The most visible improvement is that it supports all common data types, rather than being limited to float32 data. Specifically, it supports semi-precise floating-point values (float16), which, like the previous backend, also supports views and advances to avoid copies and reusable memory whenever possible. Libgpuarray12 is a separate project aimed at providing an ndarray-like object on the GPU. It has a C interface so it can be reused in other projects that do not use Python. It also supports 64-bit indexing, so that arrays with more than 232 elements are supported. Another noticeable improvement is that we have basic support for OpenCL, but a significant portion of the GPU ops in Theano currently do not support it. This could be fixed by porting."}, {"heading": "6. Data parallelism with Platoon", "text": "To take advantage of multiple computer devices, there are two main approaches: model parallelism and data parallelism. Model parallelism consists in splitting the model itself into several parts and having these parts computed by different devices. 12 http: / / deeplearning.net / software / libgpuarray /, code available at https: / / github.com / Theano / libgpuarray10It requires careful consideration of the size of the parts and the cost of communication to ensure optimal performance. Data parallelism, on the other hand, involves splitting your input data into several parts and running multiple copies of the model. It requires attention to model synchronization so that the copies do not drift too far apart during training, and to the way of aggregating the results produced. Usually, data parallelisms are executed on a single machine, with multiple threads, but this approach is not practical in Python because of the Python GIL."}, {"heading": "B. Faster compilation of graphs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1. Faster, simpler optimizer", "text": "As mentioned in Section II B 1, some optimization sets are predefined and can be easily specified. One of these optimizers, \"fast _ compile,\" has recently been expanded to include the optimizations that transfer the calculation to a GPU, as well as the optimizations necessary to apply these optimizations, drastically reducing the optimization time of the graphs at the expense of slightly slower execution time and increased memory usage. This option can speed up the development or prototyping phase of a model and allow the developer to iterate faster."}, {"heading": "2. Swapping updates without recompiling", "text": "This can be useful when creating functions that are similar but use different shared variables or update parameters, for example when creating test and validation functions. Most importantly, the optimized graph of the original function is copied, which means that the compilation is done only once. Users can use the copy interface to specify which shared variables should be swapped and whether updates should be accepted or not. It is also possible for copied functions to share a cache in memory (memory that is not input or output), which, when combined with disabled garbage collection, can increase execution speed and save memory."}, {"heading": "3. Save and reload optimized graphs", "text": "Optimized calculation graphs, such as those in Theano functions, can now be serialized and deserialized using the Pickle module without being optimized again. It is possible to force the reoptimization, for example, if the number of available optional dependencies between saving and reloading has changed, in which case the function may not be executed (if a dependency has been removed) or be suboptimal (if one has been added), which is especially useful when checking and restoring ongoing experiments. Note that the C + + or CUDA code may still need to be recompiled. Visualization, debugging, and diagnostic tools Since the definition of theano functions is separate from their execution, some specific tools have been developed to help the user visualize parts or the entire calculation graph, locate the origin of errors, and understand what happens at runtime https: / giogio.com / 13."}, {"heading": "1. Interactive visualization with d3viz", "text": "Instead of printing a text representation (such as debug print) or creating a static image (such as Pydotprint), it creates an HTML file that can be opened with current web browsers. An example is Figure 1.Multiple features are supported: Users can zoom different regions, drag and drop diagrams, and position nodes manually or automatically; visualization can retrieve additional information about nodes and edges, such as their data type or definition in source code, edit node labels, and visualize profile information. Nested graphs such as OpFromGraph nodes can also be explored by expanding or shrinking the nodes as needed. Intern, d3viz represents a graph in the Graphviz DOT language, using the Pydot package and defining a frontend based on the jdj 3.G library that allows exporting any PNG format."}, {"heading": "2. Test values", "text": "Identifying errors in the way a mathematical expression is implemented in Theano can be a challenge, as it is not possible to map an intermediate variable node directly to the value that is assigned to it during execution. To mitigate this problem, it is possible to map a test value to input variables and automatically calculate values that are assigned to intermediate variables as soon as they are defined, making it easier to detect form errors, for example, or unexpected values. Note that these values are calculated only once when the graph is created, meaning that stability optimizations are not applied to these values, so that NaN values (not numbers) could be generated at this stage, even if they were not present when evaluating the optimized graph."}, {"heading": "3. NanGuardMode", "text": "A common symptom of problems with optimizing a model is the occurrence of NaN (no number), infinity, or very large values. They can point to a wide range of problems, such as the use of uninitialized memory, lack of numerical stability in the calculation, divergence of the algorithm itself. To facilitate diagnosis of the occurrence of such values, NanGuardMode is an instrumented version of the runtime environment that can check the values of the inputs and outputs of each Apply node during execution and issue an error when some problematic values are detected.12"}, {"heading": "4. The PdbBreakPoint Op", "text": "PdbBreakPoint is an op designed to check the value of a condition that is a symbolic expression during the execution of a Theano function. If the condition is met, the program falls into the Python debugger (pdb) and provides the values associated with a list of predefined monitored variables. This is especially useful when something goes wrong during the training of a model, but only after a number of iterations, so it is not practical to log all values constantly."}, {"heading": "5. Keeping the creation stack trace", "text": "When a variable is created, a portion of the stack trace character is recorded, especially the line of the call that created it. For example, if the variable z is created by calling z = a + b, then the line in which that expression is called is associated with z. If the evaluation of that expression fails, for example because a and b have incompatible shapes, then the error message mentions that file, line, and line number.One challenge of this mechanism is that when applying optimizations, the replacement variables are not created in the same place as the ones that replace them (or that \"correspond\" to them in the more general sense).In fact, they are created within the optimization so that they are not assigned stack tracking. For example, if the above expression is optimized to move a and b to a GPU, and z is replaced by host _ from _ gpu (gpu _ z), where gpz _ times we can override the original gu = gu _ padd, or if they flow into the original gpu _ b."}, {"heading": "IV. BENCHMARKS", "text": "This section aims to give a sense of the performance Theano could expect against some of its major competitors in machine learning research software on different types of models. We have already made some of the benchmarking codes public and will try to make the remaining code available in the future, with the goal of having more comprehensive benchmarks for a wider variety of models and frameworks easier to achieve for online projects that can provide a more up-to-date picture. Among these projects, we can cite Convnet benchmarks, 14 rnnbenchmarks, 15 and hopefully DeepMark16 in the future.We have benchmarked Theano against Torch and TensorFlow (Section IV A) on three types of popular machine learning models: convolutional networks (Section IV B), recursive neural networks (Section IV C) and recursive neural networks for sequence equencing (Section IV D), and finally, we will show how we measure the speed of calculating plates with different (IV D) sections."}, {"heading": "A. Setup", "text": "All benchmarks were executed on an NVIDIA Digits DevBox, with 4 Titan X GPUs and a Core i7-5930K CPU. All benchmarks other than data parallelism were executed on only one GPU that was not used for running the X server (using CUDA _ VISIBLE _ DEVICES). We used Cuda 7.5.17 with cuDNN v4 (version 4007) and data type float32, for all frameworks and all experiments. The comparison software was installed as follows: \u2022 Theano was installed from the development version, for Commit 1bd371c. The following configuration flags were used: floatX = float32, lib.cnmem = 0.45, device = gpu0, optimizer _ including = gpu0, optimizer _ including = unsafe, dnn.conv.algo _ fwd = time _ once, dnaln.cnmem = 0.45, device = gpu0, optimizer _ unsafe = unincluding _ dndndndndn.nnnnl option was installed for aldndnnnnnnn.httd."}, {"heading": "B. Convolutional networks", "text": "We measure the performance of four different folding models successfully used on the Imagenet dataset: \u2022 AlexNet, the single-column variant of [24], with a batch size of 128; \u2022 OverFeat, the fast variant of [25], with a batch size of 128; \u2022 VGG, also known as OxfordNet, Model A [26], with a batch size of 64; \u2022 GoogLeNet V1 [27], with a batch size of 128. We used the code from https: / github.com / soumith / convennet benchmarks for Commit 84b5bb1 for Theano, Torch and TensorFlow. We specify the processing time per minibatch, for forward and reverse gear. The results shown in Figure 2 show that Theano is slightly slower than Torch and TensorFlow, but performance is comparable for both forward and reverse gear."}, {"heading": "C. Recurrent neural networks: LSTM on Penn Treebank", "text": "To present recurring network models, we applied variants of the LSTM model to the Penn Treebank dataset described in [28]. We compared: \u2022 the Torch implementation available at https: / / github.com / wojzaremba / lstm; \u2022 the TensorFlow implementation available at https: / / www.tensorflow.org / versions / r0.8 / tutorials / recurrent /; 17 and \u2022 the Theano implementation available at https: / / github.com / caglar / rnn benchmarks. We measured words per second during the training and reported the results on the following models: \u2022 Small: Single Layer, 200 hidden units, sequence length: 20; \u2022 Medium: Single Layer, 600 hidden units, sequence length: 40; \u2022 Large: Two Layers, 650 hidden units, each sequence length: 50.All models used Thesorch as the model connection for 28, but the recurring connections were slightly smaller than the average size of the batch out."}, {"heading": "D. Sequence-to-sequence: Caption generation from video", "text": "In this section, we will use the sequence-to-sequence mapping model of [29]. Input consists of a series of video images and the output is an English one-sentence description of the input. Each input video image is pre-processed by a GoogLeNet, which has been pre-trained for classification on ImageNet. Therefore, the representation of the image is a vector of 1024. Therefore, all input is represented by (M, F, 1024), where M is the minibatch size and F is the number of images. Output size is (M, L), where M is the minibatch size and L is the record length (padding is used within a minibatch to ensure the same length, but different minibatches may have different L). Specifically, the model is written as P (S | V), an LSTM on the set of S, conditioned on the video V. V, is a weighted sum of image representations. The original code for [/ / yano] is not available to the public at http: / / gith.com."}, {"heading": "E. Data parallelism for LSTM", "text": "We are using the models from Section IV C again, this time with Platoon, to train on multiple GPUs on the same machine, using ASGD. We are reporting results for 2 GPUs (with devices gpu1 and gpu2) and 4 GPUs, compared to the results on 1 GPU that were achieved without Platoon and reported in Section IV C. We have measured the overall processing speed (words per second) during the training, when we synchronize the models after each minibatch, and when we synchronize only all 100 batches. Benchmarking code with Platoon will be released soon. Figure 5 shows a consistent increase in processing speed when we add more GPUs. As you can see on the left, communication and synchronization effort result in scaling being sublinear when we synchronize after each batch, we have a speed between 1.6 and 1.7 for 2 GPUs and about 3.2 for 4 GPUs in all three models."}, {"heading": "V. LIMITATIONS AND CHALLENGES", "text": "Despite the progress made in recent years and our best efforts, there are still some limitations or shortcomings in Theano. Some of these issues have been addressed by competing frameworks mentioned in Section II E, and by other projects such as CGT (Computation Graph Toolkit)."}, {"heading": "A. Limitations from Python", "text": "Since Theano uses Python as its core language and uses NumPy arrays and other Python objects to store values, it is affected by Python's limitations, the most important of which is the Python GIL, which restricts the simultaneous execution of threads. We have seen that it is possible to execute individual threads quickly by compiling binary modules that are then loaded into Python (Sections II B 3 and II C), and it would also be possible to release the GIL during the execution of these functions. However, the GIL needs to be acquired anew each time references to Python objects are added or removed when the Python and NumPy C API are used. Since the execution of such functions is usually quite short, most threads would spend their time waiting for the lock rather than performing actual calculations. Since Python has a concept of threads and expects it to be responsible for threads, it is also impossible to create different threads."}, {"heading": "B. Graph optimization time", "text": "The execution time of the optimization phase of graphs does not scale well with the graph size. Currently, it scales supra-linearly relative to the number of nodes. A problem is that some groups of local optimizations try to repeat over and over again until none of them can be applied anymore, and the graph stops changing. In practice, it can force a number of runs through the entire graph that become larger for larger graphs (the chances that some local optimizations will be applied somewhere are higher).One option would be to completely reorganize the existing optimizations so that they are easier to apply and in a fixed number of runs of the graph. For example, it might be possible to use a one- or two-pass optimization phase, as CGT does. Doing this without regressing stability optimizations could be a major project."}, {"heading": "C. Code compilation time", "text": "Currently, the same Theano Op can generate a large amount of different C + + or CUDA modules, depending on its compile properties, such as the data type of the inputs and outputs, whether it is executed on the spot, and other flags that determine its behavior. Compiling and loading these modules can take time and strain the file system. To mitigate these problems, in most cases it would be possible to pass this information dynamically at runtime, rather than writing it in the generated code. This approach is already used in the new back-end to determine which GPU should be used to execute a particular Apply node, but it could be generalized."}, {"heading": "D. Loops and control-flow structures", "text": "The use of scan for loops and the ifelse lazy op for conditionals has proven to be a useful way to express control flow operations. Given the increasing need for more flexibility (attention mechanisms, nested loops, recursive loops, shape changes between iterations of the same loop), we may need a more principled way to express these structures. An attractive option would be to use and merge switching nodes in the calculation graph, as in a data flow graph [13]. This is the approach of TensorFlow [5] for symbolic loops. In these circumstances, this would require additional support for cycles in the calculation graph, extend runtime to recalculate values within the loop, and rewrite all graph optimizations currently available for scan, including those that limit memory usage."}, {"heading": "E. Multi-node parallelism", "text": "Scaling model execution and training to multiple machines is beyond the scope of Theano's core, but additional packages could be designed to interact with Theano, as Platoon does for multiple GPUs in a single node. In fact, tools such as parameter servers and coordinators do not need to be Theano-specific and could be common for different frameworks. F. Improving memory usage Given the limited availability of built-in GPU memory, memory usage is often a bottleneck for machine learning algorithms. This can limit the size and modeling performance of trainable models and make GPU computing power insufficient, for example when batch sizes need to be reduced. In addition to storing intermediate values in a lower precision format (for example, storing data as a float 16 is supported in Theano's new GPU back-end), various options can be explored and combined: \u2022 Change the order of execution to reduce peak memory consumption."}, {"heading": "G. The future of gradient-based computation frameworks", "text": "Tools such as Theano and TensorFlow are compilers for mathematical expressions because they require the code (or computation graph) to be defined first and then executed. On the other hand, Torch works more like an interpreter: The computation takes place as soon as the expression is called. It might be interesting to find out how to apply JIT (just-in-time) compiler ideas to the computation graph to combine the immediate response and flexibility of an interpreter (including the use of control flow instructions such as when, during, directly from the language), and the performance gains of a compiler when a phrase needs to be evaluated multiple times. Most machine-learning frameworks can now share efficient implementations of GPU cores such as those published by NVIDIA (cuDNN) and Nervana. Graph optimizations could be another component shared between projects, perhaps through a common language, to define computation diagrams and optimizations."}, {"heading": "VI. CONCLUSION", "text": "Theano's ground-breaking ideas for efficient gradient-based computation, which are now part of most established machine learning research libraries, combine, for example, a high-grade scripting language with highly optimized computing cores, especially using GPUs, symbolic graphs, and symbolic differentiation. Some other features of Theano, such as rewriting and optimizing diagrams, and the automatic generation and compilation of cores, are also beginning to become more widespread. Continuous improvements have been made to Theano's functionality, ease of use, and performance, such as wrapping libraries like cuDNN, and integrating ideas that have been successfully explored and implemented by other frameworks, such as data parallelism and model parallelism for distributed computation. Computing power is on par with other major research software, such as Torch and TensorFlow.There are ways to improve Theano (and other frameworks as well) by drawing inspiration from other machine learning programs (sometimes)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We acknowledge the support of the following organizations for research funding and computer support: NSERC, Calcul Que \u0301 bec, Compute Canada, the Canada Research Chairs, and CIFAR.Lijun Xue, Qianli Ma, Ziye Fan, and Christof Angermueller Kevin Kazao Sergeao Razao Canada, the Canada Research Chairs, and CIFAR.Lijun Xue, Qianli Ma, Ziye Marc, and Christof Angermueller contributed to Theano through the Google Summer of Code program.The authors would like to thank all other advocates of Theano: Faruk Ahmed, Diogo Moitinho de Almeida, Hani Almousli, Andrea, Martin Andrews, John Arevalo, Martin Arjovsky, Kai Arulkumaran, Ben Athiwaratkun, bbabeshkin, Markus Beissinger, Sebastian Berg, Thierry Bertin-Mahieux, Lucas Beyer, Merlijn Blaauw, Jo Cagurje Cdescu, Bogoslav Kim, Bugoslav Doslabe."}], "references": [{"title": "Theano: A CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy) (2010).", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: Deep learning on GPUs with Python", "author": ["James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Olivier Breuleux", "Pascal Lamblin", "Razvan Pascanu", "Olivier Delalleau", "Guillaume Desjardins", "David Warde-Farley", "Ian J. Goodfellow", "Arnaud Bergeron", "Yoshua Bengio"], "venue": "Big Learning Workshop, NIPS (2011).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Theano: New features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS (2012).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "Big Learning Workshop, NIPS (2011).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "(2015), software available from tensorflow.org.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The NumPy array: A structure for efficient numerical computation", "author": ["Stefan van der Walt", "S. Chris Colbert", "Gael Varoquaux"], "venue": "Computing in Science and Eng. 13, 22\u201330 (2011).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "SciPy: Open source scientific tools for Python", "author": ["Eric Jones", "Travis Oliphant", "Pearu Peterson"], "venue": "(2001\u2013), [Online; accessed 2016- 04-19].", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Pylearn2: A machine learning research library", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "arXiv e-prints abs/1308.4214 (2013).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "arXiv e-prints abs/1506.00619 (2015).", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Lasagne: First release.", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly", "Jeffrey De Fauw", "Michael Heilman", "diogo", "Brian McFee", "Hendrik Weideman", "takacsg", "peterderivaz", "Jon", "instagibbs", "Dr. Kashif Rasul", "CongLiu", "Britefury", "Jonas Degrave"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet"], "venue": "https://github.com/fchollet/keras (2015).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic programming in Python using PyMC3", "author": ["John Salvatier", "Thomas V. Wiecki", "Christopher Fonnesbeck"], "venue": "PeerJ Computer Science 2, e55 (2016).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Dataflow architectures", "author": ["Arvind", "David E. Culler"], "venue": "Annual Review of Computer Science 1, 225\u2013253 (1986).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1986}, {"title": "Fast exact multiplication by the Hessian", "author": ["Barak A. Pearlmutter"], "venue": "Neural Computation 6, 147\u2013160 (1994).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv e-prints abs/1512.01274 (2015).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv e-prints abs/1408.5093 (2014).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "Workshop on Machine Learning Systems (LearningSys), NIPS (2015).", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems (2012) pp. 1097\u20131105.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A guide to convolution arithmetic for deep learning", "author": ["V. Dumoulin", "F. Visin"], "venue": "arXiv e-prints abs/1603.07285 (2016).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "cuDNN: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer"], "venue": "arXiv e-prints abs/1410.0759 (2014).", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A common GPU n-dimensional array for Python and C", "author": ["Fr\u00e9d\u00e9ric Bastien", "Arnaud Bergeron", "Andreas Kl\u00f6ckner", "Pascal Vincent", "Yoshua Bengio"], "venue": "Big Learning Workshop, NIPS (2011).", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le", "Andrew Y. Ng"], "venue": "Advances in Neural Information Processing Systems (2012) pp. 1223\u20131231.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning with elastic averaging SGD", "author": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"], "venue": "Advances in Neural Information Processing Systems (2015) pp. 685\u2013693.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "arXiv e-prints abs/1404.5997 (2014).", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "OverFeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv e-prints abs/1312.6229 (2013).", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv e-prints abs/1409.1556 (2014).", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Computer Vision and Pattern Recognition (CVPR) (2015).  19", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv e-prints abs/1409.2329 (2014).", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on (IEEE, 2015).", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Virtualizing Deep Neural Networks for Memory-Efficient Neural Network Design", "author": ["Minsoo Rhu", "Natalia Gimelshein", "Jason Clemons", "Arslan Zulfiqar", "Stephen W. Keckler"], "venue": "arXiv e-prints abs/1602.08124 (2016).", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Training deep nets with sublinear memory cost", "author": ["Tianqi Chen", "Bing Xu", "Chiyuan Zhang", "Carlos Guestrin"], "venue": "arXiv e-prints abs/1604.06174 (2016).", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Since its introduction in [1] it has been one of the most used CPU and GPU mathematical compilers \u2013 especially in the machine learning community [2] \u2013 and has shown steady performance improvements [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Since its introduction in [1] it has been one of the most used CPU and GPU mathematical compilers \u2013 especially in the machine learning community [2] \u2013 and has shown steady performance improvements [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Since its introduction in [1] it has been one of the most used CPU and GPU mathematical compilers \u2013 especially in the machine learning community [2] \u2013 and has shown steady performance improvements [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "Section IV compares the performance of Theano against Torch7 [4] and TensorFlow [5] on several machine learning models.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Section IV compares the performance of Theano against Torch7 [4] and TensorFlow [5] on several machine learning models.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Theano\u2019s API mimics NumPy [6, 7], a widely adopted Python library that provides an n-dimensional array data type and many functions for indexing, reshaping, and performing elementary computations (exp, log, sin, etc.", "startOffset": 26, "endOffset": 32}, {"referenceID": 6, "context": "Theano\u2019s API mimics NumPy [6, 7], a widely adopted Python library that provides an n-dimensional array data type and many functions for indexing, reshaping, and performing elementary computations (exp, log, sin, etc.", "startOffset": 26, "endOffset": 32}, {"referenceID": 7, "context": "For instance, machine learning and deep learning packages, such as Pylearn2 [8], Blocks [9], Lasagne [10], and Keras [11], have been developed with the goal of making it easier to express the architecture of deep learning models, and training algorithms, as mathematical expressions to be evaluated by Theano.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "For instance, machine learning and deep learning packages, such as Pylearn2 [8], Blocks [9], Lasagne [10], and Keras [11], have been developed with the goal of making it easier to express the architecture of deep learning models, and training algorithms, as mathematical expressions to be evaluated by Theano.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "For instance, machine learning and deep learning packages, such as Pylearn2 [8], Blocks [9], Lasagne [10], and Keras [11], have been developed with the goal of making it easier to express the architecture of deep learning models, and training algorithms, as mathematical expressions to be evaluated by Theano.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "For instance, machine learning and deep learning packages, such as Pylearn2 [8], Blocks [9], Lasagne [10], and Keras [11], have been developed with the goal of making it easier to express the architecture of deep learning models, and training algorithms, as mathematical expressions to be evaluated by Theano.", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "Another example is PyMC3 [12], a probabilistic programming framework that uses Theano to derive expressions for gradients automatically, and to generate C code for fast execution.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "This structure is similar to dataflow graphs [13], where Apply nodes would correspond to operations nodes (the only kind of nodes), and Variable nodes would correspond to arcs in the dataflow graph.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "This is the R-operator introduced by [14], and corresponds to the forward mode of", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "TensorFlow [5] has a core in C++ and includes most of the features from Theano, in particular the graph-compiling approach, and symbolic differentiation (on full layers as well as on elementary operations), all directly accessible from Python through the API.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Torch7 [4] has a different approach: it implements efficient CPU and GPU computation kernels in C and makes them available in Lua, but does not provide gradient expressions for elementary operations.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "MXNet [15] and Caffe [16], both written in C++, feature the same kind of higher-level layers as Torch.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "MXNet [15] and Caffe [16], both written in C++, feature the same kind of higher-level layers as Torch.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Neon10 and Chainer [17] are two other machine learning frameworks written in Python, with GPU kernels, that feature symbolic computation graphs and symbolic differentiation.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Convolution operations are at the core of Convolutional Neural Networks (CNNs) that have lead to spectacular advances in machine learning problem involving visual data [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "A more detailed description of the convolution operations can be found in [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Efficient CUDA primitives for neural networks are implemented in the cuDNN library [20], in particular convolutions, pooling, and their gradients.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Theano now features a new GPU backend based on libgpuarray [21].", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "One example of such a model is the two-stack variant of AlexNet [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Two ways of performing the updates on the central parameters are currently implemented: Asynchronous SGD (ASGD), similar to Downpour SGD [22], and Elastic Averaging SGD (EASGD) [23].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "Two ways of performing the updates on the central parameters are currently implemented: Asynchronous SGD (ASGD), similar to Downpour SGD [22], and Elastic Averaging SGD (EASGD) [23].", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "\u2022 AlexNet, the one-column variant from [24], with a batch size of 128; \u2022 OverFeat, the fast variant from [25], with a batch size of 128; \u2022 VGG, also known as OxfordNet, model A [26], with a batch size of 64; \u2022 GoogLeNet V1 [27], with a batch size of 128.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "\u2022 AlexNet, the one-column variant from [24], with a batch size of 128; \u2022 OverFeat, the fast variant from [25], with a batch size of 128; \u2022 VGG, also known as OxfordNet, model A [26], with a batch size of 64; \u2022 GoogLeNet V1 [27], with a batch size of 128.", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "\u2022 AlexNet, the one-column variant from [24], with a batch size of 128; \u2022 OverFeat, the fast variant from [25], with a batch size of 128; \u2022 VGG, also known as OxfordNet, model A [26], with a batch size of 64; \u2022 GoogLeNet V1 [27], with a batch size of 128.", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "\u2022 AlexNet, the one-column variant from [24], with a batch size of 128; \u2022 OverFeat, the fast variant from [25], with a batch size of 128; \u2022 VGG, also known as OxfordNet, model A [26], with a batch size of 64; \u2022 GoogLeNet V1 [27], with a batch size of 128.", "startOffset": 223, "endOffset": 227}, {"referenceID": 27, "context": "To showcase recurrent network models, we benchmarked variants of the LSTM model applied to the Penn Treebank dataset described in [28].", "startOffset": 130, "endOffset": 134}, {"referenceID": 27, "context": "All three models used dropout on non-recurrent connections during training, following [28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "In this section, we use the sequence-to-sequence mapping model from [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "The original code for [29] is available at https://github.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "One appealing way would be to use switch and merge Apply nodes in the computation graph, like in a dataflow graph [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "This is the approach taken by TensorFlow [5] for symbolic loops.", "startOffset": 41, "endOffset": 44}, {"referenceID": 29, "context": "This method has been successfully implemented by [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "This approach has been used in [31], and can be especially useful for fast operations that have large outputs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "[1] James Bergstra, Olivier Breuleux, Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio, \u201cTheano: A CPU and GPU math expression compiler,\u201d in Proceedings of the Python for Scientific Computing Conference (SciPy) (2010).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] James Bergstra, Fr\u00e9d\u00e9ric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier Delalleau, Guillaume Desjardins, David Warde-Farley, Ian J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Ronan Collobert, Koray Kavukcuoglu, and Cl\u00e9ment Farabet, \u201cTorch7: A matlab-like environment for machine learning,\u201d in Big Learning Workshop, NIPS (2011).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Stefan van der Walt, S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Eric Jones, Travis Oliphant, Pearu Peterson, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Ian J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Bart van Merri\u00ebnboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, and Yoshua Bengio, \u201cBlocks and Fuel: Frameworks for deep learning,\u201d arXiv e-prints abs/1506.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Sander Dieleman, Jan Schl\u00fcter, Colin Raffel, Eben Olson, S\u00f8ren Kaae S\u00f8nderby, Daniel Nouri, Daniel Maturana, Martin Thoma, Eric Battenberg, Jack Kelly, Jeffrey De Fauw, Michael Heilman, diogo149, Brian McFee, Hendrik Weideman, takacsg84, peterderivaz, Jon, instagibbs, Dr.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Fran\u00e7ois Chollet, \u201cKeras,\u201d https://github.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] John Salvatier, Thomas V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Arvind and David E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Barak A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang, \u201cMXNet: A flexible and efficient machine learning library for heterogeneous distributed systems,\u201d arXiv e-prints abs/1512.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell, \u201cCaffe: Convolutional architecture for fast feature embedding,\u201d arXiv e-prints abs/1408.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton, \u201cChainer: a next-generation open source framework for deep learning,\u201d in Workshop on Machine Learning Systems (LearningSys), NIPS (2015).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d in Advances in Neural Information Processing Systems (2012) pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer, \u201ccuDNN: Efficient primitives for deep learning,\u201d arXiv e-prints abs/1410.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Fr\u00e9d\u00e9ric Bastien, Arnaud Bergeron, Andreas Kl\u00f6ckner, Pascal Vincent, and Yoshua Bengio, \u201cA common GPU n-dimensional array for Python and C,\u201d in Big Learning Workshop, NIPS (2011).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Sixin Zhang, Anna E Choromanska, and Yann LeCun, \u201cDeep learning with elastic averaging SGD,\u201d in Advances in Neural Information Processing Systems (2015) pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Alex Krizhevsky, \u201cOne weird trick for parallelizing convolutional neural networks,\u201d arXiv e-prints abs/1404.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00ebl Mathieu, Rob Fergus, and Yann LeCun, \u201cOverFeat: Integrated recognition, localization and detection using convolutional networks,\u201d arXiv e-prints abs/1312.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Karen Simonyan and Andrew Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv e-prints abs/1409.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, \u201cGoing deeper with convolutions,\u201d in Computer Vision and Pattern Recognition (CVPR) (2015).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, \u201cRecurrent neural network regularization,\u201d arXiv e-prints abs/1409.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville, \u201cDescribing videos by exploiting temporal structure,\u201d in Computer Vision (ICCV), 2015 IEEE International Conference on (IEEE, 2015).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin, \u201cTraining deep nets with sublinear memory cost,\u201d arXiv e-prints abs/1604.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}