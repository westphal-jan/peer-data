{"id": "1605.02697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.", "histories": [["v1", "Mon, 9 May 2016 19:04:23 GMT  (4583kb,D)", "https://arxiv.org/abs/1605.02697v1", null], ["v2", "Thu, 24 Nov 2016 10:30:18 GMT  (3940kb,D)", "http://arxiv.org/abs/1605.02697v2", "Improved version, it also has a final table from the VQA challenge, and more baselines on DAQUAR"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["mateusz malinowski", "marcus rohrbach", "mario fritz"], "accepted": false, "id": "1605.02697"}, "pdf": {"name": "1605.02697.pdf", "metadata": {"source": "CRF", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "authors": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "emails": ["mmalinow@mpi-inf.mpg.de", "rohrbach@berkeley.edu", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": null, "text": "Keywords Computer Vision \u00b7 Scene Understanding \u00b7 Deep Learning \u00b7 Natural Language Processing \u00b7 Visual Turing Test \u00b7 Visual Question AnsweringMateusz Malinowski Max Planck Institute for Informatics Saarbru cken, Germany E-Mail: mmalinow @ mpi-inf.mpg.deMarcus Rohrbach UC Berkeley EECS and ICSI Berkeley, CA, United States E-Mail: rohrbach @ berkeley.eduMario Fritz Max Planck Institute for Informatics Saarbru cken, Germany E-Mail: mfritz @ mpi-inf.mpg.deLSTM LSTM LSTM LSTMWhat is behind the table? chairs windowLSTM < END > CNNFig. 1 Our Approach Ask Your Neurons to Questioning with a Recurrent Neural Network using Long Short Term Memory (LSTM)."}, {"heading": "1 Introduction", "text": "In fact, most people who are in a position to put themselves in the world are also put in a position to put themselves in the world, namely in another world in which they are able to put themselves in, in which they move. In fact, it is in such a way that they are able to understand the world they live in, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "2 Related Work", "text": "Since we have proposed a challenge and initial methods of answering questions about images of the real world [Malinowski and Fritz, 2014a, b, 2015, Malinowski et al., 2015], often referred to as \"Visual Question Answering,\" numerous follow-up papers have appeared. In the following, we will first discuss related tasks and sub-tasks, then early approaches to addressing a broader visual Turing test and dataset proposed for this purpose. Finally, we will discuss the relationship to our work.2.1 Convolutional Neural Networks for Visual Recognition One component of answering questions about images is to extract information from visual content. As the proposal of AlexNet [Krizhevsky et al., 2012], Convolutional Neural Networks (CNNs) are dominant and most successful approaches to extracting relevant representation from the image. CNNs learn representation directly from the raw image data and are trained on large image correlations, typically igeal."}, {"heading": "3 Ask Your Neurons", "text": "The answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the answer to the question of the answer to the answer to the question of the answer to the answer to the question of the answer to the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the question of the answer to the question of the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the answer to the question of the question of the answer to the question of the answer to the question of the answer to the answer to the question of the question of the answer to the answer to the answer to the question of the question of the answer to the answer to the question of the question of the question of the answer to the answer to the question of the answer to the question of the answer to the question of the question of the question of the answer to the question of the answer of the question of the question of the answer of the question of the answer of the question of the answer of the question of the answer of the question of the answer of the question of the answer of the question of the answer of the answer of the answer of the question of the question of the answer of the question of the answer of the answer of the answer of the question of the answer of the question of the question of the answer of the answer of the answer of the question of the question of the question of the question of the question of the question of the answer of the question of the question of the question of the question of the answer of the question of the answer of the question of the question of the answer of the question of the answer of the question of the question of the answer of"}, {"heading": "3.1.1 Long-Short Term Memory (LSTM)", "text": "As shown in Figure 3, the LSTM unit takes an input vector vt at each time step t and predicts an output word zt that has its latent hidden state ht.As discussed above, zt is a linear embedding of the corresponding response at. In contrast to a simple RNN unit, the LSTM unit additionally maintains a visual cell c, which makes it easier to learn long-term dynamics and significantly reduces the disappearance and exploding gradient problems [Hochreiter and Schmidhuber, 1997]. Specifically, we use the LSTM unit as described in Zaremba and Sutskever [2014]."}, {"heading": "3.2.1 Question encoders", "text": "(D). (D). (D). (D). (D). (D). D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "3.2.2 Visual encoders", "text": "The second important component of the encoder decoder architectures is visual representation. Convolutional Neural Networks (CNNs) have become a state-of-the-art framework that provides functions from images, and the typical protocol of using the visual models is to first pre-train them on the ImageNet dataset [Russakovsky et al., 2014], a large-scale recognition dataset, and then use them as input for the rest of the architecture. It is also possible to fine-tune the weights of the encoder to the task at hand. In our experiments, we use chronologically the oldest CNN architecture that is fully trained on ImageNet - a caffe implementation by AlexNet [Jia et al., 2014, Krizhevsky et al., 2012] - as well as the recently introduced deeper networks - Caffe implementations by GoogLeNet and VGG [Szegedy et al., 2014, Simonyan Zisserman, 2014]."}, {"heading": "3.2.3 Multimodal embedding", "text": "Similarly, visual encoders encode images as vectors. A multimodal merger module combines both vector spaces into another vector on the basis of which the answer is decoded. Let me (q) be a question presentation (BOW, CNN, LSTM, GRU), and (x) a representation of an image. Then, C (q) is a function that embeds both vectors. In this thesis, we will examine three multimodal embedding techniques: concatenation, elemental multiplication, and summing. Since the latter two techniques require compatibility in the number of feature components, we will use additional visual embedding matrixWve and R (q)."}, {"heading": "W (\u03a8(q) Wve\u03a6(x)) (15)", "text": "In Eq.14 we divide W into two matrices Wq and Wv, i.e. W = [Wq; Wv]. In Eq.15 it is an elementary multiplication. The similarity between Eq.14 and Eq.16 is interesting, since the latter is the first with weight distribution and additional decomposition into WWve."}, {"heading": "3.2.4 Answer decoders", "text": "The last component of our architecture (Figure 4) is a response decoder. Inspired by the work on the image description task [Donahue et al., 2015], we use an LSTM as a decoder that shares the parameters with the encoder. Alternatively, we can cast the answer problem into different classes as a classification task with answers. This approach has been extensively researched, particularly on VQA [Antol et al., 2015]."}, {"heading": "4 Analysis on DAQUAR", "text": "We compare different variants of our proposed model to previous work in Section 4.1. Also, in Section 4.2, we analyze how well questions can be answered without using the image to gain an understanding of prejudice in the form of prior knowledge and common sense. We provide a new human baseline for this task. In Section 4.3, we discuss ambiguities in how to answer tasks and analyze them further by introducing metrics that respond sensitively to these phenomena. Specifically, the WUPS score [Malinowski and Fritz, 2014a] is broadened to a consensus that considers multiple human responses."}, {"heading": "4.3.1 DAQUAR-Consensus", "text": "In order to examine the impact of consensus in answering the question, we asked several participants to answer the same question of the DAQUAR dataset, taking into account the image in question. We followed the same pattern as in the original data collection, where the answer was a set of words or numbers. We did not place any further limitations on the answers, extending the original data [Malinowski and Fritz, 2014a] to an average of 5 test responses per image and 0 50 100050100 Human consensus (50%), full consensus (100%). Left: consensus on all the data, right: consensus on the test data. On the x-axis: no consensus (0%), at least half consensus (50%), full consensus (100%). Results in%. Left: consensus on all the data, right: consensus on the test data. Question collected by 5 internal annotators. The annotators were first tested for their English skills so that they would be able to complete the task."}, {"heading": "4.3.2 Consensus Measures", "text": "While we must recognize inherent ambiguities in our task, we are looking for a metric that favors multiple answers that are generally considered preferred. We are making two suggestions: Average consensus. We are using our new annotation table, which contains multiple answers per question, to calculate an expected value in the evaluation: 1NK N = 1 K + 1 K + 1 K + 1 min (our answer is the k + 1 possible human answer that corresponds to the k + 1 interpretation of the question. Both answers are sentences of the words DAi + (a, t)} (17), where for the i-th questionnaire Ai is the answer generated by the architecture and T + 1 is the k-th possible human answer that corresponds to the k-th interpretation of the question. Both answers DAi and T + 1 are sentences of the words, and \u00b5 is a membership measure, for example WUP [Wu and Palmer, 1994]. We call this metric \"Average Consensus Metric\" (ACM + 1), we are + 1 + 1 + 2 + 2 + + + 2 + 3."}, {"heading": "4.3.3 Consensus results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Extended Experiments on DAQUAR", "text": "In this section, we will first expand our experiments with other basic methods, and next, we will be guided by the results shown in Section 7. We will show the results with the refined model in the context of a larger result on DAQUARA. To gain a better understanding of the effectiveness of our neural approach, we will refer to the results of other basic methods. Similar basic models have also been used in Antol et al. [2015] and Ren et al. [2015], although they seek either a different dataset or a different variant of the DAQUAR approach, we use the most common answers in education to answer each question."}, {"heading": "6 Analysis on VQA", "text": "In fact, we are able to outdo ourselves, \"he said in an interview with\" Welt am Sonntag \":\" It's not that we are able to outdo ourselves, but that we are able to outdo ourselves. \""}, {"heading": "6.2.1 CNN questions encoder", "text": "First, we examine different hyperparameters for CNNs to encode the question. First, we look at the length of the filter of the coiled core. We run the model over different core lengths from 1 to 4 (Table 10, left column). We note that increasing core lengths improves performance up to length 3 when performance levels are exceeded, so we use core length 3 in the following experiments for which CNN can be interpreted as a trigram model. We have also tried to run a few cores with different lengths at the same time. In Table 10 (right column), one view corresponds to a core length 1, two views correspond to two cores with length 1 and 2, three views correspond to length 1, 2 and 3, etc. However, we find that the best performance is still achieved with a single view and core length 3 or 4."}, {"heading": "6.2.2 BOW questions encoder", "text": "As an alternative to neural network encoders, we consider the BagOf Words approach (BOW), in which the question words are first mapped to a common embedding space and then summed up (Eq.13), i.e. we sum up the word We (word) to a point. Surprisingly, such a simple approach yields very competitive results (first row in Table 11) compared to the CNN encoding discussed in the previous section (second row). Encoding recurring questions. We examine two recurring question encoders, LSTM [Hochreiter and Schmidhuber, 1997] and a simpler GRU [Cho et al., 2014]. The last two rows of Table 11 show a slight advantage of using LSTM."}, {"heading": "6.2.3 Pre-trained words embedding", "text": "In all previous experiments, we collectively learn to embed transformation W e along with the entire architecture only on the VQA dataset. This means that we do not have the standard L2 normalConcatenation 47.21 52.39 Summation 40.67 53.27 Elementary Multiplication 49.50 52.70any means of dealing with unknown words in questions at test dates, apart from using a special token < UNK > to display such a class. To address this shortcoming, we examine the pre-trained word Embedding Transformation GLOVE [Pennington et al., 2014], which encodes question words (technically it maps a hot vector into a 300-dimensional real vector).This choice expands the vocabulary of question words to about 2 million words that have extracted a large body of web data - Common Crawl [Pennington et al., 2014] - which is used to embed VLOE."}, {"heading": "6.2.4 Top most frequent answers", "text": "Our experiments in Table 12 examine predictions with different numbers of response classes. We experiment with a shortening of 1000, 2000 or 4000 most common classes. For all question coders (and always using GLOVE Word Embedding) we find that a shortening of 2000 words is best, which seems to be a good compromise between response frequency and lack of memory."}, {"heading": "6.2.5 Summary Question-only", "text": "Using GLOVE word embedding, LSTM sentence encoding and the AccuracyAlexNet method 53.69 GoogLeNet 54.52 VGG-19 54.29 ResNet-152 55.52top 2000, we achieve an accuracy of 48.86%. In the remaining experiments, we use these settings for speech and answer encoding. 6.3 Vision and LanguageAlthough question models can answer a significant number of questions because they capture sound knowledge, we will now also look at the image on which the question is based."}, {"heading": "6.3.1 Multimodal fusion", "text": "To speed up the training, we combine the last unit of the question encoder with the visual encoder, as explicitly shown in Figure 4. In the experiments, we use concatenation, summation and elementary multiplication on the BOW speech encoder with GLOVE word embedding and features extracted from the VGG-19 network. Furthermore, we also examine the L2 normalization of the visual characteristics, which separates each feature vector by its L2 standard. Experiments show that normalization is crucial for good performance, especially for concatenation and summation. In the other experiments, we use summation."}, {"heading": "6.3.2 Questions encoders", "text": "Table 14 shows how well different question encoders blend in with the visual characteristics. We see that LSTM slightly outperforms two other encoders, GRU and CNN, while BOW remains the worst, confirming our results in our pure-language experiments with GLOVE and 2000 responses (Table 12, second column)."}, {"heading": "6.3.3 Visual encoders", "text": "Next, we correct the question encoder for LSTM and vary different visual encoders: Caffe variant of AlexNet [Krizhevsky et al., 2012], GoogLeNet [Szegedy et al., 2014], VGG-19 [Simonyan and Zisserman, 2014] and have recently introduced 152 layered ResNet (we use the Facebook implementation of He et al. [2015]). Table 15 confirms our hypothesis that stronger visual models perform better."}, {"heading": "6.3.4 Qualitative results", "text": "We show our best model of the VQA test set in Tables 30, 31, 32, 33. We show selected examples with \"yes / no,\" \"counting\" and \"what\" questions where we believe our model makes valid predictions. In addition, Table 33 shows predicted composite answers. 6.4 VQA summary results Table 16 summarizes our results on the VQA test set. We see that, on the one hand, methods that use contextual language information such as CNN and LSTM perform better, on the other hand, the addition of strong vision is crucial. In addition, we use the best found models to conduct experiments on the VQA test sets: Test-dev2015 and Test Standard. To prevent overuse, the latter limits the number of submissions to 1 per day and a total of 5 submissions. Here, we also examine the effect of larger data sets in which we first train on the training set and then 20 epochs on joint training, validation and training."}, {"heading": "7 State-of-the-art on VQA", "text": "In this section, we first place our findings on VQA in a broader context by comparing our refined version of Ask Your Neurons with other publicly available approaches. Table 18 compares our Refined Ask Your Neurons model with other approaches. Some methods, like our approach, use global image representation, other attention mechanisms, but also other dynamic predictions of question weights, external textual sources, or merge the representation of compositional questions with neural networks. Table 18 shows some trends. First, better visual representation helps significantly (Table 15). Most leading approaches to VQA also use variants of ResNet, which is one of the strongest approaches to the task of image classification. However, it is important to normalize visual features (Table 13). In addition, all the best models use an explicit attention mechanism (e.g. DMN +, FDA, POSTECH, MCB, HieAtt)."}, {"heading": "8 Conclusions", "text": "A variant of our model, which does not use the image to answer the question, already explains a substantial part of the overall performance and helps to understand the contribution of visual features in this task. By comparing it to the human baseline, where the image is not shown to the human being to answer the question, we conclude that this language-only model has learned biases and patterns that can be considered as forms of common sense and prior knowledge that are also used by humans to accomplish this task. We have expanded our existing DAQUAR dataset to the new DAQUAR Consensus, which now provides multiple reference responses that allow us to study interpersonal agreement and consensus on the question when answering the task. We propose two new metrics: \"Average Consensus,\" which takes into account human disagreements."}], "references": [{"title": "Multi-cue zero-shot learning with strong supervision", "author": ["Zeynep Akata", "Mateusz Malinowski", "Mario Fritz", "Bernt Schiele"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Akata et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Akata et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Berant and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Xplore-m-ego: Contextual media retrieval using natural language queries", "author": ["Sreyasi Nag Chowdhury", "Mateusz Malinowski", "Andreas Bulling", "Mario Fritz"], "venue": "In ACM International Conference on Multimedia Retrieval (ICMR),", "citeRegEx": "Chowdhury et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chowdhury et al\\.", "year": 2016}, {"title": "A coefficient of agreement for nominal scales", "author": ["Jacob Cohen"], "venue": "Educational and psychological measurement,", "citeRegEx": "Cohen,? \\Q1960\\E", "shortCiteRegEx": "Cohen", "year": 1960}, {"title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability", "author": ["Joseph L Fleiss", "Jacob Cohen"], "venue": "Educational and psychological measurement,", "citeRegEx": "Fleiss and Cohen.,? \\Q1973\\E", "shortCiteRegEx": "Fleiss and Cohen.", "year": 1973}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In Proceedings of the National Academy of Sciences. National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Segmentation from natural language expressions", "author": ["Ronghang Hu", "Marcus Rohrbach", "Trevor Darrell"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Natural language object retrieval", "author": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": "arXiv:1604.01485, 2016", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Compositional memory for visual question answering", "author": ["Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Referit game: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L. Berg"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Hadamard product for lowrank bilinear pooling", "author": ["Jin-Hwa Kim", "Kyoung Woon On", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1610.04325,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "What are you talking about? text-to-image coreference", "author": ["Chen Kong", "Dahua Lin", "Mohit Bansal", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Krishnamurthy and Kollar.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Learning Semantics (NIPS workshop),", "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Hard to cheat: A turing test based on answering questions about images", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "AAAI Workshop: Beyond the Turing Test,", "citeRegEx": "Malinowski and Fritz.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2015}, {"title": "Tutorial on answering questions about images with deep learning", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1610.01076,", "citeRegEx": "Malinowski and Fritz.,? \\Q2016\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze.,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Cynthia Matuszek", "Nicholas Fitzgerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Fine-grained semantic typing of emerging entities", "author": ["Ndapandula Nakashole", "Tomasz Tylenda", "Gerhard Weikum"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Nakashole et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2013}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["Bryan Plummer", "Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2016}, {"title": "Highway networks for visual question answering", "author": ["Aaditya Prakash", "James Storer"], "venue": null, "citeRegEx": "Prakash and Storer.,? \\Q2016\\E", "shortCiteRegEx": "Prakash and Storer.", "year": 2016}, {"title": "Grounding Action Descriptions in Videos", "author": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Regneri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Regneri et al\\.", "year": 2013}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "A dataset for movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Berg and Fei.Fei.,? \\Q2014\\E", "shortCiteRegEx": "Berg and Fei.Fei.", "year": 2014}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "arXiv preprint arXiv:1606.06108,", "citeRegEx": "Saito et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saito et al\\.", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Shih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shih et al\\.", "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["Nathan Silberman", "Derek Hoiem", "Pushmeet Kohli", "Rob Fergus"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Movieqa: Understanding stories in movies through questionanswering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Sequence to sequence \u2013 video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wu and Palmer.,? \\Q1994\\E", "shortCiteRegEx": "Wu and Palmer.", "year": 1994}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": null, "citeRegEx": "Xu and Saenko.,? \\Q2015\\E", "shortCiteRegEx": "Xu and Saenko.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C Berg", "Tamara L Berg"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Modeling context in referring expressions", "author": ["Licheng Yu", "Patrick Poirson", "Shan Yang", "Alexander C Berg", "Tamara L Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "Learning the visual interpretation of sentences", "author": ["C Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 30, "context": "Most prominently Convolutional Neural Networks have raised the bar on image classification tasks [Krizhevsky et al., 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al.", "startOffset": 97, "endOffset": 122}, {"referenceID": 13, "context": ", 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al.", "startOffset": 44, "endOffset": 78}, {"referenceID": 59, "context": ", 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al., 2014].", "startOffset": 174, "endOffset": 198}, {"referenceID": 21, "context": "Most recently, these two trends of employing neural architectures have been combined fruitfully with methods that can generate image [Karpathy and Fei-Fei, 2015] and video descriptions [Venugopalan et al.", "startOffset": 133, "endOffset": 161}, {"referenceID": 42, "context": "We also frame the multimodal approach to answer questions about images that combines LSTM with CNN [Malinowski et al., 2015] as a special instance of an encoder-decoder framework.", "startOffset": 99, "endOffset": 124}, {"referenceID": 42, "context": "In Section 3, we present our novel approach based on recurrent neural networks for the challenging task of answering questions about images, which we presented originally in Malinowski et al. [2015]. The approach combines a CNN with an LSTM into an end-to-end architecture that predicts answers conditioning on a question and an image.", "startOffset": 174, "endOffset": 199}, {"referenceID": 30, "context": "Since the proposal of AlexNet [Krizhevsky et al., 2012], Convolutional Neural Networks (CNNs) have become dominant and most successful approaches to extract relevant representation from the image.", "startOffset": 30, "endOffset": 55}, {"referenceID": 30, "context": "Specifically, we evaluate AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], GoogleNet [Szegedy et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 58, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], GoogleNet [Szegedy et al.", "startOffset": 13, "endOffset": 43}, {"referenceID": 12, "context": ", 2014], and ResNet [He et al., 2015].", "startOffset": 20, "endOffset": 37}, {"referenceID": 43, "context": "The first approach is to encode all words of the question as a Bag-Of-Words [Manning and Sch\u00fctze, 1999], and hence ignoring an order in the sequence of words.", "startOffset": 76, "endOffset": 103}, {"referenceID": 13, "context": "In this work we investigate a Bag-Of-Words (BOW), a CNN, and two RNN variants (LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al.", "startOffset": 84, "endOffset": 118}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task.", "startOffset": 77, "endOffset": 477}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task. Weston et al. [2014] propose different kind of network - memory networks - that is used to answer questions about short stories.", "startOffset": 77, "endOffset": 616}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task. Weston et al. [2014] propose different kind of network - memory networks - that is used to answer questions about short stories. In their work, all the parts of the story are embedded into different \u201cmemory cells\u201d, and next a network is trained to attend to relevant cells based on the question and decode an answer from that. A similar idea has also been applied to question answering about images, for instance by Yang et al. [2015].", "startOffset": 77, "endOffset": 1030}, {"referenceID": 36, "context": ", 2015] or a CNN [Ma et al., 2016].", "startOffset": 17, "endOffset": 34}, {"referenceID": 62, "context": "Following Xu et al. [2015], who proposed to use spatial attention for image description, Yang et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 62, "context": "Following Xu et al. [2015], who proposed to use spatial attention for image description, Yang et al. [2015], Xu and Saenko [2015], Zhu et al.", "startOffset": 10, "endOffset": 108}, {"referenceID": 62, "context": "[2015], Xu and Saenko [2015], Zhu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 62, "context": "[2015], Xu and Saenko [2015], Zhu et al. [2016], Chen et al.", "startOffset": 8, "endOffset": 48}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al.", "startOffset": 8, "endOffset": 47}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al. [2016] predict a latent weighting (attention) of spatially localized images features (typically a convolutional layer of the CNN) based on the question.", "startOffset": 8, "endOffset": 68}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al. [2016] predict a latent weighting (attention) of spatially localized images features (typically a convolutional layer of the CNN) based on the question. The weighted image representation rather than the full frame feature representation is then used as a basis for answering the question. In contrast to the previous models using attention, Dynamic Memory Networks (DMN) [Kumar et al., 2015, Xiong et al., 2016] first pass all spatial image features through a bi-directional GRU that captures spatial information from the neighboring image patches, and next retrieve an answer from a recurrent attention based neural network that allows to focus only on a subset of the visual features extracted in the first pass. Another interesting direction has been taken by Ilievski et al. [2016] who run state-of-the-art object detector of the classes extracted from the key words in the question.", "startOffset": 8, "endOffset": 847}, {"referenceID": 64, "context": "Wu et al. [2016] argue for an approach that first represents an image as an intermediate semantic attribute representation, and next query external knowledge sources based on the most prominent attributes and relate them to the question.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences.", "startOffset": 34, "endOffset": 425}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences. Related to the Visual Turing Test, Malinowski and Fritz [2014c] have also combined a neural based representation with the compositionality of the language for the text-to-image retrieval task.", "startOffset": 34, "endOffset": 582}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences. Related to the Visual Turing Test, Malinowski and Fritz [2014c] have also combined a neural based representation with the compositionality of the language for the text-to-image retrieval task. Dynamic parameters. Noh et al. [2015] have an image recognition network and a Recurrent Neural Network (GRU) that dynamically change the parameters (weights) of visual representation based on the question.", "startOffset": 34, "endOffset": 749}, {"referenceID": 57, "context": "5 thousands manually annotated question-answer pairs about 1449 indoor scenes [Silberman et al., 2012].", "startOffset": 78, "endOffset": 102}, {"referenceID": 34, "context": "All are based on MS-COCO [Lin et al., 2014].", "startOffset": 25, "endOffset": 43}, {"referenceID": 27, "context": "[2015] have taken advantage of the existing annotations for the purpose of image description generation task and transform them into question answer pairs with the help of a set of hand-designed rules and a syntactic parser [Klein and Manning, 2003].", "startOffset": 224, "endOffset": 249}, {"referenceID": 3, "context": "Finally and currently the most popular, large scale dataset on question answering about images is VQA [Antol et al., 2015].", "startOffset": 102, "endOffset": 122}, {"referenceID": 9, "context": "In parallel to our work, Geman et al. [2015] developed another variant of the Visual Turing Test.", "startOffset": 25, "endOffset": 45}, {"referenceID": 9, "context": "Gao et al. [2015] have annotated about 158k images with 316k Chinese question answer pairs together with their corresponding English translations.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Gao et al. [2015] have annotated about 158k images with 316k Chinese question answer pairs together with their corresponding English translations. Ren et al. [2015] have taken advantage of the existing annotations for the purpose of image description generation task and transform them into question answer pairs with the help of a set of hand-designed rules and a syntactic parser [Klein and Manning, 2003].", "startOffset": 0, "endOffset": 165}, {"referenceID": 75, "context": "The Visual7W [Zhu et al., 2016] extends canonical question and answer pairs with additional groundings of all objects appearing in the questions and answers to the image by annotating the correspondences.", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer.", "startOffset": 34, "endOffset": 54}, {"referenceID": 70, "context": "Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers.", "startOffset": 28, "endOffset": 45}, {"referenceID": 50, "context": "The task requires inferring the past, describing the present and predicting the future in a diverse set of video description data ranging from cooking videos [Regneri et al., 2013] over web videos [Trecvid, 2014] to movies [Rohrbach et al.", "startOffset": 158, "endOffset": 180}, {"referenceID": 35, "context": "ing datasets [Malinowski and Fritz, 2014a,b, 2015], Yu et al. [2015] have simplified the evaluation even further by introducing Visual Madlibs - a multiple choice question answering by filling the blanks task.", "startOffset": 14, "endOffset": 69}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system.", "startOffset": 35, "endOffset": 527}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system. This work focuses on a user centric, dynamic scenario, where the provided answers are conditioned not only on questions but also on the geographical position of the questioner. Moving from asking questions about images to questions about video enhances typical questions with temporal structure. Zhu et al. [2015] propose a task which requires to fill in blanks the captions associated with videos.", "startOffset": 35, "endOffset": 975}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system. This work focuses on a user centric, dynamic scenario, where the provided answers are conditioned not only on questions but also on the geographical position of the questioner. Moving from asking questions about images to questions about video enhances typical questions with temporal structure. Zhu et al. [2015] propose a task which requires to fill in blanks the captions associated with videos. The task requires inferring the past, describing the present and predicting the future in a diverse set of video description data ranging from cooking videos [Regneri et al., 2013] over web videos [Trecvid, 2014] to movies [Rohrbach et al., 2015b]. Tapaswi et al. [2016] propose MovieQA, which requires to understand long term connections in the plot of the movie.", "startOffset": 35, "endOffset": 1331}, {"referenceID": 42, "context": "The original version of this work [Malinowski et al., 2015] belongs to the category of \u201cDeep Neural Approaches with full frame CNN\u201d, and is among the very first methods of this kind (Section 3.", "startOffset": 34, "endOffset": 59}, {"referenceID": 42, "context": "We extend [Malinowski et al., 2015] by introducing a more general and modular encoder-decoder perspective (Section 3.", "startOffset": 10, "endOffset": 35}, {"referenceID": 3, "context": "Finally, we transfer lessons learnt from VQA [Antol et al., 2015] to DAQUAR [Malinowski and Fritz, 2014a], showing a significant improvement on this challenging task (Section 6).", "startOffset": 45, "endOffset": 65}, {"referenceID": 32, "context": "More precisely, Ask Your Neurons is a deep network built of CNN [LeCun et al., 1998] and Long-Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997].", "startOffset": 64, "endOffset": 84}, {"referenceID": 13, "context": ", 1998] and Long-Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997].", "startOffset": 42, "endOffset": 76}, {"referenceID": 13, "context": "This allows to learn long-term dynamics more easily and significantly reduces the vanishing and exploding gradients problem [Hochreiter and Schmidhuber, 1997].", "startOffset": 124, "endOffset": 158}, {"referenceID": 13, "context": "This allows to learn long-term dynamics more easily and significantly reduces the vanishing and exploding gradients problem [Hochreiter and Schmidhuber, 1997]. More precisely, we use the LSTM unit as described in Zaremba and Sutskever [2014]. With the sigmoid nonlinearity \u03c3 : R 7\u2192 [0, 1], \u03c3(v) = (1 + e\u2212v) and the hyperbolic tangent nonlinearity \u03c6 : R 7\u2192 [\u22121, 1], \u03c6(v) = ev\u2212e\u2212v ev+e\u2212v = 2\u03c3(2v) \u2212 1, the LSTM updates for time step t given inputs vt, ht\u22121, and the memory cell ct\u22121 as follows:", "startOffset": 125, "endOffset": 242}, {"referenceID": 13, "context": "In this work, we investigate a few encoders within such spectrum that are compatible with the proposed Deep Learning approach: Two recurrent question encoders, LSTM [Hochreiter and Schmidhuber, 1997] (see Section 3.", "startOffset": 165, "endOffset": 199}, {"referenceID": 35, "context": "Such an encoder can range from a very structured ones like Semantic Parser used in Malinowski and Fritz [2014a] and Liang et al.", "startOffset": 83, "endOffset": 112}, {"referenceID": 32, "context": "Such an encoder can range from a very structured ones like Semantic Parser used in Malinowski and Fritz [2014a] and Liang et al. [2013] that explicitly model compositional nature of the question, to orderless Bag-Of-Word (BOW) approaches that merely compute a histogram over the question words (Figure 6).", "startOffset": 116, "endOffset": 136}, {"referenceID": 69, "context": "1 and Yang et al. [2015] for details.", "startOffset": 6, "endOffset": 25}, {"referenceID": 47, "context": "We either learn them jointly with the whole model or use GLOVE [Pennington et al., 2014] in our experiments.", "startOffset": 63, "endOffset": 88}, {"referenceID": 20, "context": "Convolutional Neural Networks (CNN) have been proposed to encode language [Kim, 2014, Kalchbrenner et al., 2014, Ma et al., 2016, Yang et al., 2015] and since have shown to be fast to compute and result in good accuracy. Since they consider a larger context, they arguably maintain more structure than BOW, but do not model such long term dependencies as recurrent neural networks. Figure 5 depicts our CNN architecture, which is very similar to Ma et al. [2016] and Yang et al.", "startOffset": 86, "endOffset": 463}, {"referenceID": 20, "context": "Convolutional Neural Networks (CNN) have been proposed to encode language [Kim, 2014, Kalchbrenner et al., 2014, Ma et al., 2016, Yang et al., 2015] and since have shown to be fast to compute and result in good accuracy. Since they consider a larger context, they arguably maintain more structure than BOW, but do not model such long term dependencies as recurrent neural networks. Figure 5 depicts our CNN architecture, which is very similar to Ma et al. [2016] and Yang et al. [2015], that convolves word embeddings with three convolutional kernels of length 1, 2 and 3.", "startOffset": 86, "endOffset": 486}, {"referenceID": 12, "context": ", 2014, Simonyan and Zisserman, 2014] \u2013 to the most recent extremely deep architectures \u2013 a Facebook implementation of 152 layered ResidualNet [He et al., 2015].", "startOffset": 143, "endOffset": 160}, {"referenceID": 3, "context": "This approach has been widely explored, especially on VQA [Antol et al., 2015].", "startOffset": 58, "endOffset": 78}, {"referenceID": 57, "context": "We evaluate our approach from Section 3 on the DAQUAR dataset [Malinowski and Fritz, 2014a], which provides 12, 468 human question answer pairs on images of indoor scenes [Silberman et al., 2012] and follow the same evaluation protocol by providing results on accuracy and the WUPS score at {0.", "startOffset": 171, "endOffset": 195}, {"referenceID": 18, "context": ", 2015] and CNN [Jia et al., 2014].", "startOffset": 16, "endOffset": 34}, {"referenceID": 65, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc.", "startOffset": 136, "endOffset": 157}, {"referenceID": 37, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc.", "startOffset": 43, "endOffset": 72}, {"referenceID": 37, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc. Smaller thresholds yield more forgiving metrics. As in Malinowski and Fritz [2014a], we report WUPS at two extremes, 0.", "startOffset": 43, "endOffset": 249}, {"referenceID": 26, "context": "Later on, in Section 7, we will show improved results on DAQUAR with a stronger visual model and a pre-trained word embedding, with ADAM [Kingma and Ba, 2014] as", "startOffset": 137, "endOffset": 158}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time.", "startOffset": 99, "endOffset": 128}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time. Table 3 shows that Ask Your Neurons also improves on the reduced DAQUAR set, achieving 34.68% Accuracy and 40.76% WUPS at 0.9 substantially outperforming Malinowski and Fritz [2014a] by 21.", "startOffset": 99, "endOffset": 441}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time. Table 3 shows that Ask Your Neurons also improves on the reduced DAQUAR set, achieving 34.68% Accuracy and 40.76% WUPS at 0.9 substantially outperforming Malinowski and Fritz [2014a] by 21.95 percent points of Accuracy and 22.6 WUPS points. Similarly to previous experiments, we achieve the best performance using the \u201csingle word\u201d variant of our method. Note that Ren et al. [2015] reported 36.", "startOffset": 99, "endOffset": 641}, {"referenceID": 65, "context": "Both answersA and T i k are sets of the words, and \u03bc is a membership measure, for instance WUP [Wu and Palmer, 1994].", "startOffset": 95, "endOffset": 116}, {"referenceID": 37, "context": "Moreover, Table 6 shows that \u201cMCM\u201d applied to human answers at test time captures ambiguities in interpreting questions by improving the score of the human baseline from Malinowski and Fritz [2014a] (here, as opposed to Table 5, we exclude the original human answers from the measure).", "startOffset": 170, "endOffset": 199}, {"referenceID": 47, "context": "Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014].", "startOffset": 111, "endOffset": 136}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR.", "startOffset": 42, "endOffset": 84}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR. The Constant technique uses the most frequent answer in the training set to answer to every question in the test set. In Constant, per question type, we first break all questions into a few categories and then use the most frequent answer per category to answer to every question in that category at the test time. Table 7 provides more details on the chosen categories. Look-up table builds a hash map from a textual question into the most frequent answer for that question at the training time. At the test time, the method just looks up the answer for the question in the hashmap. If the question exists then the most popular answer for that question is provided, otherwise an \u2018empty\u2019 answer is given. In addition, we also remove articles, such as \u2018the\u2019 and \u2018a\u2019, from all the questions. However, this brings only a minor improvement. Finally, we experiment with two nearestneighbor methods. Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014]. In the following, we call this the representation the semantic space. Nearest Neighbor, Question-only searches at test time for the most similar question in the semantic space from the training set. Then it takes the answer that corresponds to this question. Nearest Neighbor is inspired by a similar baseline introduced in Antol et al. [2015]. At the test time we first search for the 4 most similar questions in the semantic space available in the training set.", "startOffset": 42, "endOffset": 1539}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR. The Constant technique uses the most frequent answer in the training set to answer to every question in the test set. In Constant, per question type, we first break all questions into a few categories and then use the most frequent answer per category to answer to every question in that category at the test time. Table 7 provides more details on the chosen categories. Look-up table builds a hash map from a textual question into the most frequent answer for that question at the training time. At the test time, the method just looks up the answer for the question in the hashmap. If the question exists then the most popular answer for that question is provided, otherwise an \u2018empty\u2019 answer is given. In addition, we also remove articles, such as \u2018the\u2019 and \u2018a\u2019, from all the questions. However, this brings only a minor improvement. Finally, we experiment with two nearestneighbor methods. Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014]. In the following, we call this the representation the semantic space. Nearest Neighbor, Question-only searches at test time for the most similar question in the semantic space from the training set. Then it takes the answer that corresponds to this question. Nearest Neighbor is inspired by a similar baseline introduced in Antol et al. [2015]. At the test time we first search for the 4 most similar questions in the semantic space available in the training set. Next, we form candidate images that correspond to the aforementioned 4 questions. At the last step, we choose an answer that is associated with the best match in the visual space. The latter is done by a cosine similarity between global CNN representations of the test image and every candidate image. We experiment with several CNN representations (VGG-19, GoogLeNet, ResNet152) but to our surprise there is little performance difference between them. We decide to use GoogLeNet as the results are slightly better. Baseline results. Constant shows how the dataset is biased w.r.t. the most frequent answer. This answer turns out to be the number \u201c2\u201d, which also explain a good performance of Ask Your Neurons on the \u201chow many\u201d question subset in DAQUAR. Constant, per question type shows that question types provide quite strong clues for answering questions. Kafle and Kanan [2016] take advantage of such clues in their Bayesian and Hybrid models (in Table 9, we only show a better performing Hybrid model).", "startOffset": 42, "endOffset": 2543}, {"referenceID": 42, "context": "Based on our further analysis on VQA (for more details we refer to Section 6 and Section 7), we have also applied the improved model to DAQUAR, and we significantly outperform Malinowski et al. [2015] presented in Section 4.", "startOffset": 176, "endOffset": 201}, {"referenceID": 42, "context": "Based on our further analysis on VQA (for more details we refer to Section 6 and Section 7), we have also applied the improved model to DAQUAR, and we significantly outperform Malinowski et al. [2015] presented in Section 4. In the experiments, we first choose last 10% of training set as a validation set in order to determine a number of training epochs K, and next we train the model for K epochs. We evaluate model on two variants of DAQUAR: all data points (\u2018all\u2019 in Table 9), and a subset (\u2018single word\u2019 in Table 9) containing only single word answers, which consists of about 90% of the original dataset. As Table 9 shows, our model, Vision + Language with GLOVE and Residual Net that sums visual and question representations, outperforms the model of Malinowski et al. [2015] by 5.", "startOffset": 176, "endOffset": 784}, {"referenceID": 42, "context": "While Section 4 analyses our original architecture [Malinowski et al., 2015] on the DAQUAR dataset, in this section, we analyze different variants and design choices for neural question answering on the large-scale Visual Question Answering (VQA) dataset [Antol et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 3, "context": ", 2015] on the DAQUAR dataset, in this section, we analyze different variants and design choices for neural question answering on the large-scale Visual Question Answering (VQA) dataset [Antol et al., 2015].", "startOffset": 186, "endOffset": 206}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.", "startOffset": 17, "endOffset": 42}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.43 21.67 25.28 Question-only of Malinowski et al. [2015] 17.", "startOffset": 18, "endOffset": 103}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.", "startOffset": 17, "endOffset": 42}, {"referenceID": 36, "context": "79 IMG-CNN [Ma et al., 2016] 21.", "startOffset": 11, "endOffset": 28}, {"referenceID": 69, "context": "SAN (2, CNN) [Yang et al., 2015] - 29.", "startOffset": 13, "endOffset": 32}, {"referenceID": 66, "context": "60 DMN+ [Xiong et al., 2016] - 28.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "79 ABC-CNN [Chen et al., 2015] - 25.", "startOffset": 11, "endOffset": 30}, {"referenceID": 19, "context": "[Jiang et al., 2015] 24.", "startOffset": 0, "endOffset": 20}, {"referenceID": 42, "context": "Ask Your Neurons architecture: originally presented in Malinowski et al. [2015], results in %.", "startOffset": 55, "endOffset": 80}, {"referenceID": 3, "context": "We evaluate on the VQA dataset [Antol et al., 2015], which is built on top of the MS-COCO dataset [Lin et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 34, "context": ", 2015], which is built on top of the MS-COCO dataset [Lin et al., 2014].", "startOffset": 54, "endOffset": 72}, {"referenceID": 26, "context": "We use ADAM [Kingma and Ba, 2014] throughout our ex-", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": "As a performance measure we use a consensus variant of accuracy introduced in Antol et al. [2015], where the predicted answer gets score between 0 and 1, with 1 if it matches with at least three human answers.", "startOffset": 78, "endOffset": 98}, {"referenceID": 41, "context": "All the models, which are publicly available together with our tutorial [Malinowski and Fritz, 2016] 2, are implemented in Keras [Chollet, 2015] and Theano [Bastien et al.", "startOffset": 72, "endOffset": 100}, {"referenceID": 4, "context": "All the models, which are publicly available together with our tutorial [Malinowski and Fritz, 2016] 2, are implemented in Keras [Chollet, 2015] and Theano [Bastien et al., 2012].", "startOffset": 156, "endOffset": 178}, {"referenceID": 13, "context": "We examine two recurrent questions encoders, LSTM [Hochreiter and Schmidhuber, 1997] and a simpler GRU [Cho et al.", "startOffset": 50, "endOffset": 84}, {"referenceID": 47, "context": "To address such shortcoming, we investigate the pre-trained word embedding transformation GLOVE [Pennington et al., 2014] that encodes question words (technically it maps one-hot vector into a 300 dimensional real vector).", "startOffset": 96, "endOffset": 121}, {"referenceID": 47, "context": "This choice naturally extends the vocabulary of the question words to about 2 million words extracted a large corpus of web data \u2013 Common Crawl [Pennington et al., 2014] \u2013 that is used to train the GLOVE embedding.", "startOffset": 144, "endOffset": 169}, {"referenceID": 30, "context": "Next we fix the question encoder to LSTM and vary different visual encoders: Caffe variant of AlexNet [Krizhevsky et al., 2012], GoogLeNet [Szegedy et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 58, "context": ", 2014], VGG-19 [Simonyan and Zisserman, 2014], and recently introduced 152 layered ResNet (we use the Facebook implementation of He et al.", "startOffset": 16, "endOffset": 46}, {"referenceID": 12, "context": ", 2014], VGG-19 [Simonyan and Zisserman, 2014], and recently introduced 152 layered ResNet (we use the Facebook implementation of He et al. [2015]).", "startOffset": 130, "endOffset": 147}, {"referenceID": 24, "context": "SNUBI [Kim et al., 2016] - - 84.", "startOffset": 6, "endOffset": 24}, {"referenceID": 49, "context": "1 Brandeis [Prakash and Storer, 2016] 82.", "startOffset": 11, "endOffset": 37}, {"referenceID": 35, "context": "9 HieCoAtt [Lu et al., 2016] 79.", "startOffset": 11, "endOffset": 28}, {"referenceID": 55, "context": "1 DualNet [Saito et al., 2016] 82.", "startOffset": 10, "endOffset": 30}, {"referenceID": 66, "context": "8 DMN+ [Xiong et al., 2016] 80.", "startOffset": 7, "endOffset": 27}, {"referenceID": 16, "context": "3 FDA [Ilievski et al., 2016] 81.", "startOffset": 6, "endOffset": 29}, {"referenceID": 64, "context": "4 AMA [Wu et al., 2016] 81.", "startOffset": 6, "endOffset": 23}, {"referenceID": 69, "context": "4 SAN [Yang et al., 2015] 79.", "startOffset": 6, "endOffset": 25}, {"referenceID": 67, "context": "4 SMem [Xu and Saenko, 2015] 80.", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "2 VQA team [Antol et al., 2015] 80.", "startOffset": 11, "endOffset": 31}, {"referenceID": 46, "context": "2 DPPnet [Noh et al., 2015] 80.", "startOffset": 9, "endOffset": 27}, {"referenceID": 73, "context": "4 iBOWIMG [Zhou et al., 2015] 76.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "9 LSTM Q+I [Antol et al., 2015] 78.", "startOffset": 11, "endOffset": 31}, {"referenceID": 19, "context": "[Jiang et al., 2015] 78.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Baselines from Antol et al. [2015] are not considered.", "startOffset": 15, "endOffset": 35}], "year": 2016, "abstractText": "We propose a Deep Learning approach to the visual question answering task, where machines answer to questions about real-world images. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, endto-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We evaluate our approaches on the DAQUAR as well as the VQA dataset where we also report various baselines, including an analysis how much information is contained in the language part only. To study human consensus, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus. Finally, we evaluate a rich set of design choices how to encode, combine and decode information in our proposed Deep Learning formulation.", "creator": "LaTeX with hyperref package"}}}