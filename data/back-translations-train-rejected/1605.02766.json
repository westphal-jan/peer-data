{"id": "1605.02766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning", "abstract": "LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The aim of the design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU for computation and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.", "histories": [["v1", "Mon, 9 May 2016 20:33:30 GMT  (114kb,D)", "https://arxiv.org/abs/1605.02766v1", null], ["v2", "Fri, 13 May 2016 00:20:14 GMT  (114kb,D)", "http://arxiv.org/abs/1605.02766v2", null], ["v3", "Tue, 2 Aug 2016 04:00:30 GMT  (118kb,D)", "http://arxiv.org/abs/1605.02766v3", "Accepted to ACM MULTIMEDIA 2016 Open Source Software Competition"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["chengxi ye", "chen zhao", "yezhou yang", "cornelia fermuller", "yiannis aloimonos"], "accepted": false, "id": "1605.02766"}, "pdf": {"name": "1605.02766.pdf", "metadata": {"source": "CRF", "title": "LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning [Simplify Deep Learning in Hundreds of Lines of Code]", "authors": ["Chengxi Ye", "Chen Zhao", "Yezhou Yang", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos"], "emails": ["yiannis}@umiacs.umd.edu", "*chenzhao@umd.edu"], "sections": [{"heading": null, "text": "Availability: Source code and data are available at: https: / / github.com / yechengxi / LightNetCategories and Subject Descriptors D.0 [Software]: General; I.2.10 [Artificial Intelligence]: Vision and Scene UnderstandingKeywords Computer vision; natural language processing; image understanding; machine learning; deep learning; revolutionary neural networks; multilayer pereptrons; recursive neural networks; strengthening learning"}, {"heading": "1. INTRODUCTION", "text": "Most current implementations of neural network models emphasize efficiency first and foremost. These pipelines (Table 1) can consist of a quarter to half a million lines of code and often include multiple programming languages. It is necessary that these models are thoroughly understood and modified. Copies are not used for profit or commercial gain, but for the full understanding and application of deep neural network models."}, {"heading": "2. USING THE PACKAGE", "text": "An example of how to use LightNet can be found in (Fig. 1): A simple template is provided to start the training process, requiring the user to fill in some critical training parameters, such as the number of training periods or the training method. A selective SGD algorithm is available to select the optimal learning rate. The learning rate is isar Xiv: 160 5.02 766v 3 [cs.L G] 2A ug2 016selected automatic and can optionally be customized during the training. The framework supports both GPU and CPU computation through the opts.use gpu option. Two additional functions are available to prepare the training data and initialize the network structure. Each experiment in this essay can be reproduced by running the associated script file. For more details, see the project page."}, {"heading": "3. BUILDING BLOCKS", "text": "The primary calculation module includes a forward and a backward / backward propagation process. The forward and backward propagation process evaluates the model, and the backward propagation reports the network gradients. Stochastic gradient descend algorithms are used to optimize the model parameters."}, {"heading": "3.1 Core Computational Modules", "text": "LightNet allows us to focus on the mathematical modeling of the network, rather than pointing to a deeper function that is not linear. To make this paper more self-contained, we explain the main computing modules of LightNet. All the networks (and related experiments) in this paper are built with these modules. The following descriptions are selected for simplicity, and readers can easily extend the derivatives to the mini-batch configuration."}, {"heading": "3.2 Loss function", "text": "Normally a loss function is connected to the outputs of the deepest core calculation module. LightNet currently supports the Softmax log loss function for classification tasks."}, {"heading": "3.3 Optimization Algorithms", "text": "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research. It is worth noting that we are implementing a novel SGD algorithm to facilitate the selection of hyperparameters, especially the learning rate. This algorithm selects the most efficient learning rate by executing the SGD process for some iterations, using each learning rate of a single candidate. In the middle of neural network training, the SGD algorithm can also be used to select different learning rates to accelerate energy degradation."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Multilayer Perceptron Network", "text": "A multi-layer Perceptron network is built to test LightNet's performance on MNIST data. [9] The 128-dimensional features are then connected to 10 nodes to calculate Softmax output. See Fig. 2 for test results. 4.2 Convolutional Neural NetworkLightNet supports the use of state-of-the-art Convolutionary Network models pre-trained on the ImageNet dataset. It also supports the training of novel network models from the ground up. A Convolutionary Network with 4 Convolution layers is designed to test LightNet's performance on CIFAR-10 data [7]. There are 32, 32, 64, 64 convolution cores of size 5 x 5 in the first three layers."}, {"heading": "4.3 LSTM Network", "text": "The Long Short Term Memory (LSTM) [4] is a popular recursive neural network model. Due to the versatility of LightNet, the LSTM network can be implemented into the LightNet package as a specific application. Specifically, the core computing modules in LightNet are used to perform time domain forward and backward propagation for LSTM. Forward in an LSTM model can be formulated as: it = sigmoid (Wihht \u2212 1 + Wixxt + bi), (7) ot = sigmoid (Wihht \u2212 1 + Woxxt + bo), (8) ft = sigmoid (Wfhht \u2212 1 + Wfxxt + bf), (9) gt = tanh (Wghht \u2212 1 + Wgxxt + bg), (10) ct = ft ct = ft ct \u2212 it = d (ct)."}, {"heading": "4.4 Q-Network", "text": "The dynamics of the cart pole system can be learned with a two-tiered network in hundreds of iterations. An iteration of the Q network update process is: Qnew (stateold, act) = reward + \u03b3maxaQcurrent (statenew, a) = reward + \u03b3V (statenew). (15) The action is randomly selected with probability epsilon, otherwise the action leading to the highest score is selected. The desired network output Qnew is calculated based on the observed reward and the discounted value \u03b3V (statenew) of the resulting state, predicted by the current network using Eq. 15.By using a function of least squared loss: z = (y \u2212 Qcurrent (stateold, act) 2 = QV (statenew) of the resulting state, predicted by the current network using Eq. 15.By using a function of least squared loss: (qz), (qz), (\u2212 state)."}, {"heading": "5. CONCLUSION", "text": "LightNet provides an easy-to-expand ecosystem for understanding and developing deep neural network models. Its easy-to-use Matlab-based environment makes the entire computing process easy to understand and visualize."}, {"heading": "6. REFERENCES", "text": "[1] Barto, A. G., Sutton, R. S., and Anderson, C. W.Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, 5 (1983), 834-846. [2] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. Theano: new features and speed improvements. arXiv preprint arXiv: 1211.5590 (2012). [3] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research 12 (2011), 2121-2159. [4] Hochreiter, S., and Schmidhuber, J. Kinghuber."}], "references": [{"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia (2014),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A wavelet tour of signal processing: the sparse way", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature 518,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference (2015),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks [8] have given rise to major advancements in many problems of machine intelligence.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].", "startOffset": 132, "endOffset": 142}, {"referenceID": 12, "context": "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].", "startOffset": 132, "endOffset": 142}, {"referenceID": 1, "context": "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].", "startOffset": 132, "endOffset": 142}, {"referenceID": 9, "context": "According to the convolution theorem [10], convolution in the spatial domain is equivalent to point-wise multiplication in the frequency domain.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.", "startOffset": 79, "endOffset": 82}, {"referenceID": 11, "context": "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "A multilayer perceptron network is constructed to test the performance of LightNet on MNIST data [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "A convolutional network with 4 convolution layers is constructed to test the performance of LightNet on CIFAR-10 data [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "The Long Short Term Memory (LSTM) [4] is a popular recurrent neural network model.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "As an application in reinforcement learning, We created a Q-Network [11] with the MLP network.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "The Q-Network is then applied to the classic Cart-Pole problem [1].", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "LightNet is a lightweight, versatile, purely Matlabbased deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments. Availability: the source code and data is available at: https://github.com/yechengxi/LightNet", "creator": "LaTeX with hyperref package"}}}