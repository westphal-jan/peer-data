{"id": "1605.02817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Unethical Research: How to Create a Malevolent Artificial Intelligence", "abstract": "Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyber-infrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).", "histories": [["v1", "Tue, 10 May 2016 01:39:38 GMT  (305kb)", "http://arxiv.org/abs/1605.02817v1", null], ["v2", "Thu, 1 Sep 2016 18:29:13 GMT  (337kb)", "http://arxiv.org/abs/1605.02817v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["federico pistono", "roman v yampolskiy"], "accepted": false, "id": "1605.02817"}, "pdf": {"name": "1605.02817.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "It is this exchange of information between ethical hackers and security experts that leads to a balanced cyber ecosystem. In the flourishing field of AI safety engineering, hundreds of papers have been published on various proposals aimed at creating a safe machine, but to our knowledge nothing has been published about how to construct a malicious machine. Availability of such information would be of particular value to computer scientists, mathematicians and others who have an interest in the security of artificial intelligence and seek to prevent the spontaneous appearance or deliberate creation of a dangerous artificial intelligence that can adversely affect human activities and, at worst, cause the complete extinction of the human species. This paper contains some general guidelines for the creation of malicious artificial intelligence (MAI)."}, {"heading": "1 Introduction", "text": "A significant number of papers and books have been published in recent years on the safety and security of artificial intelligence, particularly in relation to superhuman intelligence [Armstrong, Sandberg, & Bostrom, 2012; Bostrom, 2006; Chalmers, 2010; Hibbard, 2001; Loosemore & Goertzel, 2012; Muehlhauser & Helm, 2012; Omohundro, February 2008; R. V. Yampolskiy, 2014; Roman V. Yampolskiy, 2015a; Roman V. Yampolskiy, 2015; R. V. Yampolskiy, July 22-25, 2015; Yudkowsky, 2008]. Most such publications deal with the unintended consequences of poor design decisions, misselected ethical frameworks or limitations of systems that do not share human values and common sense in interpreting their goals."}, {"heading": "1.2 Hazardous Intelligent Software", "text": "\"Computer software is directly or indirectly responsible for controlling many important aspects of our lives. Wall Street commerce, nuclear power plants, social security compensation, credit histories, and traffic lights are all software-driven and only one serious design flaw removed that has catastrophic consequences for millions of people. Even more dangerous is the situation with software specifically designed for malicious purposes, such as viruses, spyware, Trojan horses, worms, and other dangerous software (HS). If HS is ever given the capabilities of truly artificially intelligent systems (e.g. artificially intelligent viruses, worms), the consequences would undoubtedly be catastrophic. Such dangerous intelligent software (HIS) would pose risks that are currently unseen in malware with subhuman intelligence.\" [R. V. Yampolskiy, 2012] Nick Bostrom, in his typology of information risks, has defined the term artificial intelligence, which he explicitly defines as [Bostrom, 2011] a computer system that primarily represents an intelligent threat."}, {"heading": "1.3 Who might be interested in creating Malevolent AI?", "text": "Each of these agents would bring his own goals / resources into the equation, but it is important to understand here how widespread such attempts will be and how numerous such agents may be. Below is a short list of representative entities that is far from comprehensive. Military is developing cyberweapons and robotic soldiers to gain dominance. Governments that are trying to use artificial intelligence to establish hegemony, control people, or overthrow other governments. Companies that are trying to gain a monopoly and destroy competition by illegal means. Rogues that are trying to take over the world and use artificial intelligence as a panacea. Black hats that are trying to steal information, destroy resources, or destroy targets of cyberinfrastructure. Doomsday cults that are trying to bring about the end of the world by any means possible. Depressed people who are trying to commit suicide by artificial intelligence. Psychopaths who are trying to avoid their names in any way in the history books, trying to take on the risks that I am representing."}, {"heading": "2 How to create a Malevolent AI", "text": "The literature on AI risk [Sotala & Yampolskiy, January 2015] suggests a number of safety measures that should be implemented in any advanced AI project to minimize potential negative impacts. Simply reversing the advice would in many cases lead to a deliberately dangerous system. Indeed, the number of specific bugs in which a malicious designer can implement dangerous intelligent software is limited only by his imagination and it would be impossible to comprehensively verify it. In \"AGI Failures Modes and Levels,\" Turchin [Turchin, July 10, 2015] describes a number of ways in which an intelligent system can be dangerous at various stages of its development. Among his examples, AI: Hacks describes as many computers as possible to gain more computing power, his own robotic infrastructure using biotechnology. Prevents other AI projects from being completed by hacking or diversions. Has goals that cause suffering, interprets Putin's literal margin of probability that we will not be able to implement the July 10, 2015, despite the likelihood of some system impossibilities."}, {"heading": "2.1 No oversight boards", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2.2 Closed source code", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he said.\" But it's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves, \"adding,\" It's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves as if they were able to survive themselves. \""}, {"heading": "3 Societal Impact", "text": "In this section we will examine the possible impact of the MAI on the economy, government, legislation and military."}, {"heading": "3.1 Economy/Unemployment", "text": "There is a great risk of such a conflict arising."}, {"heading": "3.2 Government", "text": "Governments play a crucial role in the functioning of all modern societies. The balance of power and oversight has been the subject of controversy and heated debate since the first primitive forms of organized social structures that evolved over time into modern forms of government.Coups d'\u00e9tat, the sudden and illegal seizure of a state either by force or by corruption, intimidation or economic hitmen, can create instability, uprisings, civil wars, poverty and, at worst, a total collapse of the country. Throughout history, people have successfully staged coups d'\u00e9tat by disseminating false information, staging assassinations, and other relatively simple operations that could easily be accomplished by a MAI. Moreover, a sufficiently advanced MAI with access to the global communications system, Internet and social media, can facilitate multiple coups d'\u00e9tat by disseminating false information, staged assassinations, and other relatively simple operations that can be easily accomplished by a MAI."}, {"heading": "3.3 Legislative", "text": "Any action or strategy taken by a MAI at the expense of the people and their life-support systems can be strengthened and accelerated by legislation in support of such plans. Research by Lessig [Lessig, 2011] shows that the extreme concentration of power, influence, and decision-making in the hands of the few has allowed the will of the people to be completely disconnected from the outcomes of laws in countries like the United States, where 0.05% of the population can consistently pass laws that are against the will of the majority. It follows that a MAI through lobbying would seek control of the House of Representatives, the Senate, and other bodies at a fraction of the energy and time (cost) required by other means. Hidden and obscure pro-MAI activity laws can feed into a positive feedback cycle that further increases the influence of a MAI and the likelihood of developing its goals. The more power is in the hands of the few, the easier it will be for a MAI to gain control over or control of the system."}, {"heading": "3.4 Military", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "In the authors\u2019 view, military action and weapons against", "text": "The direct use of force, when other more efficient options are available, is an irrational strategy from which people go unnoticed and unsuspected due to their evolutionary biological baggage, but an MAI would have no reason and would therefore delay or even avoid the use of counterattacks or effective defenses overall. Direct use of force, when other more efficient options are available, is a very easy task for an MAI. The total number of nuclear weapons is estimated at 10,144, [Nuclear Notebook, Bulletin of the Atomic Scientists, 2014] many of which are stored in aging facilities, without adequate safety standards, underfunded, and many safety systems questioned."}, {"heading": "4. Conclusions", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "Acknowledgements", "text": "Roman Yampolskiy commends Elon Musk and FLI for partially funding his work through project funding: \"Evaluation of Safe Development Pathways for Artificial Superintelligence,\" awarded to the Global Catastrophic Risk Institute, Seth Baum (PI). Authors are grateful for valuable feedback on an early draft of this paper from Yana Feygin, Seth Baum, James Babcock, J\u00e1nos Kram\u00e1r, and Tony Barrett. Views in this paper agree with those of the authors and do not necessarily represent the views of FLI, GCRI, or others."}], "references": [{"title": "An Open Letter from US Researchers in Cryptography and Information Security. http://masssurveillance.info", "author": ["M. Abadi", "al"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2014}, {"title": "Linguistic Profiling and Behavioral Drift in Chat Bots", "author": ["N. Ali", "D. Schaeffer", "R.V. Yampolskiy"], "venue": "Midwest Artificial Intelligence and Cognitive Science Conference,", "citeRegEx": "Ali et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ali et al\\.", "year": 2012}, {"title": "Shopping for Spy Gear: Catalog Advertises NSA Toolbox", "author": ["J. Appelbaum", "J. Horchert", "C. St\u00f6cker"], "venue": null, "citeRegEx": "Appelbaum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Appelbaum et al\\.", "year": 2013}, {"title": "The case for ethical autonomy in unmanned systems", "author": ["R.C. Arkin"], "venue": "Journal of Military Ethics,", "citeRegEx": "Arkin,? \\Q2010\\E", "shortCiteRegEx": "Arkin", "year": 2010}, {"title": "Thinking inside the box: Controlling and using an oracle ai", "author": ["S. Armstrong", "A. Sandberg", "N. Bostrom"], "venue": "Minds and Machines,", "citeRegEx": "Armstrong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Armstrong et al\\.", "year": 2012}, {"title": "Ethical Issues in Advanced Artificial Intelligence", "author": ["N. Bostrom"], "venue": "Review of Contemporary Philosophy,", "citeRegEx": "Bostrom,? \\Q2006\\E", "shortCiteRegEx": "Bostrom", "year": 2006}, {"title": "Information Hazards: A Typology of Potential Harms From Knowledge", "author": ["N. Bostrom"], "venue": "Review of Contemporary Philosophy,", "citeRegEx": "Bostrom,? \\Q2011\\E", "shortCiteRegEx": "Bostrom", "year": 2011}, {"title": "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents", "author": ["N. Bostrom"], "venue": "Minds and Machines,", "citeRegEx": "Bostrom,? \\Q2012\\E", "shortCiteRegEx": "Bostrom", "year": 2012}, {"title": "The computerisation of European jobs. bruegel.org/2014/07/the-computerisation-of-european-jobs", "author": ["J. Bowels"], "venue": null, "citeRegEx": "Bowels,? \\Q2014\\E", "shortCiteRegEx": "Bowels", "year": 2014}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Chalmers,? \\Q2010\\E", "shortCiteRegEx": "Chalmers", "year": 2010}, {"title": "Cognitive illusions and the lying machine: a blueprint for sophistic mendacity. RPI", "author": ["M.H. Clark"], "venue": null, "citeRegEx": "Clark,? \\Q2010\\E", "shortCiteRegEx": "Clark", "year": 2010}, {"title": "Swiss Authorities Arrest Bot for Buying Drugs and Fake Passport. Gawker, http://internet.gawker.com/swiss-authorities-arrest-bot-forbuying-drugs-and-a-fak-1681098991", "author": ["A. Cush"], "venue": null, "citeRegEx": "Cush,? \\Q2015\\E", "shortCiteRegEx": "Cush", "year": 2015}, {"title": "Exploring the possibility of using intelligent computers to judge criminal offenders", "author": ["T. Dunn"], "venue": null, "citeRegEx": "Dunn,? \\Q1995\\E", "shortCiteRegEx": "Dunn", "year": 1995}, {"title": "The future of employment: how susceptible are jobs to computerisation? Sept", "author": ["C.B. Frey", "M.A. Osborne"], "venue": null, "citeRegEx": "Frey and Osborne,? \\Q2013\\E", "shortCiteRegEx": "Frey and Osborne", "year": 2013}, {"title": "State-of-the-Art in Robot Authentication (From the Guest Editors)", "author": ["M. Gavrilova", "R. Yampolskiy"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "Gavrilova and Yampolskiy,? \\Q2010\\E", "shortCiteRegEx": "Gavrilova and Yampolskiy", "year": 2010}, {"title": "Increased security through open source", "author": ["Hoepman", "J.-H", "B. Jacobs"], "venue": "Communications of the ACM,", "citeRegEx": "Hoepman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoepman et al\\.", "year": 2007}, {"title": "Republic, Lost: How Money Corrupts Congress-and a Plan to Stop It", "author": ["L. Lessig"], "venue": null, "citeRegEx": "Lessig,? \\Q2011\\E", "shortCiteRegEx": "Lessig", "year": 2011}, {"title": "Why an intelligence explosion is probable Singularity Hypotheses (pp", "author": ["R. Loosemore", "B. Goertzel"], "venue": null, "citeRegEx": "Loosemore and Goertzel,? \\Q2012\\E", "shortCiteRegEx": "Loosemore and Goertzel", "year": 2012}, {"title": "AI safety engineering through introduction of self-reference into felicific calculus via artificial pain and pleasure", "author": ["A.M. Majot", "R.V. Yampolskiy"], "venue": "Ethics in Science, Technology and Engineering,", "citeRegEx": "Majot and Yampolskiy,? \\Q2014\\E", "shortCiteRegEx": "Majot and Yampolskiy", "year": 2014}, {"title": "The Singularity and Machine Ethics In The Singularity Hypothesis: A Scientific and Philosophical Assessment, edited", "author": ["L. Muehlhauser", "L. Helm"], "venue": null, "citeRegEx": "Muehlhauser and Helm,? \\Q2012\\E", "shortCiteRegEx": "Muehlhauser and Helm", "year": 2012}, {"title": "The Basic AI Drives", "author": ["S.M. Omohundro"], "venue": "Proceedings of the First AGI Conference,", "citeRegEx": "Omohundro,? \\Q2008\\E", "shortCiteRegEx": "Omohundro", "year": 2008}, {"title": "Open Letter From UK Security Researchers", "author": ["K. Paterson", "M. Ryan", "P. Ryan", "V. Sassone", "S. Schneider", "N.P. Smart", "F. . . Hao"], "venue": null, "citeRegEx": "Paterson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paterson et al\\.", "year": 2013}, {"title": "Robots will steal your job, but that's ok: how to survive the economic collapse and be happy: CreateSpace", "author": ["F. Pistono"], "venue": null, "citeRegEx": "Pistono,? \\Q2012\\E", "shortCiteRegEx": "Pistono", "year": 2012}, {"title": "More about the NSA's Tailored Access Operations", "author": ["B. Schneier"], "venue": null, "citeRegEx": "Schneier,? \\Q2013\\E", "shortCiteRegEx": "Schneier", "year": 2013}, {"title": "CROSSBEAM: NSA Exploit of the Day. schneier.com/blog/archives/2014/02/crossbeam_nsa_e.html", "author": ["B. Schneier"], "venue": null, "citeRegEx": "Schneier,? \\Q2014\\E", "shortCiteRegEx": "Schneier", "year": 2014}, {"title": "IBM's Watson Gets A 'Swear Filter' After Learning The Urban Dictionary. International Business Times, http://www.ibtimes.com/ibms-watson-gets-swear-filterafter-learning-urban-dictionary-1007734", "author": ["D. Smith"], "venue": null, "citeRegEx": "Smith,? \\Q2013\\E", "shortCiteRegEx": "Smith", "year": 2013}, {"title": "Experiments in Artimetrics: Avatar Face Recognition", "author": ["R. Yampolskiy", "G. Cho", "R. Rosenthal", "M. Gavrilova"], "venue": "Transactions on Computational Science XVI,", "citeRegEx": "Yampolskiy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy et al\\.", "year": 2012}, {"title": "Safety Engineering for Artificial General Intelligence", "author": ["R. Yampolskiy", "J. Fox"], "venue": "Topoi,", "citeRegEx": "Yampolskiy and Fox,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy and Fox", "year": 2012}, {"title": "Artimetrics: Biometrics for Artificial Entities", "author": ["R. Yampolskiy", "M. Gavrilova"], "venue": "IEEE Robotics and Automation Magazine", "citeRegEx": "Yampolskiy and Gavrilova,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy and Gavrilova", "year": 2012}, {"title": "Leakproofing Singularity - Artificial Intelligence Confinement Problem", "author": ["R.V. Yampolskiy"], "venue": "Journal of Consciousness Studies (JCS),", "citeRegEx": "Yampolskiy,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy", "year": 2012}, {"title": "Artificial intelligence safety engineering: Why machine ethics is a wrong approach Philosophy and Theory of Artificial Intelligence (pp. 389-396)", "author": ["Yampolskiy", "R.V. 2013a"], "venue": null, "citeRegEx": "Yampolskiy and 2013a..,? \\Q2013\\E", "shortCiteRegEx": "Yampolskiy and 2013a..", "year": 2013}, {"title": "What to Do with the Singularity Paradox", "author": ["R.V. Yampolskiy"], "venue": "Philosophy and Theory of Artificial Intelligence (pp", "citeRegEx": "Yampolskiy,? \\Q2013\\E", "shortCiteRegEx": "Yampolskiy", "year": 2013}, {"title": "Utility Function Security in Artificially Intelligent Agents", "author": ["R.V. Yampolskiy"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence (JETAI),", "citeRegEx": "Yampolskiy,? \\Q2014\\E", "shortCiteRegEx": "Yampolskiy", "year": 2014}, {"title": "Artificial Superintelligence: a Futuristic Approach: Chapman and Hall/CRC", "author": ["R.V. Yampolskiy"], "venue": null, "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}, {"title": "The Space of Possible Mind Designs Artificial General Intelligence (pp", "author": ["R.V. Yampolskiy"], "venue": null, "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}, {"title": "Taxonomy of Pathways to Dangerous AI. AAAI 2016", "author": ["R.V. Yampolskiy"], "venue": null, "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}, {"title": "Analysis of Types of SelfImproving Software", "author": ["R.V. Yampolskiy"], "venue": "The Eighth Conference on Artificial General Intelligence,", "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}, {"title": "On the Limits of Recursively Self-Improving AGI", "author": ["R.V. Yampolskiy"], "venue": "The Eighth Conference on Artificial General Intelligence,", "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}, {"title": "Artificial General Intelligence and the Human Mental Model. Singularity Hypotheses: A Scientific and Philosophical Assessment, 129", "author": ["R.V. Yampolskiy", "J. Fox"], "venue": null, "citeRegEx": "Yampolskiy and Fox,? \\Q2013\\E", "shortCiteRegEx": "Yampolskiy and Fox", "year": 2013}, {"title": "Artificial Intelligence as a Positive and Negative Factor in Global Risk", "author": ["E. Yudkowsky"], "venue": "Global Catastrophic Risks (pp. 308-345)", "citeRegEx": "Yudkowsky,? \\Q2008\\E", "shortCiteRegEx": "Yudkowsky", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "A significant number of papers and books have been published in recent years on the topic of Artificial Intelligence safety and security, particularly with respect to superhuman intelligence [Armstrong, Sandberg, & Bostrom, 2012; Bostrom, 2006; Chalmers, 2010; Hibbard, 2001; Loosemore & Goertzel, 2012; Muehlhauser & Helm, 2012; Omohundro, February 2008; R. V. Yampolskiy, 2014; Roman V. Yampolskiy, 2015a; Roman V Yampolskiy, 2015; R. V. Yampolskiy, July 22-25, 2015a, July 22-25, 2015b; Yudkowsky, 2008].", "startOffset": 191, "endOffset": 506}, {"referenceID": 9, "context": "A significant number of papers and books have been published in recent years on the topic of Artificial Intelligence safety and security, particularly with respect to superhuman intelligence [Armstrong, Sandberg, & Bostrom, 2012; Bostrom, 2006; Chalmers, 2010; Hibbard, 2001; Loosemore & Goertzel, 2012; Muehlhauser & Helm, 2012; Omohundro, February 2008; R. V. Yampolskiy, 2014; Roman V. Yampolskiy, 2015a; Roman V Yampolskiy, 2015; R. V. Yampolskiy, July 22-25, 2015a, July 22-25, 2015b; Yudkowsky, 2008].", "startOffset": 191, "endOffset": 506}, {"referenceID": 39, "context": "A significant number of papers and books have been published in recent years on the topic of Artificial Intelligence safety and security, particularly with respect to superhuman intelligence [Armstrong, Sandberg, & Bostrom, 2012; Bostrom, 2006; Chalmers, 2010; Hibbard, 2001; Loosemore & Goertzel, 2012; Muehlhauser & Helm, 2012; Omohundro, February 2008; R. V. Yampolskiy, 2014; Roman V. Yampolskiy, 2015a; Roman V Yampolskiy, 2015; R. V. Yampolskiy, July 22-25, 2015a, July 22-25, 2015b; Yudkowsky, 2008].", "startOffset": 191, "endOffset": 506}, {"referenceID": 6, "context": "Nick Bostrom, in his typology of information hazards, has proposed the term artificial intelligence hazard, which he defines as [Bostrom, 2011] \u201ccomputer\u2010related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.", "startOffset": 128, "endOffset": 143}, {"referenceID": 39, "context": "\u201d Addressing specifically superintelligent systems, we can also look at the definition of Friendly Artificial Intelligence (FAI) proposed by Yudkowsky [Yudkowsky, 2008] and Unethical Research: How to Create a Malevolent Artificial Intelligence", "startOffset": 151, "endOffset": 168}, {"referenceID": 10, "context": "We are already getting glimpses of such technology in today\u2019s research with recently publicized examples involving lying robots [Castelfranchi, 2000; Clark, 2010], black market trading systems [Cush, January 22, 2015] and swearing computers [D.", "startOffset": 128, "endOffset": 162}, {"referenceID": 22, "context": "Research by Pistono [Pistono, 2012], Frey and Osborne [Frey & Osborne, 2013], suggests that we might already see early sign of this process today and that as much 47% of all jobs in the U.", "startOffset": 20, "endOffset": 35}, {"referenceID": 8, "context": "Further analysis by Bowels [Bowels, 2014] indicates that the same trend would apply to Europe as well, with numbers fluctuating between 61.", "startOffset": 27, "endOffset": 41}, {"referenceID": 21, "context": "The NSA, by illegally installing backdoors into hardware and software worldwide, collecting personal information and metadata of all internet users that go through US-controlled servers, has weakened the security of our communication systems, and has made us more vulnerable to espionage and terrorist attacks [Abadi & al., January 24, 2014; Paterson et al., 2013; Peha, October 4, 2013].", "startOffset": 310, "endOffset": 387}, {"referenceID": 16, "context": "Research by Lessig [Lessig, 2011] shows that the extreme concentration of power, influence, and decision-making in the hands of a few has made it possible to completely", "startOffset": 19, "endOffset": 33}, {"referenceID": 12, "context": "It is also very likely that MAI itself would attempt to enter politics [Pellissier, June 12, 2015] or penetrate our judicial system [Dunn, 1995] to avoid having", "startOffset": 132, "endOffset": 144}, {"referenceID": 3, "context": "Of course, the possibility remains that MAI will take over military robots, drones and cyber weapons to engage in a direct attack by exploiting autonomous capabilities of such weapons [Arkin, 2010].", "startOffset": 184, "endOffset": 197}, {"referenceID": 7, "context": "According to Bostrom\u2019s orthogonality thesis an AI system can have any combination of intelligence and goals [Bostrom, 2012].", "startOffset": 108, "endOffset": 123}, {"referenceID": 6, "context": "\uf0b7 Reveal informational hazards [Bostrom, 2011];", "startOffset": 31, "endOffset": 46}], "year": 2016, "abstractText": "Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyberinfrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).", "creator": "Microsoft\u00ae Word 2013"}}}