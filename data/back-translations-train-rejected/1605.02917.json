{"id": "1605.02917", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine", "abstract": "Search engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection.", "histories": [["v1", "Tue, 10 May 2016 10:05:40 GMT  (1174kb)", "http://arxiv.org/abs/1605.02917v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["seyed hamid reza mohammadi", "mohammad ali zare chahooki"], "accepted": false, "id": "1605.02917"}, "pdf": {"name": "1605.02917.pdf", "metadata": {"source": "CRF", "title": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine", "authors": ["Seyed Hamid", "Reza Mohammadi", "Mohammad Ali Zare Chahooki"], "emails": ["mohammadi_6468@stu.yazd.ac.ir", "chahooki@yazd.ac.ir"], "sections": [{"heading": null, "text": "In fact, most of them are able to set out in search of new paths that they are taking in order to put themselves at the forefront of the world. (...) Most of them have set out in search of new paths that they are following. (...) Most of them have set out in search of new paths that they are following. (...) Most of them have set out in search of new paths. (...) Most of them have set out in search of new paths. (...) Most of them have set out in search of new paths. (...) Most of them have set out in search of new paths. (...) Most of them have tried to set out in search of new paths. (...) Most of them have tried to set out in search of new paths that they are following. (...) Most of them have tried to set out in search of new paths that they are describing themselves. (...)"}, {"heading": "2- SVM Extensions", "text": "Support Vector Machine (SVM) is a non-statistical binary classifier that has attracted a lot of attention in recent years. In this method, all instances are used in conjunction with an optimization algorithm to find instances that form the boundaries of classes. However, these instances create an optimal linear decision boundary for separate classes, and it is solved by the quadratic programming problem (QPP) [23]. This classifier does not rely on statistics; the training points are used directly to determine the decision boundary between two classes. Proximal Support Vector Machine (PSVM) is a new version of SVM that aims to reduce computing costs. In this algorithm, two parallel hyperplanes are used instead of one to separate the instances. Generalized eigenvalues Proximal SVM (GEPSVM), uses two non-parallel hyperplanes, each of which is placed in the two classes."}, {"heading": "3- Non-linear Kernels", "text": "Linear models work with data that can be separated linearly. Often, the development of linear models is more appropriate than non-linear models because they are associated with less complexity time. However, in some cases, the data is not inherently linear. Such cases must be assigned to a more dimensional space in which a linear model is applicable. Fig.2 illustrates how the data is assigned to a new space. Mapping to a high-dimensional space is not always easy and can cost much more than the primary dimensional space. By offering a way to reduce the associated costs, the idea of the kernel addresses this deficiency. A kernel implicitly maps the data into a higher dimensional space. To classify non-linear data in a linear way, the SVM must use the idea of a kernel, which leads to improved classification capabilities. Generally, algorithms in which the functions of the kernel are linked together lead to a linear form of a kernel [21]."}, {"heading": "4- Proposed Method", "text": "In this paper, non-linear cores in the extended SVM are used to present a binary classifier that has superior performance in detecting spam pages. Below, the learning structure of the SVM is discussed in detail. Next, the non-linear cores in the TWSVM are presented. Finally, the proposed method, i.e. multiple kernel TSVM, is presented. As shown in the Experimental Results section, this method improves the accuracy of detecting spam pages. 5 | P a g e"}, {"heading": "4-1- TSVM", "text": "The TSVM was first introduced by Jayada [22], based on the normal SVM. In this algorithm, each class is determined on the basis of one of the hyperplanes, rather than using a single level and increasing its margins towards binary classification. It is assumed in Fig.3.We obtain a set of data with positive and negative instances. To separate them, two equations are formed for each hyperplane, which can linearly divide the data in TWSVM. bXWXFbXWXF TT) (,) (1), where nWW and bb in the next step two QPPs must be solved to obtain the optimal hyperplane. QPPs can be difficult to solve; therefore they should be simplified and solved using Lagrange's method of indeterminate coefficients. Once the QPP is solved, the final equation of the SVM is classified into two classes."}, {"heading": "4-2- Using kernels in the TWSVM", "text": "In order to convert non-linear classification into linear classification, instances are mapped into a new high-dimensional space before the classification phase, as mentioned above. In Section 3, the idea of mapping the data into a new space was explained in detail. In addition, the final equation for the TWSVM classifier was presented in the previous subsection. We intend to rewrite this equation using a core function. Eq (3) shows the final classifier according to kernel design. 6 | P a g eiT iii wCxKwbwCxK class), (|), (| minarg2,1 (3) Where K is the core function that maps data to a higher dimension and any kernel can be used to replace K, in this equation."}, {"heading": "4-3- Multiple non-linear kernels in the TWSVM", "text": "In this paper there are two separate cores located in the TWSVM. The rationality behind this is the nature of the spam pages and the distribution of the instance. This paper tries to improve the classification performance by designing two suitable kernels, both of which merge the instance into a new space. In the following, the details of the development and access to two suitable cores for the TWSVM are discussed. In subsection 4-2, the kernel design is explained in the TSVM."}, {"heading": "5- Experimental Results", "text": "In this section we present the details of implementing and testing the proposed algorithm, including the description of data sets, evaluation criteria and experimental results."}, {"heading": "5-1- Dataset", "text": "In order to train, execute and determine the accuracy of the proposed algorithm, UK-2006 and UK-2007 records were used, which have been used in numerous studies on spam sites. UK-2006 contains about 11400 hosts from the.uk domain, where 61.75% of documents are labelled as normal, 22.08% as spam and 16.16% as unknown [26]. UK-2007 holds 114529 hosts from the.uk domain, of which 94% are labelled as normal and the remaining 6% as spam. In this essay, we ignore documents with unknown labels. The samples in two sets of data are randomly divided into two parts, 75% as a train and another 25% as a test set."}, {"heading": "5-2- Evaluation", "text": "Accuracy is a measure of how many instances are correctly classified. It is popular in the context of deception detection. Compared with other metrics, it is quite robust. NPTNTP Accuracy (8) To evaluate the accuracy of our classifier, we used a technique known as ten-fold cross-validation [4]. In ten-fold cross-validation, the assessed data set is randomly divided into 10 equally sized partitions and 10 training / test steps, in which each step uses nine partitions to train the classifier and the remaining partition to test its effectiveness."}, {"heading": "5-3- Comparison of the proposed algorithm to SVMs", "text": "Despite acceptable performance in classifying different types of data, the standard SVM algorithm failed in the context of spam pages. Standard SVM was implemented with multiple cores. A comparison of the results can be seen in Table 1. Obviously, the choice of the kernel does not lead to significant changes in the performance of the algorithm, which may be due to its own structure. Standard SVM uses only one separation layer and is therefore unable to distinguish between relatively similar spam and normal instances."}, {"heading": "5-4- Comparison to other algorithms", "text": "As already mentioned, different classifiers have been used to detect a linear classifier, including decision trees [14] and artificial neural networks [16]. In addition, other studies such as [13] and [15] use different algorithms that cannot be compared to the proposed algorithm due to incompatible data sets. In Tables 2 and 3, our results are presented together with those of the most accurate algorithms in KU-2006, UK-2007 and 9 | P a g e95.6 MKTWSVMMKTWSVM, which uses two hyperplanes to separate instances of the two classes of two nuclei each, achieving acceptable accuracy. Interestingly, this method maps the input data in such a way that a linear model can be used for differentiation. The choice and design of the kernel is an important problem with this problem. The proposed core in this paper improves the accuracy of a linear classifier."}, {"heading": "6- Conclusion and Future Works", "text": "To demonstrate the efficacy of non-linear nuclei, SVM and TWSVM were tested with and without core functions, and the proposed method demonstrated that two non-linear nuclei in TWSVM increase accuracy and reduce computational complexity. One possibility for future studies is to alter the structure of extended versions of SVM to increase accuracy and make computational complexity more satisfactory. As the optimal combination of basic core functions for one of the hyperlevels can improve classification accuracy, genetic programming can be integrated to find the optimal combination of basic core functions."}], "references": [{"title": "Efficient and effective spam filtering and re-ranking for large web datasets", "author": ["G.V. Cormack", "M.D. Smucker", "C.L.A. Clarke"], "venue": "Information Retrieval, pp. 1-25", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Web Spam", "author": ["P.T. Metaxas", "J. DeStefano"], "venue": "Propaganda and Trust\u201d, in Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web, pp. 60-69", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Spam", "author": ["D. Fetterly", "M. Manasse", "M. Najork"], "venue": "damn spam, and statistics\u201d, in Proceedings of the 7th International Workshop on the Web and Databases, pp. 210-223", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting spam web pages through content analysis", "author": ["A. Ntoulas", "M. Najork", "M. Manasse", "D. Fetterly"], "venue": "Proceedings of the 15th International Conference on World Wide Web,China Beijin University, pp. 83-92", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Web spam detection: Link-based and content-based techniques", "author": ["L. Becchetti", "C. Castillo", "D. Donato", "S. Leonardi", "R. Baeza-Yates"], "venue": "The European Integrated Project Dynamically Evolving Large Scale Information Systems (DELIS): Proceedings of the Final Workshop, Paderborn University, pp. 99-113", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning from labeled and unlabeled data on a directed graph", "author": ["D. Zhou", "J. Huang", "B. Sch\u00f6lkopf"], "venue": "Proceedings of the 22nd International Conference on Machine learning, Brazil Pugn University, pp. 1036- 1043", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Link analysis for web spam detection", "author": ["L. Becchetti", "C. Castillo", "D. Donato", "R. Baeza-Yates", "S. Leonardi"], "venue": "ACM Transactions on the Web (TWEB), China, vol. 2, pp. 1-42", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Know your neighbors: web spam detection using the web topology", "author": ["C. Castillo", "D. Donato", "A. Gionis", "V. Murdock", "F. Silvestri"], "venue": "Proceedings of the 30th Annual International ACM SIGIR conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Identifying web spam with user behavior analysis\u201d.Proceedings of the 4th International Workshop on Adversarial Information", "author": ["Y.Liu", "R.Cen", "M.Zhang", "S.Ma", "L.Ru"], "venue": "Retrieval on the Web,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Cloaking and redirection: A preliminary study", "author": ["B. Wu", "B.D. Davison"], "venue": "Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web (AIRWeb), pp. 7", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Cross-Lingual Web Spam Classification\u201d,in", "author": ["K. Chellapilla", "A. Maykov"], "venue": "Proceedings of the 3rd International Workshop on Adversarial Information Retrieval on the Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Web Spam Detection Using Machine Learning in Specific Domain Features", "author": ["Hmeidi", "H.Najadat", "Ismail"], "venue": "Journal of Information Assurance and Security,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Content based web spam detection using naive bayes with different feature representation technique.", "author": ["Soni", "A.Anand", "A.Mathur"], "venue": "Journal of Engineering Research and Applications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "From a theoretical point of view, search engines are information retrieval tools responsible for downloading, indexing, processing, retrieving, and ranking of information [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Such attempts can be exploited, and led to the creation of spam pages by profiteers [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "The main purpose of spam pages is to fraud ranking algorithms, with the aim of achieving a higher rank among the top ten search engine\u2019s results of various queries, as fast as possible [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "As a result, they are created with the ranking algorithms in mind [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "[4] presented an approach based on content analysis, using examination of various features of spam and non-spam pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 6, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "In [6-8] a different approach, based on the links between pages in the web graph, is used for detection.", "startOffset": 3, "endOffset": 8}, {"referenceID": 4, "context": "In [5], spam pages are detected using a combination of content-based and link-based methods.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[9] presented a behavior-based method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Various methods of spam detection have been proposed, including those based on coding-style similarity [18] and language pattern analysis [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "In previous researches, Hidden Markov Model [12], Decision tree [13], Bayesian networks [14], and artificial neural network [15] are used to separate spam instances from non-spam ones.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "In previous researches, Hidden Markov Model [12], Decision tree [13], Bayesian networks [14], and artificial neural network [15] are used to separate spam instances from non-spam ones.", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "In order to evaluate the accuracy of our classifier, we employed a technique known as ten-fold cross validation [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "Furthermore, other studies such as [13] and [15] use different algorithms, which cannot be compared to the proposed algorithm due to incompatible datasets.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Search engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection.", "creator": "Microsoft\u00ae Word 2010"}}}