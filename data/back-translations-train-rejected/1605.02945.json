{"id": "1605.02945", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "The Yahoo Query Treebank, V. 1.0", "abstract": "A description and annotation guidelines for the Yahoo Webscope release of Query Treebank, Version 1.0, May 2016.", "histories": [["v1", "Tue, 10 May 2016 11:29:28 GMT  (15kb)", "http://arxiv.org/abs/1605.02945v1", "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016"], ["v2", "Wed, 11 May 2016 17:20:26 GMT  (15kb)", "http://arxiv.org/abs/1605.02945v2", "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016"]], "COMMENTS": "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yuval pinter", "roi reichart", "idan szpektor"], "accepted": false, "id": "1605.02945"}, "pdf": {"name": "1605.02945.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["me@yuvalpinter.com", "roiri@ie.technion.ac.il", "idan@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.02 945v 1 [cs.C L] 10 May 201 6The Yahoo Query Treebank, V. 1.0 May 2016Yuval Pinter Yahoo Researchme @ yuvalpinter.comRoi Reichart Yahoo Research & Technion IITroiri @ ie.technion.ac.ilIdan Szpektor Yahoo Researchidan @ yahoo-inc.com"}, {"heading": "1 General", "text": "The record can be accessed via the Yahoo Webscope Homepage1 under Linguistic Data as L-28. The description in Section 2 is in the record as readme.The record certainly has annotation errors that are not covered by the special cases listed in this document.Please contact the first author for any corrections and these will occur in the next release.See Section 4 for known errors."}, {"heading": "2 Dataset Description", "text": "User queries for analyzing syntactic dependencies, version 1.0."}, {"heading": "2.1 Full description", "text": "This dataset contains two files: ydata-search-parsed-queries-dev-v1 0.txt 1,000 queries, 5,344 tokens ydata-search-parsed-queries-test-v1 0.txt 4,000 queries, 26,015 tokens These files differ in their degree of annotation, but share the schema. They contain rows of tabs, each representing a single token in a web query. These lines represent a query whose ID is in the \"test\" set 183 and whose raw form is an arbitrarily selected numeric ID (without blank lines between queries). The field scheme is shown in Table 1. An example query is shown in Table 2.These lines represent a query whose ID is in the \"test\" set 183, and whose raw form a charter school graduate is selected early. It is therefore interpretable: The query consists of two syntactic segments: [Seterschule], which is modified by http: / sandwebop..com]."}, {"heading": "2.2 Linguistic pre-processing notes", "text": "All requests were tokenized with the ClearNLP tokenizer for English (Choi and McCallum, 2013) 2 and not corrected or filtered for adult content. For reasons of Excel convenience, initial quotes were replaced by backticks."}, {"heading": "3 Annotation Guidelines", "text": "In general, parse tree annotators have been instructed to comply with the Stanford Dependency Guidelines (De Marneffe and Manning, 2008) 3 with nec-2version 2.0.1, from www.clearnlp.com 3version 3.5.2, from http: / / nlp.stanford.edu / software / dependencies _ manual.pdfessary caveats that come with error-prone non-standard text. Below are some selected issues that can be seen in the test record. Section 3.1 describes issues related to segmentation marking, and the following subsections cover dependency edge attachment issues."}, {"heading": "3.1 Segmentation Ambiguity", "text": "Noun Strings The most difficult segmentation decisions have been made in cases of long strands of nouns, both common and appropriate. It is based on judgment and the question of whether the phrase can stand as part of a contiguous sentence. A clearer case is the segmentation decision in q.284: [first day of missed period] [Symptoms], where theoretically the day could be characterized as a modifying phrase head of symptoms, but no conceivable well-formed sentence of this formation as a constitution.Sometimes reasonable semantics forced us to conclude a segmentation decision on an unlikely (but syntactically well-formed) single segment component."}, {"heading": "3.2 Proper Names\u2019 Internal Structure", "text": "The dataset contains many examples of product names followed by numbers indicating the model (e.g. Query 2347: How to replace the crank sensor on 95 Saab cs9000).The guidelines in these cases were to place the phrase header as the last alphabetical word (here Saab) and the post modifier (cs9000) as npadvmod.Long proper names with complex internal structures were left as their underlying structures. Query 3252: the first call of the 2-joker is preceded by a phrase head and subsequently analyzed as a prep, further broken down into pobj (von, wild) and deformed (wild, the2).However, more trivial internal structures were smoothed into the common proper name representation: q.3260, jeeves & wooster: nn (wooster, jeeves), nn (wooster, &).Nominal connections were tagged as consecutive NPs e.g. Earth, NP, NP, NP, NP, NP, NP, NP, NP, NP, Nw.34, NP."}, {"heading": "3.3 Truncated Sentences", "text": "Many of the queries in the dataset are, in fact, sentences that are aborted halfway, some because the query is a question of completion (e.g., Query 970 was an important result of the European age of exploration) and some for more obscure reasons (e.g., why are the Russians like this). In both types, where the root of the sentence would normally be the missing addition, the root was (was, was, was, or is) assigned to the symbol closest to it in the assumed complete diagram, with the other relatives collapsing on it. The same applies to any phrase truncated before its grammatical head (e.g., q.3840) or the mandatory addition (e.g., q.1861)."}, {"heading": "3.4 Foreign Languages", "text": "The dataset contains several non-English queries that are treated as nonce (part-of-speech tag = \"FW,\" dependency relation = \"dep\"). Your parse tree is usually a flat tree with the last word (proper name convention) at the top. E.g. q.3299, q.3319. Other cases where foreign words are marked as \"FW\" are when they function metalinguistically. E.g. q.3472, which means baka FW in Japanese."}, {"heading": "3.5 Grammatical Errors", "text": "Since web queries are written in real-time by users with different knowledge and skills, the record contains many grammatical errors (as well as typos - see Section 3.6) that were not corrected during pre-processing to maintain the authenticity of the data. In such cases, unless re-segmentation was fine, the guidelines called for keeping as much of the intended structure as possible. In query 3274, for example, word 4 is meant as part and marked as such as plural nouns. In some cases, different parts of the sentence were fused together due to incorrect grammar, the solution was to represent the fused mark through the head of the intended phrase when it is fully contained (e.g. Q.3233, dogs for dogs), or to ignore a dependent case where there is structural overlap (e.g. in Q.3590, my sister in-in-law, the seizure clause was excluded as a result."}, {"heading": "3.6 Typographical Errors", "text": "In general, obvious typos were treated as intentional words, regardless of whether they led to a legal English word or not. E.g. q.3 w.13, trianlgle NN, or q.3786 w.5 if VBZ (for the intended word). Sometimes typos lead to word fusion, where they were either POS marked as XX (e.g. q.252 w.3) or through the head, if they can be interpreted as a coherent phrase (e.g. q.3115 w.1 searchwhere, which is probably a false concatenation of a meta-linguistic search with the first intended term where and was analyzed as if it were the latter alone). Sometimes, extremely creative tokens are used by users. Look at the glory questioned in 1814: green chemistry in contemporary life. We have separated the present day as if it were a noun and abandoned the phrase to represent the preposition at all."}, {"heading": "3.7 BE-sentences", "text": "Phrases with forms of the verb BE are often ambiguous between attributive sentences (where the BE verb acts as a copula for the head of the following sentence) and appropriate essential statements, where BE is the main verb. We tended to use the former in case of ambiguity, except for very clear cases of materiality (e.g. query 3264 the free market is a myth: root (ROOT, is)) and of course, where the following can only be a clauses or prepositions supplement (e.g. q.799 Trouble is that you think you have time, q.3593, which is on the shelf in season 1 episode 4)."}, {"heading": "4 Known Annotation Errors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Segmentation Errors", "text": "Query ID V 1.0 Correct 194 1.3 1 304 1.3 1 325 1.3 1 362 1.3 1 425 1.4 1 847 1.3 1 911 1.4 1 3779 1.5 1 1147 1 1.3 1812 1 1.4 2784 1 1,2,3 2883 1 1,7 2912 1 1,5,7 3348 1 1,2 3366 1 1,2,3,4"}, {"heading": "4.2 Attachment Errors", "text": "Query.Token V 1.0 Correct 3153.6 2 7 3153.7 6 2CreditsSegmentation tagged by Bettina Bolla, Avihai Mejer, Yuval Pinter, Roi Reichart and Idan Szpektor. Parsing tagged by Shir Givoni and Yuval Pinter."}], "references": [{"title": "Transition-based dependency parsing with selectional branching", "author": ["Choi", "McCallum2013] Jinho D Choi", "Andrew McCallum"], "venue": "ACL", "citeRegEx": "Choi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Stanford typed dependencies manual", "author": ["De Marneffe", "Christopher D Manning"], "venue": "Technical report,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Syntactic parsing of web queries with question intent", "author": ["Pinter et al.2016] Yuval Pinter", "Roi Reichart", "Idan Szpektor"], "venue": "In Proceedings of NAACLHLT,", "citeRegEx": "Pinter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinter et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory.", "startOffset": 33, "endOffset": 54}], "year": 2017, "abstractText": "This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory. Please cite that paper when referencing the dataset. The dataset may be accessed via the Yahoo Webscope homepage1 under Linguistic Data as dataset L-28. The description in Section 2 is included within the dataset as a Readme. The dataset is sure to have annotation errors which are not covered by the special cases specified in this document. Please approach the first author for any corrections and they will appear in the next release. See Section 4 for known errors.", "creator": "LaTeX with hyperref package"}}}