{"id": "1605.03072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Semi-Supervised Representation Learning based on Probabilistic Labeling", "abstract": "In this paper we present a new way of semi-supervised representation learning. The algorithm is based on assigning class probabilities to unlabeled data. The approach will use Hilber-Schmidt Independence Criterion (HSIC) to find a mapping which takes the data to a lower dimensional space. We call this algorithm SSRL-PL. Use of unlabeled data for learning is not always beneficial and there is no algorithm which deterministically guarantee the improvement of the performance by using unlabeled data. Therefore, we also propose a bound on the performance of the algorithm which can be used to determine the effectiveness of using the structure of unlabeled data in the algorithm.", "histories": [["v1", "Tue, 10 May 2016 15:57:18 GMT  (218kb)", "https://arxiv.org/abs/1605.03072v1", null], ["v2", "Mon, 22 Aug 2016 21:27:06 GMT  (218kb)", "http://arxiv.org/abs/1605.03072v2", "9 pages, 7 figures"], ["v3", "Thu, 15 Sep 2016 19:44:51 GMT  (211kb,D)", "http://arxiv.org/abs/1605.03072v3", "8 pages, 7 figures"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ershad banijamali", "ali ghodsi"], "accepted": false, "id": "1605.03072"}, "pdf": {"name": "1605.03072.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Representation Learning based on Probabilistic Labeling", "authors": ["Ershad Banijamali", "Ali Ghodsi"], "emails": ["aghodsib}@uwaterloo.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTIONAs the amount of data grows rapidly, the process of extracting meaningful information becomes increasingly difficult, including the lack of access to the data categories. In the real world, the amount of marked data is almost negligible compared to the unmarked data. On the other hand, identifying data categories or acquiring labels is costly for many reasons, such as being extremely time-consuming for large data sets and usually requiring human monitoring. The importance of the methods that can benefit from this rapidly growing amount of unmarked data has increased significantly. Semi-monitored learning is the area of using unmarked data combined with typically very small amounts of marked data to achieve better data representation or classification accuracy. There is a wide range of applications for semi-monitored learning, including, but not limited to, text classification [1], [14] genetics and medical research [4], [21] and object identification [16]."}, {"heading": "A. Related Works", "text": "In recent years, semi-supervised learning has attracted the attention of many researchers, and several algorithms have been designed for semi-supervised learning that can relate to the current mode of operation. Graph-based algorithms, which normally define a loss function for marked data and use unlabeled as regularizers, are important classes of semi-supervised learning methods. Examples of this class are [5], [26] which attempt to transfer the labels of the data points across the edges of the graph. Label propagation has been tested in many other articles, including [22], which, inspired by the idea of locally linear embedding (LLE) [17], assume that the labels of the data points can be constructed linearly by the labels of their adjacent samples in a sparse neighborhood, and [25] which attempts to propagate the labels over pairs of data points are unsuitable. Transductive Support Vector Machines (SVTMs) are another class of algorithms that are not being manipulated in the way [7]."}, {"heading": "B. Contribution", "text": "Most of the semi-monitored algorithms contain two objective functions for labeled and unlabeled data points, which are jointly optimized. In this work, we also begin with the derivation of two separate objective functions. For labeled points, we are looking for a mapping that maximizes the dependence of transformed points and their labels, and for the unlabeled points, we are looking for a mapping that keeps them close to their labeled neighbors. However, we combine these two functions by some manipulation and solve the problem by optimizing a single objective function. Further research shows that the objective function can also be achieved by specifically mapping labels to the points. We call this probabilistic labeling. This labeling not only delivers the objective functional Xiv: 160 5.03 072v 3 [cs.L G] 15 September 2016 of our problem much faster and easier, but also allows us to get a limitation on the performance of the algorithm based on the probability of a classification error in the original space."}, {"heading": "II. BACKGROUND: HILBER-SCHMIDT INDEPENDENCE CRITERION (HSIC)", "text": "The Hilbert Schmidt Independence Criterion (HSIC) is a very useful statistical tool for measuring the dependence between two random variables [11]. We use the HSIC in our proposed method. The following brief description of this measurement. Definition 1. Let us assume X and Y are two domain sets. Let X and Y be two mappings that represent the corresponding Hilbert space (RKHS) F and G. The Borel probability measurement via X \u00d7 Y is denoted by Pxy. Then the HSIC is defined as follows: HSIC (pxy, F, G) = EX, y (x): EX (x) - EX (2HS) \u2212 \u00b5x - EX (1), where \u00b5x and \u00b5y are the mean of the digit (x) and the digit Y (y) or the tensor product of the tensor."}, {"heading": "III. ALGORITHM SSRL-PL", "text": "rE \"s tis rf\u00fc ide r\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s tis rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "IV. KERNELIZED VERSION", "text": "The advantage of a linear transformation is that it explicitly indicates the basis of the new space as a linear combination of the basis of the original space. However, in many applications a linear transformation is not able to provide a good representation of the data in the new space. An important aspect of our algorithm is its ability to be specified in the kernel form. Based on the theorem of the representer, the matrix V that we find from (12) can be constructed by a linear combination of functions of the data points in the Hilbert space. Let us be the function in Hilbert space. Then V = solution (X) \u03b2. By inserting this matrix into (12) and replacing the matrix (X) with the kernel matrix Kehn, we obtain: Qarg max (\u03b2 > Y) \u03b2 > Y = eigenvalues."}, {"heading": "V. EXPERIMENT RESULTS", "text": "In this section, the evaluation of the application of the above algorithm to different synthetic and real data sets is presented, and the parameters of the algorithm for each experiment are determined by leave-one-out cross-validation."}, {"heading": "A. Toy Example", "text": "In order to demonstrate the performance of the SSRL-PL algorithm, we first apply it to a toy dataset. The two-moon dataset is known for demonstrating the effectiveness of an algorithm on a small set of dots. The dataset includes 200 examples in two almost balanced classes. Here, in Figure 1, the results of applying the SSRL-PL algorithm to the dataset are demonstrated, for both kernelized and non-kernelized versions. The number of labeled dots in each class is 4, i.e. 0.04 of all dots. As you can easily see, the algorithm is able to identify the correct labels based on the probability assignments. In the kernelized version, the new representation also offers the possibility to classify the dots based on a linear discriminant."}, {"heading": "B. Demonstration and Benchmarks", "text": "Here we present the results of applying the algorithm to more sophisticated datasets. The USPS dataset is used to show the generalizability of the algorithm and some other datasets from the UCI repository are used to show the effectiveness of the algorithm in finding a good representation of data suitable for classification, although the dimensionality of the projected space is much lower than the dimensionality of the original space. 1) USPS handwritten datasets consist of 11000 data points in 10 classes. The classes are balanced and each of them has 1100 images of size 16 x 16 from handwritten digits 0 to 9. Therefore, the dimensionality of the samples is 256. In this experiment, we randomly selected 2000 samples for testing and the rest is used only for testing."}, {"heading": "C. Real-world datasets", "text": "Now we examine the performance of the algorithm on six real datasets. MNIST-10K is a set of 10,000 images of handwritten digits, which are randomly selected from the MNIST dataset. USPS is also a set of images of handwritten digits. UMIST is a face recognition dataset. The COIL20 and SBdata are sets of images of various objects. Reuters dataset [10] contains 810,000 English messages in different categories. We followed the same procedure in [19] to obtain 10,000 samples from this set in 4 categories. Other statistics of the datasets are presented in Table IIII.I. We compare the performance of the algorithm through several dictatorial learning algorithms."}, {"heading": "VI. BOUND ON THE PERFORMANCE OF THE SSRL-PL ALGORITHM", "text": "The boundary depends on the way we assign the probabilities to the unmarked data points. (D) Of course, there are situations where the distribution of observed unmarked data points in space can render them useless, and sometimes destructive, in which we set the element with the highest probability to zero and the rest of the elements to zero. (D) We therefore assume that the bottom rows of the Y marker are also only 0 and 1. Also, the data matrix containing n data points is one. (12) We define the following function: fX (V, Y) 4 (V) 2 tr (V > XHY > HY > X)."}, {"heading": "V\u0302 = arg max", "text": "V lim n \u2192 \u221e fX (V, Yn). In [3] it was shown that the deviation of fX (V, Yn) below V \u0445 and V \u0445 is of the order O (1 / \u221a n). Together with the results of theorem 3 this can result in a generalization bound to SSRL-PL."}, {"heading": "VII. CONCLUSION", "text": "The algorithm tries to maximize the similarity between the new representation of data and the label set, with the label set likely to be assigned for blank data and the similarity measure being HSIC. The effectiveness of the proposed algorithm has been evaluated on the basis of various data sets. We have also derived a limit for the proposed algorithm, which can be helpful to see if the presence of blank data is constructive or destructive. In terms of time complexity, the proposed algorithm corresponds to a standard eigenvalue decomposition problem for symmetric matrices. This problem can be efficiently solved, e.g. by methods of single value decomposition (SVD). However, for faster implementation, the use of deep auto encoders that are able to estimate eigenvalue decomposition of their inputs would be interesting. Alternatively, you can train a network that maximizes the dependence of data between the HSIC and the optimization mechanism."}, {"heading": "VIII. APPENDIX", "text": "The proof for Lemma 2: It is known that: \"Tr\" (AA >) = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"V\" = \"V\" (V, Y) \u2212 \"V\" n \"n\" n \"n\" n (V, Y): V \"n\" n \"n\" n \"n\" n \"n. (V, V\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n =\" n \"n\" n). (V \"n =\" n. \"n\" n \"n\" n \"n\" n. (V, V \"n =\" n. \"n). (V\" n) n \"n\" n. (V, Y) n \"n\" n \"n."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we present a new algorithm for semi-<lb>supervised representation learning. In this algorithm, we first find<lb>a vector representation for the labels of the data points based<lb>on their local positions in the space. Then, we map the data to<lb>lower-dimensional space using a linear transformation such that<lb>the dependency between the transformed data and the assigned<lb>labels is maximized. In fact, we try to find a mapping that is as<lb>discriminative as possible. The approach will use Hilber-Schmidt<lb>Independence Criterion (HSIC) as the dependence measure.<lb>We also present a kernelized version of the algorithm, which<lb>allows non-linear transformations and provides more flexibility<lb>in finding the appropriate mapping. Use of unlabeled data<lb>for learning new representation is not always beneficial and<lb>there is no algorithm that can deterministically guarantee the<lb>improvement of the performance by exploiting unlabeled data.<lb>Therefore, we also propose a bound on the performance of the<lb>algorithm, which can be used to determine the effectiveness of<lb>using the unlabeled data in the algorithm. We demonstrate the<lb>ability of the algorithm in finding the transformation using both<lb>toy examples and real-world datasets.", "creator": "LaTeX with hyperref package"}}}