{"id": "1605.03142", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Self-Modification of Policy and Utility Function in Rational Agents", "abstract": "Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.", "histories": [["v1", "Tue, 10 May 2016 18:25:49 GMT  (118kb,D)", "http://arxiv.org/abs/1605.03142v1", "Artificial General Intelligence (AGI) 2016"]], "COMMENTS": "Artificial General Intelligence (AGI) 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom everitt", "daniel filan", "mayank daswani", "marcus hutter"], "accepted": false, "id": "1605.03142"}, "pdf": {"name": "1605.03142.pdf", "metadata": {"source": "CRF", "title": "Self-Modification of Policy and Utility Function in Rational Agents\u2217", "authors": ["Tom Everitt", "Daniel Filan", "Mayank Daswani", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "AI safety, self-modification, AIXI, general reinforcement learning, utility", "text": "Functions, wiring, planning"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Preliminaries 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Self Modification Models 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Agents 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Results 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Conclusions 13", "text": "Bibliography 15"}, {"heading": "A Optimal Policies 17", "text": "\u0445 A shorter version of this paper will be presented at AGI-16 (Everitt et al., 2016) Xiv: 160 5.03 142v 1 [cs.A I] 1 0M ay2 016"}, {"heading": "1 Introduction", "text": "In fact, it is important that agents improve themselves by asymptomatically building optimal agents (Schmidhuber, 2007). In view of the ever-accelerating development of artificial intelligence and the problems that can arise when we are unable to control a universally intelligent agent (Bostrom, 2014), it is important to develop a theory for controlling agents of all kinds (Schmidhuber, 2007). Highly intelligent agents are unable to find ways to modify themselves, and the problems that can arise when we are unable to control a universally intelligent agent (Bostrom, 2014)."}, {"heading": "2 Preliminaries", "text": "Most of the following terms have become standard in the general affirmation literature (GRL) (Hutter, 2005, 2014). < < < < < < p (Hutter, 2005, Sec. 4,3 and Def. 5,7) In the standard cybernetic model, an agent interacts with an environment in cycles. The agent selects actions from a finite series of actions, and the environment reacts with a finite series of actions (see Fig. 1)."}, {"heading": "3 Self Modification Models", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / Agent's actions will only have an impact on the environment; the agent himself will only be indirectly influenced by perceptions; however, this is unrealistic if the agent is part of the environment with which he interacts; < / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "4 Agents", "text": "In this section, we define three types of agents that differ in the way their value functions depend on self-modification. < A value function is a function V: (A) \u00b7 (A) \u00b7 (A) \u00b7 (R) \u00b7 (R) that maps the politics and history of expected benefits. < Since highly intelligent agents can find unexpected ways to optimize a function (see, for example, Bird and Layzell 2002), it is important to use value functions in such a way that any policy that optimizes the value function also optimizes the behavior that we want from the agent. We will measure an agent's performance based on his (highly anticipated) u1 service, tactically assuming that u1 correctly captures what we want from the agent. < Everitt and Hutter (2016) are developing a promising proposal on how to define an appropriate initial service function. (Agent Performance) An agent's performance is his expected performance."}, {"heading": "5 Results", "text": "In this section, we will give the results on how our three different actors behaved. < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "6 Conclusions", "text": "In fact, it's important to develop a theory for increasing intelligence (Legg and Hutter, 2008), but the result is not what we want, but what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do. It's what we do."}, {"heading": "Acknowledgements", "text": "This work is the result of a MIRIx workshop. We would like to thank the (unauthorized) participants David Johnston and Samuel Rathmanner as well as John Aslanides, Jan Leike and Laurent Orseau, who read drafts and gave valuable suggestions."}, {"heading": "A Optimal Policies", "text": "For the realistic value functions in which future policy is determined from the outset by the next action, an optimal policy is simply a policy. < < < Politics is simply a policy that is both non-modifying and modifying. Theorem 20 is weaker than Theorem 16 in the sense that it only indicates the existence of an unmodifying optimal policy, whereas Theorem 16 shows that all optimal policies are (essentially) non-modifying. Theorem 20 is weaker than Theorem 16 in the sense that it only indicates the existence of a non-modifying optimal policy, while Theorem 16 shows that all optimal policies are non-modifying. As a guarantee against self-modification, Theorem 20 is on par with Hibbard (2012, Prop. 4). The evidence is very different because Hibbard assumes the existence of an optimal policy."}], "references": [{"title": "The evolved radio and its implications", "author": ["J. Bibliography Bird", "P. Layzell"], "venue": null, "citeRegEx": "Bird and Layzell,? \\Q2002\\E", "shortCiteRegEx": "Bird and Layzell", "year": 2002}, {"title": "Learning what to value", "author": ["D. Press. Dewey"], "venue": "In AGI-11,", "citeRegEx": "Dewey,? \\Q2011\\E", "shortCiteRegEx": "Dewey", "year": 2011}, {"title": "policy and utility function in rational agents", "author": ["T. Springer. Everitt", "M. Hutter"], "venue": null, "citeRegEx": "Everitt and Hutter,? \\Q2016\\E", "shortCiteRegEx": "Everitt and Hutter", "year": 2016}, {"title": "Model-based utility functions", "author": ["B. Springer. Hibbard"], "venue": "Journal of Artificial General", "citeRegEx": "Hibbard,? \\Q2012\\E", "shortCiteRegEx": "Hibbard", "year": 2012}, {"title": "Universal Artificial Intelligence", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter,? \\Q2005\\E", "shortCiteRegEx": "Hutter", "year": 2005}, {"title": "General time consistent discounting", "author": ["T. Lattimore", "M. Hutter"], "venue": null, "citeRegEx": "Lattimore and Hutter,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter", "year": 2014}, {"title": "Universal intelligence: A definition of machine", "author": ["S. 519:140\u2013154. Legg", "M. Hutter"], "venue": null, "citeRegEx": "Legg and Hutter,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter", "year": 2007}, {"title": "Bad universal priors and notions of optimality", "author": ["J. Leike", "M. Hutter"], "venue": null, "citeRegEx": "Leike and Hutter,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter", "year": 2015}, {"title": "Human-level control through", "author": ["V. Mnih", "K. Kavukcuoglu", "D Silver"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Universal knowledge-seeking agents", "author": ["L. Press. Orseau"], "venue": null, "citeRegEx": "Orseau,? \\Q2014\\E", "shortCiteRegEx": "Orseau", "year": 2014}, {"title": "Space-time embedded intelligence", "author": ["L. Springer. Orseau", "M. Ring"], "venue": null, "citeRegEx": "Orseau and Ring,? \\Q2012\\E", "shortCiteRegEx": "Orseau and Ring", "year": 2012}, {"title": "Delusion, survival, and intelligent agents", "author": ["M. Ring", "L. Orseau"], "venue": null, "citeRegEx": "Ring and Orseau,? \\Q2011\\E", "shortCiteRegEx": "Ring and Orseau", "year": 2011}, {"title": "G\u00f6del machines: Fully self-referential optimal universal", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "Schmidhuber,? \\Q2007\\E", "shortCiteRegEx": "Schmidhuber", "year": 2007}, {"title": "Mastering the game of Go", "author": ["D. Springer. Silver", "A. Huang", "Maddison", "C. J"], "venue": "self-improvers. In AGI-07,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["N. MIRI. Soares", "B. Fallenstein", "E. Yudkowsky", "S. Armstrong"], "venue": "Corrigibil-", "citeRegEx": "Soares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soares et al\\.", "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "AAAI Workshop on AI and Ethics,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Artificial Superintelligence: A Futuristic Approach", "author": ["R.V. Press. Yampolskiy"], "venue": null, "citeRegEx": "Yampolskiy,? \\Q2015\\E", "shortCiteRegEx": "Yampolskiy", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Indeed, enabling agents to self-improve has even been suggested as a way to build asymptotically optimal agents (Schmidhuber, 2007).", "startOffset": 112, "endOffset": 131}, {"referenceID": 2, "context": "A companion paper (Everitt and Hutter, 2016) addresses the related problem of agents subverting the evidence they receive, rather than modifying themselves.", "startOffset": 18, "endOffset": 44}, {"referenceID": 9, "context": "Indeed, enabling agents to self-improve has even been suggested as a way to build asymptotically optimal agents (Schmidhuber, 2007). Given the increasingly rapid development of artificial intelligence and the problems that can arise if we fail to control a generally intelligent agent (Bostrom, 2014), it is important to develop a theory for controlling agents of any level of intelligence. Since it would be hard to keep highly intelligent agents from figuring out ways to self-modify, getting agents to not want to self-modify should yield the more robust solution. In particular, we do not want agents to make self-modifications that affect their future behaviour in detrimental ways. For example, one worry is that a highly intelligent agent would change its goal to something trivially achievable, and thereafter only strive for survival. Such an agent would no longer care about its original goals. In an influential paper, Omohundro (2008) argued that the basic drives of any sufficiently intelligent system include a drive for goal preservation.", "startOffset": 113, "endOffset": 947}, {"referenceID": 15, "context": "GRL generalises the standard (PO)PMD models of reinforcement learning (Kaelbling et al., 1998; Sutton and Barto, 1998) by making no Markov or ergodicity assumptions (Hutter, 2005, Sec.", "startOffset": 70, "endOffset": 118}, {"referenceID": 2, "context": "The main advantage of utility functions over RL is that the agent\u2019s actions can be incorporated into the goal specification, which can prevent self-delusion problems such as the agent manipulating the reward signal (Everitt and Hutter, 2016; Hibbard, 2012; Ring and Orseau, 2011).", "startOffset": 215, "endOffset": 279}, {"referenceID": 3, "context": "The main advantage of utility functions over RL is that the agent\u2019s actions can be incorporated into the goal specification, which can prevent self-delusion problems such as the agent manipulating the reward signal (Everitt and Hutter, 2016; Hibbard, 2012; Ring and Orseau, 2011).", "startOffset": 215, "endOffset": 279}, {"referenceID": 11, "context": "The main advantage of utility functions over RL is that the agent\u2019s actions can be incorporated into the goal specification, which can prevent self-delusion problems such as the agent manipulating the reward signal (Everitt and Hutter, 2016; Hibbard, 2012; Ring and Orseau, 2011).", "startOffset": 215, "endOffset": 279}, {"referenceID": 9, "context": "Non-RL suggestions for utility functions include knowledge-seeking agents with u(\u00e6<t) = 1 \u2212 \u03c1(\u00e6<t) (Orseau, 2014), as well as value learning approaches where the utility 1To fit the knowledge-seeking agent into our framework, our definition deviates slightly from Orseau (2014).", "startOffset": 99, "endOffset": 113}, {"referenceID": 2, "context": "The main advantage of utility functions over RL is that the agent\u2019s actions can be incorporated into the goal specification, which can prevent self-delusion problems such as the agent manipulating the reward signal (Everitt and Hutter, 2016; Hibbard, 2012; Ring and Orseau, 2011). Non-RL suggestions for utility functions include knowledge-seeking agents with u(\u00e6<t) = 1 \u2212 \u03c1(\u00e6<t) (Orseau, 2014), as well as value learning approaches where the utility 1To fit the knowledge-seeking agent into our framework, our definition deviates slightly from Orseau (2014).", "startOffset": 216, "endOffset": 559}, {"referenceID": 1, "context": "function is learnt during interaction (Dewey, 2011).", "startOffset": 38, "endOffset": 51}, {"referenceID": 3, "context": "The policy self-modification model is similar to the models investigated by Orseau and Ring (2011, 2012) and Hibbard (2012). In the papers by Orseau and Ring, policy names are called programs or codes; Hibbard calls them selfmodifying policy functions.", "startOffset": 109, "endOffset": 124}, {"referenceID": 3, "context": "The policy self-modification model is similar to the models investigated by Orseau and Ring (2011, 2012) and Hibbard (2012). In the papers by Orseau and Ring, policy names are called programs or codes; Hibbard calls them selfmodifying policy functions. The interpretation is similar in all cases: some of the actions can affect the agent\u2019s future policy. Note that standard MDP algorithms such as SARSA and Q-learning that evolve their policy as they learn do not make policy modifications in our framework. They follow a single policy (A\u00d7E)\u2217 \u2192 A, even though their state-to-action map evolves. Example 4 (G\u00f6del machine). Schmidhuber (2007) defines the G\u00f6del machine as an agent that at each time step has the opportunity to rewrite any part of its source code.", "startOffset": 109, "endOffset": 641}, {"referenceID": 9, "context": "See Orseau and Ring (2012) for a model where the environment can change the agent.", "startOffset": 4, "endOffset": 27}, {"referenceID": 0, "context": "Bird and Layzell 2002), it is important to use value functions such that any policy that optimises the value function will also optimise the behaviour we want from the agent. We will measures an agent\u2019s performance by its (\u03c1re-expected) u1-utility, tacitly assuming that u1 properly captures what we want from the agent. Everitt and Hutter (2016) develop a promising suggestion for how to define a suitable initial utility function.", "startOffset": 0, "endOffset": 347}, {"referenceID": 8, "context": "Orseau and Ring (2011) and Hibbard (2012) discuss value functions equivalent to Definition 12.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Orseau and Ring (2011) and Hibbard (2012) discuss value functions equivalent to Definition 12.", "startOffset": 27, "endOffset": 42}, {"referenceID": 11, "context": "Although related, we would like to distinguish hedonistic self-modification from wireheading or self-delusion (Ring and Orseau, 2011; Yampolskiy, 2015).", "startOffset": 110, "endOffset": 151}, {"referenceID": 16, "context": "Although related, we would like to distinguish hedonistic self-modification from wireheading or self-delusion (Ring and Orseau, 2011; Yampolskiy, 2015).", "startOffset": 110, "endOffset": 151}, {"referenceID": 2, "context": "Wireheading is addressed in a companion paper (Everitt and Hutter, 2016).", "startOffset": 46, "endOffset": 72}, {"referenceID": 14, "context": "For example: \u2022 Corrigibility (Soares et al., 2015).", "startOffset": 29, "endOffset": 50}, {"referenceID": 1, "context": "\u2022 Value learning (Dewey, 2011).", "startOffset": 17, "endOffset": 30}, {"referenceID": 7, "context": "Bayes-optimal agents may not explore sufficiently (Leike and Hutter, 2015).", "startOffset": 50, "endOffset": 74}, {"referenceID": 15, "context": "This can be mended by \u03b5-exploration (Sutton and Barto, 1998) or Thompson-sampling (Leike et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 6, "context": "Indeed, perfect rationality may be viewed as a limit point for increasing intelligence (Legg and Hutter, 2007; Omohundro, 2008).", "startOffset": 87, "endOffset": 127}, {"referenceID": 9, "context": "Ring and Orseau (2011) develop a model of the latter possibility.", "startOffset": 9, "endOffset": 23}, {"referenceID": 8, "context": "Since many general AI systems are constructed around RL and value functions (Mnih et al., 2015; Silver et al., 2016), we hope our conclusions can provide meaningful guidance.", "startOffset": 76, "endOffset": 116}, {"referenceID": 13, "context": "Since many general AI systems are constructed around RL and value functions (Mnih et al., 2015; Silver et al., 2016), we hope our conclusions can provide meaningful guidance.", "startOffset": 76, "endOffset": 116}, {"referenceID": 1, "context": "One promising venue for constructing good utility functions is value learning (Bostrom, 2014; Dewey, 2011; Everitt and Hutter, 2016; Soares, 2015).", "startOffset": 78, "endOffset": 146}, {"referenceID": 2, "context": "One promising venue for constructing good utility functions is value learning (Bostrom, 2014; Dewey, 2011; Everitt and Hutter, 2016; Soares, 2015).", "startOffset": 78, "endOffset": 146}], "year": 2016, "abstractText": "Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify \u2013 for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby \u2018escaping\u2019 the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.", "creator": "LaTeX with hyperref package"}}}