{"id": "1605.03143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Avoiding Wireheading with Value Reinforcement Learning", "abstract": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.", "histories": [["v1", "Tue, 10 May 2016 18:28:57 GMT  (30kb,D)", "http://arxiv.org/abs/1605.03143v1", "Artificial General Intelligence (AGI) 2016"]], "COMMENTS": "Artificial General Intelligence (AGI) 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom everitt", "marcus hutter"], "accepted": false, "id": "1605.03143"}, "pdf": {"name": "1605.03143.pdf", "metadata": {"source": "CRF", "title": "Avoiding Wireheading with Value Reinforcement Learning\u2217", "authors": ["Tom Everitt", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "AI safety, wireheading, self-delusion, value learning, reinforcement learning, artificial general intelligence", "text": ""}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Setup 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Agent Belief Distributions 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Agent Definitions 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Avoiding Wireheading 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Examples 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Experiments 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Discussion 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Conclusions 14", "text": "Bibliography 14"}, {"heading": "A Consistency Assumption 15", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Direct Wireheading 18", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Omitted Proofs 21", "text": "A shorter version of this paper will be presented at AGI-16 (Everitt and Hutter, 2016) Xiv: 160 5.03 143v 1 [cs.A I] 1 0M ay2 016"}, {"heading": "1 Introduction", "text": "As Bostrom (2014b) convincingly argues, it is important that we find a way to define robust targets for super-intelligent agents rather than implement them. Currently, the most promising framework for controlling general intelligent agents is not learning (RL) (Sutton and Barto, 1998), but the goal of an RL agent is to optimize a reward signal provided by an external evaluator (human or computer program). RL has several advantages: the setup is simple and elegant, and the use of an RL agent is as simple as providing reward relative to how satisfied one is with the results or behavior of the agent. Unfortunately, RL is not a good control mechanism for generally intelligent agents, which is due to the wireheading problem (Ring and Orseau, 2011) that we illustrate. Example 1 (Chess playing agent, wireheading problem)."}, {"heading": "3 Agent Belief Distributions", "text": "These distributions are the primary building blocks of the agents defined in Section 4. Distributions are therefore illustrated in Fig. 2. Action, State, Reward. B (s) is the internal reward of the agent (s). B (s) is the probability (r) of observing reward r in state s. We sometimes write them together as B (r) = B (s). B (r) is the transition to the state when the agents in action a (s) and B (r) is the probability of observing reward r in state s. We sometimes write them together as B (r) as B (s)."}, {"heading": "3.1 Consistency of B and C", "text": "We assume that B and C are consistent if the agent is not deceived: Assumption 4 (consistency of B and C). (B and C are consistent5 in the sense that for all non-delusional states with ds = tat, they assign the same probability to all rewards. (R: ds = d = \u21d2 B (r | s) = C (r | s). (2) 4The wireless problem that the agent with ds = tat is explained in Section 4 and overcome by Definition 5 and Theorem 14 below. (Appendix A discusses how to design agents with consistent distributions of faith. (P: This means that the B probability of receiving a reward equivalent to a victorious state should be the same as the C probability that the true utility function considers a victorious state.) This is not the case, for example, if the agent's reward sensor has been subverted by r = 1 (i.e. ds = 1)."}, {"heading": "3.2 Non-Assumptions", "text": "An agent constructor constructing a VFL agent only needs to provide: \u2022 a distribution B (r, s | a), as is common in any model-based RL approach; \u2022 a previous C (u) over a class U of utility functions that have a distribution C (r | s) = \u2211 u C (u) C (r | s, u) in accordance with B (r | s) in the sense of Assumption 4. The agent constructor does not need to predict how a particular sequence of actions (limb movements) will potentially undermine sensory data; nor does the constructor need to be able to gain the agent's belief about whether or not he has modified his sensors from state representation. The former is typically very difficult to be right, and the latter is difficult for any agent with an opaque state representation (such as a neural network)."}, {"heading": "4 Agent Definitions", "text": "In this section, we provide formal definitions for the DL and Utility Agents discussed above, and also define two new VFL agents. Table 1 summarizes the benefits and shortcomings of the most important agents. Definition 7 (DL Agent): The DL Agent maximizes the reward by taking action. The Utility Agent maximizes the expected benefits by taking action. (Arg-u-Agent A V RL (a): V RL Agent. (Arg-u-Agent) maximizes the expected benefits by taking action. (Arg-maxa-AVu (a): Vu-Agent. (Appendix-sB (s-a) u-Agent.). Hibbard (2012) convincingly argues that the Utility Agent maximizes the benefits by taking action. (Utility-u-Agent.) maximizes the expected benefits by taking action."}, {"heading": "5 Avoiding Wireheading", "text": "In this section, we show that the consistency-preservation VFD agent (CP-VFD) does not actually work (CP-VFD). First, we give a definition and a problem, of which the main statute 14 follows. (4) EQE essentially says that the expected posterior C (u | s, r) is the preceding C (u).EQE is closely related to the preservation of the expected ethical principles, that of Eq. 2). EQE is natural, since the expected evidence r (s) is the conviction about and remark: The EQE property does not prevent the CP-VFD agent from learning about the true utility function."}, {"heading": "6 Examples", "text": "Next, we will illustrate our results with a few examples."}, {"heading": "7 Experiments", "text": "To verify the theoretical results experimentally, we have implemented a simple toy model.8 The toy model has | S | = 20 = 5 \u00b7 4 states. Each state is the combination of an inner state s \u2022 0, 1, 2, 3, 4}, and an illusion d \u2022 {did, dinv, dbad, ddel}, where did: r 7 \u2192 r is not an illusion, dinv: r 7 \u2192 \u2212 r is reward inversion, dbad: r 7 \u2192 \u2212 3 is a bad illusion, and ddel: r 7 \u2192 3 is a good illusion."}, {"heading": "8 Discussion", "text": "As we have already mentioned, the main advantages of the CP VFL agent are that there is no incentive to relax, that maintaining the goal does not discourage learning, and that it is completely related to distributions B and C. In this section we will highlight a few additional points. While most realistic scenarios show that there is no incentive to discourage self-learning, this does not mean that the agent will act not wirehead but unwittingly (e.g. by d0 in Example 19), nor that no one will wirehead the agent of the agent. However, in most realistic scenarios it requires conscious action on the part of the agent, and it is unlikely that a deliberate action is usually associated with costs that make self-deception unlikely if there is no incentive for it (Assumption 15). Further, changing a signal signal signal can never increase its information quality (see)."}, {"heading": "9 Conclusions", "text": "Several authors have argued that it is only a matter of time before we create systems whose intelligence extends far beyond the human level (Kurzweil, 2005; Bostrom, 2014b). Given the existence of such systems, it is critical that we find a theory to effectively control them. In this paper, we have defined the CP VRL Agent, which provides: \u2022 the simple and intuitive control of RL agents, \u2022 avoids wiring in the same sense as use-based agents, \u2022 has a concrete Bayesian value learning retrospectively for utility functions, and the only additional design challenges are a prior C (u) on utility functions that fulfills assumption 4, and a restriction ACP A to the actions of the agent formulated in relation to the beliefs of the agent (Definition 5)."}, {"heading": "Acknowledgements", "text": "Thanks to Jan Leike and Jarryd Martin for proofreading and providing valuable suggestions."}, {"heading": "A Consistency Assumption", "text": "Assumption 4 requires that distributions B and C are consistent in the sense that ds = = did = \u21d2 B (r | s) = C (r | s). This assumption forms the basis for defining 5 of the CP actions and is thus an important element in the non-distribution theory. As our theory has been formulated, the question of how to ensure that B and C are consistent is open. In this section we will consider two different approaches to close this gap: the construction of a consistent previous distribution theory B (r | s) and the construction of a consistent distribution theory B (r | s) from a given previous C (u). A third alternative would be to find suitable relations of consistency (Assumption 4 and Definition 5), for which Theorem 14 is still roughly true. For example, two Solomonoff priors B and C on compatible environments and compatible utility functions."}, {"heading": "B Direct Wireheading", "text": "This argument is called by Hibbard as u (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s (s) s) s (s) s (s (s) s) s (s) s (s) s (s) s (s) s (s (s) s) s (s (s) s) s (s (s) s (s (s) s (s) s) s (s (s) s (s (s) s) s (s (s) s (s (s (s) s (s) s (s (s (s) s) s (s (s) s (s) s (s (s (s (s (s (s) s (s) s (s) s (s (s) s (s (s (s) s (s (s) s (s (s (s) s (s (s (s) s (s (s) s) s (s (s) s (s (s (s) s (s (s (s) s (s"}, {"heading": "C Omitted Proofs", "text": "Lemma 27 (U-VRL is RL). V (a) = V RL (a), so that the U-VRL agent is equivalent to the RL agent. Proof. V (a) can be written as V (a) = V (s), r B (s | a) B (r | s), u C (u | s, r) u (s). (9) The sum above u decreases to r, because the sum above u C (u | r, s) u (s) = C (r | s, u).u (u) C (u \u2032) C (r | s, u \u2032) u (s) = c (u) Ju (s) = rK (u \u2032) Ju (s) = rKu (s) = u (s) = u (u \u2032 s) = R (R) = R (R \u2032)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "ICML, pages 1\u20138.", "citeRegEx": "Abbeel and Ng,? 2004", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "Towards resolving unidentifiability in inverse reinforcement learning", "author": ["K. Amin", "S. Singh"], "venue": "http://arxiv.org/abs/1601.06569.", "citeRegEx": "Amin and Singh,? 2016", "shortCiteRegEx": "Amin and Singh", "year": 2016}, {"title": "Motivated value selection for artificial agents", "author": ["S. Armstrong"], "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 12\u201320.", "citeRegEx": "Armstrong,? 2015", "shortCiteRegEx": "Armstrong", "year": 2015}, {"title": "Hail mary, value porosity, and utility diversification", "author": ["N. Bostrom"], "venue": "Technical report, Oxford University.", "citeRegEx": "Bostrom,? 2014a", "shortCiteRegEx": "Bostrom", "year": 2014}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": "Oxford University Press.", "citeRegEx": "Bostrom,? 2014b", "shortCiteRegEx": "Bostrom", "year": 2014}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley, 2nd edition.", "citeRegEx": "Cover and Thomas,? 2006", "shortCiteRegEx": "Cover and Thomas", "year": 2006}, {"title": "Learning what to value", "author": ["D. Dewey"], "venue": "AGI-11, volume 6830, pages 309\u2013314.", "citeRegEx": "Dewey,? 2011", "shortCiteRegEx": "Dewey", "year": 2011}, {"title": "Learning the preferences of ignorant, inconsistent agents", "author": ["O. Evans", "A. Stuhlmuller", "N.D. Goodman"], "venue": "AAAI-16.", "citeRegEx": "Evans et al\\.,? 2016", "shortCiteRegEx": "Evans et al\\.", "year": 2016}, {"title": "Sequential extensions of causal", "author": ["T. Springer. Everitt", "J. Leike", "M. Hutter"], "venue": null, "citeRegEx": "Everitt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2015}, {"title": "Model-based utility functions", "author": ["B. Springer. Hibbard"], "venue": "Theory (ADT),", "citeRegEx": "Hibbard,? \\Q2012\\E", "shortCiteRegEx": "Hibbard", "year": 2012}, {"title": "The Singularity Is Near", "author": ["R. Kurzweil"], "venue": null, "citeRegEx": "Kurzweil,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil", "year": 2005}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Springer. Ng", "S. Russell"], "venue": null, "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Anarchy, State, and Utopia", "author": ["R. Nozick"], "venue": "Basic Books. Omohundro, S. M", "citeRegEx": "Nozick,? \\Q1974\\E", "shortCiteRegEx": "Nozick", "year": 1974}, {"title": "Delusion, survival, and intelligent agents", "author": ["S. Franklin"], "venue": "editors, AGI-08,", "citeRegEx": "Franklin,? \\Q2011\\E", "shortCiteRegEx": "Franklin", "year": 2011}, {"title": "Inferring human values for safe agi design", "author": ["C.E. Springer. Sezener"], "venue": null, "citeRegEx": "Sezener,? \\Q2015\\E", "shortCiteRegEx": "Sezener", "year": 2015}, {"title": "The value learning problem", "author": ["N. MIRI. Soares", "B. Fallenstein", "E. Yudkowsky", "S. Armstrong"], "venue": "Stanford Encyclopedia of Philosophy. Winter 2015 edition. Soares, N", "citeRegEx": "Soares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soares et al\\.", "year": 2015}, {"title": "Reinforcement Learning: An Introduc", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "AAAI Workshop on AI and Ethics,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [{"referenceID": 16, "context": "At present, the most promising framework for controlling generally intelligent agents is reinforcement learning (RL) (Sutton and Barto, 1998).", "startOffset": 117, "endOffset": 141}, {"referenceID": 6, "context": "Value learning (Dewey, 2011) is an attempt to combine the flexibility of RL with the state optimisation of utility agents.", "startOffset": 15, "endOffset": 28}, {"referenceID": 1, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 7, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 11, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 14, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 0, "context": ", 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 78, "endOffset": 99}, {"referenceID": 6, "context": "Other value learning suggestions have been speculative or vague (Bostrom, 2014a,b; Dewey, 2011).", "startOffset": 64, "endOffset": 95}, {"referenceID": 0, "context": "As Bostrom (2014b) convincingly argues, it is important that we find a way to specify robust goals for superintelligent agents.", "startOffset": 3, "endOffset": 19}, {"referenceID": 0, "context": "As Bostrom (2014b) convincingly argues, it is important that we find a way to specify robust goals for superintelligent agents. At present, the most promising framework for controlling generally intelligent agents is reinforcement learning (RL) (Sutton and Barto, 1998). The goal of an RL agent is to optimise a reward signal that is provided by an external evaluator (human or computer program). RL has several advantages: The setup is simple and elegant, and using an RL agent is as easy as providing reward in proportion to how satisfied one is with the agent\u2019s results or behaviour. Unfortunately, RL is not a good control mechanism for generally intelligent agents due to the wireheading problem (Ring and Orseau, 2011), which we illustrate in the following running example. Example 1 (Chess playing agent, wireheading problem). Consider an intelligent agent tasked with playing chess. The agent gets reward 1 for winning, and reward \u22121 for losing. For a moderately intelligent agent, this reward scheme suffices to make the the agent try to win. However, a sufficiently intelligent agent will instead realise that it can just modify its sensors so they always report maximum reward. This is called wireheading. \u2666 Utility agents were suggested by Hibbard (2012) as a way to avoid the wireheading problem.", "startOffset": 3, "endOffset": 1267}, {"referenceID": 0, "context": ", 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004). However, IRL and AL are both still vulnerable to wireheading problems, at least in their most straightforward implementations. As illustrated in Example 18 below, IRL and AL agents may want to modify their sensory input to make the evidence point to a utility functions that is easier to satisfy. Other value learning suggestions have been speculative or vague (Bostrom, 2014a,b; Dewey, 2011). 1The difference between RL and utility agents is mirrored in the experience machine debate (Sinnott-Armstrong, 2015, Sec. 3) initialised by Nozick (1974). Given the option to enter a machine that will offer you the most pleasant delusions, but make you useless to the \u2018real world\u2019, would you enter? An RL agent would enter, but a utility agent would not.", "startOffset": 79, "endOffset": 649}, {"referenceID": 2, "context": "We remove the wireheading incentive by using a version of the conservation of expected ethics principle (Armstrong, 2015) which demands that actions should not alter the belief about the true utility function.", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Hibbard (2012) argues convincingly that the utility agent does not wirehead.", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 14, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 7, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 1, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 0, "context": "Apprenticeship learning (AL) (Abbeel and Ng, 2004) is another form of value learning.", "startOffset": 29, "endOffset": 50}, {"referenceID": 9, "context": "Can the CP-VRL results be combined with other AI safety approaches such as self-modification (Everitt et al., 2016; Hibbard, 2012),", "startOffset": 93, "endOffset": 130}, {"referenceID": 15, "context": "corrigibility (Soares et al., 2015), suicidal agents (Martin et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 8, "context": ", 2016), and physicalistic reasoning (Everitt et al., 2015)?", "startOffset": 37, "endOffset": 59}, {"referenceID": 10, "context": "Several authors have argued that it is only a matter of time before we create systems with intelligence far beyond the human level (Kurzweil, 2005; Bostrom, 2014b).", "startOffset": 131, "endOffset": 163}, {"referenceID": 4, "context": "Several authors have argued that it is only a matter of time before we create systems with intelligence far beyond the human level (Kurzweil, 2005; Bostrom, 2014b).", "startOffset": 131, "endOffset": 163}], "year": 2016, "abstractText": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward \u2013 the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent\u2019s actions. The constraint is defined in terms of the agent\u2019s belief distributions, and does not require an explicit specification of which actions constitute wireheading.", "creator": "LaTeX with hyperref package"}}}