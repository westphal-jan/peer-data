{"id": "1605.03261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs", "abstract": "The paper presents a neurorobotics cognitive model to explain the understanding and generalisation of nouns and verbs combinations when a vocal command consisting of a verb-noun sentence is provided to a humanoid robot. This generalisation process is done via the grounding process: different objects are being interacted, and associated, with different motor behaviours, following a learning approach inspired by developmental language acquisition in infants. This cognitive model is based on Multiple Time-scale Recurrent Neural Networks (MTRNN).With the data obtained from object manipulation tasks with a humanoid robot platform, the robotic agent implemented with this model can ground the primitive embodied structure of verbs through training with verb-noun combination samples. Moreover, we show that a functional hierarchical architecture, based on MTRNN, is able to generalise and produce novel combinations of noun-verb sentences. Further analyses of the learned network dynamics and representations also demonstrate how the generalisation is possible via the exploitation of this functional hierarchical recurrent network.", "histories": [["v1", "Wed, 11 May 2016 02:31:21 GMT  (1672kb,D)", "http://arxiv.org/abs/1605.03261v1", "Submitted to Autonomous Robots"]], "COMMENTS": "Submitted to Autonomous Robots", "reviews": [], "SUBJECTS": "cs.RO cs.CL", "authors": ["junpei zhong", "martin peniak", "jun tani", "tetsuya ogata", "angelo cangelosi"], "accepted": false, "id": "1605.03261"}, "pdf": {"name": "1605.03261.pdf", "metadata": {"source": "CRF", "title": "Sensorimotor Input as a Language Generalisation Tool A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs", "authors": ["Junpei Zhong", "Jun Tani", "Angelo Cangelosi"], "emails": ["zhong@junpei.eu"], "sections": [{"heading": null, "text": "J. Zhong Department of Intermedia Art and Science, Waseda University, 3-4-1 Ohkubo, Shinjuku, Tokyo, Japan, 169-8555 Centre for Robotics and Neural Systems, University of Plymouth, PL4 8AA, United Kingdom Tel.: + 44 (0) 175284908, Email: zhong @ junpei.eu M. Peniak Cortexica Vision Systems, London, United Kingdom J. Tani Korea Advanced Institute of Science and Technology, Daejeon, South Korea T. Ogata Department of Intermedia Art and Science, Waseda University, Tokyo, Japan A. Cangelosi Centre for Robotics and Neural Systems, University of Plymouth, United Kingdom Keywords Recurrent Artificial Neural Networks \u00b7 Language Learning \u00b7 Multiple Time-scale Recurrent Neural Network \u00b7 Development Robotics \u00b7 Neurorobotics"}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "2 The Multiple Timescale Recurrent Neural Network Model", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2.2.1 Learning", "text": "Generally, the formation of the MTRNN follows the updating rule of the classical firing rate models, in which the activity of a neuron is determined by the average firing rate of all connected neurons. (In addition, the neuronal activity is determined by both the previous activation and the current synaptic inputs, as shown in Eq. (6) Thus, if the time step t > 0, the current firing rate of a neuron is determined by both the previous activation and the current synaptic inputs, as shown in Eq. (6) If the neuronal activity of the neuron i, t = \u2212 ui, t + jxj, t (6) where the neuronal activity of the neuron t-th time step, wi, j represents the synaptic weight of the neuron the i-th neuronal scale the parameters that determine the decay rate of that neuron."}, {"heading": "3 Experiments", "text": "To investigate the network performance, we recorded real-world training data from object manipulation experiments based on an iCub command based on an iCub system. It is a child-sized humanoid robot built as a test platform for theories and models of cognitive science and neuroscience. So it is a unique robotic platform with 53 degrees of freedom. As such, we established a learning scenario in which a human instructor teaches robotic learners a series of speech commands while offering a kinesthetic demonstration of the named actions. The aim of these experiments was to evaluate the error for generalization with a large dataset. We were also interested in the mechanisms, particularly the neural activities in the hierarchical architecture, that lead to such a generalization. 3.1 Experimental SetupFig shows the data used in our experiments."}, {"heading": "4 Generalisation Analyses", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "5 Discussion", "text": "This year, the time has come for a process of this kind, in which the question is to what extent it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a region and in which it is a country."}, {"heading": "6 Conclusion", "text": "This paper presents a neurorobotic study on the generation of nouns and verbs and the generalization based on the MTRNN networks, with a large dataset consisting of voice commands, visual objects and motor action data. Although previous research has reported on the generalization capabilities of hierarchical RNNN (RNPB, MTRNN), this is the first study to demonstrate its generalization capabilities using such a large dataset that allows the robot to deal with objects and actions of the real world. These experiments showed that the generalization capability of the network itself is possible with a large set of test sets (9 motor actions and 9 objects placed in 6 different places).This is particularly important because the recurring connections between verbs and nouns are linked to different modalities of training data that are strengthened during embodiment training by the mbodiment interaction."}], "references": [{"title": "Cognitive developmental robotics: a survey", "author": ["Minoru Asada", "Koh Hosoda", "Yasuo Kuniyoshi", "Hiroshi Ishiguro", "Toshio Inui", "Yuichiro Yoshikawa", "Masaki Ogino", "Chisato Yoshida"], "venue": "Autonomous Mental Development, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Use of a sparse structure to improve learning performance of recurrent neural networks", "author": ["Hiromitsu Awano", "Shun Nishide", "Hiroaki Arie", "Jun Tani", "Toru Takahashi", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "In Neural Information Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Language, gesture, and the developing brain", "author": ["Elizabeth Bates", "Frederic Dick"], "venue": "Developmental psychobiology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Designing sociable robots", "author": ["Cynthia L Breazeal"], "venue": "MIT press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Grounding language in action and perception: from cognitive agents to humanoid robots", "author": ["Angelo Cangelosi"], "venue": "Physics of life reviews,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Integration of action and language knowledge: A roadmap for developmental robotics", "author": ["Angelo Cangelosi", "Giorgio Metta", "Gerhard Sagerer", "Stefano Nolfi", "Chrystopher Nehaniv", "Kerstin Fischer", "Jun Tani", "Tony Belpaeme", "Giulio Sandini", "Francesco Nori"], "venue": "Autonomous Mental Development, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "How nouns and verbs differentially affect the behavior of artificial organisms", "author": ["Angelo Cangelosi", "Domenico Parisi"], "venue": "In Proceedings of the 23rd Annual Conference of the Cognitive Science", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "The processing of verbs and nouns in neural networks: Insights from synthetic brain imaging", "author": ["Angelo Cangelosi", "Domenico Parisi"], "venue": "Brain and Language,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "The processing of verbs and nouns in neural networks: Insights from synthetic brain imaging", "author": ["Angelo Cangelosi", "Domenico Parisi"], "venue": "Brain and Language,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Developmental robotics: From babies to robots", "author": ["Angelo Cangelosi", "Matthew Schlesinger"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Structured connectionist models of language, cognition and action", "author": ["Nancy Chang", "JEROME Feldman", "Srini Narayanan"], "venue": "Progress in Neural Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Socially intelligent robots: dimensions of human\u2013robot interaction", "author": ["Kerstin Dautenhahn"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Progress on evolution of communication and interaction studies", "author": ["Kerstin Dautenhahn", "Angelo Cangelosi"], "venue": "Interaction Studies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Recurrent temporal networks and language acquisition\u00e2\u0141\u201dfrom corticostriatal neurophysiology to reservoir computing", "author": ["Peter F Dominey"], "venue": "Frontiers in psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Realtime spoken-language programming for cooperative interaction with a humanoid apprentice", "author": ["Peter Ford Dominey", "Anthony Mallet", "Eiichi Yoshida"], "venue": "International Journal of Humanoid Robotics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Eye movements as a window into realtime spoken language comprehension in natural contexts", "author": ["Kathleen M Eberhard", "Michael J Spivey-Knowlton", "Julie C Sedivy", "Michael K Tanenhaus"], "venue": "Journal of psycholinguistic research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Robot programming by demonstration with situated spatial language understanding", "author": ["Maxwell Forbes", "Rajesh PN Rao", "Luke Zettlemoyer", "Maya Cakmak"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Why nouns are learned before verbs: Linguistic relativity versus natural partitioning", "author": ["Dedre Gentner"], "venue": "Center for the Study of Reading Technical Report; no", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1982}, {"title": "A cognitive neural architecture able to learn and communicate through natural language", "author": ["Bruno Golosio", "Angelo Cangelosi", "Olesya Gamotina", "Giovanni Luca Masala"], "venue": "PloS one,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Cortical interactions underlying the production of speech sounds", "author": ["Frank H Guenther"], "venue": "Journal of communication disorders,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Assumptions about word meaning: Individuation and basic-level kinds", "author": ["D Geoffrey Hall", "Sandra R Waxman"], "venue": "Child Development,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1993}, {"title": "Analysing the multiple timescale recurrent neural network for embodied language understanding", "author": ["Stefan Heinrich", "Sven Magg", "Stefan Wermter"], "venue": "In Artificial Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Adaptive learning of linguistic hierarchy in a multiple timescale recurrent neural network", "author": ["Stefan Heinrich", "Cornelius Weber", "Stefan Wermter"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Real-time parallel processing of grammatical structure in the fronto-striatal system: a recurrent network simulation study using reservoir computing", "author": ["Xavier Hinaut", "Peter Ford Dominey"], "venue": "PloS one,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Emergence of hierarchical structure mirroring linguistic composition in a recurrent neural network", "author": ["Wataru Hinoshita", "Hiroaki Arie", "Jun Tani", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Emergence of evolutionary interaction with voice and motion between two robots using rnn", "author": ["Wataru Hinoshita", "Tetsuya Ogata", "Hideki Kozima", "Hisashi Kanda", "Toru Takahashi", "Hiroshi G Okuno"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Modeling the development of pronunciation in infant speech acquisition", "author": ["Ian S Howard", "Piers Messum"], "venue": "Motor Control,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Generalization in learning multiple temporal patterns using rnnpb", "author": ["Masato Ito", "Jun Tani"], "venue": "In Neural Information Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Controlling recurrent neural networks by conceptors", "author": ["Herbert Jaeger"], "venue": "arXiv preprint arXiv:1403.3369,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Perception of motion affects language processing", "author": ["Michael P Kaschak", "Carol J Madden", "David J Therriault", "Richard H Yaxley", "Mark Aveyard", "Adrienne A Blanchard", "Rolf A Zwaan"], "venue": "Cognition, 94(3):B79\u2013B89,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "An examination of the distinction between nouns and verbs: Associations with two different kinds of motion", "author": ["Alan W Kersten"], "venue": "Memory & cognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Action-driven perception for a humanoid", "author": ["Jens Kleesiek", "Stephanie Badde", "Stefan Wermter", "Andreas K Engel"], "venue": "In Agents and Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "A perceptual account of symbolic reasoning", "author": ["David Landy", "Colin Allen", "Carlos Zednik"], "venue": "Frontiers in psychology,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Noun and verb production in maternal and child language: Continuity, stability, and prediction across the second year of life", "author": ["Emiddia Longobardi", "Pietro Spataro", "Diane L. Putnick", "Marc H Bornstein"], "venue": "Language Learning and Development,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "A unified theory ofword learning: Putting verb acquisition in context. Action meets word: How children", "author": ["MandyJ Maguire", "Kathy Hirsh-Pasek", "Roberta Michnick Golinkoff"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Learning to recognize parallel combinations of human motion primitives with linguistic descriptions using non-negative matrix factorization", "author": ["Olivier Mangin", "Pierre-Yves Oudeyer"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Cynthia Matuszek", "Evan Herbst", "Luke Zettlemoyer", "Dieter Fox"], "venue": "In Experimental Robotics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "The icub humanoid robot: an open platform for research in embodied cognition", "author": ["Giorgio Metta", "Giulio Sandini", "David Vernon", "Lorenzo Natale", "Francesco Nori"], "venue": "In Proceedings of the 8th workshop on performance metrics for intelligent systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Language as a cognitive tool", "author": ["Marco Mirolli", "Domenico Parisi"], "venue": "Minds and Machines,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Towards a vygotskyan cognitive robotics: the role of language as a cognitive tool", "author": ["Marco Mirolli", "Domenico Parisi"], "venue": "New Ideas in Psychology,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Tell me dave: Context-sensitive grounding of natural language to manipulation instructions", "author": ["Dipendra K Misra", "Jaeyong Sung", "Kevin Lee", "Ashutosh Saxena"], "venue": "Proceedings of Robotics: Science and Systems (RSS), Berkeley,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Experience and the active mind", "author": ["Alva No\u00eb"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}, {"title": "Flexible attention-based cognitive architecture for robots. 2014.  \u201cmtrnn-generalisation-2015-v12-pdf", "author": ["Rony Novianto"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Integration of behaviors and languages with a hierarchal structure self-organized in a neurodynamical model", "author": ["Takaaki Ogata", "Hiroshi G Okuno"], "venue": "In Robotic Intelligence In Informationally Structured Space (RiiSS),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A Olshausen", "David J Field"], "venue": "Vision research,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1997}, {"title": "Discovering communication", "author": ["Pierre-Yves Oudeyer", "Fr\u00e9d\u00e9ric Kaplan"], "venue": "Connection Science,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "A unified simulation scenario for language development, evolution and historical change", "author": ["Domenico Parisi", "Angelo Cangelosi"], "venue": "In Simulating the evolution of language,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2002}, {"title": "Verifying different-modality properties for concepts produces switching costs", "author": ["Diane Pecher", "Ren\u00e9 Zeelenberg", "Lawrence W Barsalou"], "venue": "Psychological Science,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2003}, {"title": "Aquila: An open-source gpu-accelerated toolkit for cognitive and neuro-robotics research", "author": ["Martin Peniak", "Anthony Morse", "Christopher Larcombe", "Salomon Ramirez-Contla", "Angelo Cangelosi"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["Fernando J Pineda"], "venue": "Physical review letters,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1987}, {"title": "The neuroscience of language: on brain circuits of words and serial order", "author": ["Friedemann Pulverm\u00fcller"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Tonotopic organization in auditory cortex of the cat", "author": ["Richard A Reale", "Thomas J Imig"], "venue": "Journal of Comparative Neurology,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1980}, {"title": "How can multimodal cues from child-directed interaction reduce learning complexity in robots", "author": ["Katharina J Rohlfing", "Jannik Fritsch", "Britta Wrede", "Tanja Jungmann"], "venue": "Advanced Robotics,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "Modulation of bold response in motion-sensitive lateral temporal cortex by real and fictive motion sentences", "author": ["Ayse Pinar Saygin", "Stephen McCullough", "Morana Alac", "Karen Emmorey"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2010}, {"title": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic", "author": ["Jeffrey Mark Siskind"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2001}, {"title": "Iterated learning: a framework for the emergence of language", "author": ["Kenny Smith", "Simon Kirby", "Henry Brighton"], "venue": "Artificial life,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2003}, {"title": "The emergence and evolution of linguistic structure: from lexical to grammatical communication systems", "author": ["Luc Steels"], "venue": "Connection science,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2005}, {"title": "Language grounding in robots", "author": ["Luc Steels", "Manfred Hild"], "venue": "Springer Science & Business Media,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "The grounding of higher order concepts in action and language: A cognitive robotics model", "author": ["Francesca Stramandinoli", "Davide Marocco", "Angelo Cangelosi"], "venue": "Neural Networks,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2012}, {"title": "Learning semantic combinatoriality from the interaction between linguistic and behavioral processes", "author": ["Yuuya Sugita", "Jun Tani"], "venue": "Adaptive Behavior,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2005}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}, {"title": "Self-organization and compositionality in cognitive brains: A neurorobotics study", "author": ["Jun Tani"], "venue": "Proceedings of the IEEE,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2014}, {"title": "Self-organization of distributedly represented multiple behavior schemata in a mirror system: reviews of robot experiments using RNNPB", "author": ["Jun Tani", "Masato Ito", "Yuuya Sugita"], "venue": "Neural Networks,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R Walter", "Ashis Gopal Banerjee", "Seth J Teller", "Nicholas Roy"], "venue": "In AAAI,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2011}, {"title": "Object permanence and relational words: A lexical training study", "author": ["Michael Tomasello", "Michael Jeffrey Farrar"], "venue": "Journal of Child Language,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1986}, {"title": "Functional organization of primate visual cortex", "author": ["David C Van Essen"], "venue": "Cerebral cortex,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1985}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Prespeech motor learning in a neural network using reinforcement", "author": ["Anne S Warlaumont", "Gert Westermann", "Eugene H Buder", "D Kimbrough Oller"], "venue": "Neural Networks,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Autonomous mental development by robots and animals", "author": ["Juyang Weng", "James McClelland", "Alex Pentland", "Olaf Sporns", "Ida Stockman", "Mriganka Sur", "Esther Thelen"], "venue": null, "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2001}, {"title": "An internal model for sensorimotor integration", "author": ["Daniel M Wolpert", "Zoubin Ghahramani", "Michael I Jordan"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1995}, {"title": "Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment", "author": ["Yuichi Yamashita", "Jun Tani"], "venue": "PLoS computational biology,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2008}, {"title": "The learning of adjectives and nouns from affordance and appearance features", "author": ["Onur Y\u00fcr\u00fcten", "Erol \u015eahin", "Sinan Kalkan"], "venue": "Adaptive Behavior,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2013}, {"title": "Artificial neural models for feedback pathways for sensorimotor integration", "author": ["Junpei Zhong"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2015}, {"title": "From continuous affective space to continuous expression space: Non-verbal behaviour recognition and generation", "author": ["Junpei Zhong", "Lola Canamero"], "venue": "In Development and Learning and Epigenetic Robotics (ICDL-Epirob),", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2014}, {"title": "Toward a self-organizing pre-symbolic neural model representing sensorimotor primitives", "author": ["Junpei Zhong", "Angelo Cangelosi", "Stefan Wermter"], "venue": "Frontiers in behavioral neuroscience,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2014}, {"title": "Robot trajectory prediction and recognition based on a computational mirror neurons model", "author": ["Junpei Zhong", "Cornelius Weber", "Stefan Wermter"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2011}, {"title": "Learning features and predictive transformation encoding based on a horizontal product model", "author": ["Junpei Zhong", "Cornelius Weber", "Stefan Wermter"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2012}, {"title": "A predictive network architecture for a robust and smooth robot docking behavior. Paladyn", "author": ["Junpei Zhong", "Cornelius Weber", "Stefan Wermter"], "venue": "Journal of Behavioral Robotics,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "For the design of social robots [4,13], besides of building robots with human-like external morphology, the ability to process, to understand and generate language is one of the key factors to support human-robot interaction.", "startOffset": 32, "endOffset": 38}, {"referenceID": 12, "context": "For the design of social robots [4,13], besides of building robots with human-like external morphology, the ability to process, to understand and generate language is one of the key factors to support human-robot interaction.", "startOffset": 32, "endOffset": 38}, {"referenceID": 74, "context": "Important recent developments in social robotics, such as robots performing human-like emotion expression [76] and social attention for autonomous movement [45], have been accompanied by language understanding approaches focusing on the grounding of natural language into the agent\u2019s sensorimotor experience and its situated interaction [5,60].", "startOffset": 106, "endOffset": 110}, {"referenceID": 43, "context": "Important recent developments in social robotics, such as robots performing human-like emotion expression [76] and social attention for autonomous movement [45], have been accompanied by language understanding approaches focusing on the grounding of natural language into the agent\u2019s sensorimotor experience and its situated interaction [5,60].", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "Important recent developments in social robotics, such as robots performing human-like emotion expression [76] and social attention for autonomous movement [45], have been accompanied by language understanding approaches focusing on the grounding of natural language into the agent\u2019s sensorimotor experience and its situated interaction [5,60].", "startOffset": 337, "endOffset": 343}, {"referenceID": 58, "context": "Important recent developments in social robotics, such as robots performing human-like emotion expression [76] and social attention for autonomous movement [45], have been accompanied by language understanding approaches focusing on the grounding of natural language into the agent\u2019s sensorimotor experience and its situated interaction [5,60].", "startOffset": 337, "endOffset": 343}, {"referenceID": 64, "context": "For instance, in [66,39], syntactic parsing techniques are used to ground the language into primitive motor actions (e.", "startOffset": 17, "endOffset": 24}, {"referenceID": 37, "context": "For instance, in [66,39], syntactic parsing techniques are used to ground the language into primitive motor actions (e.", "startOffset": 17, "endOffset": 24}, {"referenceID": 41, "context": "[43] developed a system ar X iv :1 60 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[74] proposed that in order to understand the object affordance which can be described by adjectives, the most crucial property is the shape-related one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "Besides the direct modelling methods for robot language learning, an alternative approach to build a learning model for language is based on developmental robotics [71,1,10].", "startOffset": 164, "endOffset": 173}, {"referenceID": 0, "context": "Besides the direct modelling methods for robot language learning, an alternative approach to build a learning model for language is based on developmental robotics [71,1,10].", "startOffset": 164, "endOffset": 173}, {"referenceID": 9, "context": "Besides the direct modelling methods for robot language learning, an alternative approach to build a learning model for language is based on developmental robotics [71,1,10].", "startOffset": 164, "endOffset": 173}, {"referenceID": 5, "context": "In the context of language understanding, the core of developmental robotics approaches to language learning is following a similar developmental pathway of infants acquiring grounded representations of natural language and forming a symbol system through embodied interaction with the physical environment [6].", "startOffset": 307, "endOffset": 310}, {"referenceID": 20, "context": "For example, the cognitive model presented in [21] outlines the cortical interactions in the syllable generation process which result in different developmental phenomena.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "The Elija model [28] is a vocal apparatus which strictly follows detailed developmental stages.", "startOffset": 16, "endOffset": 20}, {"referenceID": 68, "context": "Likewise, a self-organizing map together with reinforcement learning was proposed in [70], which demonstrated that the reinforcement learning based on the similarity of vocalization can improve the post-learning production of the sound of one\u2019s language.", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "Therefore, except studies focusing on the mental imagination of actions as in [20], the mechanical morphology of a robot is particularly important when modelling the acquisition of words, especially those used to name the motor actions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "For instance, the model from [38] gets as input dance-like combinations of human movement primitives plus ambiguous labels associated with these movements.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Concentrating on the second and third stages of associating lexicon, words and motor actions, the robot in [16] is able to acquire new motor behaviours in an on-line fashion by grounding the vocal commands on the predefined control motor primitives.", "startOffset": 107, "endOffset": 111}, {"referenceID": 55, "context": "Similarly, Siskind [57] proposed a model which uses visual primitives to encode notions of different actions to ground the semantics of events for verb learning.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "Using structured connectionist models (SCMs), [11] built a layered connectionist model to connect embodied representations and simulative inference for verbs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "In [8], the emergence of verb-noun separation is learned while the agents are interacting and manipulating the objects.", "startOffset": 3, "endOffset": 6}, {"referenceID": 59, "context": "[61] further developed the grounding the verbs with more complex meanings (such as \u201ckeep\u201d, \u201creject\u201d, \u201caccept\u201d and \u201cgive\u201d) which related to the internal states of the caregivers and which were used to build a robotic model for the grounding of increasingly abstract motor concepts and words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "As follow-up studies of [16], [15,25] focused on the understanding of grammatical complexity.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "As follow-up studies of [16], [15,25] focused on the understanding of grammatical complexity.", "startOffset": 30, "endOffset": 37}, {"referenceID": 24, "context": "As follow-up studies of [16], [15,25] focused on the understanding of grammatical complexity.", "startOffset": 30, "endOffset": 37}, {"referenceID": 60, "context": "Also using RNN, [62] reported experiments with a mobile robot implementing a two-level RNN architecture called Recurrent Neural Network with Parametric Bias Units (RNNPB).", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": "Comparing to RNNPB, another kind of RNN architecture called Multiple Timescale Neural Network (MTRNN) is able to ground different scales of sensorimotor information into the hierarchical structure of sentences, such as the spelling of words [46] and words and sentences [26].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "Comparing to RNNPB, another kind of RNN architecture called Multiple Timescale Neural Network (MTRNN) is able to ground different scales of sensorimotor information into the hierarchical structure of sentences, such as the spelling of words [46] and words and sentences [26].", "startOffset": 270, "endOffset": 274}, {"referenceID": 53, "context": "For instance, language commands can be acquired from learning from demonstration (LfD) [55], intrinsic plasticity [48] and evolution [59,14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 46, "context": "For instance, language commands can be acquired from learning from demonstration (LfD) [55], intrinsic plasticity [48] and evolution [59,14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 57, "context": "For instance, language commands can be acquired from learning from demonstration (LfD) [55], intrinsic plasticity [48] and evolution [59,14].", "startOffset": 133, "endOffset": 140}, {"referenceID": 13, "context": "For instance, language commands can be acquired from learning from demonstration (LfD) [55], intrinsic plasticity [48] and evolution [59,14].", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "tion [18]; a comparative study for evolving robot language with situated information can be found in [49].", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "tion [18]; a comparative study for evolving robot language with situated information can be found in [49].", "startOffset": 101, "endOffset": 105}, {"referenceID": 56, "context": "From a mathematical prospective, those learning methods overcome the natural language learning bottlenecks of building compositionality of lexical structures and maximising the observed content [58] by means of statistical methods (LfD), optimal control (Reinforcement Learning) or other methodologies.", "startOffset": 194, "endOffset": 198}, {"referenceID": 18, "context": "For the early stages of verb and noun learning, it is widely accepted that most of the common nouns are generally learned before verbs [19], by first connecting speech sounds (labels, nouns) to physical objects in view.", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "However, some nouns which relate to context, such as \u201cpassenger\u201d, are learnt relatively late, only after \u201can extensive range of situations\u201d (contexts or life phases) have been encountered [22], during which verbs may play a crucial role.", "startOffset": 188, "endOffset": 192}, {"referenceID": 31, "context": "The embodied learning of verbs and nouns is not correlated to one single modality in sensory percept\u2019s: experiments done in [32] suggest that the nouns are grounded from the intrinsic properties of an object, even at different movements and orientations, while verbs are accounted for the movement path of an object.", "startOffset": 124, "endOffset": 128}, {"referenceID": 35, "context": "As [37] suggested, some nouns and verbs can be learnt more straightforward to learn because they can be accessed perceptually.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "For instance, while infants learn the word-gesture combination at the age of two, they associate the meaning of verbs with the meanings of the higher-order nouns [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 34, "context": "Such verbs with complex meaning are obtained from both motor action and visual percept [36].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "As summarised in [7,9], comparing to the static object perception that associates to simple nouns, the early verb learning involves a temporal dynamic from motion perception.", "startOffset": 17, "endOffset": 22}, {"referenceID": 8, "context": "As summarised in [7,9], comparing to the static object perception that associates to simple nouns, the early verb learning involves a temporal dynamic from motion perception.", "startOffset": 17, "endOffset": 22}, {"referenceID": 51, "context": "[53,31,50, 56]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 30, "context": "[53,31,50, 56]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 48, "context": "[53,31,50, 56]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 54, "context": "[53,31,50, 56]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 33, "context": "[35, 41,42].", "startOffset": 0, "endOffset": 11}, {"referenceID": 39, "context": "[35, 41,42].", "startOffset": 0, "endOffset": 11}, {"referenceID": 40, "context": "[35, 41,42].", "startOffset": 0, "endOffset": 11}, {"referenceID": 65, "context": ") For the predictive effect from language to sensorimotor behaviours, vocal communication can be one of the sources that drives the visual attention to become predictive, by making inferences as to the source-inferences [67].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "In this process, language can trigger a predictive inference about the appearance of a visual percept, driving a predictive saccade [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 73, "context": "Following the hierarchical cognitive architecture proposed in [75], the language understanding can be represented hierarchically from the neural processes on the (lower) receptor level to the higher level understanding which happens in the (higher) prefrontal cortex.", "startOffset": 62, "endOffset": 66}, {"referenceID": 76, "context": "Moreover, we will use a hierarchical recurrent neural architecture, as in [78,79,80], due to the fact that the learning modalities of visual perception and motor actions can be represented as both spatial and temporal sequences, so that the recurrent connections provide possibilities to intertwine these two modalities.", "startOffset": 74, "endOffset": 84}, {"referenceID": 77, "context": "Moreover, we will use a hierarchical recurrent neural architecture, as in [78,79,80], due to the fact that the learning modalities of visual perception and motor actions can be represented as both spatial and temporal sequences, so that the recurrent connections provide possibilities to intertwine these two modalities.", "startOffset": 74, "endOffset": 84}, {"referenceID": 78, "context": "Moreover, we will use a hierarchical recurrent neural architecture, as in [78,79,80], due to the fact that the learning modalities of visual perception and motor actions can be represented as both spatial and temporal sequences, so that the recurrent connections provide possibilities to intertwine these two modalities.", "startOffset": 74, "endOffset": 84}, {"referenceID": 60, "context": "Similar RNNPB [62] or MTRNN [23] networks have been used to learn verbs and nouns features with motor actions and visual features.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "Similar RNNPB [62] or MTRNN [23] networks have been used to learn verbs and nouns features with motor actions and visual features.", "startOffset": 28, "endOffset": 32}, {"referenceID": 70, "context": "[72, 44]) and should be encoded solely as similar data structures.", "startOffset": 0, "endOffset": 8}, {"referenceID": 42, "context": "[72, 44]) and should be encoded solely as similar data structures.", "startOffset": 0, "endOffset": 8}, {"referenceID": 71, "context": "incorporating a Multiple Timescale Recurrent Neural Network (MTRNN) [73] and the self-organizing maps.", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "The sparseness in weighting matrices has a similar concept of sparse coding in computational neuroscience [47].", "startOffset": 106, "endOffset": 110}, {"referenceID": 66, "context": "The weighting matrices are sparsely distributed, which is an analogous form of the sparse distributed representations that are used in our brain, such as in visual [68] and auditory cortex [54].", "startOffset": 164, "endOffset": 168}, {"referenceID": 52, "context": "The weighting matrices are sparsely distributed, which is an analogous form of the sparse distributed representations that are used in our brain, such as in visual [68] and auditory cortex [54].", "startOffset": 189, "endOffset": 193}, {"referenceID": 1, "context": "Previous research on language learning in RNN [2] also showed that a sparse encoding results in robustness in training and a better generalisation results and improved robustness with noisy inputs.", "startOffset": 46, "endOffset": 49}, {"referenceID": 38, "context": "To examine the network performance, we recorded the real world training data from object manipulation experiments based on an iCub robot [40].", "startOffset": 137, "endOffset": 141}, {"referenceID": 63, "context": "[65,73]) We used such a large number of data to test the combinatorial complexity and mechanical feasibility of this model, as well as to evaluate the generalisation ability and its internal non-linear dynamics when using such a large data-set.", "startOffset": 0, "endOffset": 7}, {"referenceID": 71, "context": "[65,73]) We used such a large number of data to test the combinatorial complexity and mechanical feasibility of this model, as well as to evaluate the generalisation ability and its internal non-linear dynamics when using such a large data-set.", "startOffset": 0, "endOffset": 7}, {"referenceID": 49, "context": "All these experiments were run using a modified version of the Aquila software [51] in a GPU computer with one Tesla C2050 and two GeForce GTX 580 graphic cards.", "startOffset": 79, "endOffset": 83}, {"referenceID": 71, "context": "parameters settings ([73], [24] and [27]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "parameters settings ([73], [24] and [27]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "parameters settings ([73], [24] and [27]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 71, "context": "To start, we firstly set the parameters according to the minimum values (70, 5, 20, 60) from previous research [73].", "startOffset": 111, "endOffset": 115}, {"referenceID": 60, "context": "For an experiment with a similar aim, [62] reported combining two hierarchical recurrent neural networks which can also accomplish verbnoun generalisation for understanding combinatorial semantics in a situated environment.", "startOffset": 38, "endOffset": 42}, {"referenceID": 60, "context": "be/FOgKbJ-iEhM Particularly, in our case, the learning sequences contain a much larger dimension (35) of the motor joint angles for the iCub movements, compared with motor sequences that trained in [62].", "startOffset": 198, "endOffset": 202}, {"referenceID": 29, "context": "It has been reported that quite a few RNN models based on functional hierarchy, such as RNNPB, MTRNN and conceptors [30], allow the bifurcation to occur in the RNN dynamics.", "startOffset": 116, "endOffset": 120}, {"referenceID": 62, "context": "From the bifurcation explanation of the simplified RNNPB model, at the next step we can also extend this to other hierarchical RNNs such as MTRNN, as they are holding a fundamentally similar theoretical foundation [64].", "startOffset": 214, "endOffset": 218}, {"referenceID": 26, "context": "The neural dynamics in our MTRNN exhibited a dynamics which are different from those reported in [27] and [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "The neural dynamics in our MTRNN exhibited a dynamics which are different from those reported in [27] and [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 28, "context": "[29,52,77]), which expect the network to do interpolation or extrapolation with a novel input value in either temporal or spatial space.", "startOffset": 0, "endOffset": 10}, {"referenceID": 50, "context": "[29,52,77]), which expect the network to do interpolation or extrapolation with a novel input value in either temporal or spatial space.", "startOffset": 0, "endOffset": 10}, {"referenceID": 75, "context": "[29,52,77]), which expect the network to do interpolation or extrapolation with a novel input value in either temporal or spatial space.", "startOffset": 0, "endOffset": 10}, {"referenceID": 32, "context": "Considering the different generalisation abilities of generic RNN, RNNPB [33,77] and MTRNN [23], the hierarchical RNNs appear particularly suitable for the production of flexible motor behaviour and language expression simultaneously in the real-world social robot experiments.", "startOffset": 73, "endOffset": 80}, {"referenceID": 75, "context": "Considering the different generalisation abilities of generic RNN, RNNPB [33,77] and MTRNN [23], the hierarchical RNNs appear particularly suitable for the production of flexible motor behaviour and language expression simultaneously in the real-world social robot experiments.", "startOffset": 73, "endOffset": 80}, {"referenceID": 22, "context": "Considering the different generalisation abilities of generic RNN, RNNPB [33,77] and MTRNN [23], the hierarchical RNNs appear particularly suitable for the production of flexible motor behaviour and language expression simultaneously in the real-world social robot experiments.", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "A few machine learning methods have recently been proposed based on the encoder-decoder (ED) architecture [12], which achieved great performance in machine translation [63], image captioning [69], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 61, "context": "A few machine learning methods have recently been proposed based on the encoder-decoder (ED) architecture [12], which achieved great performance in machine translation [63], image captioning [69], etc.", "startOffset": 168, "endOffset": 172}, {"referenceID": 67, "context": "A few machine learning methods have recently been proposed based on the encoder-decoder (ED) architecture [12], which achieved great performance in machine translation [63], image captioning [69], etc.", "startOffset": 191, "endOffset": 195}, {"referenceID": 22, "context": "Despite their similarities, compared with LSTM, the MTRNN have other distinct features: First, from the above experiments and from other MTRNN experiments [23,27], it has been shown that the fast context layers and slow context layers exhibit various dynamics to explicitly represent the relationship between the verbs and nouns.", "startOffset": 155, "endOffset": 162}, {"referenceID": 26, "context": "Despite their similarities, compared with LSTM, the MTRNN have other distinct features: First, from the above experiments and from other MTRNN experiments [23,27], it has been shown that the fast context layers and slow context layers exhibit various dynamics to explicitly represent the relationship between the verbs and nouns.", "startOffset": 155, "endOffset": 162}], "year": 2016, "abstractText": "The paper presents a neurorobotics cognitive model to explain the understanding and generalisation of nouns and verbs combinations when a vocal command consisting of a verb-noun sentence is provided to a humanoid robot. This generalisation process is done via the grounding process: different objects are being interacted, and associated, with different motor behaviours, following a learning approach inspired by developmental language acquisition in infants. This cognitive model is based on Multiple Time-scale Recurrent Neural Networks (MTRNN). With the data obtained from object manipulation tasks with a humanoid robot platform, the robotic agent implemented with this model can ground the primitive embodied structure of verbs through training with verb-noun combination samples. Moreover, we show that a functional hierarchical architecture, based on MTRNN, is able to generalise and produce novel combinations of noun-verb sentences. Further analyses of the learned network dynamics and representations also demonstrate how the generalisation is possible via the exploitation of this functional hierarchical recurrent network. J. Zhong Department of Intermedia Art and Science, Waseda University, 3-4-1 Ohkubo, Shinjuku, Tokyo, Japan, 169-8555 Centre for Robotics and Neural Systems, University of Plymouth, PL4 8AA, United Kingdom Tel.: +44 (0)175284908, E-mail: zhong@junpei.eu M. Peniak Cortexica Vision Systems, London, United Kingdom J. Tani Korea Advanced Institute of Science and Technology, Daejeon, South Korea T. Ogata Department of Intermedia Art and Science, Waseda University, Tokyo, Japan A. Cangelosi Centre for Robotics and Neural Systems, University of Plymouth, United Kingdom", "creator": "LaTeX with hyperref package"}}}