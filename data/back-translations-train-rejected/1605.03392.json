{"id": "1605.03392", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables", "abstract": "We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables.", "histories": [["v1", "Wed, 11 May 2016 11:54:26 GMT  (110kb,D)", "http://arxiv.org/abs/1605.03392v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mauro scanagatta", "giorgio corani", "cassio p de campos", "marco zaffalon"], "accepted": false, "id": "1605.03392"}, "pdf": {"name": "1605.03392.pdf", "metadata": {"source": "CRF", "title": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables", "authors": ["Mauro Scanagatta", "Giorgio Corani", "Marco Zaffalon"], "emails": ["mauro@idsia.ch", "giorgio@idsia.ch", "c.decampos@qub.ac.uk", "zaffalon@idsia.ch"], "sections": [{"heading": null, "text": "We present a method for learning tree-width Bayesian networks from data sets containing thousands of variables. Limiting the tree width of a Bayesian network greatly reduces the complexity of conclusions. However, since it is a global property of the graph, it greatly increases the difficulty of the learning process. We propose a novel algorithm for this task that is capable of scaling to large areas and large tree widths. Our novel approach consistently exceeds the state of the art for data sets with up to ten thousand variables."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Structural learning", "text": "Consider the problem of learning the structure of a Bayesian network from a complete dataset of N instances D = (V, E), where V is the collection of nodes and E is the collection of arcs. E can be divided by the set of parent partial parts. Different values can be used to judge the fit of a DAG. We use the Bayesian information criterion (or simply BIC), which asymptotically approximates the posterior probability of the DAG under common assumptions. The BIC value is decomposable because it is the sum of the values of each variable: BIC (G) = = n value of variables indicating variability."}, {"heading": "2.1 Parent sets identification", "text": "In this section, we present the first contribution of this thesis: a bound contribution for BIC values that can be used to evaluate their scores while processing all parent sets, we cannot calculate all parent sets because they already have time complexity (N \u00b7 nk + 1). In this section, we present the first contribution of this thesis: a bound contribution for BIC values that can be used to evaluate their scores while processing all parent sets."}, {"heading": "3 Incremental treewidth-bounded structure learning", "text": "We now turn our attention to the structural optimization task. Our approach is by repeatedly scanning a sequence \u2022 through the variables and then identifying the highest rated DAG with bounded-treewidth in accordance with the sequence. The size of the search space of the possible jobs is n!, i.e. smaller than the search space of the possible ktrees. Once the sequence is scanned, we learn incrementally the DAG; it is guaranteed that at each step the moralization of the DAG is a subgraph of a k tree. Finally, the tree width of the DAG is limited by k. The algorithm proceeds as a result. Initialization The initial k tree Kk + 1 is formed by the complete clique of the first k + 1 variable in sequence. The initial DAG Gk + 1 is learned via the same k + 1 variable. Since (k + 1) is a small number of variables we can learn exactly Gk + 1."}, {"heading": "3.1 k-A*", "text": "We formulate the problem as a shortest path-finding problem. We define each state as a step toward completing the structure by adding a new variable to the DAG G. Given the variable X \u2022 i assigned in state S. We define a succession state of S for each k clique we can choose to add the variable X \u00b2 i + 1. the approach to solving the problem is based on a path-finding A * search, defining the cost function for state S as f (S) = g (S) + h (S). The goal is to minimize the state where all variables were assigned. g (S) is the cost from starting state to S, and we define it as the sum of the results of already assigned parents: g (S) = i \u00b2 j = 0 score (X \u00b2 j)."}, {"heading": "3.2 k-G", "text": "In some cases, a high number of variables or a high tree width prevents the use of k-A *. We therefore propose a greedy alternative approach, K-G. Following the previously defined pathfinding problem, he chooses a greedy approach: at each step, he selects the parent set with the highest score for the Xi variable, which is part of an existing clique in K."}, {"heading": "3.3 Space of learnable DAGs", "text": "It is as if it were a real, in which it was a reactionary, in which it was a reactionary, in which it was a reactionary, in which it was a reactionary, in which it was a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, in a reactionary, a reactionary, in a reactionary, in a"}, {"heading": "3.4 Our implementation of S2 and S2+", "text": "Here we provide the details of our implementation of S2 and S2 +. Both use the term \"Informative Score\" (Nie et al., 2015), an approximate measure of the fitness of a k-tree. Therefore, the I-score of a k-tree Tk is asIS (Tk) = Smi (Tk) | Sl (Tk) |, where Smi (Tk) measures the expected loss of representation of data with the k-tree. Let Iij denote the mutual information of the node i and j: Smi (Tk) = \"i, j Iij \u2212\" i, j \"Tk. Sl (Tk) is instead defined as the score of the best pseudo-subgraph of the k-tree, by dropping the acyclic-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-"}, {"heading": "3.4.1 Discussion", "text": "The problem with the K-tree sample is that each k-tree forces a random limitation of the arcs occurring in the final structure; the probability of randomly selecting a k-tree sample that allows good scoring arcs decreases significantly as the number of variables increases, and the space of possible k-trees increases; the probable acceptance criterion presented in the previous section was proposed to solve this problem, but it does not completely resolve the situation. Instead, our approach immediately focuses on selecting the best arcs, in a way that guarantees tree width. Experimentally, we observed that the K-tree sample is faster and produces a higher number of candidate DAGs whose values are unfortunately low. Instead, our approach generates fewer but better-rated DAGs. (Nie et al., 2016) improves the notion of k-tree by looking for the optimal one in relation to the informative score (IS), which can only divide the mutual information between the parent and the child."}, {"heading": "4 Experiments", "text": "We compare k-A *, k-G, S2 and S2 + in different experiments. We compare them with an indicator we call the W-Score: the percentage of deterioration of the BIC score of the selected treewidth-bounded method compared to the score of the Gobnilp solver (Cussens, 2011). Gobnilp achieves a higher score than the Treewidth-bounded method because there are no limits on the treewidth width width. By G, we refer to the BIC score achieved by Gobnilp, and by T to the BIC score achieved by the given treewidth-bounded method. Note that the BothG and T are negative. W-Score isW = G \u2212 TG. W stands for deterioration and thus lower values of W are better. The lowest value of W is zero, while there is no upper limit for the value of W."}, {"heading": "4.1 Learning inverted trees", "text": "In this section we will examine their performance in this worst-case scenario. We will start with the tree width k = 2. We will consider the number of variables n * {21, 41, 61, 81, 101}. All variables are binary and we will sample their conditional probability tables from a beta (1,1). We will sample a root variable X from the existing graph and add new variables until the graph contains n variables. All variables are binary and we will sample their conditional probability tables from a beta (1,1). We will then perform structural learning with k-A *, k-G, S2 and S2 + by setting k = 2 as the limit for the tree widths. We will allow any method to run for ten minutes. Both S2 and S2 + could in principle restore the true structure that is prevented."}, {"heading": "4.2 Small data sets", "text": "We now present experiments with the data sets already taken into account by (Nie et al., 2016), which include up to 100 variables. We set the limited tree width to k = 4. We provide the same pre-calculated values of parent sets to each structural learning method. We run each method for ten minutes. We run 10 experiments with each data set and report the median values in Table 1. Our results are not comparable to those reported by (Nie et al., 2016), as we use the BIC while they are in German. Remarkably, both k-A * and k-G achieve higher values than both S2 and S2 + in almost all data sets. Only in the smallest data sets do all methods achieve the same value. Between our two novel algorithms, k-A * has a slight advantage over k-G. We provide statistics on the candidate solutions generated by each method in Table 3. The results of the table refer in particular to the common data set (100 million data sets), but the conclusions of the previous sets are almost identical."}, {"heading": "4.3 Large data sets", "text": "We now look at 10 large data sets (100 \u2264 n \u2264 400), which are shown in Table 4.We look at the following tree widths: k2, 5, 8}. We randomly divide each data set into three subsets. Thus, for each tree width, we perform 10 \u00b7 3 = 30 structural learning experiments. We provide all structural learning methods with the same pre-calculated values of parent sets and run each method for an hour. For S2 +, we choose a more favorable approach that allows one hour of running. If after one hour the first k tree has not yet been solved, we run it until it has solved the first k tree. In Table 5, we report how often each method wins against another for each tree width. Entries are courageous when the number of victories of one algorithm over another is statistically significant, according to the character test (p value < 0.05)."}, {"heading": "4.4 Very large data sets", "text": "As a final experiment, we consider 14 very large datasets with more than 400 variables. We include in these experiments three randomly generated synthetic datasets with 2000, 4000 and 10000 variables. These networks were created using the software BNGenerator 1. Each variable has a number of states that are randomly drawn from 2 to 4 and a number of parents that are randomly drawn from 0 to 6. In this case, we perform 14 \u00b7 3 = 42 structural learning experiments with each algorithm. The only two algorithms that can handle these datasets are k-G and S2. Among them, k-G wins 42 times out of 42; this dominance is clearly significant. This result is consistently found in every choice of tree width (k = 2, 5, 8). On average, the improvement of k-G over S2 fills about 60% of the gap that separates S2 from the unlimited solution."}, {"heading": "5 Conclusion", "text": "Our novel approaches to tree-width structural learning in Bayesian networks are much better than modern methods, and the greedy approach scales up to thousands of nodes, suggesting that it is more important to find good k-trees than to solve the task of internal structural optimization for each one of them. Methods consistently outperform competitors in a variety of experiments, and all of these methods and others for unlimited learning Bayean networks can leverage our new boundaries on BIC values to reduce the number of parent-set evaluations during the prediction of values. Further analysis of boundaries is reserved for future work."}], "references": [{"title": "Bayesian network learning with cutting planes", "author": ["J. Cussens"], "venue": "Proceedings of the 27th Conference Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cussens,? \\Q2011\\E", "shortCiteRegEx": "Cussens", "year": 2011}, {"title": "Efficient structure learning of Bayesian networks using constraints", "author": ["P. de Campos C", "Q. Ji"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "C. and Ji,? \\Q2011\\E", "shortCiteRegEx": "C. and Ji", "year": 2011}, {"title": "Learning bounded treewidth Bayesian networks", "author": ["G. Elidan", "S. Gould"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Elidan and Gould,? \\Q2009\\E", "shortCiteRegEx": "Elidan and Gould", "year": 2009}, {"title": "Exact learning of bounded tree-width Bayesian networks", "author": ["H. Korhonen J", "P. Parviainen"], "venue": "In Proc. 16th Int. Conf. on AI and Stat.,", "citeRegEx": "J. and Parviainen,? \\Q2013\\E", "shortCiteRegEx": "J. and Parviainen", "year": 2013}, {"title": "Advances in learning Bayesian networks of bounded treewidth", "author": ["S. Nie", "D. Mau\u00e1 D", "P. de Campos C", "Q. Ji"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2014}, {"title": "Learning Bounded TreeWidth Bayesian Networks via Sampling", "author": ["S. Nie", "P. de Campos C", "Q. Ji"], "venue": "Proceedings of the 13th European Conference on Symbol and Quantitative Approaches to Reasoning with Uncertainty,", "citeRegEx": "Nie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2015}, {"title": "Learning Bayesian networks with bounded treewidth via guided search", "author": ["S. Nie", "P. de Campos C", "Q. Ji"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2016}, {"title": "Learning bounded tree-width Bayesian networks using integer linear programming", "author": ["P. Parviainen", "S. Farahani H", "J. Lagergren"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Parviainen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parviainen et al\\.", "year": 2014}, {"title": "On the structure of k-trees", "author": ["P. Patil H"], "venue": "Journal of Combinatorics, Information and System Sciences,", "citeRegEx": "H.,? \\Q1986\\E", "shortCiteRegEx": "H.", "year": 1986}, {"title": "Learning Bayesian Networks with Thousands of Variables", "author": ["M. Scanagatta", "P. de Campos C", "G. Corani", "M. Zaffalon"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Scanagatta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Scanagatta et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "A pioneering approach, polynomial in both the number of variables and the treewidth bound, has been proposed in (Elidan and Gould, 2009).", "startOffset": 112, "endOffset": 136}, {"referenceID": 4, "context": "Nie et al. (2015) proposed the approximated method S2.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Nie et al. (2015) proposed the approximated method S2. It exploits the notion of k-tree, which is an undirected maximal graph with treewidth k. A Bayesian network whose moral graph is a subgraph of a k-tree has thus treewidth bounded by k. S2 is an iterative algorithm. Each iteration consists of two steps: a) sampling uniformly a k-tree from the space of k-trees and b) recovering via sampling a high-scoring DAG whose moral graph is a subgraph of the sampled k-tree. The goodness of the k-tree is approximated by using a heuristic evaluation, called Informative Score. Nie et al. (2016) further refines this idea, proposing an exploration guided via A* for finding the optimal k-tree with respect to the Informative Score.", "startOffset": 0, "endOffset": 590}, {"referenceID": 9, "context": "Recent structural learning algorithms with unbounded treewidth (Scanagatta et al., 2015) can cope with thousands of variables.", "startOffset": 63, "endOffset": 88}, {"referenceID": 9, "context": "It follows trivially from Theorem 1 in (Scanagatta et al., 2015).", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "These inequalities provide bounds for the log-likelihood in line with the result presented in Corollary 1 of (Scanagatta et al., 2015).", "startOffset": 109, "endOffset": 134}, {"referenceID": 2, "context": "Treewidth We illustrate the concept of treewidth following the notation of (Elidan and Gould, 2009).", "startOffset": 75, "endOffset": 99}, {"referenceID": 0, "context": "In particular we adopt the method of Cussens (2011). The moral graph of Gk+1 is a subgraph of Kk+1 and thus Gk+1 has bounded treewidth.", "startOffset": 37, "endOffset": 52}, {"referenceID": 5, "context": "They both use the notion of Informative Score (Nie et al., 2015), an approximate measure of the fitness of a k-tree.", "startOffset": 46, "endOffset": 64}, {"referenceID": 4, "context": "In particular, S2 obtains k-trees by using the Dandelion sampling discussed in (Nie et al., 2014).", "startOffset": 79, "endOffset": 97}, {"referenceID": 5, "context": "where T \u2217 k is the current k-tree with the largest I-score (Nie et al., 2015).", "startOffset": 59, "endOffset": 77}, {"referenceID": 6, "context": "Instead S2+ selects the k + 1 variables with the largest Iscore and finds the k-tree maximizing the I-score from this clique, as discussed in (Nie et al., 2016).", "startOffset": 142, "endOffset": 160}, {"referenceID": 4, "context": "For this task, the authors proposed an approximate approach based on partial order sampling (Algorithm 2 of (Nie et al., 2014)).", "startOffset": 108, "endOffset": 126}, {"referenceID": 6, "context": "(Nie et al., 2016) improves on the notion of k-tree, searching for the optimal one with respect to the Informative Score (IS).", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "We compare them through an indicator which we call W-score: the percentage of worsening of the BIC score of the selected treewidth-bounded method compared to the score of the Gobnilp solver (Cussens, 2011).", "startOffset": 190, "endOffset": 205}, {"referenceID": 6, "context": "Table 1: Comparison between bounded-treewidth structural learning algorithms on the data sets already analyzed by (Nie et al., 2016).", "startOffset": 114, "endOffset": 132}, {"referenceID": 6, "context": "We now present experiments on the data sets already considered by (Nie et al., 2016).", "startOffset": 66, "endOffset": 84}, {"referenceID": 6, "context": "Our results are not comparable with those reported by (Nie et al., 2016) since we use the BIC while they use BDeu.", "startOffset": 54, "endOffset": 72}], "year": 2016, "abstractText": "We present a method for learning treewidthbounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables.", "creator": "LaTeX with hyperref package"}}}