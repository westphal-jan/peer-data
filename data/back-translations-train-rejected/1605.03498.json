{"id": "1605.03498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Deep Neural Networks Under Stress", "abstract": "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.", "histories": [["v1", "Wed, 11 May 2016 16:22:23 GMT  (1584kb,D)", "https://arxiv.org/abs/1605.03498v1", "This article corresponds to the accepted version at IEEE ICIP 2016. We will link the DOI as soon as it is available"], ["v2", "Mon, 23 May 2016 08:34:50 GMT  (1584kb,D)", "http://arxiv.org/abs/1605.03498v2", "This article corresponds to the accepted version at IEEE ICIP 2016. We will link the DOI as soon as it is available"]], "COMMENTS": "This article corresponds to the accepted version at IEEE ICIP 2016. We will link the DOI as soon as it is available", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["micael carvalho", "matthieu cord", "sandra avila", "nicolas thome", "eduardo valle"], "accepted": false, "id": "1605.03498"}, "pdf": {"name": "1605.03498.pdf", "metadata": {"source": "CRF", "title": "DEEP NEURAL NETWORKS UNDER STRESS", "authors": ["Micael Carvalho", "Matthieu Cord", "Sandra Avila", "Nicolas Thome", "Eduardo Valle"], "emails": [], "sections": [{"heading": null, "text": "Index terms - robustness of characteristics, in-depth learning, transfer learning, image classification, compression of characteristics"}, {"heading": "1. INTRODUCTION", "text": "Deep Convolutional Neural Networks have swept the computer vision community, with state-of-the-art performance for many tasks [1, 2, 3]. However, an analytical understanding of their models is still lacking, their use under a cloud of ad hoc processes - tricks of commerce - without which they simply do not function. Therefore, the full understanding of deep representations has become the new holy grail of research in machine learning and computer vision [4, 5]. Here, we examine the properties of deep networks, obtaining discriminatory information about the input, i.e., the measurement of the robustness of the features they generate."}, {"heading": "2. TRANSFER STRATEGIES", "text": "Our main goal is to explore the VGG-M Deep Convolutional Model [14], originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15]. We are conducting extensive experiments to test the robustness of this architecture, which is detailed in Table 1, against various types of loads.Let's formalize the pre-trained Deep Model as a series of functions: Rmi \u2192 Rni, where \u03c6i is the ith layer of the network, m1 is equal to the dimensionality of the input data, and ni = mi + 1 is the output of such a layer. In our stress tests, we select a layer i until we freeze the network (i.e., we keep layers insp1... inspi intact).First, we use the output of the layer inspi to train an SVM. Then we select a stress function T and train the model based on T (inspurable) as an input."}, {"heading": "2.1. Dimensionality Reduction (DR)", "text": "To understand how redundant the deep representation is, the first stress tests drop dimensions from the feature vector. The number of dimensions obtained at each step 1 \u2264 i \u2264 20 is proportional to the initial size n of the feature vector, corresponding to the expression pi = b n \u0445 (21 \u2212 i) 20 c.We compare two strategies for selecting the pi \u2212 1 \u2212 pi dimensions falling at each step i: TDR-1 falls randomly; and TDR-2 uses a PCA-based strategy. The latter discards the dimensions that encode less variance. To take into account the random selection in DR-1, we repeat the experiment ten times."}, {"heading": "2.2. Quantization (Q)", "text": "The other stressor reduces the numerical precision of the representation by quantifying the characteristic vectors. Our goal here is not to explore advanced quantization strategies, but to take into account 2 quick and simple scalar quantizations and to analyze their effects on a classification task. In our first, Q-1, all dimensions are quantified at the same h [1, 30] regular intervals, using the minimum (min) and maximum (max) scalar values observed in the training set for all dimensions. In our second, Q-2, we adjust the boundaries for each dimension individually according to the values observed in the training set. Formally, using the global step st = max \u2212 minh, Q-1 has a single dictionary generated by H = (min + st 2) + st \u00b2 (st \u00b2)."}, {"heading": "2.3. Feature Compression (FC)", "text": "The final FC experiment applies both stressors DR-2 and Q-2 simultaneously, reducing the dimensions of the characteristic vector and quantifying the values of the remaining elements. Our goal is to measure possible cross-effects between DR-2 and Q-2."}, {"heading": "3. EXPERIMENTS", "text": "In fact, most of us are able to move to another world, where we can move to another world, where we can find our way to another world."}, {"heading": "4. DISCUSSION", "text": "In this paper, we examined the robustness of deep imaging by introducing interference to vectors extracted from the upper layers of deep networks. We examined in detail the resilience of features transferred from the VGG-M model to the Pascal VOC 2007 dataset. Our results show that there is a high degree of redundancy in deep imaging, and therefore these can be highly compressed. In our experiments, we achieve a compression rate of 98.4%, while we only have 3https: / / github.com / MicaelCarvalho / DNNsUnderStress0.88% of the original value for Pascal VOC 2007. To ensure that our conclusions are not dataset-specific nor model-specific, our two main approaches - dimensionality reduction and quantization - have been extensively tested, with supplementary results for MIT-67, Food-101, GoogLeNet and BossaNova. In addition, we found that they are more robust, although they are needed for faster architectures."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was partially supported by CNPq, Santander and Samsung Eletro Nica da Amazo Nica Ltd., within the framework of Law No. 8248 / 91. We also thank CENAPAD-SP (Project 533), Microsoft Azure and Amazon Web Services for computing resources and Michel Fornaciali for his valuable advice."}, {"heading": "5. REFERENCES", "text": "[1] A. Krizhevsky, I. Sutskever, and G. Hinton, \"ImageNet classification with deep convolutional neural networks,\" Advances in Neural Information Processing Systems (NIPS), pp. 1-9, 2012. [2] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" CoRR, vol. abs / 1512.03385, 2015. [3] Thibaut Durand, Nicolas Thome, and Matthieu Cord, \"WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks,\" in Computer Vision and Recognition (CVPR), 2016. \"Invariant scattering convolution networks,\" IEEE Transactions on Pattern and Machine Intelligence (PAMI), vol."}], "references": [{"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 1\u20139, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, vol. abs/1512.03385, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks", "author": ["Thibaut Durand", "Nicolas Thome", "Matthieu Cord"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 35, no. 8, pp. 1872\u2013 1886, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1872}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "CNN features off-the-shelf : an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 3320\u20133328.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "LR-CNN for fine-grained classification with varying resolution", "author": ["M. Chevalier", "N. Thome", "M. Cord", "J. Fournier", "G. Henaff", "E. Dusch"], "venue": "2015 IEEE International Conference on Image Processing (ICIP), Sep 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking", "author": ["Thibaut Durand", "Nicolas Thome", "Matthieu Cord"], "venue": "International Conference on Computer Vision (ICCV), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the speed of neural networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M. Mao"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2011, pp. 1\u20138.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Training deep neural networks with low precision multiplications", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Binary- Connect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Reduced-precision strategies for bounded memory in deep neural nets", "author": ["P. Judd", "J. Albericio", "T. Hetherington", "T. Aamodt", "N. Jerger", "R. Urtasun", "A. Moshovos"], "venue": "CoRR, vol. abs/1511.05236, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference (BMVC), 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "The Pascal Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), vol. 88, no. 2, pp. 303\u2013338, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Pooling in image representation: The visual codeword point of view", "author": ["S. Avila", "N. Thome", "M. Cord", "E. Valle", "A. De A. Ara\u00fajo"], "venue": "Computer Vision and Image Understanding (CVIU), vol. 117, no. 5, pp. 453\u2013465, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 413\u2013420.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Recipe recognition with large multimodal food dataset", "author": ["X. Wang", "D. Kumar", "N. Thome", "M. Cord", "F. Precioso"], "venue": "IEEE International Conference on Multimedia & Expo (ICME), 2015, pp. 1\u20136.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "MatConvNet \u2013 Convolutional neural networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "ACM International Conference on Multimedia (MM), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research (JMLR), vol. 9, pp. 1871\u2013187, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1871}, {"title": "Exemplar based metric learning for robust visual localization", "author": ["C. Le Barz", "N. Thome", "M. Cord", "S. Herbin", "M. Sanfourche"], "venue": "2015 IEEE International Conference on Image Processing (ICIP), Sep 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].", "startOffset": 126, "endOffset": 135}, {"referenceID": 1, "context": "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].", "startOffset": 126, "endOffset": 135}, {"referenceID": 2, "context": "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].", "startOffset": 126, "endOffset": 135}, {"referenceID": 3, "context": "Therefore, a full understanding of deep representations became the new Holy Grail of research in Machine Learning and Computer Vision [4, 5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 4, "context": "Therefore, a full understanding of deep representations became the new Holy Grail of research in Machine Learning and Computer Vision [4, 5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "[6, 7, 8, 9]), saving both computational resources and training data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 6, "context": "[6, 7, 8, 9]), saving both computational resources and training data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "[6, 7, 8, 9]), saving both computational resources and training data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "[6, 7, 8, 9]), saving both computational resources and training data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.", "startOffset": 0, "endOffset": 16}, {"referenceID": 12, "context": "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.", "startOffset": 0, "endOffset": 16}, {"referenceID": 13, "context": "Our main objective is to explore the VGG-M deep convolutional model [14], which was originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Our main objective is to explore the VGG-M deep convolutional model [14], which was originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "To better highlight inherent properties of deep models, instead of specific characteristics of VGG-M [14], we also evaluate part of our experiments with GoogLeNet [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "To better highlight inherent properties of deep models, instead of specific characteristics of VGG-M [14], we also evaluate part of our experiments with GoogLeNet [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "Furthermore, in order to differentiate these deep models from classical approaches, we also report comparative results with the recent Bag-of-Words (BoW) model\u2019s BossaNova [17].", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "We also explore how to extend the results obtained for Pascal VOC 2007 by comparing part of the experiments with two other datasets: MIT-67 \u2013 Indoor [18] and UPMC Food101 [19] (67 and 101 classes, respectively).", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "We also explore how to extend the results obtained for Pascal VOC 2007 by comparing part of the experiments with two other datasets: MIT-67 \u2013 Indoor [18] and UPMC Food101 [19] (67 and 101 classes, respectively).", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "In our first one, Q-1, all dimensions are quantized in the same h \u2208 [1, 30] regular intervals, using the minimum (min) and maximum (max) scalar values observed in the training set for all dimensions.", "startOffset": 68, "endOffset": 75}, {"referenceID": 19, "context": "Description of layers (L) and groups (G) of the VGG-M model, from the MatConvNet toolbox [20], proposed by Chatfield et al.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We `2-normalize those feature vectors, and feed them to a linear SVM model1 [21], measuring the model\u2019s scores for dif-", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Those findings are specially useful for image retrieval and metric learning [22], in which the size of the feature vector is crucial to achieve fast response times, and for applications involving portable devices or remote classification, in which data must be efficiently transferred over the network.", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.", "creator": "LaTeX with hyperref package"}}}