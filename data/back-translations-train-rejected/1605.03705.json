{"id": "1605.03705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Movie Description", "abstract": "Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop \"Describing and Understanding Video &amp; The Large Scale Movie Description Challenge (LSMDC)\", at ICCV 2015.", "histories": [["v1", "Thu, 12 May 2016 07:34:08 GMT  (7087kb,D)", "http://arxiv.org/abs/1605.03705v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["anna rohrbach", "atousa torabi", "marcus rohrbach", "niket tandon", "christopher pal", "hugo larochelle", "aaron courville", "bernt schiele"], "accepted": false, "id": "1605.03705"}, "pdf": {"name": "1605.03705.pdf", "metadata": {"source": "CRF", "title": "Movie Description", "authors": ["Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele"], "emails": [], "sections": [{"heading": null, "text": "In this thesis, we propose a novel dataset that includes transcribed ADs that are temporally aligned with full-length movies. In addition, we have also collected and aligned movie scripts used in previous work, and compared the two sources of description. Overall, the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sets and video clips from 202 movies. First, we characterize the dataset by comparing different approaches to producing video descriptions. Comparing ADs with scripts, we find that ADs are actually more visual and describe exactly what is shown, rather than what should have happened after the scripts that were created before the movie was made."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related work", "text": "We discuss current approaches to image and video description, including existing work with film scripts and ADs. We also discuss work that builds on our data. We compare our proposed datasets with associated video description datasets in Table 3 (Section 3.5).2.1 Image descriptionPrior work on image descriptions includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2011); Li et al. (2015); Mitchell et al. (2012); Socher et al. (2014). Recently, the image description has gained increased attention with works such as those by Chen and Zitnick. (2015); Donahue et al. (2015); Karpathy et al. (2015) and Fei File File. (2014, 2015); Mao et al. (2015)."}, {"heading": "3 Datasets for movie description", "text": "The MPII Movie Description Dataset (MPII-MD), originally presented by Rohrbach et al. (2015c), is based on data from Blu-ray movies. It consists of AD data and script data and uses the manual alignment of the transcribed audio at the sentence level to the actions in the video (Section 3.1). In Section 3.2 we discuss how to fully automate AD audio segmentation and alignment for the Montreal Video Annotation Dataset (M-VAD), originally presented by Torabi et al. (2015). M-VAD was collected with the quality of the DVD data and relies solely on AD. Section 3.3 describes the data collection for the Large Scale Movie Description Challenge (LSMDC), which is based on M-VAD et al. (2015)."}, {"heading": "3.1.1 Collection of ADs", "text": "We search for Blu-ray movies with ADs in the Audio Description section of Amazon4 in the UK and select a number of movies of different genres. Since ADs are only available in audio format, we first get the Audio Stream4 www.amazon.co.ukfrom the Blu-ray hard drives. We use MakeMKV5 to extract a Blu-ray im.mkv file format, then XMediaRecode6 to select and extract the audio streams from it. Then we semi-automatically segment the sections of AD audio (which is mixed with the original audio stream) using the approach described below. The audio segments are then transcribed by a crowd-sourced transcription service7, which also provides the timestamps for each spoken phrase.Semi-automatic segmentation of ADs (which is compared with the original audio mixed stream)."}, {"heading": "3.1.2 Collection of script data", "text": "As a starting point, we use the film scripts of \"Hollywood2\" (Marszalek et al. 2009), which have the highest alignment values for their film. We are also interested in comparing the two sources (film scripts and ADs), so we look for the scripts with the names \"Final,\" \"Shooting\" or \"Production Draft,\" where ADs are also available. We have found that the \"overlap\" is quite narrow, so we analyze 11 such films in our data set. In this way, we get a total of 50 movie scripts. We follow existing approaches (Laptev et al. 2008; Laptev et al. 2008) to automatically align scripts with movies. First, we analyze the scripts and expand the method of (Laptev et al. 2008) to process the scripts that differ from the standard format."}, {"heading": "3.1.3 Manual sentence-video alignment", "text": "Since the AD is added to the original audio stream between dialogs, there may be a small misalignment between the speaking time and the corresponding visual content. Therefore, we manually align each set of ADs and scripts to the movie in-house. During manual alignment, we also filter out: a) sentences describing the introduction / completion of the movie (production logo, cast, etc.); b) texts read from the screen; c) irrelevant sentences describing something that is not present in the video; d) sentences related to audio / sound / music. In the case of movie scripts, the reduction in the number of words is about 19%, while in the case of ADs it is less than 4%. In the case of ADs, the filtering is mainly due to initial / ending film intervals and transcribed dialogues (when shown as text). In the scripts, it is mainly attributed to irrelevant sentences. Note that we keep the sentences that are \"aligned\" but contain minor errors."}, {"heading": "3.1.4 Visual features", "text": "We extract video clips from the full movie based on the aligned sentence intervals. We also consistently extract 10 frames from each video clip. As discussed above, ADs and scripts describe activities, objects and scenes (as well as emotions that we do not explicitly deal with these features, but they could still be captured, for example, by context or activities). Below, we briefly describe the visual features based on our publicly available data 10.IDT We extract the improved dense trajectories that are compensated for camera tracking (Wang and Schmid 2013). For each feature (Trajectory, HOG, HOF, MBH) we create a codebook of 4,000 clusters and calculate the appropriate histograms. We apply L1 normalization to the obtained histograms and use them as feature ures.LSDA We use the most recent large-format object detection CNN (Hoffman et al. 2014, the Net Imageet classes)."}, {"heading": "3.2.1 AD narrations segmentation using vocal isolation", "text": "Despite the advantages that AD offers, creating a fully automated approach to extracting the relevant narration or annotation from the audio track and refining the annotation with the scene still presents some challenges. Below, we discuss our automatic solution for segmenting audio videos into DVDs: 1) the standard audio film mixed with the AD narratives. 2) The isolation techniques increase vowels, including dialogues and AD narratives. This technique is used in large parts of audio machines for stereo signals to remove the vowels by reversing the phase of a channel to remove the signals from the center."}, {"heading": "3.2.2 Movie/AD alignment and professional transcription", "text": "This year is the highest in the history of the country."}, {"heading": "4 Approaches for movie description", "text": "Our first approach in Section 4.1 is based on the Statistical Machine Translation (SMT) approach of (Rohrbach et al. 2013). Our second approach (Section 4.2) learns to generate descriptions using our semantic parser (Section 4.1.1). While the first approach does not distinguish which characteristics should be used for different labels, both approaches define different semantic groups of labels and use the most relevant visual characteristics for each group. Therefore, we refer to this approach as visual labels. While the first approach does not distinguish which characteristics should be used for different labels, our second approach defines different semantic groups of labels and uses the most relevant visual characteristics for each group."}, {"heading": "4.1.1 Semantic parsing", "text": "This year it is more than ever before."}, {"heading": "4.1.2 SMT", "text": "In generating sentences, we build on the two-step translation approach of (Rohrbach et al. 2013). In the first step, we learn how to assign visual input to semantic representation (SR) and model pairs of dependencies in a CRF using visual classifiers as unarys, the only ones being trained on tight paths using an SVM (Wang and Schmid 2013). In the second step, it translates the SR into a sentence using statistical machine translation (SMT) (Koehn et al. 2007). For this purpose, the approach uses a concatenated SR as input language, e.g. knife tomatoes, and a natural sentence as output language, e.g. the person cuts the tomato. We automatically obtain the SR from the semantic parser, as described above, Section 4.1.1. In addition to the dense trajectories, we use the features + LSTMNext described in Section 3.1.4.2, and the visual label + LSTM Next we build a two-step approach based on the STM."}, {"heading": "4.2.1 Robust visual classifiers", "text": "For training, we rely on a parallel corpus of videos and weak sentence comments. As before (see Section 4.1), we analyze sentences to obtain a set of labels (singlewords or short phrases, such as looking up) to train visual classifiers. However, this time, we aim to select the most visual labels that can be robustly recognized. To do this, we take three steps. We avoid parser failure. Not all sentences can be successfully analyzed, for example, because some sentences are incomplete or grammatically incorrect. To avoid losing the potential labels in these sentences, we match our initial labels to sentences that the parser has not processed. Semantic groups. Our labels correspond to different semantic groups. In this work, we look at three main groups: verbs, objects, and places. We suggest treating each group of labels independently."}, {"heading": "4.2.2 LSTM for sentence generation", "text": "We rely on the basic LSTM architecture proposed in (Donahue et al. 2015) for the video description. Each time, an LSTM generates a word and receives the visual classifiers (input-vis) as well as the previously generated word (input-lang) as input (see Figure 6 (a)). We encode each word with a one-hot vector according to its index in a dictionary and project it into a low-dimensional embedding. The embedding is learned jointly during the training of the LSTM. We feed the classifier values as input into the LSTM, which is the best variant proposed in (Donahue et al. 2015). We analyze the following aspects for this architecture: layer structure. We compare a 1-layer architecture with a 2-layer architecture. In the 2-layer architecture, the output of the first layer is used as input and the output of the second layer as input."}, {"heading": "5 Evaluation", "text": "In this section, we offer more insights into our movie descriptions. First, we compare ADs with movie scripts, and then name the approaches to VideoCorrectness RelevanceMovie scripts 33.9 (11.2) 33.4 (16.8) ADs 66.1 (35.7) 66.9 (44.9) Table 5 Human evaluation of movie scripts and ADs: Which sentence is more correct / relevant with respect to the video (forced choice). In parentheses: at least 4 out of 5 judges agree. See Section 5.1.Description introduced in Section 4 and other related works. 5.1 Comparison of AD vs. ScriptdataWe compare the AD and script data with 11 movies from the MPII-MD Dataset, where both are available (see Section 3.1.2)."}, {"heading": "5.3.1 Automatic metrics", "text": "Automatic evaluation standards include BLEU-1, -2, -3, -4 (Papineni et al. 2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004) and CIDER (Vedantam et al. 2015). While reporting on all measures for final evaluation in the LSMDC (Section 6), we focus our discussion on the METEOR score in the preliminary evaluations in this section. According to (Elliott and Keller 2013; Vedantam et al. 2015), METEOR replaces previous measures such as BLEU or ROUGE in terms of compliance with human judgments. METEOR also exceeds CIDEr when the number of references is low, and in the case of film description data, we have only one reference."}, {"heading": "5.3.2 Human evaluation", "text": "For human evaluation, we rely on a ranking approach, i.e. human judges receive multiple descriptions of different DC systems and are asked to rate them in terms of the following criteria: correctness, relevance and grammar, motivated by previous work Rohrbach et al. (2013), and on the other hand, we asked human judges to rate sentences according to \"how helpful they would be to a blind person to understand what is happening in the movie.\" AMT workers receive randomized sentences and, in addition to some general instructions, the following definitions: grammar. \"Rank grammatical correctness of sentences: Assess the fluidity and legibility of the sentence (regardless of the correctness in relation to the video).\" Rank correctness of sentences: For which sentence is the content more correct in relation to the video (independent if it is complete, i.e. describes everything that SMSM describes), regardless of grammatical correctness in relation to the video. \"Relevance of sentences: Which sentence is the content that SMT describes independently (SMT)."}, {"heading": "5.4.1 Semantic parsing + SMT", "text": "Table 7 summarizes the results of several variants of the SMT approach when using the SR by our semantic parser. \"Combi\" refers to the combination of IDT, HYBRID, and PLACES as unaries in the CRF. We did not add LSDA because we found that it reduces the performance of the CRF. After extracting the labels, we select those that appear as our visual attributes at least 30 or 100 times. Overall, we find similar performance in all cases, with slightly better results for text labels than for sense labels. This can be attributed to perceptual errors in the semantic parser. In the following, we use the \"IDT 30\" model, which reaches the highest score of 5.59, and refer to it as \"SMT Best\" 16.16 We also evaluated the \"Semantic Parsing + SMT\" approach on a corpus where commented SRs are available."}, {"heading": "5.4.2 Visual labels + LSTM", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5.4.3 Comparison to related work", "text": "In this area, we are in a position to look for a solution."}, {"heading": "5.5.1 Difficulty versus performance", "text": "As the first study, we propose to sort the reference sentences of the tests by difficulty, defining the difficulty in several ways.Sentence length and word frequency Some of the intuitive measures of sentence difficulty are the length and average frequency of words. When sorting the data by difficulty (increasing sentence length or decreasing average word frequency), we find that all three methods have the same tendency to achieve a lower METEOR value with increasing difficulty. Figure 9 (a) shows the performance of comparative methods with increasing sentence length. In terms of word frequency, the correlation is even stronger, see Figure 9 (b). Visual labels consistently outperform the other two methods, with the difficulty increasing most noticeably."}, {"heading": "5.5.2 Semantic analysis", "text": "This year it is more than ever before."}, {"heading": "7 Conclusion", "text": "In this paper, we present the Large Scale Movie Description Challenge (LSMDC), a novel data set with aligned descriptions of film scripts and ADSs (audio descriptions for the blind, also referred to as DVS). Our approach, visual labels, leads to visual classifications and uses their results as input to an LSTM. To address the weak sentence ratios, we advocate three additional components. (1) We differ from the semantic groups of labels and locations."}], "references": [{"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Finding actors and actions in movies", "author": ["Piotr Bojanowski", "Francis Bach", "Ivan Laptev", "Jean Ponce", "Cordelia Schmid", "Josef Sivic"], "venue": "Intelligence", "citeRegEx": "Bojanowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2012}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["Piotr Bojanowski", "R\u00e9mi Lajugie", "Francis Bach", "Ivan Laptev", "Jean Ponce", "Cordelia Schmid", "Josef Sivic"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Bojanowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2014}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David Chen", "William Dolan"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv:1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Movie/script: Alignment and parsing of video and text transcription", "author": ["Timoth\u00e9e Cour", "Chris Jordan", "Eleni Miltsakaki", "Ben Taskar"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Cour et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2008}, {"title": "Learning from ambiguously labeled images", "author": ["Timothee Cour", "Benjamin Sapp", "Chris Jordan", "Ben Taskar"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Cour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2009}, {"title": "An exact dual decomposition algorithm for shallow semantic parsing with constraints", "author": ["Dipanjan Das", "Andr\u00e9 F.T. Martins", "Noah A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard Doell", "Jason Corso"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Das et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Seeing is believing: The quest for multimodal knowledge", "author": ["Gerard de Melo", "Niket Tandon"], "venue": "SIGWEB Newsl.,", "citeRegEx": "Melo and Tandon.,? \\Q2016\\E", "shortCiteRegEx": "Melo and Tandon.", "year": 2016}, {"title": "Clausie: Clausebased open information extraction", "author": ["Luciano Del Corro", "Rainer Gemulla"], "venue": "In Proceedings of the International World Wide Web Conference (WWW),", "citeRegEx": "Corro and Gemulla.,? \\Q2013\\E", "shortCiteRegEx": "Corro and Gemulla.", "year": 2013}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Automatic annotation of human actions in video", "author": ["Olivier Duchenne", "Ivan Laptev", "Josef Sivic", "Francis Bach", "Jean Ponce"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Duchenne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchenne et al\\.", "year": 2009}, {"title": "Image description using visual dependency representations", "author": ["Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Elliott and Keller.,? \\Q2013\\E", "shortCiteRegEx": "Elliott and Keller.", "year": 2013}, {"title": "hello! my name is... buffy\u201d - automatic naming of characters in tv video", "author": ["Mark Everingham", "Josef Sivic", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Everingham et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2006}, {"title": "From captions to visual concepts and back", "author": ["nick", "Geoffrey Zweig"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "nick and Zweig.,? \\Q2015\\E", "shortCiteRegEx": "nick and Zweig.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "M.A. Sadeghi", "Peter Young", "C. Rashtchian", "Julia Hockenmaier", "D.A. Forsyth"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Hodosh", "Alice Young", "Micah Lai", "Julia Hockenmaier"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Hodosh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2014}, {"title": "LSDA: Large scale detection through adaptation", "author": ["Judy Hoffman", "Sergio Guadarrama", "Eric Tzeng", "Jeff Donahue", "Ross Girshick", "Trevor Darrell", "Kate Saenko"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Extending verbnet with novel verb classes", "author": ["Karen Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation (LREC),", "citeRegEx": "Kipper et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kipper et al\\.", "year": 2006}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["Atsuhiro Kojima", "Takeshi Tamura", "Kunio Fukunaga"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Kojima et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kojima et al\\.", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander C Berg", "Tamara L Berg", "Yejin Choi"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "The semi-automatic generation of audio description from screenplays", "author": ["Lakritz", "Salway"], "venue": "Technical report, Dept. of Computing Technical Report, University of Surrey,", "citeRegEx": "Lakritz and Salway.,? \\Q2006\\E", "shortCiteRegEx": "Lakritz and Salway.", "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["Ivan Laptev", "Marcin Marszalek", "Cordelia Schmid", "Benjamin Rozenfeld"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Laptev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2008}, {"title": "Summarization-based video caption via deep neural networks", "author": ["Guang Li", "Shubo Ma", "Yahong Han"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Tvparser: An automatic tv video parsing method", "author": ["Chao Liang", "Changsheng Xu", "Jian Cheng", "Hanqing Lu"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Actions in context", "author": ["Marcin Marszalek", "Ivan Laptev", "Cordelia Schmid"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Marszalek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marszalek et al\\.", "year": 2009}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Alexander C. Berg", "Tamara L. Berg", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Berg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Trecvid 2012 \u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["Paul Over", "George Awad", "Martial Michel", "Jonathan Fiscus", "Greg Sanders", "B Shaw", "Alan F. Smeaton", "Georges Qu\u00e9enot"], "venue": "In Proceedings of TRECVID", "citeRegEx": "Over et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Over et al\\.", "year": 2012}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Wordnet:: Similarity: measuring the relatedness of concepts", "author": ["Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi"], "venue": "In Demonstration Papers at HLT-NAACL", "citeRegEx": "Pedersen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Linking people in videos with \u201dtheir\u201d names using coreference resolution", "author": ["Vignesh Ramanathan", "Armand Joulin", "Percy Liang", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Ramanathan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2014}, {"title": "Grounding Action Descriptions in Videos", "author": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Regneri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Regneri et al\\.", "year": 2013}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Wei Qiu", "Annemarie Friedrich", "Manfred Pinkal", "Bernt Schiele"], "venue": "In Proceedings of the German Conference on Pattern Recognition (GCPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "The long-short story of movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "The long-short story of movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele"], "venue": "Proceedings of the German Conference on Pattern Recognition (GCPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "A dataset for movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "A corpus-based analysis of audio description. Media for all: Subtitling for the deaf, audio description and sign", "author": ["Andrew Salway"], "venue": null, "citeRegEx": "Salway.,? \\Q2007\\E", "shortCiteRegEx": "Salway.", "year": 2007}, {"title": "Associating characters with events in films", "author": ["Andrew Salway", "Bart Lehane", "Noel E. O\u2019Connor"], "venue": "In Proceedings of the ACM international conference on Image and video retrieval (CIVR),", "citeRegEx": "Salway et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salway et al\\.", "year": 2007}, {"title": "Verbnet overview, extensions, mappings and applications", "author": ["Karin Kipper Schuler", "Anna Korhonen", "Susan Windisch Brown"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Schuler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuler et al\\.", "year": 2009}, {"title": "Video captioning with recurrent networks based on frame-and video-level features and visual content classification", "author": ["Rakshith Shetty", "Jorma Laaksonen"], "venue": null, "citeRegEx": "Shetty and Laaksonen.,? \\Q2015\\E", "shortCiteRegEx": "Shetty and Laaksonen.", "year": 2015}, {"title": "who are you?\u201d-learning person specific classifiers from video", "author": ["Josef Sivic", "Mark Everingham", "Andrew Zisserman"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sivic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2009}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Knowlywood: Mining activity knowledge from hollywood narratives", "author": ["Niket Tandon", "Gerard de Melo", "Abir De", "Gerhard Weikum"], "venue": "In Proc. CIKM,", "citeRegEx": "Tandon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tandon et al\\.", "year": 2015}, {"title": "knock! knock! who is it?\u201d probabilistic person identification in tv-series", "author": ["Makarand Tapaswi", "Martin Baeuml", "Rainer Stiefelhagen"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Tapaswi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2012}, {"title": "Movieqa: Understanding stories in movies through questionanswering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence \u2013 video to text. arXiv:1505.00487v2, 2015a", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["Heng Wang", "Cordelia Schmid"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Wang and Schmid.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Schmid.", "year": 2013}, {"title": "Dense trajectories and motion boundary descriptors for action recognition", "author": ["Heng Wang", "Alexander Kl\u00e4ser", "Cordelia Schmid", "C.L. Liu"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A. Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["Ran Xu", "Caiming Xiong", "Wei Chen", "Jason J. Corso"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Empirical performance upper bounds for image and video captioning", "author": ["Li Yao", "Nicolas Ballas", "Kyunghyun Cho", "John R Smith", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Zhong and Ng.,? \\Q2010\\E", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Large datasets of objects (Deng et al. 2009) and scenes (Xiao et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 73, "context": "2009) and scenes (Xiao et al. 2010; Zhou et al. 2014) have had an important impact in computer vision and have significantly improved our ability to recognize objects and scenes.", "startOffset": 17, "endOffset": 53}, {"referenceID": 82, "context": "2009) and scenes (Xiao et al. 2010; Zhou et al. 2014) have had an important impact in computer vision and have significantly improved our ability to recognize objects and scenes.", "startOffset": 17, "endOffset": 53}, {"referenceID": 28, "context": "The combination of large datasets and convolutional neural networks (CNNs) has been particularly potent (Krizhevsky et al. 2012).", "startOffset": 104, "endOffset": 128}, {"referenceID": 54, "context": "To be able to learn how to generate descriptions of visual content, parallel datasets of visual content paired with descriptions are indispensable (Rohrbach et al. 2013).", "startOffset": 147, "endOffset": 169}, {"referenceID": 21, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 38, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 42, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 74, "context": "2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al. 2016; Chen and Dolan 2011) or are not publicly available (Over et al.", "startOffset": 136, "endOffset": 173}, {"referenceID": 43, "context": "2016; Chen and Dolan 2011) or are not publicly available (Over et al. 2012).", "startOffset": 57, "endOffset": 75}, {"referenceID": 50, "context": "TACoS Multi-Level (Rohrbach et al. 2014) and YouCook (Das et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "2014) and YouCook (Das et al. 2013) are exceptions as they provide multiple sentence descriptions and longer videos.", "startOffset": 18, "endOffset": 35}, {"referenceID": 52, "context": "Therefore, in contrast to Salway (2007) and Salway et al.", "startOffset": 26, "endOffset": 40}, {"referenceID": 52, "context": "Therefore, in contrast to Salway (2007) and Salway et al. (2007), our dataset provides alignment to the actions in the video, rather than just to the audio track of the description.", "startOffset": 26, "endOffset": 65}, {"referenceID": 5, "context": "In addition we also mine existing movie scripts, pre-align them automatically, similar to Cour et al. (2008) and Laptev et al.", "startOffset": 90, "endOffset": 109}, {"referenceID": 5, "context": "In addition we also mine existing movie scripts, pre-align them automatically, similar to Cour et al. (2008) and Laptev et al. (2008), and then manually align the sentences to the movie.", "startOffset": 90, "endOffset": 134}, {"referenceID": 50, "context": "Second, we adapt the approach of Rohrbach et al. (2013) by automatically ex-", "startOffset": 33, "endOffset": 56}, {"referenceID": 26, "context": "iter and Schmidhuber 1997) for the image captioning problem (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Kiros et al. 2015; Vinyals et al. 2015) we propose our approachVisual-Labels.", "startOffset": 60, "endOffset": 148}, {"referenceID": 70, "context": "iter and Schmidhuber 1997) for the image captioning problem (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Kiros et al. 2015; Vinyals et al. 2015) we propose our approachVisual-Labels.", "startOffset": 60, "endOffset": 148}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al.", "startOffset": 63, "endOffset": 140}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al.", "startOffset": 63, "endOffset": 162}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al. (2015c) collected the MPII-MD dataset and presented the SMT-based description approach.", "startOffset": 63, "endOffset": 203}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al. (2015c) collected the MPII-MD dataset and presented the SMT-based description approach. Rohrbach et al. (2015b) proposed the Visual-Labels approach.", "startOffset": 63, "endOffset": 307}, {"referenceID": 79, "context": "New datasets have been released, such as the Flickr30k (Young et al. 2014) and MS COCO Captions (Chen et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al.", "startOffset": 27, "endOffset": 45}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al.", "startOffset": 41, "endOffset": 113}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al.", "startOffset": 41, "endOffset": 131}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al.", "startOffset": 41, "endOffset": 157}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al.", "startOffset": 41, "endOffset": 181}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al.", "startOffset": 41, "endOffset": 203}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al.", "startOffset": 41, "endOffset": 312}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al.", "startOffset": 41, "endOffset": 335}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al.", "startOffset": 41, "endOffset": 355}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al.", "startOffset": 41, "endOffset": 384}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al.", "startOffset": 41, "endOffset": 430}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al. (2015); Xu et al.", "startOffset": 41, "endOffset": 453}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al. (2015); Xu et al. (2015a). Much of the recent work has relied on Recurrent Neural Networks (RNNs) and in particular on Long Short-Term Memory networks (LSTMs).", "startOffset": 41, "endOffset": 472}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al. (2015) also presents a standardized protocol for image captioning evaluation.", "startOffset": 28, "endOffset": 72}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al. (2015) also presents a standardized protocol for image captioning evaluation. Other work has analyzed the performance of recent methods, e.g. Devlin et al. (2015) compare them with respect to the novelty of generated descriptions, while also exploring a nearest neighbor baseline that improves over recent methods.", "startOffset": 28, "endOffset": 228}, {"referenceID": 27, "context": "In the past video description has been addressed in controlled settings (Barbu et al. 2012; Kojima et al. 2002), on a small scale (Das et al.", "startOffset": 72, "endOffset": 111}, {"referenceID": 8, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al.", "startOffset": 24, "endOffset": 87}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al.", "startOffset": 25, "endOffset": 195}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al.", "startOffset": 25, "endOffset": 306}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time.", "startOffset": 25, "endOffset": 340}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text.", "startOffset": 25, "endOffset": 446}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text. Xu et al. (2015b) jointly address the language generation and video/language retrieval tasks by learning a joint embedding for a deep video model and a compositional semantic language model.", "startOffset": 25, "endOffset": 623}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text. Xu et al. (2015b) jointly address the language generation and video/language retrieval tasks by learning a joint embedding for a deep video model and a compositional semantic language model. Li et al. (2015) study the problem of summa-", "startOffset": 25, "endOffset": 813}, {"referenceID": 77, "context": "lenging scenario of movie description, Yao et al. (2015) propose a soft-attention based model which selects the most relevant temporal segments in a video, incorporates 3-D CNN and generates a sentence using an LSTM.", "startOffset": 39, "endOffset": 57}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth.", "startOffset": 0, "endOffset": 19}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora.", "startOffset": 0, "endOffset": 207}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework.", "startOffset": 0, "endOffset": 353}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip.", "startOffset": 0, "endOffset": 498}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description.", "startOffset": 0, "endOffset": 749}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description. To model multi-sentence description, Yu et al. (2016) propose to use two stacked RNNs where the first one models words within a sentence and the second one, sentences within a paragraph.", "startOffset": 0, "endOffset": 940}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description. To model multi-sentence description, Yu et al. (2016) propose to use two stacked RNNs where the first one models words within a sentence and the second one, sentences within a paragraph. Yao et al. (2016) has conducted an interesting study on", "startOffset": 0, "endOffset": 1091}, {"referenceID": 13, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 33, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 40, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 61, "context": "2009), as well as a resource to construct activity knowledge base (Tandon et al. 2015; de Melo and Tandon 2016).", "startOffset": 66, "endOffset": 111}, {"referenceID": 9, "context": "2015; de Melo and Tandon 2016). We rely on the approach presented by Laptev et al. (2008) to align movie scripts using subtitles.", "startOffset": 9, "endOffset": 90}, {"referenceID": 7, "context": "They rely on the semantic parser SEMAFOR (Das et al. 2012) trained on the FrameNet database (Baker et al.", "startOffset": 41, "endOffset": 58}, {"referenceID": 0, "context": "2012) trained on the FrameNet database (Baker et al. 1998), however, they limit the recognition only to two frames.", "startOffset": 39, "endOffset": 58}, {"referenceID": 56, "context": "ADs have also been used to understand which characters interact with each other (Salway et al. 2007).", "startOffset": 80, "endOffset": 100}, {"referenceID": 32, "context": "Other prior work has looked at supporting AD production using scripts as an information source (Lakritz and Salway 2006) and automatically finding scene boundaries (Gagnon et al. 2010). Salway (2007) analyses the linguistic properties on a non-public corpus of ADs from 91 movies.", "startOffset": 96, "endOffset": 200}, {"referenceID": 81, "context": "Zhu et al. (2015b) learn a visual-semantic embedding from our clips and ADs to relate movies to books.", "startOffset": 0, "endOffset": 19}, {"referenceID": 62, "context": "Tapaswi et al. (2016) used our AD transcripts for building their MovieQA dataset, which asks natural language questions about movies, requiring an understanding of visual and textual information, such as Dialogue and AD, to answer the question.", "startOffset": 0, "endOffset": 22}, {"referenceID": 62, "context": "Tapaswi et al. (2016) used our AD transcripts for building their MovieQA dataset, which asks natural language questions about movies, requiring an understanding of visual and textual information, such as Dialogue and AD, to answer the question. Zhu et al. (2015a) present a fill-in-the-blank challenge for audio description of the current, previous, and next sentence description for a given clip, requiring to understand the temporal context of the clips.", "startOffset": 0, "endOffset": 264}, {"referenceID": 50, "context": "The MPII Movie Description Dataset (MPII-MD), initially presented by Rohrbach et al. (2015c), was collected from Blu-ray movie data.", "startOffset": 69, "endOffset": 93}, {"referenceID": 64, "context": "tomate AD audio segmentation and alignment for the Montreal Video Annotation Dataset (M-VAD), initially presented by Torabi et al. (2015). M-VAD was collected", "startOffset": 117, "endOffset": 138}, {"referenceID": 40, "context": "As starting point we use the movie scripts from \u201cHollywood2\u201d (Marszalek et al. 2009) that have highest alignment scores to their movie.", "startOffset": 61, "endOffset": 84}, {"referenceID": 5, "context": "We follow existing approaches (Cour et al. 2008; Laptev et al. 2008) to automatically align scripts to movies.", "startOffset": 30, "endOffset": 68}, {"referenceID": 33, "context": "We follow existing approaches (Cour et al. 2008; Laptev et al. 2008) to automatically align scripts to movies.", "startOffset": 30, "endOffset": 68}, {"referenceID": 33, "context": "First we parse the scripts, extending the method of (Laptev et al. 2008) to handle scripts which deviate from the default format.", "startOffset": 52, "endOffset": 72}, {"referenceID": 33, "context": "Then we use the dynamic programming method of (Laptev et al. 2008) to align scripts to subtitles and infer the time-stamps for the description sentences.", "startOffset": 46, "endOffset": 66}, {"referenceID": 22, "context": "LSDA We use the recent large scale object detection CNN (Hoffman et al. 2014) which distinguishes 7,604 ImageNet (Deng et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": "2014) which distinguishes 7,604 ImageNet (Deng et al. 2009) classes.", "startOffset": 41, "endOffset": 59}, {"referenceID": 82, "context": "PLACES and HYBRID Finally, we use the recent scene classification CNNs (Zhou et al. 2014) featuring 205 scene classes.", "startOffset": 71, "endOffset": 89}, {"referenceID": 82, "context": "We use both available networks, Places-CNN and Hybrid-CNN, where the first is trained on the Places dataset (Zhou et al. 2014) only, while the second is additionally trained on the 1.", "startOffset": 108, "endOffset": 126}, {"referenceID": 65, "context": "mer toolbox (Toutanova et al. 2003), then we compute the frequency of stemmed words in the corpora.", "startOffset": 12, "endOffset": 35}, {"referenceID": 8, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 49, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 50, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 74, "context": "Recently, two video description datasets have been proposed, namely MSR-VTT (Xu et al. 2016) and TGIF (Li et al.", "startOffset": 76, "endOffset": 92}, {"referenceID": 35, "context": "2016) and TGIF (Li et al. 2016).", "startOffset": 15, "endOffset": 31}, {"referenceID": 8, "context": "YouCook (Das et al. 2013) x cooking crowd 88 - 2,668 TACoS (Regneri et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 49, "context": "2013) x cooking crowd 88 - 2,668 TACoS (Regneri et al. 2013) x cooking crowd 127 7,206 18,227 TACoS Multi-Level (Rohrbach et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 50, "context": "2013) x cooking crowd 127 7,206 18,227 TACoS Multi-Level (Rohrbach et al. 2014) x cooking crowd 185 14,105 52,593 MSVD (Chen and Dolan 2011) open crowd - 1,970 70,028 TGIF (Li et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 35, "context": "2014) x cooking crowd 185 14,105 52,593 MSVD (Chen and Dolan 2011) open crowd - 1,970 70,028 TGIF (Li et al. 2016) open crowd - 100,000 125,781 MSR-VTT (Xu et al.", "startOffset": 98, "endOffset": 114}, {"referenceID": 74, "context": "2016) open crowd - 100,000 125,781 MSR-VTT (Xu et al. 2016) open crowd 7,180 10,000 200,000", "startOffset": 43, "endOffset": 59}, {"referenceID": 54, "context": "As our first approach we adapt the two-step translation approach of (Rohrbach et al. 2013) which uses an intermediate semantic representation (SR), modeled as a tuple, e.", "startOffset": 68, "endOffset": 90}, {"referenceID": 54, "context": "While we cannot rely on an annotated SR as in (Rohrbach et al. 2013), we automatically mine the SR from sentences using semantic parsing which we introduce in this section.", "startOffset": 46, "endOffset": 68}, {"referenceID": 47, "context": "We lift the words in a sentence to a semantic space of roles and WordNet (Fellbaum 1998; Pedersen et al. 2004) senses by performing SRL (Semantic Role Labeling) and WSD (Word Sense Disambiguation).", "startOffset": 73, "endOffset": 110}, {"referenceID": 54, "context": "Figure 5 Overview of our movie description approaches: (a) SMT-based approach, adapted from (Rohrbach et al. 2013); (b) our proposed LSTM-based approach.", "startOffset": 92, "endOffset": 114}, {"referenceID": 24, "context": "to VerbNet (Kipper et al. 2006; Schuler et al. 2009), a manually curated high-quality linguistic resource for English verbs.", "startOffset": 11, "endOffset": 52}, {"referenceID": 57, "context": "to VerbNet (Kipper et al. 2006; Schuler et al. 2009), a manually curated high-quality linguistic resource for English verbs.", "startOffset": 11, "endOffset": 52}, {"referenceID": 54, "context": "translation approach of (Rohrbach et al. 2013).", "startOffset": 24, "endOffset": 46}, {"referenceID": 82, "context": "Next, we look up the list of \u201cplaces\u201d used in (Zhou et al. 2014) and search for corresponding words among our labels.", "startOffset": 46, "endOffset": 64}, {"referenceID": 22, "context": "We look up the object classes used in (Hoffman et al. 2014) and search for these \u201cobjects\u201d, as well as their base forms (e.", "startOffset": 38, "endOffset": 59}, {"referenceID": 19, "context": "To learn a more robust network which is less likely to overfit we rely on a dropout (Hinton et al. 2012), i.", "startOffset": 84, "endOffset": 104}, {"referenceID": 46, "context": "The automatic evaluation measures include BLEU-1,-2,-3,-4 (Papineni et al. 2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 66, "context": "2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam et al. 2015).", "startOffset": 72, "endOffset": 94}, {"referenceID": 66, "context": "According to (Elliott and Keller 2013; Vedantam et al. 2015), METEOR supersedes previously used measures such as BLEU or ROUGE in terms of agreement with human judgments.", "startOffset": 13, "endOffset": 60}, {"referenceID": 50, "context": "vance, and grammar, motivated by prior work Rohrbach et al. (2013) and on the other hand we asked human judges to rank sentences for \u201chow helpful they would be for a blind person to understand what is hap-", "startOffset": 44, "endOffset": 67}, {"referenceID": 50, "context": "namely TACoS Multi-Level (Rohrbach et al. 2014), and showed the comparable performance to manually annotated SRs, see (Rohrbach et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 50, "context": "17 More details can be found in our corresponding arXiv version Rohrbach et al. (2015a)", "startOffset": 64, "endOffset": 88}, {"referenceID": 77, "context": "2015a), Temporal attention (Yao et al. 2015).", "startOffset": 27, "endOffset": 44}, {"referenceID": 77, "context": "2015a) and Temporal attention (Yao et al. 2015) in METEOR score.", "startOffset": 30, "endOffset": 47}, {"referenceID": 77, "context": "0 Temporal Attention (Yao et al. 2015) 5.", "startOffset": 21, "endOffset": 38}, {"referenceID": 77, "context": "29 Temporal Attention (Yao et al. 2015) 3.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "The submission format was similar to the MS COCO Challenge (Chen et al. 2015) and we also used the identical automatic evaluation protocol.", "startOffset": 59, "endOffset": 77}, {"referenceID": 77, "context": "62 Temporal Attention (Yao et al. 2015) 3.", "startOffset": 22, "endOffset": 39}, {"referenceID": 77, "context": "2015b), Frame-Video-Concept Fusion (Shetty and Laaksonen 2015) and Temporal Attention (Yao et al. 2015) on the blind test set of the LSMDC.", "startOffset": 86, "endOffset": 103}, {"referenceID": 77, "context": "2015b), Temporal Attention (Yao et al. 2015) and Frame-Video-Concept Fusion (Shetty and Laaksonen 2015).", "startOffset": 27, "endOffset": 44}, {"referenceID": 4, "context": "As known from literature (Chen et al. 2015; Elliott and Keller 2013; Vedantam et al. 2015), automatic evaluation measures do not always agree with the human evaluation.", "startOffset": 25, "endOffset": 90}, {"referenceID": 66, "context": "As known from literature (Chen et al. 2015; Elliott and Keller 2013; Vedantam et al. 2015), automatic evaluation measures do not always agree with the human evaluation.", "startOffset": 25, "endOffset": 90}, {"referenceID": 77, "context": "2015b), Frame-Video-Concept Fusion (Shetty and Laaksonen 2015) and Temporal Attention (Yao et al. 2015) on 5 consecutive clips from the blind test set of the LSMDC.", "startOffset": 86, "endOffset": 103}, {"referenceID": 58, "context": "Based on the human evaluation the winner of the challenge is Frame-Video-Concept Fusion approach of Shetty and Laaksonen (2015).", "startOffset": 100, "endOffset": 128}], "year": 2016, "abstractText": "Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop \u201cDescribing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)\u201d, at ICCV 2015. Anna Rohrbach 1 \u00b7 Atousa Torabi \u00b7 Marcus Rohrbach \u00b7 Niket Tandon 1 \u00b7 Christopher Pal \u00b7 Hugo Larochelle \u00b7 Aaron Courville \u00b7 Bernt Schiele 1 Max Planck Institute for Informatics, Saarbr\u00fccken, Germany 2 ICSI and EECS, UC Berkeley, United States 3 Disney Research, Pittsburgh, United States 4 \u00c9cole Polytechnique de Montr\u00e9al, Montr\u00e9al, Canada 5 Universit\u00e9 de Sherbrooke, Sherbrooke, Canada 6 Twitter, Cambridge, United States 7 Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Canada AD: Abby gets in the basket. Mike leans over and sees how high they are. Abby clasps her hands around his face and kisses him passionately. Script: After a moment a frazzled Abby pops up in his place. Mike looks down to see \u2013 they are now fifteen feet above the ground. For the first time in her life, she stops thinking and grabs Mike and kisses the hell out of him. Figure 1 Audio description (AD) and movie script samples from the movie \u201cUgly Truth\u201d.", "creator": "LaTeX with hyperref package"}}}