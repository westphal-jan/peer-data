{"id": "1605.03915", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Optimizing human-interpretable dialog management policy using Genetic Algorithm", "abstract": "Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is human-incomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs respectively.Empirical experimental results are given to show the effectiveness of the proposed approach.", "histories": [["v1", "Thu, 12 May 2016 18:03:38 GMT  (174kb,D)", "https://arxiv.org/abs/1605.03915v1", "This technical report is an updated version of the conference paper: H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, 791-797. Experiments on policy training via user simulator have been enriched and the reward function is updated"], ["v2", "Fri, 13 May 2016 01:28:52 GMT  (174kb,D)", "http://arxiv.org/abs/1605.03915v2", "This technical report is an updated version of the conference paper: \"H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, 791-797\". Experiments on policy training via user simulator have been enriched and the reward function is updated"]], "COMMENTS": "This technical report is an updated version of the conference paper: H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, 791-797. Experiments on policy training via user simulator have been enriched and the reward function is updated", "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["hang ren", "weiqun xu", "yonghong yan"], "accepted": false, "id": "1605.03915"}, "pdf": {"name": "1605.03915.pdf", "metadata": {"source": "CRF", "title": "OPTIMIZING HUMAN-INTERPRETABLE DIALOG MANAGEMENT POLICY USING GENETIC ALGORITHM", "authors": ["Hang Ren", "Weiqun Xu", "Yonghong Yan"], "emails": [], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "2.1. Genetic algorithm and dialog policy template", "text": "It is a global optimization method that can solve both numerical and combinatorial problems; the key ingredient of GA is a fitness function that evaluates the benefits of each individual; the concepts of genotypes and phenotypes are not discriminated against here, including the optimization of controllers in AI games; the psudocode of optimizing DM using GA is in Algorithm 1. We refer to [9] for a detailed description of GA; the concepts of genotypes and phenotypes are not discriminated against here; in GA, a single person directly performs any information that is a fixed length; 1 Genetic algorithms Policy of Optimization 1: Input Fitness Function F, Npop, Nmut, Tmax, K, K."}, {"heading": "2.2. Policy optimization with a user simulator", "text": "Since the fitness function should be consistent with the performance of DM, the easiest way is to evaluate it online with users. However, the interaction with the real user is time and labor intensive, i.e. an agenda-based user algorithm 2 Episodically adapted Q-iteration 1: Input {(si, t, ai, t + 1, si, t + 1)} where t \u2190 1,.., Tt \u2212 1 and i \u2190 1,.., N 2: initialize Q-function approximation Q-iteration (s, a) and arrayQi, tto 0 3: for l \u2190 1,..., Lmax do 4: for i \u2190 1,., N do. for each dialogue 5: for t \u2190 1,.., Ti \u2212 1 do. For each round 6: r \u00b2 reward (si, ai, ai + 1, si, t + 1) 7: if t = Ti \u2212 1 then 8: Qi \u2212 r \u00b2, Q \u2212 1 when the dialogue ends (msj: 11)."}, {"heading": "2.3. On-corpus Q-points regression", "text": "The construction of a user simulator is not trivial and it is difficult to measure the consistency of simulated user behavior vis-\u00e0-vis the real Q. Learning a DM by means of a dialog corpus is attractive, but there is very limited previous work on this topic [11, 12]. We propose to use an existing dialog corpus to estimate the fitness of a DM. First, an offline batch RL algorithm is applied to the corpus, which provides an optimal Q function Q (s, a) and an implicitly defined political QQ (s) = argmaxa Q (s, a), which is optimal with respect to the corpus, where Q (s, a) is used to define the fitness function Q iteration [13] to learn a non-parametric approximator Q (s, a), as described in Algorithm 2. The algorithm uses Bellman to update values (10-Q)."}, {"heading": "2.4. On-corpus DM evaluation", "text": "We describe a DM rating method on the dialogue corpus, without the need to use DM online. A sustained dialogue corpus is used as a test set, and the estimated cumulative reward for the test dialogs is used as a performance metric when following the target DM policy. A similar approach has been taken in assessing the impact of different dialog state trackers on the end-to-end performance of a DM. [15] The estimate of the Q function is similar to Algorithm 2. But instead of learning the optimal policy, the value tracking for the policy to be evaluated is estimated, with the Bellman iteration (line 10) in algorithm 2 transforming into the following states: Qi, t + 2, t + 1, \u03c0 (si, t + 1) (4), where the DM policy is to be evaluated, and then the average reward for starting 1N, iQi, 0 is used as a performance metric."}, {"heading": "3.1. On-line learning experiment by simulation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2. On-corpus learning experiment", "text": "The DSTC2 test corpus rate is for the DSTC2 ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio"}], "references": [{"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Jason D. Williams", "Steve Young"], "venue": "Computer Speech & Language, vol. 21, no. 2, pp. 393\u2013422, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": "Computer Speech & Language, vol. 24, no. 2, pp. 150\u2013174, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Example-based dialog modeling for practical multi-domain dialog system", "author": ["Cheongjae Lee", "Sangkeun Jung", "Seokhwan Kim", "Gary Geunbae Lee"], "venue": "Speech Communication, vol. 51, no. 5, pp. 466\u2013484, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured Probabilistic Modelling for Dialogue Management, Ph.D", "author": ["Pierre. Lison"], "venue": "thesis, University of Oslo,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "POMDP-Based Statistical Spoken Dialog Systems: A Review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise. Thomson", "Jason D. Williams"], "venue": "Proceedings of the IEEE, vol. 101, no. 5, pp. 1160\u20131179, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning for Spoken Dialogue Systems: Comparing Strengths and Weaknesses for Practical Deployment", "author": ["Tim Paek"], "venue": "Tech. Rep. MSR-TR-2006- 62, Microsoft Research, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning with Gaussian processes", "author": ["Yaakov Engel", "Shie Mannor", "Ron Meir"], "venue": "Proceedings of the 22nd international conference on Machine learning. 2005, pp. 201\u2013208, ACM.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaussian Processes for Fast Policy Optimisation of POMDP-based Dialogue Managers", "author": ["Milica Ga\u0161i\u0107", "Filip Jur\u010d\u0131\u0301\u010dek", "Simon Keizer", "Fran\u00e7ois Mairesse", "Blaise Thomson", "Thomson", "Kai Yu", "Steve Young"], "venue": "Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Stroudsburg, PA, USA, 2010, SIGDIAL \u201910, pp. 201\u2013204, Association for Computational Linguistics.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A genetic algorithm tutorial", "author": ["Darrell Whitley"], "venue": "Statistics and Computing, vol. 4, no. 2, pp. 65\u201385, June 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Agenda-based user simulation for bootstrapping a POMDP dialogue system", "author": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young"], "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers. 2007, pp. 149\u2013152, Association for Computational Linguistics.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets", "author": ["James Henderson", "Oliver Lemon", "Kallirroi Georgila"], "venue": "Computational Linguistics, vol. 34, no. 4, pp. 487\u2013511, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Sample-efficient batch reinforcement learning for dialogue management optimization", "author": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Herv\u00e9 Frezza-Buet"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 7, no. 3, pp. 7, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "Journal of Machine Learning Research, 2005, pp. 503\u2013 556.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine Learning, vol. 63, no. 1, pp. 3\u201342, Mar. 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extrinsic Evaluation of Dialog State Tracking and Predictive Metrics for Dialog Policy Optimization", "author": ["Sungjin Lee"], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 310.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The second dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason D. Williams"], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 263.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "The best of both worlds: unifying conventional dialog systems and POMDPs", "author": ["Jason D. Williams"], "venue": "INTER- SPEECH, 2008, pp. 1173\u20131176.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A hybrid approach to dialogue management based on probabilistic rules", "author": ["Pierre Lison"], "venue": "Computer Speech & Language, vol. 34, no. 1, pp. 232\u2013255, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].", "startOffset": 91, "endOffset": 103}, {"referenceID": 1, "context": "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].", "startOffset": 91, "endOffset": 103}, {"referenceID": 2, "context": "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].", "startOffset": 91, "endOffset": 103}, {"referenceID": 3, "context": "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].", "startOffset": 91, "endOffset": 103}, {"referenceID": 4, "context": "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 5, "context": "Despite all the advantages, RL-based DMs are not widely deployed for commercial SDSs due to several reasons [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "In recent studies it has been shown that by incorporating domain knowledge into the design of kernel functions, the GPSARSA [7, 8] algorithm exhibits much faster learning speed than conventional online RL methods.", "startOffset": 124, "endOffset": 130}, {"referenceID": 7, "context": "In recent studies it has been shown that by incorporating domain knowledge into the design of kernel functions, the GPSARSA [7, 8] algorithm exhibits much faster learning speed than conventional online RL methods.", "startOffset": 124, "endOffset": 130}, {"referenceID": 8, "context": "In this paper we propose to use Genetic Algorithm (GA) [9] in optimizing DM policies (GA-DM) which are comprehensible to human designers and easy to verify and modify.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "Each individual is a real vector with consitituent scalars in [0, 1].", "startOffset": 62, "endOffset": 68}, {"referenceID": 8, "context": "We refer readers to [9] for a detailed description of GA.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "floating-point array in our experiment and each number is in [0, 1] as a free parameter of the dialog policy template.", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "If the sampling result lies outside [0, 1], the process is repeated by calling the function perturb recursively.", "startOffset": 36, "endOffset": 42}, {"referenceID": 9, "context": "simulator is utilized [10] and N interactions are conducted between the simulated user and DM.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "Learning a DM using a dialog corpus is appealing but there is very limited prior work on this subject [11, 12].", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "Learning a DM using a dialog corpus is appealing but there is very limited prior work on this subject [11, 12].", "startOffset": 102, "endOffset": 110}, {"referenceID": 12, "context": "We use fitted Q-iteration [13] to learn a nonparametric approximator Q\u0302(s, a), as described in Algorithm 2.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "Extremely Random Trees (ExtraTrees) [14] are utilized for non-parametric regression.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "However, the Q-function trained on a fixed corpus is often inaccurate in unexplored regions of the state space [15, 11].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "However, the Q-function trained on a fixed corpus is often inaccurate in unexplored regions of the state space [15, 11].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "A similar approach has been taken in evaluating the effect of different dialog state tracker on end-to-end performance of a DM [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "In the on-corpus evaluation, DSTC2 dataset [16] is used for both DM policy learning and evaluation.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "With the detailed annotation of dialog states, actions, SLU outputs and other information, it can be used as test set for end-to-end DM performance [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "The DSTC2 testing corpus is used for on-corpus DM learning and evaluation [16], which is produced by a RL-based DM and consists of 1117 dialog sessions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "ThresholdedQ DM as described in [15], which selects the action with the maximum Q-value predicted by Q\u0302(s, a) from the set of actions whose probabilities produced by P\u0302 (a|s) are greater than \u03b4.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "In [17] Williams proposed to construct a hand-crafted DM and it produces a set of candidate actions for given dialog state, from which the best one is chosen by a POMDP-DM.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Lison [18] proposed to use \u2018probabilistic rule\u2019 in specifying the transition and reward sub-models of the POMDP model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Our work bears some resemblance to [18].", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "proposed a hybrid learning method in [11] to learn a policy on an existing dialog corpus by combining the results of supervised and reinforcement learning.", "startOffset": 37, "endOffset": 41}], "year": 2016, "abstractText": "Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is humanincomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs respectively. Empirical experimental results are given to show the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}