{"id": "1605.03924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Joint Embeddings of Hierarchical Categories and Entities", "abstract": "Due to the lack of structured knowledge applied in learning distributed representation of categories, existing work cannot incorporate category hierarchies into entity information.~We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to compute meaningful semantic relatedness between entities and categories.~Compared with the previous state of the art, our framework can handle both single-word concepts and multiple-word concepts with superior performance in concept categorization and semantic relatedness.", "histories": [["v1", "Thu, 12 May 2016 18:45:18 GMT  (1676kb,D)", "http://arxiv.org/abs/1605.03924v1", "10 pages"], ["v2", "Fri, 13 May 2016 01:04:25 GMT  (1676kb,D)", "http://arxiv.org/abs/1605.03924v2", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuezhang li", "ronghuo zheng", "tian tian", "zhiting hu", "rahul iyer", "katia sycara"], "accepted": false, "id": "1605.03924"}, "pdf": {"name": "1605.03924.pdf", "metadata": {"source": "CRF", "title": "Joint Embeddings of Hierarchical Categories and Entities", "authors": ["Yuezhang Li", "Ronghuo Zheng", "Tian Tian", "Zhiting Hu", "Rahul Iyer", "Katia Sycara"], "emails": ["yuezhanl@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Hierarchical Category Embedding", "text": "To find representations for categories and entities that can capture their semantic relationship, we use existing hierarchical categories and entities labeled with these categories and examine two methods: 1) Category Embedding Model (CE model): It replaces the entities in context with their directly labeled categories to form the context of the categories; 2) Hierarchical Category Embedding Model (HCE model): It includes all ancestral categories of the context entities to use the hierarchical information."}, {"heading": "2.1 Category Embedding (CE) Model", "text": "Our Category Embedding Model (CE) is based on the Skip-gram word embedding model (WE et al., 2013). The Skip-gram model aims to generate word representations that are good at predicting context words surrounding a target word in a sliding window. Previous work (Hu et al., 2015) expands the context of the company to the entire article describing the company and acquires a series of entity pairs D = {(et, ec)} in which et denotes the target company and ec the context company. Our CE model expands these approaches by including category information such as Wikipedia, category hierarchies are usually given as DAG or tree structures, and entities are categorized into one or more categories as sheets. Thus, in KBs, each entity et is labeled with one or more categories (c1, c2, cck), \u2265 k, and others are described by an article containing text units and a context."}, {"heading": "2.2 Hierarchical Category Embedding(HCE) Model", "text": "From the above draft, we can deduce the embedding of all categories and units in CBs without grasping the semantics of the hierarchical structure of categories. In a category hierarchy, the categories at the lower levels cover fewer but more specific concepts than categories at the upper levels. To grasp this characteristic, we extend the CE model to include the ancestor categories of the target unit in predicting the context units (see HCE model in Figure 1). If a category is located near a unit, its ancestor categories would also come close to that unit. On the other hand, increasing distance of the category from the target unit would reduce the power of this category in predicting context units. Therefore, given the target unit et and the context unit ec, the goal is to maximize the following weighting of the protocol probability: L = 1 | D (ec, et) D [logP (ec | et) c) are closer to the myth, wi (P), P (wi) or P (wi) categories."}, {"heading": "2.3 Learning", "text": "Learning the CE and HCE models follows the optimization scheme of the Skip-gram model (Mikolov et al., 2013). We use negative sampling to reformulate the objective function, which is then optimized by stochastic gradient descent (SGD). Specifically, the probability of each contextual unit of a target unit with the Softmax function is defined in Equation (3), which is iterated across all units. Therefore, it is mathematically insoluble. We apply the standard negative sampling technique to transform the objective function in Equation (4) into Equation (4) below and then optimize it by SGD: L = \u2211 (ec, et); D [log \u03c3 (ec \u00b7 et) + \u2211 ci \u00b2 A (et) wi log \u03c3 (ec \u00b7 ci)] + \u0445 (e \u00b7 ci) + \u0445 (e \u2032 c, et) \u0445 (c \u00b2), where the sample is (1) and (1)."}, {"heading": "3 Applications", "text": "We test the quality of our category and entity, which are embedded in two different applications: concept categorization and semantic kinship."}, {"heading": "3.1 Concept Categorization", "text": "Concept 2 categorization, also known as concept learning or noun categorization, is a process of assigning a concept to a candidate category taking into account a number of concepts and candidate categories. Traditionally, concept categorization is achieved through concept clustering due to a lack of category representations. As our model can generate category representations, we propose a new method to directly assign each concept to a specific category."}, {"heading": "3.1.1 Concept Clustering", "text": "The concept of clustering is defined as follows: by applying a series of one-word concepts such as dog, cat, apple and the corresponding gold standard categories, all concepts are projected onto a semantic space and clustered, and the cluster results can be evaluated by comparing them with the gold standard categories. Previous methods (Almuhareb and Poesio, 2004; Almuhareb and Poesio, 2005) have managed to learn the characteristics and attributes of a concept; for example, the concept dog has attributes of (dog color) and (dog size) and the characteristics of (dog brown) and (dog small). By presenting these attributes and attributes in a high-dimensional space, concepts can be clustered on the basis of common attributes and characteristics."}, {"heading": "3.1.2 Nearest Neighbor (NN) Classification", "text": "Although the methods described above can generate vectors that have been carefully designed to capture relationships and attributes, the number of vector dimensions can be very large for some methods: from 10,000 + to 1,000,000 +, e.g. (Almuhareb and Poesio, 2005; Rothenha \ufffd usler and Guests, 2009).Due to the large dimensionality, the applicable cluster methods are limited to those that can be scaled to such high dimensions.Other methods, such as Word Embedding (Mikolov et al., 2013) require lower dimensional vectors, but suffer from granularity problems. Therefore, we propose an alternative method, namely the classification of the closest neighbors (NN), and evaluate comparative target conflicts (see Table 3).2In this paper, concept and unit denote the same. 3 (Rothenha \ufffd user and protectors, 2009) we use CLUTO (Karypis, 2002), a concept that optimizes epictor on large categories."}, {"heading": "3.1.3 Evaluation Metrics", "text": "Since purity functions as a standard yardstick for clustering (Rothenhausler und Sch\u00fctzler, 2009) to compare our model with the concept of clustering, we also use purity to measure the performance of our model. In general, purity is defined as: purity (B, G) = 1n \u0445 k max j | \u03c9k \u0445 gj |, (5), where \"cluster solution\" means a cluster solution of n clusters, \"G\" means a group of gold standard classes, \"spectrum\" means the group of labels in a cluster, and \"GJ\" means the group of labels in a class. A higher purity indicates better model performance."}, {"heading": "3.2 Semantic Relatedness", "text": "Semantic relationship correlation is a process of assigning a relation correlation score to a word pair. We use a set of standard semantic benchmarks. These benchmarks consist of pairs of words that are evaluated manually. To evaluate our result, the performance of the model is evaluated by calculating the correlation between the values generated by the model and the averages of human subjects. We use the rank correlation coefficient of the Spearman (?) of the values assigned by humans and the values assigned to the system. Note that the values given by humans are not necessarily the gold standard due to the differences between humans and the difficulty of giving a clear definition of word similarity. However, a good system should agree to some degree with humans and therefore have a relatively high correlation coefficient (although the coefficient is high enough, a higher coefficient would not necessarily reflect the better model)."}, {"heading": "4 Experiments", "text": "In the experiments, we use the data set collected from Wikipedia on December 1, 20154 as training data. We edit the category hierarchy by trimming administrative categories and deleting bottom-up edges to construct a DAG. The final version of the data contains 5,373,165 units and 793,856 categories organized as DAG with a maximum depth of 18. The root category is the \"main topic classification.\" We train category and entity vectors in various dimensions of 100, 200, 250, 300, 400, 500, with batch size B = 500 and negative sample size k = 10. With the training data set defined above, we conduct experiments on two applications: concept categorization and semantic relationship."}, {"heading": "4.1 Concept Categorization", "text": "In this section, we first present data sets used in concept categorization, and then show baselines, followed by experimental results."}, {"heading": "4.1.1 Datasets", "text": "There are two sets of data used in this experiment, the first being the Battig test introduced by (Baroni and Lenci, 2010), which comprises 83 concepts out of 10 categories, and the Battig test contains only one-word concepts without multi-word concepts (e.g. \"table tennis\"), so with this data set we are limiting the power of concept categorization to a single-word level. We are using this data set because it has been used as a benchmark for most previous approaches to concept categorization, and due to the limitations of the Battig test set, we are constructing a new data set for DOTA (enTity cAtegorization data set) with 450 entities categorized into 15 categories (see Appendix A. All categories and entities are extracted from Wikipedia, so the resulting data sets do not necessarily contain a single word. \""}, {"heading": "4.1.2 Baselines", "text": "\u2022 WEMikolov (Mikolov et al., 2013): (Baroni et al., 2014) conducted thorough experiments on word count and predictive methods of word representation, and their experimental results show that Mikolov's word embedding is state-of-the-art in concept categorization. We trained word embedding with Mikolov's word2vec toolkit5 on5https: / / code.google.com / archive / p / word2vec / the same Wikipedia corpus as we did (1.7 million tokens) and then applied the Skip-gram model with negative sample size of 10 and window size of 5 on5https: / / code.google.com / archive / p / word2vec / the same Wikipedia corpus (1.7 million tokens)."}, {"heading": "4.1.3 Results", "text": "In the experiments we used scikit-learn (Pedregosa et al., 2011) to perform clustering. We tested k-means and hierarchical clustering with different distance metrics (euclidean, cosine) and linkage criterion (ward, complete, average). We reported the best result on different clustering parameters in each experiment. 1 shows the experimental results of the concept clustering method. It is clear that hierarchical category embedding (HCE) model outperforms other methods in all datasets. For singleword entity categorization in Battig and DOTAsingle, our HCE model gives a purity of 89% and 92% respectively, while for multiword entity categorization in DOTA-mult and DOTA-all, the corresponding dolls are 91% and 89% (higher is better).The outstanding performance of our HCE model on multiple DOTA-mult and DOTA-ena-all is in the fact6t6th: / anct./ / olena."}, {"heading": "4.2 Semantic relatedness", "text": "We now present the data sets and baselines for measuring the semantic relationship and show the experimental results."}, {"heading": "4.2.1 Datasets", "text": "We use a set of standard data sets and process them to customize our method. Our method requires matching words to corresponding Wikipedia entities or categories. For example, we match the word \"cat\" to the Wikipedia entity \"cat\" and the word \"equipment\" to the Wikipedia category \"equipment.\" However, it is difficult for this approach to match some words to a specific Wikipedia entity or category for two reasons: \u2022 Wikipedia is a knowledge base that organizes categories and concepts, but some words such as adjectives (e.g. smart / stupid) cannot be matched to a Wikipedia entity or category. Furthermore, our entity-based approach cannot match word pairs with lexical differences such as \"swim / swim\" and \"swim.\""}, {"heading": "4.2.2 Baselines", "text": "We compare our methods with some state-of-the-art methods below. WN + NR: In (Radhakrishnan and Varma, 2013), word similarity measurement is derived from Wikipedia category names integrated with WordNet similarity measurement by performing a regression using a support vector machine. WN + NR1 and WN + NR2 are two of the best models described in their papers to make them comparable.WEMikolov: As described in Section 4.1.2, we trained word embedding with Mikolov's word2vec toolkit7 on the same Wikipedia corpus (1.7 million tokens) as ours. We use Skip-gram model with negative sample size of 10 and window size of 5, with vector dimensions of 100, 200, 250, 300, 400, 500. The best results we obtained from different parameter settings in Table 4.WESenna (Collobert et al., 2011) we have included this word on Wikipedia for 8-8 months."}, {"heading": "4.2.3 Results", "text": "Table 4 shows the experimental results of the semantic relationship tasks. We can see that our HCE model delivers the best results of 57%, 69% and 83% for MS, MEN and RG datasets, which suggests that entity and category embedding can be used as an indicator of semantic affinity between words. For WSS datasets, the result of our method is comparable to the WN + NR1 method, which integrates WordNet similarity measurements with the normalized representation of category names. We also found that Mikolov's word embedding works better than our method for WSR datasets, but performs worse than our method for WSS datasets. This may be because the WSR dataset focuses on topically related words and not on taxonomy-specific words, and7https: / / code.google.com / archive / worve2p / / colanrontlo.com / http: / / colsenomy."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a framework for learning how to embed units and categories in order to capture semantic relationships between units and categories, which can integrate taxonomy hierarchies from large knowledge bases. Experiments on both concept categorization and semantic relationship show that our approach surpasses modern approaches, and in future work we intend to apply our method to other applications such as hierarchical document classification."}, {"heading": "A The DOTA dataset: 300 single-word entities and 150 multi-word entities from 15 Wikipedia Categories", "text": "This year, it has come to the point where it only occurs once, until there is a scandal."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "Proceedings of Human Language Technologies: The", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Attribute-based and value-based clustering: An evaluation", "author": ["Abdulrahman Almuhareb", "Massimo Poesio."], "venue": "EMNLP, volume 4, pages 158\u2013165.", "citeRegEx": "Almuhareb and Poesio.,? 2004", "shortCiteRegEx": "Almuhareb and Poesio.", "year": 2004}, {"title": "Concept learning and categorization from the web", "author": ["Abdulrahman Almuhareb", "Massimo Poesio."], "venue": "Proc. of CogSci.", "citeRegEx": "Almuhareb and Poesio.,? 2005", "shortCiteRegEx": "Almuhareb and Poesio.", "year": 2005}, {"title": "Distributional memory: A general framework for corpusbased semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673\u2013721.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In ACL (1), pages 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "J. Artif. Intell. Res.(JAIR), 49(1-47).", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Recursive regularization for large-scale classification with hierarchical and graphical dependencies", "author": ["Siddharth Gopal", "Yiming Yang."], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data", "citeRegEx": "Gopal and Yang.,? 2013", "shortCiteRegEx": "Gopal and Yang.", "year": 2013}, {"title": "Entity hierarchy embedding", "author": ["Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric P Xing."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Hu et al\\.,? 2015", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "A unified semantic embedding: Relating taxonomies and attributes", "author": ["Sung Ju Hwang", "Leonid Sigal."], "venue": "Advances in Neural Information Processing Systems, pages 271\u2013279.", "citeRegEx": "Hwang and Sigal.,? 2014", "shortCiteRegEx": "Hwang and Sigal.", "year": 2014}, {"title": "Cluto-a clustering toolkit", "author": ["George Karypis."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Karypis.,? 2002", "shortCiteRegEx": "Karypis.", "year": 2002}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "AAAI, pages 2181\u20132187.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Scikitlearn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Extracting semantic knowledge from wikipedia category names", "author": ["Priya Radhakrishnan", "Vasudeva Varma."], "venue": "Proceedings of the 2013 workshop on Automated knowledge base construction, pages 109\u2013114. ACM.", "citeRegEx": "Radhakrishnan and Varma.,? 2013", "shortCiteRegEx": "Radhakrishnan and Varma.", "year": 2013}, {"title": "Hubs in space: Popular nearest neighbors in high-dimensional data", "author": ["Milo\u0161 Radovanovi\u0107", "Alexandros Nanopoulos", "Mirjana Ivanovi\u0107."], "venue": "The Journal of Machine Learning Research, 11:2487\u2013 2531.", "citeRegEx": "Radovanovi\u0107 et al\\.,? 2010", "shortCiteRegEx": "Radovanovi\u0107 et al\\.", "year": 2010}, {"title": "Unsupervised classification with dependency based word spaces", "author": ["Klaus Rothenh\u00e4usler", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 17\u201324. Association for Computational", "citeRegEx": "Rothenh\u00e4usler and Sch\u00fctze.,? 2009", "shortCiteRegEx": "Rothenh\u00e4usler and Sch\u00fctze.", "year": 2009}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Learning hierarchical similarity metrics", "author": ["Nakul Verma", "Dhruv Mahajan", "Sundararajan Sellamanickam", "Vinod Nair."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2280\u20132287. IEEE.", "citeRegEx": "Verma et al\\.,? 2012", "shortCiteRegEx": "Verma et al\\.", "year": 2012}, {"title": "Knowledge graph and text jointly embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "EMNLP, pages 1591\u20131601. Citeseer.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Large margin taxonomy embedding for document categorization", "author": ["Kilian Q Weinberger", "Olivier Chapelle."], "venue": "Advances in Neural Information Processing Systems, pages 1737\u20131744.", "citeRegEx": "Weinberger and Chapelle.,? 2009", "shortCiteRegEx": "Weinberger and Chapelle.", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": "These hierarchical categories could benefit applications such as concept categorization (Rothenh\u00e4usler and Sch\u00fctze, 2009), document categorization (Gopal and Yang, 2013), object categorization (Verma et al.", "startOffset": 88, "endOffset": 121}, {"referenceID": 8, "context": "These hierarchical categories could benefit applications such as concept categorization (Rothenh\u00e4usler and Sch\u00fctze, 2009), document categorization (Gopal and Yang, 2013), object categorization (Verma et al.", "startOffset": 147, "endOffset": 169}, {"referenceID": 21, "context": "These hierarchical categories could benefit applications such as concept categorization (Rothenh\u00e4usler and Sch\u00fctze, 2009), document categorization (Gopal and Yang, 2013), object categorization (Verma et al., 2012), and link prediction in knowlegde graphs (Lin et al.", "startOffset": 193, "endOffset": 213}, {"referenceID": 12, "context": ", 2012), and link prediction in knowlegde graphs (Lin et al., 2015).", "startOffset": 49, "endOffset": 67}, {"referenceID": 9, "context": "Current entity embedding methods cannot provide the relatedness measure between entities and categories, although they successfully learn entity representations and relatedness measure between entities (Hu et al., 2015).", "startOffset": 202, "endOffset": 219}, {"referenceID": 22, "context": "Knowledge graph embedding methods (Wang et al., 2014; Lin et al., 2015) give embeddings of entities and relations", "startOffset": 34, "endOffset": 71}, {"referenceID": 12, "context": "Knowledge graph embedding methods (Wang et al., 2014; Lin et al., 2015) give embeddings of entities and relations", "startOffset": 34, "endOffset": 71}, {"referenceID": 23, "context": "Though taxonomy embedding methods (Weinberger and Chapelle, 2009; Hwang and Sigal, 2014) learn category embeddings, they primarily target documents classification not entity", "startOffset": 34, "endOffset": 88}, {"referenceID": 10, "context": "Though taxonomy embedding methods (Weinberger and Chapelle, 2009; Hwang and Sigal, 2014) learn category embeddings, they primarily target documents classification not entity", "startOffset": 34, "endOffset": 88}, {"referenceID": 9, "context": "The Category Embedding model (CE model) extends the entity embedding method of (Hu et al., 2015) by using category information with entities to learn entity and category embeddings.", "startOffset": 79, "endOffset": 96}, {"referenceID": 3, "context": "We train the category and entity vectors on Wikipedia, and then evaluate our methods from two applications: concept categorization (Baroni and Lenci, 2010) and semantic relatedness (Finkelstein et al.", "startOffset": 131, "endOffset": 155}, {"referenceID": 7, "context": "We train the category and entity vectors on Wikipedia, and then evaluate our methods from two applications: concept categorization (Baroni and Lenci, 2010) and semantic relatedness (Finkelstein et al., 2001).", "startOffset": 181, "endOffset": 207}, {"referenceID": 14, "context": "Our category embedding (CE) model is based on the Skip-gram word embedding model(Mikolov et al., 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 9, "context": "Previous work (Hu et al., 2015) extends the entity\u2019s context to the whole article that describes the entity and acquires a set of entity pairs D = {(et, ec)}, where et denotes the target entity and ec denotes the context entity.", "startOffset": 14, "endOffset": 31}, {"referenceID": 13, "context": "To learn embeddings of entities and categories simultaneously, we adopt a method that incorporates the labeled categories into the entities when predicting the context entities, similar to TWE1 model (Liu et al., 2015).", "startOffset": 200, "endOffset": 218}, {"referenceID": 14, "context": "mization scheme of skip-gram model (Mikolov et al., 2013).", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "Previous methods (Almuhareb and Poesio, 2004; Almuhareb and Poesio, 2005) managed to learn the properties and attributes of a concept; for example, the concept dog has attributes of (dog color) and (dog size) and the properties of", "startOffset": 17, "endOffset": 73}, {"referenceID": 2, "context": "Previous methods (Almuhareb and Poesio, 2004; Almuhareb and Poesio, 2005) managed to learn the properties and attributes of a concept; for example, the concept dog has attributes of (dog color) and (dog size) and the properties of", "startOffset": 17, "endOffset": 73}, {"referenceID": 2, "context": ", (Almuhareb and Poesio, 2005; Rothenh\u00e4usler and Sch\u00fctze, 2009).", "startOffset": 2, "endOffset": 63}, {"referenceID": 18, "context": ", (Almuhareb and Poesio, 2005; Rothenh\u00e4usler and Sch\u00fctze, 2009).", "startOffset": 2, "endOffset": 63}, {"referenceID": 14, "context": "Other methods such as word embedding (Mikolov et al., 2013) may need lower dimensionality vectors but suffer from granularity problems.", "startOffset": 37, "endOffset": 59}, {"referenceID": 18, "context": "(Rothenh\u00e4usler and Sch\u00fctze, 2009) use CLUTO (Karypis, 2002), a clustering toolkit optimized to cluster large-scale data in reasonable time, as their standard measurements.", "startOffset": 0, "endOffset": 33}, {"referenceID": 11, "context": "(Rothenh\u00e4usler and Sch\u00fctze, 2009) use CLUTO (Karypis, 2002), a clustering toolkit optimized to cluster large-scale data in reasonable time, as their standard measurements.", "startOffset": 44, "endOffset": 59}, {"referenceID": 18, "context": "Since purity works as a standard evaluation metric for clustering (Rothenh\u00e4usler and Sch\u00fctze, 2009), to compare our model with the concept clustering, we also use purity to measure our model\u2019s perfor-", "startOffset": 66, "endOffset": 99}, {"referenceID": 3, "context": "The first one is the Battig test set introduced by (Baroni and Lenci, 2010), which includes 83 concepts from 10 categories.", "startOffset": 51, "endOffset": 75}, {"referenceID": 14, "context": "\u2022 WEMikolov(Mikolov et al., 2013): (Baroni et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 4, "context": ", 2013): (Baroni et al., 2014) conducted thorough experiments on word counts and predictive based methods on word representation.", "startOffset": 9, "endOffset": 30}, {"referenceID": 6, "context": "\u2022 WESenna (Collobert et al., 2011): We downloaded this 50-dimension word embedding6 trained on Wikipedia over 2 months.", "startOffset": 10, "endOffset": 34}, {"referenceID": 15, "context": "(Pedregosa et al., 2011) to perform clustering.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "WS: The WordSim353 dataset is introduced by (Finkelstein et al., 2001).", "startOffset": 44, "endOffset": 70}, {"referenceID": 0, "context": "work of (Agirre et al., 2009) splits the WS-353 dataset into two separate subsets: similarity subset (WSS) and relatedness subset (WSR).", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "MEN: The work of (Bruni et al., 2014) constructed this dataset with 3000 word pairs that have semantic relatedness scores obtained by crowd-sourcing.", "startOffset": 17, "endOffset": 37}, {"referenceID": 19, "context": "RG: A classic dataset contains 65 word pairs introduced by (Rubenstein and Goodenough, 1965).", "startOffset": 59, "endOffset": 92}, {"referenceID": 16, "context": "WN+NR: In (Radhakrishnan and Varma, 2013), word similarity measure is derived from Wikipedia category names integrated with WordNet similarity measure by performing regression using a Support Vector Machine.", "startOffset": 10, "endOffset": 41}, {"referenceID": 6, "context": "WESenna (Collobert et al., 2011): We downloaded this 50-dimension word embedding8 trained on Wikipedia over 2 months.", "startOffset": 8, "endOffset": 32}], "year": 2017, "abstractText": "Due to the lack of structured knowledge applied in learning distributed representation of categories, existing work cannot incorporate category hierarchies into entity information. We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to compute meaningful semantic relatedness between entities and categories. Compared with the previous state of the art, our framework can handle both single-word concepts and multipleword concepts with superior performance in concept categorization and semantic relatedness.", "creator": "TeX"}}}