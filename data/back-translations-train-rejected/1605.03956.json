{"id": "1605.03956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "On the Convergent Properties of Word Embedding Methods", "abstract": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.", "histories": [["v1", "Thu, 12 May 2016 19:59:43 GMT  (293kb,D)", "http://arxiv.org/abs/1605.03956v1", "RepEval @ ACL 2016"]], "COMMENTS": "RepEval @ ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yingtao tian", "vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1605.03956"}, "pdf": {"name": "1605.03956.pdf", "metadata": {"source": "CRF", "title": "On the Convergent Properties of Word Embedding Methods", "authors": ["Yingtao Tian", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena"], "emails": ["yittian@cs.stonybrook.edu", "vvkulkarni@cs.stonybrook.edu", "bperozzi@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings learned using neural methods have been enormously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016), but the question \"What properties are shown by word embeddings learned by different techniques?\" remains largely unexplored. Mikolov et al. (2013b) shows that embeddings by Skip-Gram model reveal linear substructures. Arora et al. (2015) Attempt to demonstrate this linear substructure by suggesting a generative model of discourse vectors and (Arora et al., 2016) show that embeddings are characterized by the work. It is posted here for your personal use."}, {"heading": "2 Background and Setup", "text": "Here we describe the data sets and word embeddings we use in our experiments. We analyze two word embeddings GloVe (Pennington et al., 2014) and Skip-gram (SG) (Mikolov et al., 2013a), where we set the number of dimensions D to 300. Our corpora consists of a combination of English Wikipedia dump with 1.6 billion tokens and the English section of News Crawl2 with 4.3 billion tokens for a total value of 6 billion tokens. We use settings similar to Pennington et al. (2014). We set the vocabulary to the 400,000 most common words, use a size 10 context window, and do 50 iterations across the entire dataset. We use News Crawl for two reasons: It is a publicly available corpus of the same genre as Gigawords 5 (formal English messages) and News Crawl is also available in Czech, German, Finnish and Russia."}, {"heading": "3 Similarities of Word Embeddings", "text": "How similar are the embeddings we learned from independent runs over the same corpus? Formally, two D-dimensional embeddings E (m) and E (n) differ only in their original random initialization: \u2022 Can we define a measure of similarity between the dimensions (E (m) i, E (n) j)? \u2022 In the face of such a measure, can we align the dimensions in E (m) with the dimensions in E (n) to check the similarity between E (m) and E (n)?"}, {"heading": "3.1 Measure of Similarity between Feature Dimensions", "text": "In view of E (m) and E (n), we define the similarity between them as \u03ba (i, j) = \u03c1 (E (m) i, E (n) j), where \u03c1 (x, y) is defined as the Pearson correlation coefficient between the column vectors x and y. We note that this measure of similarity is well suited to capture linear relations between attribute dimensions. We leave behind the study of metrics that capture nonlinear relations (such as mutual information) between attribute dimensions for future work. In Figure 1, we show in blue the histogram of the values in correlation matrices \u0445 (i, j) on E (m) and E (n) obtained on the glove and embedding of the skip gram."}, {"heading": "3.2 One-to-one Alignment", "text": "Since the optimization problems associated with GloVe and Skip-gram are not convex by nature, the learned embeddings could potentially correspond to different local optima, implying that the properties learned in different runs could be equivalent in the context of a manifold transformation. Therefore, in the first step, we ask: Is there a one-to-one agreement between features in two embeddings? (similar to Li et al. (2016))) Therefore, given E (n) d and E (m) ad, we try to find a one-to-one matching that is represented as a = (a1, a2,.. aD), meaning that for each d (1, 2,.,.,., D) E (n) d is matched with E (m) ad, whereby the aggregated correlation of 1 to 1 D (a2,.) d = 1 (n) d (n) d (d), E (maximum ad) d (d) can be fully converted into the structure of a biinstance."}, {"heading": "3.3 Many-to-one Mapping", "text": "In the previous section, we sought a one-to-one alignment between the characteristics in two embedding, the disadvantage of which is their restrictiveness, since it assumes that both models learn exactly the same characteristics that are equivalent under permutations. In this section, we assume that a characteristic in E (m) could correspond to several characteristics in E (n). We grasp this by measuring the linear relationship between E (n) and E (m), which is transformed by means of CCA (canonical correlation analysis). In view of two matrices X (T) and Y (T), we are able to recognize the linear relations between E (n) and E (m), which are transformed by means of CCA (canonical correlation analysis)."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed an intrinsic measurement value for evaluating word embedding - its consistency across random initializations. Our preliminary results showed that while overall performance remained constant across embedding, there was considerable disagreement between each instance of a particular embedding. To investigate this difference more closely, we investigated the similarity between the dimensions of each embedding space. Furthermore, we propose methods to align the dimensions of word embedding and an intrinsic metric size cca to measure the similarity of learned word embedding, which, as our preliminary results show, can correlate with downstream tasks in terms of model match, using the popular word analogy task in Mikolov et al. (2013a) as an example. We believe that the methods and metrics proposed in our work will enable a deeper investigation of the convergent properties of embedding models to improve performance and optimization algorithms and P."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1502.03520.", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Linear algebraic structure of word senses, with applications to polysemy", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1601.03764.", "citeRegEx": "Arora et al\\.,? 2016", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "An n\u02c65/2 algorithm for maximum matchings in bipartite graphs", "author": ["John E Hopcroft", "Richard M Karp."], "venue": "SIAM Journal on computing, 2(4):225\u2013231.", "citeRegEx": "Hopcroft and Karp.,? 1973", "shortCiteRegEx": "Hopcroft and Karp.", "year": 1973}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Yixuan Li", "Jason Yosinski", "Jeff Clune", "Hod Lipson", "John Hopcroft"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."], "venue": "nternational Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL, pages 746\u2013 751.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "author": ["B. Plank", "A. S\u00f8gaard", "Y. Goldberg."], "venue": "ArXiv e-prints, April.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to graph theory", "author": ["Douglas B West"], "venue": null, "citeRegEx": "West.,? \\Q1996\\E", "shortCiteRegEx": "West.", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 2, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 10, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 9, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 1, "context": "(2015) attempt to explain this linear substructure by proposing a generative model based on random walks over discourse vectors and (Arora et al., 2016) show that", "startOffset": 132, "endOffset": 152}, {"referenceID": 3, "context": "Mikolov et al. (2013b) show that embeddings learned by Skip-Gram model reveal linear substructures.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Arora et al. (2015) attempt to explain this linear substructure by proposing a generative model based on random walks over discourse vectors and (Arora et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "on downstream tasks merely a coincidence? To illustrate with an example, consider the performance of GloVe embeddings obtained on two independent runs (differing only in the random initialization) on the analogy task (Mikolov et al., 2013a) in Table 1.", "startOffset": 217, "endOffset": 240}, {"referenceID": 6, "context": "Table 1: A set of examples of questions in word analogy task (Mikolov et al., 2013a) where two independent runs of GloVe embeddings differ in the answers.", "startOffset": 61, "endOffset": 84}, {"referenceID": 6, "context": "Table 2: Performance of two independent runs of GloVe and skip-gram on word analogy task (Mikolov et al., 2013a).", "startOffset": 89, "endOffset": 112}, {"referenceID": 8, "context": "We analyze two word embeddings GloVe (Pennington et al., 2014) and skip-gram (SG) (Mikolov et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 6, "context": ", 2014) and skip-gram (SG) (Mikolov et al., 2013a) where we set the number of dimensions D to be 300.", "startOffset": 27, "endOffset": 50}, {"referenceID": 5, "context": ", 2014) and skip-gram (SG) (Mikolov et al., 2013a) where we set the number of dimensions D to be 300. Our corpora consists of a combination of English Wikipedia dump with 1.6 billions tokens and the English section of the News Crawl2 with 4.3 billion tokens totaling to 6 billion tokens. We use similar settings as Pennington et al. (2014). We set the vocabulary to the 400, 000 most frequent words, use a context window of size 10, and make 50 iterations through the whole dataset.", "startOffset": 28, "endOffset": 340}, {"referenceID": 8, "context": "Note that these accuracies are comparable to scores obtained on the same task in Pennington et al. (2014). We denote these 4 embeddings as E(m) (a matrix of dimensions |V | \u00d7 D) where m \u2208 {GloVe-1,GloVe-2,SG-1,SG-2}.", "startOffset": 81, "endOffset": 106}, {"referenceID": 4, "context": "Therefore as a first step, we ask: Is there a one-to-one alignment between features in two embeddings? (similar to Li et al. (2016))", "startOffset": 115, "endOffset": 132}, {"referenceID": 11, "context": "By building a complete bipartite graph, this can be converted into an instance of maximum weighted bipartite matching (West, 1996), which can be solved effectively using Hopcroft-Karp Algorithm (Hopcroft and Karp, 1973) in polynomial time.", "startOffset": 118, "endOffset": 130}, {"referenceID": 3, "context": "By building a complete bipartite graph, this can be converted into an instance of maximum weighted bipartite matching (West, 1996), which can be solved effectively using Hopcroft-Karp Algorithm (Hopcroft and Karp, 1973) in polynomial time.", "startOffset": 194, "endOffset": 219}, {"referenceID": 5, "context": "Furthermore we propose methods to align dimensions of word embeddings and an intrinsic metric \u03b6cca to measure the similarity of the learned word embeddings, which our preliminary results show may correlate with downstream tasks in terms of model agreement, with the popular word analogy task in Mikolov et al. (2013a) as an example.", "startOffset": 295, "endOffset": 318}], "year": 2016, "abstractText": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness \u2013 an important property to consider when developing new word embedding methods. 1", "creator": "TeX"}}}