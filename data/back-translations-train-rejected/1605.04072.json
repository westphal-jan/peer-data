{"id": "1605.04072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Towards Empathetic Human-Robot Interactions", "abstract": "Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them which can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the early stage, we described our approach using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can \"understand\" human emotion. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software based virtual android, with an animated cartoon character to present itself on the screen. She will get \"smarter\" and more empathetic through its deep learning algorithms, and by gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives.", "histories": [["v1", "Fri, 13 May 2016 07:31:50 GMT  (2190kb)", "http://arxiv.org/abs/1605.04072v1", "23 pages. Keynote at 17th International Conference on Intelligent Text Processing and Computational Linguistics. To appear in Lecture Notes in Computer Science"]], "COMMENTS": "23 pages. Keynote at 17th International Conference on Intelligent Text Processing and Computational Linguistics. To appear in Lecture Notes in Computer Science", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.RO", "authors": ["pascale fung", "dario bertero", "yan wan", "anik dey", "ricky ho yin chan", "farhad bin siddique", "yang yang", "chien-sheng wu", "ruixi lin"], "accepted": false, "id": "1605.04072"}, "pdf": {"name": "1605.04072.pdf", "metadata": {"source": "CRF", "title": "Towards Empathetic Human-Robot Interactions", "authors": ["Pascale Fung", "Dario Bertero", "Yan Wan", "Anik Dey", "Ricky Ho", "Yin Chan", "Farhad Bin Siddique", "Yang Yang", "Chien-Sheng Wu", "Ruixi Lin"], "emails": ["pascale@ece.ust.hk"], "sections": [{"heading": "1 Introduction", "text": "From movies to novels, people always have an emotional connection to virtual machines. Many people in society seem to think that the goal of creating intelligent machines is to imitate people or create a new kind of people. It's obvious that people have an intention to create machines to imitate them. It's far from being human, as they do, in order to gain power and dominance. It's obvious that we are able to create machines that imitate people."}, {"heading": "2 Architecture of An Empathetic Human-Robot Interactive Platform", "text": "To achieve human empathy with each other, we propose a platform that will consist of the following features and functionalities: 1. Embodiment of the system on a virtual robotic platform for human empathy; 2. Emotion and intention expression by the robot; 3. Face recognition of the user's ethnicity, age, etc.; 4. Speech recognition of what the user says, including humor; 5. Natural understanding of the user's intention and feeling, including humor; 6. Emotion recognition of facial expressions, audio and speech; research has shown that people prefer interacting with machines that are anthropomorphic. So the first step in designing interactive intelligent systems is to give it a humanistic form. In recent years, we have seen more efforts to give interactive systems a \"face.\" For example, the Microsoft Tay is a chatbot with a human face and personality. \"It is conceived as a teenage female character and shows the language model of a typical American teenage girl."}, {"heading": "3 Personality analysis", "text": "Empathy is the recognition and sharing of each other's emotions. To demonstrate machine empathy towards humans within a short interaction, we entrusted Zara with the task of personality analysis. We designed a set of personal questions in six different areas to classify the user personality from sixteen different personality types of the MBTI [24]. The original MBTI questionnaire contained about 70 questions about the user's preferences and feelings and would take about half an hour to complete. We asked a group of trainers to answer this questionnaire, but also to answer questions from Zara. The personality type generated by the MBTI questionnaire is used as the gold standard label for training the Zara system. Based on the user's answers to Zara questions, the values are calculated in four dimensions (i.e. introversion - extroversion, intuitive - sensing, thinking - feeling - feeling - feeling - results - judging language and judging - speech)."}, {"heading": "4 Handling user challenge", "text": "Personality testing consists largely of machine-initiated questions from Zara and human answers. However, as described in the User Analysis section below, there are scenarios where the user does not respond directly to questions from Zara. 24.62% of users who tried Zara showed some form of verbal challenge in their answers during the conversation, with 37.5% of users avoiding the questions with an irrelevant answer. 12.5% of users challenged Zara's ability to deal more directly with questions that have nothing to do with the personality test. A total of 184 responses were recorded between September and December 2015, 24.61% of the data showed some form of challenge during the conversation. Challenge refers to user responses that are difficult to handle and hinder the flow of conversation with Zara. These include the following 6 types: 1. Searching for reciprocity disclosure 2. Asking questions for clarification 3. Avoiding dealing with the avoidance of the topic."}, {"heading": "5 DNN, CNN and LSTM", "text": "In this area there is a general description of the deep neural network architecture, which we apply in the described manner. The real power of each DNN is to reduce a certain number of features in the individual areas. (...) The basic structure of a network is that it is a vector that acts as a vector. (...) It is the time in which a vector, an audio signal, or an image is capable. (...) The basic structure of a network takes as input one or more input vectors. (...) In the case of an utterance, every word is marked as a vector. (...) It is the time in which an audio signal is set. (...) It is the time in which a vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector signal is set. (...)"}, {"heading": "6 Speech Recognition", "text": "This year it is more than ever before."}, {"heading": "7 Real-Time Emotion Recognition from Time-Domain Raw", "text": "Audio Input In recent years, we have seen successful systems that had high classification accuracy on benchmark sets of emotional speech [21] or music genres and moods [34]. Most of this work consists of two main steps, namely feature extraction and classifier learning. A challenge for most emotion detection systems from speech is the time it takes to extract features from the speech file. So far, both high-level features and low-level features, as described in the following section, have been required for emotion and mood detection from audio. There are approximately 1000 features, a much larger set than the feature set used for speech detection, which must be extracted and calculated via windows of audio signals. This typically takes a few dozen seconds for each utterance, making the response time shorter than in real time, which users of interactive systems are accustomed to time-consuming feature engineering."}, {"heading": "7.1 Dataset", "text": "For our raw-tone emotion recognition experiments, we created a dataset from the TED-LIUM Corpus Release 2 [31], containing 207 hours of language extracted from 1495 TED conversations. We commented on the data using an existing commercial API, followed by manual correction. We received a total of 90041 segments, divided into the following 11 categories: creativity / passion, criticism / cynicism, defense / fear, friendly / warm, hostility / anger, leadership / charisma, loneliness / unfulfillment, love / happiness, sadness / grief, self-control / practicality, and arrogance. In our experiments, we use only the following 6 categories: criticism, fear, anger, loneliness, happiness, sadness. We achieved a total of 2389 segments in the Criticism / Unfulfillment category, 3855 in the Fear category, 12708 in the Loneliness category, 8070 in the Happiness category, and 1824 in the Sadness category."}, {"heading": "7.2 Real-Time Speech Emotion Recognition with CNN", "text": "The CNN model, where raw sound is used as input, is shown in Figure 4. The raw audio samples are initially sampled at 8kHz to optimize the sampling rate and efficiency of the display memory for longer segments. The CNN is run with a filter for real-time processing. We set a 200-millisecond folding window, which is equivalent to 25 ms, and an overlapping step size of 50, equal to about 6 ms. The folding layer performs feature extraction in each layer and models the variations between adjacent frames due to the overlapping. The network is trained using the standard feedback algorithm and performs a gradient descent over each parameter. At evaluation time, the time complexity over the length of the audio input segment due to the folding is linear. Thus, the largest time contribution is attributable to the calculations within the network [14], which can be performed with only one folding in negligible time for individual utterances."}, {"heading": "7.3 Experimental Setup", "text": "We set up our experiments as binary classification tasks, where each segment is classified either as part of a particular emotion category or not as part of that particular category.For each category, the negative samples are randomly selected from the music clips that do not belong to the positive genre.We implement our CNN with the Theano Framework [4]. Theano's automatic differentiation capabilities are used to implement the back propagation. Our models are trained with the Tesla K20 GPU on the CUDA platform.We choose the corrected linear function as non-linear for the hidden layers, as it generally provides better performance over other functions.We use standard back propagation training, with dynamics based on 0.9 and an initial learning rate.We used the validation set to determine the early stop condition when the error began to mount on it. We normalize the input data of each experiment with zero mean standard unit and we use the SVESM model as a base model of the SVESS."}, {"heading": "8 Sentiment Inference From Speech and Text", "text": "In the first version of Zara, sensitivity analysis was performed on the basis of lexical characteristics. We search for keywords from a pool of positive and negative emotion lexicons from the LIWC2 dictionary and use a N-gram model to classify mood. In the current approach, we use a Word2Vec based classifier. In this thesis, we train a layer of wrinkle formation and maximum pooling at the top of word vectors derived from a non-superimposed neural language model. We start with a sentence that we convert into a matrix."}, {"heading": "9 Humor Recognition in Conversations", "text": "In this area, we are able to learn a completely different language from the one we know. It is a method that can be used to recognize other sensations, and the way we move is also very helpful for the interaction between people. People have a sense of humor and a sense of what they are doing."}, {"heading": "Convolutional Neural Network", "text": "A simple way to model a dialogue is to retrieve language and acoustic characteristics from each utterance and to use a memory-based classifier (such as a Recurrent Neural Network or a Long Short-Term Memory) to model the discourse context. Previous work has shown that acoustic utterance-level features (such as the INTERSPEECH 2010 Paralinguistic Challenge Feature Set [36]) are very effective with simple classifiers such as logistic regression and conditional random fields, but suboptimal performance with DNN systems [5,13]. We apply two parallel neural networks to model each utterance from a lower level. The first CNN we use is dedicated to language characteristics. Word2Vec word vectors are used as token-level features."}, {"heading": "9.1 Experiments", "text": "We assembled a corpus of three popular TV sitcoms: \"The Big Bang Theory\" in seasons 6 to 9 and \"Seinfeld\" in seasons 5 to 9. We downloaded the subtitle files associated with each episode and script, and then selected the audio tracks for each episode for each character, assuming that each article is a mix."}, {"heading": "Seinfeld", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Friends", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "The Big Bang Theory", "text": "system is again the CRF described above, which has been trained and tested using the same cross-domain data. We evaluated context window lengths of size 3 and 5. The results of these experiments are shown in Table 6."}, {"heading": "10 Summary and Discussion", "text": "In this work, we have described the prototype of an empathetic virtual robot that can recognize the emotions of the user, thereby creating a new level of human-robot interaction. We have described the design of the architecture, the task of personality analysis, and the user analysis of Zara, the Supergirl. From then on, we have expanded our description to include further details on the recognition and inference of emotions and sensations from speech and speech. Zara also has a facial recognition component that we have not described in detail, as it functions as a complement to the linguistic part."}, {"heading": "Train: Big Bang + Friends Test: Seinfeld", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Train: Big Bang + Seinfeld Test: Friends", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Train: Seinfeld + Friends Test: Big Bang", "text": "We have shown how deep learning can be used for different modules in this architecture, from speech recognition to emotion recognition to humor recognition from dialogues. More importantly, by using a CNN with a filter, it is possible to get real-time performance in speech emotion recognition directly from time-dominated audio input, bypassing feature engineering. So far, we have only developed the primary tools that future emotionally intelligent robots would need. Empathetic robots, including Zara, which currently exist and will be in the near future, may not be perfect, but the most important step is to make robots more human as they are in their interactions, which means they will have mistakes, just like humans do. If done right, future machines and robots will be empathetic and cause less harm in their interactions with humans. They will be able to make us understand our emotions, and more than anything else, they will be our teachers and our friends."}], "references": [{"title": "Linguistic theories of humor", "author": ["S. Attardo"], "venue": "(Vol. 1). Walter de Gruyter", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "The semantic foundations of cognitive theories of humor.Humor-International", "author": ["S. Attardo"], "venue": "Journal of Humor Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "Y. Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy) (Vol", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep learning of audio and language features for humor prediction. in International Conference on Language Resources and Evaluation (LREC) (2016", "author": ["Bertero. D", "P. Fung"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Predicting humor response in dialogues from TV sitcoms", "author": ["Bertero. D", "P. Fung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "A long short-term memory framework for predicting humor in dialogues in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (2016", "author": ["Bertero. D", "P. Fung"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),2(3),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Intelligent robots: The question of embodiment", "author": ["B.R. Duffy", "G. Joue"], "venue": "In Proc. of the Brain-Machine Workshop", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In Proceedings of LREC (Vol", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Opensmile: the munich versatile and fast opensource audio feature extractor", "author": ["F. Eyben", "M. W\u00f6llmer", "B. Schuller"], "venue": "InProceedings of the 18th ACM international conference on Multimedia (pp. 1459-1462)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "In Interspeech (pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Conditional Neural Networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Classifying subject ratings of emotional speech using acoustic features", "author": ["J. Liscombe", "J. Venditti", "J.B. Hirschberg"], "venue": "Columbia University Academic Commons", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Using linguistic cues for the automatic recognition of personality in conversation and text", "author": ["F. Mairesse", "M.A. Walker", "M.R. Mehl", "R.K. Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "The role of embodiment in assistive interactive robotics for the elderly", "author": ["M.J. Mataric"], "venue": "In AAAI fall symposium on caring machines: AI for the elderly,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "CRFsuite: a fast implementation of conditional random fields (CRFs)", "author": ["N. Okazaki"], "venue": "URL http://www. chokkan. org/software/crfsuite", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Automatically assessing personality from speech", "author": ["T. Polzehl", "S. M\u00f6ller", "F. Metze"], "venue": "In Semantic Computing (ICSC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "J. Silovsky"], "venue": "In IEEE 2011 workshop on automatic speech recognition and understanding (No. EPFL-CONF-192584). IEEE Signal Processing Society", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "The theory and use of clarification requests in dialogue", "author": ["M. Purver"], "venue": "Unpublished doctoral dissertation, University of London", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": " sure, i did the right thing\": a system for sarcasm detection in speech", "author": ["R. Rakov", "A. Rosenberg"], "venue": "In INTERSPEECH (pp. 842-846)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Gensim\u2013Python Framework for Vector Space Mo delling", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "NLP Centre,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "A multidimensional approach for detecting irony in twitter", "author": ["A. Reyes", "P. Rosso", "T. Veale"], "venue": "Language resources and evaluation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Approach, avoidance, and coping with stress", "author": ["S. Roth", "L.J. Cohen"], "venue": "American psychologist,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "Enhancing the TED-LIUM Corpus with Selected Data for Language Modeling and More TED Talks", "author": ["A. Rousseau", "P. Del\u00e9glise", "Y. Est\u00e8ve"], "venue": "In LREC (pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A.R. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Automatic genre classification of music content: a survey", "author": ["N. 33. Scaringella", "G. Zoia", "Mlynek. D"], "venue": "Signal Processing Magazine,", "citeRegEx": "Scaringella et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Scaringella et al\\.", "year": 2006}, {"title": "Disentangling the effects of robot affect, embodiment, and autonomy on human team members in a mixed-initiative task", "author": ["P. Schermerhorn", "M. Scheutz"], "venue": "In Proceedings from the  International Conference on Advances in Computer-Human Interactions (pp. 236-241)", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "The INTERSPEECH 2009 emotion challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner"], "venue": "In INTERSPEECH (Vol", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "September). The INTERSPEECH 2010 paralinguistic challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "F. Burkhardt", "L. Devillers", "C.A. M\u00fcller", "S.S. Narayanan"], "venue": "In INTERSPEECH (Vol", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting.The", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Toward computational recognition of humorous intent", "author": ["J. Taylor", "L. Mazlack"], "venue": "In Proceedings of Cognitive Science Conference (pp. 2166-2171)", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "The role of physical embodiment in human-robot interaction", "author": ["J. Wainer", "D.J. Feil-Seifer", "D.A. Shell", "M.J. Matari\u0107"], "venue": "In Robot and Human Interactive Communication,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Effect of Non-linear Deep Architecture in Sequence Labeling", "author": ["M. Wang", "C.D. Manning"], "venue": "In IJCNLP (pp. 1285-1291)", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "The measurement of trust and its relationship to self-\u00ad\u2010 disclosure", "author": ["L.R. Wheeless", "J. Grotz"], "venue": "Human Communication Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1977}], "referenceMentions": [{"referenceID": 38, "context": "One reason behind this might be something that has been studied by human-robot interaction researchers [39].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "It is known that physical embodiment of an intelligent system, whether in virtual simulation or in a robotic form, is important for users to feel related and empathize with the system [34].", "startOffset": 184, "endOffset": 188}, {"referenceID": 21, "context": "Roboticists make great efforts to build robots in anthropomorphic form so that humans can empathize with them [22], and to have embodied cognition [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "Roboticists make great efforts to build robots in anthropomorphic form so that humans can empathize with them [22], and to have embodied cognition [10].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "Zara\u2019s current task is a conversational MBTI personality assessor and we designed 6 categories of personality-assessing questions, in order to assess the user\u2019s personality [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 23, "context": "We designed a set of personal questions in six different domains in order to classify user personality from among sixteen different MBTI personality types [24].", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "We use the output of the sentiment analysis from language and emotion recognition from speech as linguistic and speech cues to calculate the score for each personality dimension based on previous research [21].", "startOffset": 205, "endOffset": 209}, {"referenceID": 20, "context": "Summary of identified language cues for extraversion and various production levels [21]", "startOffset": 83, "endOffset": 87}, {"referenceID": 40, "context": "For instance, seeking disclosure reciprocity is not uncommon in human conversations [41].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Avoidance in psychology is viewed as a coping mechanism in response to stress, fear, discomfort, or anxiety [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "Asking for clarification examples included \u201cCan you repeat?\u201d and \u201cCan you say it again?\u201d Clarification questions observed in this dataset are primarily non- reprise questions as a request to repeat a previous utterance [26].", "startOffset": 219, "endOffset": 223}, {"referenceID": 39, "context": "low level features into a single low-dimensional feature vector, performing the appropriate feature-selection [40].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "The first model we use is the Convolutional Neural Network [9], which is useful to obtain a fixed-length vector representation of an utterance, an audio signal or an image.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "The Long Short Term Memory (LSTM) [15] is instead useful to model time sequences where it is important to remember the past context.", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "We train our acoustic models with the Kaldi speech recognition toolkit [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 2, "context": "One such model is called End-to-End Attention-based Large Vocabulary Speech Recognition [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 31, "context": "Other approach includes using Deep Convolutional Neural Nets instead of DNNs or GMMs [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "In recent years, we have seen successful systems that gave high classification accuracies on benchmark datasets of emotional speech [21] or music genres and moods [34].", "startOffset": 132, "endOffset": 136}, {"referenceID": 33, "context": "In recent years, we have seen successful systems that gave high classification accuracies on benchmark datasets of emotional speech [21] or music genres and moods [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 30, "context": "For our experiments on emotion recognition with raw audio, we built a dataset form the TED-LIUM corpus release 2 [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "time contribution is due to the computations inside the network [14], which with one", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "We implement our CNN with the Theano framework [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "As a baseline we use a linear-kernel SVM model from the LibSVM [8] library with the INTERSPEECH 2009 emotion feature set [35], extracted with openSMILE [12].", "startOffset": 63, "endOffset": 66}, {"referenceID": 34, "context": "As a baseline we use a linear-kernel SVM model from the LibSVM [8] library with the INTERSPEECH 2009 emotion feature set [35], extracted with openSMILE [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "As a baseline we use a linear-kernel SVM model from the LibSVM [8] library with the INTERSPEECH 2009 emotion feature set [35], extracted with openSMILE [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "g, the smooth methods, maximum and minimum value, mean value of the features from the frames [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification [16,17,18].", "startOffset": 151, "endOffset": 161}, {"referenceID": 16, "context": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification [16,17,18].", "startOffset": 151, "endOffset": 161}, {"referenceID": 17, "context": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification [16,17,18].", "startOffset": 151, "endOffset": 161}, {"referenceID": 0, "context": "Humor can be classified depending on the way they are intended to trigger laughter: common forms include irony, sarcasm, puns and nonsense [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "Spontaneous conversational humor typically follows a defined recurrent structure [2,38].", "startOffset": 81, "endOffset": 87}, {"referenceID": 37, "context": "Spontaneous conversational humor typically follows a defined recurrent structure [2,38].", "startOffset": 81, "endOffset": 87}, {"referenceID": 8, "context": "The first level is made of two Convolutional Neural Networks [9] to encode each individual utterance from word embedding vectors, and the audio track associated from a set of frame-level features.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "The language CNN is followed by a Long Short-Term Memory [15] to model the sequential context of the dialog.", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "Previous work showed that acoustic utterance-level features (such as the INTERSPEECH 2010 paralinguistic challenge feature set [36]) are quite effective with simple classifiers such as logistic regression and conditional random fields, but yield suboptimal performance with DNN systems [5,13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Previous work showed that acoustic utterance-level features (such as the INTERSPEECH 2010 paralinguistic challenge feature set [36]) are quite effective with simple classifiers such as logistic regression and conditional random fields, but yield suboptimal performance with DNN systems [5,13].", "startOffset": 286, "endOffset": 292}, {"referenceID": 12, "context": "Previous work showed that acoustic utterance-level features (such as the INTERSPEECH 2010 paralinguistic challenge feature set [36]) are quite effective with simple classifiers such as logistic regression and conditional random fields, but yield suboptimal performance with DNN systems [5,13].", "startOffset": 286, "endOffset": 292}, {"referenceID": 27, "context": "Our Word2Vec model is trained on the text9 Wikipedia corpus using gensim implementation [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "We then extract for each frame a vector of low level acoustic and prosodic features with openSMILE [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Both past attempts in the literature [13] and our experiments on our", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "Before the final softmax layer we incorporate some extra features not modeled by our neural network but which still add important contribution [6,7,27,29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 6, "context": "Before the final softmax layer we incorporate some extra features not modeled by our neural network but which still add important contribution [6,7,27,29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 26, "context": "Before the final softmax layer we incorporate some extra features not modeled by our neural network but which still add important contribution [6,7,27,29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 28, "context": "Before the final softmax layer we incorporate some extra features not modeled by our neural network but which still add important contribution [6,7,27,29].", "startOffset": 143, "endOffset": 154}, {"referenceID": 36, "context": "We applied a dropout regularization layer [37] after the output of the LSTM and the audio CNN, the dropout coefficient was set to 0.", "startOffset": 42, "endOffset": 46}, {"referenceID": 3, "context": "The neural network was implemented with the Theano toolkit [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 18, "context": "As a first baseline for comparison we chose a Conditional Random Field [19] trained over a set of features [6] including the same features added at the end of our neural network, bag-of-ngrams, part of speech proportion, sentiment from SentiWordNet [11], antonyms, and a prosodic feature vector from the INTERSPEECH 2010 paralinguistic challenge [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "As a first baseline for comparison we chose a Conditional Random Field [19] trained over a set of features [6] including the same features added at the end of our neural network, bag-of-ngrams, part of speech proportion, sentiment from SentiWordNet [11], antonyms, and a prosodic feature vector from the INTERSPEECH 2010 paralinguistic challenge [36].", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "As a first baseline for comparison we chose a Conditional Random Field [19] trained over a set of features [6] including the same features added at the end of our neural network, bag-of-ngrams, part of speech proportion, sentiment from SentiWordNet [11], antonyms, and a prosodic feature vector from the INTERSPEECH 2010 paralinguistic challenge [36].", "startOffset": 249, "endOffset": 253}, {"referenceID": 35, "context": "As a first baseline for comparison we chose a Conditional Random Field [19] trained over a set of features [6] including the same features added at the end of our neural network, bag-of-ngrams, part of speech proportion, sentiment from SentiWordNet [11], antonyms, and a prosodic feature vector from the INTERSPEECH 2010 paralinguistic challenge [36].", "startOffset": 346, "endOffset": 350}, {"referenceID": 22, "context": "We used the implementation from CRFSuite [23], with L2 regularization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "In Table 5 we also compare the results of our best system on the Big Bang corpus with the system proposed in [7], where a LSTM is applied to a larger set of languageonly features, which includes one-hot word vectors and character-trigram input vectors in addition to Word2Vec.", "startOffset": 109, "endOffset": 112}], "year": 2016, "abstractText": "Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them, which is a software system that can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the primary stage, current methods involve using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can \u2019understand\u2019 human emotion. Other aspects of human-robot interaction include facial expression and gesture recognition, as well as robot movement to convey emotion and intent. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software-based virtual android, with an animated cartoon character to present itself on the screen. She will get \u2019smarter\u2019 and more empathetic, by having machine learning algorithms, and gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives.", "creator": "Word"}}}