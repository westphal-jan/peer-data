{"id": "1605.04232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Review of state-of-the-arts in artificial intelligence with application to AI safety problem", "abstract": "Here, I review current state-of-the-arts in many areas of AI to estimate when it's reasonable to expect human level AI development. Predictions of prominent AI researchers vary broadly from very pessimistic predictions of Andrew Ng to much more moderate predictions of Geoffrey Hinton and optimistic predictions of Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and this broad range of predictions of AI experts, AI safety questions are also discussed.", "histories": [["v1", "Wed, 11 May 2016 17:49:24 GMT  (93kb,D)", "http://arxiv.org/abs/1605.04232v1", null], ["v2", "Tue, 6 Dec 2016 09:29:12 GMT  (93kb,D)", "http://arxiv.org/abs/1605.04232v2", "version 2 includes grant information"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vladimir shakirov"], "accepted": false, "id": "1605.04232"}, "pdf": {"name": "1605.04232.pdf", "metadata": {"source": "CRF", "title": "Review of state-of-the-arts in artificial intelligence with application to AI safety problem", "authors": ["Vladimir Shakirov"], "emails": ["schw90@mail.ru"], "sections": [{"heading": "1 Introduction", "text": "It has been argued that AI at the human level is not automatically good for humanity. It may be presumptuous and too self-confident to be sure that humans would be able to control superhuman-smart AI, that these AI would really care about humans, for example, to give us full access to the planet's mineral resources and agricultural fields. While there are numerous benefits to having smart AI in the short term, the long-term risk of having too smart AI could outweigh the long-term risk, which would lead to a negative net effect of AI progress on society. Certainly, the most common argument against considering such long-term risks is their vagueness due to the very long time span of us [3].The main purpose of my article is to show that superhuman level AI is reasonably possible even within the next 5 to 10 years. Certainly, it is very difficult to predict the future of science."}, {"heading": "2 Natural language processing", "text": "It is about the question of whether this could be a conspiracy theory, which has developed in recent years in the way in which people in the United States have behaved. (...) It is about the question of how far they see themselves able to assert themselves in the world. (...) It is about the question of how far the world has changed in the world. (...) It is about the question of whether the world in the world of the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the, in the world in the world in the, in the world in the, in the world in the, in the world in the world in the world in the world in the, in the world in the world, in the world in the, in the, in the world in the world in the world, in the world in the world in the, in the, in the world in the,"}, {"heading": "3 Computer vision", "text": "\"Identity mappings in deep residual networks\" [41] reach 5.3% top-5 error in single one-crop model while human level is reported to be 5.1% [42]. In \"deep residual networks\" [48] one-crop single model gives 6.7% but ensemble of those models with Inception gives 3.08% [66] just to see that 6.7: 3.08 = 5.3: 2.4. Another big improvement comes from \"deep networks with stochastic depth\" [115]. It has been reported that 0.3% errors in human annotations to ImageNet [42], so that real errors on ImageNet would soon even be below 2%. Human level is not only overcome in ImageNet classification task (see also an efficient implementation of 21841 classes ImageNet classification [43]), but also in boundary detection [44]. Video classification task on SPORTS-1M dataset (487 classes, 1M videos) performance improved from 63.9% [36] (Gto 1200.1) people."}, {"heading": "4 Speech recognition and generation", "text": "Here, AI has not yet surpassed the human level, but it is clear that we have every chance of seeing it in 2016, and the rate of improvement is very fast. Google reports that the error rate for words has dropped from 23% in 2013 to 8% in 2015 [67]."}, {"heading": "5 Reinforcement learning", "text": "AlphaGo and Atari agents select an action from a finite number of possible actions. There are also articles about continuous reinforcement learning, where action is a vector [69] [70] [71] [72] [73] [74] [76] [77].AlphaGo is a powerful AGI in its own very simple world, which is just a board of stones and a bunch of simple rules. If we improve AlphaGo through continuous reinforcement learning (and if we manage to make it work well in hard tasks in the real world), then it would be a real AGI in a real world. In addition, we can start pre-training on virtual video games [93]. Video games often contain much more interesting challenges than real life offers to the average person. Millions of available books and videos contain the concentrated life experience of millions of people. While AlphaGo is a successful example of combining modern RL with modern CNN, RL can be combined with neural chat bots and Rationskaters [94]."}, {"heading": "7 Is embodiment critical?", "text": "People with Tetra-Amelia syndrome [15] have not had hands or legs since birth, and they still manage to attain good intelligence. Thus, Hirotada Ototake [80] is a Japanese sports writer famous for his best-selling memoirs. He also worked as a teacher. Nick Vujicic [81] has written many books, holds a Bachelor of Commerce degree from Griffith University, and often reads motivational lectures. Prince Randian [82] spoke Hindi, English, French, and German. Modern robots have a better embodiment. There are fast and good drones, there are quite impressive results in robot detection [17]. It is argued that embodiment could take place in the virtual world of video games [93]. Among 8- to 18-year-olds, the average time spent on TV / computer / video games, etc., is 7.5 hours per day [16]. According to another study [59] British adults spend an average of 8 hours per day sleeping, and we can barely develop a healthy 8 minutes per day with a media device."}, {"heading": "9 Argument from neuroscience", "text": "This is a simplified view of the human cortex: roughly speaking [18] [19], 15% of the human brain dedicates itself to low-level tasks (occipital lobe), another 15% to image and action recognition (slightly more than half of the temporal lobe), another 15% to object recognition and tracking (parietal lobe), another 15% to speech recognition and pronunciation (BAs 41,42,22,39,44, parts of 6,4,21), another 10% to amplification learning (orbitofrontal cortex and part of the medial prefrontal cortex) and together account for about 70% of the human brain. Modern neural networks work for this 70% of the human brain at about the human level. CNNs, for example, make 1.5 x fewer mistakes than people in ImageNet, while acting about 1,000 times faster than people."}, {"heading": "9.1 Remaining 30% of human brain cortex", "text": "It is only a matter of time before it happens, \"he told the German Press Agency."}, {"heading": "11 So what separates us from human-level AI?", "text": "In fact, most of them are people who are able to survive themselves, \"he told the Deutsche Presse-Agentur in an interview with\" Welt am Sonntag \":\" I don't think they are able to survive themselves, and that they are able to survive themselves. \""}, {"heading": "12 predictions for human-level AI", "text": "Andrew Ng makes very sceptical predictions: \"Maybe technology will get to the point in hundreds of years where there might be a chance of evil killer robots.\" [32] Geoffrey Hinton makes a moderate prediction: \"I refuse to say anything beyond five years because I don't think we'll see much in five years.\" [31] Shane Legg, co-founder of DeepMind, used to make predictions about AGI at the end of each year, here's the last [65]: \"I give it a log-normal distribution with an average of 2028 and a mode of 2025, assuming nothing crazy like a nuclear war happens. I'd also like to add to this prediction that I expect to see an impressive proto-AGI forecast within the next eight years.\" Figure 2 shows the predicted log-normal distribution. This prediction is very interesting after a big advance at the end of 2011, that I don't expect it to be very likely in the next 14 years."}, {"heading": "13 Would AI be dangerous? Optimistic arguments:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "13.1 the promising approach to solve/alleviate AI safety problem", "text": "This approach, however, does not solve all AI security problems [160]. Apart from this, we can use inverse reinforcement learning, but would still need the data set of goodwill / malevolence of ethical problems to test artificial intelligence. This approach was originally formulated in \"The maverick nanny with a dopamine drip\" [157], but is probably better formulated here [158] [159] [160]."}, {"heading": "13.2 some drawbacks of the approach:", "text": "It's not that we see ourselves able to do what we have to do to save the world, \"he said.\" But it's not that we have to do it as if we were doing it. \"(\" It's not as if we were doing it. \")\" It's not as if we were doing it. \"(\" It's not as if we were doing it. \")\" It's not as if we were doing it. \"(\" It's as if we were doing it. \")\" It's not as if we were doing it. \"(\" It's as if we were doing it. \")\" It's not as if we were doing it. \"(\" It's as if we were doing it. \")"}, {"heading": "13.3 probable solution of aforementioned drawbacks:", "text": "I very much doubt that any substantial part of people would want AI to increase their power too much, or explicitly give us some kind of singularity. Most likely, people want some scientific research on AI: a cure for cancer, cold thermonuclear synthesis, AI security, etc. The dataset would certainly include many examples where AI is taught to consult with people about any serious measures AI could follow, and to immediately share with people any findings that emerge in AI, etc. (MIRI / FHI have hundreds of good papers that could be used to create examples of such a dataset [29]. This alleviates or solves almost all of the above problems, because it does not assume that AI will take a hard start. It could be argued that this exacerbates the problem of the evil / psychopath / ignorant person who first introduces insecure AI."}, {"heading": "14 Would AI be dangerous? Pessimistic arguments:", "text": "To my knowledge, the majority of MIRI research on this subject [29] [28] comes to cautionary conclusions. Regardless of the architecture chosen to build AGI, it would most likely destroy humanity in the near future. It is very important to note that arguments are almost entirely independent of the architecture of AGI. The best simple introduction is [1] (or more popular [107] and [108]), although for a serious understanding of MIRI arguments \"super intelligence\" [29] and \"AI Foom Debate\" [2] is urgently needed. At this point, I offer my own understanding and review of MIRI arguments that differ from their official views but are strongly based on them:"}, {"heading": "14.1 it\u2019s very profitable to give your AI maximal abilities and permissions", "text": "These companies would win their markets, giving their AI direct unrestricted Internet access, promoting their products, gathering user feedback, making a positive impression of the company and a negative impression of the competition, researching user behaviour, etc. Those companies would win, using their AI to invent quantum computing (AI is already used by quantum computers to select experiments [150] [143]), enabling AI to improve its own algorithms (including quantum implementation) and even thermonuclear synthesis, asteroid mining, etc. All the arguments in this paragraph also apply to countries and their defence departments."}, {"heading": "14.2 human-level AI might quickly become very superhuman-level AI", "text": "Andrew Karpathy: \"I think AI at the chimpanzee level is equally scary, because the transition from chimpanzee to human would have removed nature just a blink of an eye from the evolutionary timescale, and I suspect that could be the case in our own work. Similarly, my sense is that once we reach that level, it will be easy to go beyond it and get to super intelligence.\" [25] It takes years or decades for AI to teach humans our knowledge to other people, while AI can almost instantaneously create complete working copies of itself by simply copying it. It has a unique memory. A student forgets all that stuff very quickly after exams. AI simply saves its configuration before exams and recharges it when it is needed to perform similar tasks. Modern CNNs not only recognize images better than humans, but also make them several orders of magnitude faster. The same goes for LM's ability to easily translate all the science literature at the same time, given the thousands of translations of psychology, etc."}, {"heading": "14.3 fatal but profitable error: direct internet access", "text": "s artificial intelligence, he would be able to hack millions of computers and run his own copies or subagents like [26] on them. Afterwards, he could earn billions of dollars on the Internet. He could anonymously hire thousands of people to manufacture or buy skillful robots, 3D printers, biological laboratories, and even a space rocket."}, {"heading": "14.4 Superhuman-level AI has ability to get ultimate power over the Earth", "text": "There is a simple but effective basic solution. AI could produce a combination of deadly viruses and bacteria or some other weapon of mass destruction to kill every human being on Earth. You can't guarantee efficient control over something much smarter than you. After all, several humans have almost taken over the world, so why can't superhuman AI?"}, {"heading": "14.5 After that, it would destroy humanity, likely as a side-effect", "text": "What would AI do to us if it had full power on Earth? If it were indifferent to us, it would eliminate us as a side effect. It's exactly what indifference means when you're dealing with an incredibly powerful creature that solves its own problems by harnessing the power of the local Dyson sphere. However, if it's not indifferent to us, then everything could be worse. If it likes us, it could decide to insert electrodes into our brains that give us the greatest pleasure, but no motivation to do something. It would be the exact opposite if it doesn't like us, or if it's partially hard coded to like us, but that hard coding contained a mistake that is hard to catch. Mistakes are almost inevitable when they try to partially hardcode something that is many orders of magnitude smarter and more powerful than you. There's nothing but vague intuition behind the thought that the superhuman level AI would probably take care of us."}, {"heading": "14.6 certain people would help AI in initial parts of it\u2019s path", "text": "Consider the quote from the interview with the OpenAI project: [145] Elon Musk: \"I think the best defense against the misuse of AI is to empower as many people as possible to have AI. If everyone has AI skills, then there is not one person or a small group of individuals who can have AI superpowers.\" Sam Altman: \"I expect [OpenAI] will create super-intelligent AI, but it will simply be open source and can be used by everyone <... > Everything the group develops will be available to everyone.\" Personally, I know several AI researchers who are deeply convinced that AGI would necessarily be good because their intelligence (which is not true; see [146]) and that AGI is our only hope to escape all other existential risks such as bioterrorism."}, {"heading": "14.7 other probable negative consequences [27]", "text": "AI-enhanced computer viruses could penetrate almost anything that is computerized, i.e. almost anything. Drones could be programmed to kill thousands of laymen within seconds. As the intelligence of AI systems improves, virtually all crimes could be automated. And what about super-clever advertising chatbots that try to impose their political opinions on you? An AI that is properly designed and implemented by ISIS to enforce Sharia law can be considered malicious in the West, and vice versa."}, {"heading": "15 Conclusion", "text": "I am aware of contrary opinions, but as far as I know they are not explicitly based on a thorough analysis of current trends in deep learning, and they are implicitly based on the experience of some prominent AI scientists who hold such views. However, there are prominent AI scientists who claim that AI is very likely on a human level in the next 5 to 10 years, so it is a good idea to argue on the basis of numbers, not just on the basis of expert opinions. I would like to see some work that is similar to mine but comes to different conclusions. As for the problem of AI friendliness, there is a good approach to solving it on the basis of goodwill / malevolence. However, it is still only a raw solution that has difficult problems to implement and does not have a strong safety record."}, {"heading": "Acknowledgements", "text": "To Elieser Yudkowsky, Nick Bostrom, Roman Yampolsky, Alexey Turchin for their inspiring articles and books. Similar works were written by Katja Grace [4] in 2013."}, {"heading": "16 Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "16.1 Estimation of neural network age", "text": "The results are: age 3-4, 15.3%; age 5-8, 39.7%; age 9-12, 28.4%; age 13-17, 11.2%; age 18 +, 5.5%. The sum corresponds to 100%, however, 18 + people answer 83.3% correctly. We could estimate that the 8-year model for age 8 must be comprehensible 15.3% + 39.7% = 55% questions with 83.3% accuracy and other 45% with accuracy corresponding to the base model. If you select the most popular answer for each question type, you get 36.18%. It is reasonable to estimate the age of this base model as 0 years, because it is not real knowledge, but depends on this unique data set statistic. * However, there are two base models in article, with 36.18% and 40.61% accuracy."}, {"heading": "16.2 Notes on \u201done billion world benchmark\u201d corpus", "text": "Interestingly, the popular \"one billion word benchmark\" corpus contains mixed sentences, so that NLM trained on this corpus could not be hierarchical and understand the flow of thought. That's why it's so interesting to see examples generated from a recent context-dependent LSTM article in which the authors get 27 perplexity for Wikipedia dump. Unfortunately, the article contains no such examples. \"A neural conversation model\" [22] contains examples from NN with pplx = 17 on OpenSubtitles, but this data set is slightly worse than Wikipedia in terms of consistency and logic of thought flow."}, {"heading": "16.3 Is AlphaGo AGI?", "text": "Yann LeCun writes of Richard S. Sutton's commentary on AlphaGo [141]: \"Reich says,\" AlphaGo lacks a key component: the ability to learn how the world works, such as understanding the laws of physics and the consequences of its actions. \"I fully agree with this. Reich has long argued that the ability to predict is an essential component of intelligence. Predictive (unsupervised) learning is one of the things that some of us see as the next obstacle to better artificial intelligence. We are actively working on it. But in its own simple world, AlphaGo is able to predict the future, is able to\" learn how the world works, how to understand the laws of physics and the consequences of its actions. \"Predictive learning is getting better very quickly. 16.4 Can we have another\" artificial winter \"? There were two great artificial intelligence winters 1974 \u2212 80 and 1987 \u2212 93. [60] During the first artificial intelligence winter of 1974-1980, a desktop M1 computer would be the most powerful."}, {"heading": "16.5 My own prediction for human-level AGI", "text": "My own prediction for AGI on a human level is the normal distribution with \"mean = end of 2017, sigma = 1 year,\" if there were no major restrictions on AI research, etc., although I really want someone to give me a dozen excellent arguments as to why I'm wrong. I've heard nowhere near any reasonable, well-structured arguments to prove that human extinction due to AGI is extremely unlikely over the next 10 years (say less than 10% with 90% confidence). Arguments like \"stop, otherwise we could go into another AI winter\" or \"Oh, we've seen this in Hollywood so that can't happen\" do not count for obvious reasons. In my experience, a discussion about the safety of AI with people who don't read [29] is like a discussion about deep learning with people who don't know the basics of backpropagation. It's always boring and misleading."}, {"heading": "16.6 Is it too late already? What about cyborgization?", "text": "It can reasonably be assumed that measures designed to protect humans from the risks of powerful artificial intelligence cannot be implemented immediately, but may take many years. It may even turn out that it is already too late to effectively control the security of AI research and guarantee that the race of superhuman-intelligent artificial intelligence will not defeat humans in the next 10 to 20 years. Some talk of cyborgization as a possible solution to this problem, but do we humans really want to become cyborgs? Given the slowness of the human brain, are cyborgs really capable of competing with artificial intelligence?"}, {"heading": "16.7 Invitation for cooperation", "text": "I'm really interested in discussions, so just send me an email or contact me at http: / / facebook.com / sergej.shegurin (\"Sergei Shegurin\" is my nickname I've been choosing for a long time on the Internet) Here's my list of the best AI articles in chronological order: http: / / goo.gl / 7hJjHu"}], "references": [{"title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Saturated Reconstruction of a Volume of Neocortex", "author": ["Narayanan Kasthuri", "Kenneth Jeffrey Hayworth", "Daniel Raimund Berger", "Carey Eldin Priebe", "Hanspeter Pfister", "Jeff William Lichtman"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2015}, {"title": "The Maverick Nanny with a Dopamine Drip: Debunking Fallacies in the Theory of AI Motivation", "author": ["Richard Loosemore"], "venue": null, "citeRegEx": "157", "shortCiteRegEx": "157", "year": 2014}, {"title": "Defining Benevolence in the context of Safe AI", "author": ["Richard Loosemore"], "venue": "http://ieet.org/index", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Modern neural nets translate text \u223c1000 times faster than humans do [58].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Detailed connectome deciphering has begun to really succeed recently with creation of multi-beam scanning electron microscopy [83] (Jan 2015), which allowed labs to get a grant for deciphering the detailed connectome of 1x1x1 mm of rat cortex [85] after the proof-of-principle deciphering of 40x40x50 mcm of brain cortex with 3x3x30 nm resolution was achieved [84](July 2015).", "startOffset": 360, "endOffset": 364}, {"referenceID": 2, "context": "This approach has been initially formulated in \u201dThe maverick nanny with a dopamine drip\u201d [157] but it\u2019s arguably better formulated here [158] [159] [160].", "startOffset": 89, "endOffset": 94}, {"referenceID": 3, "context": "This approach has been initially formulated in \u201dThe maverick nanny with a dopamine drip\u201d [157] but it\u2019s arguably better formulated here [158] [159] [160].", "startOffset": 136, "endOffset": 141}], "year": 2016, "abstractText": "Here, I review current state-of-the-arts in many areas of AI to estimate when it\u2019s reasonable to expect human level AI development. Predictions of prominent AI researchers vary broadly from very pessimistic predictions of Andrew Ng to much more moderate predictions of Geoffrey Hinton and optimistic predictions of Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and this broad range of predictions of AI experts, AI safety questions are also discussed.", "creator": "LaTeX with hyperref package"}}}