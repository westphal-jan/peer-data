{"id": "1605.04359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2016", "title": "Occurrence Statistics of Entities, Relations and Types on the Web", "abstract": "The problem of collecting reliable estimates of occurrence of entities on the open web forms the premise for this report. The models learned for tagging entities cannot be expected to perform well when deployed on the web. This is owing to the severe mismatch in the distributions of such entities on the web and in the relatively diminutive training data. In this report, we build up the case for maximum mean discrepancy for estimation of occurrence statistics of entities on the web, taking a review of named entity disambiguation techniques and related concepts along the way.", "histories": [["v1", "Sat, 14 May 2016 01:13:48 GMT  (288kb,D)", "http://arxiv.org/abs/1605.04359v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aman madaan", "sunita sarawagi"], "accepted": false, "id": "1605.04359"}, "pdf": {"name": "1605.04359.pdf", "metadata": {"source": "CRF", "title": "Occurrence Statistics of Entities, Relations and Types on the Web", "authors": ["Aman Madaan", "Sunita Sarawagi"], "emails": [], "sections": [{"heading": null, "text": "The problem of collecting reliable estimates of entity occurrence on the open web is the premise of this report. Entity identification models cannot be expected to perform well when used on the web, due to the large discrepancy between the distributions of such entities on the web and the relatively low level of training data. In this report, we build the arguments for a maximum mean discrepancy in estimating entity occurrence statistics on the web, while at the same time reviewing entity naming techniques and related concepts."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Problem Statement", "text": "The Internet is a web of largely unstructured knowledge interwoven around things, but these things; people, places, technologies, films, products, books, etc. are mostly mentioned only by their names, with other crucial information scattered about them. The cosmic scale of such unstructured information has held up the dream of a semantic web. A web that is aware of meaningful linkages, understands what the user is looking for, and has the intelligence to locate the desideratum. There are several pieces of the puzzle in the semantic web, this report is an attempt to understand an important piece; entities in the net and their coexistence statistics. In the face of a knowledge base like Yago or Freebase, which consists of entities and relationships, and the web, our goal is to add reliable estimates of the frequency of events in the web of different entities and relationships as singletons, pairs (ordered and disordered) in a sentence."}, {"heading": "1.2 Named Entity Recognition and Disambiguation", "text": "To collect statistics on entities on the Web, we need a method to determine which words in the free-flowing, endless text are of interest, i.e. representing entities. Consider the following sentence: Michael Jordan is a professor at Berkeley University. First, we want to identify all the named entities in the text. The task is called entity recognition and is formally defined as: Definition 1 (entity detectory1) Named-Entity Recognition (NER) (also known as Entification and Entity Extraction) is a subtask of information extraction that aims to locate and classify atomic elements in the text into predefined categories such as names of persons, organizations, locations, time expressions, quantities, monetary values, percentages, etc., but we do not want to stop there. But we want to link each of the named entities identified as such with a knowledge base."}, {"heading": "1.2.1 Applications", "text": "Simply put, the unique naming of entities in the unstructured text gives the document a structure. We need two more data points to further estimate the power that such a tool gives us. The first is the size of the web. As of March 31, 2014, there are at least 1.8 billion indexed web pages. [7] The second is the number of Wikipedia entities. Wikipedia statistics [8] estimate the number of pages at about 32 million. 2The knowledge base is a catalog of entities such as Wikipedia. See section [2] Yago, a catalog of entities from Wikipedia has 12,727, 222 entities. Structuring documents of this size has far-reaching implications for information extraction and is a bridge to the previous dream of a semantic web site. It is strongly recommended that the reader pays http: / / www.goo.co.in / insidesearch / features / search / knowledge.html."}, {"heading": "1.2.2 Terminology", "text": "The following terms are widely used in the literature on named entities and thus in this article. \u2022 Mention, Spot A piece of text that must be unique. For example, the sentence \"Amazon has attracted a lot of visitors.\" \u2022 Entity A named entity within the meaning of the definition 1. \u2022 Candidates A set of entities that could be the right definition for a specific mention. For example, possible candidates for the above sentence are \"Amazon River\" and \"Amazon.com.\" \u2022 Previous probability of a mention being associated with a particular entity. For example, mentioning \"Amazon\" can be used to refer to the website (say) 60% of the time. \u2022 Knowledge Base A catalog of entities in which an entity is defined as above. For example, Wikipedia or yago."}, {"heading": "1.3 A Baseline : Label and Collect", "text": "The baseline that presents itself in the face of the problem described above is to label the corpora with the designated units and then collect the markers to track which unit was seen on the way. As intuitive as it seems, the method is unlikely to succeed in the current scenario due to the discrepancy in training and test distribution. [20] Our training data, hand-labeled corpora, is meagre compared to the massive open network in which such systems are to be used, even for large training records like Wikipedia."}, {"heading": "1.4 Maximum mean discrepancy", "text": "The observation that we do not really want the individual labels is a first step towards a better solution. There are three well-known methods for directly estimating the class ratio [20]. We are interested in using one of them, the maximum mean discrepancy (mmd), to solve the problem in our hand. We introduce mmd and propose a formulation for determining the class ratio in section 5."}, {"heading": "1.5 Structure", "text": "Section 2 provides an overview of what knowledge databases are, which is important because the concept of such repositories of structured knowledge is at the heart of the report.Section 3 begins with an introduction to the problem of disambiguating named entities, terminology and applications, and goes on to detail the techniques for disambiguating named entities. We give an overview of the two broad categories of disambiguation techniques, local and global disambiguation.Section 4 begins with a discussion of the definition of aggregate statistics and some of their applications. Finally, Section 5 discusses the maximum mean discrepancy and their application to estimate aggregate entity statistics."}, {"heading": "2 Structured Knowledge", "text": "Locations"}, {"heading": "2.1 What are knowledge bases?", "text": "Before the digital age, encyclopedias such as Encyclopedia Britannica were touted as repositories containing everything known to mankind. As the computer age dawned, it didn't take long for people to realize that much could be achieved if all this information could somehow be made available in a digital format. WordPress [15] was perhaps the first such attempt. Over the years, the explosion of the Internet gave a huge pat on the back to research efforts in the area of information extraction and structured knowledge creation. Wikipedia catalyzed the community that developed structured knowledge databases such as dbpedia and yago.We discuss how knowledge databases fit into the context of so-called entity disambiguation, and provide a list of several important knowledge databases, along with links to each for the interested reader."}, {"heading": "2.2 Knowledge bases and Named Entity Disambiguation", "text": "Many entity disambiguation algorithms exploit large knowledge bases. On the other hand, reliable entity disambiguators will help to fabricate gigantic knowledge bases from the open web. So we are seeing a chicken-and-egg situation here. As is so often the case in such standoffs, the cycle is broken with the help of extensive manual efforts. In this case, Wikipedia helps the situation."}, {"heading": "2.3 Existing Knowledge Bases", "text": "We give a brief overview of some of the popular knowledge bases."}, {"heading": "2.3.1 Wordnet", "text": "\u2022 WORDS has a clean, handcrafted type hierarchy. Well-documented APIs such as the nltk toolkit (http: / / www.nltk.org / howto / wordnet.html) are available to use wordnet for a variety of tasks, such as listing all the senses of a word, searching for distances between two concepts, and the like. \u2022 Introduction to WORDS http: / / wordnetcod. princeton.edu / 5papers.pdf"}, {"heading": "2.3.2 YAGO", "text": "\u2022 Attempting to create a knowledge base that combines the clean hierarchy of the word network with the huge information provided by Wikipedia. http: / / www.mpi-inf. mpg.de / yago-naga / yago / has a link to an online interface. See [16] for details."}, {"heading": "2.3.3 DBpedia", "text": "\u2022 DBpedia http: / / dbpedia.org / About extracts information from Wikipedia into RDF and provides an interface through which semantic questions can be asked. Users can use SPARQL to make complicated queries whose results span several pages. Amazon also provides a DBpedia machine image for users of AWS.2.3.4 Patty \u2022 Patty http: / / www.mpi-inf.mpg.de / yago-naga / patty /. The goal is to create \"WORDS\" for relationships. Authors also create a submission hierarchy for the 350, 569 pattern synsets. See [18] for details."}, {"heading": "2.3.5 Freebase", "text": "\u2022 Freebase [19] relies on crowdsourcing to create a rich but clean knowledge base. The development of Freebase follows the same chain as Wikipedia, where users report problems and clean and expand information. Freebase also provides access to itself via web APIs."}, {"heading": "3 Named Entity Disambigua-", "text": "We have already given an introduction to the problem and its applications in the introduction. In the next section, the solutions are discussed on the basis of local disambiguations, i.e. finding the right entity on the basis of local evidence. Section 3.2 deals with the intuition behind a global strategy for disambiguation and the optimization problem that results from such a goal. The last section summarizes a current work that pragmatically selects global and local evidence to get the best out of both worlds."}, {"heading": "3.1 Local Disambiguation of named entities", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Introduction", "text": "In local disambiguization, we collect only local evidence of their disambiguization for each mention. This was state of the art until the CSAW [1] paper appeared. We begin by defining the problem and discussing the general form of solutions, and then provide a brief summary of the approach followed in Wikify [9] and the famous Milne-and-Witten paper [6]. A solution based on machine learning [1] closes the subsection."}, {"heading": "3.1.2 Problem definition", "text": "The evidence can be anything, POS tags, gender information, dictionary searches, etc. By local clarification, we mean that we cannot use the clarifications for other places to solve the problem."}, {"heading": "3.1.3 Solutions", "text": "Any local disambiguity technology falls into one of the following two categories [9] \u2022 Knowledge derived from classical literature on the meaning of disambiguity depends on the information derived from the definitions of the Knowledge Base. (See Reading's algorithm [14]) This is based on the overlap of context with the definitions of the individual senses of the candidates in the Knowledge Base. \u2022 Machine Learning is based on gathering features from the mention and its environment and training a classifier to make a judgment on whether a particular sense is a likely disambiguity of a mention. \u2022 Machine Learning based on local disambiguity has been adopted almost unanimously by the ned community as a solution to local disambiguation. AIDA changed the scene by introducing a knowledge score based on local similarity that works well."}, {"heading": "3.1.4 Related Work", "text": "Wikify [9] Perhaps the greatest contribution of this essay is to present Wikipedia as the catalog against which the similarity is said to be ambiguous. The essay also identifies two comprehensive methods for linking named entities: knowledge-based and data-based. Since the essay dates back to 2007, when the problem of NED was not yet so well established, there are a lot of references to the problem of word ambiguity. In this essay, three different characteristics of ambiguity are defined: \u2022 generality: This is the one previously defined in Chapter 1. \u2022 relativization: Perhaps the largest contribution of this essay, the relativism score, provides a yardstick for determining how similar the two entities are. This yardstick is based on the number of common links to the entities in question. The relativity measurement as defined here has been used in many works. In fact, all approaches presented in the following sections use the Relatism Entity for determining the weather."}, {"heading": "3.1.5 Machine learning based local disambiguation", "text": "As already mentioned, there are primarily two approaches to local disambiguation. This subsection describes a machine learning method based on local disambiguations in some details. This subsection is based on the local disambiguation approach taken in [1]. Definitions We will first repeat the definitions for quick reference: \u2022 s: Spot, a unit to be disambiguated (Christian leader John Paul) \u2022 \u03b3: An entity label value (http: / / en. wikipedia.org / wiki / Po-pe _ John _ Paul _ II) \u2022 fs (\u03b3): A feature function that generates a vector of features that identify a spot and a candidate. Local compatibility: Feature design The feature function takes the spot and the candidate as arguments. \u2022 The following information about a candidate is usable - text from the first descriptive paragraph of an article in which we use a vector of text from the full text of the class text for the placement spot within Wikipedia Spot and Spot text."}, {"heading": "3.2 Collective Disambiguation of Named Entities", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 The key intuition", "text": "Milne and Witten [6] tried to solve the problem by gathering evidence around a mention and then using it for ambiguity. After a wait of 2 years, CSAW [1] took the game to a whole new level by working on the following key intuition: \u2022 A document is usually about a topic \u2022 Deciphering each unit on the basis of local clues ignores an important piece of information: topic of a page \u2022 A page usually has a topic, one can expect that all units are related to the topic of a Michael Jackson: 30 ambiguities John Paul: 10 ambiguities But if they are mentioned on the same page, it is most likely Christianity, a great indication of the ambiguity of both."}, {"heading": "3.2.2 Challenges", "text": "Although the notion of local coherence is very natural and intuitive, there are a lot of challenges when it comes to actually mapping these intuitions to an optimization problem. We present the related challenges and the solution provided by the CSAW team. \u2022 Capturing local compatibility - creating a scoring function to classify potential candidates \u2022 Introducing Current coherence into the overall goal - Defining Topical coherence. In Chapter 2, different solutions to the problem of collectively disambiguating are presented. In this subsection, we will focus on the problem of collective disambiguity."}, {"heading": "3.2.3 The Dominant Topic Model", "text": "\u2022 Need to define a collective score based on the paired topical coherence of all communications used for labeling. Data: A Document d Result: annotated document d 'with each mention associated with the best candidate, forty mentions m in document docalculate argmaxcm \u00d1 w T fm (cm), where A = cm: cm is a possible disambiguation of repair algorithm 1: local disambiguation. \u2022 The paired topical coherence r (N, N) is defined as above. \u2022 For one page general topical coherence: B = S \u00b2 S0r (N, N \u00b2 s) \u2022 Can be written as a clique potential as in the case of a node potential (B = S \u00b2 S0r (N, N)."}, {"heading": "3.2.4 The Optimization objective", "text": "With various notations as above, we want to maximize the following in order to achieve the best results.1 (| S0 | 2) \u0411s 6 = s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2"}, {"heading": "3.2.5 Solving the optimization objective", "text": "The authors compare two different approaches to solving the optimization goal. \u2022 LP rounding approach: The first set of binary variables decides on the candidate each mention chooses, and the second set contains a binary variable for each possible candidate pair. The authors loosen this integer programming to linear programming and then use rounding with a threshold of 0.5 to obtain the best solution. \u2022 Uphill Ascent Based on all assignments set on NA, the assignments are based exclusively on local potentials. The following figure (from the paper) illustrates the process."}, {"heading": "3.3 Pragmatic combination of Local and Global Disambiguations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Introduction", "text": "Recall that Chapter 2 was about local terms. In Section 3 we saw how global terms can be combined with the overall goal. Recent work, Robust terminology of named terms in the text [10], suggests that it may not always be right to blindly opt for global terms. Consider the sentence: \"Manchester will play Madrid in Barcelona.\" All three named terms in this sentence are cities and football clubs. Collective terminology can force all three terms to be either football clubs or cities."}, {"heading": "3.3.2 Approach", "text": "The example diagram for the sentence \"They performed Kashmir written by Page and Plant. Page played unusual chord on his Gibson.\" is presented as follows: Figure 1: Mention Entity GraphOnce we have created the graph, we must assign the edge weights. Clearly, there are two types of edges: \u2022 Mention - Entity edge: The authors used a knowledge-based approach to assign this weight. This is as outlined in Section 2. Details of this score are given in [11]. \u2022 Entity - Entity edge: Milne weitten Score, as defined in Section 2, is used for this purpose.When the graph is finished, the authors pluck greedily, so that there is only one edge between each mention and entity."}, {"heading": "3.4 Further Readings on Named Entity Disambiguation", "text": "The following list may be valuable to interested readers. \u2022 Mining evidences for named entity disambiguation. \u2022 Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2013. \u2022 We have singled out Wikipedia as a catalog. \u2022 The following paper presents a general approach for named entities. \u2022 Linking named entities to any database. \u2022 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Sil, Avirup, et al. \"Proceedings of the Natural Language Processing and Computational Natural Language Learning."}, {"heading": "4 Distributional Statistics of", "text": "Named entitiesOnce you have a catalog of things, it makes sense to ask which of these \"things\" are more important than the others. In fact, you could broaden the question and ask, \"What pairs (or triples) of these things appear together on the open web?\" We define various statistics that might interest us about these entity catalogs, discuss some applications, suggest a basic method, and finally prepare the ground for the next section by outlining a solution that aims to provide us directly with the statistics we are looking for."}, {"heading": "4.1 What Statistics?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Which sense dominates for an entity?", "text": "First of all, we should calculate the number of times in which a certain \"sense\" of an entity is used. 4 For example, the entity Michael Jordan has several ambiguities: the professor, basketball player, and botanist. We want to find out the distribution of the occurrences of these senses. We call this number the sensory priority. It is important to note that Entity Prior differs from the previous mention, which is the fraction of time in which a mention refers to a particular entity. For example, the text \"Gingerbread\" could refer to several different concepts, from perhaps the most famous Android 2.3 to the novel. Mentioned before is to find out how many mentions of Gingerbread on the Web refer to the operating system. Entity Sense Before would tell us how often Gingerbread is the operating system compared to Gingerbread is the novel.Sense Prior (Si, E) = P (E) appears as an entity in the sense of) = Gingerbread (Si \u2032 | E) would tell us how often the operating system is in the forecake."}, {"heading": "4.1.2 How often do the 2 entities appear together?", "text": "A second interesting statistic would be to count how often two given entities appear together taking into account two given senses. For example, we would like to know how often Nokia http: / / en.wikipedia.org / wiki / Nokia with Gingerbread http: / / en.wikipedia.org / wiki / Gingerbread _ (operating _ system) occurs. We note that, unlike word bigrams and relationalgrams [12], entity bigrams are symmetrical and there is no obvious use case where we need to know the sequence-dependent frequency number of entities. However, such a formulation will result in a sparse distribution, as each count must be normalized by the total number of entity bigrams. We define the entity bi-gram count as follows E2 | 1 (EP)."}, {"heading": "4.2 Applications", "text": "We list a few applications of the sense beforehand and outline an application of the entity bigrams."}, {"heading": "4.2.1 Sense Prior", "text": "Beforehand will be helpful in many applications related to retrieving information."}, {"heading": "4.2.2 Entity Bigrams", "text": "Since the solution is only a minor modification of the proposed solution [13] to find out important relationships, we will only sketch an outline here. For the unit we are interested in, say X, create a node. Now, add all units E to node X, for which P (E | X) > there is a threshold. Let the weight of the edge be defined as P (E | X) + P (X | E) (3). We will then apply the personalized page rank on the X subgraph, starting with X with a page rank of 1 and other nodes with a page rank of 0. Then, we can sort the nodes by their side ranks."}, {"heading": "4.3 Baseline Approach : Label and Collect", "text": "How do we collect the above statistics? This question should not be too difficult to answer now. The entire third part was devoted to marking the entities mentioned in the text. We can use any of the methods (for example, AIDA can be set up as a rest service) to mark the corpus, and then iterate over the corpus to collect these statistics in a single pass."}, {"heading": "4.4 Solution based on estimating class ratios", "text": "While estimating class relations by ambiguities per mention seems quite intuitive, we do more than what we have to do. We are not interested in what each mention clearly refers to, a count of how often a particular entity appears is desideratum. In the open field, there are three different methods for directly estimating class relations [20] without going through the label and collecting the path. [20] In particular, we discuss a solution based on maximum intermediate discrepancy and demonstrate some upper limits of error. If mmd really works, we should expect a better estimate of the sense before and after entity grams. In the next section, the solution based on mmd is outlined and how mmd can be used to estimate the sense priors for different entities."}, {"heading": "5 MMD for estimating ratios of", "text": "This section discusses the MSD approach to the direct estimation of class relations [20]. First, we provide an intuition for the solution, followed by some results."}, {"heading": "5.1 Introduction", "text": "Suppose that in a factory that makes balls there are three different ball production machines, (say) A, B and C. Since none of the machines are perfect, they do not produce spherical balls. Rather, the balls are ellipsoids. Therefore, for each ball we have three different properties corresponding to the three semi-axes. Since all machines are different, they have their own unique view of how the balls should look, and therefore we expect that the half-axes are a good method of telling the machine that produced a particular ball. Moreover, let us assume that for all three machines we also have the most likely (expected) half-axes of the balls they produced. Let's call these half (x) and half (x) and half (x) and half (x) and half (x) and half (x). These are the expected characteristics that weigh the matxes. Let's say that we get 150 balls produced from these three machines."}, {"heading": "5.2 MMD Formulation", "text": "With the example above at our side"}, {"heading": "5.2.1 Problem Definition", "text": "We reproduce the problem from [20] \u2022 Let X = x-Rd be the set of all instances and Y = 0, 1,..., c be the set of all labels. \u2022 Design an estimator based on a labeled data set D (\u0395X x Y) that can estimate the class ratios \u03b8 = [\u03b80, \u03b81,..., \u03b8c] for each given set U (\u0395X), where \u03b8y denotes the fraction of instances with class designation y in U."}, {"heading": "5.2.2 Objective", "text": "\u2022 Agreement of two distributions based on the mean value of the Hilbert space characteristics caused by a kernel K. \u2022 Suppose the distribution of the characteristics is equal in both the training data and the test data PU (x | y) = PD (x | y) = Y (y | y). \u2022 Suppose, therefore, that the test distribution must correspond to Q (x) = Y (x | y) Ph\u00e4nomeny. \u2022 Let us specify the true mean values of the characteristic vectors of the y-th class and the unlabeled data. \u2022 Suppose that we somehow get the true class ratios. The true mean value of the characteristic vector of the unlabeled data can then be determined by Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-"}, {"heading": "5.2.3 Estimating entity ratios using MMD", "text": "In this section we whitewash the solution. \u2022 Features Each mention has several candidate ambiguities. This gives a way to formulate the characteristics. For each mention we can have a (sparse) feature vector with unequal zero values for the candidates. \u2022 Training data can be obtained by splitting the named entity ambiguity pipeline of one of the popular named entity ambiguities. [21] It is discussed how this can be achieved for AIDA, a popular entity ambiguity factor."}, {"heading": "6 Conclusion", "text": "The potential of the open web can only be exploited to the full by adding structure to it. The process involves creating structured repositories derived from the web that can answer interesting questions related to entities existing on the web. Many such intelligent applications that rely on the structured web will depend on frequencies of occurrence of the former. The report was a building block to achieving this. We started by informing what knowledge bases are. In the second part, we presented the problem of ambiguity in mentioning named entities and presented solutions that extend approximately over the last 8 years of research in the field. In the third part, we worked on what is meant by aggregate statistics and presented several applications of them. We presented the approach of maximum mean discrepancy in estimating class relations based on an example and discussed problem formulation. We briefly outlined how mmd can be applied to estimating event statistics of entities."}, {"heading": "7 Acknowledgement", "text": "This report is a summary of selected measurements carried out during the work under the direction of Prof. Sunita Sarawagi on the use of mmd for the collection of business event statistics on the Internet. I would like to thank her for her guidance, which has been tremendously helpful in gaining the understanding required for the preparation of this report. Thank you to Mr. Arun Iyer for all the help in understanding the maximum discrepancy and its implementation. Prof. Soumen Chakarbarti's presentations provided useful insights into the problem of corporate discambiguity."}], "references": [{"title": "Collective annotation of Wikipedia entities in web text.", "author": ["Kulkarni", "Sayali"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning to link with wikipedia.", "author": ["Milne", "David", "Ian H. Witten"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Wikify!: linking documents to encyclopedic knowledge.", "author": ["Mihalcea", "Rada", "Andras Csomai"], "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Robust disambiguation of named entities in text.", "author": ["Hoffart", "Johannes"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Kore: keyphrase overlap relatedness for entity disambiguation.", "author": ["Hoffart", "Johannes"], "venue": "Proceedings of the 21st ACM international conference on Information and knowledge management", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Rel-grams: a probabilistic model of relations in text.", "author": ["Balasubramanian", "Niranjan", "Stephen Soderland", "Oren Etzioni"], "venue": "Proceedings of the 12  Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Generating Coherent Event Schemas at Scale.", "author": ["Balasubramanian", "Niranjan", "Stephen Soderland", "Oren Etzioni Mausam"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone", "author": ["Michael Lesk"], "venue": "In Proceedings of the 5th annual international conference on Systems documentation (SIGDOC \u201986),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "Yago: a core of semantic knowledge.", "author": ["Suchanek", "Fabian M", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "Proceedings of the 16th international conference on World Wide Web. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Dbpedia: A nucleus for a web of open data.", "author": ["Auer", "S\u00f6ren"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "PATTY: a taxonomy of relational patterns with semantic types.", "author": ["Nakashole", "Ndapandula", "Gerhard Weikum", "Fabian Suchanek"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge.", "author": ["Bollacker", "Kurt"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Maximum Mean Discrepancy for Class Ratio Estimation: Convergence Bounds and Kernel Selection.", "author": ["Iyer", "Arun", "Saketha Nath", "Sunita Sarawagi"], "venue": "Proceedings of The 31st International Conference on Machine Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "well in the present scenario, owing to the mismatch in the training and test distribution [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "There are 3 reported methods for direct estimation of class ratios [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "Refer [16] for details.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "Refer [18] for details.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "5 Freebase \u2022 Freebase [19] relies on crowd sourcing for creation of a rich but clean knowledge base.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "This was state of the art until the CSAW[1] paper came along.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "We then provide a short summary of approach followed in Wikify [9] and the famous Milne and Witten paper [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "We then provide a short summary of approach followed in Wikify [9] and the famous Milne and Witten paper [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "A solution based on machine learning[1] concludes the subsection.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "Every local disambiguation techniques fall into one of the following two categories[9]", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "(See Lesk\u2019s algorithm [14]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "Wikify[9] The biggest contribution of this paper is perhaps presenting Wikipedia as the catalog against which were supposed to disambiguate.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Learning to link with Wikipedia[6] This paper defined three different features for disambiguation :", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "This subsection is based on the local disambiguation approach taken in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Milne and Witten [6] came close to inculcating some sort of coherence, but they couldn\u2019t totally build up the intuition.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "It was after a wait of 2 years that CSAW [1] took the game to a whole new level by working on the following key intuition :", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Since the CSAW[1] paper, every work on named entity disambiguation includes a notion of Topical coherence in the solution.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "3Reproduced from [1] 3.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "recent work, Robust disambiguation of named entities in text [10], proposes that blindly opting for global disambiguation may not be always right.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "The details about this score are given in [11].", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "We note that in contrast to word bi-grams and relational grams [12], entity bi grams are symmetric, and there is no obvious use case where we might need to know the order dependent occurrence count of the entities.", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "Entity Bi Gram(E2|E1) = P (E2 follows E1) = P (E2|E1) (2) We propose an application of Entity bi grams for finding out important entities motivated by [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "in [13] for finding out important relations, we only sketch an outline here.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "There are 3 different methods in the open domain for directly estimating the class ratio[20] , without going through the label and collect route.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "In particular, [20] discuss a solution based on maximum mean discrepancy and proves some upper bounds on errors.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "This section discusses the MMD approach for direct estimation of class ratios[20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "We reproduce the problem statement from [20]", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Such that \u2022 \u2200y, \u03b8y \u2265 0 \u2022 \u2211c y=0 \u03b8y = 1 Interesting discussion on theoretical bounds on the error in the class ratios thus predicted and methods for learning Kernel can be found in [20]", "startOffset": 180, "endOffset": 184}], "year": 2016, "abstractText": "The problem of collecting reliable estimates of occurrence of entities on the open web forms the premise for this report. The models learned for tagging entities cannot be expected to perform well when deployed on the web. This is owing to the severe mismatch in the distributions of such entities on the web and in the relatively diminutive training data. In this report, we build up the case for maximum mean discrepancy for estimation of occurrence statistics of entities on the web, taking a review of named entity disambiguation techniques and related concepts along the way.", "creator": "LaTeX with hyperref package"}}}