{"id": "1605.04655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment", "abstract": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question.", "histories": [["v1", "Mon, 16 May 2016 05:50:54 GMT  (23kb)", "http://arxiv.org/abs/1605.04655v1", "submitted to repl4nlp workshop at ACL Berlin 2016"], ["v2", "Wed, 22 Jun 2016 22:41:26 GMT  (24kb)", "http://arxiv.org/abs/1605.04655v2", "repl4nlp workshop at ACL Berlin 2016"]], "COMMENTS": "submitted to repl4nlp workshop at ACL Berlin 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["petr baudis", "silvestr stanko", "jan sedivy"], "accepted": false, "id": "1605.04655"}, "pdf": {"name": "1605.04655.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["baudipet@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.04 655v 1 [cs.C L] 16 May 2We compare several variants of neural networks for embedding sentences in decision-making based on evidence of varying relevance. We propose a basic model for integrating evidence for embedding sentences, show that joint training of sentence embedding for model relevance and embedding is feasible even without explicit evidence control, and demonstrate the importance of evaluating strong baselines. We also demonstrate the benefit of transferring a text comprehension model trained in an unrelated task for our small datasets. Our research is primarily motivated by a new open dataset that we are introducing, consisting of binary questions and message-based evidence snippets. We also apply the proposed relevance model to a similar task of evaluating multiple-choice test responses by using a preliminary test of school dataset questions and state-of-the-art school dataset technology."}, {"heading": "1 Introduction", "text": "ieD eeirg\u00dfnlrteeaeVnlrlrtee\u00fcgr rf\u00fc ide eeisrteeVnlrrrtee\u00fceegnln rf\u00fc ide eeirg\u00dfeeirteeeeeegnln rf\u00fc ide eeirg\u00dfnrrrteeeeeVnlrrrrrteeu ni rde eeirrrrrsrteeeeeirrrrrrrrrgggggeeeeeeeeeegnln rf\u00fc ide eeirgssnrrrtee\u00fce.nlrneD eiD eiD \"W\" s, so asds os os, so asos asos asos, rf\u00fc sdsas asds asds asds hics rf\u00fc ide nlrrrrrrf\u00fc, eaDs os, \"nlsa os os os,\" nos os os os os.rE \"D\" D \"iDe, so asos, so asos\" so asos \"De rosos.\""}, {"heading": "2 The Hypothesis Evaluation Task", "text": "Formally, the task of hypothesis evaluation is to construct a function yi = fh (Hi), where yi [0, 1] is a binary term (no to yes) and Hi = (qi, Mi) is a hypothesis in the form of the question text qi and a set of egg = {eij} evidentiary texts eij as extracted from an evidence-bearing corpus."}, {"heading": "2.1 Argus Dataset", "text": "Our main goal is to propose a solution to the Argus task where the Argus system (Baudis, 2015) (Baudis et al., 2016b) involves at least the automatic analysis and answering of questions related to the Augur prediction market platform. \u2022 In a prediction market, some users ask questions about the future, while other users bet on yes or no answers, assuming that the bet price reflects the real likelihood of the event. At some point in time (e.g. after the date of a sports item to be forecasted1 https: / augur.net / match), the correct answer is determined retrospectively and bets are paid. On a larger volume of questions that can represent the determination of weather outcomes, a significant overhead for the operation of the market. This motivates the Argus system, which should partially automate this determination - questions related to recent events based on open news sources."}, {"heading": "2.2 AI2-8grade/CK12 Dataset", "text": "AI2 Elementary School Science Questions (no-diagrams variant) 4, published by the Allen Institute, cover 855 basic four-choice questions related to high school science and follow the Allen Kaggle Challenge.5 Vocabulary includes scientific jargon and designated entities, and many questions are not factoid and require real-world reasoning or thought experiments. We have combined each answer with the respective question (by roughly replacing the word in the question with the answer. We consider this data set preliminary because it has not been verified by a human being and many hypotheses appear to be unprovable by the evidence we have gathered (i.e. the theoretical peak accuracy is much lower than 1.0)."}, {"heading": "2.3 MCTest Dataset", "text": "The Machine Comprehension Test dataset (Richardson et al., 2013) was introduced to provide researchers with a challenge to develop models that approximate human reading comprehension and serve as an overarching alternative to semantic parsing tasks that force specific knowledge representation; the dataset consists of a set of 660 stories that contain multiple sentences, written in simple and clear language (but with less limited vocabulary than, for example, the bAbI dataset (Weston et al., 2015); each story is accompanied by four questions, each of which contains four possible answers; the questions are tagged as being based on a single sentence in a story or with multiple sentence conclusions; we use an official extension of the dataset for RTE evaluation, which replaces the answers with complete statements combining each answer with the question; the dataset is split into two parts, MC-160 and MC-500, based on provenance, but similar in quality."}, {"heading": "3 Related Work", "text": "Our primary concern in integrating natural language queries and textual evidence is to form multiple sentences that can be used for both relevance weighting and response prediction. Sentence level representations in the retrieval + inference context are generally proposed within the Memory Network (Weston et al., 2014), but only averaged word embeddings are examined; the task consists of very simple sentences and a small vocabulary. Much more realistic approaches are introduced in the Answer Sentence Selection context (Wang et al., 2007) (Baudis, et al., 2016a), with state-of-the-art models using complex deep neural architectures with attention (dos Santos et al., 2016), but the selection taskforce consists only of retrieval and no inference (response prediction). A more indirect retrieval task in terms of message summary has been investigated by (Cao et al., 2016)."}, {"heading": "4 Neural Model", "text": "Our approach is to use a sequence of word embeddings for each hypothesis and evidence, then use the proposition embeddings to estimate the relevance and intricacy of each evidence in relation to that hypothesis, and to integrate the evidence through various strategies."}, {"heading": "4.1 Sentence Embeddings", "text": "To produce sentence embeddings, we examined the use of neural models from the Dataset-sts Framework for deep learning of sentence pair scoring functions. (Baudis and al., 2016a) We refer the reader to (Baudis and al., 2016a) and its references for detailed model descriptions. We evaluate an RNN model that uses bi-directionally aggregated GRU memory cells (Cho and al., 2014) and use the final states as embeddings; a CNN model that uses sentence max-pooled convolutional filters as embeddings (Kim, 2014); an RNN-CNN model that puts CNN on top of the pro-tokens GRU rather than using the word embeddings. (Tan et al, 2015); and an attn1511 model inspired by (Tan et al., 2015) that integrates the RNN-CNN model with word-attention to build hypotheses."}, {"heading": "4.2 Evidence Integration", "text": "Our main suggested scheme for evidence integration is evidence weighting. From each pair of hypotheses and evidence weighting, we produce 9 two [0, 1] predictions using a pair of MLP point collectors of data sets (Baudis and al., 2016a) 10 with sigmoid activation function. To integrate the predictions across multiple pieces of evidence, we propose a weighted average model: y = \u2211 iCiRi-iRiWe do not have access to explicit evidence labels for evidence, but we train the model end-to-end with only y labels and the formula for y is distinguishable by transferring the gradient to the Selection-Sdding model. This can be considered a simple, continuous prediction model. As a basic strategy, we consider evidence averaging, where we simply generate a hypothesis based on a few available evidence assumptions (selection)."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "We implement the differentiated model in the Keras framework (Chollet, 2015) and train the entire network from word embedding to the output of evidence-integrated hypotheses using binary cross-entropy loss as objective11 and the Adam optimization algorithm (Kingma and Ba, 2014). We apply L2 = 10 \u2212 4 regularization and a p = 1 / 3 discontinuation. Following the recommendation of (Baudis et al., 2016a), we report on the expected test set accuracy 12, which is determined by the average accuracy in 16 independent training runs and with 95% confidence intervals based on the T distribution of the student."}, {"heading": "5.2 Evaluation", "text": "In Fig. 2, we report on the model performance of the Argus task and show that the Ubuntu Dialogue Transfer RNN far outperforms other suggested models. However, a comparison of evidence integration approaches in Fig. 3 shows that contrary to our expectations, evidence integration is not the decisive factor and that there are no statistically significant differences between the assessed approaches. In contrast to (Yin et al., 2016), we found rank-based loss functions ineffective for this task. 12In the MCTest and AI2-8grade / CK12 datasets, we test and evaluate four hypotheses per question, while in the Argus dataset, each hypothesis is a single question. In Fig. 4, we look at model performance on the AI2-8grade / CK12 task, comparing the history of Ubuntu Dialogue Transfer RNN with other models."}, {"heading": "5.3 Analysis", "text": "While we can universally proclaim Ubu. RNN as the best model, we observe many aspects of the hypothesis evaluation problem shared by the AI2-8grade / CK12 and MCTest tasks, but not by the Argus task. Our biggest surprise lies in the inefficiency of the evidence weighing on the Argus task, since the observations of irrelevant passages initially led us to examine this model. We can also see that unprepared RNN performs very well on the Argus task, while CNN is a better model. One aspect that could explain this gap is that the latter two tasks are primarily retrievable, trying to judge any evidence as irrelevant or essentially a paraphrase of the hypothesis. On the other hand, the Argus task is highly semantic and compositional, with the questions often differing only by the presence of the negation."}, {"heading": "6 Conclusion", "text": "We have put forward a general hypothesis, the evaluation task with three sets of data of different characteristics, and demonstrated that neural models can perform strongly (with less effort than non-neural classifiers); we propose evidence that balances the model that is never harmful and improves the performance of some tasks; we also show that simple models outperform or closely correspond to the performance of complex architectures; all the models we are looking at are task-independent and have been successfully used in various contexts as a hypothesis evaluation."}, {"heading": "Acknowledgments", "text": "This work was co-financed by the Augur project of the Forecast Foundation and financially supported by the funding agency of the Czech Technical University Prague, grant no. SGS16 / 084 / OHK3 / 1T / 13. Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085 under the program \"Major Research, Development and Innovation Infrastructure Projects.\" We would like to thank Peronet Despeignes of the AugurProject for his support. Carl Burke provided instructions for searching for CK-12 e-books within the framework of the Kaggle Challenge."}], "references": [{"title": "Sentence pair scoring: Towards unified framework for text comprehension", "author": ["Petr Baudi\u0161", "Jan Pichl", "Tom\u00e1\u0161 Vysko\u010dil", "Jan Sediv\u00fd."], "venue": "CoRR, abs/1603.06127.", "citeRegEx": "Baudi\u0161 et al\\.,? 2016a", "shortCiteRegEx": "Baudi\u0161 et al\\.", "year": 2016}, {"title": "2016b. Argus: An artificial-intelligence assistant for augur\u2019s prediction market platform reporters", "author": ["Petr Baudis", "Silvestr Stanko", "Peronet Despeignes"], "venue": null, "citeRegEx": "Baudis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baudis et al\\.", "year": 2016}, {"title": "Argus: Deciding questions about events", "author": ["Petr Baudis"], "venue": null, "citeRegEx": "Baudis.,? \\Q2015\\E", "shortCiteRegEx": "Baudis.", "year": 2015}, {"title": "YodaQA: A Modular Question Answering System Pipeline", "author": ["Petr Baudi\u0161."], "venue": "POSTER 2015 - 19th International Student Conference on Electrical Engineering.", "citeRegEx": "Baudi\u0161.,? 2015", "shortCiteRegEx": "Baudi\u0161.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on https://github.com/brmson/dataset-sts", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Attsum: Joint learning of focusing and summarization with neural attention", "author": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei."], "venue": "arXiv preprint arXiv:1604.00125.", "citeRegEx": "Cao et al\\.,? 2016", "shortCiteRegEx": "Cao et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "CoRR, abs/1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "CoRR, abs/1505.08075.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "CoRR, abs/1506.08909.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Towards neural network-based reasoning", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong."], "venue": "arXiv preprint arXiv:1508.05508.", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Okapi at trec-3", "author": ["Stephen E Robertson", "Steve Walker", "Susan Jones"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom."], "venue": "CoRR, abs/1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Lstmbased deep learning models for non-factoid answer selection", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR, abs/1511.04108.", "citeRegEx": "Tan et al\\.,? 2015", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal Kevin Gimpel David McAllester."], "venue": "Proceedings of ACL, Volume 2: Short Papers:700.", "citeRegEx": "Wang and McAllester.,? 2015", "shortCiteRegEx": "Wang and McAllester.", "year": 2015}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze."], "venue": "CoRR, abs/1602.04341.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "A typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems (Baudi\u0161, 2015)), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 (Robertson et al.", "startOffset": 123, "endOffset": 137}, {"referenceID": 20, "context": "A typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems (Baudi\u0161, 2015)), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 (Robertson et al., 1995)) and prune the evidence by the relevancy score.", "startOffset": 226, "endOffset": 250}, {"referenceID": 9, "context": "On the other hand, textual entialment systems that seek to confirm hypotheses based on evidence (Dagan et al., 2006) (Marelli et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 16, "context": ", 2006) (Marelli et al., 2014) (Bowman et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 4, "context": ", 2014) (Bowman et al., 2015) are typically provided with only a single piece of evidence or only evidence pre-determined as relevant, and are often restricted to short and simple sentences without open-domain named entity occurences.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "To this end, we introduce an open dataset of questions and newspaper evidence, and a neural model within the Sentence Pair Scoring framework (Baudi\u0161 et al., 2016a) that (A) learns sentence embeddings for the question and evidence, (B) the embeddings represent both relevance and entailment characteristics as linear classifier inputs, and (C) the model aggregates all available evidence to produce a binary signal as the answer, which is the only training supervision.", "startOffset": 141, "endOffset": 163}, {"referenceID": 2, "context": "Our main aim is to propose a solution to the Argus Task, where the Argus system (Baudis, 2015) (Baudis et al.", "startOffset": 80, "endOffset": 94}, {"referenceID": 19, "context": "The Machine Comprehension Test (Richardson et al., 2013) dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehension, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation.", "startOffset": 31, "endOffset": 56}, {"referenceID": 26, "context": "the bAbI dataset (Weston et al., 2015)).", "startOffset": 17, "endOffset": 38}, {"referenceID": 25, "context": "Sentence-level representations in the retrieval + inference context have been popularly proposed within the Memory Network framework (Weston et al., 2014), but just averaged word embeddings are explored; the task includes only very simple sentences and a small vocabulary.", "startOffset": 133, "endOffset": 154}, {"referenceID": 24, "context": "Much more realistic setting is introduced in the Answer Sentence Selection context (Wang et al., 2007) (Baudi\u0161 et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 0, "context": ", 2007) (Baudi\u0161 et al., 2016a), with state-of-art models using complex deep neural architectures with attention (dos Santos et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": "A more indirect retrieval task regarding news summarization was investigated by (Cao et al., 2016).", "startOffset": 80, "endOffset": 98}, {"referenceID": 4, "context": "In the entailment context, (Bowman et al., 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) sentences They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are studied more recently (Rockt\u00e4schel et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": ", 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) sentences They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are studied more recently (Rockt\u00e4schel et al., 2015) (Cheng et al.", "startOffset": 370, "endOffset": 396}, {"referenceID": 6, "context": ", 2015) (Cheng et al., 2016).", "startOffset": 8, "endOffset": 28}, {"referenceID": 19, "context": "In the MCTest text comprehension challenge (Richardson et al., 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 23, "context": ", 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al.", "startOffset": 113, "endOffset": 140}, {"referenceID": 27, "context": ", 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al., 2016) uses convolutional neural networks to build sentence representations, and attention on multiple levels to select evidencing sentences.", "startOffset": 178, "endOffset": 196}, {"referenceID": 0, "context": "(Baudi\u0161 et al., 2016a)", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "We refer the reader to (Baudi\u0161 et al., 2016a) and its references for detailed model descriptions.", "startOffset": 23, "endOffset": 45}, {"referenceID": 7, "context": "We evaluate an RNN model which uses bidirectionally summed GRU memory cells (Cho et al., 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al.", "startOffset": 76, "endOffset": 94}, {"referenceID": 13, "context": ", 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al.", "startOffset": 128, "endOffset": 139}, {"referenceID": 22, "context": ", 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al., 2015); and an attn1511 model inspired by (Tan et al.", "startOffset": 240, "endOffset": 258}, {"referenceID": 22, "context": ", 2015); and an attn1511 model inspired by (Tan et al., 2015) that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings.", "startOffset": 43, "endOffset": 61}, {"referenceID": 12, "context": "We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged sentence emebddings (Iyyer et al., 2015).", "startOffset": 262, "endOffset": 282}, {"referenceID": 0, "context": "The original attn1511 model (Baudi\u0161 et al., 2016a) (as tuned for the Answer Sentence Selection task) used a softmax attention mechanism that effectively selects only a few key words of the sentence to focus on \u2014 for a hypothesis-evidence token t scalar attention score ah,e(t), the focus sh,e(t) is:", "startOffset": 28, "endOffset": 50}, {"referenceID": 18, "context": "As model input, we use the standard GloVe embeddings (Pennington et al., 2014) extended with binary inputs denoting token type and overlap with token or bigram in the paired sentence, as described in (Baudi\u0161 et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 0, "context": ", 2014) extended with binary inputs denoting token type and overlap with token or bigram in the paired sentence, as described in (Baudi\u0161 et al., 2016a).", "startOffset": 129, "endOffset": 151}, {"referenceID": 0, "context": "RNN transfer learning method proposed by (Baudi\u0161 et al., 2016a) where an RNN model (as described above) is trained on the Ubuntu Dialogue task (Lowe et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 15, "context": ", 2016a) where an RNN model (as described above) is trained on the Ubuntu Dialogue task (Lowe et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 0, "context": "From each pair of hypothesis and evidence embeddings,9 we produce two [0, 1] predictions using a pair of MLP point-scorers of dataset-sts (Baudi\u0161 et al., 2016a)10 with sigmoid activation function.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": "Finally, following success reported in the Answer Sentence Selection task (Baudi\u0161 et al., 2016a), we consider a BM25 Feature combined with Evidence Averaging, where the MLP scorer producing the pair scalar prediction as above has an additional BM25 word overlap score input (Robertson et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 20, "context": ", 2016a), we consider a BM25 Feature combined with Evidence Averaging, where the MLP scorer producing the pair scalar prediction as above has an additional BM25 word overlap score input (Robertson et al., 1995) besides the elementwise embedding comparisons.", "startOffset": 186, "endOffset": 210}, {"referenceID": 8, "context": "We implement the differentiable model in the Keras framework (Chollet, 2015) and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective11 and the Adam optimization algorithm (Kingma and Ba, 2014).", "startOffset": 61, "endOffset": 76}, {"referenceID": 14, "context": "We implement the differentiable model in the Keras framework (Chollet, 2015) and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective11 and the Adam optimization algorithm (Kingma and Ba, 2014).", "startOffset": 263, "endOffset": 284}, {"referenceID": 0, "context": "Following the recommendation of (Baudi\u0161 et al., 2016a), we report expected test set question accuracy12 as determined by average accuracy in 16-way independent training runs and with 95% confidence intervals based on the Student\u2019s t-distribution.", "startOffset": 32, "endOffset": 54}, {"referenceID": 27, "context": "Unlike (Yin et al., 2016), we have found ranking-based loss functions ineffective for this task.", "startOffset": 7, "endOffset": 25}, {"referenceID": 23, "context": "6, we compare our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features (Wang and McAllester, 2015), as well as past neural models from the literature, all using attention mechanisms \u2014 the Attentive Reader of (Hermann et al.", "startOffset": 127, "endOffset": 154}, {"referenceID": 11, "context": "6, we compare our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features (Wang and McAllester, 2015), as well as past neural models from the literature, all using attention mechanisms \u2014 the Attentive Reader of (Hermann et al., 2015), Neural Reasoner of (Peng et al.", "startOffset": 264, "endOffset": 286}, {"referenceID": 17, "context": ", 2015), Neural Reasoner of (Peng et al., 2015) and the HABCNN model family of (Yin et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 27, "context": ", 2015) and the HABCNN model family of (Yin et al., 2016).", "startOffset": 39, "endOffset": 57}, {"referenceID": 27, "context": "(Yin et al., 2016) also reports the results on the former models.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "We also demonstrate that simple models can outperform or closely match performance of complex architectures; all the models we consider are taskindependent and were successfully used in different contexts than Hypothesis Evaluation (Baudi\u0161 et al., 2016a).", "startOffset": 232, "endOffset": 254}, {"referenceID": 27, "context": "Finally, on the MCTest dataset, our best proposed model is better of statistically indistinguishable from the best neural network model reported so far (Yin et al., 2016), even though it has a simpler architecture and only a naive attention mechanism.", "startOffset": 152, "endOffset": 170}, {"referenceID": 10, "context": "A promising approach could extend the flexibility of the final sentence representation, moving from attention mechanism to a memory mechanism17 by allowing the network to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done for example on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015).", "startOffset": 339, "endOffset": 358}], "year": 2017, "abstractText": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question. We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit perevidence supervision, and show the importance of evaluating strong baselines. We also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets. Our research is motivated primarily by a new open dataset we introduce, consisting of binary questions and news-based evidence snippets. We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}