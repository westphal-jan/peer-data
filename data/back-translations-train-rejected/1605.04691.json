{"id": "1605.04691", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "On Avoidance Learning with Partial Observability", "abstract": "We study a framework where agents have to avoid aversive signals. The agents are given only partial information, in the form of features that are projections of task states. Additionally, the agents have to cope with non-determinism, defined as unpredictability on the way that actions are executed. The goal of each agent is to define its behavior based on feature-action pairs that reliably avoid aversive signals. We study a learning algorithm, called A-learning, that exhibits fixpoint convergence, where the belief of the allowed feature-action pairs eventually becomes fixed. A-learning is parameter-free and easy to implement.", "histories": [["v1", "Mon, 16 May 2016 09:26:53 GMT  (237kb,D)", "http://arxiv.org/abs/1605.04691v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom j ameloot"], "accepted": false, "id": "1605.04691"}, "pdf": {"name": "1605.04691.pdf", "metadata": {"source": "CRF", "title": "On Avoidance Learning with Partial Observability", "authors": ["Tom J. Ameloot"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related Work 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Fundamental Notions 7", "text": "3.1 Tasks........................................ 7 3.2 Strategies.................................."}, {"heading": "4 Avoidance Learning 11", "text": "4.1 A-learning algorithm..............................................................................................................................."}, {"heading": "5 Simple Grid Navigation 16", "text": "5.1 Definitions........................................................................."}, {"heading": "6 Conclusion and Further Work 20", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "The main objective of this paper is for agents to solve tasks by avoiding negative signals forever. This approach includes an interesting and perhaps entirely different type of information that cannot be dismissed out of hand. Part of the motivation is to understand how animals succeed in solving problems, such as navigation (GevaSagiv et al., 2015), with limited sensory information and unpredictable effects on the environment. An animal should find food or return home before it is exhausted. We are investigating a general framework in which agents encounter tasks, receive a defensive signal, and this way could avoid the problem by avoiding the use of actions and sequences of action."}, {"heading": "2 Related Work", "text": "The idea of avoiding negative signals or problems in general is related to safe reinforcement learning (Garc'\u0131a and Ferna'ndez, 2015), where the main goal is to perform reinforcement learning, which is often based on approach techniques to optimize numerical reward, with the avoidance of certainties added. (Section 4) In our description of A-Learning (Section 4), the marked feature action pairs are removed from the agent's memory. Problematic areas in the task state space. An example could be to train a robot for navigation tasks, but to avoid damage to the robot as much as possible. In the current work, the feedback to the agent consists of the negative signals. Reward becomes more implicit, since it lies in the avoidance of negative signals. Thus, the point of view in this work is that the agent is called optimal, when finally there is a trend to avoid all negative signals (there is no rejection of a)."}, {"heading": "3 Fundamental Notions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tasks", "text": "A task is a tuple T = (S, B, A, F, B, B) where \u2022 S is a non-empty group of states; \u2022 B S is a finite subset of start states; 5 \u2022 A is a non-empty finite group of states; \u2022 F is a non-empty finite group of states; \u2022 S \u00b7 P (F) is a finite subset of states in which all states s \u00b2 S are attainable in the sense that there is a sequence s0, s1, a1, a1. \u2022 S \u2192 P (F) is the finite subset of start states; and \u2022 S \u00b7 A is the series of deterrent signals in which all states s \u00b2 S are attainable in the sense that there is a sequence s0, s1, a1, a1."}, {"heading": "3.2 Strategies", "text": "Since the agent can only recognize characteristics and not states directly, the agent behavior must be based on feature action associations.Let T = (S, B, A, F, E, E, E, E) All this is a task. A policy for T is a total function \u03c0: F, P (A). We allow characteristics to be mapped to empty action approaches. If the task is understood from the context, for a state s, which we define as action (s, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, S, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, F, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E"}, {"heading": "4 Avoidance Learning", "text": "We present and examine an avoidance learning algorithm and its relationship to the concept of the strategy presented in Section 3.2."}, {"heading": "4.1 A-learning Algorithm", "text": "Algorithm 4.1 is an avoidance learning algorithm. The algorithm describes how the agent interacts with the task and how feature-action combinations are forgotten as a direct or indirect result of negative signals. Some aspects of the interaction are not under the control of the agent, in particular, how a successor state is selected using function \u03b4, and how characteristics from states are derived using function reaction. We are now offering more discussions about the algorithm. Henceforth, we will refer to algorithm 4.1 as A-learning. The essential product of A-learning is a legal P F \u00b7 A representing the permitted feature-action pairs; the symbol P stands for \"possibilities.\" At any time, the set P clearly defines a policy line as follows: for each f F, we define \u03c0 (f) = {a-Learning-Line) = (f, a-Learning-Line)."}, {"heading": "15 end", "text": "16 s: = s \u2032;"}, {"heading": "17 end", "text": "There are only a limited number of possible action packages, although we can clearly divide the context, although we can perform many test series. In line 11, we will then receive a successor state that has been arbitrarily selected to apply the current states (s, a). \u2022 Next, in line 12, we check whether we have encountered a regular reaction or whether the successor state is blocked. In both cases, we exclude the function action packages that caused us to apply an action in one state (line 13), and we restart the task (line 14). \u2022 If we do not encounter a problem and the successor state is not blocked, then we continue with the action during loop (line 16).Note that in general, there are several runs of A-learning on one task, and the choice on the successor state. Each run of A-learning is infinitely long. Nevertheless, there is always a possible fix point on the set, because we only remove a limited number of possible action spreads."}, {"heading": "4.2 Results", "text": "For all tasks T = (S, B, A, F, B, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,"}, {"heading": "4.3 Fairness", "text": "For example, we did not explicitly require that the agent actually receive a divergent signal when applying action a to a state where the task is sufficiently explored. A practical application of A-learning (Algorithm 4.1) could take into account the following assumptions of fairness: \u2022 if we execute line 2 infinitely often, then we select each starting state infinitely often. \u2022 To fully learn the task from each starting state, we must start external tasks infinitely often. \u2022 These restarts are not required by A-Learning itself. \u2022 on line 10, when we encounter the same pair."}, {"heading": "5 Simple Grid Navigation", "text": "We are investigating a simple class of network navigation problems."}, {"heading": "5.1 Definitions", "text": "We assume that we have a time restriction on the following assumptions: \"We assume that we have a time restriction on the total number of the total number of the total number of the total number.\" We assume that \"we have a time restriction on the total number of the total number of the total number.\" We assume that \"we have a time restriction on the total number of the total number of the total number.\" We assume that we have a time restriction on the total number of the total number of the total number. \"We assume that\" we have a time restriction on the total number of the total number of the total number. \"We assume that we have a time restriction on the total number of the total number.\" We assume that we have a time restriction on the total number of the total number of the total number of the total number of the total number. \""}, {"heading": "11 else", "text": "12 s. \"target: = s.target; 13 s.\" time: = max (0, s.time \u2212 1);"}, {"heading": "14 end", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 Results", "text": "Proposition 5.1. For each grid problem G = x, y), we define a strategy for each starting state in task (G) = = 3.5; Proof. Denote task (G) = (S, B, A, F, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E,"}, {"heading": "6 Conclusion and Further Work", "text": "We have used the notion of strategies to think about successfully avoiding aversion signals in tasks. We have shown that our avoidance learning algorithms always maintain these strategies. Now, we are discussing some interesting topics for further work. In this paper, we have considered a framework in which the characteristics are essentially designed so that we assume nothing about the way they are calculated. Therefore, we do not know how the characteristics relate to the working environment. It would be interesting to develop more detailed insights into how they work to ensure that strategies or similar strategies are possible. In particular, it seems fascinating to explore possible links between our frameworks and neural models."}], "references": [{"title": "POMDPs under probabilistic semantics", "author": ["K. Chatterjee", "M. Chme\u013a\u0131k"], "venue": "Artificial Intelligence, 221:46 \u2013 72.", "citeRegEx": "Chatterjee and Chme\u013a\u0131k,? 2015", "shortCiteRegEx": "Chatterjee and Chme\u013a\u0131k", "year": 2015}, {"title": "Fairness", "author": ["N. Francez"], "venue": "Springer-Verlag New York, Inc.", "citeRegEx": "Francez,? 1986", "shortCiteRegEx": "Francez", "year": 1986}, {"title": "Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules", "author": ["N. Fr\u00e9maux", "W. Gerstner"], "venue": "Frontiers in Neural Circuits.", "citeRegEx": "Fr\u00e9maux and Gerstner,? 2015", "shortCiteRegEx": "Fr\u00e9maux and Gerstner", "year": 2015}, {"title": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "PLoS Computational Biology, 9(4):e1003024.", "citeRegEx": "Fr\u00e9maux et al\\.,? 2013", "shortCiteRegEx": "Fr\u00e9maux et al\\.", "year": 2013}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Gar\u0107\u0131a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research, 16:1437\u20131480.", "citeRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez,? 2015", "shortCiteRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez", "year": 2015}, {"title": "Spatial cognition in bats and rats: from sensory acquisition to multiscale maps and navigation", "author": ["M. Geva-Sagiv", "L. Las", "Y. Yovel", "N. Ulanovsky"], "venue": "Nature Reviews Neuroscience, 16(4):94\u2013108.", "citeRegEx": "Geva.Sagiv et al\\.,? 2015", "shortCiteRegEx": "Geva.Sagiv et al\\.", "year": 2015}, {"title": "Consideration of risk in reinforcement learning", "author": ["M. Heger"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pages 105\u2013 111.", "citeRegEx": "Heger,? 1994", "shortCiteRegEx": "Heger", "year": 1994}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M. Jordan", "S. Singh"], "venue": "Neural Computation, 6(6).", "citeRegEx": "Jaakkola et al\\.,? 1994", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "Nonapproximability results for partially observable markov decision processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research, 14:83\u2013103.", "citeRegEx": "Lusena et al\\.,? 2001", "shortCiteRegEx": "Lusena et al\\.", "year": 2001}, {"title": "Understanding Intelligence", "author": ["R. Pfeifer", "C. Scheier"], "venue": "The MIT Press.", "citeRegEx": "Pfeifer and Scheier,? 1999", "shortCiteRegEx": "Pfeifer and Scheier", "year": 1999}, {"title": "An imperfect dopaminergic error signal can drive temporal-difference learning", "author": ["W. Potjans", "M. Diesmann", "A. Morrison"], "venue": "PLoS Computational Biology, 7(5):e1001133.", "citeRegEx": "Potjans et al\\.,? 2011", "shortCiteRegEx": "Potjans et al\\.", "year": 2011}, {"title": "Finding approximate POMDP solutions through belief compression", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research, 23:1 \u2013 40.", "citeRegEx": "Roy et al\\.,? 2005", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Reinforcement Learning, An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "The MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "The logic of adaptive behavior", "author": ["M. van Otterlo"], "venue": null, "citeRegEx": "Otterlo,? \\Q2009\\E", "shortCiteRegEx": "Otterlo", "year": 2009}, {"title": "Learning from delayed rewards", "author": ["C. Watkins"], "venue": "PhD thesis, Cambridge University.", "citeRegEx": "Watkins,? 1989", "shortCiteRegEx": "Watkins", "year": 1989}, {"title": "Q-learning", "author": ["C. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3\u20134):279\u2013 292. 22", "citeRegEx": "Watkins and Dayan,? 1992", "shortCiteRegEx": "Watkins and Dayan", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 11, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 0, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 9, "context": "Strategies The focus of this paper to understand agents based on their behavior in tasks, which could be a useful way to understand intelligence in general (Pfeifer and Scheier, 1999).", "startOffset": 156, "endOffset": 183}, {"referenceID": 7, "context": "(Jaakkola et al., 1994; Watkins and Dayan, 1992).", "startOffset": 0, "endOffset": 48}, {"referenceID": 15, "context": "(Jaakkola et al., 1994; Watkins and Dayan, 1992).", "startOffset": 0, "endOffset": 48}, {"referenceID": 12, "context": "In practice, a non-decreasing step-size, although potentially useful to model flexible agents that keep learning from their latest experiences (Sutton and Barto, 1998), can lead to problems of its own.", "startOffset": 143, "endOffset": 167}, {"referenceID": 14, "context": "3: We have simulated the Q-learning algorithm (Watkins, 1989; Watkins and Dayan, 1992) on the example task shown in Figure 1.", "startOffset": 46, "endOffset": 86}, {"referenceID": 15, "context": "3: We have simulated the Q-learning algorithm (Watkins, 1989; Watkins and Dayan, 1992) on the example task shown in Figure 1.", "startOffset": 46, "endOffset": 86}, {"referenceID": 6, "context": "Indeed, Heger (1994) has previously proposed a learning algorithm in tasks where actions have numeric costs, representing aversive signals.", "startOffset": 8, "endOffset": 21}, {"referenceID": 4, "context": "The idea of avoiding aversive signals, or problems in general, is related to safe reinforcement learning (Gar\u0107\u0131a and Fern\u00e1ndez, 2015).", "startOffset": 105, "endOffset": 133}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion.", "startOffset": 49, "endOffset": 77}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work.", "startOffset": 49, "endOffset": 157}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals.", "startOffset": 49, "endOffset": 219}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions.", "startOffset": 49, "endOffset": 449}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions. The learning algorithm proposed by Heger (1994) maps each state-action pair to the worst outcome (or cost), by means of the max-operator.", "startOffset": 49, "endOffset": 615}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions. The learning algorithm proposed by Heger (1994) maps each state-action pair to the worst outcome (or cost), by means of the max-operator. By remembering the highest incurred cost for a state-action pair, the agent in some sense learns about \u201cwalls\u201d in the state space that constrain its actions towards lower costs. The avoidance learning algorithm discussed in this paper (Section 4) is similar in spirit to the one by Heger (1994). A deviation, however, is that we assume here a boolean interpretation of aversive signals, which leads to a neat and computationally efficient framework.", "startOffset": 49, "endOffset": 1000}, {"referenceID": 12, "context": "Our definition of task resembles that of a standard Markov decision process (Sutton and Barto, 1998), but we have added features and aversive signals.", "startOffset": 76, "endOffset": 100}, {"referenceID": 10, "context": "This viewpoint resembles the way that an individual neuron (or a small group of neurons) in the brain could represent a distinct concept and could be individually linked to actions (Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 181, "endOffset": 225}, {"referenceID": 3, "context": "This viewpoint resembles the way that an individual neuron (or a small group of neurons) in the brain could represent a distinct concept and could be individually linked to actions (Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 181, "endOffset": 225}, {"referenceID": 12, "context": "This is related to the Markov assumption (Sutton and Barto, 1998), because we do not have to remember any features that were seen during previous time steps, and we may instead choose actions based on just f by itself.", "startOffset": 41, "endOffset": 65}, {"referenceID": 12, "context": "This is an important deviation from the -greedy exploration principle (Sutton and Barto, 1998), where at each time step the agent chooses a random action with small probability \u2208 [0, 1].", "startOffset": 70, "endOffset": 94}, {"referenceID": 4, "context": "We do not use that mechanism here because otherwise the agent keeps running the risk of encountering aversive signals (Gar\u0107\u0131a and Fern\u00e1ndez, 2015).", "startOffset": 118, "endOffset": 146}, {"referenceID": 1, "context": "This brings us to the topic of fairness (Francez, 1986).", "startOffset": 40, "endOffset": 55}, {"referenceID": 2, "context": "It is currently an open question whether or not feature learning in the brain is a completely unsupervised process (Fr\u00e9maux and Gerstner, 2015), i.", "startOffset": 115, "endOffset": 143}, {"referenceID": 12, "context": "Concretely, a feature f may only propose an action a if the pair (f, a) has been observed to be correlated to reward, either directly, or transitively by means of eligibility traces (Sutton and Barto, 1998).", "startOffset": 182, "endOffset": 206}], "year": 2016, "abstractText": "We study a framework where agents have to avoid aversive signals. The agents are given only partial information, in the form of features that are projections of task states. Additionally, the agents have to cope with non-determinism, defined as unpredictability on the way that actions are executed. The goal of each agent is to define its behavior based on featureaction pairs that reliably avoid aversive signals. We study a learning algorithm, called A-learning, that exhibits fixpoint convergence, where the belief of the allowed feature-action pairs eventually becomes fixed. A-learning is parameter-free and easy to implement.", "creator": "LaTeX with hyperref package"}}}