{"id": "1605.04809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT", "abstract": "This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, the phrase-based system cannot surpass state-of-the-art stand-alone neural models. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure-neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. In follow-up experiments we improve these results by additional 0.7 BLEU.", "histories": [["v1", "Mon, 16 May 2016 15:34:19 GMT  (52kb)", "https://arxiv.org/abs/1605.04809v1", null], ["v2", "Wed, 18 May 2016 12:15:46 GMT  (52kb)", "http://arxiv.org/abs/1605.04809v2", null], ["v3", "Thu, 23 Jun 2016 13:22:46 GMT  (52kb)", "http://arxiv.org/abs/1605.04809v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "tomasz dwojak", "rico sennrich"], "accepted": false, "id": "1605.04809"}, "pdf": {"name": "1605.04809.pdf", "metadata": {"source": "CRF", "title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT", "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich"], "emails": ["junczys@amu.edu.pl", "t.dwojak@amu.edu.pl", "rico.sennrich@ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.04 809v 3 [cs.C L] 23 Jun 2016"}, {"heading": "1 Introduction", "text": "This article describes the AMU-UEDIN submissions to the WMT 2016 Shared Task on the basis of message translations. We investigate methods for decoding and time integration of attention-based neural translation models with phrase decoding. Experiments were conducted for the English-Russian language pair in both translation directions. For these experiments, we implemented the follow-up step of the models described in Bahdanau et al. (2015) (more precisely, the DL4MT1 variant, which is also present in Nematus2) in efficient C + + / CUDA code, which is directly based on 1https: / github.com / nyu-dl / dl4mt-tutorial 2https: / github.com / rsennrich / nematus2 as a function of Moses. The GPU-based compilations come with their own characteristics, which we codify with the two most popular phrasedecoding algorithms of our English-BLD-1.1 and BLD-1.1-based system."}, {"heading": "2 Preprocessing", "text": "Since we are reusing the neural systems of Sennrich et al. (2016), we are also following their preprocessing scheme for the phrase-based systems. All data is tokenized with the Moses tokenizer, for English the Penn format tokenization scheme was used. Tokenized text is fallen.Sennrich et al. (2016) use byte pair encoding (BPE) to achieve an open vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015b). For English, the vocabulary size is limited to 50,000 units, for Russian to 100,000. This has the interesting consequence that subword units are used for phrase-based SMT. Although SMT seems to be better equipped to handle large vocabularies, the case of Russian still raises problems that are normally solved using translation mechanisms (Durrani et al., 2014)."}, {"heading": "3 Neural translation systems", "text": "As already mentioned, we use the English-Russian and Russian-English NMT models by Sennrich et al. (2016) and refer the reader to this paper for a more detailed description of these systems. In this section, for the sake of completeness, we give a brief summary. The neural machine translation system is an attentive encoder decoder (Bahdanau et al., 2015) trained with Nematus. Additional parallel training data was generated by automatic translation of a random sample (2 million sentences) of the monolingual Russian News Crawl corpus 2015 into English (Sennrich et al., 2015a), which was combined with the original parallel data in a ratio of 1 to 1. 4 The same was done for the other direction. We used mini-batches of size 80, a maximum sentence length of 50, word embedding size 500 and hidden layers of size 1024."}, {"heading": "4 Phrase-Based baseline systems", "text": "We rely on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the standard configuration with a lexical reorder model, in experiments not described in this paper we have attempted to explore the BPE encoding for the English-German language pair and to find subword units to handle German compound nouns well when they are used for the phrase-based SMT.4 This artificial data has not been used to create the phrase-based system, but it may be worthwhile exploring this possibility in the future. It may enable the phrase-based system to produce translations more similar to neural outputs.We add a 5-gram operation sequence model (Durrani et al., 2013).We do not make language-specific adjustments or modifications; the two systems differ only in terms of translation facility and available training data (monolingual)."}, {"heading": "5 NMT as Moses feature functions", "text": "As mentioned in the introduction, for the neural models trained with DL4MT or Nematus, we have implemented a C + + / CUDA version of the inference step, which can be used directly with our code. Thus, adding several models as separate features can be added to the Moses log linear model as different instances of the same feature, which can be weighted separately during tuning. Thus, adding several models as separate features is similar to ensemble translation with pure neural models. In this section, we give algorithmic details about integrating GPU-based translation models for soft attention decoding into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: 1. While Alkhouli et al. (2015) integrate RNN-based translation models into phrase decoding in Moses, this work of our knowledge is the first time that GNN-based algorithms are cut to 2 times our attention models are integrated into our 3 times."}, {"heading": "5.1 Scoring hypotheses and their expansions", "text": "\"the rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "5.2 Two-pass stack decoding", "text": "To keep the number of Moses modifications to a minimum, we propose two passes in which the first pass is a hypothesis and extension, and the second pass is the original expansion and scoring step. Between the two steps we anticipate using the method described above, the data structure introduced in Figure 1 is then used for the probability described during the scoring phrase of the stack decoding, as if individual hypotheses in which the score is on-the-fly.Figure 3 contains our full proposal for the two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces, the func-1: Procedure TWOPASSSTACKDECODING 2: place empty hypotheses in S0."}, {"heading": "5.3 Stack rescoring", "text": "The previous approach cannot be used with lazy decoding algorithms - such as cube pruning - which was also implemented in Moses. Apart from the fact that due to the large number of extensions, even small batch sizes of about 30 or 50 will quickly lead to large matrices in the middle steps of BATCHSCORE, where the prefix trees have the largest number of edges on the same plane. In the worst case, the matrix size will increase by a factor bd, where b is the branching factor and d is the current depth. In practice, however, the maximum is reached in the third or fourth stage, as few target formulations contain five or more words. To address both shortcomings, we propose a second algorithm in which the desperation disparity occurs."}, {"heading": "6 Experiments and results", "text": "For decoding, we use the cube-clipper algorithm with a stack size of 1,000 and a cube-clipper limit of 2,000 during the vote. At test date, a stack size of 1,000 is maintained, but the cube-clipper limit is increased to 5,000. We set a distortion limit of 12. We perform 10 iterations of Batch-Mira (Cherry and Foster, 2012) and select the best set of weights based on the development set. Our development set consists of a subset of 2,000 sets from the latest test set from 2014. Sentences have been selected to be shorter than 40 words to avoid GPUmemory problems. Our GPUs are three Nvidia GeForce GTX-970 cards, each with 4GB of RAM. In this essay, similar to Alkhouli et al. (2015), we ignore the implications of the infinite neural state and the recombination of the state we should rely on the recombination of the other functions in the hypothesis."}, {"heading": "6.1 Speed", "text": "Even with three GPUs, our system is ten times slower than a pure PB SMT system that runs with 24 CPU threads. However, at the moment it is unclear whether the large stack sizes we use are really necessary."}, {"heading": "6.2 Submitted results", "text": "Table 1 summarizes the results of our experiments. BLEU values are shown for the test sets newstest2015 and newstest-2016. Our baseline phrase-based systems (PB) are quite competitive compared to the best results of last year's WMT (24.4 and 27.9 for English-Russian and Russian-English, respectively). NMT-4 is the best pure neural ensemble from Sennrich et al. (2016) for both translation directions. Due to memory limitations, we were unable to use all four models as separate feature functions and limit ourselves to the best two models for English-Russian and the best three for Russian-English. Pure neural ensembles are NMT-2 (en-ru) and NMT-3 (ru-en). For English-Russian, our results lag behind the purely neural 4-ensembles NMT-4 in terms of the BLEU system. In direct comparison between ensembles of 2 models (PB + NMT-MT-English and BL2-MT-M3 systems), the BLEU-M2 and BLMT-MT-M3 are only."}, {"heading": "6.3 Follow-up experiments", "text": "Frustrated with the limited memory of our GPU cards and against our better knowledge6, we calculated the elementary average of all model weights in the NMT ensembles and stored the resulting model. Interestingly, the performance of these new models (NMT-4-Avg) is not much worse than the actual ensemble (NMT-4), while it is four times smaller and four times faster in decoding. The average models outperform each individual model or the smaller 2 ensembles. All models participating in the average are parameter dumps that are stored at different times during the same training run. This seems to be an interesting result for compressing and providing models. We can also use more models on average: For the Russian-English direction, we are experimenting with the parameter average of ten models (NMT10-Avg), which is even slightly higher than the real four-model ensemble NMT-4.With this smaller model, it is easier to adjust our function."}, {"heading": "Acknowledgments", "text": "This project was funded by the European Union's Horizon 2020 research and innovation programme under funding agreements 644333 (TraMOOC) and 688139 (SUMMA) and was partly funded by the Amazon Academic Research Awards programme."}], "references": [{"title": "Investigations on phrasebased decoding with recurrent neural network language and translation models", "author": ["Felix Rietig", "Hermann Ney"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Alkhouli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alkhouli et al\\.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Cherry", "Foster2012] Colin Cherry", "George Foster"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Cherry et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2012}, {"title": "Can Markov models over minimal translation units help phrase-based SMT", "author": ["Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn"], "venue": "In ACL,", "citeRegEx": "Durrani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Integrating an unsupervised transliteration model into statistical machine translation", "author": ["Hassan Sajjad", "Hieu Hoang", "Philipp Koehn"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Asso-", "citeRegEx": "Durrani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the ACL,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the ACL, pages 177\u2013180. ACL.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2010\\E", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proc. of the Conference on Machine Translation", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Weight averaging for neural networks and local resampling schemes", "author": ["Joachim Utans"], "venue": "In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models,", "citeRegEx": "Utans.,? \\Q1996\\E", "shortCiteRegEx": "Utans.", "year": 1996}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "For these experiments we re-implemented the inference step of the models described in Bahdanau et al. (2015) (more exactly the DL4MT1 variant also present in Nematus2) in efficient C++/CUDA code that can be directly", "startOffset": 86, "endOffset": 109}, {"referenceID": 10, "context": "As we reuse the neural systems from Sennrich et al. (2016), we follow their preprocessing scheme for the phrase-based systems as well.", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "transliteration mechanisms (Durrani et al., 2014).", "startOffset": 27, "endOffset": 49}, {"referenceID": 10, "context": "As mentioned before, we reuse the EnglishRussian and Russian-English NMT models from Sennrich et al. (2016) and refer the reader to that paper for a more detailed description of these systems.", "startOffset": 85, "endOffset": 108}, {"referenceID": 1, "context": "The neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which has been trained with Nematus.", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 14, "context": "Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.", "startOffset": 34, "endOffset": 48}, {"referenceID": 10, "context": "For our experiments with PB-SMT integration, we chose the same four models that constituted the best-scoring ensemble from Sennrich et al. (2016). If less than four models were used, we chose the models with the highest BLEU scores among these four models as measured on a development set.", "startOffset": 123, "endOffset": 146}, {"referenceID": 3, "context": "we add a 5-gram operation sequence model (Durrani et al., 2013).", "startOffset": 41, "endOffset": 63}, {"referenceID": 5, "context": "All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds (Heafield et al., 2013).", "startOffset": 104, "endOffset": 127}, {"referenceID": 8, "context": "The same concatenated files and pruning settings are used to create a 9-gram word-class language model with 200 word-classes produced by word2vec (Mikolov et al., 2013).", "startOffset": 146, "endOffset": 168}, {"referenceID": 0, "context": "Our work differs from Alkhouli et al. (2015) in the following aspects:", "startOffset": 22, "endOffset": 45}, {"referenceID": 0, "context": "While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models.", "startOffset": 6, "endOffset": 29}, {"referenceID": 0, "context": "Our implementation is GPU-based and our algorithms being tailored towards GPU computations require very different caching strategies from those proposed in Alkhouli et al. (2015). Our implementation seems to be about 10 times faster on one GPU, 30 times faster when three GPUs are used.", "startOffset": 156, "endOffset": 179}, {"referenceID": 7, "context": "Figure 3 contains our complete proposal for two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces that can be passed func1: procedure TWOPASSSTACKDECODING 2: Place empty hypothesis h0 into stack S0 3: for stack S in stacks do", "startOffset": 138, "endOffset": 151}, {"referenceID": 12, "context": "3 NMT-4 (Sennrich et al., 2016) 27.", "startOffset": 8, "endOffset": 31}, {"referenceID": 12, "context": "8 NMT-4 (Sennrich et al., 2016) 28.", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "In this paper, similar as Alkhouli et al. (2015), we ignore the implications of the infinite neural state and hypothesis recombination in the face of infinite state.", "startOffset": 26, "endOffset": 49}, {"referenceID": 10, "context": "NMT-4 is the best pure neural ensemble from Sennrich et al. (2016) for both translation directions.", "startOffset": 44, "endOffset": 67}], "year": 2016, "abstractText": "This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, our system stays behind the state-of-the-art pure neural models in terms of BLEU. Among restricted systems, manual evaluation places it in the first cluster tied with the pure neural model. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the best restricted system in its own cluster. In follow-up experiments we improve results by additional 0.8 BLEU.", "creator": "LaTeX with hyperref package"}}}