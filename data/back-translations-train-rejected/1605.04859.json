{"id": "1605.04859", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Reducing the Model Order of Deep Neural Networks Using Information Theory", "abstract": "Deep neural networks are typically represented by a much larger number of parameters than shallow models, making them prohibitive for small footprint devices. Recent research shows that there is considerable redundancy in the parameter space of deep neural networks. In this paper, we propose a method to compress deep neural networks by using the Fisher Information metric, which we estimate through a stochastic optimization method that keeps track of second-order information in the network. We first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. We evaluate our method on a classification task with a convolutional neural network trained on the MNIST data set. Experimental results show that our method outperforms existing methods for both network pruning and quantization.", "histories": [["v1", "Mon, 16 May 2016 18:12:45 GMT  (1022kb,D)", "http://arxiv.org/abs/1605.04859v1", "To appear in ISVLSI 2016 special session"]], "COMMENTS": "To appear in ISVLSI 2016 special session", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["ming tu", "visar berisha", "yu cao", "jae-sun seo"], "accepted": false, "id": "1605.04859"}, "pdf": {"name": "1605.04859.pdf", "metadata": {"source": "CRF", "title": "Reducing the Model Order of Deep Neural Networks Using Information Theory", "authors": ["Ming Tu", "Visar Berisha", "Yu Cao", "Jae-sun Seo"], "emails": [], "sections": [{"heading": null, "text": "It has been shown that we are able to outperform flat learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], with DNNs also having large sets of parameters that often make them unaffordable for small parameters [6]. For example, while the original network of LeNet5 [8] (a classification system based on the Convolutionary Neural Network) has less than 100K parameters, the winner of the 2012 ImageNet [9] competition has more than 60M parameters. The storage access costs alone can render these larger networks unusable for low-power settings.It has been positioned that the expressiveness of DNNs comes from its large parameter spaces and hierarchical structures; however, recent studies have shown that there is often a large portion of the parameters redundancy in DNN [10], [11] which renders them unusable."}, {"heading": "II. DNNS PRUNING AND QUANTIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Fisher Information and DNNs", "text": "We show an imaginary DNN architecture in Fig. 1. Consider the output y of the DNN as a conditional probability distribution p (y | x; \u03b8), parameterized by the DNN input, x, and its parameters, \u03b8. The FIM evaluated to a certain value of \u03b8 is defined as: ar Xiv: 160 5.04 859v 1 [cs.L G] 16 May 201 6F (\u03b8) = Ey [(\u2202 log p (y | x; \u03b8) 20s))) (\u2202 log p (y | x; \u03b8) \u2202) T]. (1) We can see from eqn. (1) that the FIM is the covariance of the gradient of the probability of the protocol with respect to its parameter \u03b8. Thus, Fattle is a p x symmetric positive symmetric positive semidefined matrix. It is easy to see that the diagonal elements of the FIM are multiplicable by the expectation of the element."}, {"heading": "B. Estimating the Fisher Information", "text": "A number of recent studies on natural gradient lineage (NGD) [28] use the information geometry of the underlying parameter in a variety of ways and apply it to gradient-based optimization of DNNs. Natural gradient lineage uses the inverse FIM method to limit the magnitude of the update steps so that the Kullback Leibler (KL) maintains the divergence between the output distribution of the network at iteration t and iteration t + 1 [29], [30], [31]. However, this approach avoids large update steps and results at faster conversion. The algorithm 1 Adam algorithm, which was extracted from [32] Excerpted, shows: step size \u03b1, exponential decay rates \u03b21, \u03b22, given initial parameter vector vector vector vector vector vector-0, initial moment-0, and second vector-0 and non-vector-vector-vector-vector-1 are converted during initial moment-0 and second vector-0-vector-1."}, {"heading": "C. Network Pruning and Quantization", "text": "The simplest approach to network cutting is to sort the parameters by comparing their entries in the FIM diagonal and removing those with the lowest entries. However, as we will see in the results section, this method does not work well because estimating small values in the FIM diagonal is challenging and unreliable. If the model is overparameterized, the actual parameter space is much smaller than the number of parameters used in the network. As a result, after training, a number of parameters will be close to zero and estimating their impact on the Fisher information becomes difficult [27]. To solve this problem, we use a combination of size-based and FIM-based cutting. For example, if we want to cut L parameters from the network, we first remove L (1 \u2212 r) parameters with the smallest size and then the smallest bits with the smallest size. We then order the remaining parameters by their FIM diagonals and remove the additional Lr parameters with the smallest entries in the FIM."}, {"heading": "III. EXPERIMENTS AND RESULTS ANALYSIS", "text": "In this section we present the experiments and results in two parts: network truncation and network quantification. All experiments were performed with the Python Neural Network Library Keras [33], which was implemented with Theano on an NVIDIA GTX 760 GPU. We evaluated our algorithms on the MNIST data set, which consists of 60K binary images for training and 10K for testing. There are 10 classes (digits from 0 to 9) in the data and the size of each image (and the input dimension of the neural network) is 28 x 28. We trained a Convolutionary Neural Network (CNN) with 2 Convolutionary Layers (CNN), each with 32 filters. The size of the Convolutionary Kernel was 3 x 3 and the Reflected Linear Unit (ReLU) activation function was used. There was a constellation of 2 x 2 max-pooling layers (CNN), each with 2 revolutionary layers of 32 filters each being used as a constellation \u2212 9."}, {"heading": "A. Network pruning", "text": "The trained network consisted of a total of 591,242 parameters in the fully connected levels. We removed different numbers of parameters using three different methods: (1) magnitude-based pruning, in which parameters of the smallest order of magnitude were removed; (2) Fisher information-based pruning, in which parameters with small entries in the FIM diagonal were removed; and (3) a combination of magnitude and Fisher information-based pruning, in which we traded parameter r between the two methods (see Sec. II-C). The number of pruned parameters ranged from 1.0E4 (1.69% of the total parameters) to 5.9E5 (99.79% of the total parameters) with a step size of 2.0E4. For the third method, we set the r value to 0.05. In this network, we found that R values below 0.1 give good results; however, cross-validation is used for other networks to identify an appropriate value."}, {"heading": "B. Network quantization", "text": "Our quantization method is a non-uniform quantization method based on clustering Kmeans. We classify weights according to an importance metric (\"Fischer\" or \"mag\") and group them into k-clusters. We then quantify the quantization methods based on different bit depths, from 1 bit / parameter (least important) to k-bit (most important). To remove the effects of different pruning methods, we first removed 5.5 E5 parameters (91.33% of the total parameters) using only magnitude-based quantifications; the resulting model had 51,242 (8.67%) remaining parameters and an overall accuracy of 98.67% (less than 1% loss of accuracy). We quantified the remaining parameters using three different methods: (1) non-uniform quantification based on Fisher information ranking; (2) non-uniform quantization based on magnitude."}, {"heading": "IV. DISCUSSION", "text": "To evaluate the compression ratio for the example shown here, we analyze the effects of both network reduction and quantification. As we have previously seen in Fig. 3, if we limit our acceptable reduction in performance to 1%, we can remove 92.18% parameters (accuracy is 98.37%) for order-based circumcision, resulting in a compression ratio of 12.8 \u00d7. For our proposed combination of order of magnitude and Fisher information-based circumcision, we can remove 94.72% parameters (accuracy is 98.38%) and the compression ratio is 18.9 \u00d7. For network quantification, we can choose between uniform or non-uniform quantification. If we assume that the original parameters are stored in FLOAT32 format, as shown in Fig. 5 using a uniform quantification, we can achieve a reduction of 32 3 = 10.7 \u00d7 1; by using a non-uniform quantification, we can achieve a reduction of 13.4 \u00d7 332."}, {"heading": "V. CONCLUSION", "text": "In this article, we propose a new network reduction and quantization scheme that uses a combination of the size of the parameters and the Fisher information as a measure of the importance of the parameters. To reduce the network, the proposed algorithm first removes small parameters and then further reduces the network by removing additional parameters based on the Fisher information. After reducing the network, we propose a non-uniform quantization scheme for remaining parameters based on the same Fisher criterion. Results show that the combination of network reduction and quantification results in high compression rates."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported in part by the Office of Naval Research Scholarship N000141410722 (Berisha), an ASU-Mayo Seed Scholarship, and a hardware scholarship from NVIDIA."}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u2013 97, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": "arXiv preprint arXiv:1510.00726, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["Vinayak Gokhale", "Jonghoon Jin", "Aysegul Dundar", "Berin Martini", "Eugenio Culurciello"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014, pp. 682\u2013687.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2857\u20132865.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "INTERSPEECH, 2013, pp. 2365\u20132369.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1269\u20131277.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1135\u20131143.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla"], "venue": "Advances in Neural Information Processing Systems, 1990, pp. 598\u2013 605.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in Neural Information Processing Systems, 1993, pp. 164\u2013171.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 4409\u20134412.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Weight quantization for multi-layer perceptrons using soft weight sharing", "author": ["Fatih K\u00f6ksal", "Ethem Alpaydyn", "G\u00fcnhan D\u00fcndar"], "venue": "Artificial Neural Networks (ICANN) 2001, pp. 211\u2013216. Springer, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Boundary contraction training for acoustic models based on discrete deep neural networks", "author": ["Ryu Takeda", "Naoyuki Kanda", "Nobuo Nukaga"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Accurate and compact large vocabulary speech recognition on mobile devices", "author": ["Xin Lei", "Andrew Senior", "Alexander Gruenstein", "Jeffrey Sorensen"], "venue": "INTERSPEECH, 2013, pp. 662\u2013665.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011, vol. 1.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "arXiv preprint arXiv:1502.02551, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep neural networks on the fly", "author": ["Guillaume Souli\u00e9", "Vincent Gripon", "Ma\u00eblys Robert"], "venue": "arXiv preprint arXiv:1509.08745, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Small-footprint highperformance deep neural network-based speech recognition using splitvq", "author": ["Yongqiang Wang", "Jinyu Li", "Yifan Gong"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4984\u20134988.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Ranking the parameters of deep neural network using the fisher information", "author": ["Ming Tu", "Visar Berisha", "Martin Woolf", "Jae-sun Seo", "Yu Cao"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, under publication, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation, vol. 10, no. 2, pp. 251\u2013276, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1301.3584, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural neural networks", "author": ["Guillaume Desjardins", "Karen Simonyan", "Razvan Pascanu"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2062\u20132070.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["Franois Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "Computer vision\u2013ECCV 2014, pp. 818\u2013833. Springer, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Assessing the accuracy of the maximum likelihood estimator: Observed versus expected fisher information", "author": ["Bradley Efron", "David V Hinkley"], "venue": "Biometrika, vol. 65, no. 3, pp. 457\u2013483, 1978.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 4, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 206, "endOffset": 209}, {"referenceID": 5, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 211, "endOffset": 214}, {"referenceID": 6, "context": "Deep neural networks (DNNs) have been shown to outperform shallow learning algorithms in applications such as computer vision [1], [2], automatic speech recognition [3], [4] and natural language processing [5], [6]; however DNNs also have large parameter sets, often making them prohibitive for small-footprint devices [7].", "startOffset": 319, "endOffset": 322}, {"referenceID": 7, "context": "For example, while the original LeNet5 network [8] (a classification system based on convolutional neural network) has less than 100K parameters, the winner of the 2012 ImageNet competition [9] has over 60M parameters.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "For example, while the original LeNet5 network [8] (a classification system based on convolutional neural network) has less than 100K parameters, the winner of the 2012 ImageNet competition [9] has over 60M parameters.", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "structure; however recent studies have shown that there is often a great deal of parameter redundancy in DNNs [10], [11], making them unnecessarily complex.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "structure; however recent studies have shown that there is often a great deal of parameter redundancy in DNNs [10], [11], making them unnecessarily complex.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "For example, the authors in [12], [13] used low rank decomposition of the weights to", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "For example, the authors in [12], [13] used low rank decomposition of the weights to", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Similarly, the authors in [11] showed that over 95% parameters of DNNs can be predicted without any training and without impacting accuracy.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "In addition to low-rank parameter decomposition, network pruning and quantization methods have also been proposed [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "Neural network pruning has been investigated in early studies, including pruning weights with small magnitudes, optimal brain damage [15] and optimal brain surgeon [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "Neural network pruning has been investigated in early studies, including pruning weights with small magnitudes, optimal brain damage [15] and optimal brain surgeon [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "As a result, for large-scale DNNs, magnitude-based weight pruning is still a popular method [17], [14], [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "The studies in [19], [20] discretized the weights of a neural network according to the range of the weights.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The studies in [19], [20] discretized the weights of a neural network according to the range of the weights.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "The methods in [21] and [22] used uniform scalar parameter quantization to implement fixed-point versions of the networks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "The methods in [21] and [22] used uniform scalar parameter quantization to implement fixed-point versions of the networks.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "In [23], a new fixed-point representation for DNN training was proposed, using stochastic rounding for the parameters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "Vector quantization based schemes have been applied to CNNs for both computer vision and automatic speech recognition tasks [24], [25], [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "In [27], we introduced a new method to calculate the diagonal of the Fisher Information Matrix (FIM) and showed that it can be used to reduce the size of DNNs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "A number of recent studies on natural gradient descent (NGD) [28] exploit the information geometry of the underlying parameter manifold and apply it to gradient-based", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 234, "endOffset": 238}, {"referenceID": 29, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 240, "endOffset": 244}, {"referenceID": 30, "context": "Natural gradient descent uses the inverse FIM to constrain the magnitude of the update steps such that the Kullback-Leibler (KL) divergence between the output distribution of the network at iteration t and iteration t + 1 is constant [29], [30], [31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 31, "context": "Algorithm 1 Adam algorithm, excerpted from [32]", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "A recent paper proposed a new stochastic optimization method called \u201cAdam\u201d and showed we can efficiently estimate the FIM diagonal at each iteration while minimizing this loss function [32].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "number of parameters become close to zero and estimating their influence based on the Fisher Information is challenging [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "All the experiments were done using the Python neural network library Keras [33] implemented using Theano on an NVIDIA GTX 760 GPU.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "Since the fully connected layers of CNNs accounts for \u223c 90% of the total weights [34], we only focus on the weights (including", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "bias terms) in the fully connected layers in this paper as others have done in [24] [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "bias terms) in the fully connected layers in this paper as others have done in [24] [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "This is consistent with our finding in [27], where we used the FIM criterion to remove parameters in an autoencoder.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Consistent with findings from our previous work [27], the Fisher Information better captures the importance of larger parameters when compared to magnitude-based pruning.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 171, "endOffset": 175}, {"referenceID": 26, "context": "There is a relationship between our method and other previous methods based on estimation of the Hessian diagonal, namely optimal brain damage [15], optimal brain surgeon [16] and our previous work [27].", "startOffset": 198, "endOffset": 202}, {"referenceID": 34, "context": "These approaches are closely related to our approach when the cost function is the log-likelihood since the second derivative of log-likelihood function (Hessian) evaluated at the maximum likelihood estimate is the observed Fisher Information [35].", "startOffset": 243, "endOffset": 247}, {"referenceID": 26, "context": "Our approach in [27] made use of the relationship between Fisher Information and the family of f -divergences to estimate the FIM diagonal.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Finally, it is important to note that further gains in performance can be obtained by retraining the network after pruning and before quantization [14].", "startOffset": 147, "endOffset": 151}], "year": 2016, "abstractText": "Deep neural networks are typically represented by a much larger number of parameters than shallow models, making them prohibitive for small footprint devices. Recent research shows that there is considerable redundancy in the parameter space of deep neural networks. In this paper, we propose a method to compress deep neural networks by using the Fisher Information metric, which we estimate through a stochastic optimization method that keeps track of secondorder information in the network. We first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. We evaluate our method on a classification task with a convolutional neural network trained on the MNIST data set. Experimental results show that our method outperforms existing methods for both network pruning and quantization.", "creator": "LaTeX with hyperref package"}}}