{"id": "1605.05011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Locally Weighted Ensemble Clustering", "abstract": "Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, without access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art.", "histories": [["v1", "Tue, 17 May 2016 03:52:38 GMT  (3767kb,D)", "http://arxiv.org/abs/1605.05011v1", "Under peer review"], ["v2", "Fri, 5 May 2017 16:35:41 GMT  (3380kb,D)", "http://arxiv.org/abs/1605.05011v2", "To appear in IEEE Transactions on Cybernetics. The MATLAB source code and experimental data of this work are available at:this https URL"]], "COMMENTS": "Under peer review", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dong huang", "chang-dong wang", "jian-huang lai"], "accepted": false, "id": "1605.05011"}, "pdf": {"name": "1605.05011.pdf", "metadata": {"source": "CRF", "title": "Locally Weighted Ensemble Clustering", "authors": ["Dong Huang", "Chang-Dong Wang", "Jian-Huang Lai"], "emails": ["huangdonghere@gmail.com."], "sections": [{"heading": null, "text": "In recent decades, the number of cluster algorithms has developed through the exploitation of various techniques. [3], [4], [5], [5], [5], [5], [6], [6], [6], [6], [6], [6], [6], [7], [7], [7], [9], and [9], [9], [10], [10], [10], [11], [11], [8], [8], [8,], [8,], [9], [9], [9] and [9], [10], [10], [11], [11], \"[11,\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\","}, {"heading": "II. RELATED WORK", "text": "In the last ten years, the number of those who are able to exist in the United States and other countries of the world has multiplied, both in the United States and in the United States. (...) In the United States, the number of those who are able to survive themselves has dramatically decreased. (...) In the United States, the number of those who are able to survive themselves has increased dramatically. (...) In the United States, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States, the United States of America, the United States, the United States of America, the United States, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of America of America, the United States of America, the United States of America, the United States of America, the United States of America, the United States of"}, {"heading": "III. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Entropy", "text": "In this section we present the concept of entropy. In information theory [34] entropy is a measure of the uncertainty associated with a random variable. The formal definition of entropy is in definition 1.Definition 1. For a discrete random variable X, entropy H (X) becomes H (X) = \u2212 \u2211 x x x p (x) log2 p (x), (1) where X is the value set that X can assume, and p (x) the probability mass function of X.Common entropy is a measure of the uncertainty associated with a number of random variables. The formal definition of common entropy is in definition 2.Definition 2. For a pair of discrete random variables (X, Y), common entropy H (X, Y) is defined as H (X, Y), H (X, Y), Y), independent (X, Y) and H (H)."}, {"heading": "B. Ensemble Clustering", "text": "In this section we present the general formulation of the ensemble cluster problem. Consider O = {o1, \u00b7 \u00b7, oN} as a data set, where oi is the i-th data object and N the number of objects in O. Consider M partitions (or clusters) for record O, each of which is treated as a base cluster and consists of a certain number of clusters. Formally, we designate the ensemble of M-base clusters as follows: 1, 2, 3, 4, where \u03c0m = {Cm1, \u00b7, Cmnm} (5) is the m-th cluster of cluster formation in \u0438, Cmi stands for the i-th cluster in \u03c0m, and nm stands for the number of clusters in \u03c0m. Each cluster is a group of data objects. Obviously, the unification of all clusters in the same data cluster formation is."}, {"heading": "IV. LOCALLY WEIGHTED ENSEMBLE CLUSTERING", "text": "In this paper, we propose a novel cluster cluster approach based on an ensemble-driven cluster uncertainty assessment and a local weighting strategy. Without access to the original data characteristics or some assumptions about data distribution, we use the ensemble information to estimate the uncertainty (or unreliability) of clusters based on an entropic criterion. With the cluster uncertainty obtained, an ensemble-driven cluster validity index (ECI) is presented to evaluate the reliability of each cluster using the cluster designations in the cluster ensemble. In this paper, we argue that the diverse clusters in the overall ensemble can provide an effective indication for evaluating the reliability of each cluster. We then refine the conventional CA matrix using a local weighting strategy based on the ECI measurement and introduce the concept of the locally weighted co-association (LWCA), which is described in more detail as a summary of the cluster ensemble with two different clusters."}, {"heading": "A. Measuring Cluster Uncertainty in Ensembles", "text": "In order to assess the reliability of each cluster, we need to look at the concept of entropy using the cluster labels throughout the ensemble. As introduced in Section III-A, entropy is a measure of uncertainty associated with a random variable. Each cluster is a set of data objects. In fact, the objects in Ci can belong to most of the different clusters in Ci, where nm is the total number of clusters in the ensemble. The uncertainty (or entropy) of the objects in Ci to more than one cluster in the ensemble.3 The objects in Ci can belong to most of the different clusters in the ensemble.3, where nm is the total number of clusters in the entropy. The uncertainty (or entropy) of Ci w.r.t. The objects in Ci can be calculated by looking at the objects in the ensemble."}, {"heading": "B. Ensemble-Driven Cluster Validity", "text": "Once we have obtained the uncertainty (or entropy) of each cluster in the cluster ensemble, we further propose an ensemble-driven cluster index (ECI), which measures the reliability of a cluster by taking into account its uncertainty. Definition 5. If a cluster index with M base clusters is specified, the ensemble-driven cluster index (ECI) for a cluster Ci is defined as asECI (Ci) = e \u2212 HB (Ci) \u03b8 \u00b7 M, (10), where \u03b8 > 0 is a parameter for adjusting the influence of cluster uncertainty over the index. The formal definition of the EKI is in Definition 5. According to the definition, because the ECI (Ci) values are specified in a dramatic manner, it is determined that ECI (Ci) -1] is the uncertainty for each Ci-C index. Obviously, a smaller uncertainty of a cluster leads to a larger ECI value. As an example, the ECI value illustrates the value for the cluster in the ECI-1 table."}, {"heading": "C. Refining Co-association Matrix by Local Weighting", "text": "The Co-Association (CA) Matrix is first proposed by Fred and Jain [17], reflecting how often two data objects are grouped into the same cluster among the multiple base clusters in the Ensemble.Definition 6. In view of an ensemble, the Co-Association (CA) Matrix is calculated as A = {aij} N \u00b7 N (11) 6 withaij = 1M \u00b7 M \u0445 m = 1 \u03b4mij, (12) \u03b4mij = {1, if Clsm (oi) = Clsm (oj), 0, otherwise, (13) where Clsm (oi) denotes the cluster in which the object oi belongs. The CA Matrix is a classic and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40]. Despite its significant success, one limitation of the CA Matrix is that it treats all clusters and all base clusters equally in the association strategy and ensemble strategy."}, {"heading": "D. Consensus Functions", "text": "This book deals with the question of how it was possible for the people who are in crisis to be able to reform themselves, and how it was possible for the people who are in crisis to get into the crisis. (...) It is about the question of how it was possible to get into the crisis. (...) It is about the question of how it was possible to get into the crisis. (...) It is about the question of how it was possible to get into the crisis. (...) It is about the question of how it was possible to get into the crisis. (...) It is about the question of whether it was possible to get into the crisis. (...) It is about the question of whether it was necessary to get into a new crisis. (...) It is about the question of whether it should get into a new crisis. (...) It is about the question of how it should get into a new crisis. (...) It is about how it should get into a new crisis. (...) It is about the question of how it should get into a new crisis. (...) It is about how it should get into a new crisis."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we evaluate the proposed LWEA and LWGP methods against the state-of-the-art cluster methods on a variety of real data sets. All experiments are performed in Matlab R2014a 64-bit on a workstation (Windows Server 2008 R2 64-bit, 8 Intel 2.40 GHz processors, 96 GB RAM).Algorithm 2 (Locally Weighted Graph Partitioning) Input: VP, k.1: Calculation of cluster uncertainty in Definition 4.2: Calculation of ECI measurements of clusters in Definition 5.3: Creation of LWBG diagram w.r.t. Definition 8. 4: Division of LWBG into a specified number of segments using the Tcut algorithm [41].5: Treatment of objects in the same segment as clusters and formation of clusters for the entire dataset."}, {"heading": "A. Datasets and Evaluation Metric", "text": "In our experiments, ten real datasets are used, namely Semeion, Multiple Features (MF), Image Segmentation (IS), Forest Covertype (FCT), MNIST, Optical Digit Recognition (ODR), Landsat Satellite (LS), ISOLET, USPS, and Letter Recognition (LR).The MNIST dataset and the USPS dataset are both from [42] and [43], while the other eight datasets are from the UCI Machine Learning Repository [44].The details of the ten datasets are in Table II. We use the Normalized Mutual Data Pool (NMI) [14] to evaluate the quality of the clusterings.The NMI measurement provides a solid indication of the common information between clusters and clusters and is one of the most commonly used dataset utmetric data for cluster analysis. A greater value of NMI indicates the number of clusters generates the large number of clusters."}, {"heading": "B. Choices of Parameter \u03b8", "text": "The parameter \u03b8 controls the influence of the cluster uncertainty on the consensus process of LWEA and LWGP. A smaller \u03b8 leads to a stronger influence of the cluster uncertainty on the consensus process via the ECI measurement (see Fig. 3).We evaluate the cluster performance of LWEA and LWGP with different parameters. For each value of the parameters \u03b8 we perform the suggested LWEA and LWGP methods 20 times, whereby the total ensemble of the base clusterings is drawn randomly from the base cluster pool, and give their average NMI values with different parameters in Table III and Table IV. As can be seen in Table III and Table IV, the proposed LWEA and LWGP methods provide consistent cluster performance with respect to NMI with different values from economy to the benchmark data sets. Empirically it is proposed that the parameter Africa be set to moderate values, e.g. in the interval of the following paper [1] for the benchmark 0.4."}, {"heading": "C. Comparison against Base Clusterings", "text": "In this section, we compare the consensus clustering of the proposed LWEA and LWGP methods with the base clustering. For each benchmark dataset, we perform the proposed LWEA and LWGP methods 100 times each, with the average NMI values and deviations of LWEA, LWGP, and the base clustering each shown in Figure 5. The proposed methods show significant improvements over the base clustering in all ten benchmark datasets (see Figure 5), especially for the Semeion, MF, IS, MNIST, ODR, LS, and USPS datasets, the advantage of the proposed methods over the base clustering is even greater."}, {"heading": "D. Comparison against Other Ensemble Clustering Methods", "text": "In this section we compare the proposed LWEA and LWL methods with those of ensemble members, namely with CSPA [14], MCLA methods [14], weighted methods (WCT) [20], weighted evidence of clusters (WEAC) [29], graphics (GP-MGLA) and implemented measures (WKN)."}, {"heading": "E. Robustness to Ensemble Sizes M", "text": "Furthermore, we evaluate the performance of our methods and basic methods with different ensemble sizes M. For each ensemble size M, we perform the proposed methods and basic methods 20 times on each benchmark data set, selecting the ensemble of M base clusters at random. We then illustrate the average performance (over 20 runs) of different methods with different ensemble sizes in Figure 6. In the MF data set, the TOME method delivers the best performance, while LWGP and LWEA each provide the second- and third-best values. However, in all the other nine data sets, the proposed methods significantly exceed the TOME method. As shown in Figure 6, the proposed methods achieve overall the most consistent and robust performance with different ensemble sizes on the benchmark data sets compared to the basic methods. In addition, we illustrate the average performance (different methods) on ten data sets in Figure 7. In fact, Figure 7 of the average ten ensemble sizes is best on the benchmark data sets."}, {"heading": "F. Execution Time", "text": "The LR dataset consists of a total of 20,000 data objects. In testing the data size of N \u2032, we randomly select a subset of N \u2032 objects from the LR dataset and execute various methods on that subset to evaluate their execution time. As illustrated in Fig. 8, the proposed LWEA method takes 75.20 seconds to process the entire LR dataset, which is comparable to GPMGLA, but much faster than CSPA, WCT, SRS and TOME. Of the total of twelve test methods, the MCLA method is the fastest, while the proposed LWGP method is the second fastest method. The MCLA method and the proposed LWGP method take 5.31 seconds and 8.74 seconds to process the entire LR dataset, respectively."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a novel cluster cluster approach based on an ensemble-driven cluster uncertainty assessment and local weighting strategy. We propose to estimate cluster uncertainty by looking at the cluster labels of the entire ensemble on the basis of an entropic criterion and developing a new ensemble-driven cluster validity index called EKI. EKI measurement does not require access to the original data characteristics and does not make assumptions about data distribution. Subsequently, a local weighting scheme is presented to extend the conventional CA matrix into the LWCA matrix via EKI measurement. In view of the examined reliability of clusters and local diversity in exploited ensembles, we also propose two new consensus functions, called LWEA and LWGP. We have conducted extensive experiments with a variety of real data sets. Experimental results have shown the superiority of the proposed cluster formation technique compared to the state of the art."}], "references": [{"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Rival penalized competitive learning for clustering analysis, RBF net, and curve detection", "author": ["L. Xu", "A. Krzyzak", "E. Oja"], "venue": "IEEE Transactions on Neural Networks, vol. 4, no. 4, pp. 636\u2013649, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2002, pp. 849\u2013856.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, vol. 315, pp. 972\u2013976, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximate pairwise clustering for large data sets via sampling plus extension", "author": ["L. Wang", "C. Leckie", "R. Kotagiri", "J. Bezdek"], "venue": "Pattern Recognition, vol. 44, no. 2, pp. 222\u2013235, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph-based multiprototype competitive learning and its applications", "author": ["C.-D. Wang", "J.-H. Lai", "J.-Y. Zhu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 6, pp. 934\u2013946, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "SVStream: A support vector based algorithm for clustering data streams", "author": ["C.-D. Wang", "J.-H. Lai", "D. Huang", "W.-S. Zheng"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1410\u20131424, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-exemplar affinity propagation", "author": ["C.-D. Wang", "J.-H. Lai", "C.Y. Suen", "J.-Y. Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 9, pp. 2223\u20132237, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Constraint neighborhood projections for semi-supervised clustering", "author": ["H. Wang", "T. Li", "T. Li", "Y. Yang"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 5, pp. 636\u2013643, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask spectral clustering by exploring intertask correlation", "author": ["Y. Yang", "Z. Ma", "Y. Yang", "F. Nie", "H.T. Shen"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 5, pp. 1083\u20131094, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint local and global consistency on interdocument and interword relationships for co-clustering", "author": ["B.-K. Bao", "W. Min", "T. Li", "C. Xu"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 1, pp. 15\u201328, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view clustering based on belief propagation", "author": ["C.-D. Wang", "J.-H. Lai", "P.S. Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, in press, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A hybrid approach to clustering in big data", "author": ["D. Kumar", "J. Bezdek", "M. Palaniswami", "S. Rajasegarar", "C. Leckie", "T. Havens"], "venue": "IEEE Transactions on Cybernetics, in press, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cluster ensembles: A knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding median partitions using information-theoretical-based genetic algorithms", "author": ["D. Cristofor", "D. Simovici"], "venue": "Journal of Universal Computer Science, vol. 8, no. 2, pp. 153\u2013172, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving cluster ensemble problems by bipartite graph partitioning", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "Proc. of International Conference on Machine Learning (ICML), 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Combining multiple clusterings using evidence accumulation", "author": ["A.L.N. Fred", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 6, pp. 835\u2013850, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Clustering ensembles: models of consensus and weak partitions", "author": ["A. Topchy", "A.K. Jain", "W. Punch"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 12, pp. 1866\u20131881, 2005.  12", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1866}, {"title": "Clustering aggregation by probability accumulation", "author": ["X. Wang", "C. Yang", "J. Zhou"], "venue": "Pattern Recognition, vol. 42, no. 5, pp. 668\u2013675, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A link-based approach to the cluster ensemble problem", "author": ["N. Iam-On", "T. Boongoen", "S. Garrett", "C. Price"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 2396\u2013 2409, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "CA-Tree: A hierarchical structure for efficient and scalable coassociation-based cluster ensembles", "author": ["T. Wang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 41, no. 3, pp. 686\u2013698, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble clustering by means of clustering embedding in vector spaces", "author": ["L. Franek", "X. Jiang"], "venue": "Pattern Recognition, vol. 47, no. 2, pp. 833\u2013842, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "K-means-based consensus clustering: A unified view", "author": ["J. Wu", "H. Liu", "H. Xiong", "J. Cao", "J. Chen"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 1, pp. 155\u2013169, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient clustering aggregation based on data fragments", "author": ["O. Wu", "W. Hu", "S. Maybank", "M. Zhu", "B. Li"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42, no. 3, pp. 913\u2013926, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive noise immune cluster ensemble using affinity propagation", "author": ["Z. Yu", "L. Li", "J. Liu", "J. Zhang", "G. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 12, pp. 3176\u20133189, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A theoretic framework of k-meansbased consensus clustering", "author": ["J. Wu", "H. Liu", "H. Xiong", "J. Cao"], "venue": "Proc. of International Joint Conference on Artificial Intelligence, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Weighted consensus clustering", "author": ["T. Li", "C. Ding"], "venue": "Proc. of SIAM International Conference on Data Mining (SDM), 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Hybrid clustering solution selection strategy", "author": ["Z. Yu", "L. Li", "Y. Gao", "J. You", "J. Liu", "H.-S. Wong", "G. Han"], "venue": "Pattern Recognition, vol. 47, no. 10, pp. 3362\u20133375, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining multiple clusterings via crowd agreement estimation and multi-granularity link analysis", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Neurocomputing, vol. 170, pp. 240\u2013250, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A clustering ensemble: Two-level-refined co-association matrix with path-based transformation", "author": ["C. Zhong", "X. Yue", "Z. Zhang", "J. Lei"], "venue": "Pattern Recognition, vol. 48, no. 8, pp. 2699\u2013 2709, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "On the point for which the sum of the distances to n given points is minimum", "author": ["E. Weiszfeld", "F. Plastria"], "venue": "Annals of Operations Research, vol. 167, no. 1, pp. 7\u201341, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "To improve the quality of cluster ensembles by selecting a subset of base clusters", "author": ["H. Alizadeh", "B. Minaei-Bidgoli", "H. Parvin"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence, vol. 26, no. 1, pp. 127\u2013150, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble clustering using factor graph", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Pattern Recognition, vol. 50, pp. 131\u2013142, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Elements of Information Theory, 2nd ed", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Robust ensemble clustering by matrix completion", "author": ["J. Yi", "T. Yang", "R. Jin", "A.K. Jain"], "venue": "Proc. of IEEE International Conference on Data Mining (ICDM), 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting the wisdom of crowd: A multi-granularity approach to clustering ensemble", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Proc. of International Conference on Intelligence Science and Big Data Engineering (IScIDE), 2013, pp. 112\u2013119.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Weighted partition consensus via kernels", "author": ["S. Vega-Pons", "J. Correa-Morris", "J. Ruiz-Shulcloper"], "venue": "Pattern Recognition, vol. 43, no. 8, pp. 2712\u20132724, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Weighted association based methods for the combination of heterogeneous partitions", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper", "A. Guerra-Gand\u00f3n"], "venue": "Pattern Recognition Letters, vol. 32, no. 16, pp. 2163\u20132170, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 25, no. 3, pp. 337\u2013372, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining multiple clusterings using similarity graph", "author": ["S. Mimaroglu", "E. Erdil"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 694\u2013703, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation using superpixels: A bipartite graph partitioning approach", "author": ["Z. Li", "X.-M. Wu", "S.-F. Chang"], "venue": "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Affinity propagation: Clustering data by passing messages", "author": ["D. Dueck"], "venue": "Ph.D. dissertation, University of Toronto, 2009.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION DATA clustering is a fundamental yet very challenging problem in the field of data mining and machine learning [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 152, "endOffset": 155}, {"referenceID": 8, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 11, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 221, "endOffset": 225}, {"referenceID": 14, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 227, "endOffset": 231}, {"referenceID": 15, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 16, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 245, "endOffset": 249}, {"referenceID": 18, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 251, "endOffset": 255}, {"referenceID": 19, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 257, "endOffset": 261}, {"referenceID": 20, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 263, "endOffset": 267}, {"referenceID": 21, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 269, "endOffset": 273}, {"referenceID": 22, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 275, "endOffset": 279}, {"referenceID": 23, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 281, "endOffset": 285}, {"referenceID": 24, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 287, "endOffset": 291}, {"referenceID": 25, "context": "Ensemble clustering aims to combine multiple clusterings to obtain a probably better and more robust clustering result, which has shown advantages in finding bizarre clusters, dealing with noise, and integrating clustering solutions from multiple distributed sources [26].", "startOffset": 267, "endOffset": 271}, {"referenceID": 26, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 157, "endOffset": 161}, {"referenceID": 27, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 163, "endOffset": 167}, {"referenceID": 28, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 29, "context": "[30] proposed to evaluate the reliability of clusters by considering the Euclidean distances between data objects in clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The method in [30] requires access to the original data features, and its efficacy heavily relies on the data distribution of the dataset.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "By exploiting the CA matrix as the similarity matrix, the conventional clustering techniques, such as the agglomerative clustering methods [1], can be exploited to build the final clustering result.", "startOffset": 139, "endOffset": 142}, {"referenceID": 16, "context": "Fred and Jain [17] for the first time presented the concept of CA matrix and proposed the evidence accumulation clustering (EAC) method.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "[19] extended the EAC method by taking the sizes of clusters into consideration, and proposed the probability accumulation method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] refined the CA matrix by considering the shared neighbors between clusters to improve the consensus results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Wang [21] introduced a dendrogram-like hierarchical data structure termed CA-tree to facilitate the co-association based ensemble clustering process.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "The graph partitioning based approaches [14], [16] address the ensemble clustering problem by constructing a graph model to reflect the ensemble information.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "The graph partitioning based approaches [14], [16] address the ensemble clustering problem by constructing a graph model to reflect the ensemble information.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Strehl and Ghosh [14] proposed three graph partitioning based ensemble clustering algorithms, i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Fern and Brodley [16] constructed a bipartite graph for the clustering ensemble by treating both clusters and objects as graph nodes, and obtain the consensus clustering by partitioning the bipartite graph.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "The median partition problem is NPhard [18].", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Cristofor and Simovici [15] proposed to obtain an approximate solution using the genetic algorithm, where clusterings are treated as chromosomes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18] cast the median partition problem into a maximum likelihood problem and approximately solve it by the EM algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Franek and Jiang [22] cast the median partition problem into an Euclidean median problem by clustering embedding in vector spaces.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "The median vector is found by the Weiszfeld algorithm [31] and then transformed into a clustering again, which is treated as the consensus clustering.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "Li and Ding [27] cast the ensemble clustering problem into a nonnegative matrix factorization problem and proposed a weighted consensus clustering approach, where each base clustering is assigned a weight in order to improve the consensus result.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "[28] exploited the feature selection techniques to weight and select the base clusterings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In fact, clustering selection [28] can be viewed as a 0-1 weighting scheme, where 1 indicates selecting a clustering and 0 indicates removing a clustering.", "startOffset": 30, "endOffset": 34}, {"referenceID": 28, "context": "[29] proposed to evaluate and weight the base clusterings based on the normalized crowd agreement index (NCAI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "[32] proposed to evaluate clusters in the ensemble by averaging normalized mutual information (NMI) [14] between clusterings, which results in a very expensive computational cost and is not feasible for large datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[32] proposed to evaluate clusters in the ensemble by averaging normalized mutual information (NMI) [14] between clusterings, which results in a very expensive computational cost and is not feasible for large datasets.", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "[30] exploited the Euclidean distances between objects to estimate the cluster reliability, which needs access to the original data features and is only applicable to numerical data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "Moreover, by measuring the within-cluster similarity based on Euclidean distances, the efficacy of the method in [30] heavily relies on some implicit assumptions about data distribution, which places an unstable factor in the consensus process.", "startOffset": 113, "endOffset": 117}, {"referenceID": 29, "context": "Different from [30], in this paper, our ensemble clustering approach requires no access to the original data features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "In information theory [34], the entropy is a measure of the uncertainty associated with a random variable.", "startOffset": 22, "endOffset": 26}, {"referenceID": 33, "context": "Hence, given n independent random variables X1, \u00b7 \u00b7 \u00b7 , Xn, we have [34] H(X1, \u00b7 \u00b7 \u00b7 , Xn) = H(X1) + \u00b7 \u00b7 \u00b7+H(Xn).", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 161, "endOffset": 165}, {"referenceID": 15, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 179, "endOffset": 183}, {"referenceID": 18, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 197, "endOffset": 201}, {"referenceID": 21, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 203, "endOffset": 207}, {"referenceID": 32, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 209, "endOffset": 213}, {"referenceID": 34, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 215, "endOffset": 219}, {"referenceID": 35, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 221, "endOffset": 225}, {"referenceID": 29, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 36, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 145, "endOffset": 149}, {"referenceID": 37, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "In this paper, we comply with the first formulation of the ensemble clustering problem, which is also the common practice for most of the existing ensemble clustering approaches [39].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "Because p(Ci, C j ) \u2208 [0, 1] for any i, j, m, it is obvious that H(Ci) \u2208 [0,+\u221e).", "startOffset": 22, "endOffset": 28}, {"referenceID": 38, "context": "Without loss of generality, based on the assumption that the base clusterings in the ensemble are independent [39], the uncertainty (or entropy) of Ci w.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Refining Co-association Matrix by Local Weighting The co-association (CA) matrix is first proposed by Fred and Jain [17], which reflects how many times two data objects are grouped into the same cluster among the multiple base clusterings in the ensemble.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 111, "endOffset": 115}, {"referenceID": 39, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "[29] exploited the NCAI index to weight the base clusterings and thereby construct a weighted co-association (WCA) matrix, which, however, only considers the reliability of base clusterings, but still neglects the cluster-wise diversity inside the same base clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Different from the (globally) weighting strategy [29] that treats each base clustering as a whole, in this section, we refine the CA matrix by a local weighting strategy based on the ensemble-driven cluster validity and propose the concept of locally weighted co-association (LWCA) matrix.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Hierarchical agglomerative clustering is a widely used clustering technique [1], which typically takes a similarity matrix as input and performs region merging iteratively to achieve a dendrogram, i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Typically, we adopt the average-link (AL), which is a classical agglomerative clustering method [1], to update the similarity matrix for the t-step.", "startOffset": 96, "endOffset": 99}, {"referenceID": 40, "context": "Having constructed the LWBG according to Definition 8, we proceed to partition the graph using the Tcut algorithm [41], which is able to take advantage of the bipartite graph structure to greatly facilitate the computation of the graph partitioning process.", "startOffset": 114, "endOffset": 118}, {"referenceID": 40, "context": "4: Partition the LWBG into a certain number of segments using the Tcut algorithm [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 41, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 61, "endOffset": 65}, {"referenceID": 42, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 70, "endOffset": 74}, {"referenceID": 43, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "We use the normalized mutual information (NMI) [14] to evaluate the quality of clusterings.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 240, "endOffset": 244}, {"referenceID": 16, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 250, "endOffset": 254}, {"referenceID": 19, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 345, "endOffset": 349}, {"referenceID": 28, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 400, "endOffset": 404}, {"referenceID": 28, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 472, "endOffset": 476}, {"referenceID": 29, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 537, "endOffset": 541}], "year": 2017, "abstractText": "Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, without access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art.", "creator": "TeX"}}}