{"id": "1605.05087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Word2Vec is a special case of Kernel Correspondence Analysis and Kernels for Natural Language Processing", "abstract": "We show Correspondence Analysis (CA) is equivalent to defining Gini-index with appropriate scaled one-hot encoding. Using this relation, we introduce non-linear kernel extension of CA. The extended CA gives well-known analysis for categorical data (CD) and natural language processing by specializing kernels. For example, our formulation can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as a special case. We introduce two kernels for natural language processing based on our formulation. First is a stop word(SW) kernel. Second is word similarity(WS) kernel. The SW kernel is the system introducing appropriate weights for SW. The WS kernel enables to use WS test data as training data for vector space representations of words. We show these kernels enhances accuracy when training data is not sufficiently large.", "histories": [["v1", "Tue, 17 May 2016 10:07:34 GMT  (23kb)", "https://arxiv.org/abs/1605.05087v1", null], ["v2", "Sun, 14 Aug 2016 19:13:22 GMT  (21kb)", "http://arxiv.org/abs/1605.05087v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["hirotaka niitsuma", "minho lee"], "accepted": false, "id": "1605.05087"}, "pdf": {"name": "1605.05087.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hirotaka Niitsuma"], "emails": ["niitsuma@cs.okayama-u.ac.jp", "mholee@knu.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.05 08"}, {"heading": "1 Introduction", "text": "In fact, these two methods are analyses of a correlation or variance-covariance matrix of a multivariate quantitative dataset. CA is also an analysis of eye and hair color to represent the associations between the levels of a two-way contingency table. One could look at the contingency table in Table 1, which shows the data of Fisher. [4] For the eye and hair color of people in Caithness, Scotland. The CA of these data provide the basic idea of a two-way contingency table."}, {"heading": "2 Correspondence Analysis", "text": "In fact, CA is a generalized singular value decomposition (GSVD) 1 based on a contingency table for two categorical variables such as U, S, V, V, T, N, N, Rct, N2 (2) U, tD (r) \u2212 1U, V, tD (c) \u2212 1V, E (3) where N is a contingency table, with entries nij, which indicate the frequency with the series categorical variables xr = i along with the column categorical variable xc = j. In addition, xr is that categorical variable representing the row side of the contingency table. xc, x, r, the category table, xc, x, c, 2, c, nc, 2, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, xoric, c, c, c, c, c, c, c, xc, c, c, c, c, c, c, c, c, c, c, c, xc, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, xc, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, xoric, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "3 Gini\u2019s definition of Variance and its Extension", "text": "A contingency table can by means of the matrix product of two indicator matrices of the person b = HrtHc (7) whereHr = [er (xr (a1))), e r (xr (a2)),..., e r (xr (an)) t Hc = [ec (xc (a1), e c (xc (a2))), e \u2212 \u2212 \u2212 \u2212 ec (8) t (8) Hr is the n \u00b7 nr indicator of the person xr. er (xr) is one-hot encoding of xr. a1..., an represent the observations of xr (ab) is the value of b-th observation. Hc is an n \u00b7 nc indicator indicator of xr. er (xr) er (xr) is one-hot encoding of xr. er (xr) is one-hot encoding of xr. a1..., an represent the observations. xr (ab) is the value of xr (b-th observation."}, {"heading": "4 Non-linear extension", "text": "This section is an attempt to generalize CA as much as possible, while the relationship between Gini index and Y (= 31) is useful. (Let's introduce the nonlinear mapping of systems on nonlinear mapped spaces: r, c, c, s, s (29) with this nonlinear extension yields the following.maximize R14n2 (e) c c (xc (b))))) r (e r (e) r (e) r (xr (b)))))) tR (e c (xc) c c c c (xc) c (b))))) r (e r (xr1)))) r (e r (e r (e) r (xr2)))))),... tR (xr2), c (e c (e c (e) c (e) c (e c (e c)))."}, {"heading": "5 Kernels for Natural language processing", "text": "In this section, we will examine specific vector representations of words. We will consider the following contingency table: N = [nij] = [# (wi, cj) (44), where # (w, c) is the number of times the word w appears in context c. Using this contingency table, we can formulate SGNS as a specialized KCA, as in Table 2. Linear CA and G-Test also provide vector representations of words using the contingent table. KCA can provide tunable weights for stopwords (SW) using the following kernel asKr = D (StopWordVector (\u03b1w)). D (r) \u2212 1 Kc = D (StopWordVector (\u03b1w)."}, {"heading": "6 Experiments", "text": "To calculate the contingency table N = [nij] = [# (wi, cj)], we use the program code2, which is used in [7]. For the SW kernel, we use stopwords set in NLTK [1]. For the WS, we use the score of the MEN dataset [2], that of Bruni et al. Firstly, 20% data of the Text8 corpus have a small effect if we use the entire Text8 corpus3 as training text data. The stopword and word similarity data are too small compared to the entire Text8 corpus. Consequently, the stopword core and the word similarity core have a small effect if we use the entire Text8 corpus.For evaluation, we use the WordSim3533 dataset [3], the MEN dataset [2] by Bruni et al."}, {"heading": "7 Conclusion", "text": "We show that linear correspondence analysis (CA) is equivalent to defining the Gini index with rotated and scaled one-hot encoding. In addition, we have tried to generalize CA as much as possible to maintain the relationship between the Gini index and CA. Kernel Correspondence Analysis (KCA) is introduced based on CA's nonlinear generalization. KCA provides various known analyses of categorical data and natural language processing by specializing kernels. For example, KCA can provide G-Test, Skip-gram with negative sampling (SGNS) and GloVe as a special case. We introduce two kernels for processing natural language based on KCA. The proposed mechanism is evaluated based on the problem of vector representations of words. Although linear CA and G-Test do not have adjustable parameters, these results are comparable to those for SGNS. Furthermore, we show that we can improve the parameters with uncapable ones."}], "references": [{"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.K. Tran"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136\u2013145, Jeju Island, Korea, July", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. Inf. Syst., 20(1):116\u2013131, Jan.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "The precision of discriminant functions", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics (London), 10:422\u2013429,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1940}, {"title": "Variability and mutability, contribution to the study of statistical distributions and relations", "author": ["C. Gini"], "venue": "studi economico-giuridici della r. universita de cagliari (1912). reviewed in: Light, r.j., margolin, b.h.: An analysis of variance for categorical data. J. American Statistical Association, 66:534\u2013 544,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1971}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8- 13 2014, Montreal, Quebec, Canada, pages 2177\u20132185,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "TACL, 3:211\u2013225,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111\u2013 3119. Curran Associates, Inc.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel PCA for categorical data", "author": ["H. Niitsuma", "T. Okada"], "venue": "IE- ICE technical report. Artificial intelligence and knowledge-based processing, 103(305):13\u201317, sep", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Covariance and PCA for categorical variables", "author": ["H. Niitsuma", "T. Okada"], "venue": "Advances in Knowledge Discovery and Data Mining, 9th Pacific-Asia Conference, PAKDD 2005, Hanoi, Vietnam, May 18-20, 2005, Proceedings, pages 523\u2013528,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "A note on covariances for categorical data", "author": ["T. Okada"], "venue": "K. Leung, L. Chan, and H. Meng, editors, Intelligent Data Engineering and Automated Learning - IDEAL 2000,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-linear correspondence analysis in text retrieval: A kernel view", "author": ["D. Picca", "B. Curdy", "F. Bavaud"], "venue": "Proceedings of JADT,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A word at a time: Computing word relatedness using temporal semantic analysis", "author": ["K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch"], "venue": "Proceedings of the 20th International Conference on World Wide Web, WWW \u201911, pages 337\u2013346, New York, NY, USA,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "To illustrate CA, one might consider the contingency table presented in Table 1, which shows data well known as Fisher\u2019s data[4] for eye and hair color of people in Caithness, Scotland.", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Okada [11, 10] reported that defining the Giniindex using one-hot encoding on appropriately rotated space yields reasonable values for the covariance.", "startOffset": 6, "endOffset": 14}, {"referenceID": 9, "context": "Okada [11, 10] reported that defining the Giniindex using one-hot encoding on appropriately rotated space yields reasonable values for the covariance.", "startOffset": 6, "endOffset": 14}, {"referenceID": 12, "context": "[13] introduced nonlinear kernel extension of CA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] introduced nonlinear kernel extension of the rotated Gini-index.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.", "startOffset": 77, "endOffset": 83}, {"referenceID": 11, "context": "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "[8] introduced dense vector representations of words referred to as word2vec.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] showed that skip-gram with negative-sampling (SGNS), which is one of word2vec, can be regarded as analysis of the two-way contingency table in which the row represents a word, and the column represents context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] showed that SGNS is the matrix factorization of pointwise mutual information (PMI) of the contingency table.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Gini-index is the variance of a categorical variable[5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 10, "context": "A simple extension of this definition to the covariance \u03c3(x , x) by replacing (x(a)\u2212 x(b)) to (x(a)\u2212 x(b))(x(a)\u2212 x(b)) does not give reasonable values for the covariance \u03c3(x, x) [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 10, "context": "Okada[11, 10] shown using rotated one-hot encoding provides reasonable values for the covariance of CD: \u03c3(x , x) =maximize R 1 2n2 n", "startOffset": 5, "endOffset": 13}, {"referenceID": 9, "context": "Okada[11, 10] shown using rotated one-hot encoding provides reasonable values for the covariance of CD: \u03c3(x , x) =maximize R 1 2n2 n", "startOffset": 5, "endOffset": 13}, {"referenceID": 10, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 64, "endOffset": 72}, {"referenceID": 9, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 64, "endOffset": 72}, {"referenceID": 7, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 127, "endOffset": 133}, {"referenceID": 11, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "Name K K X \u2296 Y X \u2295 Y Linear CA D(r) D(c) X \u2212 Y X + Y Gini-index [11, 10] E E X \u2212 Y X + Y G-test E E X(logX \u2212 log Y ) X + Y SGNS[8, 6] E E (logX \u2212 log Y \u2212 log k) X + Y GloVe[12] E E (logX \u2212 log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )\u2212e r(xrj )| 2 E X \u2212 Y X + Y", "startOffset": 227, "endOffset": 230}, {"referenceID": 6, "context": "[7] showed GloVe[12] is matrix factorization of sifted PMI.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[7] showed GloVe[12] is matrix factorization of sifted PMI.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Based on that discussion, one can regard GloVe[12] as that of the KCA, as shown in Table 2.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "For example[3], score(movie, theater) = 7.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "To calculate the contingency table N = [nij ] = [#(wi, cj)], we use the program code 2 used in [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "For the SW kernel, we use stop words set in NLTK[1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "For WS, we use the score of the MEN dataset[2] reported by Bruni et al.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "For evaluation, we use WordSim3533 dataset[3], the MEN dataset[2] of Bruni et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "For evaluation, we use WordSim3533 dataset[3], the MEN dataset[2] of Bruni et al.", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": "and the Mechanical Turk dataset[14] reported by Radinsky et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.", "startOffset": 47, "endOffset": 50}], "year": 2016, "abstractText": "We show that Correspondence Analysis (CA) is equivalent to defining the Gini-index with appropriately scaled one-hot encoding. Using this relation, we introduce nonlinear kernel extension of CA. The extended CA gives well-known analysis for categorical data (CD) and natural language processing by specializing kernels. For example, our formulation can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as special cases. We introduce two kernels for natural language processing based on our formulation: a stop word (SW) kernel and a word similarity (WS) kernel. The SW kernel is a system introducing appropriate weights for SW. The WS kernel enables the use of WS test data as training data for vector space representations of words. We show that these kernels enhance accuracy when training data are insufficiently numerous.", "creator": "LaTeX with hyperref package"}}}