{"id": "1605.05110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Incorporating Loose-Structured Knowledge into Conversation Modeling via Recall-Gate LSTM", "abstract": "Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems.", "histories": [["v1", "Tue, 17 May 2016 11:03:25 GMT  (548kb)", "http://arxiv.org/abs/1605.05110v1", "10 pages, 5 figures"], ["v2", "Mon, 6 Feb 2017 03:43:17 GMT  (399kb)", "http://arxiv.org/abs/1605.05110v2", "under review of IJCNN 2017; 10 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhen xu", "bingquan liu", "baoxun wang", "chengjie sun", "xiaolong wang"], "accepted": false, "id": "1605.05110"}, "pdf": {"name": "1605.05110.pdf", "metadata": {"source": "CRF", "title": "Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling", "authors": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang"], "emails": ["wangxl}@insun.hit.edu.cn", "baoxwang@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.05 110v 1 [cs.C L] 17 May 201 6"}, {"heading": "1 Introduction", "text": "In recent years, the demand for chat bots has shifted from answering simple questions to conducting glass conversations with real people."}, {"heading": "2 Related Work", "text": "Previous studies on conversation modeling have focused mainly on task accomplishment, such as file classification, state tracking in the spoken field (Jurc\u0131cek et al., 2011; Williams et al., 2013; Henderson et al., 2013).With the rapid accumulation of available conversation data from SNS (e.g. Twitter3, Weibo4), data-driven tasks such as response ranking or generation of answers have attracted more attention in this area (Serban et al., 2015b).Early studies on conversation modeling focus on a conversation, including a query and an answer, whereby, hypocritically, information retrieval (IR) or statistical machine translation (SMT) is based on methods for obtaining answers. IR systems are based on film scripts (Banchs and Li, 2012) or subtitles (Ameixa et al., 2014) to find answers to the given questions."}, {"heading": "3 LSTM with Recall Gate for Conversation Modeling", "text": "As mentioned in Section 1, our motivation for conversation modeling is to provide reliable evidence to identify context-conscious queries from users and to select the best answers based on conversation history to build automated dialogue systems. Obviously, utterance 5 in a conversation is likely to be semantically relevant, both to the historical context and background knowledge; the context consists of the sequence of utterances that appear in the conversation before the query and answer; the semantic relevance between utterances implicitly acts as a guide for conversation modeling, while background knowledge provides essential clues to enhance the impact of semantic cues. In this section, we propose an in-depth learning framework, using the specially designed Recall Gate to smoothly integrate knowledge cues into LSTM to capture such semantic cues. 5In this essay, utterance is used as an alias of the phrase in conversations."}, {"heading": "3.1 Architecture", "text": "In fact, it is as if most of them are able to survive themselves if they do not put themselves in the position they are in. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...). (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...)"}, {"heading": "3.2 LSTM Cell with Recall Gate", "text": "In fact, integrating knowledge into deep learning architectures is still a challenge. Because knowledge is indeed a global signal, and it is unwise simply to include knowledge as an additional expression in conversations supported by the experimental results in Section 4. In essence, our model absorbs the knowledge of embedding knowledge in the trigger results described in Subsection 3.1, and drives it to reactivate the details of the LSTM cell with the recall gate (r-LSTM cell)."}, {"heading": "3.3 Loose-Structured Knowledge Base", "text": "It is well known that building a complex structured knowledge (e.g. WordNet6, Yago7) requires a large amount of human work. Obviously, a lightly structured knowledge base is quite valuable for applications. In this paper, a loosely structured knowledge base is introduced consisting of elements with a flexible format \"entity attribute,\" in which the attributes can be either other entities or the associated keywords. The extraction process of entity-attribute pairs is described as follows: (1) For a particular domain, we extract entity or attributes from the domain-specific corpus using statistical metrics such as tf-idf, entropy, etc. Then, the KL divergence between domain-specific and general corpus is filtered out by filtering entities or attributes from the domain-specific corpus using statistical metrics such as entropy, etc."}, {"heading": "4 Experiment", "text": "In this part, our approach to conversation modeling is evaluated using contextual selection tasks that are considered a binary classification problem, as illustrated in Figure 2, i.e. the goal of our model is to give more confidence to contextual responses."}, {"heading": "4.1 Experimental Setup", "text": "It is not as if we are able to generate and test the results of our training. For every positive sample in training, a negative sample is prepared, for every positive sample in testing and validation, there are 9 negative ones. Finally, we construct a training set with 1 million conversations for both Tie and Ubuntu, and we test 50,000 conversations for validation and verification."}, {"heading": "4.2 Results and Analysis", "text": "In fact, it is such that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "4.3 Case Study", "text": "In order to intuitively demonstrate the working mechanism in our model, we give two cases in which we compare our responses to r-LSTM and LSTM + kb in Table 4. For a better understanding, we translate both the contexts and candidates \"responses from Chinese into English. As Table 4 shows, each case contains the context consisting of three utterances and three candidates\" responses with labels and predicted results. Label 1 shows the true answer to the given context. Values represent the trust of the answers as the best. The table shows that r-LSTM gives the best answer, while \"LSTM + kb\" provides unsatisfactory results. Both methods, as shown in 4, provide comprehensive background knowledge, so that this result is caused by the effectiveness of the use of knowledge. In our r-LSTM architecture, background knowledge is considered as a global memory and can be remembered in conversation, and our All Recate could also effectively build semantic relationships of expression."}, {"heading": "5 Conclusions and Future Work", "text": "Our approach shows good potential for the context-based response selection task. The contributions of this paper can be summarized as follows: (1) By investigating the impact of domain knowledge on conversation understanding, we present an LSTM framework with a designed recall gate to leverage knowledge smoothly and effectively; and by transforming knowledge into global memory, the recall gate LSTM enables global memory to be integrated into sequential local memory to perform enhanced conversation modeling; (2) To ensure the flexibility of the domain knowledge base for practical application, this paper introduces the loosely structured knowledge base, organized as entity-attribute pairs, which can be easily extracted from raw corpora; knowledge can be absorbed directly from the recall gate after it has been triggered according to dialogue contexts; and our future study will be conducted along the following lines: First, we will continue to reconstruct the working mechanism of the global conversation mechanism to forge our conversation."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems.", "creator": "LaTeX with hyperref package"}}}