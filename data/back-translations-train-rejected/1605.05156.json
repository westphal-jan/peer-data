{"id": "1605.05156", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Tweet Acts: A Speech Act Classifier for Twitter", "abstract": "Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. In this paper, we explored speech act recognition on Twitter by treating it as a multi-class classification problem. We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. We trained and tested a logistic regression classifier using a data set of manually labelled tweets. Our method achieved a state-of-the-art performance with an average F1 score of more than $0.70$. We also explored classifiers with three different granularities (Twitter-wide, type-specific and topic-specific) in order to find the right balance between generalization and overfitting for our task.", "histories": [["v1", "Tue, 17 May 2016 13:31:14 GMT  (181kb,D)", "http://arxiv.org/abs/1605.05156v1", "ICWSM'16, May 17-20, Cologne, Germany. In Proceedings of the 10th AAAI Conference on Weblogs and Social Media (ICWSM 2016). Cologne, Germany"]], "COMMENTS": "ICWSM'16, May 17-20, Cologne, Germany. In Proceedings of the 10th AAAI Conference on Weblogs and Social Media (ICWSM 2016). Cologne, Germany", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["soroush vosoughi", "deb roy"], "accepted": false, "id": "1605.05156"}, "pdf": {"name": "1605.05156.pdf", "metadata": {"source": "CRF", "title": "Tweet Acts: A Speech Act Classifier for Twitter", "authors": ["Soroush Vosoughi", "Deb Roy"], "emails": ["soroush@mit.edu,", "dkroy@media.mit.edu"], "sections": [{"heading": "Introduction", "text": "In recent years, the micro-blogging platform Twitter has become an important social media platform with hundreds of millions of users, people turning to Twitter for a variety of purposes, from daily chatter to reading breaking news. Twitter's volume plus the public nature (less than 10% of Twitter accounts are private (Moore 2009)) have made Twitter a great source of data for social and behavioural studies, which often require an understanding of what people are tweeting about. Although this can be manually coded to take advantage of the volume of tweets available, automated analysis methods must be used. Extensive work has been done on computerised methods of analysing the linguistic content of tweets. However, there has been very little work on classifying the pragmatism of tweets. Pragmatism goes beyond the literal meaning of an utterance and looks at how context and intention contribute to meaning. An important element of the pragmatics is the intended action of an utterance, or what should be conventional utterance."}, {"heading": "Problem Statement", "text": "To create such a dataset, we first created a taxonomy of the language file for Twitter by identifying and defining a number of common language files. Next, we manually commented on a large collection of tweets using our taxonomy. Our primary task was to use the expertly annotated dataset to analyze and select various syntactic and semantic characteristics derived from tweets predicting their corresponding language files. Using our designated dataset and robust features, we trained standard classifiers (such as SVMs, Naive Bayes, etc.) for our task of language character recognition. Using Searle's taxonomy of the language file (Searle 1976), we created a list of six categories of language files commonly seen on Twitter: Assertion, Recommendation Expression, Question Request, and Miscelleous Taxonomy (Searle 1976)."}, {"heading": "Data Collection and Datasets", "text": "Given the variety of topics discussed on Twitter, we wanted to examine topics and type-dependent speech act classifiers (e.g. Boston Marathon bombings, Red Sox, etc.), which characterizes the type of topic, which are: holistic, event-oriented topics and long-standing topics (topics on topics that are frequently discussed). We selected two topics for a total of six topic types for each of the three topic types described in the last section (see Figure 1 for a list of topics). We collected a few thousand tweets from the Twitter Public API for each of these topics using topic-specific queries (e.g. # fergusonriots, # redsox, etc.) types of speech files that contain a total of six topic types (see Figure 1 for a list of topics)."}, {"heading": "Features", "text": "Our characteristics can be divided into two general categories: semantic and syntactic characteristics. Some of these characteristics have been motivated by various work on classification of language files, while others are new characteristics. In total, we selected 3313 binary characteristics consisting of 1647 semantic and 1666 syntactic characteristics."}, {"heading": "Semantic Features", "text": "It is a data set commonly used in sentiment classification tasks to identify 2,442 strong, negative and positive words of opinion (such as robust, terrible, untrustworthy, etc.). The intuition here is that these words of opinion tend to signal certain speech acts such as expressions and recommendations. A binary feature indicates whether one of these words appears in a tweet. Vulgar words: Similar to opinion words, vulgar words can signal either great emotion or informality, which is more commonly seen in expressions than any other type of speech act (at least in statements). We have used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack of these words. Emoticons: Emoticons have become ubiquitous in online communication and cannot be ignored."}, {"heading": "Syntactic Features", "text": "Specifically, punctuation can signal? a question or request, while! can signal an expression or recommendation. We have two binary characters that indicate the appearance or absence of these symbols.Twitter-specific characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT. The position of these characters is also important because Twitter-specific characters used in the starting position of a tweet are more predictive than in other positions. Therefore, we have three additional binary characters that indicate whether these symbols appear in the starting position. Abbreviations: Abbreviations are seen with great frequency in online communication. Use of abbreviations (such as b4 for before, jk for jokes only, and irl for real life) can signal activities that indicate informal language."}, {"heading": "Supervised Speech Act Classifier", "text": "We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers on three granularities: Twitter-wide, type-specific, and topic-specific. All of our classifiers are evaluated with a 20-fold cross-validation. Table 2 shows the performance of our five classifiers, who are trained and evaluated on all data. We report the F1 score for each class. As shown in Table 2, the logistic regression of the powerful classifiers was with a weighted average F1 score of.70 So we chose logistic regression as our classifier and the rest of the results reported is for LR only. Table 3 shows the average performance of the LR classifiers for Twitter-wide, type, and topic-specific."}, {"heading": "Conclusions and Future Work", "text": "We treated the classification of language files on Twitter as a multi-class classification problem and developed a taxonomy of language files on Twitter with six distinct classes. We then proposed a set of semantic and syntactic characteristics for classifying Twitter language files. Using these characteristics, we were able to achieve a state-of-the-art performance for classifying Twitter language files, with an average F1 value of 0.70. Language file classification has many applications; for example, we used our classifier to detect rumors on Twitter in a companion paper (Vosoughi and Roy 2016)."}], "references": [{"title": "J", "author": ["Austin"], "venue": "L.", "citeRegEx": "Austin 1962", "shortCiteRegEx": null, "year": 1962}, {"title": "N", "author": ["L. Kong", "N. Schneider", "S. Swayamdipta", "A. Bhatia", "C. Dyer", "Smith"], "venue": "A.", "citeRegEx": "Kong et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Moore"], "venue": "J.", "citeRegEx": "Moore 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved part-ofspeech tagging for online conversational text with word clusters", "author": ["Owoputi"], "venue": null, "citeRegEx": "Owoputi,? \\Q2013\\E", "shortCiteRegEx": "Owoputi", "year": 2013}, {"title": "M", "author": ["Porter"], "venue": "F.", "citeRegEx": "Porter 1980", "shortCiteRegEx": null, "year": 1980}, {"title": "J", "author": ["Searle"], "venue": "R.", "citeRegEx": "Searle 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "J", "author": ["Searle"], "venue": "R.", "citeRegEx": "Searle 1976", "shortCiteRegEx": null, "year": 1976}, {"title": "Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics 26(3):339\u2013373", "author": ["Stolcke"], "venue": null, "citeRegEx": "Stolcke,? \\Q2000\\E", "shortCiteRegEx": "Stolcke", "year": 2000}, {"title": "M", "author": ["P. Stone", "D.C. Dunphy", "Smith"], "venue": "S.; and Ogilvie, D.", "citeRegEx": "Stone et al. 1968", "shortCiteRegEx": null, "year": 1968}, {"title": "and Roy", "author": ["S. Vosoughi"], "venue": "D.", "citeRegEx": "Vosoughi and Roy 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Roy", "author": ["S. Vosoughi"], "venue": "D.", "citeRegEx": "Vosoughi and Roy 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Jiang", "author": ["X. Zhao"], "venue": "J.", "citeRegEx": "Zhao and Jiang 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. In this paper, we explored speech act recognition on Twitter by treating it as a multi-class classification problem. We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. We trained and tested a logistic regression classifier using a data set of manually labelled tweets. Our method achieved a state-of-the-art performance with an average F1 score of more than 0.70. We also explored classifiers with three different granularities (Twitter-wide, type-specific and topic-specific) in order to find the right balance between generalization and overfitting for our task.", "creator": "LaTeX with hyperref package"}}}