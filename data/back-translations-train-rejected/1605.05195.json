{"id": "1605.05195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Enhanced Twitter Sentiment Classification Using Contextual Information", "abstract": "The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research.", "histories": [["v1", "Tue, 17 May 2016 14:51:54 GMT  (959kb,D)", "http://arxiv.org/abs/1605.05195v1", "EMNLP 2015 workshop, WASSA 2015, Lisbon, Portugal. In proceedings of the IEEE ICDM 2015 workshop on Event Analytics using Social Media Data (EASM). Atlantic City, New Jersey"]], "COMMENTS": "EMNLP 2015 workshop, WASSA 2015, Lisbon, Portugal. In proceedings of the IEEE ICDM 2015 workshop on Event Analytics using Social Media Data (EASM). Atlantic City, New Jersey", "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.CL cs.IR", "authors": ["soroush vosoughi", "helen zhou", "deb roy"], "accepted": false, "id": "1605.05195"}, "pdf": {"name": "1605.05195.pdf", "metadata": {"source": "CRF", "title": "Enhanced Twitter Sentiment Classification Using Contextual Information", "authors": ["Soroush Vosoughi", "Helen Zhou", "Deb Roy"], "emails": ["soroush@mit.edu", "hlzhou@mit.edu", "dkroy@media.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "We are a micro-blogging platform and a social network where users can post and exchange sentimental messages of up to 140 characters long (also known as tweets). Twitter has experienced a huge rise in popularity in recent years due to its availability and ease of use. This increase in popularity and the public nature of Twitter (less than 10% of Twitter accounts are private (Moore, 2009) has made it an important tool for the study of people's behavior and attitudes to Twitter. One area of research that has attracted great attention in recent years is the Tweet Sentiment Classification. Through sentiment classification and analysis, one can get an idea of people's attitudes to certain topics on Twitter. This can be used to measure people's attitudes toward brands, political candidates and social topics. There have been several works that perform sentiment classification on Twitter using standard sentiment classification techniques, with variations of grammar and phrases, and attempts to synchronize them with other tactics, as there have been in attempts to do."}, {"heading": "2 Related Work", "text": "This mood analysis and classification of text is a problem that has already been well studied in many different areas, such as blogs, film reviews and product reviews (e.g., (Pang et al., 2002; Cui et al., 2006; Chesley et al., 2006). There is also extensive work on mood analysis for Twitter. Most work on Twitter mood classification focuses either on different machine learning techniques (e.g., (Wang et al., 2011; Jiang et al., 2011), novel functions (e.g. (Davidov et al., 2010; Kouloumpis et al., 2011; Saif et al., 2012), new data collection and labeling techniques (e.g., (Go et al., 2009)) or the use of mood classification to analyze people's attitudes on certain topics on Twitter (e.g., (Diakopoulos and Shamma, 2010; Bollen et al., 2011)."}, {"heading": "3 Approach", "text": "The main hypothesis behind this work is that the average mood of messages on Twitter is different in different categories. Specifically, tweets in different spatial, temporal and authoritative contexts have on average different sensations. Basically, these factors (many of which are environmentally friendly) have an impact on the emotional states of people, which in turn have an effect on the feelings that people express on Twitter and elsewhere. In this paper, we used this contextual information to better predict the mood of tweets. Fortunately, tweets are provided with very rich metadata, including location, timestamps and author information. By analyzing the designated data collected from these different contexts, we calculated previous probabilities of negative and positive sensations for each of the contextual categories shown below. \u2022 The States in the US (a total of 50). \u2022 Hour of the Day (HoD) (24 in total). \u2022 Day of the Week (DoW) (7 in total). \u2022 Month (12 in total). \u2022 Authors in the United States (a total of 50)."}, {"heading": "4 Data Collection and Datasets", "text": "We collected two sets of data, one massive dataset labeled by remote monitoring, the other small dataset labeled by humans; the massive dataset was used to calculate the previous probabilities for each of our contextual categories; both datasets were used to train and test our mood classifier; and the human-labeled dataset was used as a health check to ensure that the dataset labeled with the Emoticons classifier was not too loud and that the human and emoticons labels matched for the majority of tweets."}, {"heading": "4.1 Emoticon-based Labelled Dataset", "text": "We collected a total of 18 million geo-tagged English-language tweets over three years, from January 1, 2012 to January 1, 2015, evenly distributed over every 36 months, using GNIP3 \"s Historical PowerTrack for Twitter2. We created Geolocation Boxes4 for each of the 50 states used to collect our data set. All 18 million tweets came from one of the 50 states and are marked as such. In addition, all1Japanese pictographs, which resemble ASCII emoticons 2Historical PowerTrack for Twitter, contained full access to the full archive of public Twitter data. 3https: / / gnip.com / 4The Bounding Boxes were created using http: / / boundingbox.klokantech.com / tweets and contained one of the six emoticons in Table 1 and were marked as positive or negative based on the emoticon the emoticon. Of the 18 million tweets, 11.2 million (62%) were marked as positive and 6.8 million (38%) as negative."}, {"heading": "4.2 Human Labelled Dataset", "text": "We randomly selected 3,000 tweets from our large data set and had all of their emoticons removed, and then had those tweets labeled as positive or negative by three human commenters (Fleiss, 1971).The Kappa score for the three commenters was 0.82, meaning that a small portion of the tweets had disagreements, but the number of tweets labeled as equal by at least two of the three human commenters was 2,908 out of 3,000 tweets (96%).Of these 2,908 tweets, 60% were rated as positive and 40% as negative. We then measured the match between human labels and emoticons-based labels, using only tweets labeled as equal by at least two of the three human commenters (the majority of the tweets were used as a label for the tweet).The confusion between human labels and emoticons-based labels was measured using labels that matched with 885 and 155%."}, {"heading": "4.3 Data Preparation", "text": "Since the data is labeled with emoticons, we removed all emoticons from the training data. This ensures that emoticons are not used as a feature in our sentiment classifier. A large portion of the tweets contain links to other websites. These links are mostly not semantically meaningful and therefore cannot help in the classification of feelings. Therefore, all links in tweets have been replaced by the token \"URL.\" Likewise, all mentions of usernames (which are labeled with the @ symbol) have been replaced by the token \"USERNAME,\" as they also cannot help in the classification of feelings. Tweets also contain a very informal language and as such, letters in words are often repeated for emphasis (e.g. the word \"good\" is used in many tweets with an arbitrary number of o). Any character that has been repeated more than twice has been removed (e.g. \"goood\" has been replaced with \"good\"). Finally, all words in tweets have been contained with Porter Stemming 1980."}, {"heading": "5 Baseline Model", "text": "For our base model for classifying mood, we used our massive dataset to train a negative and positive n-gram language model from negative and positive tweets. As a base model, we built purely linguistic Bigram models in Python, using some components from NLTK (Bird et al., 2009), which used a vocabulary that was filtered to remove words that occur 5 or less often. Probability distributions were calculated using Kneser-Ney smoothing (Chen and Goodman, 1999). In addition to smoothing Kneser-Ney, the Bigram models also used \"backoff smoothing\" (Katz, 1987), in which an n-gram model relies on a (n \u2212 1) gram model for words that were not observed in the n-gram context. To classify the mood of a new tweet, the probability of adjustment using both the negative model and our positive model is not calculated using a Bigram-1 hypothesis (that our model is based on a Bigram-1)."}, {"heading": "6 Contextual Model", "text": "The Bayesian approach allows us to easily integrate the contextual information into our models. Pr (\u03b8s) in Equation 2 is the previous probability of a tweet with the sentiment s. The previous probability (Pr (\u03b8s) can be calculated on the basis of the context information of the tweets. Therefore, in Equation 2, Pr (\u03b8s | C) is replaced by Pr (\u03b8s | C), which is the probability of the hypothesis given the context information. Pr (\u03b8s | C) is the rear probability of the following Bayesian equation: Pr (\u03b8s | C) = Pr (\u03b8s | C) Pr (successs) Pr (C) (C) (3) Where C is the set of context variables: {State, HoD, Dow, Month, Author}. Pr (\u03b8s | C) captures the probability that a tweet is positive or negative because the state, the hour, the day, the word, and the author of the equation (Dow, the month, the author, the month) is the month."}, {"heading": "7 Sentiment in Context", "text": "We looked at five contextual categories: one spatial, three temporal, and one authoritarian. Here's the list of five categories: \u2022 U.S. states (50 total) (spatial). \u2022 Hour of the day (HoD) (24 total) (temporal). \u2022 Day of the week (DoW) (7 total) (temporal). \u2022 Month (12 total) (temporal). \u2022 Authors (57,710 total) (authoritative). We used our massive emoticon called dataset to calculate the average mood for all of these five categories. A tweet received a score of \u2212 1 if it was negative and a score of 1 if it was positive, so that an average mood of 0 for a contextual category would mean that tweets in that category were equally designated as positive and negative."}, {"heading": "7.1 Spatial", "text": "All 18 million tweets in our dataset are from the United States and geo-tagged. Of course, the tweets are not evenly distributed across the 50 states due to the large differences between states. Figure 1 shows the percentage of tweets per state, small by large. Unsurprisingly, California has the highest number of tweets (2, 590, 179), and Wyoming has the lowest number of tweets (11, 719). Even the state with the lowest number of tweets has more than ten thousand tweets, which is enough to calculate a statistically significant average sentiment for that state. Sentiment for all states on average of tweets over the three years shows Figure 2. Note that an average sentiment of 1.0 means that all tweets were classified as positive - 1.0 means that all tweets were classified as negative and 0.0 means that there was an even distribution of positive and negative tweets. The average sentiment of all states tends to be more positive."}, {"heading": "7.2 Temporal", "text": "We looked at three time variables: time of day, day of the week, and month. All tweets are provided with timestamp data from which we extracted these three variables. Since all timestamps are located in the historical Twitter archives (and the public API) in the UTC time zone, we first converted the timestamp to the local time of the place from which the tweet was sent. Then, we calculated the mood for each day of the week (Figure 4), each hour (Figure 5), and each month (Figure 6), on average, over all 18 million tweets over three years. The 18 million tweets were distributed evenly over each month, with 1.5 million tweets per month. The tweets were also more or less evenly distributed over each day of the week, with each day having between 14% and 15% of the tweets. Similarly, the tweets were almost evenly distributed over each hour, with each tweet having somewhere between 3% and 5% of the tweets. Some of these results make sense intuitively."}, {"heading": "7.3 Authorial", "text": "The last contextual variable we looked at was that of the author. People have different basic attitudes, some are optimistic and positive, some are pessimistic and negative, and some are in between. That personality difference can manifest itself in the mood of the tweets. We tried to capture this difference by looking at the history of users \"tweets. However, the 18 million tweets designated as tweets in our data set were from 7, 657, 158 authors. To calculate a statistically significant average mood for each author, we need our voluptuous size not to be too small. However, a large number of users in our data set tweeted only once or twice during the three years. Figure 7 shows the number of users in containers of 50 tweets. (So the first trash corresponds to the number of users who have fewer than 50 tweets during the entire three-year period.) The number of users in the first few containers in our data set was so large that the graph had to be readable by 20."}, {"heading": "8 Results", "text": "We used 5x cross validation to train and evaluate our base and context models to ensure that the tweets in the training folds were not used in calculating any of the previous models or in training the bigram models. Table 3 shows the accuracy of our models. The context model outperformed the base model using any of the context variables alone, with the condition being the best performance and the day of the week the worst. The model in which all the context variables were used showed a 10% relative and 8% absolute improvement over the base model. Due to the large increase in data volume, far-away mood classifiers for Twitter tend to monitor more standard classifiers using human-labeled data sets. Therefore, it makes sense to compare the performance of our classifier with other far-away monitored classifiers. Although not directly comparable, our contextual classifier outperforms the absolute Twitter Go-Go classifier by 4%, and Positive by 4%."}, {"heading": "9 Discussions", "text": "Although our contextual classifier was able to outperform the previous state of the art, it should be noted that only about one to two percent of tweets are geo-tagged in the wild. Therefore, we trained and evaluated our contextual model using all variables except the state. The accuracy of this model was 0.843, which is still significantly better than the performance of the purely linguistic classifier. Fortunately, all tweets are timestamps and authoring information, so all four other contextual variables used in our model can be used to classify the feeling of each tweet. Note that the previous probabilities we have calculated need to be recalculated and updated each time to take into account changes in the world."}, {"heading": "10 Conclusions and Future Work", "text": "The classification of feelings in tweets is an important area of research. By classifying and analysing feelings on Twitter, one can gain an understanding of people's attitudes to specific topics. In this work, we used the power of remote monitoring to collect millions of noise-intensive tweets from across the US over a three-year period. We used this data set to create previous probabilities for the average mood of tweets in various spatial, temporal and authoritarian contexts. Subsequently, we used a Bajesian approach to combine these predictions with standard Bigram language models, and the resulting combined model achieved an accuracy of 0.862, exceeding the previous, far-flung, monitored Twitter identifier by more than 3%. In the future, we would like to investigate additional contextual characteristics that could be a prediction of mood on Twitter. Specifically, we would like to integrate the topic types of tweets into our model. The theme type describes the types of topics of tweets that Twitter has made for news, which we have already discussed in 2010 (for example, the Jiang), and so on."}, {"heading": "11 Acknowledgements", "text": "We would like to thank all commenters for their efforts. We would also like to thank Brandon Roy for sharing his insights into Bayesian modeling, which was supported by a generous donation from Twitter."}], "references": [{"title": "Robust sentiment detection on twitter from biased and noisy data", "author": ["Barbosa", "Feng2010] Luciano Barbosa", "Junlan Feng"], "venue": "In Proc. COLING", "citeRegEx": "Barbosa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barbosa et al\\.", "year": 2010}, {"title": "Natural language processing with Python", "author": ["Bird et al.2009] Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena", "author": ["Bollen et al.2011] Johan Bollen", "Huina Mao", "Alberto Pepe"], "venue": "In Proc. ICWSM", "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1999] Stanley F Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Using verbs and adjectives to automatically classify blog sentiment", "author": ["Bruce Vincent", "Li Xu", "Rohini K Srihari"], "venue": null, "citeRegEx": "Chesley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chesley et al\\.", "year": 2006}, {"title": "Happiness in everyday life: The uses of experience sampling", "author": ["Csikszentmihalyi", "Jeremy Hunter"], "venue": "Journal of Happiness Studies,", "citeRegEx": "Csikszentmihalyi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Csikszentmihalyi et al\\.", "year": 2003}, {"title": "Comparative experiments on sentiment classification for online product reviews", "author": ["Cui et al.2006] Hang Cui", "Vibhu Mittal", "Mayur Datar"], "venue": "In AAAI,", "citeRegEx": "Cui et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2006}, {"title": "Twitter content classification", "author": ["Stephen Dann"], "venue": "First Monday,", "citeRegEx": "Dann.,? \\Q2010\\E", "shortCiteRegEx": "Dann.", "year": 2010}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["Oren Tsur", "Ari Rappoport"], "venue": "In Proc. COLING", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Characterizing debate performance via aggregated twitter sentiment", "author": ["Diakopoulos", "David A Shamma"], "venue": "In Proc. SIGCHI", "citeRegEx": "Diakopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Diakopoulos et al\\.", "year": 2010}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Fleiss.,? \\Q1971\\E", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Targetdependent twitter sentiment classification", "author": ["Jiang et al.2011] Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao"], "venue": "In Proc. ACL 2011: Human Language Technologies-Volume", "citeRegEx": "Jiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2011}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava Katz"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Twitter sentiment analysis: The good the bad and the omg", "author": ["Theresa Wilson", "Johanna Moore"], "venue": null, "citeRegEx": "Kouloumpis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kouloumpis et al\\.", "year": 2011}, {"title": "The geography of happiness: Connecting twitter sentiment and expression, demographics, and objective characteristics", "author": ["Morgan R Frank", "Kameron Decker Harris", "Peter Sheridan Dodds", "Christopher M Danforth"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2013}, {"title": "Twitter data analysis: An investor\u2019s perspective", "author": ["Robert J Moore"], "venue": null, "citeRegEx": "Moore.,? \\Q2009\\E", "shortCiteRegEx": "Moore.", "year": 2009}, {"title": "Dependency treebased sentiment classification using crfs with hidden variables", "author": ["Kentaro Inui", "Sadao Kurohashi"], "venue": "In Proc. NAACL-HLT", "citeRegEx": "Nakagawa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "Well-being across america", "author": ["Oswald", "Wu2011] Andrew J Oswald", "Stephen Wu"], "venue": "Review of Economics and Statistics,", "citeRegEx": "Oswald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Oswald et al\\.", "year": 2011}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proc. EMNLP 2002-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130\u2013137", "author": ["Martin F Porter"], "venue": null, "citeRegEx": "Porter.,? \\Q1980\\E", "shortCiteRegEx": "Porter.", "year": 1980}, {"title": "Using emoticons to reduce dependency in machine learning techniques for sentiment classification", "author": ["Jonathon Read"], "venue": "In Proceedings of the ACL Student Research Workshop,", "citeRegEx": "Read.,? \\Q2005\\E", "shortCiteRegEx": "Read.", "year": 2005}, {"title": "Grounding language models in spatiotemporal context", "author": ["Roy et al.2014] Brandon C Roy", "Soroush Vosoughi", "Deb Roy"], "venue": "In Fifteenth Annual Conference of the International Speech Communication Association", "citeRegEx": "Roy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2014}, {"title": "Alleviating data sparsity for twitter sentiment analysis", "author": ["Saif et al.2012] Hassan Saif", "Yulan He", "Harith Alani"], "venue": "CEUR Workshop Proceedings (CEUR-WS. org)", "citeRegEx": "Saif et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Saif et al\\.", "year": 2012}, {"title": "Short text classification in twitter to improve information filtering", "author": ["Dave Fuhry", "Engin Demir", "Hakan Ferhatosmanoglu", "Murat Demirbas"], "venue": "In Proc. ACM SIGIR", "citeRegEx": "Sriram et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriram et al\\.", "year": 2010}, {"title": "Improving automatic speech recognition through head pose driven visual grounding", "author": ["Soroush Vosoughi"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Vosoughi.,? \\Q2014\\E", "shortCiteRegEx": "Vosoughi.", "year": 2014}, {"title": "Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach", "author": ["Wang et al.2011] Xiaolong Wang", "Furu Wei", "Xiaohua Liu", "Ming Zhou", "Ming Zhang"], "venue": "In Proc. CIKM", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "An empirical comparison of topics in twitter and traditional media. Singapore Management University School of Information Systems Technical paper series", "author": ["Zhao", "Jiang2011] Xin Zhao", "Jing Jiang"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "This rise in popularity and the public nature of Twitter (less than 10% of Twitter accounts are private (Moore, 2009)) have made it an important tool for studying the behaviour and attitude of people.", "startOffset": 104, "endOffset": 117}, {"referenceID": 20, "context": "There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains (Read, 2005; Nakagawa et al., 2010), however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form (Saif et al.", "startOffset": 124, "endOffset": 159}, {"referenceID": 16, "context": "There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains (Read, 2005; Nakagawa et al., 2010), however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form (Saif et al.", "startOffset": 124, "endOffset": 159}, {"referenceID": 22, "context": ", 2010), however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form (Saif et al., 2012).", "startOffset": 211, "endOffset": 230}, {"referenceID": 18, "context": ", (Pang et al., 2002; Cui et al., 2006; Chesley et al., 2006)).", "startOffset": 2, "endOffset": 61}, {"referenceID": 6, "context": ", (Pang et al., 2002; Cui et al., 2006; Chesley et al., 2006)).", "startOffset": 2, "endOffset": 61}, {"referenceID": 4, "context": ", (Pang et al., 2002; Cui et al., 2006; Chesley et al., 2006)).", "startOffset": 2, "endOffset": 61}, {"referenceID": 25, "context": ", (Wang et al., 2011; Jiang et al., 2011)), novel features (e.", "startOffset": 2, "endOffset": 41}, {"referenceID": 11, "context": ", (Wang et al., 2011; Jiang et al., 2011)), novel features (e.", "startOffset": 2, "endOffset": 41}, {"referenceID": 8, "context": ", (Davidov et al., 2010; Kouloumpis et al., 2011; Saif et al., 2012)), new data collection and labelling techniques (e.", "startOffset": 2, "endOffset": 68}, {"referenceID": 13, "context": ", (Davidov et al., 2010; Kouloumpis et al., 2011; Saif et al., 2012)), new data collection and labelling techniques (e.", "startOffset": 2, "endOffset": 68}, {"referenceID": 22, "context": ", (Davidov et al., 2010; Kouloumpis et al., 2011; Saif et al., 2012)), new data collection and labelling techniques (e.", "startOffset": 2, "endOffset": 68}, {"referenceID": 2, "context": ", (Diakopoulos and Shamma, 2010; Bollen et al., 2011)).", "startOffset": 2, "endOffset": 53}, {"referenceID": 2, "context": "(Bollen et al., 2011) used Twitter to measure the daily mood of the public and compare that to the record of social, political, cultural and economic events in the real world.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "(Mitchell et al., 2013), in which they estimated the happiness levels of different states and cities in the USA using Twitter and found statistically significant correlations between happiness level and the demographic characteristics (such as obesity rates and education levels) of those regions.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "Finally, improving natural language processing by incorporating contextual information has been successfully attempted before (Vosoughi, 2014; Roy et al., 2014); but as far as we are aware, this has not been attempted for sentiment classification.", "startOffset": 126, "endOffset": 160}, {"referenceID": 21, "context": "Finally, improving natural language processing by incorporating contextual information has been successfully attempted before (Vosoughi, 2014; Roy et al., 2014); but as far as we are aware, this has not been attempted for sentiment classification.", "startOffset": 126, "endOffset": 160}, {"referenceID": 20, "context": "There are different methods of obtaining labelled data using distant supervision (Read, 2005; Go et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010).", "startOffset": 81, "endOffset": 156}, {"referenceID": 8, "context": "There are different methods of obtaining labelled data using distant supervision (Read, 2005; Go et al., 2009; Barbosa and Feng, 2010; Davidov et al., 2010).", "startOffset": 81, "endOffset": 156}, {"referenceID": 20, "context": "We used emoticons to label tweets as positive or negative, an approach that was introduced by Read (Read, 2005) and used in multiple works (Go et al.", "startOffset": 99, "endOffset": 111}, {"referenceID": 8, "context": "We used emoticons to label tweets as positive or negative, an approach that was introduced by Read (Read, 2005) and used in multiple works (Go et al., 2009; Davidov et al., 2010).", "startOffset": 139, "endOffset": 178}, {"referenceID": 10, "context": "We measured the inter-annotator agreement using Fleiss\u2019 kappa, which calculates the degree of agreement in classification over that which would be expected by chance (Fleiss, 1971).", "startOffset": 166, "endOffset": 180}, {"referenceID": 19, "context": "Finally, all words in the tweets were stemmed using Porter Stemming (Porter, 1980).", "startOffset": 68, "endOffset": 82}, {"referenceID": 1, "context": "As our baseline model, we built purely linguistic bigram models in Python, utilizing some components from NLTK (Bird et al., 2009).", "startOffset": 111, "endOffset": 130}, {"referenceID": 12, "context": "In addition to Kneser-Ney smoothing, the bigram models also used \u201cbackoff\u201d smoothing (Katz, 1987), in which an n-gram model falls back on an (n \u2212 1)gram model for words that were unobserved in the n-gram context.", "startOffset": 85, "endOffset": 97}, {"referenceID": 7, "context": "There has already been extensive work done on topic categorization schemes for Twitter (Dann, 2010; Sriram et al., 2010; Zhao and Jiang, 2011) which we can utilize for this task.", "startOffset": 87, "endOffset": 142}, {"referenceID": 23, "context": "There has already been extensive work done on topic categorization schemes for Twitter (Dann, 2010; Sriram et al., 2010; Zhao and Jiang, 2011) which we can utilize for this task.", "startOffset": 87, "endOffset": 142}], "year": 2016, "abstractText": "The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classification. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classifier. This combined classifier outperforms the purely linguistic classifier, showing that integrating the rich contextual information available on Twitter into sentiment classification is a promising direction of research.", "creator": "LaTeX with hyperref package"}}}