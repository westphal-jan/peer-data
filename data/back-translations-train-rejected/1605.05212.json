{"id": "1605.05212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Multimodal Sparse Coding for Event Detection", "abstract": "Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset.", "histories": [["v1", "Tue, 17 May 2016 15:37:19 GMT  (450kb,D)", "http://arxiv.org/abs/1605.05212v1", "Multimodal Machine Learning Workshop at NIPS 2015"]], "COMMENTS": "Multimodal Machine Learning Workshop at NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["youngjune gwon", "william campbell", "kevin brady", "douglas sturim", "miriam cha", "h t kung"], "accepted": false, "id": "1605.05212"}, "pdf": {"name": "1605.05212.pdf", "metadata": {"source": "CRF", "title": "Multimodal Sparse Coding for Event Detection", "authors": ["Youngjune Gwon", "William M. Campbell", "Kevin Brady", "Douglas Sturim", "Miriam Cha H. T. Kung"], "emails": [], "sections": [{"heading": null, "text": "Unsupervised feature learning methods have proven effective in classification tasks based on a single modality. We present multi-modal, sparse encodings for learning feature representations that are shared across multiple modalities, applied to the detection of multimedia events (MED), and evaluated against unimodal counterparts and other feature learning methods such as GMM supervectors and sparse RBM. We report on the cross-validated classification accuracy and average precision of the MED system, which has been trained on features we learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset."}, {"heading": "1 Introduction", "text": "Multimedia Event Detection (MED) aims to identify complex activities that occur at a particular place and time and affect different interactions of human actions and objects. MED is considered more difficult than concept analysis, such as action detection, and has received much attention in research on computer vision and machine learning. In this paper, we propose the use of sparse encoding for multimodal feature learning in the context of MED. Originally proposed to explain the relationship between correlated data sources [7], sparse encoding provides an unattended method to learn basic vectors for efficient data representation. More recently, sparse encoding has been used to model the relationship between correlated data sources. By sharing training of dictionaries with audio and video tracks from the same multimedia clip, we can force the two modalities to share a similarly sparse representation, the benefits of which are described in multimodal detection and the next interactive section."}, {"heading": "2 Audio-video Feature Learning", "text": "In summary, our approach is to build feature vectors through sparse encoding of low-level audio and video features. Multiple feature vectors (sparse codes) are aggregated by max pooling, the resulting feature vectors can be scaled at file level, and we use them to train a number of classifiers for MED. * This work was sponsored by the Department of Defense under the Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are provided by the authors and are not necessarily endorsed by the United States.ar Xiv: 160 5.05 212v 1 [cs.L G] 17 May 2"}, {"heading": "2.1 Low-level feature extraction and preprocessing", "text": "We start by locating the keyframes of a given multimedia clip. We apply a simple two-pass algorithm that calculates the difference in the color histograms of two consecutive frames and determines a keyframe candidate based on the threshold calculated on the mean and default deviation of the histogram differences. We examine the number of different colors present in the keyframe candidates and discard those with less than 26 colors, thereby ensuring that our keyframes are not completely black or all-white. Around each keyframe, we extract 5-sec audio data and an additional 10 uniformly sampled video frames over the duration as shown in Figure 1a. If the extracted audio is stereo, we take only the left channel. The audio waveform is changed to 22.05 kHz and regulated by the time frequency automatic amplification control (TF-AGC) to balance the energy in particles."}, {"heading": "2.2 High-level feature modeling via sparse coding", "text": "Similarly, we use nV unmarked video examples to train the classifiers for event detection. Unimodal Feature Learning (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Learning) (Unimodal Feature Vectional Feature Learning) (Unimodal Feature) (Unimodal Feature Video) (Unimodal Feature) (Unimodal Feature Video) (Unimodal Feature) (Unimodal Feature) (Unimodal Vactional Feature) (Unimodal Feature) (Unimodal Video) (Unimodal Feature) (Unimodal Feature) (Unimodal Vactional Feature) (Uniactional Feature) (Unimodal Feature) (Unimodal Video) (Unimodal Feature) (Unimodal Vactional Feature) (Unimodal Vactional Feature) (Unimodal Feature) (Unimodal Feature Vactional Feature) (Unimodal Feature) (Unimodal Feature Vactional Feature) (Unimodal Feature) (Unimodal Feature Vactional Feature) (Unimodal Feature) (Unimodal Feature (Unimodal Feature) (Unimodal Feature) (Unimodal Video) (Unimodal Feature) (Unimodal Feature) (Unimodal Feature-Vactional Feature) (Unimodal Feature) (Unimodal Feature) (Unimodal Feature-Vactional Feature) (Unimodal Feature) (Unimodal Feature) (Unimodal Feature) (Unimodal Feature-Vactional Feature-Learning) (Unimodal Feature) (Unimodal-Vactional Feature) (Unimodal"}, {"heading": "3 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset, task, and experiments", "text": "We use the data set TRECVID MED 2014 [1] to evaluate our schemes. We consider event detection and query using the 10Ex and 100Ex data scenarios, with 10Ex containing 10 multimedia examples per event and 100 examples for 100Ex. There are 20 event classes (E021 to E040) with event names such as \"bicycle trick,\" \"dog show\" and \"marriage proposal.\" To evaluate, we calculate Classification Accuracy and Mean Accuracy (MAP) metrics according to the NIST standard using the following experiments: 1. Cross-validation to 10Ex; 2. 10Ex / 100Ex (train with 10Ex and test to 100Ex). We use the number of base vectors K = 512 equal for all dictionaries DA, DV and DAV. We aggregate the codes around each key frame of a training example by forming feature vectors for classification."}, {"heading": "3.2 Other feature learning methods for comparison", "text": "We evaluate the performance of the Gaussian mixing model (GMM) and the restricted Boltzmann machine (RBM) [8] under similar unimodal and multimodal conditions. For GMM, we use the Expectation Maximization Model (EM) to adjust the pre-processed input vectors xA, xV, xAV in 512 mixtures and to form GMM supervectors [4] as characteristics that contain posterior probabilities in relation to each Gauss. Maximum pooled GMM supervectors are used to form linear SVMs. We adopt the flat bimodal pretraining model from Ngiam et al. [6] for RBM. Activations from the hidden layer of a size of 512 are also pooled in front of SVM max. We have applied a target parity of 0.1 to both GMM and RBM."}, {"heading": "3.3 Results", "text": "For the 10Ex / 100Ex experiment, we used the best parameter setting from the 10Ex cross-validation to test 100Ex examples. In general, we observe that the unification of audio and video function vectors works better than the use of uniodal or intermodal features only. However, the composite systems work better than the common systems. In Table 2, we report on the mean accuracy and MAP for GMM and RBM within the composite and common feature learning scheme on the 10Ex / 100Ex experiment. Our results show that the economical coding is 5-6% better than GMM and the accuracy of RBM compared to 8% of the common feature learning schemes on the BM experiment."}, {"heading": "4 Conclusion", "text": "Our approach can build common sparse feature vectors, which we have learned from different modalities and scales, to file-level descriptors suitable for training classifiers in a MED system. Using the TRECVID MED 2014 dataset, we have empirically validated our approach and achieved promising results measured in accuracy and precision metrics recommended by the NIST standard."}], "references": [{"title": "Support Vector Machines Using GMM Supervectors for Speaker Verification", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Multimodal Deep Learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Image Super-Resolution via Sparse Representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Originally proposed to explain neurons encoding sensory information [7], sparse coding provides an unsupervised method to learn basis vectors for efficient data representation.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "We have tried out pretrained convolutional neural network (CNN) models and ended up choosing VGG ILSVRC 19 layers, the 19layer model by University of Oxford\u2019s Visual Geometry Group (VGG) [9] for the ImageNet Largescale Visual Recognition Challenge (ILSVRC).", "startOffset": 187, "endOffset": 190}, {"referenceID": 7, "context": "We use the joint sparse coding technique used in image super-resolution [11]", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "We use the INRIA SPAMS (SPArse Modeling Software) [2], VOICEBOX Speech Processing Toolkit [3], MatConvNet [10] to drive the pretrained deep CNN models, and LIBSVM [5].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "We use the INRIA SPAMS (SPArse Modeling Software) [2], VOICEBOX Speech Processing Toolkit [3], MatConvNet [10] to drive the pretrained deep CNN models, and LIBSVM [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "We evaluate the performance of Gaussian mixture model (GMM) and restricted Boltzmann machine (RBM) [8] under similar unimodal and multimodal settings.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "For GMM, we use the expectation-maximization (EM) to fit the preprocessed input vectors xA, xV, xAV in 512 mixtures and form GMM supervectors [4] as feature that contain posterior probabilities with respect to each Gaussian.", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "[6] for RBM.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Unsupervised feature learning methods have proven effective for classification tasks based on a single modality. We present multimodal sparse coding for learning feature representations shared across multiple modalities. The shared representations are applied to multimedia event detection (MED) and evaluated in comparison to unimodal counterparts, as well as other feature learning methods such as GMM supervectors and sparse RBM. We report the cross-validated classification accuracy and mean average precision of the MED system trained on features learned from our unimodal and multimodal settings for a subset of the TRECVID MED 2014 dataset.", "creator": "LaTeX with hyperref package"}}}