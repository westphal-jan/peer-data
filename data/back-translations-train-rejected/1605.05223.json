{"id": "1605.05223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "On the boosting ability of top-down decision tree learning algorithm for multiclass classification", "abstract": "We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes. The algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. We prove important properties of this objective and explore its connection to three well-known entropy-based decision tree objectives, i.e. Shannon entropy, Gini-entropy and its modified version, for which instead online optimization schemes were not yet developed. We show, via boosting-type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy-based objectives. The bounds we obtain critically depend on the strong-concavity properties of the entropy-based criteria, where the mildest dependence on the number of classes (only logarithmic) corresponds to the Shannon entropy.", "histories": [["v1", "Tue, 17 May 2016 16:11:36 GMT  (136kb,D)", "http://arxiv.org/abs/1605.05223v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "krzysztof choromanski", "mariusz bojarski"], "accepted": false, "id": "1605.05223"}, "pdf": {"name": "1605.05223.pdf", "metadata": {"source": "CRF", "title": "On the boosting ability of top-down decision tree learning algorithm for multiclass classification", "authors": ["Anna Choromanska", "Krzysztof Choromanski", "Mariusz Bojarski"], "emails": ["achoroma@cims.nyu.edu", "kchoro@google.com", "mbojarski@nvidia.com"], "sections": [{"heading": null, "text": "Tags: Classification of Multiple Classes, Decision Trees, Boosting, Online Learning"}, {"heading": "1. Introduction", "text": "This year, it will be able to explore the aforementioned brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind-consecrated mind."}, {"heading": "2. Objective function and its theoretical properties", "text": "In this section, we will describe the objective function that is of central interest to this paper and provide its theoretical properties."}, {"heading": "2.1 Objective function", "text": "We assume access to a hypothesis class H, in which each h-H is a binary classifier, h: X 7 \u2192 {\u2212 1, 1}, and each node in the tree consists of a classifier of H. The classifiers are trained so that hn (x) = 1 (hn) denotes the classifier in the node n of the tree; for a fixed node n, we will simply refer to hn as h, which means that the example x is sent to the right sub-tree of the node n, while hn (x) = \u2212 1 x sends to the left sub-tree. When we reach a leaf node, we predict according to the label with the highest frequency among the examples reaching this leaf. Note that from the perspective of reducing the computational complexity, we consider the number of examples going left and right balanced."}, {"heading": "2.2 Theoretical properties of the objective function", "text": "We first define the concept of balance and purity of division, which are critical to providing the theoretical properties of the objective function in this publication. (\u03b2) Definition 1 (Purity and Balance, Choromanska and Langford (2014) The hypothesis h \u00b2 H induces a pure division if\u03b1: = k \u00b2 i = 1 \u03c0i min (P (h (h) > 0 | i), P (h) < 0 | i))), where there is a pure division if\u03b1 (0, 0.5), and \u03b1 is called a purity factor. The hypothesis h \u00b2 H induces a balanced division ifc \u00b2 h (h) > 0), where the distribution is 1 \u2212 c, where c (0, 0.5] and \u03b2 is called the balanced division. A division is called maximum pure when \u03b1 = 0 (each class is sent exclusively to the left or right)."}, {"heading": "3. Main theoretical results", "text": "We begin with the explanation of the nodes and their two children in this step. (...) We begin with the explanation of the three heaviest nodes and the three heaviest nodes in this step. (...) We begin with the explanation of the three heaviest nodes in this step. (...) We have the probability that a node of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots of the three knots, the three knots of the three knots of the three, and the three knots of the three, three knots of the three, three knots of the three, three knots of the three of the three."}, {"heading": "3.1 Properties of the entropy-based criteria", "text": "Each of the entropy-based criteria presented has a number of useful properties, which we will next use with their proofs.Bounds on the entropy-based criteria We first give bounds on the values of the entropy-based functions.Lemma 9 The entropy function Get at time t = 0 \u2264 Get \u2264 (t + 1) w ln k.Proof The lower limit results from the fact that the entropy of each leaf p p p p p p i = 1 \u03c0l, i ln (1 \u03c0l, i) is non-negative. We next check the upper limit. Note thatGet = 1 \u04451 l l l l, i ln (1 \u03c0l, i) \u2264 l l l l l l l l l l l l l l k \u2264 w ln k. L 1 = (t + 1) w ln k, where the first inequality comes from the fact that the even distribution is maximized, and the last uniformity comes from the fact that a tree with t."}, {"heading": "3.2 Proof of the main theorems", "text": "First, we introduce some mathematical tools used in the following proofs, the next two of which are of fundamental importance. The first one refers to the following three theorems (1 \u2212 1) and the second is a simple property of the exponential function. Lemma 19 Next, let us have theorem 6, 7 and 8. Proof of the entropy resulting from Equation 2 and Lemma 14, which comes from Equation 12 knows (1 \u2212 \u03b2) the following attitudes (1 \u2212 1) and the following attitudes (1 \u2212 1 \u2212 21 = 12w\u00df (1 \u2212 \u03b2). Next, we go to the proof of theorem 6, 7, and 8. Proof the entropy follows from Equation 2 and Lemma 14, which comes from Equation 12 knows (1 \u2212 \u03b2)."}, {"heading": "4. Numerical experiments", "text": "We run the LOMtree algorithm implemented in the open source learning system Vowpal Wabbit Langford et al. (2007) on four benchmark multiclass datasets: Mnist (10 classes, downloaded from http: / / yann.lecun.com / exdb / mnist /), Isolet (26 classes, downloaded from http: / / www.cs.huji.ac.il / ~ shais / datasets / ClassificationDatasets.html), Sector (105 classes, downloaded from http: / / www.csie.ntu.edu.tw / ~ cjlin / libsvmtools / datasets / multiclass.html) and Aloi (1000 classes, downloaded from http: / / www.csie. ntu.edu.tw / libsvmtools / libsvmtools / datas.html)."}, {"heading": "5. Conclusions", "text": "This work focuses on the characteristics of the recently proposed LOMtree algorithm. We offer a comprehensive theoretical analysis of the objective function underlying the algorithm. We present a unified framework for analyzing the augmentability of the algorithm by examining the connection of its object with entropy-based criteria, such as entropy, Gini entropy and its modified version. We show that the strong concentration characteristics of these criteria have a decisive influence on the character of the obtained limits. Experiments suggest that narrower limits are possible, especially for the modified version of Gini entropy."}], "references": [{"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Conditional probability tree estimation analysis and algorithms", "author": ["A. Beygelzimer", "J. Langford", "Y. Lifshits", "G.B. Sorkin", "A.L. Strehl"], "venue": "In UAI,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Error-correcting tournaments", "author": ["A. Beygelzimer", "J. Langford", "P.D. Ravikumar"], "venue": "In ALT,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Logarithmic time online multiclass prediction", "author": ["Anna Choromanska", "John Langford"], "venue": "CoRR, abs/1406.1822,", "citeRegEx": "Choromanska and Langford.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska and Langford.", "year": 2014}, {"title": "Fast and balanced: Efficient label tree learning for large scale object recognition", "author": ["J. Deng", "S. Satheesh", "A.C. Berg", "L. Fei-Fei"], "venue": "In NIPS,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M. Kearns", "Y. Mansour"], "venue": "Journal of Computer and Systems Sciences,", "citeRegEx": "Kearns and Mansour.,? \\Q1996\\E", "shortCiteRegEx": "Kearns and Mansour.", "year": 1996}, {"title": "A multi-class svm classifier utilizing binary decision", "author": ["G. Madzarov", "D. Gjorgjevikj", "I. Chorbev"], "venue": "tree. Informatica,", "citeRegEx": "Madzarov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Madzarov et al\\.", "year": 2009}, {"title": "A theory of multiclass boosting", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee and Schapire.,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee and Schapire.", "year": 2013}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rifkin and Klautau.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau.", "year": 2004}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "Schapire and Freund.,? \\Q2012\\E", "shortCiteRegEx": "Schapire and Freund.", "year": 2012}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis, The Hebrew University of Jerusalem,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "Lyapunov Functions in Differential Games. Stability and Control: Theory, Methods and Applications", "author": ["V.I. Zhukovskiy"], "venue": "Taylor & Francis,", "citeRegEx": "Zhukovskiy.,? \\Q2003\\E", "shortCiteRegEx": "Zhukovskiy.", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "Abstract We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes.", "startOffset": 167, "endOffset": 199}, {"referenceID": 9, "context": "Straightforward extensions of the binary approaches to the multiclass setting, such as one-against-all approach Rifkin and Klautau (2004), do not often work in the presence of strict computational constraints.", "startOffset": 112, "endOffset": 138}, {"referenceID": 4, "context": "troduced in Choromanska and Langford (2014) along with the algorithm optimizing it in an online fashion called LOMtree.", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given.", "startOffset": 12, "endOffset": 39}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given. It is provably consistent and achieves regret bound which depends logarithmically on the number of classes. The conditional probability tree Beygelzimer et al. (2009a) instead learns the tree structure and uses the node splitting criterion which compromises between obtaining balanced split in a tree node and violating the recommendation for the split from the node regressor.", "startOffset": 12, "endOffset": 311}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given. It is provably consistent and achieves regret bound which depends logarithmically on the number of classes. The conditional probability tree Beygelzimer et al. (2009a) instead learns the tree structure and uses the node splitting criterion which compromises between obtaining balanced split in a tree node and violating the recommendation for the split from the node regressor. The authors also provide regret bounds which scales with the tree depth. Other works, which come with no guarantees, consider splitting the data in every tree node by optimizing efficiency with accuracy constraints allowing fine-grained control of the efficiency-accuracy tradeoff Deng et al. (2011), or by performing clustering Bengio et al.", "startOffset": 12, "endOffset": 821}, {"referenceID": 0, "context": "(2011), or by performing clustering Bengio et al. (2010); Madzarov et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 0, "context": "(2011), or by performing clustering Bengio et al. (2010); Madzarov et al. (2009). The splitting criterion (objective function) analyzed in this paper differs from the criteria considered in previous works and comes with much stronger theoretical justification given in Section 2.", "startOffset": 36, "endOffset": 81}, {"referenceID": 7, "context": "The main theoretical analysis of this paper is kept in the boosting framework Schapire and Freund (2012) and relies on the assumption of the existence of weak learners in the tree nodes, where the top-down algorithm we study will amplify this weak advantage to build a tree achieving any desired level of accuracy wrt.", "startOffset": 78, "endOffset": 105}, {"referenceID": 6, "context": "We add new theoretical results to the theory of boosting for the multiclass classification problem (the multiclass boosting is largely ununderstood, we refer the reader to Mukherjee and Schapire (2013) for comprehensive review), and we show that LOMtree is a boosting algorithm reducing standard entropy-based criteria, where the obtained bounds depend on the strong concativity properties of these criteria.", "startOffset": 172, "endOffset": 202}, {"referenceID": 4, "context": "Our work extends two previous works: it significantly adds to the theoretical analysis of Choromanska and Langford (2014), where only Shannon entropy is considered, in which case we also slightly improve their bound, and it extends beyond the boosting analysis of the binary case of Kearns and Mansour (1999).", "startOffset": 90, "endOffset": 122}, {"referenceID": 4, "context": "Our work extends two previous works: it significantly adds to the theoretical analysis of Choromanska and Langford (2014), where only Shannon entropy is considered, in which case we also slightly improve their bound, and it extends beyond the boosting analysis of the binary case of Kearns and Mansour (1999). The main theoretical results are presented in Section 3.", "startOffset": 90, "endOffset": 309}, {"referenceID": 4, "context": "These two criteria, purity and balancedness, were discussed in Choromanska and Langford (2014). This work also proposes an objective (convex) expressing both criteria, and thus measuring the quality of a hypothesis h \u2208 H in creating partitions at a fixed node n in the tree.", "startOffset": 63, "endOffset": 95}, {"referenceID": 4, "context": "Definition 1 (Purity and balancedness, Choromanska and Langford (2014)) The hypothesis h \u2208 H induces a pure split if", "startOffset": 39, "endOffset": 71}, {"referenceID": 4, "context": "Lemma 2 contains a stronger statement than the one in the original paper Choromanska and Langford (2014) (Lemma 2).", "startOffset": 73, "endOffset": 105}, {"referenceID": 4, "context": "Lemma 4 (Choromanska and Langford (2014)) For any hypothesis h, and any distribution over examples (x, y), the purity factor \u03b1 and the balancing factor \u03b2 satisfy \u03b1 \u2264 min {(2\u2212 J(h))/4\u03b2 \u2212 \u03b2, 0.", "startOffset": 9, "endOffset": 41}, {"referenceID": 4, "context": "Proof [Lemma 2] The proof that J(h) \u2208 [0, 1] and that if h induces a maximally pure and balanced partition then J(h) = 1 was done in Choromanska and Langford (2014) (Lemma 2).", "startOffset": 133, "endOffset": 165}, {"referenceID": 5, "context": "In our analysis we use elements of the proof techniques from Kearns and Mansour (1999) (the proof of Theorem 10) and Choromanska and Langford (2014) (the proof of Theorem 1).", "startOffset": 61, "endOffset": 87}, {"referenceID": 4, "context": "In our analysis we use elements of the proof techniques from Kearns and Mansour (1999) (the proof of Theorem 10) and Choromanska and Langford (2014) (the proof of Theorem 1).", "startOffset": 117, "endOffset": 149}, {"referenceID": 6, "context": "These criteria are natural extensions of the criteria used in Kearns and Mansour (1999) in the context of binary classification, to the multiclass classification setting3.", "startOffset": 62, "endOffset": 88}, {"referenceID": 4, "context": "Definition 5 (Weak Hypothesis Assumption, Choromanska and Langford (2014)) Let m denotes any node of the tree T , and let \u03b2m = P (hm(x) > 0) and Pm,i = P (hm(x) > 0|i).", "startOffset": 42, "endOffset": 74}, {"referenceID": 4, "context": "They guarantee that the top-down decision tree algorithm which optimizes J(h), such as the one in Choromanska and Langford (2014), will amplify the weak advantage, captured in the weak learning assumption, to build a tree achieving any desired level of accuracy wrt.", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "Note that there is more than one way of extending the entropy-based criteria from Kearns and Mansour (1999) to the multiclass classification setting, e.", "startOffset": 82, "endOffset": 108}, {"referenceID": 4, "context": "This guarantee is slightly tighter compared to Theorem 1 in Choromanska and Langford (2014).", "startOffset": 60, "endOffset": 92}, {"referenceID": 11, "context": "Proof Lemma 14 is proven in Shalev-Shwartz (2012) (Example 2.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "Lemma 15 (Shalev-Shwartz (2007)(Lemma 14)) If the function \u03a6(\u03c0) is twice differentiable, then the sufficient condition for strong concativity of \u03a6 is that for all \u03c0, x, \u3008 \u22072\u03a6(\u03c0)x,x \u3009 \u2264 \u2212\u03c3\u2016x\u20162, where \u22072\u03a6(\u03c0) is the Hessian matrix of \u03a6 at \u03c0, and \u03c3 > 0 is the strong concativity modulus.", "startOffset": 10, "endOffset": 32}, {"referenceID": 13, "context": "Lemma 17 (Zhukovskiy (2003), Remark 2.", "startOffset": 10, "endOffset": 28}, {"referenceID": 4, "context": "Clearly the larger the objective J(h) is at time t, the larger the entropy reduction ends up being, which confirms the plausibility of the approach in Choromanska and Langford (2014) where the goal is to maximize J(h).", "startOffset": 151, "endOffset": 183}, {"referenceID": 3, "context": "The regressors in the tree nodes are linear and were trained by SGD Bottou (1998) with 20 epochs and the learning rate chosen from the set {0.", "startOffset": 68, "endOffset": 82}, {"referenceID": 3, "context": "The regressors in the tree nodes are linear and were trained by SGD Bottou (1998) with 20 epochs and the learning rate chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. We investigated different swap resistances5 chosen from the set {4, 8, 16, 32, 64, 128, 256}. We selected the learning rate and the swap resistance as the one minimizing the validation error, where the number of splits in all experiments were set to 10k. Figure 3 shows the entropy, Gini-entropy, modified Gini-entropy, and the error, all normalized to the interval [0, 1], as the function of the number of splits. The behavior of the entropy and Gini-entropy match the theoretical findings. However, the modified Ginientropy instead drops the fastest with the number of splits, which in particular suggests that in this case perhaps tighter bounds could possibly be proved (for the binary case tighter analysis was shown in Kearns and Mansour (1999), but it is highly non-trivial to generalize", "startOffset": 68, "endOffset": 921}, {"referenceID": 4, "context": "see Choromanska and Langford (2014) for details", "startOffset": 4, "endOffset": 36}], "year": 2016, "abstractText": "We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes. The algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. We prove important properties of this objective and explore its connection to three well-known entropy-based decision tree objectives, i.e. Shannon entropy, Gini-entropy and its modified version, for which instead online optimization schemes were not yet developed. We show, via boosting-type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy-based objectives. The bounds we obtain critically depend on the strong-concavity properties of the entropy-based criteria, where the mildest dependence on the number of classes (only logarithmic) corresponds to the Shannon entropy.", "creator": "LaTeX with hyperref package"}}}