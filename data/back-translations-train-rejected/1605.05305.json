{"id": "1605.05305", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Combat Models for RTS Games", "abstract": "Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require access to a forward model (or \"simulator\") of the game at hand. However, in some games such forward model is not readily available. This paper presents three forward models for two-player attrition games, which we call \"combat models\", and show how they can be used to simulate combat in RTS games. We also show how these combat models can be learned from replay data. We use StarCraft as our application domain. We report experiments comparing our combat models predicting a combat output and their impact when used for tactical decisions during a real game.", "histories": [["v1", "Tue, 17 May 2016 19:47:13 GMT  (864kb,D)", "http://arxiv.org/abs/1605.05305v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alberto uriarte", "santiago onta\\~n\\'on"], "accepted": false, "id": "1605.05305"}, "pdf": {"name": "1605.05305.pdf", "metadata": {"source": "META", "title": "Combat Models for RTS Games", "authors": ["Alberto Uriarte"], "emails": ["santi@cs.drexel.edu;", "albertouri@cs.drexel.edu)."], "sections": [{"heading": null, "text": "In this paper, we focus on the problem of defining \"forward\" models, in which we limit ourselves only to combating individual situations. Furthermore, the importance of real-time strategies (RTS) for games such as Monte Carlo Tree Search (MCTS) and the existence of a forward model that allows the state to move forward after executing a particular action in the current game state is underlined. While this assumption is reasonable in certain areas, such as the simulation of actions, the effect of actions is trivial, precise descriptions of the effect of actions are not available. Thus, techniques for defining forward models become central to RTS games as they allow the application of game-tree-up-out models."}, {"heading": "II. BACKGROUND", "text": "Real-Time Strategy (RTS) games in general, and STARCRAFT in particular, have emerged as a fertile testing ground for new AI algorithms [3], [4]. One of the most frequently recurring techniques for tactical decision-making are those based on game trees, such as the Alpha Beta Search [5] or the MCTS [6] - [9]. Game tree search algorithms require a certain representation of the game state, a forward model that gives us the game state resulting from the application of an action to another game state, and a rating function that rewards game statutes. As for the representation of the game state, the use of a direct representation of the game state in RTS games with the current search algorithms is not feasible due to the resulting large branching factor (number of possible actions a player can perform in a particular game state)."}, {"heading": "III. COMBAT IN RTS GAMES", "text": "One of the mechanisms present in almost all RTS games is combat. & pos > In a combat situation, each player commands an army of units to defeat the opponent. We model battles using wear games [2]. A wear game is a simultaneous move by two players in a directed diagram in which each node has health, attack power and belongs to a player. A directed edge indicates that the node x can inflict damage to the node y and when the health of a node reaches zero, the node is removed from the diagram. The goal of the game is to destroy all opposing nodes. In each turn, each node can select a target and at the end of the turn all damage is done at the same time. This abstraction can be considered a target selection problem as no movement is involved in the process and nodes have no complex characteristics. A similar combat unit has been investigated by Kovarsky and Buro [11] as a unit."}, {"heading": "IV. EXISTING COMBAT MODELS FOR STARCRAFT", "text": "Different types of combat models are proposed in the literature for RTS games. In this section, we will first divide these existing models into three broad classes: low-level end-state prediction models (which try to model the battle as close as possible to the real game); high-level end-state prediction models (which use a certain amount of abstraction but try to give an accurate prediction of the remaining army composition at the end of a battle); and high-level end-state prediction models (which only predict the winner)."}, {"heading": "A. Low-level Final State Prediction Models", "text": "The most representative low-level model for STARCRAFT is SPARCRAFT2. SPARCRAFT was developed through a large2https: / / github.com / davechurchill / ualbertabot / tree / master / SparCrafthuman effort to observe the behavior of various STARCRAFT units frame by frame. Although it is extremely accurate for small combat scenarios, SPARCRAFT is not exhaustive (for example, it does not model unit collisions, and it does not support some unit types such as Rubik's Cube or transports) due to the enormous effort it would take to model the complete STARCRAFT game. However, there has been recent work [12] to improve the accuracy of unit motion simulation in SPARCRAFT. Despite achieving a high degree of reliability, a perfect low-level model is impossible that considers the CRAFT models as abstract in terms of state."}, {"heading": "B. High-level Final State Prediction Models", "text": "s laws [14] assume that battles take place between two homogeneous armies; a combat effectiveness (or Lanchester wear-rate coefficient) for each army is predictable; and any unit in an army can attack any opposing unit. However, this is not the case in most RTS games, where, for example, not all units can attack flying units. To overcome these deficiencies, several extensions have been proposed [15], but only some of them have been explored in the context of RTS games. Soemers calculated the combat effectiveness of heterogeneous armies, in which the mean DPF (damage per frame) of an army is aggregated, divided by the mean HP of the opposing army. Similarly, Stanescu et al. [16] used the Lanchester-Genesis law (a generalization of the law)."}, {"heading": "C. High-level Winner Prediction Models", "text": "Finally, some approaches focus directly on predicting the winner of a battle, rather than providing a prediction of the final state of the battle. These models can be divided into two subcategories: heuristic and learning-based on machine learning. In terms of heuristic models, Uriarte [17] defined a basic heuristic assumption that both armies will continuously exchange their initial amount of DPF with each other until one of the armies is destroyed. Kovarsky and Buro [11] proposed a function that makes it more important to have multiple units with less HP than just one unit with full HP: Life Time Damage 2 (LTD2). LTD2 = [A].HP \u00d7 u.DPF \u2212 B. \"HP \u00d7 u.DPFNguyen et al al al al. [18] took into account that RTS gaming properties are not additive, i.e., if \u00b5 (X) is effective through a unit combination, X (compatible) > X2."}, {"heading": "V. PROPOSED COMBAT MODELS FOR STARCRAFT", "text": "In this section, three new high-level end-state prediction models are presented, which aim to predict the final state of a battle with greater accuracy than existing models. Furthermore, to be useful in the context of the search for game trees, these models can predict the final state of a battle (which has survived even for heterogeneous army compositions), predict the duration of a battle, and simulate partial battles (i.e., not just battles to the end). A. Target-Selection Lanchester's square right model (TSLanchester2) TS-Lanchester2 refers to the model [13] proposed by Soemers, in which they are both based on Lanchester's square law [14] but differ in two key aspects: Firstly, TS-Lanchester2 is based on the original formulation of Lanchester2 square law (as formulated in the simulation), while Soemers uses a reformulation to support army reinforcements."}, {"heading": "VI. COMBAT MODELS PARAMETERS", "text": "All three proposed models take as their input the DPF (Damage per Frame) and a Target Selection Policy. This section shows how these parameters can be calculated. For each of the two parameters, we have conducted experiments by learning these parameters from replay data (assuming that no information about the game rules is available), and also using a collection of basic configurations for them (e.g. taking into account the DPF values directly from the StarCraft manual). \u2022 Effective DPF a unit may depend not only on the damage and cooling of its weapon, but also on its maneuverability, special abilities, the specific enemy being targeted, and the skill of the player. Therefore, we call effective DPF on the expected DPF that a unit can actually act in a real-world game situation."}, {"heading": "A. Learning Combat Parameters", "text": "This section shows how to use an offline learning method from within the game to learn the combat parameters."}, {"heading": "VII. COMBAT MODELS IN GAME TREE SEARCH", "text": "In this section, we will show how combat models can be integrated into an MCTS framework to play RTS games, especially STARCRAFT. To deal with the huge branching factor that occurs in RTS games, we will abstract both the game state and the action set. Furthermore, we will only consider the movement of military units and attacks as part of this MCTS framework, with all other tasks (resource collection, unit training, etc.) not considered as part of the search. We will integrate this MCTS approach into a STARCRAFT game bot that takes responsibility for all military units. In the next sub-sections, we will first describe the high-level representation of the game states we use; then we will describe the set of high-level actions that will be considered in our framework; and finally, how this is integrated into an actual RTS game bot (how a low-level state is assigned to a high-level state and then assigned to high-level actions that can be performed in the game)."}, {"heading": "A. High-Level State Representation", "text": "We have the map of an RTS game as a graph in which each node corresponds to a map, and edges representing adjacent regions. Also, instead of modeling each individual unit, we will group all units of the same type into groups. We will also assume that units within the same region are close enough to attack each other, and units in different regions are far enough to attack any other region."}, {"heading": "B. High-Level Actions", "text": "We define the following possible actions for each high level group: N / A, Move, Attack and Idle: \u2022 N / A: only for buildings as they cannot perform an action, \u2022 Move: move to an adjacent region, \u2022 Attack: attack enemies in the current region, and \u2022 Idle: nothing during 400 frames.DR AFTFor example, the groups of player 1 in Table I can perform the following actions using an RCMB abstraction: \u2022 Bases: N / A. \u2022 Tanks: Assuming that the tanks did not perform an action, they could move into region 1 or 3 or remain idle. They cannot attack because there is no enemy in region 2. \u2022 Vultures: Assuming that the vultures did not perform an action, they can move into region 2, or remain idle. Since player 1 can focus any combination of these actions on their units, the branching factor of this example (1 + 1) x (1) x AFART representation of the actual game under RT factor is low."}, {"heading": "C. Connecting Low-Level and High-Level States and Actions", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "VIII. EXTRACTING COMBATS FROM GAME REPLAYS", "text": "In order to learn the parameters required by combat models from repetitions, we need a dataset of combat. Other researchers achieved this by creating artificial datasets that used the default STARCRAFT AI to play against themselves [21] and DR AFT to record the state in tracks; or by using a constructed combat model (SPARCRAFT) to perform thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26]. In this work, we use the latter to capture the performance of each unit as it is commanded by an expert."}, {"heading": "A. Combat Records", "text": "We define a combat record as a tuple of CR = < t0, tf, R, A0, B0, Af, Bf, K, P >, where: \u2022 t0 is the initial image when the battle began, and tf is the final image when it ended, \u2022 R is the reason why the battle ended. Options are: - Army destroyed. One of the two armies was completely destroyed during the battle. - Peace. None of the units attacked for the last x frames. In our experiments x = 144, which is 6 seconds game time. - Amplification. New units participating in the battle. This happens when units that were far away from the battle at the beginning of the battle begin to participate in the battle. - End of game. This occurs when a battle is still in progress, but one of the two players gives up the game. \u2022 A0 and B0 are the armies of each player at the beginning of the battle, and Af and Bf are the armies of each player at the end of the battle."}, {"heading": "B. Detecting Combat Start", "text": "One of the most difficult parts is defining the beginning and end of a battle. In Synnaeve and Bessie's work [19], they consider a new battle when a unit is killed. Although this helps to curtail all non-combat activities involved in an RTS game, in some cases we lose useful information. For example, imagine the situation when a ranged unit tilts a melee unit and the melee unit is killed at the end. If we track the fight after the kill, it looks as if the ranged unit has effortlessly killed the melee unit, even though it has actually taken a while and can detect a smart move-and-hit behavior. Therefore, we begin to track a new fight when a military unit is aggressive or exposed and is not already in a battle: \u2022 A military unit is a unit that can do damage (by a weapon or a spell), detect cloaked units or a transporter. A unit is aggressive when it is in an attack area, or if it has an attack area."}, {"heading": "C. Units Involved in Combat", "text": "s define D = inRange (u) inRange (u) inRange (u). Ai is the subset of units belonging to player A at the time ti; and Bi is the subset of units belonging to D belonging to player B at the time ti. Figure 4 shows a representation of a battle triggered by unit u (the black filled square) and the units in battle (the filled squares). Note that a military unit is considered a participant in a battle, even if it does not participate at the end, but is marked as passive."}, {"heading": "D. Combat Dataset", "text": "Since STARCRAFT replays only contain the mouse click events of the players (this is the minimum amount of information required to reproduce the game in STARCRAFT), we do not know the complete game state at a given time (no information about the location of the units or their health), so we need to perform the replay in STARCRAFT and record the required information using BWAPI3. This is not a trivial task, as if we record the game state in each frame, we may have too much information (some consecutive recorded game state variables may have the same value) and the size of the data collected may grow too large to process it efficiently. Some researchers suggested capturing different information in different resolutions to have a good trade in the recorded game state variables."}, {"heading": "IX. EXPERIMENTAL EVALUATION", "text": "In order to evaluate the performance of each combat model, we first collected a data set of repetitions (section IX-A); then we compared the accuracy of the victory forecast with the combat data set (section IX-B); then we evaluated the accuracy of the prediction of the final state (section IX-C. To observe performance in a real game, we integrated the combat models as forward models into an MCTS algorithm (MCTSCD [8]) suitable for RTS games. Within the framework of MCTSCD, we evaluated the game performance achieved with various abstractions (section IX-D) and different combat models (section IX-E)."}, {"heading": "A. Combat Dataset", "text": "We extracted the fights from 600 repetitions (100 for each contest: Terran vs. Terran, Terran vs. Protos, etc.) and yielded a record of 140,797 fights, with: \u2022 99,598 fights ending with reinforcements, \u2022 40,820 fights ending with peace, \u2022 11,454 fights ending with a destroyed army, \u2022 653 fights ending with game end. To evaluate the performance of the combat models, we used only the subset of battles ending with a destroyed army. To form the training set, we also removed battles with Vulture mines (to avoid problems with friendly damage) and battles in which an army was passive (none of its units fought back), resulting in a record of 6,066 fights with an average battle length of 169.68 frames (max. 3,176, min. 1, battles with tiny runtimes are performed due to 1-shot kills (no of its units fought back)."}, {"heading": "B. Winner Prediction Accuracy", "text": "This year, we are talking about just under a million euros."}, {"heading": "C. Final State Prediction Accuracy", "text": "These sections are compared with the models that have been developed in recent years and with the models that have been developed in recent years."}, {"heading": "D. MCTS Abstraction Configuration", "text": "This section evaluates the performance of MCTS with the various abstractions presented in Section VII-A."}, {"heading": "E. MCTS and Combat Model Performance", "text": "We compare the performance with two fundamentals: the original version of the Nova Bot (which uses a programmed manual strategy to control the military units) and a random approach that issues random commands (at the same abstraction level as the MCTS implementation), all experiments were conducted against the built-in Terran AI. Unfortunately, we were unable to test the MCTS with the combat models against other state-of-the-art STARCRAFT AIs, because there was no war fog, and because we interrupted the game during the MCTS search. Configuration for the experiment is the same as in the previous sections with an R-MA abstraction. In this experiment, we gathered the same information as in the previous one. Results are shown in Table V. The first thing we see is that the TS Lanchester2 and the Decreasing models both outperform the Sustained model, with the Decreasing model achieving the best performance we can achieve in this game (and we cannot lose two games)."}, {"heading": "X. CONCLUSIONS", "text": "This work focused on the problem of using MCTS approaches in areas where forward models are not available. Specifically, we focused on RTS games and presented three combat models that can be used as forward models for combat in RTS games. We also presented a method for training the parameters of these models on the basis of repetition data. We have seen that in areas where the parameters of the models (damage per frame, target selection) from the game definition are available, these can be used directly, but in areas where this information is not available, it can be estimated on the basis of repetition data. Our results show that our combat models perform better and are much faster than hand-made low-level models such as SPARCRAFT. This makes our proposed models suitable for MCTS approaches that need to perform a large number of simulations. All combat models are very sensitive to target selection policy, but we showed how target selection can be learned from repetition data."}], "references": [{"title": "A survey of monte carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "TCIAIG, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "On the complexity of two-player attrition games played on graphs", "author": ["T. Furtak", "M. Buro"], "venue": "AIIDE, G. M. Youngblood and V. Bulitko, Eds. AAAI Press, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Real-time strategy games: a new AI research challenge", "author": ["M. Buro"], "venue": "IJCAI. Morgan Kaufmann Publishers Inc., 2003, pp. 1534\u20131535.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey of real-time strategy game AI research and competition in StarCraft", "author": ["S. Onta\u00f1\u00f3n", "G. Synnaeve", "A. Uriarte", "F. Richoux", "D. Churchill", "M. Preuss"], "venue": "TCIAIG, vol. 5, no. 4, pp. 293\u2013311, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast heuristic search for RTS game combat scenarios", "author": ["D. Churchill", "A. Saffidine", "M. Buro"], "venue": "AIIDE. AAAI Press, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "UCT for tactical assault planning in real-time strategy games", "author": ["R.-K. Balla", "A. Fern"], "venue": "IJCAI, 2009, pp. 40\u201345.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Portfolio greedy search and simulation for large-scale combat in StarCraft", "author": ["D. Churchill", "M. Buro"], "venue": "CIG. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Game-tree search over high-level game states in RTS games", "author": ["A. Uriarte", "S. Onta\u00f1\u00f3n"], "venue": "AIIDE. AAAI Press, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Script- and clusterbased UCT for StarCraft", "author": ["N. Justesen", "B. Tillman", "J. Togelius", "S. Risi"], "venue": "CIG. IEEE, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Monte carlo planning in RTS games", "author": ["M. Chung", "M. Buro", "J. Schaeffer"], "venue": "CIG, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Heuristic search applied to abstract combat games", "author": ["A. Kovarsky", "M. Buro"], "venue": "Conference of the Canadian Society for Computational Studies of Intelligence (Canadian AI 2005), vol. 3501. Springer, 2005, pp. 66\u201378.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "StarCraft unit motion: Analysis and search enhancements", "author": ["D. Schneider", "M. Buro"], "venue": "AIIDE, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Tactical planning using MCTS in the game of StarCraft", "author": ["D. Soemers"], "venue": "Master\u2019s thesis, Department of Knowledge Engineering, Maastricht University, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Lanchester, Aircraft in Warfare: The dawn of the Fourth Arm", "author": ["W. F"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1916}, {"title": "Lanchester-type models of warfare. volume I", "author": ["J.G. Taylor"], "venue": "DTIC Document, Tech. Rep., 1980.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1980}, {"title": "Using Lanchester attrition laws for combat prediction in StarCraft", "author": ["M. Stanescu", "N. Barriga", "M. Buro"], "venue": "AIIDE, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-reactive planning for real-time strategy games", "author": ["A. Uriarte"], "venue": "Master\u2019s thesis, Universitat Aut\u00f2noma de Barcelona, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Heuristic search exploiting non-additive and unit properties for RTS-game unit micromanagement", "author": ["T.D. Nguyen", "K.Q. Nguyen", "R. Thawonmas"], "venue": "Journal of Information Processing (JIP 2015), vol. 23, no. 1, pp. 2\u20138, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset for StarCraft AI & an example of armies clustering", "author": ["G. Synnaeve", "P. Bessi\u00e8re"], "venue": "AIIDE. AAAI Press, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting army combat outcomes in StarCraft", "author": ["M. Stanescu", "S.P. Hernandez", "G. Erickson", "R. Greiner", "M. Buro"], "venue": "AIIDE. AAAI Press, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting the outcome of small battles in Star- Craft", "author": ["A.A. S\u00e1nchez-Ruiz"], "venue": "ICCBR, Workshop Proceedings, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The original Borda count and partial voting", "author": ["P. Emerson"], "venue": "Social Choice and Welfare, vol. 40, no. 2, pp. 353\u2013358, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Terrain analysis in real-time strategy games: An integrated approach to choke point detection and region decomposition", "author": ["L. Perkins"], "venue": "AIIDE. AAAI Press, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A bayesian tactician", "author": ["G. Synnaeve", "P. Bessi\u00e8re"], "venue": "Computer Games Workshop at ECAI, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A data mining approach to strategy prediction", "author": ["B.G. Weber", "M. Mateas"], "venue": "CIG. IEEE, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic learning of combat models for RTS games", "author": ["A. Uriarte", "S. Onta\u00f1\u00f3n"], "venue": "AIIDE, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "An improved dataset and extraction process for StarCraft AI", "author": ["G. Robertson", "I. Watson"], "venue": "The Twenty-Seventh International Flairs Conference, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "A significant number of different artificial intelligence (AI) algorithms that play Real-Time Strategy (RTS) games, like Monte Carlo Tree Search (MCTS) [1], assume the existence of a forward model that allows to advance (or predict) the state after executing a certain action in the current game state.", "startOffset": 152, "endOffset": 155}, {"referenceID": 1, "context": "We propose to model a combat as an attrition game [2] (an abstract combat game where individual units cannot move and only their damage and hit points are considered).", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "Real-Time Strategy (RTS) games in general, and STARCRAFT in particular, have emerged as a fruitful testbed for new AI algorithms [3], [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Real-Time Strategy (RTS) games in general, and STARCRAFT in particular, have emerged as a fruitful testbed for new AI algorithms [3], [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "One of the most recurrent techniques for tactical decisions are those based on game tree search, like alpha-beta search [5] or MCTS [6]\u2013[9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "One of the most recurrent techniques for tactical decisions are those based on game tree search, like alpha-beta search [5] or MCTS [6]\u2013[9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "One of the most recurrent techniques for tactical decisions are those based on game tree search, like alpha-beta search [5] or MCTS [6]\u2013[9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "For example, the branching factor in STARCRAFT can reach numbers between 30 and 30 [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "[10] applied Monte Carlo planning to an RTS game by simplifying the decision space: assuming that each player can choose only one amongst a finite set of predefined plans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Balla and Fern [6] performed an abstraction of the game state representation grouping the units in groups but keeping information of each individual unit at the same time, and allowing only two types of actions per group: attack and merge with another group.", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "Kovarsky and Buro [11] considered all units as non movable units and without attack range, i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "Churchill and Buro [7], simplified the possible actions to a set of scripted behaviors to reduce the search with their proposed Portfolio Greedy Search algorithm; Justesen et al.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "[9] extended Churchill and Buro\u2019s work allowing different units to perform different scripted behavior and investigating how to cluster the units that may perform the same script.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Finally, Uriarte and Onta\u00f1\u00f3n [8] used an abstraction based on dividing the terrain in regions using the BroodWar Terrain Analysis (BWTA1), and grouped the units by type and region.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "We will model combats by using attrition games [2].", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "A similar combat abstraction was studied by Kovarsky and Buro [11] in the context of heuristic search, and they showed that finding the optimal strategy in these games is PSPACE-hard [2].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "A similar combat abstraction was studied by Kovarsky and Buro [11] in the context of heuristic search, and they showed that finding the optimal strategy in these games is PSPACE-hard [2].", "startOffset": 183, "endOffset": 186}, {"referenceID": 11, "context": "However, there has been recent work [12] on improving the accuracy of the unit movement simulation in SPARCRAFT.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Branching factor at the level of abstraction at which SPARCRAFT operates can be as high as 10 [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 12, "context": "For example, Soemers [13] proposed a combat model based on Lanchester\u2019s Square Law.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Lanchester\u2019s Laws [14] assume that combats are between two homogeneous armies; a combat effectiveness (or Lanchester attrition-rate coefficient) for each army is computable; and any unit in an army can attack any opponent unit.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "To overcome these shortcomings, several extensions have been proposed [15], but only some of them have been explored in the context of RTS games.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "[16] used the Lanchester\u2019s Generalized Law (a generalization of the Square Law), but this time learning the combat effectiveness from replay data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Concerning heuristic models, Uriarte [17] defined a basic heuristic assuming that both armies continuously deal their starting amount of DPF to each other, until one of the armies is destroyed.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Kovarsky and Buro [11] proposed a function that gives more importance to having multiple units with less HP than only one unit with full HP: Life Time Damage 2 (LTD2).", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "[18] considered that RTS games properties are non-additive, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Concerning machine learning-based models, Synnaeve and Bessi\u00e8re [19] clustered armies based on their unit compositions and they were able to predict the winner using a Gaussian mixture model; Stanescu et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "[20] improved the previous work defining a Bayesian network and using a Gaussian Density Filtering to learn some features of the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Finally, S\u00e1nchez-Ruiz [21] experimented with some machine learning algorithms (such as LDA, QDA, SVM or kNN) to predict the winner of small combats over time.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "TS-Lanchester is related to the model proposed by Soemers [13] in that they are both based on Lanchester\u2019s Square Law [14], but differs in two key aspects: First, TS-Lanchester is based on the original formulation of Lanchester\u2019s Square Law (as formulated in [15]), while Soemers uses a reformulation in order to support army reinforcements.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "TS-Lanchester is related to the model proposed by Soemers [13] in that they are both based on Lanchester\u2019s Square Law [14], but differs in two key aspects: First, TS-Lanchester is based on the original formulation of Lanchester\u2019s Square Law (as formulated in [15]), while Soemers uses a reformulation in order to support army reinforcements.", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "TS-Lanchester is related to the model proposed by Soemers [13] in that they are both based on Lanchester\u2019s Square Law [14], but differs in two key aspects: First, TS-Lanchester is based on the original formulation of Lanchester\u2019s Square Law (as formulated in [15]), while Soemers uses a reformulation in order to support army reinforcements.", "startOffset": 259, "endOffset": 263}, {"referenceID": 16, "context": "Sustained is an extension of the model presented by Uriarte in [17].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "We used a Borda Count method [22] to estimate the target selection policy used by the players in our dataset.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "1) Map decomposition: Given a STARCRAFT map, we decompose it into a graph of regions using Perkins\u2019 algorithm [23] (implemented in the BWTA library).", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "Since most of RTS combats happen in these locations, we can conceivably expand the graph returned by Perkins\u2019 algorithm by turning each chokepoint into a region (this idea was also explored by Synnaeve and Bessi\u00e8re [24]).", "startOffset": 215, "endOffset": 219}, {"referenceID": 3, "context": "Most STARCRAFT bots are decomposed in several individual agents that perform different tasks in the game, such as scouting or construction [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 20, "context": "Other researchers achieved this by creating artificial datasets making the default STARCRAFT AI to play against itself [21] and", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "recording the state in traces; or using a crafted combat model (SPARCRAFT) to run thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "recording the state in traces; or using a crafted combat model (SPARCRAFT) to run thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "recording the state in traces; or using a crafted combat model (SPARCRAFT) to run thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26].", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "recording the state in traces; or using a crafted combat model (SPARCRAFT) to run thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 25, "context": "recording the state in traces; or using a crafted combat model (SPARCRAFT) to run thousands of combat situations [16], [20], while others used data from professional human players [19], [25], [26].", "startOffset": 192, "endOffset": 196}, {"referenceID": 18, "context": "In Synnaeve and Bessi\u00e8re\u2019s work [19] they consider a new combat when a unit is killed.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "STARCRAFT replays of professional player games are widely available, and several authors have compiled collections of such replays in previous work [19], [25].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "STARCRAFT replays of professional player games are widely available, and several authors have compiled collections of such replays in previous work [19], [25].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "For example, Synnaeve and Bessi\u00e8re [19] proposed recording information at three different levels of abstraction:", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "On the other hand, Robertson and Watson [27] proposed a uniformed information gathering, recording all the information every 24 frames or every 7 frames during attacks to have a better resolution than the previous work.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "Then, in order to observe the performance in a real game, we incorporated the combats models as forward models in a MCTS algorithm appropriate for RTS games (MCTSCD [8]).", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "perform this evaluation, we incorporated our MCTS approach into the STARCRAFT bot Nova [28] and evaluated the performance of using MCTSCD [8] to command the army during a real game (all the military units are controlled by a single MCTS algorithm simulating the whole game).", "startOffset": 138, "endOffset": 141}], "year": 2016, "abstractText": "Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require access to a forward model (or \u201csimulator\u201d) of the game at hand. However, in some games such forward model is not readily available. This paper presents three forward models for two-player attrition games, which we call \u201ccombat models\u201d, and show how they can be used to simulate combat in RTS games. We also show how these combat models can be learned from replay data. We use STARCRAFT as our application domain. We report experiments comparing our combat models predicting a combat output and their impact when used for tactical decisions during a real game.", "creator": "LaTeX with hyperref package"}}}