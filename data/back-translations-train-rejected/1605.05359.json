{"id": "1605.05359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering", "abstract": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. The skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over a common state space. Another major advantage of the approach is that it does not need a prior model of the MDP and can work well even when the MDPs are constructed from sampled trajectories. Finally, we present our attempts to extend the automated skills acquisition framework to complex tasks such as learning to play video games where we use deep learning techniques for representation learning to aid our spatio-temporal abstraction framework.", "histories": [["v1", "Tue, 17 May 2016 20:44:19 GMT  (830kb,D)", "http://arxiv.org/abs/1605.05359v1", "13 pages, 8 figures"], ["v2", "Tue, 20 Sep 2016 19:14:20 GMT  (5028kb,D)", "http://arxiv.org/abs/1605.05359v2", "Revised version of ICML 16 Abstraction in Reinforcement Learning workshop paper"]], "COMMENTS": "13 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["aravind s lakshminarayanan", "ramnandan krishnamurthy", "peeyush kumar", "balaraman ravindran"], "accepted": false, "id": "1605.05359"}, "pdf": {"name": "1605.05359.pdf", "metadata": {"source": "META", "title": "Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks", "authors": ["Ramnandan Krishnamurthy", "Aravind Lakshminarayanan", "Peeyush Kumar", "Balaraman Ravindran"], "emails": ["NANDPARIKRISH@GMAIL.COM", "ARAVINDSRINIVAS@GMAIL.COM", "AGOOVI@GMAIL.COM", "RAVI@CSE.IITM.AC.IN"], "sections": [{"heading": null, "text": "Reports from the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Copyright 2016 by the author (s)."}, {"heading": "1. Motivation and Introduction", "text": "This year it is as far as never before in the history of the city, where it is as far as never before that it is a place where it is a place, it is a place."}, {"heading": "2. Option Generation Framework", "text": "We divide this section into two parts: First, we explain the spectral cluster algorithm PCCA + and motivate its use for spatial abstraction. In the second part, option generation using PCCA + is discussed."}, {"heading": "2.1. Spatial Abstraction using PCCA+", "text": "Given an algebraic representation of the graph representing an MDP, we want to find suitable abstractions aligned to the underlying structure. We use a spectral clustering algorithm to do this. Central to the idea of spectral clustering is the graph Laplacian, which is derived from the similarity graph. There are many close links between the topological properties of the graphs and the graph Laplacian, which uses spectral clustering methods to divide the data into clusters. However, although the spectra of the Laplacian preserve the structural properties of the graph, clustering data in the eigenraum of the Laplacian do not guarantee this. For example, k-means clustering (Ng et al, 2001) in the eigenraum of the Laplacian will only work if the clusters are in disjoint convex sets of the underlying eigenspace."}, {"heading": "2.2. Option generation from PCCA+", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1. OPTION", "text": "Formally, an option is a tuple O = (I, \u00b5, \u03b2), where: \u2022 I is the initiation set: a set of states from which the action can be activated. \u2022 \u00b5 is a policy function in which \u00b5 (s, a) represents the preference value given to the action when it is in state s and the following option O. \u2022 \u03b2 is the termination function: when an agent enters a state s while following option O, it most likely terminates the option \u03b2 (s). If the agent is in state s in which it can start an option, it can choose from all options O for which s, I (O), and from all primitive measures that can be taken in s. The choice is dictated by the guidelines that guide the agent. As it executes an option, the agent follows this option and moves from state to state, with the option to terminate by \u03b2 being decided."}, {"heading": "2.2.2. COMPOSING OPTIONS FROM PCCA+HRL", "text": "PCCA + divides our state into abstract states - so moving between different abstract states is an important operation that involves a sequence of primitive actions. We create options that allow our agent to move from a state belonging to a particular abstract state to another abstract state, that is, abstract tasks (options) are transitions between abstract states. The membership function returned by PCCA + provides a very elegant way to compose options for realizing these abstract states. Each state has a specific membership value for each abstract state. We can move to another abstract state (options) simply by following the positive course of the membership value to these abstract states. This will gradually move us through states that have a greater and greater affiliation to the abstract state, until we finally reach a state belonging to that abstract state. In the case of multiple exits or bottlenecks, SCCA + HRL is able to compose several options, each of which is placed in the respective state."}, {"heading": "2.2.3. ONLINE AGENT", "text": "The previous sections describe what to do in light of the MDP model. However, we want our agent to work online. Here, we assess our transition matrix based on sampled trajectories and feed them into PCCA + to obtain the abstract states. We generate options and expand our agent with them. The agent then goes from state to state and selects the available options and primitive actions, updating its value function using any appropriate amplification technology such as SMDP Q-Learning. He continually updates the counting of the transition matrix and re-runs PCCA + to obtain the new success matrix and generate new options, repeatedly until it converges. However, in practice it is too expensive to perform PCCA + after each episode. Therefore, we implement the agent with a simple amplification learning algorithm such as Q-Learning for a fixed number of episodes until he learns a decent policy. Then, we implement this policy by freezing the period and following the other states for several more episodes."}, {"heading": "2.2.4. 2-ROOM DOMAIN", "text": "This is a very simple area where you can still acquire Skills. The area consists of 2 rooms of different sizes, where the agent must start from the first room and reach a specific target state in the second room (Figure 2). Typical skills acquired would be to reach the door from any state in the first room and navigate from that gate to the intended destination state (2 abstract tasks and 3 abstract states), which is exactly what we observe when using the PCCA + HRL framework (Figure 3) after 3 episodes of each 3000 kilometre trajectory. We find 3 abstract states in which an abstract state corresponds to the individual target state itself. Figure 4 compares the average return for different methods while solving the same task. We plot the average return in terms of the number of decision periods applied and compare it with different methods. Our approach identifies 2 options compared to 12 options by LCut and 10 by Random Options with consistently higher average returns than those between the two states."}, {"heading": "2.2.5. MODEL ESTIMATION", "text": "We now propose an online method to find spatio-temporal abstractions efficiently, while the agent pursues a different strategy, inspired by the UCT framework, which is a Monte Carlo search algorithm based on rollouts; a rollout-based algorithm builds its look-ahead tree by repeatedly scanning episodes from the initial state; the tree is built by adding to it incrementally the information collected during an episode; we use the UCT algorithm because it is more effective than the vanilla Monte Carlo planning, in which the actions are uniformly scanned, while UCT performs a selective scanning of actions; in the UCT approach, in the state of s, in the depth of d, the action that Qt maximizes (s, d) + cNs, d (t) + cNs, d (t), Ns, d (t), (t), d (d), is scanned (d), an abtact (d) is selected, an abtact (d) is scanned (d)."}, {"heading": "Q =\u21d2 ActionV alueFunction", "text": "1: Note the initial state so 2: Initialize Q arbitrarily 3: Initialize T transition matrix 4: U = {} 5: for e = 1 to do maximum number of episodes 6: Initialize the membership function, Simplex vertex Y = PCCA + (\u03c4) 7: Find all pairs of interconnected abstract states Ck = (Sj, Sk) from the non-zero entries in the previous entries, for the so-called Ck (1) 12: i = k; si = s0 13: while not the end of episode Do14: Oi {argmaxOQ (si, O) w.p 1 \u2212 random option, for the so-called Ck (1) 12: i = k."}, {"heading": "2.2.6. TAXI DOMAIN", "text": "The taxi problem can be formulated as an episode MDP with the 3 state variables: the position of the taxi (values 1-25), the position of the passenger in this world, which is marked as R, B (reen), G (reen) and Y (re). The taxi problem is in each episode inhabited by a taxi driver. There are four specially designated places in this world, which are marked as R."}, {"heading": "2.2.7. NOTE:", "text": "The PCCA + HRL periodically updates the transition matrix and learns options. With each sampled trajectory, the structure of the transition matrix changes, which in turn alters the determined spatial abstractions. In order to define the SMDP update rule, we still need a mechanism to synchronize the previous options with new ones. This can easily be done by mapping the vertices of the simplex returned by PCCA +. Let Y-1 and Y-2 be the eigenvector matrices returned by PCCA + in iterations 1 and 2. The similarity metric between simplex vertex i in iteration 1 and vertex j in iteration 2 is given by Sim12 (i, j), where Squ12 = Y-1-Y-2-1. Using \u042112, we assign the vertex i of Simplex-1 to the vertex j of Simplex-2 using the Munkre algorithm (a.k.12) to the convergence between the vertex and the weighting method."}, {"heading": "3. Playing video games with options", "text": "In the previous section, the effectiveness of the PCCA + HRL framework was demonstrated using issues such as the 2-room domain and the taxi domain. Although these issues require abstraction in politics, they have a considerably small state space. The PCCA + HRL framework needs to be expanded to include large complex problems with much higher dimensional state spaces and many other complex options. We conducted experiments with Infinite Mario and the Atari 2600 game Seaquest to test our proposed model, and the two games differ in the level at which the emulator provides the agent with the game state - Mario provides an overarching representation that tells the agent what type of object is present in a particular grid cell, while Atari games such as Seaquest provide a pixel space representation that makes object recognition an additional task for the agent."}, {"heading": "3.1. Sampling trajectories", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3.2. Representation Learning", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "3.3. State Aggregation", "text": "In this area, we are able to establish a functioning structure that meets people's needs."}, {"heading": "3.3.1. NOTE", "text": "We have selected mini-batch-K-Means with local feature selection to solve the problems of the enormous number of data points and high dimensionality. However, we do not claim that mini-batch-K-Means is the best approach to accomplish the aggregation of state space in addition to the representation of the action-driven network. We will leave future work to experiment with other state aggregation techniques (clusters) and evaluate the effectiveness of the PCCA + HRL framework for each state."}, {"heading": "3.4. Model Learning", "text": "After state aggregation, we closely follow the PCCA + HRL framework, which has already been described to identify the metastable regions of the state and the options they naturally derive from the PCCA + connectivity information. That is, when an agent (in the sense of aggregated micro-states) is used in a given time period to populate a counting matrix that considers the number of transitions for each (s, s) tuple, that is, when an agent becomes active at a certain time period and passes into the state of s, we increase the element corresponding to the (s, s) tasks in the matrix. Note that the states here are not the original states (at the pixel level), but the aggregated micro-states that we cluster after Deep Representation Learning & K-Means. As previously described, we integrate the reward structure of the environment into the transition matrix."}, {"heading": "3.5. Intra-Option Value Learning", "text": "In this section, we describe our intensified learning attempts after deriving the options of PCCA +. We are experimenting with two different approaches. In the first phase, we use the TemporalReLearner (which uses convolutional and recurring layers) to engage in our representation in the K-Means group; the second phase is frozen, and the third phase follows the third; in the third phase, we use the hidden layers and signatures to have as many units as the number of primitive actions and options; the third phase is frozen, in which we refer only to the MLP layers; and in the third phase, we learn the representation of a DQN layer. \""}, {"heading": "4. Discussion", "text": "Dre rf\u00fc eid eeirrrrrrteeSrteee\u00fcgr rf\u00fc ide rf\u00fc eeisrsrrteeVnlrteeeirrrrrlteeeeeoiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuirrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5. Acknowledgement", "text": "We thank the reviewers of the Abstraction in Reinforcement Learning ICML 2016 workshop for their useful comments on various components of our framework. The DQN implementation we use is: https: / / github.com / spragunr / deep _ q _ rl and is based on Theano (Bergstra et al., 2010)."}], "references": [{"title": "High-level reinforcement learning in strategy games", "author": ["Amato", "Christopher", "Shani", "Guy"], "venue": "9th International Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Amato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2010}, {"title": "Reinforcement learning with long shortterm memory", "author": ["Bakker", "Bram"], "venue": null, "citeRegEx": "Bakker and Bram.,? \\Q2002\\E", "shortCiteRegEx": "Bakker and Bram.", "year": 2002}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Model approximation for HEXQ hierarchical reinforcement learning", "author": ["Hengst", "Bernhard"], "venue": "In Machine Learning: ECML 2004,", "citeRegEx": "Hengst and Bernhard.,? \\Q2004\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2004}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Crafting papers on machine learning", "author": ["P. Langley"], "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML", "citeRegEx": "Langley,? \\Q2000\\E", "shortCiteRegEx": "Langley", "year": 2000}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical Report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning (ICML", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "A random walks view of spectral segmentation", "author": ["Meila", "Marina", "Shi", "Jianbo"], "venue": null, "citeRegEx": "Meila et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Meila et al\\.", "year": 2001}, {"title": "Qcut - dynamic discovery of sub-goals in reinforcement learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Machine Learning: ECML 2002, 13th European Conference on Machine Learning,", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Subspace clustering for high dimensional data: a review", "author": ["Parsons", "Lance", "Haque", "Ehtesham", "Liu", "Huan"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Parsons et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Parsons et al\\.", "year": 2004}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": null, "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,", "citeRegEx": "Sculley,? \\Q2010\\E", "shortCiteRegEx": "Sculley", "year": 2010}, {"title": "Reinforcement learning of local shape in the game of go", "author": ["Silver", "David", "Sutton", "Richard", "Muller", "Martin"], "venue": "IJCAI, 7:1053\u20131058,", "citeRegEx": "Silver et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2007}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Intra-option learning about temporally abstract actions", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder P"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Temporal abstraction in monte carlo tree search", "author": ["Vafadost", "Mostafa"], "venue": "Masters thesis, Department of Computer Science, University of Alberta,", "citeRegEx": "Vafadost and Mostafa.,? \\Q2013\\E", "shortCiteRegEx": "Vafadost and Mostafa.", "year": 2013}, {"title": "Perron cluster analysis and its connection to graph partitioning for noisy data", "author": ["Weber", "Marcus", "Rungsarityotin", "Wasinee", "Schliep", "Alexander"], "venue": "Technical Report 04-39, ZIB,", "citeRegEx": "Weber et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weber et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "\u2022 Using clustering methods (spectral or otherwise) to separate out different strongly connected components of the Markov Decision Process (MDP) and identifying access-states that connect different clusters (Menache et al., 2002).", "startOffset": 206, "endOffset": 228}, {"referenceID": 20, "context": "We use PCCA+, a spectral clustering algorithm from conformal dynamics (Weber et al., 2004) that not only partitions the MDP but also returns the connectivity information between the regions.", "startOffset": 70, "endOffset": 90}, {"referenceID": 12, "context": "For example, k-means clustering (Ng et al., 2001) in the eigenspace of the Laplacian will only work if the clusters lie in disjoint convex sets of the underlying eigenspace.", "startOffset": 32, "endOffset": 49}, {"referenceID": 20, "context": "Therefore, we take inspiration from the conformal dynamics literature, where (Weber et al., 2004) do a similar analysis to detect conformal states of a dynamical system.", "startOffset": 77, "endOffset": 97}, {"referenceID": 20, "context": "A projection method described in (Weber et al., 2004) is used to find the membership of each of", "startOffset": 33, "endOffset": 53}, {"referenceID": 15, "context": "The main idea proposed in (Sculley, 2010) is to use randomly sampled batches (of a fixed small batch-size) at each iteration.", "startOffset": 26, "endOffset": 41}, {"referenceID": 13, "context": "(Parsons et al., 2004) propose a soft feature selection procedure in which features are assigned local weights", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "The cost function of the above network is designed such that the parameters \u03b8i at iteration i are adjusted according to the Intra-Option Q learning rule (Sutton et al., 1998).", "startOffset": 153, "endOffset": 174}, {"referenceID": 5, "context": "erarchical Reinforcement Learning (Kulkarni et al., 2016).", "startOffset": 34, "endOffset": 57}, {"referenceID": 5, "context": "However, we believe this (with human trajectories sampled for model learning) is still a better framework compared to explicitly giving the agent a list of skills as done in recent work by (Kulkarni et al., 2016).", "startOffset": 189, "endOffset": 212}, {"referenceID": 5, "context": "Therefore, we need to experiment with different networks for the actions and options as done by (Kulkarni et al., 2016).", "startOffset": 96, "endOffset": 119}, {"referenceID": 14, "context": "It would also be a good direction to see how much the PCCA+HRL framework benefits from the more recent attempts at improving experience replays like the Prioritized Experience Replay proposed by (Schaul et al., 2016).", "startOffset": 195, "endOffset": 216}], "year": 2016, "abstractText": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. The skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over a common state space. Another major advantage of the approach is that it does not need a prior model of the MDP and can work well even when the MDPs are constructed from sampled trajectories. Finally, we present our attempts to extend the automated skills acquisition framework to complex tasks such as learning to play video games where we use deep learning techniques for representation learning to aid our spatio-temporal abstraction framework. Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s). 1. Motivation and Introduction The core idea of hierarchical reinforcement learning is to break down the reinforcement learning problem into subtasks through a hierarchy of abstractions. Typically, in the full reinforcement learning problem, the agent is assumed to be in one state of the Markov Decision Process at every time step. The agent then performs one of several possible primitive actions. Based on the agent\u2019s state at time t, and the action it takes from that state, the agent\u2019s state at time t + 1 is determined. For large problems, however, this can lead to too much granularity: when the agent has to decide on each and every primitive action at every granular state, it can often lose sight of the bigger picture. However, if a series of actions can be abstracted out as an abstract action, the agent can just remember the series of actions that was useful in getting it to a temporally distant useful state from the initial state. This is typically referred to as an option or a skill in the reinforcement learning literature. A good analogy is a human planning his movement for a traversal from current location A to a destination B. We identify intermediate destinations Ci to lead us fromA toB when planning fromA, instead of worrying about the exact mechanisms of immediate movement at A which are abstracted over. Options are a convenient way of formalising this abstraction. In keeping with the general philosophy of reinforcement learning, we want to build agents that can automatically discover options with no prior knowledge, purely by exploring the environment. Thus, our approach falls into the broad category of automated discovery of skills. In order to exploit task structure, hierarchical decomposition introduces models defined by stand-alone policies (also known as temporally-extended actions, options, or skills) that can take multiple time steps to execute. Skills can exploit representing structure by representing subroutines that are executed multiple times during execution of a task. Such skills which are learnt in one task can be reused ar X iv :1 60 5. 05 35 9v 1 [ cs .L G ] 1 7 M ay 2 01 6 Submission and Formatting Instructions for ICML 2016 in a different task as long as it requires execution of the same subroutine. Options also make exploration more efficient by providing the decision maker with a high-level behaviour to look ahead to the completion of the corresponding subroutine. Automated discovery of skills or options has been an active area of research and several approaches have been proposed for the same. The current methods could be broadly classified into sample trajectory based and partition based methods. Some of them are: \u2022 Identifying bottlenecks in the state space, where the state space is partitioned into sets and the transitions between two sets of states that are rare can be seen as introducing bottleneck sets at the respective points of such rare transitions. Policies to reach such states are cached as options (McGovern & Barto, 2001). \u2022 Using the structure present in a factored state representation to identify sequences of actions that cause what are otherwise infrequent changes in the state variables: these sequences are cached away as options (Hengst, 2004). \u2022 Obtaining a graphical representation of an agent\u2019s interaction with its environment and using betweenness centrality measures to identify subtasks (Simsek & Barto, 2008). \u2022 Using clustering methods (spectral or otherwise) to separate out different strongly connected components of the Markov Decision Process (MDP) and identifying access-states that connect different clusters (Menache et al., 2002). While these methods have had varying amounts of success, they have certain deficiencies. Bottleneck based approaches don\u2019t have a natural way of identifying the part of the state space where options are applicable without external knowledge about the problem domain. Spectral methods need some form of regularization in order to prevent unequal splits that might lead to arbitrary splitting of the state space. We present a framework that detects well-connected or meta stable regions of the state space from a MDP model estimated from trajectories. We use PCCA+, a spectral clustering algorithm from conformal dynamics (Weber et al., 2004) that not only partitions the MDP but also returns the connectivity information between the regions. We then propose a very effective way of composing options using the same framework to take us from one metastable region to another, giving us the policy for free. Once we have these options, we can use standard reinforcement learning algorithms to learn a policy over subtasks to solve the given task. Specifically, we show results using SMDP Qlearning on the 2-room domain. For our attempt at extending it to higher dimensional state space tasks such as Atari 2600 video games, we append the learnt options to the set of primitive actions using Intra-Option Value learning to learn a policy solving the given task. One major advantage of the approach is that we get the policy for the options for free while doing the partitioning by exploiting the membership functions returned by PCCA+. Our approach is able to learn reasonably good skills even with limited sampling which makes it useful in situations where exploration is limited by the environment costs. It also provides a way to refine the abstractions in an online fashion without explicitly reconstructing the entire MDP. More importantly, we extend it to the case where the state space is so large that exact modeling is not possible. In this case, we take inspiration from the recent work on forward prediction to learn the model (Oh et al., 2015) to use Deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) to learn spatio-temporal representations of the state space. Using the learnt representation, we perform state-aggregation using clustering techniques and estimate our transitional model for the abstract space on these aggregated states. We list the advantages of this approach below: \u2022 Skills are acquired online, from sampled trajectories instead of requiring a prior model of the MDP. \u2022 Instead of looking for bottleneck states, we look for well connected regions and hence, the discovered options are better aligned to the structure of the state space. \u2022 The approach returns connectivity information between the metastable regions which can be used to construct an abstract graph of the state space, combining spatial and temporal information meaningfully. \u2022 The clustering algorithm provides a fuzzy membership for every state in belonging to a particular metastable region, which provides a powerful way to compose options naturally. We organize the rest of the paper as follows: We first explain the Option Generation Framework that we propose, by delving on the important aspects of the spectral clustering algorithm PCCA+ and how PCCA+ can be used to generate options. We show results on the 2-room domain for this framework. The next part of the paper focuses on our attempt to extend this framework for more complex tasks such as playing video games like Seaquest on the Atari 2600 domain with options. We explain the motivation and usage of deep networks, the clustering algorithms used for state-aggregation, followed by the model used and Submission and Formatting Instructions for ICML 2016 initial results. We then conclude with a discussion on the challenges in this approach and also comparison with other recent attempts at Hierarchical reinforcement learning for higher dimensional tasks. 2. Option Generation Framework We divide this section into two parts: We first explain the spectral clustering algorithm PCCA+ and motivate its usage for spatial abstraction. The second part discusses the option generation using PCCA+. 2.1. Spatial Abstraction using PCCA+ Given an algebraic representation of the graph representing a MDP we want to find suitable abstractions aligned to the underlying structure. We use a spectral clustering algorithm to do this. Central to the idea of spectral clustering is the graph Laplacian which is obtained from the similarity graph. There are many tight connections between the topological properties of graphs and the graph Laplacian matrices, which spectral clustering methods exploit to partition the data into clusters. However, although the spectra of the Laplacian preserves the structural properties of the graph, clustering data in the eigenspace of the Laplacian does not guarantee this. For example, k-means clustering (Ng et al., 2001) in the eigenspace of the Laplacian will only work if the clusters lie in disjoint convex sets of the underlying eigenspace. Partitioning the data into clusters by projecting onto the largest k-eigenvectors (Meila & Shi, 2001) does not preserve the topological properties of the data in the eigenspace of the Laplacian. For the task of spatial abstraction, the proposed framework requires a clustering approach that exploits the structural properties in the configurational space of objects as well as the spectral subspace, quite unlike earlier methods. Therefore, we take inspiration from the conformal dynamics literature, where (Weber et al., 2004) do a similar analysis to detect conformal states of a dynamical system. They propose a spectral clustering algorithm PCCA+, which is based on the the principles of Perron Cluster Analysis of the transition structure of the system. We extend their analysis to detect spatial abstractions in autonomous controlled dynamical systems. In this approach, the spectra of the Laplacian L (derived from the adjacency matrix S) is constructed and the best transformation of the spectra is found such that the transformed basis aligns itself with the clusters of data points in the eigenspace. A projection method described in (Weber et al., 2004) is used to find the membership of each of the states to a set of special points lying on the transformed basis, which are identified as vertices of a simplex in the R subspace (the Spectral Gap method is used to estimate the number of clusters k). For the first order perturbation, the simplex is just a linear transformation around the origin and to find the simplex vertices, one needs to find the k points which form a convex hull such that the deviation of all the points from this hull is minimized. This is achieved by finding the data point which is farthest located from the origin and iteratively identify data points which are located farthest from the hyperplane fit to the current set of vertices. Figure 1. Simplex First order and Higher order Perturbation Algorithm 1 PCCA+ 1: Construct Laplacian L 2: Compute n (number of vertices) eigenvalues of L in descending order 3: Choose first k eigenvalues for which ek\u2212ek+1 1\u2212ek+1 > tc (Spectral Gap Threshold). 4: Compute the eigenvectors for corresponding eigenvalues (e1, e2, \u00b7 \u00b7 \u00b7 , ek) and stack them as column vectors in eigenvector matrix Y . 5: Let\u2019s denote the rows of Y as Y(1),Y(2), \u00b7 \u00b7 \u00b7 ,Y(N) \u2208 R 6: Define \u03c0(1) as that index, for which ||Y (\u03c0(1))||2 is maximal. Define \u03b31 = span{Y (\u03c0(1))} 7: For i = 2, \u00b7 \u00b7 \u00b7 , k: Define \u03c0i as that index, for which the distance to the hyperplane \u03b3i\u22121, i.e., ||Y (\u03c0i) \u2212 \u03b3i\u22121||2 is maximal. Define \u03b3i = span{Y (\u03c01), \u00b7 \u00b7 \u00b7 , Y (\u03c0i)}. ||Y (\u03c0i) \u2212 \u03b3i\u22121||2 = ||Y (\u03c0i)\u2212 \u03b3 i\u22121((\u03b3i\u22121\u03b3 i\u22121)\u03b3i\u22121)Y (\u03c0i) )|| The PCCA+ algorithm returns a membership function, \u03c7, defining the degree of membership of each state s to an abstract state Sj . The connectivity information between two abstract states (Si, Sj) is given by (i, j) entry of \u03c7L\u03c7 while the diagonal entries provide relative connectivity information within a cluster. The connectivity information is utilized to learn decision policies across abstract states which is described in the next section. There is an intrinsic mechanism to return information about the goodness of clustering of states from the presence of sharp peaks (indicates good clustering) in the eigenvalue distribution. Submission and Formatting Instructions for ICML 2016 2.2. Option generation from PCCA+", "creator": "LaTeX with hyperref package"}}}