{"id": "1605.05362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Yelp Dataset Challenge: Review Rating Prediction", "abstract": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.", "histories": [["v1", "Tue, 17 May 2016 20:52:33 GMT  (184kb,D)", "http://arxiv.org/abs/1605.05362v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["nabiha asghar"], "accepted": false, "id": "1605.05362"}, "pdf": {"name": "1605.05362.pdf", "metadata": {"source": "META", "title": "Yelp Dataset Challenge: Review Rating Prediction", "authors": ["Nabiha Asghar"], "emails": ["nasghar@uwaterloo.ca"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able, are able, are not able to determine for themselves what they want and what they want to do."}, {"heading": "2. Related Work", "text": "Most of the recent work related to IMDb rating predictions relies on sentiment analysis to extract features from the review text. Qu et al. (2010) solved this problem for Amazon.com ratings by proposing a novel feature extraction method called Bag-of-Opinions that extracts opinions (consisting of a root word, a modifier and / or a negative word) from the review corpus, calculates their sensitivity rating and predicts ratings by aggregating the opinions existing in that review text, and combines them with a domain-dependent rating model. Leung et al. (2006) use a novel relative frequency method to create an opinion dictionary to derive ratings from the review text. This method estimates the strength of a word in relation to a particular sentiment class as the relative frequency of its occurrence in that film dictionary, and integrates that rating algorithm with the IMDb."}, {"heading": "3. Data Description", "text": "We use the Yelp dataset as part of their Dataset Challenge 2014 (Dataset, 2014) to train and test predictive models. The dataset contains data from Phoenix, Las Vegas, Madison, Waterloo, and Edinburgh, and contains information about 42,153 companies, 320,002 business ratings, 31,617 check-in sets, 403,210 tips, and 1,125,458 text ratings. Specifically, the dataset consists of five files, one for each object type: business, rating, user, check-in, and tip. Each file consists of one Json object per line. Thus, a company is represented in the \"business.json\" file as a Json object that specifies the business ID, its name, its location, stars, rating count, opening hours, and so on. A text rating is a json object in the \"review.json\" file that specifies the business ID, user ID, and star values between 1 and 5."}, {"heading": "4. Experimental Setup", "text": "In this thesis, we build sixteen different prediction models by combining each of four different methods of feature extraction with four different monitored learning algorithms. In this section, we describe the pre-processing phase, the four feature extraction methods, the four monitored learning algorithms, and two key performance indicators."}, {"heading": "4.1. Preprocessing", "text": "We first write some basic Python scripts to separate the restaurants from the business.json file and separate the restaurant reviews from the review.json file. We then edit the text reviews as follows. Yelp allows users to write text reviews in free form, which means that a user can use excessive capital letters and punctuation (for example, to express his / her strong dislike of a review) and slang words within a review. In addition, stopwords such as \"that,\" \"that,\" that, \"that\" is, \"etc. often appear in reviews and are not very useful, so it is necessary to pre-edit the reviews to extract meaningful content from each of them. To do this, we use standard Python libraries to remove capital letters, stopwords and punctuations."}, {"heading": "4.2. Feature Extraction", "text": "We use four methods to extract useful features from the review body and create a feature vector for each review, each of which is based on a semantic analysis of the text."}, {"heading": "4.2.1. Unigrams", "text": "In the Unigram model (also referred to as the \"dictionary model\"), each unique word in the pre-edited corpus of the review is considered a feature. Therefore, building a feature vector for a review is straightforward. First, a dictionary of all the words that occur in the review corpus is created; then, a text weighting matrix is created in which entry (i, j) is the frequency of occurrence of word i in the j'th review. Finally, we apply the TF-IDF (Term Frequency - Inverse Document Frequency) weighting method to this matrix to obtain the final feature matrix. This weighting method attributes less weight to words that occur more frequently in reviews (e.g. \"Food\") because they are generally not good distinctions between the two ratings, and a high weight to rarer words. Each column of this matrix is a feature vector of the overall case of this review.46 The size of this review is the 17tz."}, {"heading": "4.2.2. Unigrams & Bigrams", "text": "The unique model is a widely used method of feature extraction in the processing of natural language. It is quite easy to implement and in many cases delivers surprisingly good results. However, its disadvantage is its inability to capture relationships between two words (e.g. a word and its modifier, a word and its negation, etc.) because it treats each word in isolation. To capture the effect of phrases such as \"delicious burger\" and \"not delicious,\" we add Bigrame to the unique model. Now, in addition, the dictionary consists of all the 2-tuples of words (i.e. all consecutive pairs of words) that occur in the corpus of ratings. The matrix is calculated as before; it now has more lines. As before, we apply the weighting of TF and IDF to this matrix, so that ordinary words are given less importance and rare words are given more meaning."}, {"heading": "4.2.3. Unigrams, Bigrams & Trigrams", "text": "To grasp the effect of phrases such as \"delicious fish burger,\" we now add trigrams (i.e. all triple words of consecutive words) to the Unique + Bigram model. The other calculations for the structure of the character matrix are the same as before. Note, however, that the same trigram would rarely occur in different evaluations, since two different people are unlikely to use the same 3-word phrase in their evaluations. Therefore, the results of this model are not expected to differ greatly from the Unique + Bigram model. In this case, the total number of features is 31,677,669."}, {"heading": "4.2.4. Latent Semantic Indexing (LSI)", "text": "Latent semantic indexing (LSI) (Hofmann, 1999) is a more complex method of lexical concordance that goes beyond the exact concordance of words. It finds \"topics\" in reviews that are words with similar meanings or words that occur in a similar context. In LSI, we first construct a word-review matrix M of size m \u00b7 t using the Unikram model, and then do the Singular Value Decomposition (SVD) of M.SV D (M) = U \u00b7 S \u00b7 V TThe SVD function outputs three matrices: the word-topic matrix U of size m \u00b7 m, the rectangular diagonal matrix S of size m \u00b7 t, which contains t singular values, and the transposition of the theme-review matrix V of size t \u00b7 t. We use V as a character matrix of size m \u00b7 m. Singular matrix S have no decreasing sequence of singular values, singular matrix S are the most important ones in the sequence."}, {"heading": "4.3. Supervised Learning", "text": "To train our prediction models, we use four monitored learning algorithms."}, {"heading": "4.3.1. Logistic Regression", "text": "In the logistic regression (Freedman, 2009) the conditional probability function P (s | r) is modeled, where r is a characteristic vector for the check r and s belongs to the group of class names {1, 2, 3, 4, 5}. Then this probability function is calculated for all values of s, and the s-value, which corresponds to the highest probability, is output as the final class name (star classification) for this check."}, {"heading": "4.3.2. Na\u0308\u0131ve Bayes Classification", "text": "A classifier of Na \ufffd \u0131ve Bayes (Ng and Jordan, 2002) assumes the assumption of Na \ufffd \u0131ve Bayes (i.e. it assumes conditional independence between any pair of characteristics of a certain class) to model the common probability P (r, s) for any character vector r and a star rating s. Then, the common probability function for all values of s is calculated, and the s value corresponding to the highest probability is output as the definitive class name for the rating r *. In this paper, we use the multinomic classification of Na \ufffd \u0131ve Bayes, which assumes that P (ri | s) is a multinomic distribution for all i. This is a typical choice for the classification of documents because it works well for data that can be converted into censuses, e.g. weighted word frequencies in the text."}, {"heading": "4.3.3. Perceptrons", "text": "A perceptron (Rosenblatt, 1957) is a linear classifier that outputs class names instead of probabilities. It uses a gradient-like rule to iterate over the training set several times to reclassify all misclassified examples until they are all correctly classified. For linear separable data, a perceptron convergence rule says that after a limited number of iterations, a solution is always found. For data that cannot be linearly separated, there is oscillation that can be detected automatically. Perception solutions cannot be unique because the edge of the linear decision boundary is ignored. In our experiments, we set the number of iterations to 50 (a typical choice), meaning the classifier rotates 50 times over the entire training set."}, {"heading": "4.3.4. Linear Support Vector Classification (SVC)", "text": "Support Vector Machines (SVM) (Tsochantaridis et al., 2004) are advanced versions of perceptrons, eliminating the non-uniqueness of solutions by optimizing the margin around the decision boundary, and treating non-separable data by allowing misclassifications. A parameter C controls the overfit. If C is small, the algorithm focuses on maximizing the margin, even if this means more misclassifications, and for positional values of C, the margin decreases if this helps to correctly classify more examples. In our experiments, we use linear SVMs for multiclassification. The tolerance of the convergence criterion is 0.001. For each extraction method of characteristics, we perform an internal triple cross-validation to select the value of C that provides the highest accuracy. It turns out that C = 1.0 works best each time."}, {"heading": "4.4. Performance Metrics & Implementation Details", "text": "We use 80% of the data set for training and 20% for testing. For each of the sixteen prediction systems, we perform a triple cross-validation of the training set and calculate two key figures, Root Mean Squared Error (RMSE) and accuracy for the training fold and validation fold. The entire implementation is done on an Intel Core i5 CPU with 4 cores (1.6 GHz each), 8 GB RAM and 64 bit Ubuntu 12.04 operating system. The programming language used is Python, and the numpy, scipy and scikit-learn libraries are widely used."}, {"heading": "5. Results and Analysis", "text": "In this section, we present the results of the four extraction methods separately from each other. For each method, we show an RMSE diagram and an accuracy diagram; each diagram contains diagrams for the four classifiers. We then analyze these results to select the best of the sixteen systems, and finally, we evaluate the selected system on the test set."}, {"heading": "5.1. Unigrams", "text": "Figures 3 (a) and 4 (a) show the performance of the four classifiers in terms of the unique characteristics. The total number of features available is 171,846, and we do not know how many of them are useful, so we record the RMSE and the accuracy against the top 1 x number of characteristics. We see that as the number of characteristics increases, the training-related RMSE increases for each classifier and the training-related accuracy of each classifier decreases, but the validation-related RMSE and the validation-related accuracy decrease with about 10,000 characteristics. The only exception is the classifier Nave Bayes, whose RMSE reaches a minimum of about 10,000 characteristics, but then increases again. Clearly, perceptions perform worst, achieving a lowest RMSE of 1.25 and the highest accuracy of 43%. The Na \ufffd vive Bayes classifier is the second-worst, with the 96 RSE and 0.52% accuracy."}, {"heading": "5.2. Unigrams & Bigrams", "text": "Figures 3 (b) and 4 (b) show the performance of the four classifiers on the top x number of characteristics achieved by Uniques & Bigrams. In this case, the total number of available characteristics is over 7 million. We see that the RMSE and accuracy for each classifier improves compared to Figures 3 (a) and 4 (a). This is because bigrams occur often enough in the corpus, capture a lot of information that unigrams cannot capture, and there are, for example, the top 10 characteristics with the highest 10 TF-IDF weights.Much more meaningful characteristics. However, the general trends in training and validation foldabilities are fairly similar. Perceptions are still the worst, followed by the classifier Na \u00bfvive Bayes. Linear SVC and logistic regression are again fairly close together. The best RMSE and accuracy values for the SVC are 0.64% and 0.78% for the SVC, respectively."}, {"heading": "5.3. Unigrams, Bigrams & Trigrams", "text": "The results of this feature extraction method are almost identical to those of the Unigrams & Bigrams method (Figures 3 (b) and 4 (b)), and we omit the graphs due to the brevity of the space.The best RMS and accuracy values are 0.78 and 64%, achieved by logistic regression.Adding trigrams to the previous model is not helpful as trigrams rarely repeat.It is unlikely that two different users would use exactly the same 3 tuples to describe a restaurant.The TF-IDF weighting technique weighs almost all 3 tuples as very rare, so they are not very useful as a feature."}, {"heading": "5.4. Latent Semantic Indexing (LSI)", "text": "Figure 5 shows the results of the experiments with LSI. Figure 5 (a) is a graph of the 1000 highest individual values. We see that the graph starts at 200, which means that the 200 most important topics in the reviews are the most important for the formation of the model. Next, we evaluate each classifier using characteristic vectors up to 200 in length; the performance of each classifier is represented in Figure 5 (b) and (c) for the validation fold. Figure 5 (b) and (c) show some interesting patterns. Perceptions perform worst as usual, however, we see two peaks in the performance of about 170 and 200 characteristics where RMSE and accuracy suddenly improve. It is not clear why this is happening, but it suggests that we should consider more than 200 characteristics to see if more of these peaks occur and improve the best values for perception shadows."}, {"heading": "5.5. The Best Model: Performance on the Test Set", "text": "Based on the results in Figure 3, 4, and 5, we can see that logistic regression reached the highest accuracy of 64% by using the top 10,000 unigrams & bigrams as characteristics.2We could not obtain the training-intensive RMSE and accuracy results due to time constraints. For linear SVC, the RMSE and accuracy values are 1.05 and 56%, and for logistic regression, the values are 0.92 and 54%. These values are slightly worse than the validation-intensive values and may indicate overmatch; to correct this, we would have to add / adjust the regulation parameters and repeat all experiments."}, {"heading": "6. Conclusions & Future Work", "text": "We treat it as a 5-class classification problem and examine various features we get from Unigrams & Bigrams, which have better prediction systems than the others. Our system can be used to generate star ratings on review techniques, where users can write free text ratings without giving a star. Although the methods tested in this paper are extensive, they are by no means exhaustive. In fact, there are many avenues for improvement and future work. We can try more sophisticated technical methods, such as Parts-of-Speech (POS) tagging and spelling-checkers, to get more useful n-grams."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The impact of online recommendations and consumer feedback on sales", "author": ["P.Y. Chen", "S.Y. Wu", "J. Yoon"], "venue": "In Proceedings of the International Conference on Information Systems,", "citeRegEx": "Chen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2003}, {"title": "The digitization of word of mouth: promise and challenges of online feedback mechanisms", "author": ["C. Dellarocas"], "venue": "Management Science,", "citeRegEx": "Dellarocas.,? \\Q2003\\E", "shortCiteRegEx": "Dellarocas.", "year": 2003}, {"title": "Predicting a business star in yelp from its reviews text alone", "author": ["Mingming Fan", "Maryam Khademi"], "venue": "arXiv preprint arXiv:1401.0864,", "citeRegEx": "Fan and Khademi.,? \\Q2014\\E", "shortCiteRegEx": "Fan and Khademi.", "year": 2014}, {"title": "Statistical models: theory and practice", "author": ["David Freedman"], "venue": null, "citeRegEx": "Freedman.,? \\Q2009\\E", "shortCiteRegEx": "Freedman.", "year": 2009}, {"title": "Beyond the stars: Improving rating predictions using review text content", "author": ["Gayatree Ganu", "Noemie Elhadad", "Am\u00e9lie Marian"], "venue": "In WebDB,", "citeRegEx": "Ganu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ganu et al\\.", "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Integrating collaborative filtering and sentiment analysis: A rating inference approach", "author": ["Cane WK Leung", "Stephen CF Chan", "Fu-lai Chung"], "venue": "Proceedings of the ECAI 2006 workshop on recommender systems,", "citeRegEx": "Leung et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Leung et al\\.", "year": 2006}, {"title": "Incorporating reviewer and product information for review rating prediction", "author": ["F. Li", "N. Liu", "H. Jin", "K. Zhao", "Q. Yang", "X. Zhu"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and n\u00e4\u0131ve bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ng and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2002}, {"title": "The bag-of-opinions method for review rating prediction from sparse text patterns", "author": ["L. Qu", "G. Ifrim", "G. Weikum"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING", "citeRegEx": "Qu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2010}, {"title": "The perceptron, a perceiving and recognizing automaton Project Para", "author": ["Frank Rosenblatt"], "venue": "Cornell Aeronautical Laboratory,", "citeRegEx": "Rosenblatt.,? \\Q1957\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1957}, {"title": "Independent component analysis", "author": ["James V Stone"], "venue": "Wiley Online Library,", "citeRegEx": "Stone.,? \\Q2004\\E", "shortCiteRegEx": "Stone.", "year": 2004}, {"title": "Sparse nonnegative matrix approximation: new formulations and algorithms", "author": ["Rashish Tandon", "Suvrit Sra"], "venue": "Max Planck Institute for Biological Cybernetics,", "citeRegEx": "Tandon and Sra.,? \\Q2010\\E", "shortCiteRegEx": "Tandon and Sra.", "year": 2010}, {"title": "Support vector learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ACM", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "These online reviews function as the \u2018online word-of-mouth\u2019 (Dellarocas, 2003) and a criterion for consumers to choose between similar products.", "startOffset": 60, "endOffset": 78}, {"referenceID": 1, "context": "(Chen et al., 2003)) show that they have a significant impact on consumer purchase decisions as well as on product sales and business revenues.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Qu et al. (2010) tackle this problem for Amazon.", "startOffset": 0, "endOffset": 17}, {"referenceID": 6, "context": "Latent Semantic Indexing (LSI) (Hofmann, 1999) is a more sophisticated method of lexical matching, which goes beyond exact matching of words.", "startOffset": 31, "endOffset": 46}, {"referenceID": 4, "context": "In logistic regression (Freedman, 2009), the conditional probability function P (s|r) is modelled, where r is a feature vector for review r and s belongs to the set of class labels {1, 2, 3, 4, 5}.", "startOffset": 23, "endOffset": 39}, {"referenceID": 9, "context": "A N\u00e4\u0131ve Bayes (Ng and Jordan, 2002) classifier makes the N\u00e4\u0131ve Bayes assumption (i.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": "A perceptron (Rosenblatt, 1957) is a linear classifier that outputs class labels instead of probabilities.", "startOffset": 13, "endOffset": 31}, {"referenceID": 14, "context": "Support Vector Machines (SVM) (Tsochantaridis et al., 2004) are enhanced versions of perceptrons, in that they eliminate the non-uniqueness of solutions by optimizing the margin around the decision boundary, and handle non-separable data by allowing misclassifications.", "startOffset": 30, "endOffset": 59}, {"referenceID": 0, "context": "For feature extraction, we could try topic modelling techniques such as Latent Dirichlet Allocation (Blei et al., 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 100, "endOffset": 119}, {"referenceID": 13, "context": ", 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 43, "endOffset": 65}, {"referenceID": 12, "context": ", 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 104, "endOffset": 117}], "year": 2016, "abstractText": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user\u2019s star rating for a product, given the user\u2019s text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) N\u00e4\u0131ve Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.", "creator": "LaTeX with hyperref package"}}}