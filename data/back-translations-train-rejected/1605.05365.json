{"id": "1605.05365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Dynamic Frame skip Deep Q Network", "abstract": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.", "histories": [["v1", "Tue, 17 May 2016 20:58:41 GMT  (319kb,D)", "http://arxiv.org/abs/1605.05365v1", "6 pages, 8 figures. arXiv admin note: text overlap witharXiv:1506.08941by other authors"], ["v2", "Sat, 11 Jun 2016 01:04:13 GMT  (421kb,D)", "http://arxiv.org/abs/1605.05365v2", "IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and Challenges; 6 pages, 8 figures"]], "COMMENTS": "6 pages, 8 figures. arXiv admin note: text overlap witharXiv:1506.08941by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["aravind s lakshminarayanan", "sahil sharma", "balaraman ravindran"], "accepted": false, "id": "1605.05365"}, "pdf": {"name": "1605.05365.pdf", "metadata": {"source": "CRF", "title": "Dynamic Frame skip Deep Q Network", "authors": ["Aravind Lakshminarayanan", "Sahil Sharma", "Balaraman Ravindran"], "emails": ["aravindsrinivas@gmail.com,", "ssahil08@gmail.com,", "ravi@cse.iitm.ac.in"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related work", "text": "One of the first efforts to significantly advance the state of the art in the Atari 2600 domain was [Mnih et al., 2015]. Its architecture (DQN) motivated the use of revolutionary neural networks for playing in the Atari 2600 domain. [DQN] is a work that focuses on the power of frame rate with the experiments in the Atari 2600 domain. Our work is a direct modification of the DQN architecture. [Braylan et al] is a work that focuses on the power of frame rate."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Q Learning", "text": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015]. A game can be formulated as a sequence of state transitions (s, a, r, s) in a Markov decision-making process (MDP). The agent takes action a in a state by using the state action value Q (s, a), which is a measure of the long-term reward of action. Q-Learning ([Watkins and Dayan, 1992]) is a non-political Temporal Difference (TD) learning algorithm ([Sutton and Barto, 1998]) that can be used to learn an optimal Q (s, a) for the agent. Starting from a random initialization, the agent iteratively updates Q values by playing the game and receiving rewards."}, {"heading": "3.2 Deep Q Network (DQN)", "text": "In large games, it is often impractical to maintain the Q value for all possible state-action pairs. A solution to this problem is to approximate Q (s, a) by means of a parameterized function Q (s, a; \u03b8), which can generalize about states and actions using higher properties ([Sutton and Barto, 1998]). In order to be able to discover these features without feature engineering, we need an approximator for neural network functions. DQN ([Mnih et al., 2015]) approaches the Q value function with a deep neural network in order to be able to predict Q (s, a) over all actions a, for all states. The loss function used for learning a deep Q network is as follows: Li (\u03b8i et al.) [(yDQNi \u2212 Q (s, a; \u03b8i) et."}, {"heading": "4 The Dynamic Frame skip Deep Q Network", "text": "The key motivation behind the DFDQN architecture is the observation that the people in the architecture are almost always repeated. For certain states in the game, we play long sequences of the same action, while for some states, we adjust the repetitive actions that we introduce for the following architectural changes in the DQN. Let's revive the agent and replenish the oxygen with a long sequence of actions. To integrate the repetitive actions, we must initiate the following architectural changes in the DQN."}, {"heading": "5 Experiments and Results", "text": "In fact, it is the case that you will be able to put yourself at the top without being able to do what you want to do in order to do it."}, {"heading": "6 Conclusions", "text": "In this article we present a new architecture called Dynamic Frame skip Deep Q-Network (DFDQN), which introduces temporal abstractions through advanced actions without losing the ability to make faster reflexes when needed. We show empirically that DFDQN leads to a significant improvement in performance compared to DQN, with results in two Atari 2600 domain games via DQN: Seaquest and Space Invaders. The results section illustrates the importance of advanced actions, with the temporally extended versions of actions selected 69% of the time in Seaquest and 78% of the time in Space Invaders."}, {"heading": "7 Discussion", "text": "A generic framework would enable the agent to play with a wide range of frame-skip rates selected according to the current state of the game. We outline such a generic framework with a structured policy in which the agent decides not only the action options, but also how long the action should last. To do this, we need an actor composed of continuously evaluated parameters such as Power, Direction in addition to the action probabilities of Kick, Turn, Tackle, Dash. Parameters are limited to a tightly parameterized policy in the Half Field Offense (Robo Soccer) domain. Their policy consists of continuously evaluated parameters such as Power, Direction in addition to the action probabilities of Kick, Turn, Dash. We can adapt this setup for learning structures in the Atari 2600 domain."}], "references": [{"title": "9th International Conference on Autonomous Agents and Multi-Agent Systems", "author": ["Christopher Amato", "Guy Shani. High-level reinforcement learning in strategy games"], "venue": "1:75\u201382,", "citeRegEx": "Amato and Shani. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Bastien et al", "2012] Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research, pages 253\u2013 279, June", "citeRegEx": "Bellemare et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra et al", "2010] James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Frame skip is a powerful parameter for learning to play atari", "author": ["Alex Braylan", "Mark Hollenbeck", "Elliot Meyerson", "Risto Miikkulainen"], "venue": "AAAI workshop,", "citeRegEx": "Braylan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Arxiv", "author": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks"], "venue": "February", "citeRegEx": "Foerster et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning in parametrized action space", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "ICLR,", "citeRegEx": "Hausknecht and Stone. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Technical Report", "author": ["Long-Ji Lin. Reinforcement learning for robots using neural networks"], "venue": "DTIC Document,", "citeRegEx": "Lin. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Human-level control through deep reinforcement learning", "author": ["Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Kumaran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2015}, {"title": "Arxiv", "author": ["Volodymyr Mnih", "Adria Puigdom enech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Timothy P. Lillicrap", "David Silver", "Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning"], "venue": "February", "citeRegEx": "Mnih et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay"], "venue": "EMNLP,", "citeRegEx": "Narasimhan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Elsevier", "author": ["Juan Ortega", "Noor Shaker", "Julian Togelius", "Georgios N. Yannakakis. Imitating human playing styles in Super Mario Bros. Entertainment Computing"], "venue": "4:93\u2013104,", "citeRegEx": "Ortega et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "ICLR,", "citeRegEx": "Schaul et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "IJCAI", "author": ["David Silver", "Richard Sutton", "Martin Muller. Reinforcement learning of local shape in the game of go"], "venue": "7:1053\u20131058,", "citeRegEx": "Silver et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Introduction to reinforcement learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "MIT Press,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Department of Computer Science", "author": ["Mostafa Vafadost. Temporal abstraction in monte carlo tree search. Masters thesis"], "venue": "University of Alberta,", "citeRegEx": "Vafadost. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "venue": "Arxiv,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Technical note: Q-learning", "author": ["Christopher J.C.H. Watkins", "Peter Dayan"], "venue": "Mach. Learn., 8(3-4):279\u2013292, May", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "One of the important parameters in the Arcade Learning Environment (ALE, [Bellemare et al., 2013]) is the frame skip rate.", "startOffset": 73, "endOffset": 97}, {"referenceID": 16, "context": ", 2015]) and Dueling Network Architectures (DuDQN, [Wang et al., 2016]) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state.", "startOffset": 51, "endOffset": 70}, {"referenceID": 2, "context": "served as a test bed to measure performance of learning algorithms in Artificial Intelligence ([Bellemare et al., 2013]).", "startOffset": 95, "endOffset": 119}, {"referenceID": 4, "context": "[Braylan et al., 2015] is one work that focuses on the power of the frame skip rate parameter with experiments in the Atari 2600 domain.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "The idea of dynamic length temporal abstractions in the policy space on Atari Domain has been explored by [Vafadost, 2013].", "startOffset": 106, "endOffset": 122}, {"referenceID": 15, "context": "The way in which our approach differs from [Vafadost, 2013] is in terms of using DQN to build neural network Q-value approximators instead of making use of search techniques that cannot generalize.", "startOffset": 43, "endOffset": 59}, {"referenceID": 11, "context": "Similar to us, [Ortega et al., 2013] attempt to design high", "startOffset": 15, "endOffset": 36}, {"referenceID": 13, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 0, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 10, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 17, "context": "Q-learning ([Watkins and Dayan, 1992]) is an off-policy Temporal Difference (TD) learning ([Sutton and Barto, 1998]) algorithm that can be used to learn an optimal Q(s, a) for the agent.", "startOffset": 12, "endOffset": 37}, {"referenceID": 14, "context": "Q-learning ([Watkins and Dayan, 1992]) is an off-policy Temporal Difference (TD) learning ([Sutton and Barto, 1998]) algorithm that can be used to learn an optimal Q(s, a) for the agent.", "startOffset": 91, "endOffset": 115}, {"referenceID": 14, "context": "The iterative updates are derived from the Bellman optimality equation ([Sutton and Barto, 1998]):", "startOffset": 72, "endOffset": 96}, {"referenceID": 14, "context": "([Sutton and Barto, 1998])", "startOffset": 1, "endOffset": 25}, {"referenceID": 14, "context": "One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; \u03b8) which can generalize over states and actions using higher level features ([Sutton and Barto, 1998]).", "startOffset": 170, "endOffset": 194}, {"referenceID": 7, "context": "To avoid correlated updates from learning on the same transitions that the current network simulates, an experience replay ([Lin, 1993]) D (of fixed maximum capacity) is used,", "startOffset": 124, "endOffset": 135}, {"referenceID": 12, "context": "However, [Schaul et al., 2016] and [Wang et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": ", 2016] and [Wang et al., 2016] show that having a prioritized sampling can lead to substantial improvement in performance.", "startOffset": 12, "endOffset": 31}, {"referenceID": 16, "context": "We also report the scores obtained from original DQN architecture with 512 pre-final hidden layer neurons from the most recent usage of DQN in [Wang et al., 2016], where DQN is reported as baseline for their DuDQN model (the current state-of-the-art model for Atari 2600 domain).", "startOffset": 143, "endOffset": 162}, {"referenceID": 6, "context": "For this, we would need an Actor Critic setup similar to [Hausknecht and Stone, 2016].", "startOffset": 57, "endOffset": 85}, {"referenceID": 6, "context": "[Hausknecht and Stone, 2016] use Deep Actor Critic for learning parametrized policies in the Half Field Offense (Robo Soccer) domain.", "startOffset": 0, "endOffset": 28}, {"referenceID": 9, "context": "All three of them share the same lower level convolutional filters up to a desired depth similar to the architecture used in [Mnih et al., 2016] which as far as we know, is the only successful attempt at using Actor Critic algorithms in the Atari 2600 domain.", "startOffset": 125, "endOffset": 144}, {"referenceID": 9, "context": "The error function to be optimized and the manner in which gradients are to be backpropagated can closely follow the Actor-Critic implementation given in [Mnih et al., 2016].", "startOffset": 154, "endOffset": 173}, {"referenceID": 5, "context": "An alternative way of learning to perform repeated actions could be to use the previous action as input to the policy similar to [Foerster et al., 2016].", "startOffset": 129, "endOffset": 152}, {"referenceID": 12, "context": "As future work, we would also like to explore the DF paradigm on more games in the Atari 2600 domain with the more recent state of the art methods such as Prioritized Replay [Schaul et al., 2016], Dueling DQN [Wang et al.", "startOffset": 174, "endOffset": 195}, {"referenceID": 16, "context": ", 2016], Dueling DQN [Wang et al., 2016] and AsynchronousDQN [Mnih et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 9, "context": ", 2016] and AsynchronousDQN [Mnih et al., 2016].", "startOffset": 28, "endOffset": 47}], "year": 2017, "abstractText": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE, [Bellemare et al., 2013]) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of k allows the agent to repeat a selected action k number of times. The current state of the art architectures like Deep Q-Network (DQN,[Mnih et al., 2015]) and Dueling Network Architectures (DuDQN, [Wang et al., 2016]) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep QNetwork (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.", "creator": "LaTeX with hyperref package"}}}