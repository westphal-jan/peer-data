{"id": "1605.05414", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "abstract": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "histories": [["v1", "Wed, 18 May 2016 01:36:29 GMT  (910kb,D)", "https://arxiv.org/abs/1605.05414v1", "5 pages, submitted to SIGDIAL"], ["v2", "Sat, 23 Jul 2016 00:00:36 GMT  (912kb,D)", "http://arxiv.org/abs/1605.05414v2", "Accepted to SIGDIAL 2016 (short paper). 5 pages"]], "COMMENTS": "5 pages, submitted to SIGDIAL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ryan lowe", "iulian v serban", "mike noseworthy", "laurent charlin", "joelle pineau"], "accepted": false, "id": "1605.05414"}, "pdf": {"name": "1605.05414.pdf", "metadata": {"source": "CRF", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "authors": ["Ryan Lowe", "Iulian V. Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau"], "emails": ["jpineau}@cs.mcgill.ca,", "michael.noseworthy@mail.mcgill.ca", "iulian.vlad.serban@umontreal.ca", "laurent.charlin@hec.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Related Work", "text": "Evaluation methods for monitored systems have been well studied, including the PARADISE framework (Walker et al., 1997) and MeMo (Mo \ufffd ller et al., 2006), which provide a metric for task completion, and a more comprehensive overview of these metrics can be found in (Jokinen et al., 2009). In this paper, we focus on uncontrolled dialog systems, for which proper evaluation is an open problem. Recent evaluation metrics for uncontrolled dialog systems include BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), which compare the similarity between the response generated by the model and the actual response of the participant, depending on a context of the conversation. Word perplexity, which fulfils a function of probability that examples from the training corpus are regenerated, is also used. However, in 2016 such metrics correlate very poorly with the human judgments, which are also proven to be weak."}, {"heading": "3 Technical Background on NUC", "text": "Our long-term goal is to develop and deploy artificial interlocutors. Re-cent deep neural architectures may offer the most promising framework for addressing this problem. However, the training of such architectures usually requires large amounts of call data from the target domain, and a way to automatically assess predictive errors. Next-UtteranceClassification (NUC, see Figure 1) is a task that is easy to evaluate, designed for the training and validation of dialog systems. It is evaluated using the metric Recall @ k that we define in this section. In NUC, a model or user, when presented with the context of a conversation and a (usually small) predefined list of responses, must be selected the most appropriate answer from this list. This list includes the actual next response of the call that is the desired prediction of the model. The other entries that appear to be false are labeled as false."}, {"heading": "4 Survey Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Corpora", "text": "Unlike the larger OpenSubtitles1 dataset, the SubTle Corpus contains regular information indicating when it is each user's turn. Twitter Corpus (Ritter et al., 2010) contains a large number of conversations between users on the microblogging platform Twitter. Finally, the Ubuntu Dialogue Corpus contains conversations extracted from IRC chat logs (Lowe et al., 2015a). 2 For more information on these datasets, we refer the reader to a recent survey on dialogue corpora (Serban et al., 2015). We focus our attention on them as they cover a number of popular domains (Ubuntus et al., 2015a) and are among the largest dialog datasets available, making them good candidates for building data-driven dialog systems (Serban et al., 2015)."}, {"heading": "4.2 Task description", "text": "Each participant was asked to answer either 30 or 40 questions (mean = 31.9). To ensure a sufficient variety of questions from each data set, the participants were given four versions of the survey with different questions; for the AMT participants, the questions were roughly evenly distributed across the three data sets, while for the laboratory experts, half of the questions were related to Ubuntu and the rest were evenly distributed across Twitter and movies; each question had 1 correct answer and 4 wrong answers randomly drawn from other parts of the (same) corpus; a sample question can be seen in Figure 1; the participants had a time limit of 40 minutes; the conversations were extracted to form NUC conversation pairs as described in Figure 3. The number of statements in the context were sampled in accordance with the procedure in (Lowe et al., 2015a)."}, {"heading": "4.3 ANN model", "text": "To compare human results with a strong model of artificial neural networks (ANN), we use the dual encoder model from Lowe et al. (2015a). This model uses recursive neural networks (RNNs) with long-term short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) to encode the context c of the conversation, and a candidate response r. Specifically, at each step a word of text is entered into the LSTM, and its hidden state is updated according to: ht = f (Whht \u2212 1 + Bxxt), whereas W are weight matrices, and f (\u00b7) is a nonlinear activation function. After all the T-words have been processed, the final hidden state hT can be considered as a vector representation of the input sequence. To determine the probability that an answer for the actual next response to a context c is, the model calculates a weighted product of the vector representation between the inputs."}, {"heading": "5 Results", "text": "As we can see from Table 1, the AMT participants are mostly young adults who speak fluent English, with some student degrees. The gender breakdown is roughly the same, and the majority of respondents had never used Ubuntu before. Table 2 shows the NUC results on each corpus. Human results are divided into AMT non-experts, consisting of paid respondents who are \"beginners\" or have no knowledge of Ubuntu terminology; AMT experts who claimed to have \"intermediate\" or \"advanced\" knowledge of Ubuntu; and Lab experts, who are the unpaid respondents with Ubuntu experience and university computer science education. We also present results on the same task for a state-of-the-of-of-art dialogue model of artificial neural networks (ANN) (see Lowe et al., 2015a) for implementation details. We first observe that the subjects are above the random level (20% R for all people), so the task is feasible for all areas."}, {"heading": "6 Discussion", "text": "Our findings show that people outperform current dialogue models in the task of NextUtterance classification, suggesting that there is much room for improvement in these models to better understand the nature of human dialogue. While our findings suggest that NUC is a useful task, it is by no means sufficient; we strongly support the automatic evaluation of dialogue systems with as many relevant metrics as possible. Further research should be conducted to find metrics or tasks that accurately reflect human judgment in the evaluation of dialogue systems."}], "references": [{"title": "Movie-dic: A movie dialogue corpus for research and development", "author": ["R.E. Banchs."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2.", "citeRegEx": "Banchs.,? 2012", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie."], "venue": "ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Interval estimation for a binomial proportion", "author": ["L.D. Brown", "T.T. Cai", "A. DasGupta."], "venue": "Statistical science, pages 101\u2013117.", "citeRegEx": "Brown et al\\.,? 2001", "shortCiteRegEx": "Brown et al\\.", "year": 2001}, {"title": "Evaluating prerequisit qualities for learning end-to-end dialog systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Dodge et al\\.,? 2016", "shortCiteRegEx": "Dodge et al\\.", "year": 2016}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["M. Galley", "C. Brockett", "A. Sordoni", "Y. Ji", "M. Auli", "C. Quirk", "M. Mitchell", "J. Gao", "B. Dolan."], "venue": "Proceedings of the Annual Meeting of the Association", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Spoken Dialogue Systems", "author": ["K. Jokinen", "M. McTear."], "venue": "Morgan Claypool.", "citeRegEx": "Jokinen and McTear.,? 2009", "shortCiteRegEx": "Jokinen and McTear.", "year": 2009}, {"title": "Improved deep learning baselines for ubuntu corpus dialogs", "author": ["R. Kadlec", "M. Schmid", "J. Kleindienst."], "venue": "NIPS on Machine Learning for Spoken Language Understanding.", "citeRegEx": "Kadlec et al\\.,? 2015", "shortCiteRegEx": "Kadlec et al\\.", "year": 2015}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau."], "venue": "arXiv preprint arXiv:1603.08023.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau."], "venue": "Proceedings of SIGDIAL.", "citeRegEx": "Lowe et al\\.,? 2015a", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Incorporating unstructured textual knowledge sources into neural dialogue systems", "author": ["R. Lowe", "N. Pow", "I.V. Serban", "L. Charlin", "J. Pineau."], "venue": "NIPS Workshop on Machine Learning for Spoken Language Understanding.", "citeRegEx": "Lowe et al\\.,? 2015b", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error simulations", "author": ["S. M\u00f6ller", "R. Englert", "K.-P. Engelbrecht", "V.V. Hafner", "A. Jameson", "A. Oulasvirta", "A. Raake", "N. Reithinger."], "venue": "INTERSPEECH.", "citeRegEx": "M\u00f6ller et al\\.,? 2006", "shortCiteRegEx": "M\u00f6ller et al\\.", "year": 2006}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu."], "venue": "Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL).", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["O. Pietquin", "H. Hastie."], "venue": "The Knowledge Engineering Review.", "citeRegEx": "Pietquin and Hastie.,? 2013", "shortCiteRegEx": "Pietquin and Hastie.", "year": 2013}, {"title": "Unsupervised modeling of twitter conversations", "author": ["A. Ritter", "C. Cherry", "B. Dolan."], "venue": "North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["J. Schatzmann", "K. Georgila", "S. Young."], "venue": "Proceedings of SIGDIAL.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["I.V. Serban", "R. Lowe", "L. Charlin", "J. Pineau."], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), 2016, pages", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan."], "venue": "Conference of the North American Chapter of the Association for", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le."], "venue": "ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["M.A. Walker", "D.J. Litman", "C.A. Kamm", "A. Abella."], "venue": "Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 271\u2013", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (Galley et al., 2015; Schatzmann et al., 2005), without requiring human labels or", "startOffset": 116, "endOffset": 162}, {"referenceID": 15, "context": "to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (Galley et al., 2015; Schatzmann et al., 2005), without requiring human labels or", "startOffset": 116, "endOffset": 162}, {"referenceID": 13, "context": "neering, to general conversational agents (Pietquin and Hastie, 2013).", "startOffset": 42, "endOffset": 69}, {"referenceID": 20, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 18, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 17, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 8, "context": "been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (Liu et al., 2016).", "startOffset": 158, "endOffset": 176}, {"referenceID": 9, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 7, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 3, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 11, "context": "First introduced for evaluating user simulations by Schatzmann et al. (2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 19, "context": "difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems (Sordoni et al., 2015; Serban et al., 2016), and 5) models trained with NUC can be", "startOffset": 211, "endOffset": 254}, {"referenceID": 17, "context": "difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems (Sordoni et al., 2015; Serban et al., 2016), and 5) models trained with NUC can be", "startOffset": 211, "endOffset": 254}, {"referenceID": 8, "context": "converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016).", "startOffset": 65, "endOffset": 83}, {"referenceID": 22, "context": "for language understanding (Weston et al., 2016), and as a useful framework for building chatbots.", "startOffset": 27, "endOffset": 48}, {"referenceID": 9, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 0, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 14, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 8, "context": "cant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016).", "startOffset": 104, "endOffset": 122}, {"referenceID": 0, "context": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012), the Twitter Corpus (Ritter et al.", "startOffset": 92, "endOffset": 106}, {"referenceID": 14, "context": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012), the Twitter Corpus (Ritter et al., 2010), and the Ubuntu Dialogue Corpus (Lowe et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 9, "context": ", 2010), and the Ubuntu Dialogue Corpus (Lowe et al., 2015a).", "startOffset": 40, "endOffset": 60}, {"referenceID": 0, "context": "We find that there is indeed a significant separation between machine and expert huFigure 1: An example NUC question from the SubTle Corpus (Banchs, 2012).", "startOffset": 140, "endOffset": 154}, {"referenceID": 21, "context": "They include the PARADISE framework (Walker et al., 1997), and MeMo (M\u00f6ller et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 11, "context": ", 1997), and MeMo (M\u00f6ller et al., 2006), which include a measure of task completion.", "startOffset": 18, "endOffset": 39}, {"referenceID": 6, "context": "of these metrics can be found in (Jokinen and McTear, 2009).", "startOffset": 33, "endOffset": 59}, {"referenceID": 1, "context": "2002) and METEOR (Banerjee and Lavie, 2005), which compare the similarity between response generated by the model, and the actual response of the participant, conditioned on some context of the conversation.", "startOffset": 17, "endOffset": 43}, {"referenceID": 8, "context": "However, such metrics have been shown to correlate very weakly with human judgement of the produced responses (Liu et al., 2016).", "startOffset": 110, "endOffset": 128}, {"referenceID": 8, "context": "fer from several other drawbacks (Liu et al., 2016), including low scores, lack of interpretability, and inability to account for the vast space of acceptable outputs in natural conversation.", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": "This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 7, "context": "This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 0, "context": "The SubTle Corpus (Banchs, 2012) consists of movie dialogues as extracted from subti-", "startOffset": 18, "endOffset": 32}, {"referenceID": 14, "context": "The Twitter Corpus (Ritter et al., 2010) contains a large number of conversations between users on the microblogging platform Twitter.", "startOffset": 19, "endOffset": 40}, {"referenceID": 9, "context": "Finally, the Ubuntu Dialogue Corpus contains conversations extracted from IRC chat logs (Lowe et al., 2015a).", "startOffset": 88, "endOffset": 108}, {"referenceID": 16, "context": "2 For more information on these datasets, we refer the reader to a recent survey on dialogue corpora (Serban et al., 2015).", "startOffset": 101, "endOffset": 122}, {"referenceID": 9, "context": "in Lowe et al. (2015b).", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "The number of utterances in the context were sampled according to the procedure in (Lowe et al., 2015a), with a maximum context length of 6 turns \u2014 this was done for both the human trials and ANN model.", "startOffset": 83, "endOffset": 103}, {"referenceID": 9, "context": "In order to compare human results with a strong artificial neural network (ANN) model, we use the dual encoder (DE) model from Lowe et al. (2015a). This model uses recurrent neu-", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "ral networks (RNNs) with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) to encode the context c of the conversation, and a candidate response r.", "startOffset": 61, "endOffset": 95}, {"referenceID": 9, "context": "plementation details, see Lowe et al. (2015a).", "startOffset": 26, "endOffset": 46}, {"referenceID": 9, "context": "7% (Lowe et al., 2015a)", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "Starred (*) results indicate a poor approximation of the confidence interval due to high scores with small sample size, according to the rule of thumb by Brown et al. (2001).", "startOffset": 154, "endOffset": 174}], "year": 2016, "abstractText": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed NextUtterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "creator": "TeX"}}}