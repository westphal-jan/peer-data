{"id": "1605.05628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "Detecting Novel Processes with CANDIES -- An Holistic Novelty Detection Technique based on Probabilistic Models", "abstract": "In this article, we propose CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), a new approach to novelty detection in technical systems. We assume that in a technical system several processes interact. If we observe these processes with sensors, we are able to model the observations (samples) with a probabilistic model, where, in an ideal case, the components of the parametric mixture density model we use, correspond to the processes in the real world. Eventually, at run-time, novel processes emerge in the technical systems such as in the case of an unpredictable failure. As a consequence, new kinds of samples are observed that require an adaptation of the model. CANDIES relies on mixtures of Gaussians which can be used for classification purposes, too. New processes may emerge in regions of the models' input spaces where few samples were observed before (low-density regions) or in regions where already many samples were available (high-density regions). The latter case is more difficult, but most existing solutions focus on the former. Novelty detection in low- and high-density regions requires different detection strategies. With CANDIES, we introduce a new technique to detect novel processes in high-density regions by means of a fast online goodness-of-fit test. For detection in low-density regions we combine this approach with a 2SND (Two-Stage-Novelty-Detector) which we presented in preliminary work. The properties of CANDIES are evaluated using artificial data and benchmark data from the field of intrusion detection in computer networks, where the task is to detect new kinds of attacks.", "histories": [["v1", "Wed, 18 May 2016 15:47:59 GMT  (1331kb,D)", "http://arxiv.org/abs/1605.05628v1", "17 Pages, contains 21 Figures. Currently under review for publication in International Journal of Machine Learning and Cybernetics (Springer)"]], "COMMENTS": "17 Pages, contains 21 Figures. Currently under review for publication in International Journal of Machine Learning and Cybernetics (Springer)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["christian gruhl", "bernhard sick"], "accepted": false, "id": "1605.05628"}, "pdf": {"name": "1605.05628.pdf", "metadata": {"source": "CRF", "title": "Detecting Novel Processes with CANDIES \u2013 An Holistic Novelty Detection Technique based on Probabilistic Models", "authors": ["Christian Gruhl", "Bernhard Sick"], "emails": ["cgruhl@uni-kassel.de).", "bsick@uni-kassel.de)."], "sections": [{"heading": null, "text": "Index Terms - ovelty detection Gaussian mixture models KANDIES Online Goodness-of-Fitovelty Detection Gaussian mixture models KANDIES Online Goodness-of-FitN"}, {"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 OVERVIEW OF CANDIES", "text": "With CANDIES, we have three main objectives: 1) to detect clusters of suspicious samples (i.e. those that differ significantly from what is expected); 2) to detect such clusters throughout the input area, i.e. in LDR and in HDR. 3) to use the detected clusters to model new processes.The algorithms consist of several detectors for HDR and a single one for LDR. It works (simplified) in the following way. The basis of the whole approach is a GMM, which provides a density estimate of the expected data. An advantage of GMM is that it can easily be extended to include a classifier and that it belongs to the family of generative models, i.e. additional structural information can be derived about the expected observations (as opposed to discriminatory classifiers such as SVM)."}, {"heading": "3 RELATED WORK", "text": "This year is the highest in the history of the country."}, {"heading": "4 METHODICAL FOUNDATIONS", "text": "In a self-contained article, we briefly present the most important techniques for implementing our approach: an overview of Gaussian mixtures, a method to expand them into a classifier, a brief introduction to non-parametric density estimation as the basis for cluster analysis, and a brief description of statistical quality testing."}, {"heading": "4.1 Gaussian Mixtures", "text": "A commonly used generative modeling approach is the Gaussian mixing model (GMM), that is, an overlay of multiple multivariate normal distributions (referred to as N and commonly referred to as Gaussian) and a mixing coefficient \u03c0j (equivalent) (1)). Each Gaussian is referred to as a component and has its own set of parameters, which are the mean vector \u00b5j RD and a covariance matrix j (with dimensions (B) = D \u00d7 D. The mixing coefficients \u03c0j (with limitations; J = 1 \u03c0j = 1, \u03c0j R +) ensure that the resulting p (x) (x-RD is the random variable) still meets the requirements for a density function. They can also be considered as priors for each component (i.e, the probability that an unobserved sample is generated by the corresponding component)."}, {"heading": "4.2 Classification Paradigm", "text": "To derive a classifier h (x) from the learned density model p (x), we estimate the class posteriors p (c | x) in a second, monitored (i.e. in relation to class names) iteration. The classification of a given sample x is then performed as shown in Equation (2) by selecting the maximum a-posteriori (MAP) of the class probabilities: h (x) = argmax c {p (c | x)}, (2) withp (c | x) = J \u2211 j = 1 p (c | j) \u00b7 p (j | x) = J \u2211 j = 1 \u0441j, c \u00b7 \u03b3x, j, (3) where\u03b3x, j = \u03c0jN (x | \u00b5j)."}, {"heading": "4.3 Density Based Clustering", "text": "Instead of assuming a specific function form such as parametric methods, non-parametric techniques yield a point estimate for the density p (x) at a particular point x. A well-known non-parametric method is the Parzen window (or kernel) density estimator, here with Gaussian core: p (x) = 1 N \u00b7 hD N \u00b2 n = 1 k (x \u2212 xn h). (6) It is the sum of a finite number of N samples xn of an underlying training set, to which a corresponding core function is applied. The kernel is placed at the point at which the density is to be estimated. The parameter h is a smoothing factor that controls how smooth the estimate is, while D is the number of dimensions. Closely related to the Parces window are histograms (cf. [5]), where the DBSCAN cluster algorithms (cf. [11]) represent a density estimate quite similar to a window."}, {"heading": "4.4 Statistical Goodness-of-Fit Tests", "text": "s chi-square test [28], which compares observed frequencies from mutually exclusive events (finite series of possible results / values of a discrete random variable) with expected theoretical frequencies (determined from a suitable adapted distribution) of these events. Test statistics (or t-value) are calculated by: t = k; i (xi \u2212 ei) 2 ei (8), where xi is the observed event frequency of the event i and k is the total number of different events. Expected frequencies of events i are indicated by ei: ei = Pfit (i \u2212 value) (9), where Pfit is the adjusted distribution. The test aggregates the square deviations between observed and expected frequencies and weights them by the expected frequency."}, {"heading": "5 ONLINE NOVELTY DETECTION", "text": "This section is divided into three parts: the first part is based on our previous work [15] and deals with the detection and reaction of novelties in LDR with 2SND; the second part deals with the detection of novelties in HDR with online-enabled \u03c72 Goodness-of-Fit tests; and the last part introduces CANDIES, a detector that is able to detect novelties throughout the entrance area by combining both of the aforementioned techniques, all of which share the characteristic of being applicable to online environments (i.e. soft real-time)."}, {"heading": "5.1 Novelty Detection in Low-Density Regions", "text": "The algorithm works on top of an existing GMM or CMM (as described in Section 4.2) and expands it with new detection capacities; furthermore, with 2SND it is possible to update and improve the underlying samples by including components that include the model of detected novel processes; the algorithm itself consists of two procedures: a main procedure 2SND (Alg. 1) and an auxiliary ProcedurePROPAGATE that extends the clusterid to all interconnected samples. To detect new processes, we propose a two-step approach that identifies suspicious samples in the first stage and novel processes in the second stage; each sample examined is individually tested to see how well it matches the current model by determining whether it exhibits resistance in a high (HDR) or a low density."}, {"heading": "5.1.1 Model Adaptation", "text": "The last part of the 2SND process is responsible for deciding whether a novel process exists in the monitored LDR and how to adapt the model. If a new process in the form of a cluster is identified, the underlying GMM must be updated by conducting a VI training on all samples associated with the respective cluster. After this training step, the algorithm 1 2SNDInput: Probe x, Parameter \u03b1, minPts, b \"Global: Model M, Buffer B Initialize a novel behavior associated with the respective expert cluster. {1. Stage - Detection of Suspicious Samples} for all components j in the M-Doif model 2j (x\") is then possible {The observation is not suspicious} back classification of x \"based on M.end when it ends, when | = b\" thenRemove oldest sample from buffer B. Add end of model x to buffer B."}, {"heading": "5.2 Novelty Detection in High-Density Regions", "text": "At the moment, we concentrate on the detection of overlapping processes for individual components and extend the idea later to GMM (where several components are present).The difficulty in detecting new processes in dense regions is that we cannot decide whether an observed sample is the legitimate result of a known (i.e. the existing component) or an unknown overlapping process without knowing its affiliation (which in this case is a latent variable).We therefore use a sliding window tracking the traces of the last samples observed, regardless of whether they are actually new or not. It is clear that in the presence of a new process (which deviates from the existing component at least on average or in the codiction), the sample population observed (i.e. the content of the sliding window will not correspond to the distribution (described by the component) and a discrepancy between this mean or codience of the existing component and the deviation of the deviation of the deviation component from the deviation of the deviation or the amount to be expected from the deviation of the deviation of the component from the component of the deviation from the deviation of the deviation from the deviation from the component of the deviation from the component of the component."}, {"heading": "5.2.1 Transformation of the distance distribution", "text": "One of the requirements of the \u03c72 goodness-of-fit test is that the different events must be mutually exclusive. Therefore, the continuous distance density must be transformed into a discrete one. As each distribution-5 -4 -3 -2 -1 0 1 3-5 -4 -2 -1 0 1 3 4 5 (a) Initial investigation with samples from two different classes, green circle and blue cross +. The density model is trained with VI and extended to a classifier, as in Section 4.2.-5 -4 -2 -1 0 1 1 2 4-5 -3 -2 0 1 0 2 2 2 0 2 4 5 (b). The resulting GMM with two components after VI training. The black line is the combination of the decision limits and the \u03b1 regions. Samples appearing in the outer region (cyan colored) LDR are identified as suspicious. 9 is normalized (i.e., p (x) dx = 1) and are always (always)."}, {"heading": "5.2.2 Learning of the distance distribution", "text": "Real data sets or sensory data from embedded systems often differ from their assumed distributions, but this is not a problem for the classification of our quality-of-fit approach to novelty detection, since the t-value curve in Figure 9 has highlights where a single gauss is adjusted based on the samples shown, which are uniformly distributed and not normal. Therefore, the distances are not distributed as assumed in our test, and the critical value is almost permanently exceeded by the test statistics.The problem can be solved by estimating the cell boundaries directly from the Xtrain samples used to train the component: celli = [li, ri) 1 < i < i = 1 [li, ri) i = 1 [li) li =, (21) li = (xdi \u00b7 w\u043c e), xj \u00b2 Sort (Xtrain), (22) ri = li + 1 (23), where the cell point is an equal cell."}, {"heading": "5.2.3 Extension to Gaussian Mixture Models", "text": "In order to extend the high density approach to multi-component GMM, each component requires its own detector (> j \u2032 j = 24 x). If a new sample x \u2032 is observed, it should be used to update the detector of its connected component. However, the affiliation is a latent variable and therefore not known at runtime. A method of estimating the affiliations is (Monte Carlo) random samples, which require only the evaluation of the unstandardized (without mixing coefficients \u03c0j) densities Pj (x \u2032) = N (x \u2032), electronic samples for each component j and a continuous uniform pseudo-random number generator, without the evaluation of the unstandardized value [0, 1]. Sampling works by dividing the unit interval into J parts. Each partition mj is connected to exactly one component j and the limits are given by: mj = 3000 x."}, {"heading": "5.3 CANDIES", "text": "CANDIES is our holistic approach to detecting novelties in regions with high or low density GMM. This is achieved by using a single 2SND detector (see Section 5.1) in combination with several HDR detectors for each component (see Section 5.2.3)."}, {"heading": "5.3.1 Requirements to merge LDR and HDR detectors", "text": "If this is not the case, the sample is passed to the second stage of the 2SND and the density-based samples are reprocessed, at which point a new process could be discovered. Otherwise, the random sample described in Section 5.2.3 is executed and x \u00b2 is linked to exactly one of the novel components J. Then12Algorithm 2 CANDIESInput: Sample x \u00b2, Parameter \u03b1, minPts, b \u00b2 Global: Model M, Buffer B Initialize \u03c1 = F \u2212 1\u03c72D (Good-). {decide whether x \u00b2 is in a region of high or low density} for all components j do in M if it is 2j (x \u00b2). Observation is done in dense region."}, {"heading": "5.3.2 Novelty Measure \u2013 Human Readability", "text": "We propose two new metrics to quantify how much novelty is present in different regions (i.e. HDR or LDR) in a way that is understandable to (data scientists). Therefore, the metrics should express the absence of a new process (or novelty) with a value close to 0, while the presence of such a process should be expressed by a value close to 1. The metric close to 2snd for LDR is: \u03bd2snd, n = 1 \u2212 | C | + | | | (31) 13Where | B | is the number of observations currently stored in the buffer, | C | is the number of different clusters and | Noise | the number of samples associated with the noise cluster. If a single cluster containing the most samples currently held in the buffer is present (which is a strong indicator of a new process), the metric will be close to 1."}, {"heading": "5.3.3 Overview of Parameters", "text": "CANDIES contains a considerable number of adjustable parameters. Table 1 gives an overview of all the parameters contained in CANDIES, including a brief description, recommendations for (good) default values (if possible) and which detector is affected by the parameter. Note that in particular, the parameters of the buffer size depend on how many new processes are expected at once and how many samples they will generate."}, {"heading": "5.3.4 Handling of Noise", "text": "Because novelty detection is designed to detect novel processes rather than individual observations, it is relatively robust against distributed noise in the entrance area. While novel samples of a novel process appear in dense form, random noise is scattered across the entrance area, so it is rather unlikely that sufficiently large clusters will form. In the LDR detection part (based on 2SND), robustness is achieved by the two-stage architecture that suspicious samples pass through. Figure 20 shows a scenario that includes uniformly distributed noise mixed into a test set with observations of a known process and a novel process (right, outside the \u03b1 zone in the low density range). Depending on its parameterization, the LDR approach detects only a novel process in which the novel observations are actually located. Figure 21 (a) shows the same exemplary dataset already used in Section 5.2.1, interspersed with uniform crosses (corresponding to the corresponding roughness) in this image."}, {"heading": "5.3.5 An alternative view on CANDIES", "text": "At first glance, the two different approaches to novelty detection may seem quite different for LDR and HDR. However, it is possible to get a uniform view by interpreting one detector according to the other. As already mentioned, the last cells of each HDR detector correspond to the low density parts of the input space. Therefore, the ring buffer B used for 2SND can be considered as the common cell of all HDR detectors. On the other hand, the individual buffers of each HDR detector allow an interpretation as a cluster (with a different adaptation predicate P) and are therefore suitable for the second stage of 2SND."}, {"heading": "6 CASE STUDY", "text": "In order to confirm that the presented approach is applicable to real applications, we present experimental results based on the known data set for intrusion into the KDD Cup 1999 network. [21] Although it is pointed out that the data set has some serious deficiencies that make it unsuitable for the evaluation of real intrusion detection systems, its properties are nevertheless suitable for our purposes as we are not interested in building a modern intrusion detection system."}, {"heading": "6.1 Setup", "text": "As already mentioned, our new approach is compared to a new detection technology proposed in [12], whereby novel samples are also identified using a GMM and the squared Mahalanobis distance between the processed samples and the mean value of the various components. Each time a new sample is processed, an internal state variable Sn is updated so that Sn = Sn \u2212 1 + \u03c72nov, with a penalty or reward paid depending on how well the new sample matches the model. To calculate whether the state variable is rewarded or punished, the indicator functions are rewarded, j (x) \u2212 \u03b1 1 \u2212 \u03b1, j (x), with the penalty or reward depending on how well the new sample fits the model, the indicator functions are rewarded or punished."}, {"heading": "6.1.1 Results", "text": "The resulting average classification performance is summarized in Table 2, which states that both adaptive approaches are capable of identifying the attacks and making model adjustments that integrate the knowledge acquired. In all scenarios, the accuracy and observed F1Score of CANDIES are equal or higher than those of the CSND approach. In three out of five scenarios, our approach performs comparably well to the static baseline and is still satisfactory on the other hand. CANDIES \"higher performance compared to CSND is explained by Table 3, which shows the average number of actually novel samples (samples that are actually under attack) that are processed before the novel process is detected and a model adaptation is triggered. Here, CANDIES demonstrates its strength in exploiting spatial information between suspicious samples in LDR in the form of clusters, speeding detection compared to the slowly changing state variable of SND mode, which can be designed in the algorithm."}, {"heading": "7 CONCLUSION AND OUTLOOK", "text": "(...). (...). (...). (...). (...). (...). (...). (...). It is not that we are able to understand the rules that we have imposed on ourselves. (...). It is not that we are able to fulfill them. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (...). (...). (...). (...). (...). (...). (. (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (...). (...). (...). (...). (...). (...). (). (...). (). (...). (). (...). (...). (...). (...). (). (. (). (. (). (). (). (). (. (). (). (). (). (). (). (). (). (). (). (). (). ()."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for its support within the framework of the DFG project CYPHOC (SI 674 / 9-1).17"}], "references": [{"title": "Towards a better understanding of context and context-awareness", "author": ["G. Abowd", "A. Dey", "P. Brown", "N. Davies", "M. Smith", "P. Steggles"], "venue": "H.-W. Gellersen, editor, Handheld and Ubiquitous Computing, volume 1707 of Lecture Notes in Computer Science, pages 304\u2013307. Springer Berlin Heidelberg", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Extreme Learning Machine based Novelty Detection for Incremental Semi- Supervised Learning", "author": ["H. Al-Behadili", "A. Grumpe", "C. Dopp", "C. Wohler"], "venue": "International Conference on Image Information Processing, pages 230\u2013235", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "On a measure of divergence between two multinomial populations", "author": ["A. Bhattacharyya"], "venue": "Sankhy\u0101: The Indian Journal of Statistics, pages 401\u2013406", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1946}, {"title": "Novelty detection and neural network validation", "author": ["C.M. Bishop"], "venue": "Vision, Image and Signal Processing, volume 141, pages 217\u2013222", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Springer-Verlag", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks applied in intrusion detection systems", "author": ["J. Bonifacio", "A. Cansian", "A. Carvalho", "E. Moreira"], "venue": "Proc. of IJCNN, volume 1, pages 205\u2013210", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "LOF: Identifying Density-Based Local Outliers", "author": ["M.M. Breunig", "H.-P. Kriegel", "R.T. Ng", "J. Sander"], "venue": "ACM SIGMOD Record, 29(2):93\u2013104", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Novelty detection with multivariate extreme value statistics", "author": ["D.A. Clifton", "S. Hugueny", "L. Tarassenko"], "venue": "Journal of Signal Processing Systems, 65(3):371\u2013389", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Identification of patient deterioration in vital-sign data using one-class support vector machines", "author": ["L. Clifton", "D.A. Clifton", "P.J. Watkinson", "L. Tarassenko"], "venue": "2011 Federated Conference on Computer Science and Information Systems (FedCSIS), (2):125\u2013131", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic novelty detection with support vector machines", "author": ["L. Clifton", "D.A. Clifton", "Y. Zhang", "P. Watkinson", "L. Tarassenko", "H. Yin"], "venue": "IEEE Transactions on Reliability, 63(2):455\u2013467", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["M. Ester", "H. Kriegel"], "venue": "J., and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. of KDD-96, pages 226\u2013231. AAAI Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Techniques for knowledge acquisition in dynamically changing environments", "author": ["D. Fisch", "M. J\u00e4nicke", "E. Kalkowski", "B. Sick"], "venue": "TAAS, 7(1):1\u201325", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Quantitative emergence \u2013 a refined approach based on divergence measures", "author": ["D. Fisch", "M. J\u00e4nicke", "B. Sick", "C. M\u00fcller-Schloer"], "venue": "SASO, pages 94\u2013103", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge fusion for probabilistic generative classifiers with data mining applications", "author": ["D. Fisch", "E. Kalkowski", "B. Sick"], "venue": "TKDE, 26(3):652\u2013666", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A building block for awareness in technical systems: Online novelty detection and reaction with an application in intrusion detection", "author": ["C. Gruhl", "B. Sick", "A. Wacker", "S. Tomforde", "J. H\u00e4hner"], "venue": "IEEE iCAST, pages 194\u2013200. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Runtime Self-Integration as Key Challenge for Mastering Interwoven Systems", "author": ["J. Haehner", "U. Brinkschulte", "P. Lukowicz", "S. Mostaghim", "B. Sick", "S. Tomforde"], "venue": "pages 1\u20138", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Outlier detection using k-nearest neighbour graph", "author": ["V. Hautam\u00e4ki", "I. K\u00e4rkk\u00e4inen", "P. Fr\u00e4nti"], "venue": "Proc. \u2013 International Conference on Pattern Recognition, 3(09):430\u2013433", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme value statistics for vibration spectra outlier detection", "author": ["A. Hazan", "J. Lacaille", "K. Madani"], "venue": "International Conference on Condition Monitoring and Machinery Failure Prevention Technologies, pages 736\u2013744, London, UK", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Neue Begr\u00fcndung der Theorie quadratischer Formen von unendlich vielen Ver\u00e4nderlichen", "author": ["E. Hellinger"], "venue": "Journal f\u00fcr die reine und angewandte Mathematik, 136:210\u2013271", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1909}, {"title": "Gaussian mixture pdf in one-class classifcation: computing and utilizing confidence values", "author": ["J. Ilonen", "P. Paalanen", "J. Kamarainen", "H. K\u00e4lvi\u00e4inen"], "venue": "ICPR, volume 2, pages 577\u2013580", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "KDD Cup 1999 Data \u2013 Data Set", "author": ["KDD Cup"], "venue": "http://kdd.ics.uci. edu/databases/kddcup99/kddcup99.html, 1999. ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics, 22:79\u201386", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1951}, {"title": "Novelty Detection: a review \u2013 part 1: statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing, 83:2481\u20132497", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Novelty Detection: a review \u2013 part 2: neural network based approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing, 83:2499\u20132521", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Organic Computing \u2013 A Paradigm Shift for Complex Systems", "author": ["C. M\u00fcller-Schloer", "H. Schmeck", "T. Ungerer"], "venue": "Springer-Verlag Berlin Heidelberg", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "LOCI: Fast Outlier Detection Using the Local Correlation Integral", "author": ["S. Papadimitriou", "H. Kitagawa", "P.B. Gibbons", "C. Faloutsos"], "venue": "In Data Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["K. Pearson"], "venue": "Philosophical Magazine Series 5, 50(302):157\u2013175", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1900}, {"title": "A review of novelty detection", "author": ["M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko"], "venue": "Signal Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Unsupervised condition change detection in large diesel engines", "author": ["N.H. Pontoppidan", "J. Larsen"], "venue": "Neural Networks for Signal Processing \u2013 Proc. of the IEEE XIII Workshop, pages 565\u2013574", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Extreme value statistics for novelty detection in biomedical data processing", "author": ["S. Roberts"], "venue": "IEE Proc. \u2013 Science, Measurement and Technology, 147(6):363\u2013367", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Novelty detection using extreme value statistics", "author": ["S.J. Roberts"], "venue": "Vision, Image and Signal Processing, IEEE Proc., 146(3):124\u2013129", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "F", "author": ["E.J. Spinosa"], "venue": "de Carvalho, A. deLeon, and J. Gama. Novelty detection with application to data streams. Intelligent Data Analysis, 13(3):405\u2013422", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Novelty detection for the identification of masses in mammograms", "author": ["L. Tarassenko", "P. Hayton", "N. Cerneaz", "M. Brady"], "venue": "Artificial Neural Networks, 1995., Fourth International Conference on, (10):442\u2013 447", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Uniform Object Generation for Optimizing One-class Classifiers", "author": ["D. Tax", "R. Duin"], "venue": "The Journal of Machine Learning Research, 2:155\u2013173", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Outlier identification and market segmentation using kernel-based clustering techniques", "author": ["C.H. Wang"], "venue": "Expert Systems with Applications, 36(2):3744\u20133750", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Parzen-window network intrusion detectors", "author": ["D. Yeung", "C. Chow"], "venue": "Proc. of ICPR, volume 4, pages 385\u2013388", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Novelty detection for practical pattern recognition in condition monitoring of multivariate processes: A case study", "author": ["F. Zorriassatine", "A. Al-Habaibeh", "R.M. Parkin", "M.R. Jackson", "J. Coy"], "venue": "International Journal of Advanced Manufacturing Technology, 25(9- 10):954\u2013963", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "In a preliminary article (see [15]), we presented 2SNDR, an approach to solve the novelty detection problem sketched above for situations, where novel processes start to \u201cgenerate\u201d data in LDR of a probabilistic knowledge model (based on Gaussian mixtures).", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "[24]) or neural network based (cf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Novelty detection techniques based on nonparametric density modeling are, for example, those using k-nearest neighbors approaches or kernel density estimators, see [37] for a sample application in intrusion detection.", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "In preliminary work [12] we detect novelty based on a parametric Gaussian mixture model and a state variable which monitors how well the observations fit the model.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.", "startOffset": 71, "endOffset": 74}, {"referenceID": 23, "context": ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": "OneClass SVM as described by Tax and Duin [35].", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "Now, a more recent survey [29] suggest five different categories to group novelty detection approaches: i) probabilistic, ii) distance-based, iii) reconstruction-based, iv) domainbased, and v) information theoretical.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).", "startOffset": 48, "endOffset": 52}, {"referenceID": 36, "context": "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "[8], [18], [32]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[8], [18], [32]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "[8], [18], [32]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "[2] to implement incremental semi-supervised learning based on novelty detection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], for example, where Support Vector Machines are used for detection and resulting novelty values calibrated in order to be interpreted as class-conditional probabilities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "[33], [36]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[33], [36]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.", "startOffset": 65, "endOffset": 68}, {"referenceID": 29, "context": "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "This leads to the result, that learning does not only happen in a isolated training phase, off-line at design-time, but it is also conducted at run-time [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "An extensive introduction to VI is given by [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "Relying on VI gives rise to two advantages: (1) prior knowledge about the data can be included, which is especially valuable in real-world applications, and (2) multiple GMM can be fused into one model as described in [14].", "startOffset": 218, "endOffset": 222}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11]) uses a density estimation that is quite similar to a Parzen window estimator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "A fast and reliable method is Pearson\u2019s chi-squared (\u03c7) test [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "This section is split into three parts: the first part is based on our previous work [15] and discusses novelty detection and reaction in LDR with 2SND.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "To update the model, we exploit the properties of the hyperdistributions and use a fusion technique proposed in [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "[3], [19]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[3], [19]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "As investigated by [12], it is also possible to exchange knowledge with other systems so that a novel process can be faster detected by another system.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Due to their high computational complexity divergence measures such as the Kullback-Leibler divergence [22] or Hellinger distance [19] are intractable for the measurements, especially if the input space is of high dimensionality.", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "Due to their high computational complexity divergence measures such as the Kullback-Leibler divergence [22] or Hellinger distance [19] are intractable for the measurements, especially if the input space is of high dimensionality.", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "1000 Uniformly distributed 2D samples in the interval [0, 10] and trained Gaussian component.", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "One method to estimate the affiliations is (Monte Carlo) random sampling, which requires only the evaluation of the unnormalized (without mixing coefficient \u03c0j) densities Pj(x) = N (x|\u03bcj ,\u03a3j) for each component j and a continuous uniform pseudo random number generator unif(0, 1) for the unit interval [0, 1].", "startOffset": 302, "endOffset": 308}, {"referenceID": 20, "context": "based on the well-known KDD Cup 1999 network intrusion data set [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "As mentioned before, our new approach is compared to a novelty detection technique that is proposed in [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "[13]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", [1], [26]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 24, "context": ", [1], [26]).", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "In this article, we propose CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), a new approach to novelty detection in technical systems. We assume that in a technical system several processes interact. If we observe these processes with sensors, we are able to model the observations (samples) with a probabilistic model, where, in an ideal case, the components of the parametric mixture density model we use, correspond to the processes in the real world. Eventually, at run-time, novel processes emerge in the technical systems such as in the case of an unpredictable failure. As a consequence, new kinds of samples are observed that require an adaptation of the model. CANDIES relies on mixtures of Gaussians which can be used for classification purposes, too. New processes may emerge in regions of the models\u2019 input spaces where few samples were observed before (low-density regions) or in regions where already many samples were available (high-density regions). The latter case is more difficult, but most existing solutions focus on the former. Novelty detection in lowand high-density regions requires different detection strategies. With CANDIES, we introduce a new technique to detect novel processes in high-density regions by means of a fast online goodness-of-fit test. For detection in low-density regions we combine this approach with a 2SND (Two-Stage-Novelty-Detector) which we presented in preliminary work. The properties of CANDIES are evaluated using artificial data and benchmark data from the field of intrusion detection in computer networks, where the task is to detect new kinds of attacks.", "creator": "LaTeX with hyperref package"}}}