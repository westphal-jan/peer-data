{"id": "1605.05826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Declarative Machine Learning - A Classification of Basic Properties and Types", "abstract": "Declarative machine learning (ML) aims at the high-level specification of ML tasks or algorithms, and automatic generation of optimized execution plans from these specifications. The fundamental goal is to simplify the usage and/or development of ML algorithms, which is especially important in the context of large-scale computations. However, ML systems at different abstraction levels have emerged over time and accordingly there has been a controversy about the meaning of this general definition of declarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance, to ML task (learning problem) specifications with optimization for performance and accuracy. We argue that these different types of declarative ML complement each other as they address different users (data scientists and end users). This paper makes an attempt to create a taxonomy for declarative ML, including a definition of essential basic properties and types of declarative ML. Along the way, we provide insights into implications of these properties. We also use this taxonomy to classify existing systems. Finally, we draw conclusions on defining appropriate benchmarks and specification languages for declarative ML.", "histories": [["v1", "Thu, 19 May 2016 06:39:28 GMT  (32kb,D)", "http://arxiv.org/abs/1605.05826v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.DC cs.LG cs.PL", "authors": ["matthias boehm", "alexandre v evfimievski", "niketan pansare", "berthold reinwald"], "accepted": false, "id": "1605.05826"}, "pdf": {"name": "1605.05826.pdf", "metadata": {"source": "META", "title": "Declarative Machine Learning \u2013 A Classification of Basic Properties and Types", "authors": ["Matthias Boehm", "Alexandre V. Evfimievski", "Niketan Pansare", "Berthold Reinwald"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. BASIC PROPERTIES", "text": "As a basis for discussing types of declarative ML, we define essential, fundamental properties in the three categories data, operations, and result correctness. We also discuss the implications of each property and provide examples of how Apache SystemML - as a representative system for declarative ML - implements these properties."}, {"heading": "2.1 Physical Data Independence", "text": "The most important goal of declarative ML is data independence, because it decouples the high-level specification of ML tasks or algorithms from the underlying data representations and operations. Data structure independence serves two important purposes: First, the decision for distributed vs data representations is replaced by specific tasks or algorithms that are independent of data size and context (e.g. distributed compatibility with data structures); second, abstract data types such as matrix hide the decision for distributed vs data representations."}, {"heading": "2.2 Operation Semantics", "text": "This year, as never before in the history of the city, where it is so far that it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year, a year,"}, {"heading": "2.3 Result Correctness", "text": "The characteristics of data independence and operational semantics are necessary, but not sufficient for declarative ML. We must continue to define the concept of result correctness. In terms of practicality for distributed computing, we define operating results as equivalent if they are essentially the same, i.e. they are algebraic (logical) equivalent, which ignores rounding errors (e.g. due to partial aggregation or alternative evaluation orders of operations).Property 7. Implementation-agnostic results: The results of ML tasks or algorithms as well as individual operations are equivalent (essentially the same), regardless of the type and location of the underlying runtime operations. This property further qualifies implementation-agnostic operations (P5) to achieve correct results, regardless of optimization decisions, alternative execution strategies must provide equivalent results, regardless of whether they are performed locally or as distributed operations."}, {"heading": "3. TYPES OF DECLARATIVE ML", "text": "So far, we have discussed the general characteristics of declarative machine learning that apply to all types of declarative ML. We are now creating a taxonomy of types of declarative ML, namely declarative ML algorithms and declarative ML tasks. These types refer to fundamentally different concepts and therefore differ in their scope of specification."}, {"heading": "3.1 Declarative ML Algorithms (Type 1)", "text": "Declarative ML algorithms enable data scientists to write and customize ML algorithms in a declarative manner, requiring fine-grained semantics, including control flow and data flow, with primitive core operations often based on linear algebra or statistical functions, and the common optimization goal being to minimize execution time, but also other goals such as resource usage. Algorithm-centric specification defines precise semantics, but leaves considerable freedom for data visualization and execution plan optimization. This level of abstraction allows data scientists to encode algorithms as they are most naturally expressed, quickly taking advantage of the latest algorithmic advances. End-users also benefit from simply invoking these algorithms for automatic optimization, adaptability, and portability. Typical system categories are DSL-centric, QL-centric, and DF."}, {"heading": "3.2 Declarative ML Tasks (Type 2)", "text": "Unlike declarative ML algorithms, declarative ML tasks allow the end user (without ML background) to define ML tasks such as classification, factorization, and optimization independently of ML algorithm specifications. This coarse-grained scope includes automatic feature and model selection, and allows optimization of both model accuracy and runtime. Primitive core operations are task-specific, i.e. dependent on the task at hand, alternative candidate algorithms, loss functions (measure of fit accuracy), and hyperparameters. Operating semantics are built in or commented on either at the level of the algorithms or loss functions used. The properties of declarative ML must be applied to the core optimization problem of the given ML task, not to the entire stack of used ML algorithms, as long as they are provided with relevant properties that allow alternative plans and costs to be considered."}, {"heading": "4. SYSTEMS CLASSIFICATION", "text": "Considering the taxonomy of basic characteristics and types of declarative ML, we are now classifying existing systems. There have been some related papers on similar classifications, in particular Kumar et al. defined the concept of a model selection management system [28], together with categories of ML systems, but focused primarily on the coverage of industrial systems and not on the specifics of declarative machine learning. Tables 2 and 3 classify - within the framework of declarative ML algorithms and tasks - existing systems in terms of the defined basic characteristics and types of declarative machine learning. This classification also indicates distributed versus local operations and the optimization goal used."}, {"heading": "4.1 Declarative ML Algorithms", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.2 Declarative ML Tasks", "text": "Dre rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "5. BENCHMARKING ML SYSTEMS", "text": "After discussing the individual types of declarative ML, it is clear that there cannot be a single benchmark that covers them all. Existing benchmarks for large-scale calculations such as BigBench [5, 21], SparkBench [2, 29] or HiBench [26] cover machine learning, but often simply refer to reference implementations of large-scale ML libraries. This is fine to evaluate underlying Hadoop or Spark implementations, but simply cannot serve as a benchmark for declarative ML.A benchmarks: We argue that industry and science are best served by benchmarks for types of declarative machine learning (see Section 3). To adequately reflect common workload characteristics, the benchmarks should continue to be tailored to key subcategories of systems. For example, systems for declarative ML algorithms should cover the most important subcategories of DSL-centric systems, despite the fact that DSL-centric and centric QL-centric systems already cover the different subcategories of DSL-centric systems."}, {"heading": "6. CONCLUSION", "text": "The classification of existing systems has shown that this taxonomy is indeed a useful tool for systematically qualifying system properties. Essentially, this suggests a syntax-independent classification of declarative ML that disqualifies the philosophical argument against loops and control flow in general. We are at the dawn of an exciting era of declarative ML, with a good understanding of various aspects, but also many open challenges in research. As advanced analytics becomes ubiquitous and technological environments increasingly change, a declarative specification of ML tasks or algorithms becomes increasingly important. Accordingly, we encourage the research community to engage in this discussion of the fundamental properties of declarative ML in order to ultimately arrive at a common understanding."}, {"heading": "7. REFERENCES", "text": "[1] M. Abadi et al. TensorFlow: Large-Scale Machine Learningon Heterogeneous Distributed Systems. CoRR, abs / 1603.04467, 2016. [2] D. Agrawal et al. SparkBench - A Spark Performance Testing Suite. In TPCTC, 2015. [3] A. Alexandrov et al. The Stratosphere Platform for Big Data Analytics. VLDB J., 23 (6), 2014. [4] A. Alexandrov et al. Implicit Performance Testing Suite through Deep Language Embedding. In SIGMOD, 2015. [5] C. K. Baru et al. Discussion of BigBench."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["M. Abadi"], "venue": "Learning on Heterogeneous Distributed Systems. CoRR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "SparkBench - A Spark Performance Testing Suite", "author": ["D. Agrawal"], "venue": "TPCTC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The Stratosphere Platform for Big Data Analytics", "author": ["A. Alexandrov"], "venue": "VLDB J.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Implicit Parallelism through Deep Language Embedding", "author": ["A. Alexandrov"], "venue": "In SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Discussion of BigBench: A Proposed Industry Standard Performance Benchmark for Big Data", "author": ["C.K. Baru"], "venue": "In TPCTC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML", "author": ["M. Boehm"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "SystemML\u2019s Optimizer: Plan Generation for Large-Scale Machine Learning Programs", "author": ["M. Boehm"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Declarative Systems for Large-Scale Machine Learning", "author": ["V.R. Borkar"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Overview of SciDB: Large Scale Array Storage, Processing and Analysis", "author": ["P.G. Brown"], "venue": "In SIGMOD,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms", "author": ["Z. Cai", "Z.J. Gao", "S. Luo", "L.L. Perez", "Z. Vagena", "C.M. Jermaine"], "venue": "In SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Simulation of Database-Valued Markov Chains Using SimSQL", "author": ["Z. Cai", "Z. Vagena", "L.L. Perez", "S. Arumugam", "P.J. Haas", "C.M. Jermaine"], "venue": "In SIGMOD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "MAD Skills: New Analysis Practices for Big Data", "author": ["J. Cohen", "B. Dolan", "M. Dunlap", "J.M. Hellerstein", "C. Welton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "An Architecture for Compiling UDF-centric", "author": ["A. Crotty"], "venue": "Workflows. PVLDB,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "MapReduce: Simplified Data Processing on Large Clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "In OSDI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Processing Forecasting Queries", "author": ["S. Duan", "S. Babu"], "venue": "In VLDB,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Spinning Fast Iterative Data Flows", "author": ["S. Ewen", "K. Tzoumas", "M. Kaufmann", "V. Markl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Towards a Unified Architecture for In-RDBMS Analytics", "author": ["X. Feng", "A. Kumar", "B. Recht", "C. R\u00e9"], "venue": "In SIGMOD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "F2DB: The Flash-Forward Database System", "author": ["U. Fischer", "F. Rosenthal", "W. Lehner"], "venue": "In ICDE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "A Skip-List Approach for Efficiently Processing", "author": ["T. Ge", "S.B. Zdonik"], "venue": "Forecasting Queries. PVLDB,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "BigBench: Towards an Industry Standard Benchmark for Big Data Analytics", "author": ["A. Ghazal"], "venue": "In SIGMOD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "SystemML: Declarative Machine Learning on MapReduce", "author": ["A. Ghoting"], "venue": "In ICDE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The MADlib Analytics Library or MAD Skills, the SQL", "author": ["J.M. Hellerstein"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Cumulon: Optimizing Statistical Data Analysis in the Cloud", "author": ["B. Huang", "S. Babu", "J. Yang"], "venue": "In SIGMOD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Cumulon: Matrix-Based Data Analytics in the Cloud with Spot Instances", "author": ["B. Huang", "N.W.D. Jarrett", "S. Babu", "S. Mukherjee", "J. Yang"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "The HiBench Benchmark Suite: Characterization of the MapReduce-based data analysis", "author": ["S. Huang", "J. Huang", "J. Dai", "T. Xie", "B. Huang"], "venue": "In ICDE Workshops,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "MLbase: A Distributed Machine-learning System", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith", "M.J. Franklin", "M.I. Jordan"], "venue": "In CIDR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Model Selection Management Systems: The Next Frontier of Advanced Analytics", "author": ["A. Kumar", "R. McCann", "J. Naughton", "J.M. Patel"], "venue": "SIGMOD Record,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "SparkBench: A Comprehensive Benchmarking Suite For In Memory Data Analytic Platform Spark", "author": ["M. Li", "J. Tan", "Y. Wang", "L. Zhang", "V. Salapura"], "venue": "In CF,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Incremental Knowledge Base Construction", "author": ["J. Shin", "S. Wu", "F. Wang", "C.D. Sa", "C. Zhang", "C. R\u00e9"], "venue": "Using DeepDive. PVLDB,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Automating Model Search for Large Scale Machine Learning", "author": ["E.R. Sparks", "A. Talwalkar", "D. Haas", "M.J. Franklin", "M.I. Jordan", "T. Kraska"], "venue": "SOCC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "The Architecture of SciDB", "author": ["M. Stonebraker", "P. Brown", "A. Poliakov", "S. Raman"], "venue": "In SSDBM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning", "author": ["A.K. Sujeeth"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "GenBase: A Complex Analytics Genomics Benchmark", "author": ["R. Taft", "M. Vartak", "N.R. Satish", "N. Sundaram", "S. Madden", "M. Stonebraker"], "venue": "In SIGMOD,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Scalable and Numerically Stable Descriptive Statistics in SystemML", "author": ["Y. Tian", "S. Tatikonda", "B. Reinwald"], "venue": "In ICDE,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Presto: Distributed Machine Learning and Graph Processing with Sparse Matrices", "author": ["S. Venkataraman", "E. Bodzsar", "I. Roy", "A. AuYoung", "R.S. Schreiber"], "venue": "In EuroSys,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Exploiting Matrix Dependency for Efficient Distributed Matrix Computation", "author": ["L. Yu", "Y. Shao", "B. Cui"], "venue": "In SIGMOD,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", "author": ["M. Zaharia"], "venue": "In NSDI,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Materialization Optimizations for Feature Selection Workloads", "author": ["C. Zhang", "A. Kumar", "C. R\u00e9"], "venue": "In SIGMOD,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "RIOT: I/O-Efficient Numerical Computing without SQL", "author": ["Y. Zhang", "H. Herodotou", "J. Yang"], "venue": "In CIDR,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 128, "endOffset": 131}, {"referenceID": 11, "context": "Large-Scale ML Libraries: Large-scale ML libraries like MLlib (aka SparkML) [30], Mahout [37], and MADlib [12, 23] are currently the predominant tools for largescale ML.", "startOffset": 106, "endOffset": 114}, {"referenceID": 21, "context": "Large-Scale ML Libraries: Large-scale ML libraries like MLlib (aka SparkML) [30], Mahout [37], and MADlib [12, 23] are currently the predominant tools for largescale ML.", "startOffset": 106, "endOffset": 114}, {"referenceID": 25, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 9, "endOffset": 17}, {"referenceID": 30, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 9, "endOffset": 17}, {"referenceID": 38, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 21, "endOffset": 28}, {"referenceID": 31, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 21, "endOffset": 28}, {"referenceID": 6, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 56, "endOffset": 63}, {"referenceID": 20, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 56, "endOffset": 63}, {"referenceID": 10, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": ", MLlib [30], Mahout [37], (fixed plan) MADlib [12, 23], ORE, Rev R", "startOffset": 47, "endOffset": 55}, {"referenceID": 21, "context": ", MLlib [30], Mahout [37], (fixed plan) MADlib [12, 23], ORE, Rev R", "startOffset": 47, "endOffset": 55}, {"referenceID": 0, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 20, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 36, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 38, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 39, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 12, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 68, "endOffset": 76}, {"referenceID": 15, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 68, "endOffset": 76}, {"referenceID": 7, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 129, "endOffset": 132}, {"referenceID": 25, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 30, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 38, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 5, "context": "Note that sequences of operations like t(X) %*% X %*% p from Figure 1(a) or specifications like independent foreach loops (parfor) [6] are assertions on semantics and properties of the algorithm rather than imperative execution strategies.", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "Furthermore, we bound the round-off errors via numerically stable operations (based on Kahan+) [38] for descriptive statistics and aggregations.", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 91, "endOffset": 98}, {"referenceID": 20, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 91, "endOffset": 98}, {"referenceID": 35, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 120, "endOffset": 128}, {"referenceID": 23, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 120, "endOffset": 128}, {"referenceID": 36, "context": "runtime constraints DMac [40] X X X X X X X N/A min runtime s.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "memory constraints TensorFlow [1] X X X X X X X N/A min runtime s.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 6, "endOffset": 13}, {"referenceID": 31, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 6, "endOffset": 13}, {"referenceID": 10, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "defined a notion of a model selection management system [28], along with categories of ML systems, but primarily focused on coverage of industrial systems rather than specifics of declarative machine learning.", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "Early examples of declarative systems are RIOT [43] and OptiML [35], which provide R and Scala DSLs, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "Early examples of declarative systems are RIOT [43] and OptiML [35], which provide R and Scala DSLs, respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "SystemML [7, 22] covers both single-node and distributed computation (on MapReduce and Spark) and satisfies all eight properties of declarative ML as described throughout this paper.", "startOffset": 9, "endOffset": 16}, {"referenceID": 20, "context": "SystemML [7, 22] covers both single-node and distributed computation (on MapReduce and Spark) and satisfies all eight properties of declarative ML as described throughout this paper.", "startOffset": 9, "endOffset": 16}, {"referenceID": 22, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 33, "endOffset": 41}, {"referenceID": 23, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 33, "endOffset": 41}, {"referenceID": 36, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 90, "endOffset": 93}, {"referenceID": 22, "context": "Cumulon [24, 25] can be seen as a declarative system.", "startOffset": 8, "endOffset": 16}, {"referenceID": 23, "context": "Cumulon [24, 25] can be seen as a declarative system.", "startOffset": 8, "endOffset": 16}, {"referenceID": 35, "context": "Mahout Samsara [15], Distributed R [39], and DMac [40] are not declarative because they expose physical data structures and distributed operations to the user (P1, P5).", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "Mahout Samsara [15], Distributed R [39], and DMac [40] are not declarative because they expose physical data structures and distributed operations to the user (P1, P5).", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "TensorFlow [1] is a compelling system but focuses more on extensibility than declarative specification.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "A prime example of array databases is SciDB [9, 34] which indeed satisfies all basic properties for declarative ML.", "startOffset": 44, "endOffset": 51}, {"referenceID": 31, "context": "A prime example of array databases is SciDB [9, 34] which indeed satisfies all basic properties for declarative ML.", "startOffset": 44, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, also SimSQL [11]\u2014as an example of Bayesian ML or more broadly stochastic analysis\u2014 satisfies all basic properties of declarative ML.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 21, "endOffset": 24}, {"referenceID": 12, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 228, "endOffset": 231}, {"referenceID": 16, "context": "Bismarck [18] X X X N/A min runtime s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "accuracy constraints TensorFlow [1] X X X X X X X X X 2 min runtime s.", "startOffset": 32, "endOffset": 35}, {"referenceID": 25, "context": "accuracy constraints MLbase [27, 33] X X X X X X X X X 2 max accuracy s.", "startOffset": 28, "endOffset": 36}, {"referenceID": 30, "context": "accuracy constraints MLbase [27, 33] X X X X X X X X X 2 max accuracy s.", "startOffset": 28, "endOffset": 36}, {"referenceID": 38, "context": "Columbus [42] X X X X X X X X 2 min runtime s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "accuracy constraints DeepDive [32] X X X X X X X X 2 max accuracy s.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "General-Purpose Optimization: Bismarck [18] provides in-database general-purpose optimization via incremental gradient decent, where users provide UDFs for initialization, transition, and termination.", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "The latter also does not satisfy P7/P8 due to Hogwild!-style [31] model updates.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "TensorFlow [1] also provides primitives for general-purpose optimization via different optimization algorithms.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Model and Feature Selection: MLbase [27] with its TUPAQ [33] component for automatic model search allows users to specify candidate model configurations, a quality measure, and runtime constraints (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "Model and Feature Selection: MLbase [27] with its TUPAQ [33] component for automatic model search allows users to specify candidate model configurations, a quality measure, and runtime constraints (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "Columbus [42] allows users to specify feature engineering workflows in R including data preparations and model building with existing R packages.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "DeepDive [32] enables knowledge base construction via statistical inference.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 62, "endOffset": 69}, {"referenceID": 19, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 62, "endOffset": 69}, {"referenceID": 1, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 82, "endOffset": 89}, {"referenceID": 27, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 82, "endOffset": 89}, {"referenceID": 24, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 88, "endOffset": 95}, {"referenceID": 31, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 88, "endOffset": 95}, {"referenceID": 10, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 33, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 216, "endOffset": 220}], "year": 2016, "abstractText": "Declarative machine learning (ML) aims at the high-level specification of ML tasks or algorithms, and automatic generation of optimized execution plans from these specifications. The fundamental goal is to simplify the usage and/or development of ML algorithms, which is especially important in the context of large-scale computations. However, ML systems at different abstraction levels have emerged over time and accordingly there has been a controversy about the meaning of this general definition of declarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance, to ML task (learning problem) specifications with optimization for performance and accuracy. We argue that these different types of declarative ML complement each other as they address different users (data scientists and end users). This paper makes an attempt to create a taxonomy for declarative ML, including a definition of essential basic properties and types of declarative ML. Along the way, we provide insights into implications of these properties. We also use this taxonomy to classify existing systems. Finally, we draw conclusions on defining appropriate benchmarks and specification languages for declarative ML.", "creator": ""}}}