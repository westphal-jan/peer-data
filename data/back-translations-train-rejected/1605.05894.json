{"id": "1605.05894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages", "abstract": "Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.", "histories": [["v1", "Thu, 19 May 2016 11:32:29 GMT  (296kb,D)", "https://arxiv.org/abs/1605.05894v1", "Accepted at the 10th Language Resources and Evaluation Conference (LREC), 6 pages"], ["v2", "Tue, 31 May 2016 17:30:05 GMT  (296kb,D)", "http://arxiv.org/abs/1605.05894v2", "Accepted at the 10th Language Resources and Evaluation Conference (LREC), 6 pages"]], "COMMENTS": "Accepted at the 10th Language Resources and Evaluation Conference (LREC), 6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.SI", "authors": ["muhammad imran", "prasenjit mitra", "carlos castillo"], "accepted": false, "id": "1605.05894"}, "pdf": {"name": "1605.05894.pdf", "metadata": {"source": "CRF", "title": "Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages", "authors": ["Muhammad Imran", "Prasenjit Mitra", "Carlos Castillo"], "emails": ["mimran@qf.org.qa,", "pmitra@qf.org.qa,", "chato@acm.org"], "sections": [{"heading": null, "text": "Tags: Natural Language Processing, Twitter, Civil Protection, Supervised classification"}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "1.1. Contributions", "text": "The contributions to this paper are as follows: 1. We present humanly annotated crisis-related messages collected during 19 different crisis.2. We use human annotations to build machine learning classifiers in a multi-class classification setting to classify messages that are useful for humanitarian purposes.3 We provide world-class word2vec word embeddings that have been trained on 52 million crisis-related messages. 4. We use the collected data to identify OOV words (not in vocabulary) and provide humanly annotated normalized lexical resources for various lexical variations."}, {"heading": "1.2. Paper organization", "text": "The rest of the essay is structured as follows: In the next section, we describe details of data sets and annotation schemes. Section 3 describes the supervised classification task and word2vec word embedding. Section 4 contains details on text normalization and we present related work in Section 5. We conclude the essay in Section 6."}, {"heading": "2. Crises Corpora Collection and Annotation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Data collection", "text": "We collected crisis-related messages from Twitter that were posted during 19 different crises between 2013 and 2015. Table 1 shows the list of crisis events along with their names, the type of crisis (e.g. earthquakes, floods), the countries in which they took place, and the number of tweets that each crisis contains. We collected these messages through our Artificial Intelligence for Disaster Response (Imran et al., 2014) platform, which is an open source platform for collecting and classifying Twitter messages during the occurrence of a humanitarian crisis. AIDR has been used by UN OCHA in many major disasters such as Nepal Earthquake, Typhoon Hagupit. AIDR offers various practical ways to collect messages from Twitter via the Streaming API. Different strategies for collecting data can be used. For example, tweets containing some keywords are collected that originate specifically from a specific geographical area / region / city / city (e.g. New York). Detailed strategies for collecting data are included in the table 1 of each data set."}, {"heading": "2.2. Data annotation", "text": "These messages have been presented very differently on social media in terms of the information they contain. For example, users post messages of a personal nature, messages that are useful for situational awareness (e.g. infrastructure damage, causalities, individual needs), or they do not refer to the crisis at all. Depending on their information needs, different humanitarian organizations use different comments on categories of these messages. In this work, we use a subset of the comments used by the United Nations Office for the Coordination of Humanitarian Affairs (UN OCHA). The 9 categories (including two catch-all classes: \"Other useful information\" and \"Irrelevant\") used by the United Nations OCHA are shown in the annotation scheme below. For most of the datasets we have conducted, employing volunteers and paid workers. To perform voluntary comments, messages from Twitter were collected in real time and routed through a deduplication process. Only unique messages were considered for human comments."}, {"heading": "3. Classification of Messages", "text": "To make sense of large volumes of Twitter messages posted during a crisis, we consider a basic operation, namely the automatic categorization of messages into categories of interest. This is a multi-class categorization problem, in which instances are categorized into one of several classes. Specifically, we aim to learn a predictor h: X \u2192 Y, where X is the set of messages and Y is a finite set of categories. To this end, we use three well-known learning algorithms, namely Naive Bayes (NB), Support Vector Machines (SVM) and Random Forest (RF)."}, {"heading": "3.1. Preprocessing and feature extraction", "text": "Before learning a classifier, we perform the following pre-processing steps: First, we remove stopwords, URLs, and usernames from Twitter messages. We use the Lovins stamp. We use unigrams and bigrams as traits. Previous studies have shown that these two traits are better when used for similar classification tasks (Imran et al., 2013). Finally, we used information gain, a well-known method for selecting the best 1k traits. The data we used in this task were commented on by the paid workers."}, {"heading": "3.2. Evaluation and Results", "text": "We trained all three types of classifiers based on the pre-processed data. To evaluate the trained models, we used 10-fold cross-validation techniques. Table 2 shows the results of the classification task in the form of Area Under ROC curve 4 for all classes of the 8 different disaster data sets. We also show the proportion of each class in each data set. Given the complexity of the task, i.e. the classification of text messages in multiple classes, we can see that all three classifiers have fairly outdated results. In this case, a random classifier represents an AUC = 0.50 and higher values. Apart from the class \"Missing or Found Persons,\" which is the smallest class in relation to all data sets, the results for most other classes are at an acceptable level (i.e. above 0.80)."}, {"heading": "3.3. Crisis word embeddings", "text": "Many applications of machine learning and computational linguistics are based on semantic representations and relationships between words of a text document. Many different types of methods have been proposed that use continuous representations of words such as Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this thesis, we train word embeddings (i.e. distributed word embeddings) using the 52 million Twitter messages in our data sets and make them available to the research community. To our knowledge, this is the first large word embeddings trained on crisis-related tweets. In this thesis, we use word2vec, a very popular software to train word embeddings (Mikolov et al., 2013).As pre-processing, we have fixed URLs and digits (using URLs and digits)."}, {"heading": "4. Twitter Text Normalization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Language issues in Twitter messages", "text": "Typically, Twitter messages are short, informal, loud, unstructured and often contain spelling and grammatical errors. Moreover, Twitter users deliberately shorten words by using abbreviations, acronyms, slangs and sometimes words without spaces due to Twitter's 140-character limitation. We would improve the accuracy of natural language processing techniques if we could recognize the informal nature of the language in tweets and normalize OOV terms (Han et al., 2013). We classify these lexical variations into the following five categories: 1. Spelling mistakes / spelling errors: e.g. earthquakes, misunderstanding (missing), ovrcme (overcoming) 2. Word abbreviations / slangs: e.g. platelets (please), srsly (seriously), govt (government), msg (message) 3. Prapal abbreviations (override), 2. Abbreviations / slangs: (four-line) for four-line (four-line) (four-line) (four-line) (four-line (four-line) (four-line) (four-line) (for four-line (four-line) (four-line) (four-line) (for four-line (four-line) (four-line) (four-line) (four-line (four-line)) (four-line (four-line) (for four-line (four-line)) (four-line (four-line) (four-line (four-line)) (four-line (four-line (four-line) for four-line (four-line (four-line)) (four-line (four-line (four-line))))."}, {"heading": "4.2. Identification of candidate OOV words", "text": "To identify candidates OOV words that require normalization, we first build vocabulary consisting of lexical variations mentioned in the previous section. We use a dictionary that is available on the Web, abbreviations, chat shortcuts, and slang.5 We also use the SCOWL (Spell Checker Oriented Word Lists) aspell English dictionary 6, which consists of 349,554 English words. The SCOWL dictionary is suitable for English spell checkers for most English dialects. Although the SCOWL dictionary contains place names (e.g. names of countries and famous cities) aspell English dictionary 6, which consists of 349,554 English words, we found that its coverage is incomplete and a large number of cities in Nepal are missing. To overcome this problem, we use the 5http: / / www.innocentenglish.com / news / texting-abbreviations-one."}, {"heading": "4.3. Normalization of OOV words", "text": "In order to normalize the identified OOV words, we used the CrowdFlower crowdsourcing platform. In this case, a crowdsourcing task consists of a Twitter message containing one or more OOV words and a series of instructions shown in Figure 1. Employees are asked to read the instructions and examples carefully before giving a response. An employee reads the message provided and provides a correct OOV tag (i.e. slang / abbreviation / acronym, a place name, an organization name, a misspelled word or a person name). If an OOV is a misspelled word, the employee also provides its corrected form. We provide all resources and the results of crowdsourcing to the research community."}, {"heading": "5. Related Work", "text": "The use of microblogging platforms such as Twitter during the sudden emergence of a crisis situation has increased in recent years. Thousands of crisis-related messages posted online contain important information that can also be useful for humanitarian organizations in disaster response if processed in a timely and effective manner (Hughes and Palen, 2009; Imran et al., 2015) Many different types of processing techniques, from machine learning to natural language processing to computational linguistics, have been developed for different purposes (Corvey et al., 2010) (Imran et al., 2016). Although there are some resources (Temnikova et al., 2015; Olteanu et al., 2015), crisis informatics experts are still unable to fully exploit the capabilities of various computational methods due to the lack of relevant data, especially human-annotated data. To overcome these problems, we present to the research community a company that consists of tagged and untagged crisis-related Twitter resources that we provide for alienated analytics."}, {"heading": "6. Conclusions", "text": "We present Twitter corporations, which consist of over 52 million crisis-related tweets collected during 19 crisis events. We provide two sets of comments on the topic categorization of tweets and the marking of vocabulary words and their normalization. We build classifiers for machine learning to empirically validate the effectiveness of annotated data sets. We also provide word2vec word embeddings trained on 52 million messages. We believe that these resources and the tools that are built with them will help improve the automatic processing of crisis-related messages in natural language and ultimately be useful for humanitarian organizations."}, {"heading": "7. References", "text": "Bontcheva, K., Derczynski, L., Funk, A., Greenwood, M. A., Maynard, D., and Aswani, N. (2013). Twitie: An open-source information extraction pipeline for microblog text. In RANLP, pages 83-90. Cameron, M. A., Power, R., Robinson, B., and Yin, J. (2012). Emergency situation awareness from twitter for crisis management. In Proc. of the 21st international conference companion on on World Wide Web, pages 695- 698.Corvey, W. J., Vieweg, S., Rood, T., and Palmer, M. (2010). Twitter in mass emergency disaster: what nlp techniques can contribute. In Proc. of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, S. Foster, J., C., and Palmer, M. (2010)."}], "references": [{"title": "Twitie: An open-source information extraction pipeline for microblog text", "author": ["K. Bontcheva", "L. Derczynski", "A. Funk", "M.A. Greenwood", "D. Maynard", "N. Aswani"], "venue": "RANLP, pages 83\u201390.", "citeRegEx": "Bontcheva et al\\.,? 2013", "shortCiteRegEx": "Bontcheva et al\\.", "year": 2013}, {"title": "Emergency situation awareness from twitter for crisis management", "author": ["M.A. Cameron", "R. Power", "B. Robinson", "J. Yin"], "venue": "Proc. of the 21st international conference companion on World Wide Web, pages 695\u2013 698.", "citeRegEx": "Cameron et al\\.,? 2012", "shortCiteRegEx": "Cameron et al\\.", "year": 2012}, {"title": "Twitter in mass emergency: what nlp techniques can contribute", "author": ["W.J. Corvey", "S. Vieweg", "T. Rood", "M. Palmer"], "venue": "Proc. of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 23\u201324.", "citeRegEx": "Corvey et al\\.,? 2010", "shortCiteRegEx": "Corvey et al\\.", "year": 2010}, {"title": " hardtoparse: Pos tagging and parsing the twitterverse", "author": ["J. Foster", "\u00d6. \u00c7etinoglu", "J. Wagner", "J. Le Roux", "S. Hogan", "J. Nivre", "D. Hogan", "J. Van Genabith"], "venue": "AAAI 2011 Workshop on Analyzing Microtext, pages 20\u201325.", "citeRegEx": "Foster et al\\.,? 2011", "shortCiteRegEx": "Foster et al\\.", "year": 2011}, {"title": "Lexical normalization for social media text", "author": ["B. Han", "P. Cook", "T. Baldwin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 4(1):5.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Twitter adoption and use in mass convergence and emergency events", "author": ["A.L. Hughes", "L. Palen"], "venue": "International Journal of Emergency Management, 6(3-4):248\u2013 260.", "citeRegEx": "Hughes and Palen,? 2009", "shortCiteRegEx": "Hughes and Palen", "year": 2009}, {"title": "Extracting information nuggets from disaster-related messages in social media", "author": ["M. Imran", "S.M. Elbassuoni", "C. Castillo", "F. Diaz", "P. Meier"], "venue": "Proc. of ISCRAM, Baden-Baden, Germany.", "citeRegEx": "Imran et al\\.,? 2013", "shortCiteRegEx": "Imran et al\\.", "year": 2013}, {"title": "AIDR: Artificial intelligence for disaster response", "author": ["M. Imran", "C. Castillo", "J. Lucas", "P. Meier", "S. Vieweg"], "venue": "Proc. the 23rd international conference on World wide web companion, pages 159\u2013162.", "citeRegEx": "Imran et al\\.,? 2014", "shortCiteRegEx": "Imran et al\\.", "year": 2014}, {"title": "Processing social media messages in mass emergency: A survey", "author": ["M. Imran", "C. Castillo", "F. Diaz", "S. Vieweg"], "venue": "ACM Computing Surveys (CSUR), 47(4):67.", "citeRegEx": "Imran et al\\.,? 2015", "shortCiteRegEx": "Imran et al\\.", "year": 2015}, {"title": "Enabling digital health by automatic classification of short messages", "author": ["M. Imran", "P. Meier", "C. Castillo", "A. Lesa", "M. Garcia Herranz"], "venue": "Proceedings of the 6th International Conference on Digital Health Conference, pages 61\u201365. ACM.", "citeRegEx": "Imran et al\\.,? 2016", "shortCiteRegEx": "Imran et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "What to expect when the unexpected happens: Social media communications across crises", "author": ["A. Olteanu", "S. Vieweg", "C. Castillo"], "venue": "Proc. of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, pages 994\u20131009. ACM.", "citeRegEx": "Olteanu et al\\.,? 2015", "shortCiteRegEx": "Olteanu et al\\.", "year": 2015}, {"title": "Twitter as a corpus for sentiment analysis and opinion mining", "author": ["A. Pak", "P. Paroubek"], "venue": "LREC, volume 10, pages 1320\u20131326.", "citeRegEx": "Pak and Paroubek,? 2010", "shortCiteRegEx": "Pak and Paroubek", "year": 2010}, {"title": "Emergency-relief coordination on social media: Automatically matching resource requests and offers", "author": ["H. Purohit", "C. Castillo", "F. Diaz", "A. Sheth", "P. Meier"], "venue": "First Monday, 19(1).", "citeRegEx": "Purohit et al\\.,? 2013", "shortCiteRegEx": "Purohit et al\\.", "year": 2013}, {"title": "Unsupervised modeling of twitter conversations", "author": ["A. Ritter", "C. Cherry", "B. Dolan"], "venue": "Proc of NAACL.", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Information Systems for Crisis Response and Management, ISCRAM", "author": ["I. Temnikova", "C. Castillo", "S. Vieweg"], "venue": "Emterms", "citeRegEx": "Temnikova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Temnikova et al\\.", "year": 2015}, {"title": "Integrating social media communications into the rapid assessment of sudden onset disasters", "author": ["S. Vieweg", "C. Castillo", "M. Imran"], "venue": "Social Informatics, pages 444\u2013461. Springer.", "citeRegEx": "Vieweg et al\\.,? 2014", "shortCiteRegEx": "Vieweg et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Twitter has been extensively used as an active communication channel, especially during mass convergence events such as natural disasters like earthquakes, floods, typhoons (Imran et al., 2015; Hughes and Palen, 2009).", "startOffset": 173, "endOffset": 217}, {"referenceID": 5, "context": "Twitter has been extensively used as an active communication channel, especially during mass convergence events such as natural disasters like earthquakes, floods, typhoons (Imran et al., 2015; Hughes and Palen, 2009).", "startOffset": 173, "endOffset": 217}, {"referenceID": 16, "context": "(Vieweg et al., 2014).", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Recent studies have shown the importance of social media messages to enhance situational awareness and also indicate that these messages contain significant actionable and tactical information (Cameron et al., 2012; Imran et al., 2013; Purohit et al., 2013).", "startOffset": 193, "endOffset": 257}, {"referenceID": 6, "context": "Recent studies have shown the importance of social media messages to enhance situational awareness and also indicate that these messages contain significant actionable and tactical information (Cameron et al., 2012; Imran et al., 2013; Purohit et al., 2013).", "startOffset": 193, "endOffset": 257}, {"referenceID": 13, "context": "Recent studies have shown the importance of social media messages to enhance situational awareness and also indicate that these messages contain significant actionable and tactical information (Cameron et al., 2012; Imran et al., 2013; Purohit et al., 2013).", "startOffset": 193, "endOffset": 257}, {"referenceID": 0, "context": "Many Natural-LanguageProcessing (NLP) techniques such as automatic summarization, information classification, named-entity recognition, information extraction can be used to process such social media messages (Bontcheva et al., 2013; Imran et al., 2015).", "startOffset": 209, "endOffset": 253}, {"referenceID": 8, "context": "Many Natural-LanguageProcessing (NLP) techniques such as automatic summarization, information classification, named-entity recognition, information extraction can be used to process such social media messages (Bontcheva et al., 2013; Imran et al., 2015).", "startOffset": 209, "endOffset": 253}, {"referenceID": 4, "context": "However, many social media messages are very brief, informal, and often contain slangs, typograpical errors, abbreviations, and incorrect grammar (Han et al., 2013).", "startOffset": 146, "endOffset": 164}, {"referenceID": 14, "context": "These issues degrade the performance of many NLP techniques when used down the processing pipeline (Ritter et al., 2010; Foster et al., 2011).", "startOffset": 99, "endOffset": 141}, {"referenceID": 3, "context": "These issues degrade the performance of many NLP techniques when used down the processing pipeline (Ritter et al., 2010; Foster et al., 2011).", "startOffset": 99, "endOffset": 141}, {"referenceID": 12, "context": "are useful for formal crisis response organizations as well as for the research community to build more effective computational methods (Pak and Paroubek, 2010; Imran et al., 2015) on top.", "startOffset": 136, "endOffset": 180}, {"referenceID": 8, "context": "are useful for formal crisis response organizations as well as for the research community to build more effective computational methods (Pak and Paroubek, 2010; Imran et al., 2015) on top.", "startOffset": 136, "endOffset": 180}, {"referenceID": 7, "context": "We collected these messages using our AIDR (Artificial Intelligence for Disaster Response) platform (Imran et al., 2014).", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "Previous studies found these two features outperform when used for similar classification tasks (Imran et al., 2013).", "startOffset": 96, "endOffset": 116}, {"referenceID": 10, "context": "We use word2vec, a very popular software to train word embedding (Mikolov et al., 2013).", "startOffset": 65, "endOffset": 87}, {"referenceID": 4, "context": "The accuracy of natural language processing techniques would improve if we can identify the informal nature of the language in tweets and normalize OOV terms (Han et al., 2013).", "startOffset": 158, "endOffset": 176}, {"referenceID": 5, "context": "Thousands of crisis-related messages that are posted online contain important information that can also be useful to humanitarian organizations for disaster response efforts, if processed timely and effectively (Hughes and Palen, 2009; Imran et al., 2015).", "startOffset": 211, "endOffset": 255}, {"referenceID": 8, "context": "Thousands of crisis-related messages that are posted online contain important information that can also be useful to humanitarian organizations for disaster response efforts, if processed timely and effectively (Hughes and Palen, 2009; Imran et al., 2015).", "startOffset": 211, "endOffset": 255}, {"referenceID": 2, "context": "Many different types of processing techniques ranging from machine learning to natural language processing to computational linguistics have been developed (Corvey et al., 2010) for different purposes (Imran et al.", "startOffset": 156, "endOffset": 177}, {"referenceID": 9, "context": ", 2010) for different purposes (Imran et al., 2016).", "startOffset": 31, "endOffset": 51}, {"referenceID": 15, "context": "(Temnikova et al., 2015; Olteanu et al., 2015), however, due to the scarcity of relevant data, in particular human-annotated data, crisis informatics researchers still cannot fully utilize the capabilities of different computational methods.", "startOffset": 0, "endOffset": 46}, {"referenceID": 11, "context": "(Temnikova et al., 2015; Olteanu et al., 2015), however, due to the scarcity of relevant data, in particular human-annotated data, crisis informatics researchers still cannot fully utilize the capabilities of different computational methods.", "startOffset": 0, "endOffset": 46}], "year": 2016, "abstractText": "Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.", "creator": "LaTeX with hyperref package"}}}