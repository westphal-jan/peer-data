{"id": "1605.05906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Automatic TM Cleaning through MT and POS Tagging: Autodesk's Submission to the NLP4TM 2016 Shared Task", "abstract": "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish.", "histories": [["v1", "Thu, 19 May 2016 12:05:55 GMT  (18kb,D)", "http://arxiv.org/abs/1605.05906v1", "Presented at the 2nd Workshop on Natural Language Processing for Translation Memories (NLP4TM 2016)"]], "COMMENTS": "Presented at the 2nd Workshop on Natural Language Processing for Translation Memories (NLP4TM 2016)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alena zwahlen", "olivier carnal", "samuel l\\\"aubli"], "accepted": false, "id": "1605.05906"}, "pdf": {"name": "1605.05906.pdf", "metadata": {"source": "CRF", "title": "Automatic TM Cleaning through MT and POS Tagging: Autodesk\u2019s Submission to the NLP4TM 2016 Shared Task", "authors": ["Alena Zwahlen", "Olivier Carnal", "Samuel L\u00e4ubli"], "emails": ["samuel.laubli}@autodesk.com"], "sections": [{"heading": null, "text": "Keywords: Translation Memory, Machine Learning, Machine Translation, Part-of-Speech Tagging"}, {"heading": "1. Introduction", "text": "Autodesk has amassed more than 40 million professionally translated segments (TUs) over the past 17 years, based primarily on user interfaces and documentation of software products localized in 32 languages. As we are now in the process of unifying and centralizing all translations into a single repository, it is high time to sort out duplicate, outdated, and faulty TUs. Exploring ways to deal with the latter - far more difficult than eliminating duplicate and outdated material - motivated us to participate in the first common task of cleaning translation memories (Barbu et al., 2016). In this paper, we describe our submitted system for distinguishing between correct and incorrect translation (by showing translators less faulty, blurred matches) and machine translation more accurately (by reducing noise in training data)."}, {"heading": "2. Background", "text": "In fact, it is a way in which people are able to unfold, and in which they are able to unfold, and in which they are able to unfold, \"he said in an interview with the New York Times."}, {"heading": "3. Method", "text": "Our system uses so-called TUs to train classifiers based on language-independent features (see Section 3.1) with language-specific plug-ins (see Section 3.2). The feature extraction pipeline is implemented in Scala (see Section 3.3), and our final template - which aims to distinguish correct or near-correct TUs from wrong ones - is based on a selection of nine features (see Section 3.4)."}, {"heading": "3.1. Features", "text": "We have reimplemented the 17 features proposed by Barbu (2015, see also section 2). In addition, we examine \u2022 mt _ coverage the percentage of target words contained in the n-best machine translations of the source segment. We use n = 20 in our experiments. \u2022 mt _ cfs the character-based levenshtein spacing between target segment and machine-translated source segment. We normalize this value so that identical and completely dissimilar segments produce results of 1.0 and 0.0, respectively, i.e. cfs = 1 \u2212 levenshtein spacing in characters in a longer segment. This value is calculated individually for each of the 20 best translation options; the best of these values illustrates the feature value. \u2022 mt _ bleu the BLEU value (Papineni et al., 2002) between target segment and machine-translated source segment."}, {"heading": "3.2. Resources", "text": "For machine translation, we use our internal systems (Plitt and Masselot, 2010; Zhechev, 2014), which are based on the Moses SMT Framework (Koehn et al., 2007). They are trained and selected for convenience only for translated software and user manuals of Autodesk products; we expect better performance of our MT-based features in conjunction with MT engines that target the text domains used in this common task (see Table 1). Our engines are integrated into a scalable infrastructure built on an elastic computing cloud, ensuring high throughput even with large translation memorys. For POS tagging, we rely on Schmids (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish to make them comparable to the POS tags used by Petrov in 2011."}, {"heading": "3.3. Classification", "text": "Our feature extraction pipeline, including Barbu's (2015) and our own features (see Section 3.1), is implemented in Scala. This pipeline is used to convert translation units using the scikitlearn framework (Pedregosa et al., 2011) into feature vectors and train classifiers. Of the various classification algorithms we tested, random forests performed best with our selection of features (see below)."}, {"heading": "3.4. Feature Selection", "text": "For the reasons given in Section 1, our goal was to find a combination of features that would do well with all language pairs, rather than solutions on individual1http: / / www.cis.uni-muenchen.de / ~ schmid / tools / TreeTagger / 2https: / / github.com / slavpetrov / universal-pos-tags3https: / / open.xerox.com / Services / LanguageIdentifiedlanguages. We focused on aligning our classifiers so that they can distinguish correct or almost correct (classes 1, 2) from incorrect TUs (classes 3) - i.e. the task of binary classification (II) - by optimizing the weighted F1 score to training data (see Tables 2a and 2b)."}, {"heading": "4. Results", "text": "We tested our final template - a Random Forests classifier based on the nine features described in Section 3.4 - on three language pairs (en-de, en-es, en-it) and two tasks: Binary II and Fine Grained Classification (see Section 4.1 and 4.2, respectively). The classifier was trained solely on data provided by the organizers of this common task for each of the language task conditions. Each TU in this data was assigned one of three terms: correct, almost correct, and wrong (see Table 1)."}, {"heading": "4.1. Binary Classification (II)", "text": "Our reasoning for focusing on distinguishing correct or almost correct TUs from false TUs was that, if successful, an initial application of our method would most likely be filtering TM data for MT training. While eliminating almost correct TUs could reduce rather than increase MT quality, filtering out false segments can have a positive effect (Vogel, 2003). Prior to submission, we compared our system with the two baseline criteria provided by the organizers: a dummy classifier that assigns random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church Gale algorithm that was adapted by Barbu (2015) (Baseline 2). More importantly, however, we compared our system with Barbu's approach (2015) by using the classification algorithms that supposedly best matched the 17 characteristics in its work, barbos performed well on both of our language comparisons, except where our system was paired and where they were."}, {"heading": "4.2. Fine-Grained Classification", "text": "Although we focused on the task of binary classification (II) (see above), we also examined our system on the task of fine-grained classification, with the aim of distinguishing between all three classes, i.e. determining whether a TU is correct, nearly correct or incorrect. Here, too, we compared the performance of our system with the Barbu method (2015), using 2 / 3-1 / 3 splits of training data (5-fold cross-validation).The results presented in Table 2b implied that the nine characteristics we selected would not be sufficient for a finer-grained classification of TU. This was confirmed in the official evaluation and ranking: our system performed poorly on en-de and moderately on en-it. Further work is needed to analyze these results in more detail."}, {"heading": "5. Conclusions", "text": "We have proposed a machine learning-based method for identifying incorrect entries in translation memories, applicable to any language pair for which an MT system, a POS tagger and a language identifier are available. In the Binary Classification (II) task, our system achieved the best classification results for two out of three language pairs (English-Italian and English-Spanish). In future work, we would like to evaluate the impact of the interlocking of NLP components on the target domains on classification accuracy. Training data in this joint task comes from news (German) and medical texts (Italian, Spanish) for which our MT systems have not been optimized, for example. This discrepancy in this domain may partially explain why our system does not perform well in the English-German test. More importantly, however, we want to test our implementation as it is for software localization in Autodesk's production environments."}, {"heading": "6. Acknowledgements", "text": "We would like to thank Val\u00e9ry Jacot for his important support and guidance."}, {"heading": "7. Bibliographical References", "text": "Barbu, E. (2015). Spotting false translation segments inlation memories. In Proceedings of the Workshop on Natural Language Processing for Translation Memories (NLP4TM), pages 9-16, Hissar, Bulgaria.Barbu, E., Escart\u00edn, C. P., Bentivogli, L., Negri, M., Turchi, M., Federico, M., Mastrostefano, L., and Orasan, C. (2016). 1st shared task on automatic translation memory cleaning preparation and lessons learned. In Proceedingsof the 2nd Workshop on Natural Language Processing for Translation Memories (NLP4TM), Portoro\u017e, Slovenia.Green, S., Cer, D., and Manning, C. D. (2014). Phrasal: A toolkit for new directions in statistical machine translation."}], "references": [{"title": "Spotting false translation segments in translation memories", "author": ["E. Barbu"], "venue": "Proceedings of the Workshop on Natural Language Processing for Translation Memories (NLP4TM), pages 9\u201316, Hissar, Bulgaria.", "citeRegEx": "Barbu,? 2015", "shortCiteRegEx": "Barbu", "year": 2015}, {"title": "1st shared task on automatic translation memory cleaning preparation and lessons learned", "author": ["E. Barbu", "C.P. Escart\u00edn", "L. Bentivogli", "M. Negri", "M. Turchi", "M. Federico", "L. Mastrostefano", "C. Orasan"], "venue": "Proceedings", "citeRegEx": "Barbu et al\\.,? 2016", "shortCiteRegEx": "Barbu et al\\.", "year": 2016}, {"title": "Phrasal: A toolkit for new directions in statistical machine translation", "author": ["S. Green", "D. Cer", "C.D. Manning"], "venue": "Proceedings of the 9th Workshop on Statistical Machine Translation (WMT), pages 114\u2013121, Baltimore, USA.", "citeRegEx": "Green et al\\.,? 2014", "shortCiteRegEx": "Green et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311\u2013318, Philadelphia, Pennsylvania.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Jour-", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint, arXiv:1104.2086.", "citeRegEx": "Petrov et al\\.,? 2011", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "A productivity test of statistical machine translation post-editing in a typical localisation context", "author": ["M. Plitt", "F. Masselot"], "venue": "Prague Bulletin of Mathematical Linguistics, 93:7\u201316.", "citeRegEx": "Plitt and Masselot,? 2010", "shortCiteRegEx": "Plitt and Masselot", "year": 2010}, {"title": "Improvements in part-of-speech tagging with an application to German", "author": ["H. Schmid"], "venue": "Proceedings of the ACL SIGDAT Workshop, pages 47\u201350, Dublin, Ireland.", "citeRegEx": "Schmid,? 1995", "shortCiteRegEx": "Schmid", "year": 1995}, {"title": "Parallel data, tools and interfaces in OPUS", "author": ["J. Tiedemann"], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 2214\u20132218, Istanbul, Turkey.", "citeRegEx": "Tiedemann,? 2012", "shortCiteRegEx": "Tiedemann", "year": 2012}, {"title": "Creating the world\u2019s largest translation memory", "author": ["M. Trombetti"], "venue": "Proceedings of the 12th Machine Translation Summit (MT Summit), Ottawa, Canada.", "citeRegEx": "Trombetti,? 2009", "shortCiteRegEx": "Trombetti", "year": 2009}, {"title": "Using noisy biligual data for statistical machine translation", "author": ["S. Vogel"], "venue": "Proceedings of Meeting of the 10th Conference of the European Chapter of the Association of Computational Linguistics (EACL), pages 175\u2013 178, Budapest, Hungary.", "citeRegEx": "Vogel,? 2003", "shortCiteRegEx": "Vogel", "year": 2003}, {"title": "Analysing the post-editing of machine translation at Autodesk", "author": ["V. Zhechev"], "venue": "O\u2019Brian, S., Balling, L. W., Carl, M., Simard, M., and Specia, L., editors, Post-editing of Machine Translation: Processes and Applications, pages 2\u201323. Cambridge Scholars Publishing.", "citeRegEx": "Zhechev,? 2014", "shortCiteRegEx": "Zhechev", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features.", "startOffset": 28, "endOffset": 41}, {"referenceID": 1, "context": "Exploring methods to handle the latter \u2013 clearly more challenging than removing duplicate and outdated material \u2013 motivated us to participate in the First Shared Task on Translation Memory Cleaning (Barbu et al., 2016).", "startOffset": 198, "endOffset": 218}, {"referenceID": 0, "context": "The system is based on previous work by Barbu (2015) and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.", "startOffset": 40, "endOffset": 53}, {"referenceID": 0, "context": "In Section 3, we describe our method and, in Section 4, show how it compares to Barbu\u2019s (2015) approach as well as other submissions to this shared task.", "startOffset": 80, "endOffset": 95}, {"referenceID": 9, "context": "With crowd-sourced and automatically constructed TMs in particular, it is also necessary to identify translation units with source and target segments that do not correspond at all (e.g., Trombetti, 2009; Tiedemann, 2012).", "startOffset": 181, "endOffset": 221}, {"referenceID": 0, "context": "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \u201ca neglected research area\u201d (Barbu, 2015).", "startOffset": 185, "endOffset": 198}, {"referenceID": 4, "context": "We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.", "startOffset": 150, "endOffset": 173}, {"referenceID": 0, "context": "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu\u2019s (2015) experiments.", "startOffset": 120, "endOffset": 135}, {"referenceID": 6, "context": "eleven coarse-grained, language-independent grammatical groups (Petrov et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 4, "context": "\u2022 mt_bleu the BLEU score (Papineni et al., 2002) between target segment and machine translated source segment.", "startOffset": 25, "endOffset": 48}, {"referenceID": 2, "context": "We employ the sentence-level version of the metric as implemented in Phrasal (Green et al., 2014).", "startOffset": 77, "endOffset": 97}, {"referenceID": 7, "context": "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al.", "startOffset": 53, "endOffset": 94}, {"referenceID": 12, "context": "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al.", "startOffset": 53, "endOffset": 94}, {"referenceID": 3, "context": "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007).", "startOffset": 128, "endOffset": 148}, {"referenceID": 3, "context": "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007). They are trained on translated software and user manuals from Autodesk products only and chosen for the sake of convenience; we would expect better performance of our MT-based features in conjunction with MT engines geared to the text domains used in this shared task (listed in Table 1). Our engines are integrated into a scalable infrastructure deployed on an elastic compute cloud, allowing high throughput even with large translation memories to be cleaned. For POS tagging, we rely on Schmid\u2019s (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish.", "startOffset": 129, "endOffset": 656}, {"referenceID": 3, "context": "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007). They are trained on translated software and user manuals from Autodesk products only and chosen for the sake of convenience; we would expect better performance of our MT-based features in conjunction with MT engines geared to the text domains used in this shared task (listed in Table 1). Our engines are integrated into a scalable infrastructure deployed on an elastic compute cloud, allowing high throughput even with large translation memories to be cleaned. For POS tagging, we rely on Schmid\u2019s (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish. To make POS tags comparable across these languages, they are mapped2 to the Universal Tagset proposed by Petrov et al. (2011). Lastly, we use the publicly available Xerox Language Identifier API3 for language detection.", "startOffset": 129, "endOffset": 870}, {"referenceID": 5, "context": "This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011).", "startOffset": 128, "endOffset": 152}, {"referenceID": 0, "context": "Our feature extraction pipeline, including Barbu\u2019s (2015) as well as our own features (see Section 3.", "startOffset": 43, "endOffset": 58}, {"referenceID": 0, "context": "74 Barbu (2015) SVM (linear kernel) .", "startOffset": 3, "endOffset": 16}, {"referenceID": 0, "context": "74 Barbu (2015) SVM (linear kernel) .74 .85 .78 .84 .85 .83 .83 .83 .79 Barbu (2015) Random Forests .", "startOffset": 3, "endOffset": 85}, {"referenceID": 11, "context": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003).", "startOffset": 145, "endOffset": 158}, {"referenceID": 0, "context": "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2).", "startOffset": 290, "endOffset": 303}, {"referenceID": 0, "context": "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2). More importantly, however, we compared our system to Barbu\u2019s (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work.", "startOffset": 290, "endOffset": 385}, {"referenceID": 0, "context": "Again, we compared our system\u2019s performance to Barbu\u2019s (2015) method, using 2/3\u20131/3 splits of the training data (5fold cross-validation).", "startOffset": 47, "endOffset": 62}], "year": 2016, "abstractText": "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English\u2013Italian and English\u2013Spanish.", "creator": "LaTeX with hyperref package"}}}