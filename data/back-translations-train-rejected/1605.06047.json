{"id": "1605.06047", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization", "abstract": "Self-Organizing Map (SOM) is a neural network model which is used to obtain a topology-preserving mapping from the (usually high dimensional) input/feature space to an output/map space of fewer dimensions (usually two or three in order to facilitate visualization). Neurons in the output space are connected with each other but this structure remains fixed throughout training and learning is achieved through the updating of neuron reference vectors in feature space. Despite the fact that growing variants of SOM overcome the fixed structure limitation they increase computational cost and also do not allow the removal of a neuron after its introduction. In this paper, a variant of SOM is proposed called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates a more flexible structure where neuron positions are dynamically altered during training and on the other hand tackles the drawback of having a predefined grid by allowing neuron addition and/or removal during training. Experiments using multiple literature datasets show that the proposed method improves training performance of SOM, leads to a better visualization of the input dataset and provides a framework for determining the optimal number and structure of neurons.", "histories": [["v1", "Thu, 19 May 2016 16:41:00 GMT  (1370kb,D)", "http://arxiv.org/abs/1605.06047v1", "ICAART 2016 accepted full paper"]], "COMMENTS": "ICAART 2016 accepted full paper", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["gerasimos spanakis", "gerhard weiss"], "accepted": false, "id": "1605.06047"}, "pdf": {"name": "1605.06047.pdf", "metadata": {"source": "CRF", "title": "AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization", "authors": ["Gerasimos Spanakis", "Gerhard Weiss"], "emails": ["gerhard.weiss}@maastrichtuniversity.nl"], "sections": [{"heading": "1 INTRODUCTION", "text": "The Self-Organizing Map (SOM) (Kohonen, 2001) is an unattended neural network model that effectively maps high-dimensional data into a low-dimensional space (usually two-dimensional).Low-dimensional space (also called output space) consists of a network of neurons connected to each other according to a specific structure (can be hexagonal, rectangular, etc.).This structure enables the conservation of input data (i.e. similar input patterns are expected to be mapped to neighboring neurons in the output grid).In this way, SOM succeeds in reducing dimensionality, achieving abstraction and visualization of input data, and this is why it is successfully applied to many different domains and datasets such as financial data (Deboeck and Kohonen, 2013) to obtain image speech recognition."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 SOM and Competitive Learning", "text": "The Self-Organizing Map (SOM) is a fully connected single-layer linear neural network. The SOM uses a series of neurons often arranged in a rectangular or hexagonal 2-D grid to form a discrete topological mapping of an entrance space, X-RD. The entrance space consists of a series of vectors x-j-RD: x j = [x j1, x j2,..., x jD] T (1) wi is the weight vector associated with neuron i and is a vector of the same dimension (D) of the entrance space, M is the total number of neurons. Obviously, these weights represent the synaptic connections of each neuron i and can be designated: wi = [wi1, wi2,..., wiD] T (2) The basic principle of the SOM is the soft competition between the nodes in the output layer; not only the nodes are found (the winners), but also the numbers are updated (the neighbors)."}, {"heading": "2.1.1 On-line Training of SOM", "text": "In the traditional \"on-line\" or \"flow-through\" method, the weight vectors are updated recursively after the presentation of each input vector. In the presentation of each input vector, the Euclidean distance between the input vector and each weight vector is calculated: di (t) = | x (t) \u2212 wi (t) | | 2 (3) Next, the winning or most suitable node (designated by subscript c) is suppressed by: c = {i, minidi (t)} (4) Note that we suppress the implicit dependence of c on discrete time t. Weight vectors are updated using the following rule: wi (t + 1) = wi (t) + \u03b1 (t) \u00b7 hci (t) \u2212 wi (t) \u2212 wi (t)]]] (5) where \u03b1 (t) is the learning rate factor and hci (t) is the neighborhood function."}, {"heading": "2.1.2 Batch Training of SOM", "text": "The SOM update of Equation (5) is \"on-line\" in the sense that the weight vectors are updated after the presentation of the individual input data. (5) The SOM update of Equation (5) is \"on-line\" in the sense that the weight vectors are updated after the presentation of the individual input data. (5) The SOM update of Equation (5) is \"on-line\" in the sense that the weight vectors are only updated at the end of each epoch. (7) In which the weight vectors are only at the beginning and end of each epoch. (5) The weight vectors are accumulated at the end of the current epoch. (6) The sums are accumulated during a complete transition over the input data. The winning node for each presentation of new input vectors is calculated using the following methods: d (t) \u2212 wi \u2212 wi \u2212 wi the weight vectors are accumulated at the end of the current epoch."}, {"heading": "2.2 Flexible Structure in Neural Networks and SOM", "text": "However, there are cases where the structural design of the network is strongly influenced by the environment and where constructive and truncated algorithms are used. (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012), (Yang and Chen, 2012) There are many approaches that apply these algorithms in the classical neural networks. (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012)."}, {"heading": "3 EXPANDING THE IDEA OF", "text": "SELF-STORGANISATION IN NEURON LOCATION During the classical SOM algorithm, the positions of the neurons remain unchanged and the network is fixed from the beginning to the end of the training, facilitating the learning process (since the neighbourhood structure is known in advance), but limiting the final result and the possibilities of its visualisation. AMSOM proposes a different and more flexible scheme with respect to the position vectors ri of neurons, which allows a more adaptable form of the neuron grid and functions as an extension of the batch learning algorithm. Starting from an already grown map size, AMSOM can adjust both its size and its structure to better represent the data at a certain level of detail. After a certain number of steps, neurons are analysed to see if the display level is sufficient or if adjustments are required: distance and / or addition of neurons. Initially, connections between neurons are determined on the basis of the network structure, but these may also change in the course of the training, as well as in detail."}, {"heading": "3.1 Phase I: AMSOM Initialization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Grid Structure and Size.", "text": "The first step of the AMSOM algorithm is to define the initial grid structure (like the classical SOM), which facilitates training time as opposed to a small-scale structure and builds on it as other approaches do (Vesanto et al., 2000), and is also in line with neuronal development, which suggests that almost all neuronal cells used in the course of human life were produced in the first months of life (Dowling, 2007). This overproduction of neurons is considered a competitive strategy for building efficient connectivity (Changeux and Danchin, 1976), in which case the initial structure of SOM is determined. Several empirical rules (Park et al., 2006) suggest that the number of neurons should be 5 \u00b7 \u221a N, with N being the number of patterns in the dataset. In this case, the two largest eigenvalues of the training data are first calculated, then the ratio between the lateral lengths of the grid and the ratio between the maximum eigenvalues is set."}, {"heading": "3.1.2 Vector, Matrix and Parameters Initialization.", "text": "For each neuron, the following are defined and initialized accordingly: \u2022 Neuron vector (weight vector, wi): It is the same as the classical SOM (see Equation (2) and shows the representation of the neuron in the characteristic (in-put) space. The initialization of neuron vectors is random according to literature standards. \u2022 Neuron position (position vector, ri): Depending on the initial space (usually it is two-dimensional), it is a vector that indicates the position of the neuron. Initial position vectors are equal to the positions of the neurons in the network, i.e., in Figure 4 you can see the coordinates of the neurons according to structure (hexagonal or rectal). Since the structure of the network is subject to changes during training, we must keep an eye on the neighbors of the neurons."}, {"heading": "3.2 Phase II: Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Weight and Position Updating", "text": "To learn the weight of neurons, the SOM batch algorithm is used, as it was given in the equations 7- 10, which are repeated here for clarification. (t + 1) = jj (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x x (t) x x (t) x (t) x x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x x (t) x (t) x x (t) x (t) x x x (t) x x x (t) x (t) x x (t) x x x (t) x (t) x (t) x (t) x x (t) x (t) x (t) x x (t) x (t) x (t) x x x (t) x (t) x (t) x x x (t) x (t) x (t) x (t) x x (t) x (t) x x (t) x x x (t) x x x (t) x (t) x (t) x x (t) x (t) x x (t) x (t) x x (t (t) x x (t) x (t) x (t (t) x (t) x (t) x (t (t) x (t) x x x x (t) x (t) x x (t (t) x (t) x (t) x (t (t) x (t) x (t) x (t) x (t (t) x (t) x (t) x (t (t) x (t) x x"}, {"heading": "3.2.2 Adding and Removing Neurons", "text": "This year, the number of new arrivals has tripled compared to the previous year, tripling the number of arrivals in recent years, while the number of new arrivals has increased by 20% compared to previous years."}, {"heading": "3.2.3 Architecture Adaptation and Termination Criterion", "text": "As already described, the initial structure of AMSOM is adapted through learning and training to find out what is optimal for the number of neurons, their weights and their connections, and the adaptation process begins with training the initial structure of AMSOM. If the criteria for adding or removing neurons are met, the network is adapted. In order to maintain (as possible) the initial structure (i.e., a rectangular or hexagonal grid or another selected grid), after this adaptation process, we re-evaluate all connections of all neurons and ensure that each neuron has at most Q neighbours (with Q being decided at the beginning, i.e. in the case of a rectangular grid Q = 4): this can be ensured by checking the edge matrix E after each epoch, and if a neuron has more than Q connections, then only the Q \"youngest\" connections are maintained (using the age of the edges in matrix A)."}, {"heading": "3.3 Phase III: AMSOM Finalization", "text": "The final phase of AMSOM occurs when learning is complete and the structure of the network no longer changes. In this phase, no neurons are added or removed and no connections between neurons are added or removed, but adjustment to weight and position vectors is continued at a slower rate. The purpose of this process is to compensate for possible quantization errors and fine-tuning of neuron weights and positions, especially for neurons added in recent epochs. To this end, the neighborhood function (for both equations 11 and 13) is limited to the immediate neighborhood and the learning rate \u03b1 (t) in Equation (13) is set to 0.001 (even smaller than in Phase II). Phase III is completed when there is no significant change in mean quantization errors (i.e., when mqe (t) \u2212 mqe (t \u2212 1) < \u03b52), whereby \u03b52 is set to a value smaller than \u03b51 (such as 1E \u2212 10)."}, {"heading": "4 EXPERIMENTS", "text": "The results of the study show that the number of data collected from the UCI repository 1, apart from the CLUSTER dataset, which is a simple and random but large two-dimensional dataset with four groups, is effective enough to read (Bauer et al., 1999). All datasets used with their characteristics are presented in the table. Each dataset is shuffled and split to training, testing and validation set (60%, 20% and 20% respectively). Each experiment has been performed 20 times and the results presented here are average over these runs (deviations were small and are not presented here)."}, {"heading": "5 CONCLUSION", "text": "In this paper, we introduced AMSOM, an extension of the original SOM algorithm that allows neurons to change positions according to a similar competitive technology used in classical SOM training. In addition, neurons can be added or removed during this \"duplicate\" training process, enabling a more flexible structural grid that can render the data set more efficiently. Experimental results on different data sets improve the performance of AMSOM compared to the classical SOM algorithm. AMSOM produces better reference vectors by reducing the quantization error, the topology is obtained by the movement of the neurons by significantly reducing the topographic error, and the visualization result is as close as possible to the original data set partitions. In addition, AMSOM produces fewer nodes without significant effect, while simultaneously reducing the required number of epochs. The results obtained provide new insights on how the concept of competitive textual learning and the ability to adapt to the larger data sets can be further used for the purpose of learning and self-behavior in networks."}], "references": [{"title": "Dynamic self-organizing maps with controlled growth for knowledge discovery", "author": ["D. Alahakoon", "S.K. Halgamuge", "B. Srinivasan"], "venue": "Neural Networks, IEEE Transactions on, 11(3):601\u2013 614.", "citeRegEx": "Alahakoon et al\\.,? 2000", "shortCiteRegEx": "Alahakoon et al\\.", "year": 2000}, {"title": "MIGSOM: multilevel interior growing self-organizing maps for high dimensional data clustering", "author": ["T. Ayadi", "T.M. Hamdani", "A.M. Alimi"], "venue": "Neural processing letters, 36(3):235\u2013 256.", "citeRegEx": "Ayadi et al\\.,? 2012", "shortCiteRegEx": "Ayadi et al\\.", "year": 2012}, {"title": "Neural maps and topographic vector quantization", "author": ["Bauer", "H.-U.", "M. Herrmann", "T. Villmann"], "venue": "Neural networks, 12(4):659\u2013676.", "citeRegEx": "Bauer et al\\.,? 1999", "shortCiteRegEx": "Bauer et al\\.", "year": 1999}, {"title": "Incremental grid growing: encoding highdimensional structure into a two-dimensional", "author": ["J. Blackmore", "R. Miikkulainen"], "venue": null, "citeRegEx": "Blackmore and Miikkulainen,? \\Q1993\\E", "shortCiteRegEx": "Blackmore and Miikkulainen", "year": 1993}, {"title": "A growing and pruning method for radial basis function networks", "author": ["M. Bortman", "M. Aladjem"], "venue": "Neural Networks, IEEE Transactions on, 20(6):1039\u20131045.", "citeRegEx": "Bortman and Aladjem,? 2009", "shortCiteRegEx": "Bortman and Aladjem", "year": 2009}, {"title": "Competitive neural networks on message-passing parallel computers", "author": ["M. Ceccarelli", "A. Petrosino", "R. Vaccaro"], "venue": "Concurrency: Practice and Experience, 5(6):449\u2013470.", "citeRegEx": "Ceccarelli et al\\.,? 1993", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 1993}, {"title": "Selective stabilisation of developing synapses as a mechanism for the specification of neuronal networks", "author": ["Changeux", "J.-P.", "A. Danchin"], "venue": "Nature, 264(5588):705\u2013712.", "citeRegEx": "Changeux et al\\.,? 1976", "shortCiteRegEx": "Changeux et al\\.", "year": 1976}, {"title": "Visual explorations in finance: with self-organizing maps", "author": ["G. Deboeck", "T. Kohonen"], "venue": "Springer Science & Business Media.", "citeRegEx": "Deboeck and Kohonen,? 2013", "shortCiteRegEx": "Deboeck and Kohonen", "year": 2013}, {"title": "TreeGNG-hierarchical topological clustering", "author": ["K. Doherty", "R. Adams", "N. Davey"], "venue": "ESANN, pages 19\u201324.", "citeRegEx": "Doherty et al\\.,? 2005", "shortCiteRegEx": "Doherty et al\\.", "year": 2005}, {"title": "The Great Brain Debate: Nature Or Nurture", "author": ["J.E. Dowling"], "venue": null, "citeRegEx": "Dowling,? \\Q2007\\E", "shortCiteRegEx": "Dowling", "year": 2007}, {"title": "Advances in Self-Organizing Maps: 9th International Workshop, WSOM 2012 Santiago, Chile, December 12-14, 2012 Proceedings", "author": ["P.A. Est\u00e9vez", "J.C. Pr\u0131\u0301ncipe", "P. Zegers"], "venue": null, "citeRegEx": "Est\u00e9vez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Est\u00e9vez et al\\.", "year": 2012}, {"title": "Soms mathematics", "author": ["Fort", "J.-C."], "venue": "Neural Networks, 19(6):812\u2013816.", "citeRegEx": "Fort and J..C.,? 2006", "shortCiteRegEx": "Fort and J..C.", "year": 2006}, {"title": "Growing cell structuresa selforganizing network for unsupervised and supervised learning", "author": ["B. Fritzke"], "venue": "Neural networks, 7(9):1441\u2013 1460.", "citeRegEx": "Fritzke,? 1994", "shortCiteRegEx": "Fritzke", "year": 1994}, {"title": "Growing grida self-organizing network with constant neighborhood range and adaptation strength", "author": ["B. Fritzke"], "venue": "Neural Processing Letters, 2(5):9\u201313.", "citeRegEx": "Fritzke,? 1995", "shortCiteRegEx": "Fritzke", "year": 1995}, {"title": "A growing neural gas network learns topologies", "author": ["B Fritzke"], "venue": "Advances in neural information processing systems, 7:625\u2013632.", "citeRegEx": "Fritzke,? 1995", "shortCiteRegEx": "Fritzke", "year": 1995}, {"title": "A structure optimisation algorithm for feedforward neural network construction", "author": ["Han", "H.-G.", "Qiao", "J.-F."], "venue": "Neurocomputing, 99:347\u2013357.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Hierarchical growing cell structures: TreeGCS", "author": ["V.J. Hodge", "J. Austin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 13(2):207\u2013218.", "citeRegEx": "Hodge and Austin,? 2001", "shortCiteRegEx": "Hodge and Austin", "year": 2001}, {"title": "A new adaptive merging and growing algorithm for designing artificial neural networks", "author": ["M. Islam", "A. Sattar", "F. Amin", "X. Yao", "K. Murase"], "venue": "Systems, Man, and Cybernetics, Part B:", "citeRegEx": "Islam et al\\.,? 2009", "shortCiteRegEx": "Islam et al\\.", "year": 2009}, {"title": "Automatic formation of topological maps of patterns in a self-organizing system", "author": ["T. Kohonen"], "venue": null, "citeRegEx": "Kohonen,? \\Q1981\\E", "shortCiteRegEx": "Kohonen", "year": 1981}, {"title": "The\u2019neural\u2019phonetic typewriter", "author": ["T. Kohonen"], "venue": "Computer, 21(3):11\u201322.", "citeRegEx": "Kohonen,? 1988", "shortCiteRegEx": "Kohonen", "year": 1988}, {"title": "Things you haven\u2019t heard about the Self-Organizing Map", "author": ["T. Kohonen"], "venue": "Neural Networks, 1993., IEEE International Conference on, pages 1147\u20131156. IEEE.", "citeRegEx": "Kohonen,? 1993", "shortCiteRegEx": "Kohonen", "year": 1993}, {"title": "Self-organizing Maps, vol", "author": ["T. Kohonen"], "venue": "30 of Springer Series in Information Sciences. Springer Berlin.", "citeRegEx": "Kohonen,? 2001", "shortCiteRegEx": "Kohonen", "year": 2001}, {"title": "Self-organization and associative memory, volume 8", "author": ["T. Kohonen"], "venue": "Springer.", "citeRegEx": "Kohonen,? 2012", "shortCiteRegEx": "Kohonen", "year": 2012}, {"title": "WEBSOM for textual data mining", "author": ["K. Lagus", "T. Honkela", "S. Kaski", "T. Kohonen"], "venue": "Artificial Intelligence Review, 13(5-6):345\u2013364.", "citeRegEx": "Lagus et al\\.,? 1999", "shortCiteRegEx": "Lagus et al\\.", "year": 1999}, {"title": "Pattern classification using selforganizing feature maps", "author": ["Lu", "S.-y."], "venue": "1990 IJCNN International Joint Conference on, pages 471\u2013480.", "citeRegEx": "Lu and S..y.,? 1990", "shortCiteRegEx": "Lu and S..y.", "year": 1990}, {"title": "A self-organising network that grows when required", "author": ["S. Marsland", "J. Shapiro", "U. Nehmzow"], "venue": "Neural Networks, 15(8):1041\u20131058.", "citeRegEx": "Marsland et al\\.,? 2002", "shortCiteRegEx": "Marsland et al\\.", "year": 2002}, {"title": "Learning rate schedules for self-organizing maps", "author": ["F. Mulier", "V. Cherkassky"], "venue": "Pattern Recognition, 1994. Vol. 2-Conference B: Computer Vision &amp; Image Processing., Proceedings of the 12th IAPR International. Con-", "citeRegEx": "Mulier and Cherkassky,? 1994", "shortCiteRegEx": "Mulier and Cherkassky", "year": 1994}, {"title": "Evolutional development of a multilevel neural network", "author": ["S.V. Odri", "D.P. Petrovacki", "G.A. Krstonosic"], "venue": "Neural Networks, 6(4):583\u2013595.", "citeRegEx": "Odri et al\\.,? 1993", "shortCiteRegEx": "Odri et al\\.", "year": 1993}, {"title": "Application of a selforganizing map to select representative species in multivariate analysis: A case study determining diatom distribution patterns across France", "author": ["Park", "Y.-S", "J. Tison", "S. Lek", "Giraudel", "J.-L", "M. Coste", "F. Delmas"], "venue": null, "citeRegEx": "Park et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Park et al\\.", "year": 2006}, {"title": "The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data", "author": ["A. Rauber", "D. Merkl", "M. Dittenbach"], "venue": "Neural Networks, IEEE Transactions on, 13(6):1331\u20131341.", "citeRegEx": "Rauber et al\\.,? 2002", "shortCiteRegEx": "Rauber et al\\.", "year": 2002}, {"title": "DoSO: a document self-organizer", "author": ["G. Spanakis", "G. Siolas", "A. Stafylopatis"], "venue": "Journal of Intelligent Information Systems, 39(3):577\u2013610.", "citeRegEx": "Spanakis et al\\.,? 2012", "shortCiteRegEx": "Spanakis et al\\.", "year": 2012}, {"title": "SOM toolbox for Matlab 5", "author": ["J. Vesanto", "J. Himberg", "E. Alhoniemi", "J. Parhankangas"], "venue": "Citeseer.", "citeRegEx": "Vesanto et al\\.,? 2000", "shortCiteRegEx": "Vesanto et al\\.", "year": 2000}, {"title": "The geometric structure of the brain fiber pathways", "author": ["V.J. Wedeen", "D.L. Rosene", "R. Wang", "G. Dai", "F. Mortazavi", "P. Hagmann", "J.H. Kaas", "Tseng", "W.-Y.I."], "venue": "Science, 335(6076):1628\u2013 1634.", "citeRegEx": "Wedeen et al\\.,? 2012", "shortCiteRegEx": "Wedeen et al\\.", "year": 2012}, {"title": "An evolutionary constructive and pruning algorithm for artificial neural networks and its prediction applications", "author": ["Yang", "S.-H.", "Chen", "Y.-P."], "venue": "Neurocomputing, 86:140\u2013149.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "The Self-Organizing Map (SOM) (Kohonen, 2001) is an unsupervised neural network model which effectively maps high-dimensional data to a lowdimensional space (usually two-dimensional).", "startOffset": 30, "endOffset": 45}, {"referenceID": 18, "context": ", similar input patterns are expected to be mapped to neighboring neurons in the output grid) (Kohonen, 1981).", "startOffset": 94, "endOffset": 109}, {"referenceID": 7, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al.", "startOffset": 229, "endOffset": 256}, {"referenceID": 19, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al.", "startOffset": 277, "endOffset": 292}, {"referenceID": 23, "context": "SOM manages to achieve dimensionality reduction, abstraction, clustering and visualization of the input data and this is the reason that it has been applied successfully to many different domains and datasets like financial data (Deboeck and Kohonen, 2013), speech recognition (Kohonen, 1988), image classification (Lu, 1990), document clustering (Lagus et al., 1999), (Spanakis et al.", "startOffset": 347, "endOffset": 367}, {"referenceID": 30, "context": ", 1999), (Spanakis et al., 2012) .", "startOffset": 9, "endOffset": 32}, {"referenceID": 22, "context": "The fundamental principle of SOM is the soft competition between the nodes in the output layer; not only the node (winner) but also its neighbors are updated (Kohonen, 2012).", "startOffset": 158, "endOffset": 173}, {"referenceID": 20, "context": "In the batch SOM algorithm (proposed in (Kohonen, 1993)), the weights", "startOffset": 40, "endOffset": 55}, {"referenceID": 26, "context": "In addition to facilitating the development of data-partitioned parallel methods, this also eliminates concerns (Mulier and Cherkassky, 1994) that input records encountered later in the training sequence may overly influence the final results.", "startOffset": 112, "endOffset": 141}, {"referenceID": 5, "context": "The learning rate parameter \u03b1(t) does not appear in the batch SOM algorithm, thus eliminating a potential source of poor convergence (Ceccarelli et al., 1993) if this parameter is not properly specified.", "startOffset": 133, "endOffset": 158}, {"referenceID": 17, "context": "There are many approaches which apply these algorithms in classic neural networks (Islam et al., 2009), (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012).", "startOffset": 82, "endOffset": 102}, {"referenceID": 4, "context": ", 2009), (Bortman and Aladjem, 2009), (Han and Qiao, 2013), (Yang and Chen, 2012).", "startOffset": 9, "endOffset": 36}, {"referenceID": 13, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al.", "startOffset": 184, "endOffset": 199}, {"referenceID": 3, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al.", "startOffset": 216, "endOffset": 250}, {"referenceID": 0, "context": "Also, there are many variations of SOM that allow a more flexible structure of the output map which can be divided into two categories: In the first type, we include growing grid (GG) (Fritzke, 1995), incremental GG (Blackmore and Miikkulainen, 1993), growing SOM (GSOM) (Alahakoon et al., 2000) all coming with different variants.", "startOffset": 271, "endOffset": 295}, {"referenceID": 1, "context": "MIGSOM (Ayadi et al., 2012) allows a more flexible structure by adding neurons internally and from the", "startOffset": 7, "endOffset": 27}, {"referenceID": 12, "context": "We denote growing cell structures (GCSs) (Fritzke, 1994), growing neural gas (GNG) (Fritzke et al.", "startOffset": 41, "endOffset": 56}, {"referenceID": 25, "context": ", 1995) and growing where required (Marsland et al., 2002).", "startOffset": 35, "endOffset": 58}, {"referenceID": 29, "context": "Limitations in growing and visualization led to hierarchical variants of the previous model like the Growing Hierarchical SOM (GHSOM) (Rauber et al., 2002).", "startOffset": 134, "endOffset": 155}, {"referenceID": 8, "context": "Other approaches (like TreeGNG (Doherty et al., 2005) or TreeGCS (Hodge and Austin, 2001)) use dendrograms for representation but due to this tree structure they lose the topological properties.", "startOffset": 31, "endOffset": 53}, {"referenceID": 16, "context": ", 2005) or TreeGCS (Hodge and Austin, 2001)) use dendrograms for representation but due to this tree structure they lose the topological properties.", "startOffset": 19, "endOffset": 43}, {"referenceID": 31, "context": "This process facilitates training time in contrast to starting from a small-size structure and building on that as other approaches do (Vesanto et al., 2000).", "startOffset": 135, "endOffset": 157}, {"referenceID": 9, "context": "It is also in agreement with the neural development which suggests that nearly all neural cells used through human lifetime have been produced in the first months of life (Dowling, 2007).", "startOffset": 171, "endOffset": 186}, {"referenceID": 28, "context": "Several empirical rules (Park et al., 2006) suggest that the number of neurons should be 5 \u00b7 \u221a N", "startOffset": 24, "endOffset": 43}, {"referenceID": 10, "context": "The eigenvalues ratio shows how well the data is flattened and elongated (Est\u00e9vez et al., 2012).", "startOffset": 73, "endOffset": 95}, {"referenceID": 0, "context": "Formula used is GT = \u2212ln(D)\u00d7 ln(SF) (from (Alahakoon et al., 2000)).", "startOffset": 42, "endOffset": 66}, {"referenceID": 27, "context": "Regarding the new neuron that will be added, we follow the the biological process of \u2018cell division\u2019 (Odri et al., 1993).", "startOffset": 101, "endOffset": 120}, {"referenceID": 27, "context": "where wu refers to the weight vector of neuron u (neuron that is splitted) and \u03b2 is a mutation parameter which can take either a fixed or random value according to a certain distribution rule (following (Odri et al., 1993)).", "startOffset": 203, "endOffset": 222}, {"referenceID": 32, "context": "vision- or hearing-impaired subjects (Wedeen et al., 2012).", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Quantization Error (QE) and Topographic Error (TE) were used as intrinsic measures of evaluation (for more details readers are encouraged to read (Bauer et al., 1999)).", "startOffset": 146, "endOffset": 166}], "year": 2016, "abstractText": "Self-Organizing Map (SOM) is a neural network model which is used to obtain a topology-preserving mapping from the (usually high dimensional) input/feature space to an output/map space of fewer dimensions (usually two or three in order to facilitate visualization). Neurons in the output space are connected with each other but this structure remains fixed throughout training and learning is achieved through the updating of neuron reference vectors in feature space. Despite the fact that growing variants of SOM overcome the fixed structure limitation they increase computational cost and also do not allow the removal of a neuron after its introduction. In this paper, a variant of SOM is proposed called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates a more flexible structure where neuron positions are dynamically altered during training and on the other hand tackles the drawback of having a predefined grid by allowing neuron addition and/or removal during training. Experiments using multiple literature datasets show that the proposed method improves training performance of SOM, leads to a better visualization of the input dataset and provides a framework for determining the optimal number and structure of neurons.", "creator": "TeX"}}}