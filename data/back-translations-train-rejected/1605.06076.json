{"id": "1605.06076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "On a convergent off -policy temporal difference learning algorithm in on-line learning environment", "abstract": "In this paper we provide a rigorous convergence analysis of a \"off\"-policy temporal difference learning algorithm with linear function approximation and per time-step linear computational complexity in \"online\" learning environment. The algorithm considered here is TDC with importance weighting introduced by Maei et al. We support our theoretical results by providing suitable empirical results for standard off-policy counterexamples.", "histories": [["v1", "Thu, 19 May 2016 18:32:50 GMT  (242kb,D)", "http://arxiv.org/abs/1605.06076v1", "14 pages. arXiv admin note: text overlap witharXiv:1503.09105"]], "COMMENTS": "14 pages. arXiv admin note: text overlap witharXiv:1503.09105", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["prasenjit karmakar", "rajkumar maity", "shalabh bhatnagar"], "accepted": false, "id": "1605.06076"}, "pdf": {"name": "1605.06076.pdf", "metadata": {"source": "CRF", "title": "On a convergent off -policy temporal difference learning algorithm in on-line learning environment", "authors": ["Prasenjit Karmakar", "Rajkumar Maity", "Shalabh Bhatnagar"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "2 Background and description of TDC with importance weighting", "text": "We must estimate the value function for a target policy (a) in view of the ongoing evolution of the underlying MDP (with finite state and action spaces S and A), specified by the expected reward r (\u00b7, \u00b7) and the transition probability kernel p (\u00b7 \u00b7, \u00b7) for a behavioural policy \u03c0b (6 = \u03c0b). Suppose the above-mentioned progression form is (Xn, An, Rn, Xn + 1), n \u00b2 0, where {Xn} is a temporomogeneous irreducible Markov chain with unique stationary distribution and generated from the behavioural guideline \u03c0b. Here is the quadruplet (s, a, s) represented (current state, action, reward, next state). Let us also assume that b (a | s) > 0, s (S, a) > 0, an answer to question A, a) we must find the solution."}, {"heading": "3 Almost sure convergence proof of ONTDC", "text": "As already mentioned, in order to analyze the convergence of iterations (2) and (3), one must first extend the classical stochastic approximation frame on two time scales from Borkar [11] to an environment with Markov noise. Full extension is shown in the appendix. We will only give one specific case of this theory that will suffice for us. Therefore, we will start with this extension and show later how the TDC iterations can be inserted into this framework and proven to be convergent."}, {"heading": "3.1 Two timescale stochastic approximation with Markov noise", "text": "Our goal is to perform an asymptotic analysis of the following coupled recursions:"}, {"heading": "3.2 Convergence Proof of ONTDC", "text": "Theorem 3.2. Let us consider the iterations (2) and (3) of the TDC (X). Suppose: (i) {a (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n (n), (n), (n), (n), (n), (n (n), (n), (n), (n), (n), (n), (n (n), (n), n (n, n, n (n), n (n, n, n (n), n (n, n, n (n), n (n, n, n, n, (n, n, n, n, n, n, n, (n, n, n, n, n, n, n, (n, n, n, n, n, n, n, n, n, (n, n, n, n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, (n, n, n, n, n, n, n, n, n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n, n, n, n), n (n, n, n, n, n, n, n (n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n"}, {"heading": "4 Empirical results", "text": "For the evaluation of the algorithm, experimentally the result of a deviation from the classical Baird number against the actual values was shown. (2, Fig. 2.4] and \u03b8 \u2192 2\u03b8 Problem [8, Section 3]. In both cases, we compare the behavior of TD (0), OFFTDC and ONTDC. In contrast to [10], where the update was performed synchronously in dynamic programming-like, we do not use knowledge of the probabilities for the underlying Markov decision process. For Baird's problem, we consider the usual stochastic alignment scenarios, in which only simulated sample gradients are used as input to the algorithms, i.e. the algorithms do not use knowledge of the underlying Markov decision processes. The performance metric is defined as Root Mean Squared Error (RMSE) to be the square root of the average of the deviation between the true function and the estimated value function."}, {"heading": "5 Conclusion", "text": "We have presented almost certain convergence evidence for a non-political temporal differential learning algorithm, which is also applicable to authorization tracking (for a sufficiently large range of \u03bb) with linear functional approximation, assuming that the \"political\" path for behavioral policy is only available, which has not been done so far with our knowledge. A future direction would be to extend non-political control algorithms ([3]) to the more realistic settings we consider in this paper."}], "references": [{"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R.Mahmood", "R.S.Sutton"], "venue": "Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R.Maei", "C.Szepesv\u00e1ri", "S.Bhatnagar", "R.S.Sutton"], "venue": "International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S.Sutton", "A.G.Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "An emphatic approach to the problem of offpolicy temporal-difference learning", "author": ["R.S.Sutton", "A.R.Mahmood", "M.White"], "venue": "Technical report, University of Alberta,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Fast gradientdescent methods for temporal-difference learning with linear function approximation", "author": ["R.S.Sutton", "H.R.Maei", "D.Precup", "S.Bhatnagar", "D.Silver", "E.Wiewiora"], "venue": "International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "See [7] for additional uses.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "In [9, 10, 2] the gradient temporal difference learning (GTD) algorithms were proposed to solve this problem.", "startOffset": 3, "endOffset": 13}, {"referenceID": 4, "context": "It was shown in [10] that such an algorithm can be proved to be convergent using the classical convergence proof for two timescale stochastic approximation with martingale difference noise [11].", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Recently, emphatic temporal difference learning has been introduced in [8] to solve the off-policy evaluation problem.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Another related work is the much complex off-policy learning algorithms that obtain the benefits of weighted importance sampling (to reduce variance) with O(d) computational complexity [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "In this context we mention that the way single timescale Borkar-Meyn theorem was used in [10] to prove stability of two time-scale recursions is not a proper way to prove the same.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "The main advantage is that it works for \u03bb < 1 L\u03b3 (\u03bb \u2208 [0, 1] being the eligibility function) whereas the analysis in [4] is shown only for \u03bb very close to 1.", "startOffset": 54, "endOffset": 60}, {"referenceID": 4, "context": "Unlike [10] where updating was done synchronously in dynamic-programming-like sweeps through the state space, we consider the usual stochastic approximation scenario where only simulated sample trajectories are taken as input to the algorithms i.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "While the analysis has been shown for the diminishing step-size case, we implement here the algorithm with constant step-sizes as in [2, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "A future direction would be to similarly extend algorithms for off-policy control ([3]) to the more realistic settings as we consider in this paper.", "startOffset": 83, "endOffset": 86}], "year": 2016, "abstractText": "In this paper we provide a rigorous convergence analysis of a \u201coff\u201d-policy temporal difference learning algorithm with linear function approximation and per time-step linear computational complexity in \u201conline\u201d learning environment. The algorithm considered here is TDC with importance weighting introduced by Maei et al. We support our theoretical results by providing suitable empirical results for standard off-policy counterexamples.", "creator": "LaTeX with hyperref package"}}}