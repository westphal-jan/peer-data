{"id": "1605.06083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Stereotyping and Bias in the Flickr30K Dataset", "abstract": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \"focus only on the information that can be obtained from the image alone\" (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.", "histories": [["v1", "Thu, 19 May 2016 19:17:23 GMT  (128kb,D)", "http://arxiv.org/abs/1605.06083v1", "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016), pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio"]], "COMMENTS": "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016), pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["emiel van miltenburg"], "accepted": false, "id": "1605.06083"}, "pdf": {"name": "1605.06083.pdf", "metadata": {"source": "CRF", "title": "Stereotyping and Bias in the Flickr30K Dataset", "authors": ["Emiel van Miltenburg"], "emails": ["emiel.van.miltenburg@vu.nl"], "sections": [{"heading": null, "text": "Keywords: image annotation, stereotypes, bias, Flickr30K"}, {"heading": "1. Introduction", "text": "The Flickr30K Dataset (Young et al., 2014) is a collection of over 30,000 images, each with 5 crowd-sourced descriptions. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. (Vinyals et al., 2015). An untested assumption behind the datasets is that the descriptions are based on the images, and nothing else. Here are the authors (via the Flickr8K Dataset, a subset of Flickr30K): \"By asking people to describe the people, objects, scenes, and activities shown in an image, without giving them any further information about the context in which the image was taken, we could obtain conceptual descriptions that focus only on the information that can be obtained from the image alone.\" (Hodosh et al., 2013, p. 859) What overlooks this assumption is the amount of interpretation or recontextualization that the image alone is performed by the announcer to us a concrete image."}, {"heading": "2. Stereotype-driven descriptions", "text": "Stereotypes are ideas about how other (groups of) people usually behave and what they are likely to do. These ideas guide the way we talk about the world. I distinguish two types of verbal behavior that result from stereotypes: (i) linguistic bias and (ii) unwarranted conclusions. The former is discussed in greater detail by Beukeboom (2014), which defines linguistic bias as \"a systematic asymmetry in the choice of words as a function of the social category to which the target belongs.\" Thus, this bias becomes apparent through the distribution of terms used to describe entities in a particular way.ar Xiv: 160 5.06 083v 1 [cs.C L] 19 May 201 category. E.g. Description No. 4 above, which the girl in the picture calls hot. Analysis of this judgmental language goes beyond the scope of this paper."}, {"heading": "2.1. Linguistic bias", "text": "In general, people tend to use more specific or specific phrases when they have to describe a person who does not meet their expectations. Beukeboom (2014) lists several linguistic \"tools\" that people use to identify people who deviate from the norm. I will mention two of them. 2 Adjectives A well-researched example (Stahlberg et al., 2007; Romaine, 2001) is sexist language in which a person's gender tends to be mentioned more often when their role or profession is incompatible with \"traditional\" gender roles (e.g. surgeon, nurse). Beukeboom also points out that adjectives are used to create \"narrower labels [or subtypes] for individuals who do not agree with general societal beliefs\" (p. 3). For example, a hard woman makes an exception to the \"rule\" that women should not be considered harsh. Negation can be used when previous beliefs about a particular category are not violated, for example, the dumb."}, {"heading": "2.2. Unwarranted inferences", "text": "They are based on additional assumptions about the world. After reviewing a subset of Flickr30K data, I have grouped these conclusions into six categories (examples of images between brackets): Activity We have seen an example in the introduction where the \"manager\" allegedly talks about work performance and rants in a stern lecture (8063007). Ethnicity Many dark-skinned individuals are referred to as African-Americans, regardless of whether the image was taken in the US or not (4280272). And people given 2 examples are also referred to (Beukeboom, 2014).look Asian are referred to as Chinese (1434151732) or Japanese (4834664666). Event In Figure 4183120, people sitting in a gym are told that they are watching a game, although it may be most closely related to a situation."}, {"heading": "3. Detecting stereotype-driven descriptions", "text": "To get an idea of the stereotypical descriptions contained in the Flickr30K dataset, I have developed a browser-based annotation tool that displays both the images and associated descriptions.3 You can simply browse through the images by clicking \"Next\" or \"Random\" until you find an interesting pattern.3Code and data are available on GitHub: https: / / github. com / evanmiltenburg / Flickr30K Image Viewer (Asian | White | Black | African American | Skinned) Baby."}, {"heading": "3.1. Ethnicity/race", "text": "An interesting pattern is that the ethnicity / race of the babies does not seem to be mentioned unless the baby is black or Asian. In other words, white seems to be the standard, and others seem to be marked. How can we tell if the data is actually biased? We do not know if a unit belongs to a particular social class or not (in this case: ethnic group) until it is marked as such. However, we can determine the ratio by looking at all the images in which the announcers have used a marker (in this case: adjectives such as black, white, Asian), and counting for those images how many descriptions (out of five) contain a marker. This gives us an upper limit that tells us how often the ethnicity is indicated by the announcers that this upper limit lies somewhere between 20% (a description) and 100% (5 descriptions)."}, {"heading": "3.2. Other methods", "text": "Another method that readers may find particularly useful is to use the structure of Flickr30K entities (Plummer et al., 2015).This dataset enriches Flickr30K by adding correlation annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I used this data to create a correlation graph by linking all phrases referring to the same entity. Subsequently, I applied Louvain annotations (Blondel et al., 2008) to the correlation graph, resulting in clusters of expressions referring to similar entities."}, {"heading": "4. Discussion", "text": "In the previous section, I outlined several methods for manually detecting stereotypes, biases, and strange phrases. As there are many ways in which a phrase can be biased, it is difficult to detect bias from the data automatically. So, how should we deal with stereotype-driven descriptions? Neutralizing stereotypes for production One way forward might be to work with multilingual data. Elliott et al. (2015) suggest a model that generates image descriptions that provide data from multiple languages, in their case German and English. Multilingual, or rather: multicultural data could force models to place less emphasis on features that are relevant only to annotators from a particular country. Stereotypes and interpretation While stereotypes could be a problem for production, another study of cultural stereotyping could be useful for systems that interpret human descriptions and determine probable references to those descriptions."}, {"heading": "5. Conclusion", "text": "I have divided these descriptions into two classes: linguistic bias and unwarranted conclusions; the former corresponds to commentators \"choice of words when confronted with an image that may or may not meet their stereotypical expectations; the latter corresponds to commentators\" tendency to go beyond what physical data can tell us and expand their descriptions on the basis of their past experiences and knowledge of the world. Recognition of these phenomena is important because, on the one hand, it helps us think about what can be learned from the data; and, on the other, it serves as a warning: when we train and evaluate language models based on these data, we effectively teach them to be biased. I have also looked at methods of recognizing stereotypical descriptions, but because of the wealth of language, it is difficult to find an automated measure. Depending on whether this interpretation is a goal or whether it is useful to supplement human behavior, I ultimately consider stereotypical descriptions to be circumcised."}, {"heading": "6. Acknowledgments", "text": "Thanks to Piek Vossen and Antske Fokkens for the discussion and to Desmond Elliott and an anonymous reviewer for comments on an earlier version of this work. This research was supported by the Dutch Organization for Scientific Research (NWO) through the Spinoza Prize, which was awarded to Piek Vossen (SPI 30-673, 2014-2019)."}, {"heading": "7. Bibliographical references", "text": "Negation bias: when negations signal stereotypic expectations. Journal of personality and social psychology, 99 (6): 978. Beukeboom, C. J. (2014). Mechanisms of linguistic bias: How words reflect and keep stereotypic expectations. In J. Laszlo, et al., editors, Social cognition and communication, Volume 31, pp. 313-330. Psychology Press. Author's pdf: http: / / dare.ubvu.vu.nl / handle / 1871 / 47698. Blondel, V. D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Almost unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008 (10)."}], "references": [{"title": "The negation bias: when negations signal stereotypic expectancies", "author": ["C.J. Beukeboom", "C. Finkenauer", "D.H. Wigboldus"], "venue": "Journal of personality and social psychology,", "citeRegEx": "Beukeboom et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beukeboom et al\\.", "year": 2010}, {"title": "Mechanisms of linguistic bias: How words reflect and maintain stereotypic expectancies", "author": ["C.J. Beukeboom"], "venue": "Social cognition and communication,", "citeRegEx": "Beukeboom,? \\Q2014\\E", "shortCiteRegEx": "Beukeboom", "year": 2014}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "Guillaume", "J.-L", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment,", "citeRegEx": "Blondel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2008}, {"title": "Multilingual image description with neural sequence models. CoRR, abs/1510.04709", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "metrics. JAIR,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Conceptual framework for indexing visual information at multiple levels. In Electronic Imaging, pages 2\u201315. International Society for Optics and Photonics", "author": ["A. Jaimes", "Chang", "S.-F"], "venue": null, "citeRegEx": "Jaimes et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaimes et al\\.", "year": 1999}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "In Proceedings of the IEEE ICCV,", "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A corpus-based view of gender in british and american english. Gender Across Languages: The linguistic representation of women and men", "author": ["S. Romaine"], "venue": null, "citeRegEx": "Romaine,? \\Q2001\\E", "shortCiteRegEx": "Romaine", "year": 2001}, {"title": "Stereotypic explanatory bias: Implicit stereotyping as a predictor of discrimination", "author": ["D. Sekaquaptewa", "P. Espinoza", "M. Thompson", "P. Vargas", "W. von Hippel"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "Sekaquaptewa et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sekaquaptewa et al\\.", "year": 2003}, {"title": "Analyzing the subject of a picture: a theoretical approach", "author": ["S. Shatford"], "venue": "Cataloging & classification quarterly,", "citeRegEx": "Shatford,? \\Q1986\\E", "shortCiteRegEx": "Shatford", "year": 1986}, {"title": "Representation of the sexes in language", "author": ["D. Stahlberg", "F. Braun", "L. Irmen", "S. Sczesny"], "venue": "Social communication,", "citeRegEx": "Stahlberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \u201cfo-", "startOffset": 99, "endOffset": 119}, {"referenceID": 13, "context": "The Flickr30K dataset (Young et al., 2014) is a collection of over 30,000 images with 5 crowdsourced descriptions each.", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "(Vinyals et al., 2015)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "I will build on earlier work on linguistic bias in general (Beukeboom, 2014), providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences.", "startOffset": 59, "endOffset": 76}, {"referenceID": 6, "context": "Rashtchian et al. (2010) do note that some descriptions are speculative in nature, which they say hurts the accuracy and the consistency of the descriptions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "The former is discussed in more detail by Beukeboom (2014), who defines linguistic bias as \u201ca systematic asymmetry in word choice as a function of the social category to which the target belongs.", "startOffset": 42, "endOffset": 59}, {"referenceID": 1, "context": "Beukeboom (2014) lists several linguistic \u2018tools\u2019 that people use to mark individuals who deviate from the norm.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Adjectives One well-studied example (Stahlberg et al., 2007; Romaine, 2001) is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with \u2018traditional\u2019 gender roles (e.", "startOffset": 36, "endOffset": 75}, {"referenceID": 8, "context": "Adjectives One well-studied example (Stahlberg et al., 2007; Romaine, 2001) is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with \u2018traditional\u2019 gender roles (e.", "startOffset": 36, "endOffset": 75}, {"referenceID": 0, "context": "See also (Beukeboom et al., 2010).", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "Examples given are also due to (Beukeboom, 2014).", "startOffset": 31, "endOffset": 48}, {"referenceID": 6, "context": "One method readers may find particularly useful is to leverage the structure of Flickr30K Entities (Plummer et al., 2015).", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "Following this, I applied Louvain clustering (Blondel et al., 2008) to the coreference graph, resulting in clusters of expressions that refer to similar entities.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Elliott et al. (2015) propose a model that generates image descriptions given data from multiple languages, in their case German and English.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "of images (Shatford, 1986; Jaimes and Chang, 1999).", "startOffset": 10, "endOffset": 50}, {"referenceID": 12, "context": "But recent neural network models like (Vinyals et al., 2015) do not match this procedure.", "startOffset": 38, "endOffset": 60}], "year": 2016, "abstractText": "In: Proceedings of the Workshop on Multimodal Corpora: Computer vision and language processing (MMC-2016), pages 1\u20134. Workshop held: 24 May 2016, collocated with LREC 2016, Portoro\u017e, Slovenia. Proceedings available at: http://www.lrec-conf.org/proceedings/lrec2016/workshops/ LREC2016Workshop-MCC-2016-proceedings.pdf An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \u201cfocus only on the information that can be obtained from the image alone\u201d (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.", "creator": "LaTeX with hyperref package"}}}