{"id": "1605.06155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Inter-Battery Topic Representation Learning", "abstract": "In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uni- and multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.", "histories": [["v1", "Thu, 19 May 2016 21:44:12 GMT  (26105kb,D)", "https://arxiv.org/abs/1605.06155v1", null], ["v2", "Thu, 28 Jul 2016 10:08:40 GMT  (13982kb,D)", "http://arxiv.org/abs/1605.06155v2", "ECCV 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["cheng zhang", "hedvig kjellstrom", "carl henrik ek"], "accepted": false, "id": "1605.06155"}, "pdf": {"name": "1605.06155.pdf", "metadata": {"source": "CRF", "title": "Inter-Battery Topic Representation Learning", "authors": ["Cheng Zhang", "Hedvig Kjellstr\u00f6m", "Carl Henrik Ek"], "emails": ["chengz@kth.se", "hedvig@kth.se", "carlhenrik.ek@bristol.ac.uk"], "sections": [{"heading": null, "text": "Keywords: Factorized Representation, Topic Model, Multi-View Model, CNN Feature, Image Classification"}, {"heading": "1 Introduction", "text": "In fact, it is a case of blaming oneself and others. (...) In fact, it is a case of blaming oneself and others. (...) It is a case of blaming others. (...) It is a case of blaming oneself and others. (...) It is a case of blaming oneself. (...) It is a case of blaming oneself. (...) It is a case of blaming oneself and blaming oneself. (...) It is a case of blaming oneself. (...) It is a case of blaming oneself and blaming others. (...) It is a case of blaming oneself and blaming oneself. (...) It is a case of blaming ones. (...) It is a case of blaming ones. (...) It is a case of blaming ones. (...) It is a case of blaming ones. (...) It is a case of blaming ones."}, {"heading": "2 Related Work", "text": "In fact, most people who are able are able to determine for themselves what they want to do and what they want to do."}, {"heading": "3 Model", "text": "In this section, we will first briefly review the LDA [3] on which IBTM is based, and then present the modelling details and conclusions of IBTM. Finally, we will describe how the latent representation can be used for classification tasks with which we evaluate our approach."}, {"heading": "3.1 Latent Dirichlet Allocation", "text": "LDA is a classical generative model that is able to model the latent structure of discrete data, e.g. a bag of words for the presentation of documents. Figure 2 (a) shows the graphical representation of LDA [3]. In LDA it is assumed that the words (visual words) w are generated by sampling from a thematic distribution per document \u03b8 \u0445 Dir (\u03b1) and a word distribution per topic \u03b2 \u0445 Dir (\u03c3). The Dirichlet distribution is a natural choice as it is conjugated with a multinomial distribution."}, {"heading": "3.2 Inter-Battery Topic Model", "text": "First, we will explain how to apply IBTM to a scenario with two views, so that it can be easily compared with other models [7,8,18,20]. Below, we will present the generalized views that can encode any number of views; the two views can represent different types of data, such as two modalities, image and caption as shown in Figure 1 (b); or two different descriptors for the same data, such as SIFT and SURF characteristics of the same image. You can also represent two types of views of the same class, such as the two types of coffee as shown in Figure 1 (a).The key of IBTM is that we assume that topics are fully compared with each other."}, {"heading": "3.3 Classification", "text": "Theme models provide a compact representation of the data. Both LDA and IBTM are unattended models and can be used for presentation learning, and the presentation can be applied to various tasks, such as image classification and image recovery. Typically, for these tasks, the entire presentation of the topic is used using LDA. When using IBTM, we rely only on the common theme space, which represents the information essence. We can simply use a support vector machine or Softmax regression for image classification, using the common presentation of the topic as input. Softmax regression is used in our experimental evaluation. Although there are different types of monitored topic models [13,16] where class names are encoded as part of the model, the work in [12] shows that performance in computer vision classification tasks using a supervised model and an unattended model with an additional classifier is similar. A slight improvement in performance is often accompanied by a marked improvement in compression costs."}, {"heading": "4 Experiments", "text": "In the experiments, we will first evaluate the sequence scheme and demonstrate the model behavior in a controlled manner in Section 4.1. Then, we will use two benchmark datasets to evaluate the model behavior in real scenarios in Section 4.2. To this end, we will use the LabelMe data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization."}, {"heading": "4.1 Inference Evaluation using Synthetic Data", "text": "To test the key performance, we generate a set of synthetic data using the model with the different subject distributions \u0443, \u03b2, \u03b7, \u03c4 and hyperparameters for \u00b5, \u03c1, \u043d, \u03bd. We generate 500 documents and each document has 100 words for each view. Given the data generated, a correct key algorithm can restore all latent parameters. Figure 3 (a) shows the basic truth we have used for the distribution of words per topic and the estimation of these latent variables using variable inference as described in Section 3.2. All topics are correctly restored. Due to the interchangeability of the dirichlet distribution, the estimate indicates a different order of topics, which is shown in Figure 3 (b) as line-by-line replacement. Figure 4 shows the restoration of the parameters for the partition parameters \u043c and \u00b5, which are generated from the beta distribution. In the example, we use the hyperparameters for the first four parameters, the noise parameters in the first view is almost error-free."}, {"heading": "4.2 Performance Evaluation using Real-World Data", "text": "We present two experimental groups: the first is the use of the LabelMe dataset [18,24] and the second is the use of the Leeds Butterfly dataset [26] for a fine-grained classification. We focus on the model performance, where we study the distribution of themes and partition parameters, which will give us insights into the data structure and behavior of the model. Afterwards, we will present the classification performance. In these experiments, the classification results are obtained by applying softmax regression on representation. In all experimental settings, the hyper-parameters for the thematic distributions per document are set to determine the distribution of documents."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a different variant of the IBTM theme model with a factorized latent representation. It is able to model common information and private information using different views, which has been shown to be beneficial for different computer vision tasks. Experimental results show that IBTM can effectively encode task-relevant information. Based on this representation, the current results are obtained in various experimental scenarios. In this paper, the focus was on the concept of factorized representations and the experiments focused on two viewpoint scenarios. In future work, we plan to evaluate the performance of IBTM through any number of views and in different scenarios such as random integration. In the end, efficient inference algorithms are the key to probabilistic graphical models in general. In this paper, we used varying inferences in batch manners. In the future, more efficient and robust inference algorithms [29,30] can be explored."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI 35(8)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic Principal Component Analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society 61(3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR 3", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Gaussian Process Latent Variable Models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "NIPS.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR. Volume 2., IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning tags from unsegemented videos of multiple human actions", "author": ["T.M. Hospedales", "S.G. Gong", "T. Xiang"], "venue": "ICDM.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextual Modeling with Labeled Multi-LDA", "author": ["C. Zhang", "D. Song", "H. Kjellstrom"], "venue": "IROS, Tokyo, Japan", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "An Inter-Battery Method of Factory Analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika 23", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1958}, {"title": "Manifold Relevance Determination", "author": ["A. Damianou", "H.C. Ek", "M. Titsias", "N.D. Lawrence"], "venue": "ICML.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Factorized Topic Models", "author": ["C. Zhang", "C.H. Ek", "A. Damianou", "H. Kjellstrom"], "venue": "ICLR.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorized multi-modal topic model", "author": ["S. Virtanen", "Y. Jia", "A. Klami", "T. Darrell"], "venue": "arXiv preprint arXiv:1210.4920", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "How to Supervise Topic Models", "author": ["C. Zhang", "H. Kjellstrom"], "venue": "ECCV workshop on Graphical Models in Computer Vision.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised Topic Models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "arXiv preprint arXiv:1003.0783", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised hierarchical Dirichlet processes with variational inference", "author": ["C. Zhang", "C.H. Ek", "X. Gratal", "F.T. Pokorny", "H. Kjellstr\u00f6m"], "venue": "ICCV workshop on Inference for probabilistic graphical models.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "DiscLDA: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "NIPS.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "ICML.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Gibbs max-margin supervised topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "ICML.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D. Blei", "L. Fei-Fei"], "venue": "CVPR, IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Max-margin Latent Dirichlet Allocation for Image Classification and Annotations", "author": ["Y. Wang", "G. Mori"], "venue": "BMVC.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "International Conference on Research and Development in Information Retrieval, ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Ambiguity modeling in latent spaces", "author": ["C.H. Ek", "J. Rihan", "P. Torr", "G. Rogez", "N. Lawrence"], "venue": "Machine learning for multimodal interaction. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling general and specific aspects of documents with a probabilistic topic model", "author": ["C. Chemudugunta", "P. Smyth", "M. Steyvers"], "venue": "NIPS. Volume 19.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Correlated topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "NIPS. Volume 18.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Objects as attributes for scene classification", "author": ["L.J. Li", "H. Su", "Y. Lim", "L. Fei-Fei"], "venue": "Trends and Topics in Computer Vision. Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Topic Modeling of Multimodal Data: An Autoregressive Approach", "author": ["Y. Zheng", "Y.J. Zhang", "H. Larochelle"], "venue": "CVPR.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning models for object recognition from natural language descriptions", "author": ["J. Wang", "K. Markert", "M. Everingham"], "venue": "BMVC.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Divergence measures and message passing", "author": ["T.P. Minka"], "venue": "Microsoft Research Technical Report.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Structured stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei"], "venue": "AISTATS.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This has generated a huge interest in directly learning representation from data [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 3, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 5, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 6, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 5, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 7, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 8, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 9, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 10, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 11, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 7, "context": "The idea of factorized representation can be traced back to the early work of Tucker, \u2019An Inter-Battery Method of Factory Analysis\u2019 [8], hence, we name the model presented in this paper Inter-Battery Topic Model (IBTM).", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Latent Dirichlet Allocation (LDA) [3] is the corner stone of topic modeling.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 5, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 6, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 13, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 14, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 15, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 16, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 11, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 292, "endOffset": 296}, {"referenceID": 10, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 17, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 18, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 19, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 6, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 222, "endOffset": 225}, {"referenceID": 5, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 7, "context": "The benefit of modeling the between-view variance separately from the within-view variance was first pointed out by Tucker [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 11, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 21, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 5, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 8, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 20, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 21, "context": "For uni-modal scenarios, a special words topic model with a background distribution (SWB) [22] is one of the first studies on factorized representation using topic model for information retrieval tasks.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 72, "endOffset": 78}, {"referenceID": 10, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 72, "endOffset": 78}, {"referenceID": 10, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 22, "context": "The multi-modal factorized topic model which is based on correlated topic models [23] only provides an implicit link between different modalities with hierarchical Dirichlet priors since the factorization is enforced on the logistic normal prior, while VTT is only designed for the specific application.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "In this section, firstly, we will shortly review LDA [3] which IBTM is based on and then present the modeling details and inference of IBTM.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Figure 2 (a) shows the graphic representation of LDA [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 7, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 17, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 19, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 6, "context": "Otherwise, if \u03c1 = 1 and \u03bc = 1, IBTM becomes a regular multi-modal topic model [7,20].", "startOffset": 78, "endOffset": 84}, {"referenceID": 19, "context": "Otherwise, if \u03c1 = 1 and \u03bc = 1, IBTM becomes a regular multi-modal topic model [7,20].", "startOffset": 78, "endOffset": 84}, {"referenceID": 12, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 23, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 24, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 25, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 23, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 25, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "We use the pre-trained Oxford VGG 16-layer CNN [27] for feature extraction.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "We create sliding windows in 3 scales with a 32 pixels step size to extract features, in the same manner as [28], and use K-means clustering to create a codebook and represent each image using a bag-of-visual-words.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "We use the LabelMe Dataset as in [18,25] for this group of the experiments.", "startOffset": 33, "endOffset": 40}, {"referenceID": 24, "context": "We use the LabelMe Dataset as in [18,25] for this group of the experiments.", "startOffset": 33, "endOffset": 40}, {"referenceID": 21, "context": "While using SWB [22] 5, the performance is unsatisfactory for such computer vision tasks due to the noisy properties of images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "5 We implemented SWB using Gibbs Sampling following the description in the paper [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "The parameter settings are the same as in [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "Full SVM PCA 15 LDA15 SWB 2V [22] IBTM15 1V IBTM15 2V 87.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "When both modalities are available, the performance goes up to 95%, while ideal classification by humans for this dataset is reported to be 90% in [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "In this section, the Leeds butterfly dataset [26] is used to evaluate the IBTM model on a fine-grained classification task.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "In the future, more efficient and robust inference algorithms [29,30] can be explored.", "startOffset": 62, "endOffset": 69}, {"referenceID": 29, "context": "In the future, more efficient and robust inference algorithms [29,30] can be explored.", "startOffset": 62, "endOffset": 69}], "year": 2016, "abstractText": "In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uniand multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.", "creator": "LaTeX with hyperref package"}}}