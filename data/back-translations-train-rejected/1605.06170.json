{"id": "1605.06170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Evaluation System for a Bayesian Optimization Service", "abstract": "Bayesian optimization is an elegant solution to the hyperparameter optimization problem in machine learning. Building a reliable and robust Bayesian optimization service requires careful testing methodology and sound statistical analysis. In this talk we will outline our development of an evaluation framework to rigorously test and measure the impact of changes to the SigOpt optimization service. We present an overview of our evaluation system and discuss how this framework empowers our research engineers to confidently and quickly make changes to our core optimization engine", "histories": [["v1", "Thu, 19 May 2016 23:10:15 GMT  (730kb,D)", "http://arxiv.org/abs/1605.06170v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ian dewancker", "michael mccourt", "scott clark", "patrick hayes", "alexandra johnson", "george ke"], "accepted": false, "id": "1605.06170"}, "pdf": {"name": "1605.06170.pdf", "metadata": {"source": "META", "title": "Evaluation System for a Bayesian Optimization Service", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark", "Alexandra Johnson"], "emails": ["IAN@SIGOPT.COM", "MIKE@SIGOPT.COM", "SCOTT@SIGOPT.COM", "PATRICK@SIGOPT.COM", "ALEXANDRA@SIGOPT.COM", "L2KE@UWATERLOO.CA"], "sections": [{"heading": "1. Introduction", "text": "SigOpt offers its customers an optimization service to optimize complex systems, simulations, and models. Our optimization engine applies several concepts from Bayean optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customer metrics as quickly as possible. In particular, we look at problems where the maximum for an expensive function f: X \u2192 R, xopt = argmax x X f (x) is sought within an X Rd range, which is a boundary range. Hyperparameter optimization for machine learning is of particular relevance because the calculation costs for evaluating model variations are high, d are typically small, and hyperparameter gradients are typically not available. SigOpt's kernoptization engine is a closed fork of the open source MOE project (Clark et al., 2014)."}, {"heading": "2. Metrics", "text": "The SigOpt service is aimed at maximizing objective functions. The performance indicators we consider for comparisons to a given objective function are the best value seen at the end of the optimization (Best Found) and the area below the most visible curve (AUC). The AUC indicators can help to better differentiate performance, as shown in Figure 1.ar Xiv: 160 5.06 170v 1 [cs.L G] 19 May 2The stochastic nature of the optimization algorithms under consideration requires a statistical interpretation of the performance indicators. That is, the optimization performance of a given function varies by nature from sequence to sequence, so that several runs to a given function are required to detect statistically significant changes. In general, optimization algorithms are executed 20 times for each function and the distributions of the performance indicators are compared using the non-parametric Mann-Whitney U test, which was proposed in previous empirical studies for optimization (Hyebacker) in 2016."}, {"heading": "3. Benchmark Suite", "text": "The tests for our evaluation system consist of closed-form optimization features (McCourt, 2016), which are extensions of an earlier black box optimization set proposed by Gavana, 2013. These features are quick to evaluate and extendable. We were looking for a collection that had a variety of properties, such as uneven oscillatory ones. Figure 2Design shows some representative features and related interesting features as a major concern when creating a benchmark test suite or data set. An example of a design bias we first encountered in our test suite were features that had optimizations in predictable places, such as the center of the domain or on integer coordinates. In this benchmark suite, we have tried to adequately classify and separate functions of this kind, although more work is needed to identify and resolve less obvious distortions."}, {"heading": "4. Infrastructure", "text": "Obtaining the empirical distributions of performance indicators for each test function in our benchmark suite requires significant computing resources. Fortunately, these evaluation tasks are scrupulously parallel, as each function optimization can be performed independently of the others in the test suite, and each repeated run with the same function is independent of other repeat runs. To coordinate this effort, lightweight function evaluation processes are run simultaneously on a large master machine with many cores, and each process communicates with an on-demand cluster of SigOpt API employees, who in turn coordinate each optimization request with a cluster of instances running the SigOpt optimization engine, as well as with a database used by the service. The database maintains important states for each optimization and is central to the production service. Basic optimization procedures are executed directly on the master machine.Instances for the evaluation system are created using cloud compute vendors and digital AWS Ocean."}, {"heading": "5. Visualization Tools", "text": "Each time all test functions are optimized, key performance indicators and the most visible traces are collected. In raw form, this information is scary enough to summarize and extract actionable insights. To quickly summarize these results and immerse ourselves in the performance results of certain test functions, we have developed an interactive web application that houses various visualizations of the evaluation data."}, {"heading": "5.1. Comparative Optimization Traces", "text": "The track represents the best value of the objective metric to be seen after each function evaluation. Bayesian optimization setting assumes that each function evaluation is expensive, so the efficiency of the methods is most naturally compared to this measurement. Each track of a particular function is stochastical, so that the interquartile range of all tracks and the median track is plotted. Tracks are always generated in a comparative setting; either between two versions of SigOpt or between SigOpt and an external optimization method. Figure 5.1 shows a comparative version of SigOpt compared to an implementation of Particle Swarm Optimization (PSO) (Lee, 2014). A comparative metric summary table is also provided for each most visible track, or between SigOpt and an external optimization method. This table summarizes the results of the Mann-Whitney U test compared to a metric distribution metric (Lee, 2014)."}, {"heading": "5.2. Comparative p-value Histograms", "text": "While the most visible traces are useful for checking the performance of individual functions, it is also useful to have visualizations that help to summarize the complete relative performance between two optimization methods on a given metric. To this end, comparative histograms are created that represent the distribution of test functions across the p value ranges of a given metric. For each metric M, we divide the test functions into two groups and create two histograms of p values, which are returned by the Mann-Whitney U tests to the empirical distributions after the evaluation runs. Example histograms are shown in Figure 5.2wins (A > B) M = {func | E [MA] > E [MB]} wins (B > A) M = {func | E [MB] > E [MA]}."}, {"heading": "5.3. Comparative Total Performance Tables", "text": "The p-value histogram is useful for summarizing performance differences between methods on a particular metric, but it is often useful to quickly understand a measurement of total relative performance between two methods that are summarized across all metrics. To this end, we create summary tables that count the number of wins, losses, draws, and mixed performance comparisons between methods.An example table is shown below in Table 1Total gains represent the number of test functions in which at least one metric with statistical significance has improved, and all other metrics have not changed with statistical significance.Total gains (A > B) = | {func | M (1): E [M (1) > func (M (2) < E [M (2) B]: pval (M (2) < M (2) B) > otristic functions that increase with mixed significance (2) (0.0001 = 0.0001 001 = 0.0001 001 001 = 000000001 = 0000000000001 = 0000000000001 = 0000000000000000001) (00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}, {"heading": "6. Conclusions", "text": "Our evaluation system has become a valuable analysis tool when it comes to algorithm or system changes to the SigOpt optimization service. Data-driven performance analysis is an effective method to enable faster iteration and evaluation of a wide range of ideas. It continues to improve the SigOpt core service by providing empirical comparisons between internal changes and alternative methods from the Bayesian optimization community and helping to detect errors and bugs."}], "references": [{"title": "Algorithms for hyper-parameter optimization", "author": ["Bergstra", "James S", "Bardenet", "R\u00e9mi", "Bengio", "Yoshua", "K\u00e9gl", "Bal\u00e1zs"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "MOE: A global, black box optimization engine for real world metric optimization", "author": ["Clark", "Scott", "Liu", "Eric", "Frazier", "Peter", "Wang", "JiaLei", "Oktay", "Deniz", "Vesdapunt", "Norases"], "venue": "https://github.com/Yelp/MOE,", "citeRegEx": "Clark et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2014}, {"title": "A stratified analysis of bayesian optimization methods", "author": ["Dewancker", "Ian", "McCourt", "Michael", "Clark", "Scott", "Hayes", "Patrick", "Johnson", "Alexandra", "Ke", "George"], "venue": "arXiv preprint arXiv:1603.09441,", "citeRegEx": "Dewancker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dewancker et al\\.", "year": 2016}, {"title": "AMPGO global optimization benchmark functions", "author": ["Gavana", "Andrea"], "venue": "https://github.com/andyfaff/ ampgo,", "citeRegEx": "Gavana and Andrea.,? \\Q2013\\E", "shortCiteRegEx": "Gavana and Andrea.", "year": 2013}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Hutter", "Frank", "Hoos", "Holger H", "Leyton-Brown", "Kevin"], "venue": "In Learning and Intelligent Optimization,", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "pyswarm : Particle swarm optimization (PSO) with constraint support", "author": ["Lee", "Abraham"], "venue": "https://github. com/tisimst/pyswarm,", "citeRegEx": "Lee and Abraham.,? \\Q2014\\E", "shortCiteRegEx": "Lee and Abraham.", "year": 2014}, {"title": "Optimization Test Functions", "author": ["McCourt", "Michael"], "venue": "https: //github.com/sigopt/evalset,", "citeRegEx": "McCourt and Michael.,? \\Q2016\\E", "shortCiteRegEx": "McCourt and Michael.", "year": 2016}, {"title": "Taking the human out of the loop: A review of bayesian optimization", "author": ["Shahriari", "Bobak", "Swersky", "Kevin", "Wang", "Ziyu", "Adams", "Ryan P", "de Freitas", "Nando"], "venue": "Technical report, Universities of Harvard,", "citeRegEx": "Shahriari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.", "startOffset": 76, "endOffset": 143}, {"referenceID": 8, "context": "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.", "startOffset": 76, "endOffset": 143}, {"referenceID": 7, "context": "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.", "startOffset": 76, "endOffset": 143}, {"referenceID": 1, "context": "SigOpt\u2019s core optimization engine is a closed-source fork of the open-source MOE project (Clark et al., 2014).", "startOffset": 89, "endOffset": 109}, {"referenceID": 4, "context": "Generally, optimization algorithms are run 20 times on each function and the distributions of the performance metrics are compared using the non-parametric Mann-Whitney U test, which has been suggested in previous empirical studies of Bayesian optimization methods (Hutter et al., 2011).", "startOffset": 265, "endOffset": 286}, {"referenceID": 2, "context": "Further discussion of these metrics and statistical analysis is presented in (Dewancker et al., 2016)", "startOffset": 77, "endOffset": 101}], "year": 2016, "abstractText": "Bayesian optimization is an elegant solution to the hyperparameter optimization problem in machine learning. Building a reliable and robust Bayesian optimization service requires careful testing methodology and sound statistical analysis. In this talk we will outline our development of an evaluation framework to rigorously test and measure the impact of changes to the SigOpt optimization service. We present an overview of our evaluation system and discuss how this framework empowers our research engineers to confidently and quickly make changes to our core optimization engine", "creator": "LaTeX with hyperref package"}}}