{"id": "1605.06377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers", "abstract": "After data selection, pre-processing, transformation, and feature extraction, knowledge extraction is not the final step in a data mining process. It is then necessary to understand this knowledge in order to apply it efficiently and effectively. Up to now, there is a lack of appropriate techniques that support this significant step. This is partly due to the fact that the assessment of knowledge is often highly subjective, e.g., regarding aspects such as novelty or usefulness. These aspects depend on the specific knowledge and requirements of the data miner. There are, however, a number of aspects that are objective and for which it is possible to provide appropriate measures. In this article we focus on classification problems and use probabilistic generative classifiers based on mixture density models that are quite common in data mining applications. We define objective measures to assess the informativeness, uniqueness, importance, discrimination, representativity, uncertainty, and distinguishability of rules contained in these classifiers numerically. These measures not only support a data miner in evaluating results of a data mining process based on such classifiers. As we will see in illustrative case studies, they may also be used to improve the data mining process itself or to support the later application of the extracted knowledge.", "histories": [["v1", "Fri, 20 May 2016 14:34:49 GMT  (1939kb,D)", "http://arxiv.org/abs/1605.06377v1", "29 pages with 9 figures and 4 tables. Currently under review for Information Sciences"]], "COMMENTS": "29 pages with 9 figures and 4 tables. Currently under review for Information Sciences", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["dominik fisch", "christian gruhl", "edgar kalkowski", "bernhard sick", "seppo j ovaska"], "accepted": false, "id": "1605.06377"}, "pdf": {"name": "1605.06377.pdf", "metadata": {"source": "CRF", "title": "Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers", "authors": ["Dominik Fisch", "Christian Gruhl", "Edgar Kalkowski", "Bernhard Sick", "Seppo J. Ovaska"], "emails": ["dominik.fisch@bmw.de)", "cgruhl@uni-kassel.de)", "kalkowski@uni-kassel.de)", "bsick@uni-kassel.de)", "seppo.ovaska@aalto.fi)"], "sections": [{"heading": null, "text": "Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers, knowledge extraction is not the last step in a data mining process; it is then necessary to understand this knowledge in order to apply it efficiently and effectively. However, there is currently a lack of suitable techniques to support this important step, in part because the evaluation of knowledge is often very subjective, e.g. in terms of aspects such as novelty or usefulness. These aspects depend on the specific knowledge and requirements of the data miner. However, there are a number of aspects that are objective and for which it is possible to take appropriate action. In this article, we focus on classification problems and use probabilistic generative classifiers based on mixture density models, which are quite common in data mining applications."}, {"heading": "1 Introduction", "text": "In fact, most of them are not a \"normal\" person, but a \"normal\" person who is able to move around without being able to move."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most people who are able to determine themselves what they are doing and what they want to do are able to determine themselves. (...) It is not as if they are able to determine themselves. (...) It is as if they are able to determine themselves what they are doing. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are doing it, as if they are doing it, as if they are doing it, as if they are doing it, as if they are doing it, as if they are doing it. (...)"}, {"heading": "3 Methodological Foundations", "text": "In this section, we first present the generative classification paradigm. A generative classifier aims to model the processes underlying the \"generation\" of the data [6]. It is called \"generative\" because, with perfect modeling of these processes, a generative model could be used to generate artificial data with exactly the same characteristics as the real data. However, discriminative classifiers aim to find the optimal decision boundary directly. Today, these two approaches are often combined to exploit their respective advantages."}, {"heading": "3.1 Probabilistic Classifier CMM", "text": "That is, for a given, specific D-Dimensional Input Sample x \"we want the posterior distribution p (c), i.e., the probabilities for a class affiliation (with classes c) can be based on the input sample x.\" To minimize the risk of classification errors, we can then select the class with the highest posterior probability (cf), where the principle of \"winner-takes-all\" applies. In general, the posterior distribution p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (i) p (i) p (x) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) is a multinomic distribution with parameters designated as class components."}, {"heading": "3.1.1 Training of CMM", "text": "How can the different parameters of the classifier be determined? For a given training set X with N samples xn it is assumed that the xn are independently and equally distributed (i.e.). First, X is divided into C subsets Xc, each of which contains all samples of the corresponding class c, i.e., Xc = {xn | xn belongs to class c}. (8) For each Xc, a mixing model is trained. This approach has two important advantages over other methods, such as a standard expectation maximization approach (EM). First, the estimation process is more robust, i.e. it avoids the \"collapse\" parameters as random variables whose distributions need to be trained."}, {"heading": "3.1.2 Classification Performance of CMM Compared to 1NN, SVM, and Decision Trees", "text": "To confirm that CMM is comparable to other classification paradigms in terms of classification accuracy and is therefore sufficiently meaningful to define measures for understanding knowledge, we evaluate its performance on 21 benchmark data sets: phoneme, satimage (real data from the UCL Machine Learning Group [64]); Australian, Credit _ g, glass, heart, iris, pima, quality, segment, vehicle, vowel, wine, yeast (real data from the UCI Machine Learning Repository [3]); clouds (artificial data from the UCI Machine Learning Repository [3]); ripley (artificial data proposed in a vehicle); and two moons (own artificial data from the UCI Machine Learning Repository); clouds (artificial data from the UCI Machine Learning Repository)."}, {"heading": "3.1.3 Rule Extraction From CMM", "text": "This is possible with our classifier, if it is parameterized accordingly. At the moment, we focus on a single component p (x / i) and ignore the identification threshold. Limitations on the covariance matrix are not necessary, which implies that the information about the dependencies between different dimensions (i.e.) given by the covariances is lost. So we recommend to force the covariance matrices to be diagonal. Then, the multivariate Gaussian N (xcont) can be divided into a product that unifies from Dcont."}, {"heading": "3.2 Measures for Knowledge Understanding", "text": "In the following, we will describe seven objective metrics by which we can objectively evaluate the knowledge contained in the CMM. We will only use the term rule instead of components if we want to extract explicitly human-readable rules from the CMM. In this article, we will focus on metrics for individual components (i.e. rules). Metrics for total classifiers could easily be determined by averaging metrics for components or taking into account worst cases, etc. It would then be possible, for example, to compare different classifiers. In addition, classical metrics (e.g. classification errors on independent test data) should also be used. If the class to which a component belongs is not relevant for a measurement, the component is identified by a single index i-value."}, {"heading": "3.2.1 Informativeness", "text": "A component of the CMM is considered very informative, assuming that it describes a truly different type of process that generates \"q\" data. To numerically assess the informativity of a component, we use the Hellinger distance Hel (p (x), q (x))) of two probability densities p (x) and q (x) (cf. [6]). Compared to other statistical distance measures such as the Kullback-Leibler divergence (cf. Section 3.2.5), the Hellinger distance has the advantage of being between 0 and 1. It is defined by Hel (p (x), q (x)) = 1 \u2212 BC (p x), q (x), x (x), where BC (p), q (x), q (x) and the component Hhattacharyya (x) are defined: BC (p), q (x), q (x), q (x)."}, {"heading": "3.2.2 Uniqueness", "text": "The knowledge contained in the components of a CMM should be unique. It is measured by the uniqueness of a component i, which reflects the degree to which samples of different classes are covered by that component. (17) Then we define the uniqueness of the component i by Uniql (i), i.e., \u03c1i (xn): = p (xn | i) p (i) p (xn). (18) For example, to evaluate the uniqueness of an entire classifier, we can calculate the weighted average of the uniqueness values of each component (e.g. using the mixing coefficients as weights). The runtime complexity required to evaluate the uniqueness of a component is O (N \u00b7 I \u00b7 (D3t + DD), since we have to evaluate the uniqueness of each component across the entire CMM category."}, {"heading": "3.2.3 Importance", "text": "The importance of a component measures the relative weight of a component within the classifier. In general, either a small or a large number of components can be considered \"important,\" depending on a specific application. Here, a component i is considered very important if its mixing coefficient \u03c0i is well above the average mixing coefficient \u03c0 = 1I. To scale the importance of one component to the interval [0, 1], we additionally use a boundary function consisting of two linear functions, one projecting all mixing coefficients that are smaller than the average to the interval [0, 0.5] and the other mapping all mixing coefficients that are larger than the average to [0.5, 1]. The importance of component i is then calculated by impo (i): = {\u03c0i 2\u03c0, \u03c0i \u2264 1 \u2212 \u03c0i 2 (\u03c0 \u2212 1) + 1, \u03c0i > \u03c0. (19) Again, the importance of the component i is calculated in terms of a whole classifier, for example, a weighted component is used."}, {"heading": "3.2.4 Discrimination", "text": "The measure of discrimination evaluates the impact of a component i on the decision limit - and thus on the classification performance - of the overall classifier. To calculate the discrimination of component i, we generate a second CMM by removing i from the original CMM and normalizing the mixing coefficients of the remaining components. We then compare the classification error achieved on training data (or test data, if available) of the original CMM (Ewith) with the classification error of the CMM without component i (Ewithout): Disc (i): = Ewithout \u2212 Ewith. (20) If it requires concrete application (e.g. in some medical applications, false positives are acceptable, while false negatives could be fatal), it is also possible to apply more detailed measures such as sensitivity, specificity or precision to assess the discriminatory ability of a component."}, {"heading": "3.2.5 Representativity", "text": "The performance of a generative classifier also depends on how well it models the data. However, this type of fitness is determined by the continuous dimensions in which we explicitly assume that the data distribution can be modelled by a mixture of Gaussians. However, since it is always possible for categorical data to find a distribution that perfectly models the data (cf. determination of a histogram), the representation measurement only takes into account the continuous dimensions xcont. Since the actual underlying distribution q (xcont) is unknown, it has to come to terms with a non-parametric density technique (cf. determination of a standard window for estimating the particle density xcont. Since the actual underlying distribution q (xcont) is unknown, we have to get acquainted with a non-parametric density technique (cf.). (21) Here h is a user-defined parameter."}, {"heading": "3.2.6 Uncertainty", "text": "The value of the CMM estimate is unlimited and can even be used for negative measurements (see Section 3.1), i.e. they are considered random variables whose distributions of sample data must be determined. However, for the parameters of the categorical dimensions, the corresponding distributions are dirichlet distributions. To quantify the uncertainty of these parameter estimates, we use entropy H [6]. The entropy of a continuous random variable x with the density p (x) isH [x] = \u2212 categorical p (x) ln p (x) dx. (27) The probability mass of the distribution p (x) is, i.e. the safer the parameter estimate, the lower the entropy H [x]."}, {"heading": "3.2.7 Distinguishability", "text": "In order to limit the distinctness of two components, we omit the normalizing coefficients of the projected Gaussians. This also ensures that the highly overlapping Gaussians, who are not easily distinguishable for a human being, are assigned a low distinctness value. To limit the distinctness of two components, we disregard the normalizing coefficients of the projected Gaussians. To assess the distinctness of the two components, we disregard the normalizing coefficients of the projected Gaussians. This also ensures that the highly overlapping Gaussians, who are not easily distinguishable for a human being, are assigned a low distinctness value. To assess the distinctness of the two components i and i, we use the intersection of the two projected Gaussians located between their centers."}, {"heading": "4 Case Studies", "text": "In this section, we examine in detail the characteristics of the proposed measures by (1) analyzing correlations between these measures and durations, and (2) conducting four case studies that show how the measures can be applied in practice. These case studies show how measures can be used in the classifier's learning phase to improve classification performance in an active learning environment, while we evaluate the trained classifier prior to its online application and finally during the classifier's application phase. These case studies can be considered illustrative examples; many other ways of applying the measures are possible."}, {"heading": "4.1 Correlation Analysis and Run-Time", "text": "In the first series of experiments, we have correlations between the seven metrics to investigate their dependencies. Furthermore, we measure the duration of all the assessments we need to calculate these correlations in order to obtain some empirical evidence to support the theoretical runtimes specified in Section 3.2."}, {"heading": "4.2 Knowledge Acquisition Phase: Controlling the Training of a Classifier", "text": "In this set of experiments, we use some of our measures, meaning, and uncertainty to control the VI-based training processes of a classifier. Training consists of three steps, one step M, and an additional step that is able to capture the responsibilities. However, in the e-mail, the components of the classifier and the M-step are gradually assigned. The parameters of the classifier are updated according to the samples. In the ceremonial session, the unnecessary components are removed from the classification. Here, we focus on processing steps and consider three different processing methods: 1. This method is comparable to the traditional method used in VI training."}, {"heading": "4.3 Knowledge Analysis Phase: Ranking Components of a Trained Classifier", "text": "In this experiment, we analyze the components of a trained classifier with our measures to help a potential user of the classifier understand the components or rules from these components (cf. Section 3.1.3). We trained a classifier on the phoneme data from the UCL Machine Learning Group [64] with the restriction to diagonal covariance matrices to allow rule extraction. Fig. 6 shows the resulting classifier. Components are colored according to their class density and their opacity depending on their mixing coefficients. The background of the action is colored according to the rear probabilities of the classifier. The black, fixed line is the decision limit. In Table 5, some of our measures are evaluated for the components presented in Fig. 6 measures were missed for the benefit of the components that were missed."}, {"heading": "4.4 Knowledge Analysis in Active Learning", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "4.5 Knowledge Application Phase: Detecting Novel Processes", "text": "Our last set of experiments deals with the application phase of a classifier. We consider the task of novelty detection, i.e. the task of recognizing the need for new components that model emerging processes in the entrance area that were not known during the training period (cf. [43]). Our example is based on Intrusion Detection Data from the 1999 KDD Cup. Specifically, we use data of the attack types \"neptune,\" \"smurf,\" \"satan.\" We have re-sampled the data to consider longer periods of time. Each of the four data sets that we have constructed starts with 54 000 samples of background traffic, but without any attacks. Then, an attack phase begins, which lasts until the time."}, {"heading": "5 Conclusion and Outlook", "text": "In this article, we present an approach to support or automate the process of knowledge understanding, considering the proposed measures as a first step in this direction, since not all aspects of the real world can be taken into account. With the knowledge we need for the analysis of knowledge in order to collect knowledge, this can be done either (referred to here as \"understanding\") or online (referred to here as \"experience\"), applying the knowledge that all or only part of the measures can be useful. Also, it can be advantageous to assess continuous and categorical dimensions separately. The measures proposed in this article are objective, but they relate to subjective interests."}, {"heading": "Acknowledgment", "text": "This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) with the funding programme SI 674 / 9-1 (CYPHOC project)."}], "references": [{"title": "Online, http://www.encyclopediaofmath.org/index.php?title= Fubini_theorem&oldid=33052, (last access: 13.05.2016", "author": ["Fubini theorem"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Rough-fuzzy MLP: modular evolution, rule generation, and evaluation", "author": ["M. Atzmueller", "J. Baumeister", "F. Puppe"], "venue": "Proceedings of the 15th International Conference of Declarative Programming and Knowledge Management", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Evaluating the novelty of text-mined rules using lexical knowledge", "author": ["S. Basu", "R.J. Mooney", "K.V. Pasupuleti", "J. Ghosh"], "venue": "Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Novelty detection and neural network validation", "author": ["C. Bishop"], "venue": "IEE Proceedings \u2013 Vision, Image, and Signal Processing 141(4),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Interestingness \u2013 directing analyst focus to significant data", "author": ["M. Bourassa", "J. Fug\u00e8re", "D. Skillicorn"], "venue": "Proceedings of the 2011 European Intelligence and Security Informatics Conference (EISIC", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Domain-driven data mining: Challenges and prospects", "author": ["L. Cao"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Flexible frameworks for actionable knowledge discovery", "author": ["L. Cao", "Y. Zhao", "H. Zhang", "D. Luo", "C. Zhang", "E. Park"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "LIBSVM: a library for support vector machines. ACM transactions on intelligent systems and technology", "author": ["C. Chang", "C. Lin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory 13(1),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1967}, {"title": "Visualizing interestingness", "author": ["F. Di Fiore"], "venue": "(eds.) Data Mining III. WIT Press,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "A systematic approach to the assessment of fuzzy association rules. Data Mining and Knowledge Discovery", "author": ["D. Dubois", "E. Hullermeier", "H. Prade"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Introduction to scientific data mining: Direct kernel methods and applications", "author": ["M.J. Embrechts", "B. Szymanski", "K. Sternickel"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Knowledge discovery and data mining: Towards a unifying framework", "author": ["U.M. Fayyad", "G. Piatetsky-Shapiro", "P. Smyth"], "venue": "Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Techniques for Knowledge Acquisition in Dynamically Changing Environments", "author": ["D. Fisch", "M. J\u00e4nicke", "E. Kalkowski", "B. Sick"], "venue": "ACM Transactions on Autonomous and Adaptive Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Learning from others: Exchange of classification rules in intelligent distributed systems", "author": ["D. Fisch", "M. J\u00e4nicke", "E. Kalkowski", "B. Sick"], "venue": "Artificial Intelligence 187\u2013188,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "In your interest: Objective interestingness measures for a generative classifier", "author": ["D. Fisch", "E. Kalkowski", "B. Sick", "S.J. Ovaska"], "venue": "Proceedings of the 3rd International Conference on Agents and Artificial Intelligence (ICAART", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "So near and yet so far: New insight into properties of some well-known classifier paradigms", "author": ["D. Fisch", "B. K\u00fchbeck", "B. Sick", "S.J. Ovaska"], "venue": "Information Sciences", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Training of radial basis function classifiers with resilient propagation and variational Bayesian inference", "author": ["D. Fisch", "B. Sick"], "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Intelligente technische Systeme mit der F\u00e4higkeit zum kollaborativen Wissenserwerb. No. 1 in Intelligent Embedded Systems, kassel university", "author": ["D. Fisch"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Knowledge fusion for probabilistic generative classifiers with data mining applications", "author": ["D. Fisch", "E. Kalkowski", "B. Sick"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 26(3),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Sugli integrali multipli", "author": ["G. Fubini"], "venue": "Rend. Acc. Naz. Lincei 16,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1907}, {"title": "Entailment and symmetry in confirmation measures of interestingness", "author": ["D.H. Glass"], "venue": "Information Sciences 279,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Properties of rule interestingness measures and alternative approaches to normalization of measures", "author": ["S. Greco", "R. S\u0142owi\u0144ski", "I. Szcz\u0119ch"], "venue": "Information Sciences 216,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Self-adapting generative modeling techniques \u2013 a basic building block for many organic computing techniques", "author": ["C. Gruhl"], "venue": "Organic Computing Doctoral Colloquium", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "A concept for securing cyber-physical systems with organic computing techniques", "author": ["J. H\u00e4hner", "S. Rudolph", "S. Tomforde", "D. Fisch", "B. Sick", "N. Kopal", "A. Wacker"], "venue": "Proceedings of the 26th International Conference on Architecture of Computing Systems (ARCS", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A unified view of objective interestingness measures", "author": ["C. Hebert", "B. Cremilleux"], "venue": "Machine Learning and Data Mining in Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "J.T. Riedl"], "venue": "ACM Transactions on Information Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Evaluation of interestingness measures for ranking discovered knowledge", "author": ["R.J. Hilderman", "H.J. Hamilton"], "venue": "Proceedings of the 5th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Knowledge Discovery and Measures of Interest", "author": ["R.J. Hilderman", "H.J. Hamilton"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "A study on interestingness measures for associative classifiers", "author": ["M. Jalali-Heravi", "O. Zaiane"], "venue": "Proceedings of the 2010 ACM Symposium on Applied Computing", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Interestingness of frequent itemsets using Bayesian networks as background knowledge", "author": ["S. Jaroszewicz", "D.A. Simovici"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Variational Bayes for continuous hidden Markov models and its application to active learning", "author": ["S. Ji", "B. Krishnapuram", "L. Carin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Extracting interpretable fuzzy rules from rbf networks", "author": ["Y. Jin", "B. Sendhoff"], "venue": "Neural Processing Letters 17(2),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Association rule pruning based on interestingness measures with clustering", "author": ["S. Kannan", "R. Bhaskaran"], "venue": "International Journal of Computer Science Issues 6(1),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Automatic rule generation for protein annotation with the c4.5 data mining algorithm applied on swiss-prot", "author": ["E. Kretschmann", "W. Fleischmann", "R. Apweiler"], "venue": "Bioinformatics 17(10),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "On selecting interestingness measures for association rules: User oriented description and multiple criteria decision aid", "author": ["P. Lenca", "P. Meyer", "B. Vaillant", "S. Lallich"], "venue": "Computing, Artificial Intelligence and Information Management 184,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "A sequential algorithm for training text classifiers", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1994}, {"title": "Probabilistic measures for interestingness of deviations \u2013 a survey", "author": ["A. Masood", "S. Ouaguenouni"], "venue": "International Journal of Artificial Intelligence & Applications (IJAIA)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Measuring interestingness \u2013 perspectives on anomaly detection", "author": ["A. Masood", "S. Soong"], "venue": "Computer Engineering and Intelligent Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "A survey of interestingness measures for knowledge discovery", "author": ["K. McGarry"], "venue": "The Knowledge Engineering Review 20(1),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2005}, {"title": "Finite Mixture Models", "author": ["G. McLachlan", "D. Peel"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2000}, {"title": "Unexpectedness as a measure of interestingness in knowledge discovery", "author": ["B. Padmanabhan", "A. Tuzhilin"], "venue": "Decision Support Systems 27(3),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "On characterization and discovery of minimal unexpected patterns in rule discovery", "author": ["B. Padmanabhan", "A. Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "The interestingness of deviations", "author": ["G. Piatetsky-Shapiro", "C. Matheus"], "venue": "Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases (KDD", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1994}, {"title": "Fast training of support vector machines using sequential minimal optimization. In: Advances in Kernel Methods - Support Vector Learning", "author": ["J.C. Platt"], "venue": "MIT Press (January", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1998}, {"title": "Programs for Machine Learning", "author": ["R. Quinlan"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1993}, {"title": "Resp-knn: A semi-supervised classifier for sparsely labeled data in the field of organic computing", "author": ["T. Reitmaier", "A. Calma"], "venue": "Organic Computing Doctoral Dissertation Colloquium", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Let us know your decision: Pool-based active training of a generative classifier with the selection strategy 4DS", "author": ["T. Reitmaier", "B. Sick"], "venue": "Information Sciences 230,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}, {"title": "Transductive active learning \u2013 a new semi-supervised learning approach based on iteratively refined generative models to capture structure in data", "author": ["T. Reitmaier", "A. Calma", "B. Sick"], "venue": "Information Sciences 293,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "The responsibility weighted mahalanobis kernel for semi-supervised training of support vector machines for classification", "author": ["T. Reitmaier", "B. Sick"], "venue": "Information Sciences 323,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Pattern recognition and neural networks. http://www.stats.ox.ac.uk/pub/ PRNN/, [Online] (last access", "author": ["B. Ripley"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Interestingness measures for association rules based on statistical validity", "author": ["I. Shaharanee", "F. Hadzic", "T. Dillon"], "venue": "Knowledge-Based Systems", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "A framework for evaluating knowledge-based interestingness of association rules. Fuzzy Optimization and Decision Making", "author": ["B. Shekar", "R. Natarajan"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2004}, {"title": "In pursuit of interesting patterns with undirected discovery of exception rules", "author": ["E. Suzuki"], "venue": "Progress in Discovery Science,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2002}, {"title": "Evaluation and ordering of rules extracted from feedforward networks", "author": ["I. Taha", "J. Ghosh"], "venue": "Proceedings of the International Conference on Neural Networks", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1997}, {"title": "Behavior-based clustering and analysis of interestingness measures for association rule mining. Data Mining and Knowledge Discovery", "author": ["C. Tew", "C. Giraud-Carrier", "K. Tanner", "S. Burton"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Knowledge evaluation: Other evaluations: usefulness, novelty, and integration of interesting news measures", "author": ["A. Tuzhilin"], "venue": "Handbook of Data Mining and Knowledge Discovery", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2002}, {"title": "Interestingness measures in rule mining: A valuation", "author": ["J. Vashishtha"], "venue": "Garima Int. Journal of Engineering Research and Applications 4(7),", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}, {"title": "What do people want in microblogs? measuring interestingness of hashtags in twitter", "author": ["J. Weng", "E.P. Lim", "Q. He", "C.K. Leung"], "venue": "Proceedings of the 10th International Conference on Data Mining (ICDM", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "in [16].", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Figure 1: The data mining pyramid (adopted from [16]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "We use classifiers based on probabilistic mixture models (CMM, see [22, 6]) which can be used in many DM applications.", "startOffset": 67, "endOffset": 74}, {"referenceID": 4, "context": "We use classifiers based on probabilistic mixture models (CMM, see [22, 6]) which can be used in many DM applications.", "startOffset": 67, "endOffset": 74}, {"referenceID": 4, "context": "CMM are trained from data samples using expectation maximization or related techniques such as variational Bayesian approaches [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 18, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Data mining (DM), today often used as a synonym of knowledge discovery in databases (KDD), deals with the \u201cthe nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data\u201d [17].", "startOffset": 227, "endOffset": 231}, {"referenceID": 30, "context": ", [32, 33, 44, 58, 62]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 31, "context": ", [32, 33, 44, 58, 62]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 42, "context": ", [32, 33, 44, 58, 62]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 55, "context": ", [32, 33, 44, 58, 62]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 59, "context": ", [32, 33, 44, 58, 62]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 60, "context": ", on so-called information criteria or on data-based measurement techniques (see [65] for an overview).", "startOffset": 81, "endOffset": 85}, {"referenceID": 38, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 54, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 58, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 36, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 40, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 32, "context": ", [40, 57, 61, 38, 42, 34]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 2, "context": ", [4, 30, 37, 14]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 28, "context": ", [4, 30, 37, 14]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 35, "context": ", [4, 30, 37, 14]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 12, "context": ", [4, 30, 37, 14]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 2, "context": "Sometimes, several measures are combined [4, 60].", "startOffset": 41, "endOffset": 48}, {"referenceID": 57, "context": "Sometimes, several measures are combined [4, 60].", "startOffset": 41, "endOffset": 48}, {"referenceID": 25, "context": "In [27], interestingness measures for rules are evaluated with regard to the four properties of confirmation, locality, symmetry, and a property termed Ex1 which assures that conclusively confirmatory rules are assigned a higher interestingness value than non-conclusively confirmatory rules and vice versa.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Especially, in [27] weaker forms of the locality property and EX1 are proposed together with a new interestingness measure that fulfills the weaker forms of those properties.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Two further measures are proposed in [26] and compared to the measure from [27] in terms of the properties they fulfill.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "Two further measures are proposed in [26] and compared to the measure from [27] in terms of the properties they fulfill.", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Also, in [26] two new Bayesian confirmation measures for the evaluation of rule interestingness measures are proposed.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "An interestingness measure that does not evaluate rules or classifier components but individual samples is presented in [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 61, "context": "In [66], a support vector machine is used to learn interestingness values of Twitter hashtags from data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "Subjective measures consider additional knowledge about the application and information about the data miner such as skills and needs [49, 47].", "startOffset": 134, "endOffset": 142}, {"referenceID": 44, "context": "Subjective measures consider additional knowledge about the application and information about the data miner such as skills and needs [49, 47].", "startOffset": 134, "endOffset": 142}, {"referenceID": 3, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 45, "endOffset": 52}, {"referenceID": 15, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 45, "endOffset": 52}, {"referenceID": 15, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 109, "endOffset": 116}, {"referenceID": 7, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 109, "endOffset": 116}, {"referenceID": 11, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 137, "endOffset": 153}, {"referenceID": 33, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 137, "endOffset": 153}, {"referenceID": 56, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 137, "endOffset": 153}, {"referenceID": 45, "context": "Examples for subjective measures are novelty [5, 17], usefulness [17], understandability [17], actionability [10, 9], and unexpectedness [13, 35, 59, 48].", "startOffset": 137, "endOffset": 153}, {"referenceID": 1, "context": ", in a content-based approach or a collaborative filtering approach) [2, 31].", "startOffset": 69, "endOffset": 76}, {"referenceID": 29, "context": ", in a content-based approach or a collaborative filtering approach) [2, 31].", "startOffset": 69, "endOffset": 76}, {"referenceID": 4, "context": "A generative classifier aims at modeling the processes underlying the \u201cgeneration\u201d of the data [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 20, "context": "[22])", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": ", [15]) which roughly states that the sum of independent samples from any distribution with finite mean and variance converges to a normal distribution as the sample size goes to infinity.", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "Moreover, any continuous distribution can be approximated arbitrarily well by a finite mixture of normal densities [45].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "In general, these probabilistic generative classifiers offer some other interesting features: risk minimizing cost functions can easily be combined with probabilistic outputs, class priors can be compensated, different models can easily be combined, or anomaly detection techniques can be defined [6, 18].", "startOffset": 297, "endOffset": 304}, {"referenceID": 16, "context": "In general, these probabilistic generative classifiers offer some other interesting features: risk minimizing cost functions can easily be combined with probabilistic outputs, class priors can be compensated, different models can easily be combined, or anomaly detection techniques can be defined [6, 18].", "startOffset": 297, "endOffset": 304}, {"referenceID": 20, "context": "Here, we perform the parameter estimation by means of a technique called variational Bayesian inference (VI) which realizes the Bayesian idea of regarding the model parameters as random variables whose distributions must be trained [22].", "startOffset": 232, "endOffset": 236}, {"referenceID": 4, "context": "For a more detailed discussion on Bayesian inference, and, particularly, VI see [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "More details concerning the training algorithm can be found in [22].", "startOffset": 63, "endOffset": 67}, {"referenceID": 53, "context": "2 Classification Performance of CMM Compared to 1NN, SVM, and Decision Trees To confirm that CMM are comparable to other classification paradigms regarding their classification accuracy and, thus, sufficiently meaningful to define measures for knowledge understanding, we evaluate their performance on 21 benchmark data sets: phoneme, satimage (real-world data from the UCL Machine Learning Group [64]); australian, credit_a, credit_g, ecoli, glass, heart, iris, pendigits, pima, quality, seeds, segment, vehicle, vowel, wine, yeast (real-world data from the UCI Machine Learning Repository [3]); clouds (artificial data from the UCI Machine Learning Repository [3]); ripley (artificial data proposed in [56]); and two_moons (own, artificial data set).", "startOffset": 704, "endOffset": 708}, {"referenceID": 47, "context": "With the training algorithm Sequential Minimal Optimization (SMO) [50], Support Vector Machines (SVM) are one of the most widely used discriminative approaches for pattern recognition tasks.", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Consequently, we train SVM with the frequently used libsvm library using RBF (or Gaussian) kernels [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 48, "context": "5 [51], for instance.", "startOffset": 2, "endOffset": 6}, {"referenceID": 37, "context": ", [39]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "Even though it only classifies with respect to the nearest neighbor, it can be shown that for N \u2192\u221e the maximum classification error is at most twice the maximum of a classifier that yields the best possible classification [12].", "startOffset": 222, "endOffset": 226}, {"referenceID": 4, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[25, 1]) and considering the discrete nature of the multinomial distribution, the Bhattacharyya coefficient of two components i and i\u2032 as defined in Eq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[25, 1]) and considering the discrete nature of the multinomial distribution, the Bhattacharyya coefficient of two components i and i\u2032 as defined in Eq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "To scale the importance of a component to the interval [0, 1] we additionally use a boundary function that is comprised of two linear functions.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "Suitable values of h depend on the data set X [6], but there are a number of heuristics to estimate h.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "In [7], for instance, h is set to the average distance of", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "The centers \u03bci and covariance matrices \u03a3i of the continuous dimensions are modeled with Gaussian-Inverse-Wishart distributions [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "To quantify the uncertainty of these parameter estimates we use the entropy H [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "[6, 36], for instance.", "startOffset": 0, "endOffset": 7}, {"referenceID": 34, "context": "[6, 36], for instance.", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[6])", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "More details on training models using second-order distributions can be found in [6, 22, 20].", "startOffset": 81, "endOffset": 92}, {"referenceID": 20, "context": "More details on training models using second-order distributions can be found in [6, 22, 20].", "startOffset": 81, "endOffset": 92}, {"referenceID": 18, "context": "More details on training models using second-order distributions can be found in [6, 22, 20].", "startOffset": 81, "endOffset": 92}, {"referenceID": 0, "context": "Since importance values lie in the interval [0, 1], thresholds in that interval may be considered.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "It can be seen that the Resp method yields the best results for low thresholds in the range [1, 3] but still requires a larger number of VI steps than the other methods.", "startOffset": 92, "endOffset": 98}, {"referenceID": 39, "context": "4 Knowledge Analysis in Active Learning The pool-based active learning (PAL) [41] paradigm repetitively asks users (generally termed as oracles) to provide label information for unlabeled data, e.", "startOffset": 77, "endOffset": 81}, {"referenceID": 51, "context": "In [54], this process is extended with a transductive learner, which aims at updating the underlying model using the uniqueness measure, which determines how ambiguous the knowledge modeled by each component is.", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "17) are determined, a sample-based classifier called Resp-kNN [52] is trained and used to transductively label the underlying samples.", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "Then, a new CMM is trained and the resulting components fused with \u201cnon-disputed\u201d components, where necessary(for details of the fusion technique, see [24]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 51, "context": "For further information regarding PAL with a transductive learner see [54].", "startOffset": 70, "endOffset": 74}, {"referenceID": 52, "context": "For the case study presented in this section, we embed the probabilistic generative model in a kernel function of a support vector machine (SVM) as described in [55].", "startOffset": 161, "endOffset": 165}, {"referenceID": 50, "context": "The PAL process started with an initially labeled set of 40 samples and selected, based on the 4DS [53] selection strategy (corresponds to Q in Fig.", "startOffset": 99, "endOffset": 103}, {"referenceID": 41, "context": "[43]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Figure 9: Novelty detection with our representativity measure (top) in comparison to the \u03c72 novelty detector as described in [23] (bottom) on 4 intrusion detection data sets.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "9 (b) contains results obtained by the novelty detection technique called \u03c72 novelty which is proposed in [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "As we have shown in [21], the probabilistic classifiers with continuous input dimensions are functionally equivalent to certain kinds of support vector machines (with Gaussian kernels), radial basis function neural networks, fuzzy classifiers (with Gaussian membership functions and sum-prod inference), nonlinear Fisher discrimination techniques, relevance vector machines, or direct kernel machines.", "startOffset": 20, "endOffset": 24}, {"referenceID": 52, "context": "Another option is to use the probabilistic model contained in our CMM in other classifiers, for instance, as we have already shown in [55], to create a data dependent kernel function for SVM.", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "We have shown that these measures may be used to control the training process of the classifier (for more details on this problem see also [28]), to analyze the knowledge contained in a trained classifier, and to support tasks such as novelty detection in the application phase of the classifier.", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": ", [20]), concept drift or obsoleteness detection.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": ", by assessing its usefulness as in [19, 18]), extending the VI training techniques to apply them to large data sets, and on applications of the measures, e.", "startOffset": 36, "endOffset": 44}, {"referenceID": 16, "context": ", by assessing its usefulness as in [19, 18]), extending the VI training techniques to apply them to large data sets, and on applications of the measures, e.", "startOffset": 36, "endOffset": 44}, {"referenceID": 27, "context": ", in the field of collaborative intrusion detection in cyber-physical systems [29].", "startOffset": 78, "endOffset": 82}], "year": 2016, "abstractText": "After data selection, pre-processing, transformation, and feature extraction, knowledge extraction is not the final step in a data mining process. It is then necessary to understand this knowledge in order to apply it efficiently and effectively. Up to now, there is a lack of appropriate techniques that support this significant step. This is partly due to the fact that the assessment of knowledge is often highly subjective, e.g., regarding aspects such as novelty or usefulness. These aspects depend on the specific knowledge and requirements of the data miner. There are, however, a number of aspects that are objective and for which it is possible to provide appropriate measures. In this article we focus on classification problems and use probabilistic generative classifiers based on mixture density models that are quite common in data mining applications. We define objective measures to assess the informativeness, uniqueness, importance, discrimination, representativity, uncertainty, and distinguishability of rules contained in these classifiers numerically. These measures not only support a data miner in evaluating results of a data mining process based on such classifiers. As we will see in illustrative case studies, they may also be used to improve the data mining process itself or to support the later application of the extracted knowledge.", "creator": "LaTeX with hyperref package"}}}