{"id": "1605.06394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Bayesian Hyperparameter Optimization for Ensemble Learning", "abstract": "In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization.", "histories": [["v1", "Fri, 20 May 2016 15:08:08 GMT  (2355kb,D)", "http://arxiv.org/abs/1605.06394v1", "To appear in proceedings of UAI 2016"]], "COMMENTS": "To appear in proceedings of UAI 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["julien-charles l\\'evesque", "christian gagn\\'e", "robert sabourin"], "accepted": false, "id": "1605.06394"}, "pdf": {"name": "1605.06394.pdf", "metadata": {"source": "CRF", "title": "Bayesian Hyperparameter Optimization for Ensemble Learning", "authors": ["Julien-Charles L\u00e9vesque", "Christian Gagn\u00e9", "Robert Sabourin"], "emails": ["julien-charles.levesque.1@ulaval.ca"], "sections": [{"heading": null, "text": "In this thesis, we build a bridge between hyperparameter optimization and ensemble learning by performing Bayean optimization of an ensemble with respect to its hyperparameters. Our method consists of building an ensemble of fixed size, optimizing the configuration of a classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into account the interaction with the other models in evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase by greedily selecting from the pool of models generated during optimization. We examine the performance of our proposed method in three different hyperparameter spaces and show that our approach is better than both the best single model and a greedy ensemble construction over the models generated by a Bayesian standard optimization."}, {"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 HYPERPARAMETER OPTIMIZATION", "text": "The behavior of a learning algorithm A is often adjustable with respect to a number of external parameters (called hyperparameters \u03b3 = {\u03b31, \u03b32,..} that are not learned during training. Hyperparameter selection problem is a stage of bi-level optimization problem where the first goal is the tuning of the parameters of the model and the second goal is performance with respect to the hyperparameters \u03b3.The procedure requires two datasets, one for training and one for hyperparameter optimization (also known as validation), namely XT and XV, which are each assumed to be evaluated by an underlying distribution. The objective function to minimize hyperparameter optimization takes the form of the empirical generalization error on XV: f (GP) = L (hGP), XV) + (1) L (hGP), XV | XV) = 1 l0 \u2212 1 (hGP)."}, {"heading": "2.1 HYPERPARAMETER OPTIMIZATION AND ENSEMBLES", "text": "The idea of creating ensembles with hyperparameter optimization has already received some attention. Bergstra and Cox (2013) have applied hyperparameter optimization in a multi-step approach to generate better representations of images. Lacoste et al. (2014b) have proposed sequential model-based ensemble optimization (SMBEO) to optimize ensembles based on bootstrapping of validation datasets to simulate multiple independent hyperparameter optimization processes and to combine the results with the agnostic Bayesian combination. The process of hyperparameter optimization generates many trained models and is usually compared by selecting a model after the hold-out (or cross-validation) generalization error."}, {"heading": "3 ENSEMBLE OPTIMIZATION", "text": "In this work, we aim directly at optimizing an ensemble of classifiers through Bayesian Hyperparameter Optimization (E = E). The strategies discussed in the previous section are mostly aimed at reusing the product of a completed hyperparameter optimization in retrospect. The goal is to make an online selection of hyperparameters that might be more interesting for an ensemble, but that do not necessarily maximize the objective function of Equation 1 in their own way. To directly position a model in the space of all possible ensembles of a given size f (E) = f (A) (A) (A) would lead to a very hard and inefficient optimization problem that effectively duplicates the formation of many models. To palliate this model, we propose a more focused approach to be the performance of a given ensemble. We define the objective function to be the performance of a given ensemble E when it is combined with a new ensemble."}, {"heading": "3.1 ALTERNATE FORMULATIONS", "text": "In order to be able to generalize a problem across the space of hyperparameters (Tsymbolic problem), it is crucial to have an overall set that does not contain all the classifiers in H, because if it did, there would be no information in the calculation of Equation 5. Another formulation of the problem could be derived that compares classifiers with the entire pool of trained models, which would take the form f (\u03b3 | H) = q (h\u03b3 | H, XV), where q (\u00b7) is an indicator of performance for a classifier in relation to the pool. For example, a variety of metric values would provoke dissent such as pairwise (Kuncheva, 2004), but this would lead to degenerated pools of classifiers, since diversity is easily increased by trivial and degenerated classifiers (all for one class or the other). Multi-objective optimization approaches have been taken to maximize both diversities and accuracy, which is typically solved with an algorithmic problem."}, {"heading": "3.2 ENSEMBLE UPDATE", "text": "In this section, we will address the problem of building and updating the ensemble as the optimization progresses. To make things easier, an ensemble size m will be specified beforehand - this number can be refined at the end of the optimization process. Each iteration of the optimization process will include two steps: first, the evaluation as described in Section 3, and then an ensemble update step. As the members of the ensemble move forward and better models are found, a round-robin strategy will be used for building the ensemble. Indeed, the ensemble will consist of m fixed positions, and in each iteration i, the classifier at position j = (i mod m) will be removed from the ensemble before hyperparameters are found that minimize equation 5 - effectively optimizing the classifier at this point."}, {"heading": "3.3 COMPUTATIONAL COMPLEXITY", "text": "The computational effort of the proposed method results mainly from the evaluation of the empirical error of ensembles. It is very low in terms of the cost of executing most learning algorithms (which are usually square or inferior in the number of samples) and also in terms of the cost of conditioning the probable model to the observations (which are cubic in the number of iterations). Calculation of the empirical error of ensembles occurs in step 5 of algorithm 1. In view of an ensemble of sizem, a validation dataset of size n and a history of trained classifiers of size t, the complexity of this step is O (t (mn + n) = O (\u207b), since it requires a run over all models in history, and for each of them the combination of the classifiers by majority tuning (mn) and the calculation of the empirical error is no."}, {"heading": "3.4 LOSS FUNCTION", "text": "In fact, it is a very rare disease that is able to unfold, \"he told the German Press Agency.\" But it is too early to unfold, \"he said.\" It is too early to say whether it is a disease or a disease. \""}, {"heading": "4 EXPERIMENTS", "text": "The first two problems consist of different algorithms and hyperparameter spaces evaluated on the same benchmark of media sets originating from the UCI repository, represented in Table 1. The remaining instances of the data set were used to complete a 5-fold crossing procedure. Final models are retrained on all available data for training and validation. In any case, the previous function is a Gaussian Process (GP) with Mate-52 kernel that uses automatic relevance determination."}, {"heading": "4.1 SVM SEARCH SPACE", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 SCIKIT-LEARN MODELS SEARCH SPACE", "text": "The results of the study are not yet known, but they are not yet in full swing."}, {"heading": "4.3 CONVOLUTIONAL NEURAL NETWORKS", "text": "Finally, we evaluated the performance of our approach in fine-tuning the parameters of a Convolutionary Neural BO Network for the CIFAR-10 dataset. To have a reproducible baseline, we used the implementation of the Cuda convex with the specified reference model files, which achieved an 18% generalization error on the test datasets3. A batch of training data was set aside for validation (batches 1-4 for training, 5 for validation, and 6 for testing).The performance of the baseline configuration on the given training packages was approximately 22.4% \u00b1 0.9 for 250 training phases. The optimized parameters were the same as in (Snoek et al., 2012), namely the learning rates and weight decreases for the conversion and the Softmax layers, and the parameters of the local reaction normalization layer (Rescuensemble)."}, {"heading": "5 CONCLUSION", "text": "In this paper, we presented a method for Bayesian optimization of ensembles by hyperparameter optimization. We address the various challenges posed by ensemble optimization in this context, and the result is an optimization strategy that is capable of leveraging trained models and generating better ensembles of classifiers at the computational cost of regular hyperparameter optimization. We demonstrate the performance of our approach on three different problem suites, and in all cases show a significant difference in generalization accuracy between our approach and post-hoc ensembles based on classic hyperparameter optimization, such as Wilcoxon signature rank tests. This is a strong confirmation of our method, especially considering that it requires little additional computation."}, {"heading": "Acknowledgements", "text": "This research benefited from the computational resources of Calcul Que'bec, Compute Canada and Nvidia. We would also like to thank Annette Schwerdtfeger for proofreading this paper."}], "references": [{"title": "Lessons from the Netflix prize challenge", "author": ["Bell", "Robert M.", "Yehuda Koren"], "venue": "In: ACM SIGKDD Explorations Newsletter 9.2, pp. 75\u201379. Bergstra, James and Yoshua Bengio (2012). \u201cRandom Search for Hyper-Parameter Optimization\u201d. In: Journal of", "citeRegEx": "Bell et al\\.,? 2007", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a \u201c Null ", "author": ["Bergstra", "James", "David D. Cox"], "venue": "Machine Learning Research", "citeRegEx": "Bergstra et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2013}, {"title": "Ensemble Selection from Libraries of Models", "author": ["Learning. Caruana", "Rich", "Alexandru Niculescu-Mizil", "Geoff Crew", "Alex Ksikes"], "venue": "In: Proceedings of the 21st International Conference on Machine Learning, p. 9.", "citeRegEx": "Caruana et al\\.,? 2004", "shortCiteRegEx": "Caruana et al\\.", "year": 2004}, {"title": "Statistical Comparisons of Classifiers over Multiple Data Sets", "author": ["Dem\u0161ar", "Janez"], "venue": "In: The Journal of Machine Learning Research 7, pp. 1\u201330. Didaci, Luca, Giorgio Fumera, and Fabio Roli (2013). \u201cDiversity in Classifier Ensembles : Fertile Concept or Dead", "citeRegEx": "Dem\u0161ar and Janez,? 2006", "shortCiteRegEx": "Dem\u0161ar and Janez", "year": 2006}, {"title": "Efficient and Robust Automated Machine Learning", "author": ["Feurer", "Matthias", "Aaron Klein", "Katharina Eggensperger", "Jost Tobias Springenberg", "Manuel Blum", "Frank Hutter"], "venue": "Multiple Classifier Systems,", "citeRegEx": "Feurer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feurer et al\\.", "year": 2015}, {"title": "Initializing Bayesian Hyperparameter Optimization via Meta-Learning", "author": ["tems. Feurer", "Matthias", "Jost Tobias Springenberg", "Frank Hutter"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "Feurer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feurer et al\\.", "year": 2015}, {"title": "UCI Machine Learning Repository", "author": ["A. Frank", "A. Asuncion"], "venue": "URL: https://archive.ics.uci. edu/ml/datasets.html. Guyon, Isabelle, Kristin Bennett, Gavin Cawley, Hugo Jair Escalante, Sergio Escalera, Tin Kam Ho, N\u00faria Maci\u00e0,", "citeRegEx": "Frank and Asuncion,? 2010", "shortCiteRegEx": "Frank and Asuncion", "year": 2010}, {"title": "Design of the 2015 ChaLearn AutoML Challenge", "author": ["Bisakha Ray", "Mehreen Saeed", "Alexander Statnikov", "Evelyne Viegas"], "venue": "In: 2015 International Joint Conference on Neural Networks (IJCNN). ChaLearn. Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel, Matthew W. Hoffman,", "citeRegEx": "Ray et al\\.,? 2015", "shortCiteRegEx": "Ray et al\\.", "year": 2015}, {"title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions", "author": ["Zoubin Ghahramani"], "venue": "Advances in Neural Information Processing", "citeRegEx": "Ghahramani,? \\Q2014\\E", "shortCiteRegEx": "Ghahramani", "year": 2014}, {"title": "Agnostic Bayesian Learning of Ensembles", "author": ["Lacoste", "Alexandre", "Hugo Larochelle", "Mario Marchand", "Fran\u00e7ois Laviolette"], "venue": "In: Proceedings of the 31st International Conference on Machine Learning 32. Lacoste, Alexandre, Hugo Larochelle, Mario Marchand,", "citeRegEx": "Lacoste et al\\.,? 2014a", "shortCiteRegEx": "Lacoste et al\\.", "year": 2014}, {"title": "Boosting: Foundations and Algorithms", "author": ["Press. Schapire", "Robert E.", "Yoav Freund"], "venue": "MIT Press. Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams (2012). \u201cPractical Bayesian Optimization of Machine", "citeRegEx": "Schapire et al\\.,? 2012", "shortCiteRegEx": "Schapire et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Feurer et al. (2015a) performed post-hoc ensemble generation by reusing the product of a completed hyperparameter optimization, winning phase 1 of the ChaLearn AutoML challenge (Guyon et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Feurer et al. (2015a) performed post-hoc ensemble generation by reusing the product of a completed hyperparameter optimization, winning phase 1 of the ChaLearn AutoML challenge (Guyon et al., 2015). Lastly, Snoek et al. (2015) also constructed post-hoc ensembles of neural networks for image captioning.", "startOffset": 0, "endOffset": 227}, {"referenceID": 4, "context": "In order to speed up convergence, Feurer et al. (2015b) have shown that hyperparameter optimization can be warm started with meta features about datasets.", "startOffset": 34, "endOffset": 56}, {"referenceID": 9, "context": "Lacoste et al. (2014b) proposed the Sequential Model-based Ensemble Optimization (SMBEO) method to optimize ensembles based on", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Forward greedy selection has been shown to perform well in the context of pruning a pool of classifiers (Caruana et al., 2004).", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "In this case, the combination rule is majority voting, as it is less prone to overfitting (Caruana et al., 2004; Feurer et al., 2015a).", "startOffset": 90, "endOffset": 134}, {"referenceID": 9, "context": "Other possible combination rules include weighted voting, stacking (Kuncheva, 2004) and agnostic Bayesian combination (Lacoste et al., 2014a), to name only a few.", "startOffset": 118, "endOffset": 141}, {"referenceID": 6, "context": "The dataset used is the Pima Indian Diabetes dataset available from UCI (Frank and Asuncion, 2010), with separate training and validation splits.", "startOffset": 72, "endOffset": 98}, {"referenceID": 2, "context": "greedy selection \u2013 this form of warm starting is recommended in (Caruana et al., 2004) to reduce overfitting.", "startOffset": 64, "endOffset": 86}], "year": 2016, "abstractText": "In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization.", "creator": "LaTeX with hyperref package"}}}