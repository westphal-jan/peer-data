{"id": "1605.06402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-of-art networks require billions of arithmetic operations and millions of parameters. To enable embedded devices such as smartphones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference. We present Ristretto, a fast and automated framework for CNN approximation. Ristretto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy. Since training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network. Given a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.", "histories": [["v1", "Fri, 20 May 2016 15:22:29 GMT  (1632kb,D)", "http://arxiv.org/abs/1605.06402v1", "Master's Thesis, University of California, Davis; 73 pages and 29 figures"]], "COMMENTS": "Master's Thesis, University of California, Davis; 73 pages and 29 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["philipp gysel"], "accepted": false, "id": "1605.06402"}, "pdf": {"name": "1605.06402.pdf", "metadata": {"source": "CRF", "title": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks", "authors": ["Philipp Matthias Gysel"], "emails": [], "sections": [{"heading": null, "text": "Ristretto: Hardware-oriented approximation by Convolutional Neural Networks ksByPhilipp Matthias Gysel B.S. (Bern University of Applied Sciences, Switzerland) 2012Thesis Professor S. Ghiasi, Chairman Professor J. D. OwensProfessor V. AkellaProfessor Y. J. LeeCommittee in Charge2016-i-ar Xiv: 160 5.06 402v 1 [cs.C V] 20 May 201 6Copyright c \u00a9 2016 by Philipp Matthias GyselsProfessor V. AkellaProfessor Y. J. LeeCommittee in Charge2016-i-ar Xiv: 160 5.06 402v 1 [cs.C. V] 20 May 201 6Copyright c \u00a9 2016 by Philipp Matthias GyselsProfessor. All rights reserved."}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Convolutional Neural Networks 3", "text": "2.1 Training and inference......................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Related Work 19", "text": "3.1 Network approximation..................................................................................................."}, {"heading": "4 Fixed Point Approximation 25", "text": "4.1 Baseline Convolutionary Neural Networks........................ 25 4.2 Fixed point format.........................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Dynamic Fixed Point Approximation 32", "text": ".............................................................................................................."}, {"heading": "6 Minifloat Approximation 38", "text": "6.1 Motivation................................................ 38 6.2 IEEE-754 Single Precision Standard.................. 38 6.3 Minifloat Number Format......................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7 Turning Multiplications Into Bit Shifts 43", "text": "7.1 Multiplier-free arithmetic......................................... 43 7.2 Maximum number of shifts............................. 44 7.3 Data path for the accelerator.................................................................................................................................................................................................................................................."}, {"heading": "8 Comparison of Different Approximations 47", "text": "..........................................................................................................................................."}, {"heading": "9 Ristretto: An Approximation Framework for Deep CNNs 51", "text": "The number of unemployed in Germany increased slightly compared to the previous year, while the number of unemployed in Germany rose slightly compared to the previous year."}, {"heading": "2.1 Training and Inference", "text": "Convolutionary neural networks have two calculation phases: Forward propagation is used for classification and backward propagation for training. Like other machine learning algorithms, CNNs use a supervised training method to find network parameters that provide good classification accuracy. In this thesis, we use the terms parameters and weights interchangeable.3"}, {"heading": "2.1.1 Forward Propagation", "text": "A typical CNN architecture, called AlexNet, is shown in Figure 2.1. For input into the network, we use an RGB image measuring 224 x 224. The first five layers in AlexNet are revolutionary layers, and the last three layers are fully interconnected layers. Convolutionary layers use 2D filters to extract properties from the input. The first revolutionary layer generates 2 x 48 feature cards, each representing the presence or absence of a low-level feature in the input image. To reduce the spatial dimension of feature cards, and to add translation and inventory layers, combined layers are used to perform sub-sampling. In addition, a non-linear layer is added that enables the network to learn non-linear layers. These three x 1000 layers are weighted by the last layer 12 x 13."}, {"heading": "2.1.2 Backward Propagation", "text": "Training a deep CNN requires thousands or even millions of labeled images. The network is exposed to these images, and the network parameters are gradually updated to compensate for prediction errors. The purpose of backward propagation is to find the error gradient relative to each network parameter. In a later step, this error gradient is used for a parameter update.The training takes place in stacks of images. Multiple images are executed by the network in forward process. naming x as network input, w as network parameter, and f as the entire CNN function, the network output is given by z \u2032 = f (x, w). Since all images are labeled, the desired network output z is known. Given many image pairs and ground truth (x1, z1),... (xn, zn) we define a loss function l (z, z \u2032) which is the penalty for predicting z \u2032 s instead of currently losing the stack of images, deriving the parameters of updating and updating."}, {"heading": "2.2 Layer Types", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2.3 Applications", "text": "Deep Convolutionary Neural Networks have pushed the boundaries of artificial intelligence in a variety of applications, and the recent winners of the ImageNet competition (Krizhevsky et al., 2012; Simonyan et al., 2015; Szegedy et al., 2015; He et al., 2015) have continuously improved the machine's image classification capabilities, with the recent winners even surpassing human vision. In addition to image classification, deep networks feature cutting-edge capabilities in object recognition (Girshick et al., 2015) and speech recognition (Hinton et al., 2012), as well as games (Silver et al., 2016) and art (Gatys et al., 2015)."}, {"heading": "2.4 Computational Complexity and Memory Require-", "text": "The complexity of deep CNNs can be divided into two parts: First, the revolutionary layers contain more than 90% of the required arithmetic operations; second, resource-intensive layers are fully interconnected layers that contain over 90% of the network parameters; an energy-efficient accelerator for CNNs must 1) provide sufficient computing throughput; and 2) provide a memory bandwidth that employs the processing elements. CaffeNet is the caffe conversion of AlexNet by layer type by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet dataset, which includes 1000 image classes. Figure 2.6 shows the required arithmetic operations and the parameter size of AlexNet by layer type. Most of the arithmetic operations originate from conventional layers: This layer type requires a total of 2.15 G operations; the arithmetic operations in all other layers add up to these 2MB of conventional caffeine operations, where most of the 2MB of these arithmetic operations are fully related."}, {"heading": "2.5 ImageNet Competition", "text": "The ILSVRC competition (Russakovsky et al., 2015) is a major image classification and recognition challenge that has been held annually since 2010. More than fifty institutions have participated in the challenge, including companies such as Google, Microsoft, Qualcomm, and various universities. In the first year, the competition consisted solely of an image classification challenge. In the meantime, several new challenges have been introduced, such as object recognition and localization in images, object recognition from videos, and scene classification. For classifier training, the ILSVRC provides the ImageNet dataset, which originally consisted of 1,000 image categories and a total of 1.2 million images. In the meantime, this database has been substantially expanded and currently includes over 14 million labeled images. A common performance measurement for deep CNNs is its classification accuracy on the ILSVRC dataset 2012. For this challenge, the classifier is trained on a training dataset and tested on a validation dataset."}, {"heading": "2.5.1 Network Size vs Accuracy", "text": "The winner of the 2014 localization challenge (Simonyan and Zisserman, 2015) experimented with 12 with different depths for his network. From 11 parameter levels to 19, they improved their top-1 classification by over 4%. Another experiment by the 2015 winner (He et al., 2015) used very deep networks. Their network architecture improved by over 2% as the network expanded from 34 to 152 levels. On the other hand, some research shows that even relatively small networks can achieve good classification performance. In the 2014 classification challenge, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity smaller than 19X. The GoogLeNet network is based on the conception idea described in Section 2.5.2. A newer network architecture by Iandola et al. (2016) uses an adapted concept for trading with smaller volumes."}, {"heading": "2.5.2 Inception Idea", "text": "The founding idea is a concept proposed by Szegedy et al. (2015), which was used to build the GoogLeNet architecture. GoogLeNet was developed with the aim of achieving high classification accuracy on the ImageNet dataset with a budget of 1.5 billion MAC operations for inferences. Authors avoid sparse representations and opt for readily available, dense components. The GoogLeNet architecture contains several replicas of the Inceptions module shown in Figure 2.8. An Inceptions module contains 1 x 1, 3 x 3 and 5 x 5 folding cores. To reduce the number of feature maps and thus the complexity of calculations, 1 rotation is added before the expensive 3 x 3 and 5 x 5 rotations. In addition, the Inceptions module contains an alternative max pooling path."}, {"heading": "2.6 Neural Networks With Limited Numerical Pre-", "text": "Most deep learning frameworks (Jia et al., 2014; Theano Development Team, 2016; Abadi et al., 2015) use 32-bit or 64-bit floating points for CNN training and conclusions. However, it has been shown (Du et al., 2015) that CNNs have relatively high error resistance and CNNs can be trained in a separate parameter space. In the following subsections, we describe the process of quantifying a complete precision network to limited precision numbers. In addition, we present various rounding schemes for the quantization step and describe various options for optimizing the classification accuracy of a limited precision network."}, {"heading": "2.6.1 Quantization", "text": "As discussed earlier, the aim of this work is to provide a framework for approximating the forward trajectory of a given CNN. To this end, we compress the number format into sinuous and fully connected layers. These two layer types, which represent the most resource-intensive part of a deep network, require the same arithmetic operations, namely a series of multiplication and accumulation data (MAC). In this work, we simulate the arithmetic of a hardware accelerator. The simulated data path is shown in Figure 2.9. The difference between this simulated data path and the original full precision data path is the quantization step of weights, layer inputs and layer outputs. Therefore, the condensed networks suffer from quantization errors that can affect network accuracy. In this work, we propose a framework that can approximate the 32-bit floating networks by condensed, quantified values."}, {"heading": "2.6.1.1 Data path in hardware", "text": "To exploit the inherent sources of parallelism, a hardware accelerator uses many of these units in parallel. Each multiplication unit receives one layer input value and one network weight per calculation round. The different results are collected in an addition tree, and the final sum is the layer output. Note that some of the layer input and weight values are actually reused for different multipliers, depending on the exact characteristics of the layer in question. To reduce the number of bits required for numerical representation, our approximation frame quantifies both the layer inputs and the weight values. The resulting values can be represented with significantly fewer bits. As a result, the multiplication units require less space. To simplify the simulation of the hardware, our framework uses 32-bit floating point for the accumulation. To achieve the same result in a hardware accelerator, the addition tree should use 32 bits."}, {"heading": "2.6.2 Rounding Schemes", "text": "Various rounding schemes can be used for quantifying values.Round nearest even: Round nearest is an unbiased scheme rounding to the nearest discrete value. Described as a quantization step quantity and bxc as the largest quantization value smaller or equal to x, Gupta et al. (2015) define round nearest as follows: Round (x) = bxc, if bxc \u2264 x + 2bxc +, if bxc + 2 < x \u2264 x + (2.7) Since round is even deterministic, we chose this rounding scheme for inference, i.e. at test points, all parameters are deterministically rounded, and the same applies to layer output. Round pochastically: Another scheme known as stochastic rounding was explained by Gupta et al. (2015) for the weight updates of 16-bit neural networks as an average quantification. Round is explained as circular chastic, during this round."}, {"heading": "2.6.3 Optimization in Discrete Parameter Space", "text": "The formation of neural networks can be seen as an optimization problem where the goal is to find the optimal set of parameters that minimize the classification error for a given set17 of the images. A practical solution to this problem is the use of stochastic gradient descent, as explained in Section 2.1.1. In the traditional setting of 64-bit floating-point training, this optimization is a continuous problem with a smooth interface. The interface of neural networks depends on their input and current parameters. In quantized networks, this error interface is discrete. This new optimization problem - where the goal is to find an optimal set of discretely evaluated parameters - is a NP-hard problem. An approach to find a good set of discrete parameters is to train the limited precision \"from the ground up.\" In this approach, we would train the network with quantized parameters discredited from the outset."}, {"heading": "3.1 Network Approximation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Fixed Point Approximation", "text": "Traditionally, neural networks are formed in 32-bit floating-point numbers. However, fixed-point arithmetics is less resource-hungry than floating-point arithmetics. Furthermore, fixed-point arithmetics has been shown to be suitable for calculating neural networks, an observation recently used to condense deep CNNs. Gupta et al. (2015) show that networks can be trained in 16-bit on datasets such as CIFAR-10 (10 image classes). Further trimming of the same network uses only 7-bit multipliers (Courbariaux et al., 2014). Another approach by Courbariaux et al. (2015) uses binary weights only, again on the same network. A similar proposal represents the weights of the same network with + 1, 0 and -1 values (Sung et al., 2015)."}, {"heading": "3.1.2 Network Pruning and Shared Weights", "text": "Since deep CNNs typically have more than 10 MB of parameters, compressing the size of the network parameters is an important step. The deep compression pipeline proposed by Han et al. (2016b) addresses this problem. The authors achieve a compression rate of network parameters up to 49X for deep CNNs using a three-stage pipeline. As a first step, the \"unimportant\" connections of a trained network are removed, the resulting sparse network is then retrained to regain its classification accuracy, and the cut step is repeated. After some iterations of editing and fine-tuning, the remaining parameters are bundled into common weights. These common weights are refined to find optimal centring. In a final step, a lossless data compression scheme (Huffman Coding) is applied to the final weights."}, {"heading": "3.1.3 Binary Networks", "text": "This motivated BinaryConnect (Courbariaux et al., 2015), a paper that presents weights in binary format rather than the traditional 32-bit floating point. However, this approach reduces the parameter size by a factor of 32X and eliminates the need for forward multiplication. BinaryConnect achieves almost state-of-the-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN). A later paper by Lin et al. (2016) takes this idea a step further by converting backward multiplications into bit shifts. Grid activations are approximated by integrating 2 numbers while maintaining error gradients with full precision. This proposal significantly reduces the hardware requirements for binary operations."}, {"heading": "3.2 Accelerators", "text": "Various accelerator platforms have been used to accelerate CNN conclusions. Below, we review the proposed accelerators for GPUs, FPGAs, and ASICs."}, {"heading": "3.2.1 GPU Acceleration", "text": "Given the high throughput and memory bandwidth of today's GPUs, various research focused on the acceleration of GPU-based neural network inference. A proposal by Denton et al. (2014) uses cluster filters and an approximation to the low rank. They achieve an acceleration of 2X for Convolutionary Layers of AlexNet compared to an unoptimized GPU implementation. Another paper by Mathieu et al. (2013) achieves better results by replacing the conversion with FFT. Finally, the Neural Network Compression Pipeline proposed by Han et al. (2016b) uses intersection and weight distribution. When applied to dense layers of AlexNet, forward propagation is 4x faster and 3x more energy efficient. In their later paper (Han et al., 2016a), they show a Titan X-based GPU implementation with a throughput of 3.23 TOPs of high-end GUs requiring an average 2W of energy."}, {"heading": "3.2.2 FPGA-based Accelerators", "text": "FPGAbase accelerators have a shorter development time than ASICs, but can't keep up with the throughput of GPUs. Various FPGA-based accelerators for neural networks have already been proposed. An approach by Zhang et al. (2015) uses Vivado HLS to accelerate the revolutionary layers of AlexNet. Their floating-point implementation achieves a throughput of over 60 GFLOPs with a power budget of 20 W. A follow-up proposal by Suda et al. (2016) uses OpenCL to implement entire VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board. Their throughput-optimized design achieves a total throughput of 117.8 GOPs. Finally, a recent xilinx-based implementation (Qiu et al., 2016) achieves the start-of-art throughput of 137 GOPs."}, {"heading": "3.2.3 Custom Accelerators (ASIC)", "text": "The DaDianNao by Chen et al. (2014) is a supercomputer for machine learning at 28 nm technology. Its chip is based on large on-chip memory (which occupies almost half the surface area) and achieves significant acceleration and energy savings compared to the GPU. A later implementation, called Eyeriss (Chen et al., 2016), can execute the convolutionary layers of AlexNet at 34 frames per second (74.6 GOPs) in forward motion, consuming only 0.278W. The chip is about 2x slower than a throughput-optimized embedded GPU, but 13x more energy efficient. Eyeriss uses a 16-bit fixed point. Finally, EIE (Han et al., 2016a) is an ASIC that utilizes the deep compression spiration (Han et al., 2016b)."}, {"heading": "3.2.4 Comparison Of Accelerator Platforms", "text": "In this section, we compare different accelerator platforms in terms of throughput and throughput per performance. We take performance figures from recently published papers, the source of which can be found in Table 3.1.22The ASIC design by Chen et al. (2016) is optimized for large networks and low power consumption, and their work focuses on Convolutionary Layers from AlexNet. Other work focusing on fully connected layers has only a similar throughput (Han et al., 2016a). Predictably, the ASIC design shows the highest throughput per performance (see Figure 3.1). For GPU performance, we consider an implementation that uses cuBLAS for fully connected layers from AlexNet. Convolutionary layers would deliver lower throughput, as this layer type requires reclassification of data prior to matrix matrix multiplication (see Figure 3.1). Acceleration on the GPU achieves the highest throughput among all accelerators (see Figure 3.1)."}, {"heading": "4.1 Baseline Convolutional Neural Networks", "text": "In each section we approach the following CNNs: 1. LeNet1 was proposed by LeCun et al. (1998). This network consists of two convolutionary layers and two fully connected layers and can be used to classify handwritten digits (MNIST dataset). 2. The CIFAR-10 dataset (Krizhevsky, 2009) has 10 image classes such as airplanes, birds and trucks. CIFAR-10 full model2 was developed by Caffe for the CIFAR-10 dataset. The network has three revolutionary layers followed by a fully connected layer. In addition, the model has two local response normalization layers (LRN)."}, {"heading": "4.2 Fixed Point Format", "text": "We replace the parameter and layer output with the following fixed point number format: [IL.FL], where IL and FL specify the integer and / or fragmentary length of the numbers. The number of bits used to represent each value is IL + FL. To quantify floating point numbers at the fixed point, we use round numbers. We use 2s complementary numbers, i.e. the largest positive value we can represent is: xmax = 2 IL \u2212 1 \u2212 2 \u2212 FL (4.1) Note that in the following experiments, all truncated numbers use a common fixed point format, i.e. they have the same integer and fraction length. For a representation using dynamic adaptation of integer and partial numbers, please refer to Chapter 5.3https: / / githubub.com / BVLC / caffe / blob / master / bvlc _ reference _ caffenet / train _ valdeval.protot.prototx.prototx.4https / / / fraction master, see chapter 5.3https / httthubtrain / httbhttb / 4valgib / 4x.4x.4valgib / 4x.NET / 4x.NET / 4x.NET."}, {"heading": "4.3 Dynamic Range of Parameters and Layer Out-", "text": "putsIn this section we analyze the dynamic range of numbers in two neural networks. This analysis will help to understand the optimal choice for integer and broken bits in fixed point representation."}, {"heading": "4.3.1 Dynamic Range in Small CNN", "text": "As a first step, we perform this analysis for LeNet. We performed the forward propagation of 100 frames to calculate intermediate values in the network. The value distribution is shown in Figure 4.1. Note that this histogram data is the result of shortening all values to integer power of two. We can see that parameters are on average smaller than layer outputs. 99% of trained network parameters are between 20 and 2 \u2212 10. However, with fully connected layers, 99% of layer outputs are in the range 25... 2 \u2212 4. To quantify both layer outputs and network parameters to 8-bit fixed point, a portion of the values must be saturated. We obtained the best quantization results with the Q.4.4 format. This indicates that large layer outputs are more important than small network parameters. 27"}, {"heading": "4.3.2 Dynamic Range in Large CNN", "text": "This subsection contains the analysis for the relatively large CaffeNet network. We performed the forward propagation of 50 images on a trained CaffeNet network, and the resulting dynamic range is shown in Figure 4.2. Similar to a small network, the parameters tend to be smaller than the output layers of the layers. However, in this large network, the average difference between these two number categories is much larger, which is to be expected because the output layers are the result of a multiplication process that yields a much larger result for large layers. In this case, we can compare the relatively large second parameter layer (447.9M MAC operations) with the relatively small last parameter layer (4.1M MAC operations). While the largest value of the second layer is larger than 29, all values in the last layer are below 26. Since the dynamic value range is much larger than in LeNet, more bits are required for a solid point display. Our experiments show the best value of the 16-point-7 using the fixed-result."}, {"heading": "4.4 Results", "text": "This subsection covers the results of quantifying trained 32-bit floating point networks to fixed points."}, {"heading": "4.4.1 Optimal Integer and Fractional Length", "text": "The bit width is the sum of integer and fragment lengths. It is the choice of fraction length that is crucial and determines which values need to be saturated. Our quantization method attempts different partitions of the bit width into integer and fragment parts. The best setting is retained and the resulting fixed point mesh is fine tuned. Note that different options for integer and fraction lengths are conceivable. For example, only the quantification of layer output can be considered good partitioning, as the network parameters can be adjusted in the fine-tuning step. However, our experiments on three different networks show that a common optimization of layer outputs and parameters after fine tuning delivers the best results."}, {"heading": "4.4.2 Quantization to Fixed Point", "text": "We quantified three of our baseline networks on fixed point: LeNet, CIFAR-10 and CaffeNet. To calculate the relative accuracy of a reduced network with bit width, we divide the accuracy of the fixed point by the accuracy of 32-bit floating point. First, we look at the relatively small LeNet network for handwritten detection. Quantifying 32-bit floating point to 8-bit fixed point results in a relative loss of accuracy of 10.3% (see Figure 4.3). After fine tuning, the loss of absolute accuracy shrinks to 0.27% (Table 4.1), which works well in 8-bit fixed point. 29The second baseline network we are looking at is CIFAR-10. This network classifies images into classes such as truck, ship, dog, bird. As this is a more difficult task requiring a larger network, the shift outputs are also larger, and the network is more susceptible to quantization errors."}, {"heading": "5.1 Mixed Precision Fixed Point", "text": "The data path of fully connected and corrugated layers consists of a series of MAC operations (multiplication and accumulation) as shown in Figure 5.1. Layer activations are multiplied by network weights, and these multiplication results are summarized for output. As shown in Figure 5.1, m and n refer to the number of bits used to represent layer output and / or layer weights. Multiplication results are accumulated using a thickening viper tree, i.e. different parts of a CNN use different bit widths. Figure 5.1 is m + n + 1 bits wide, and the bit width increases by 1 bit in each layer. In the last level, the bit width is m + n + lg2 (x), with the number of output operations in the first level being m + 1 bits, and the bit width in each layer increasing by 1 bit."}, {"heading": "5.2 Dynamic Fixed Point", "text": "The different parts of a CNN have a significant dynamic range. In large layers, the output is the result of thousands of accumulations, so the network parameters are much smaller than the output layers. The fixed point is only able to cover a wide dynamic range to a limited extent. The dynamic fixed point can be a good solution to solve this problem, as Courbariaux et al. (2014) showed. In the dynamic fixed point, each number is represented as follows: (-1) s \u00b7 2 \u2212 FL B \u2212 2 \u2211 i = 0 2i \u00b7 xi (5.1) Here B denotes the bit width, s the sign bit, FL the fraction length, and x the mantissabits. Since the intermediate values in a network have different ranges, it is desirable to group fixed point numbers into groups with constant FL. Thus, the number of bits assigned to the fraction within this group is constant but different compared to other groups. Each network layer is divided into two groups: one for layer output, one for layer weights."}, {"heading": "5.2.1 Choice of Number Format", "text": "When approaching a neural network with dynamic fixed-point numbers, we must select a number format for each set of numbers. Each layer has two such groups: the layer parameters and the layer output. Within each group, all numbers will be of the same integer and fraction length. To find the optimal set of number formats, we could perform an exhaustive search, which is not efficient for large neural networks. Instead, we follow a special rule that automatically specifies the required number of integer bits. Specifically, we select enough bits to avoid saturation. Thus, for a given number of numbers S, the required integer length IL is obtained by equation 5.2.IL = dlg2 (max S x + 1) e (5.2) This relationship defines the integer length of the layer parameters. For layer outputs, we reduce the integer length by one, as our experiments show slightly better results."}, {"heading": "5.3 Results", "text": "In this section we present the results of the approximation of 32-bit floating point networks using condensed dynamic fixed point models. All classification accuracies were obtained during the execution of the respective network over the entire validation dataset. We follow the general approximation procedure explained in Section 2.6."}, {"heading": "5.3.1 Impact of Dynamic Fixed Point", "text": "We used our Ristretto framework to quantify CaffeNet (AlexNet) into a fixed point and compare the traditional fixed point with the dynamic fixed point. To facilitate comparison, all layer output and network parameters share the same bit width. Results show good performance of the static fixed point already at 18-bit (Figure 5.3), but if the bit width is further reduced, accuracy starts to decline significantly, while the dynamic fixed point has a stable accuracy. We can conclude that the dynamic fixed point works much better for such a large network, because the dynamic fixed point allows us to match the number of bits associated with an integer and a fraction to the dynamic range of different parts of the network."}, {"heading": "5.3.2 Quantization of Individual Network Parts", "text": "In this section, we present the results for the approximation of different parts of a network. For each experiment, only one category is quantified to the dynamic fixed point, and the rest remains in full precision. Table 5.1 shows the quantization effect for three different networks. For each network, we quantify the layer outputs, the Convolutionary Nuclei (CONV) and the parameters of fully connected layers (FC) independently of each other. In all three networks, the convolution and activation of layers can be trimmed to 8-bits with an absolute change in accuracy of only 0.3%. Fully connected layers are more affected by the trimming of weights to 8-bits, the absolute change being a maximum of 0.9%. Interestingly, LeNet weights can be trimmed to only 2 bits, with the absolute change in accuracy being less than 0.4%."}, {"heading": "5.3.3 Fine-tuned Dynamic Fixed Point Networks", "text": "Here we report on the accuracy of five networks that have been compressed and finely tuned using Ristretto. All networks use dynamic fixed point parameters as well as dynamic fixed point outputs for revolutionary and fully connected layers. LeNet performs well in 2 / 4-bit, while CIFAR-10 and the three ImageNet CNNs can be trimmed to 8-bit (see Table 5.2). Surprisingly, these compressed networks still perform almost as well as their gliding compass roundness. The relative accuracy of LeNet, CIFAR-10 and SqueezeNet is very low (< 0.6%), while approximating the larger CaffeNet and GoogLeNet result in slightly higher costs (0.9% and 2.3%, respectively)."}, {"heading": "6.1 Motivation", "text": "Chapters 4 and 5 focused on the fixed-point approximation of deep CNNs. Since the formation of neural networks normally takes place in floating points, it is an intuitive approach to condense these models into smaller floating-point numbers. This section analyzes the network approximation by minifloat, i.e. floating-point numbers with 16 bits or less."}, {"heading": "6.2 IEEE-754 Single Precision Standard", "text": "According to the IEEE-754 standard, individual precision numbers have 1 character bit, 8 exponent bits and 23 mantissabits. The first bit of the mantissa (always \"1\") is added implicitly, and the stored exponent is distorted by 127. Numbers with all zeros or ones in the exponent have a special meaning. An exponent with all zeros represents either the number 0 or a denoralized number, depending on the mantissabits. In the case of all ones in the exponent, the displayed number is either + / -INF or NaN."}, {"heading": "6.3 Minifloat Number Format", "text": "In order to compress networks and reduce their computing and memory requirements, we present floating-point numbers with much less bits than the IEEE 754 standard. We largely follow the standard when switching to 12-bit, 8-bit, or 6-bit numbers, but our format differs in some details, because the exponent bias is reduced according to the number of bits assigned to the exponent: bias = 2 bits \u2212 1 \u2212 1 (6.1) Here, bits denote the number of bits assigned to the exponent. Another difference from the IEEE standard is that we do not support denoralized numbers, INF and NaN. INF is replaced by saturated numbers, and denorated numbers are replaced by 0. Finally, the number of bits assigned to the exponent and mantis part does not follow any particular rule. To be more precise, our Ristretto framework automatically looks for the best balance between exponent and mantis."}, {"heading": "6.3.1 Network-specific Choice of Number Format", "text": "Similar to the dynamic fixed point, we must select a specific number format per bit width, i.e. split the available bits into exponents and mantissas. To approximate a neural network of minifloat numbers, we must find an appropriate number of exponent and mantissabits. We use enough exponent bits to avoid saturation: Bits = dlg2 (lg2 (max S x) \u2212 1) + 1e (6.2) S is the set of numbers we approximate. This selection of exponent bits does not require saturation, assuming we use an infinite number of mantissabits."}, {"heading": "6.4 Data Path for Accelerator", "text": "The data path of convoluted and fully connected layers is illustrated in Figure 6.2. For simplicity, we only consider fixed-precision arithmetic operations, i.e. all number categories share the same minifloat format. Similar to the fixed-point data path, network parameters and 39-layer inputs are multiplied and cumulated. For each multiplier, a pair of numbers is entered in minifloat format, and the output of each multiplier is 3 bits wider than the input numbers. In the next step, the multiplication results are accumulated in full precision. In a final step, the bias is added in minifloat format, and the end result is trimmed to minifloat. Implemented in a hardware accelerator, the input and output numbers of the minifloat data path are accumulated with full precision. If the neural network in question is too large to fit into the memory on the chip, the layer outputs and parameters must be stored in the minifloat, as we can save significant energy thanks to both the off-chip."}, {"heading": "6.5 Results", "text": "In this section, we will analyze the effects of reducing the bit width of floating point numbers. To find the accuracy of the condensed networks, we will follow the quantization flow described in Section 2.6. We quantified three CNNs on 12-, 8- and 6-bit minifloats. Quantization is done for layer outputs and parameters of fully connected and revolutionary layers. For each network, we will show the classification accuracy of both the 32-bit baseline, followed by minifloat versions (Figure 6.3). We will calculate the normalized accuracy by dividing the performance of the 40minifloat network by the 32-bit floating point accuracy. Results indicate that LeNet has no loss of classification when the layer outputs and layer parameters shrink to 8-bit. CIFAR-10 and CaffeNet can be used in 12-bit, compared to a fixed net accuracy of 6.1%."}, {"heading": "6.6 Comparison to Previous Work", "text": "Previous work (Deng et al.) approached network parameters of AlexNet (CaffeNet) with 8-bit minifloat. They analyze the effects of quantifying a different percentage of network parameters. Our results achieve significantly higher accuracy thanks to careful selection of minifloat format and fine-tuning."}, {"heading": "7.1 Multiplier-free Arithmetic", "text": "Hardware accelerators for Convolutionary Neural Networks must be energy efficient to enable use in mobile devices.Fully connected layers and Convolutionary Layers consist of addition and multiplication, the latter of which requires a much larger chip area, which motivated previous research to eliminate all multipliers by using integer powers of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008).These weights can be regarded as minifloat numbers with zero mantissabbits. By using such weights, all multiplications turn into bit shifts, saving a significant amount of energy on a hardware accelerator.We now detail the approximation of Convolutionary and fully connected layers with multiplicator-free arithmetics. Although we assume strictly positive weights in this discussion, it is straightforward to extend the approximation procedures to positive and negative weights."}, {"heading": "7.2 Maximal Number of Shifts", "text": "Almost all network weights of a trained network are between + 1 and \u2212 1, but most of them are close to zero. Quantifying these parameters on Power-of-Two has the greatest impact (in terms of absolute value changes) on weights near + 1 and \u2212 1. These weights will only be able to take on the values of 1, 1 2, 1 4, etc. We encode the number of shifts in 4 bits (see Figure 7.1), which means that the parameter exponents can have 8 different values. We select the exponent values so that ei [\u2212 8,..., \u2212 1] represents them and use this format for subsequent experiments. The motivation for this format is twofold: firstly, the use of only 4 bits for parameters greatly reduces the memory requirement. Secondly, the smallest possible value in this format is 2 \u2212 8. Parameters smaller than this have little impact on network performance. Furthermore, only a few parameters below this smallest value are lower than 5.83 Our analysis of 10.44% of the lowest value of 3.44% is even lower."}, {"heading": "7.3 Data Path for Accelerator", "text": "To allow shifts of several bits in a cycle, barrel shifters should be used. Note that this data path has no multipliers at all, which can potentially save significant chip areas. To simplify this analysis, we focus only on the effects of removing multiplications. Layer inputs and outputs are kept in full precision format."}, {"heading": "7.4 Results", "text": "We used our Ristretto framework to simulate the effect of removing all multiplications from conventional and fully connected networks. Our framework quantifies all network45parameters to the nearest integer power-of-two number. Table 7.1 compares networks with power-of-two weights and networks with single precision weights. In LeNet, the absolute decrease in classification accuracy for the quantified weights is 0.1%. CIFAR-10 and CaffeNet are more affected by weight quantification (4.21% and 3.65% absolute accuracy drop). At first glance, the results may be discouraging for the larger two networks. However, it is surprising that the networks with weight quantification still have decent accuracy. The power-of-two weights can be stored in only 4 bits (the exponents range from -1 to -8). This enables huge energy savings: firstly, the traffic is reduced to off-weights, but the chip-4 is not."}, {"heading": "8.1 Fixed Point Approximation", "text": "Fixed point is the approximation scheme that requires the lowest energy and development time for a hardware accelerator, but also the lowest performance for small bit widths. For CaffeNet, for example, the dynamic range is significant, as shown in Section 4.3.2. In 15-bit layers, 0.45% of the layer outputs are too large and must be displayed in saturated format. If you move to the 14-bit fixed point, 2.82% of the layer outputs are saturated. As large layer outputs are very important for the accuracy of the network, this results in a significant loss of accuracy (see 48 Figure 8.1)."}, {"heading": "8.2 Dynamic Fixed Point Approximation", "text": "Dynamic fixed point combines the advantages of fixed point and minifloat: on the one hand, this format allows the use of all bits for the mantissa part, which contributes to good accuracy; on the other hand, a dynamic fixed point can cover a wide dynamic range, just like floating point, thanks to the implicit stored exponent. LeNet can be approximated with only 5-bit numbers and achieves the same accuracy as the 32-bit floating point model. The same is true for an 8-bit CIFAR-10 network. Finally, the CaffeNet architecture can be approximated with 8-bit dynamic fixed points, with an absolute accuracy loss of 0.3%."}, {"heading": "8.3 Minifloat Approximation", "text": "Floating-point numbers can cover a wide dynamic range thanks to their exponent. Minifloat performs considerably better than static fixed point. However, the minifloat approximation shows a significant decrease in accuracy at very small bit widths. This sharp decrease is at a point where there are not enough bits for the exponent. In LeNet, for example, the accuracy of 5-bit arithmetic is 97.96%. In this setting, we use 4 bits for the exponent, no mantissabits and one character bit. If we further reduce the bit width, the exponent cannot cover the dynamic range, and the accuracy drops steeply to 10.09%. In the other two networks, we see a similar effect. Both CIFAR-10 and CaffeNet require 5 exponent bits according to Equation 6.2. Since we need one bit more for the character, these two networks need at least 6-bit minifloat numbers to achieve good classification performance."}, {"heading": "8.4 Summary", "text": "Dynamic fixed point is very suitable for approximating neural networks. This approximation shows the best accuracy at small bit widths. Although dynamic fixed point requires slightly more chip area than pure fixed point arithmetic, this approximation is very suitable for hardware acceleration of neural networks. Bit width can be reduced to 4-bit or 8-bit for LeNet, CIFAR-10 and CaffeNet. This significantly reduces the required memory bandwidth and footprint, which should result in significant energy savings for FPGA and ASIC designs."}, {"heading": "9.1 From Caffe to Ristretto", "text": "Ristretto is, according to Wikipedia, \"a short shot of espresso made with the normal amount of ground coffee but extracted with about half the amount of water.\" Likewise, our compressor removes the unnecessary parts of a CNN while ensuring that the essence - the ability to predict classes from images - is preserved. With its strong community and fast training for deep CNNs, Caffe by Jia et al. (2014) is an excellent framework to build on. Ristretto takes a trained model as input and automatically brews a condensed network version. Ristretto input and output are a network description file (prototxt) and network parameters, and the condensed model in caffe format can then be used for a hardware accelerator."}, {"heading": "9.2 Quantization Flow", "text": "Ristretto can compress each 32-bit floating point network to a fixed point, a minifloat or an integer power of two parameters (Figure 9.1). In the first step, the dynamic range of weights is analyzed to find a compressed numerical representation. In the case of a dynamic fixed point, Ristretto assigns enough bits to the integer part to avoid saturation of large values. In the second step, the minifloat approximation assigns enough bits to the exponent. In order to quantify complete 51 precision numbers to a smaller numerical format, Ristretto uses round, nearest levels. In the second step, several thousand images are executed in forward gear. The generated layer actuations are analyzed to generate statistical parameters. Ristretto distributes enough bits to the new numerical format to avoid saturation of the layers. In the next step, Ristretto performs a fixed search for the optimum number of weights, the binary search for the optimal number of the number of bits connected to each other."}, {"heading": "9.3 Fine-tuning", "text": "In order to compensate for the loss of accuracy resulting from the quantification, the quantified network is finely tuned in Ristretto. During this retraining process, the network learns how to classify images with discretely evaluated parameters w \u00b2. During fine tuning, we calculate small weight updates w \u00b2. Since these small weight updates may be below the quantization step size of the discrete parameters, we also retain a set of full precision weights w.Ristretto uses the fine tuning procedure shown in Figure 9.2. For each batch, the full precision weights are quantified to a reduced precision format. During forward propagation, these discrete weights are used to calculate the layer outputs zl. Each layer transforms its input charge xl into the output zl, according to its function fl: (xl, w \u00b2), the full precision weights are quantified in reduced form."}, {"heading": "9.4 Fast Forward and Backward Propagation", "text": "For the simulation of forward propagation in hardware, Ristretto uses the full floating point for accumulation. This follows the idea of Gupta et al. (2015) and corresponds to our description of the forward data path in Section 2.6.1.1. During fine tuning, the full precision weights for each batch must be quantified, but after that all calculations can be performed in the floating point (Figure 9.2). Therefore, Ristretto can fully utilize the opti-53mized matrix matrix multiplication routines for both forward and backward propagation. Thanks to its quick implementation on the GPU, a fixed point CaffeNet on the ILSVRC 2014 validation dataset (50k images) can be tested in less than 2 minutes (with a Tesla K-40 GPU)."}, {"heading": "9.5 Ristretto From a User Perspective", "text": "Ristretto is based on the highly optimized Caffe framework and follows its principles. A Caffe user will appreciate the smooth and easy-to-understand integration of Ristretto.Caffe: Developing an image processing algorithm in Caffe starts with a network description file (see Figure 9.3), which is written by the user and contains the hyperparameters of the neural network architecture. The Caffe framework uses the Google Protocol buffer format to encode the network information. Networks are presented as directed acyclic graphics. The recesses are layers based on the input data and layer parameters. Data flows from one layer to the other \"blobs.\" As a second element, the Caffe user needs a described acyclic graphic file. The recesses are layers that form a compilation based on the input data and layer parameters."}, {"heading": "9.6 Release of Ristretto", "text": "Ristretto is published as an open source project and has the following strengths: \u2022 Automation: Ristretto performs automatic trimming of any CNN. \u2022 Flexibility: Various trimming programs are supported. 55 \u2022 Accuracy: Ristretto fine tuned networks. \u2022 Speed: Ristretto runs on the GPU and uses optimized CUDA routines. Ristretto has a homepage1 and the source code is available.2"}, {"heading": "9.7 Future Work", "text": "Ristretto follows the modular source code architecture of Caffe. New features such as new layer types with limited precision can be easily added to Ristretto. In this section we discuss various possible future steps."}, {"heading": "9.7.1 Network Pruning", "text": "The most energy-consuming operation for CNN accelerators is memory access without a chip. As large networks do not fit into the memory on the chip, it is imperative to compress the network. Most network weights come from fully connected layers. It has been shown that a significant portion of the connections can be removed in fully connected layers. Previous work (Han et al., 2016b) achieved high network compression rates without compromising classification accuracy."}, {"heading": "9.7.2 Binary Networks", "text": "The first published work on the representation of ImageNet binary weights networks was by Rastegari et al. (2016). Their results show that very deep binary weights networks can be approximated, albeit with an accuracy loss of about 3% for CaffeNet and 6% for GoogLeNet. Replacing 32-bit floating-point parameters with just one bit of information inevitably reduces the accuracy of the network to extract functions from images. The challenge with binary parameters is to achieve high predictive accuracy without increasing the parameter size or adding additional computational effort."}, {"heading": "9.7.3 C-Code Generation", "text": "The standard development toolchain starts with a C implementation of the1http: / / ristretto.lepsucd.com / 2https: / / github.com / pmgysel / caffe56 algorithm, which then undergoes many unit tests to verify correct functionality. In the next step, the C code is manually converted to System C, which serves as an input to High Level Synthesis (HLS). Highly optimized HLS tools can produce highly efficient verilog codes within a time span needed for manual verilog encoding. We plan to add a feature to Ristretto enable automatic generation of the C code of a condensed network, which produces the necessary code files as well as a dump of the generated low-precision parameters. We hope that Ristretto researchers will improve their development time to accelerate CNN acceleration."}], "references": [{"title": "Advances in Optimizing Recurrent Networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Y.-H", "T. Krishna", "J. Emer", "V. Sze"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Training Deep Neural Networks with Low Precision Multiplications", "author": ["M. Courbariaux", "David", "J.-P", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "Reduced-Precision Memory Value Approximation for Deep Learning. http://www.labs.hpe.com/techreports/2015/ HPL-2015-100.html", "author": ["Z. Deng", "C. Xu", "Q. Cai", "P. Faraboschi"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2016}, {"title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Leveraging the Error Resilience of Neural Networks for Designing Highly Energy Efficient Accelerators", "author": ["Z. Du", "A. Lingamneni", "Y. Chen", "K.V. Palem", "O. Temam", "C. Wu"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on,", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "A Neural Algorithm of Artistic Style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Girshick,? \\Q2015\\E", "shortCiteRegEx": "Girshick", "year": 2015}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Hardware-oriented Approximation of Convolutional Neural Networks", "author": ["P. Gysel", "M. Motamedi", "S. Ghiasi"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-R", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": null, "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Stanford CS231n course: Convolutional Neural Networks for Visual Recognition. http://cs231n.github.io/neural-networks-1", "author": ["A. Karpathy"], "venue": null, "citeRegEx": "Karpathy,? \\Q2016\\E", "shortCiteRegEx": "Karpathy", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fixed Point Quantization of Deep Convolutional Networks", "author": ["D. Lin", "S. Talathi", "S. Annapureddy"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural Networks with Few Multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "A backpropagation neural network design using adder-only arithmetic", "author": ["V. Mahoney", "I. Elhanany"], "venue": "In 51st Midwest Symposium on Circuits and Systems,", "citeRegEx": "Mahoney and Elhanany,? \\Q2008\\E", "shortCiteRegEx": "Mahoney and Elhanany", "year": 2008}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "PLACID: A Platform for Accelerator Creation for DCNNs", "author": ["M. Motamedi", "P. Gysel", "S. Ghiasi"], "venue": "Under review,", "citeRegEx": "Motamedi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Motamedi et al\\.", "year": 2016}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Going Deeper with Embedded FPGA Platform for Convolutional Neural Network", "author": ["J. Qiu", "J. Wang", "S. Yao", "K. Guo", "B. Li", "E. Zhou", "J. Yu", "T. Tang", "N. Xu", "S. Song", "Y. Wang", "H. Yang"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Qiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A 1.42 TOPS/W deep convolutional neural network recognition processor for intelligent IoE systems", "author": ["J. Sim", "Park", "J.-S", "M. Kim", "D. Bae", "Y. Choi", "Kim", "L.-S"], "venue": "IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "Sim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sim et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks", "author": ["N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S. Vrudhula", "Seo", "J.-s", "Y. Cao"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Suda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suda et al\\.", "year": 2016}, {"title": "Resiliency of Deep Neural Networks under Quantization", "author": ["W. Sung", "S. Shin", "K. Hwang"], "venue": "arXiv preprint arXiv:1511.06488,", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Multilayer Feedforward Neural Networks with Single Powers-of-Two Weights", "author": ["C.Z. Tang", "H.K. Kwan"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Tang and Kwan,? \\Q1993\\E", "shortCiteRegEx": "Tang and Kwan", "year": 1993}, {"title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 37, "context": "(2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al.", "startOffset": 12, "endOffset": 42}, {"referenceID": 40, "context": "(2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al., 2015) and ResNet (He et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 17, "context": ", 2015) and ResNet (He et al., 2015).", "startOffset": 19, "endOffset": 36}, {"referenceID": 4, "context": ", 2014) and Nvidia\u2019s cuDNN (Chetlur et al., 2014), a Tesla K-40 GPU can process an image in just 4ms.", "startOffset": 27, "endOffset": 49}, {"referenceID": 38, "context": "Various FPGA-based accelerators (Suda et al., 2016; Qiu et al., 2016) have proven that it is possible to use reconfigurable hardware for end-to-end inference of large CNNs like AlexNet and VGG.", "startOffset": 32, "endOffset": 69}, {"referenceID": 33, "context": "Various FPGA-based accelerators (Suda et al., 2016; Qiu et al., 2016) have proven that it is possible to use reconfigurable hardware for end-to-end inference of large CNNs like AlexNet and VGG.", "startOffset": 32, "endOffset": 69}, {"referenceID": 21, "context": "This annually held competition has seen state-of-the-art image classification accuracies by deep networks such as AlexNet by Krizhevsky et al. (2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al.", "startOffset": 125, "endOffset": 150}, {"referenceID": 2, "context": "CNNs (Chen et al., 2016; Sim et al., 2016; Han et al., 2016a).", "startOffset": 5, "endOffset": 61}, {"referenceID": 36, "context": "CNNs (Chen et al., 2016; Sim et al., 2016; Han et al., 2016a).", "startOffset": 5, "endOffset": 61}, {"referenceID": 15, "context": "Gysel et al. (2016)", "startOffset": 0, "endOffset": 20}, {"referenceID": 31, "context": "Motamedi et al. (2016)", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features.", "startOffset": 5, "endOffset": 17}, {"referenceID": 8, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features.", "startOffset": 62, "endOffset": 86}, {"referenceID": 8, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features. Neural networks in contrast can automatically create both high-level and low-level features. For a long time, deep neural networks were hindered by their computational complexity. However, advances in both personal computers and general purpose computing have enable the training of larger networks with more parameters. In 2012, the first deep convolutional neural network with 8 parameter layers was proposed by Krizhevsky et al. (2012). State-of-the art deep CNNs use a series of convolutional layers which enables them to extract very high-level features from images.", "startOffset": 62, "endOffset": 609}, {"referenceID": 23, "context": "Network architecture of AlexNet by Krizhevsky et al. (2012).", "startOffset": 35, "endOffset": 60}, {"referenceID": 22, "context": "(2013) or Adam rule (Kingma and Ba, 2015).", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "There actually exist many optimizations for this parameter update such as Nesterov momentum as explained by Bengio et al. (2013) or Adam rule (Kingma and Ba, 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 31, "context": "Image credit: Motamedi et al. (2016).", "startOffset": 14, "endOffset": 37}, {"referenceID": 31, "context": "Image credit: Motamedi et al. (2016).", "startOffset": 14, "endOffset": 37}, {"referenceID": 30, "context": "This activation function was first proposed by Nair and Hinton (2010) for Restricted Boltzmann Machines.", "startOffset": 47, "endOffset": 70}, {"referenceID": 23, "context": "The work by Krizhevsky et al. (2012) was the first to apply this simplified activation to a deep neural networks.", "startOffset": 12, "endOffset": 37}, {"referenceID": 24, "context": "This creates competition between neurons generated by different kernels and improves the top-1 accuracy of AlexNet (Krizhevsky et al., 2012) by 1.", "startOffset": 115, "endOffset": 140}, {"referenceID": 24, "context": "Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015).", "startOffset": 59, "endOffset": 106}, {"referenceID": 40, "context": "Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015).", "startOffset": 59, "endOffset": 106}, {"referenceID": 20, "context": "Most recently, a new normalization strategy termed Batch Normalization (BN) was proposed (Ioffe and Szegedy, 2015).", "startOffset": 89, "endOffset": 114}, {"referenceID": 22, "context": "This creates competition between neurons generated by different kernels and improves the top-1 accuracy of AlexNet (Krizhevsky et al., 2012) by 1.4%. The exact mathematical formulation of LRN across channels can be found in the same paper. Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015). One notable exception is the network by Simonyan and Zisserman (2015) which performs very well without any kind of normalization.", "startOffset": 116, "endOffset": 418}, {"referenceID": 20, "context": "Most recently, a new normalization strategy termed Batch Normalization (BN) was proposed (Ioffe and Szegedy, 2015). This strategy was adopted by the most recent winner of the ILSVRC competition (Russakovsky et al., 2015). While both normalization strategies help for faster convergence and better prediction accuracy, they also add computational overhead, especially batch normalization. Unfortunately these normalization layers require a very large dynamic range for intermediate values. In AlexNet for example, the intermediate values of LRN layers are 2 times larger than any intermediate value from another layer. For this reason this thesis assumes LRN and BN layers are to be implemented in floating point, and we concentrate on the approximation of other layer types. Notice that previous work by Suda et al. (2016) chose 32-bit floating point for FPGA-based LRN layers.", "startOffset": 90, "endOffset": 823}, {"referenceID": 24, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 37, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 40, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 17, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 13, "context": "Besides image classification, deep networks show state-of-art performance in object detection (Girshick, 2015) as well as speech recognition (Hinton et al.", "startOffset": 94, "endOffset": 110}, {"referenceID": 18, "context": "Besides image classification, deep networks show state-of-art performance in object detection (Girshick, 2015) as well as speech recognition (Hinton et al., 2012).", "startOffset": 141, "endOffset": 162}, {"referenceID": 35, "context": "Other applications include playing games (Silver et al., 2016), as well as art (Gatys et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": ", 2016), as well as art (Gatys et al., 2015).", "startOffset": 24, "endOffset": 44}, {"referenceID": 23, "context": "CaffeNet is the Caffe-version of AlexNet by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet data set, which has 1000 image classes.", "startOffset": 44, "endOffset": 69}, {"referenceID": 23, "context": "CaffeNet is the Caffe-version of AlexNet by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet data set, which has 1000 image classes. Figure 2.6 shows the required arithmetic operations and parameters size of AlexNet by layer type. The major part of arithmetic operations comes from convolutional layers: this layer type requires a total of 2.15 G operations. The arithmetic operations in all other layers sum up to 117 M operations. The parameter size of CaffeNet is 250 MB, of which 235 MB comes from fully connected layers. The same trend can be observed for the 16-layer version of VGG by Simonyan and Zisserman (2015): Extracting features in convolutional layers is computation-intense, while fully connected layers are memory-intense.", "startOffset": 44, "endOffset": 639}, {"referenceID": 37, "context": "The winner of 2014\u2019s localization challenge (Simonyan and Zisserman, 2015) experimented", "startOffset": 44, "endOffset": 74}, {"referenceID": 17, "context": "Another experiment by the winner of 2015 (He et al., 2015) used very deep networks.", "startOffset": 41, "endOffset": 58}, {"referenceID": 40, "context": "In the classification challenge of 2014, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller.", "startOffset": 51, "endOffset": 73}, {"referenceID": 37, "context": ", 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller.", "startOffset": 25, "endOffset": 55}, {"referenceID": 24, "context": "SqueezeNet has the accuracy of AlexNet (Krizhevsky et al., 2012), but contains 50X fewer parameters.", "startOffset": 39, "endOffset": 64}, {"referenceID": 17, "context": "Another experiment by the winner of 2015 (He et al., 2015) used very deep networks. Their network architecture improves by over 2% when expanding the net from 34 to 152 layers. On the other hand, some research shows that even relatively small networks can achieve good classification performance. In the classification challenge of 2014, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller. The GoogLeNet network is based on the inception idea described in section 2.5.2. A newer network architecture by Iandola et al. (2016) uses an adapted inception concept with smaller convolutional kernels.", "startOffset": 42, "endOffset": 605}, {"referenceID": 40, "context": "2 Inception Idea The inception idea is a concept proposed by Szegedy et al. (2015) which was used to build the GoogLeNet architecture.", "startOffset": 61, "endOffset": 83}, {"referenceID": 40, "context": "Image credit: Szegedy et al. (2015).", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": "However, it has be shown (Du et al., 2015) that CNNs have a relatively high error resilience; moreover CNNs can be trained in a discrete parameter space.", "startOffset": 25, "endOffset": 42}, {"referenceID": 14, "context": "Denoting as the quantization step size and bxc as the largest quantization value less or equal to x, Gupta et al. (2015) define round nearest as follows:", "startOffset": 101, "endOffset": 121}, {"referenceID": 14, "context": "Round stochastic: Another rounding scheme termed stochastic rounding was used by Gupta et al. (2015) for the weight updates of 16-bit neural networks.", "startOffset": 81, "endOffset": 101}, {"referenceID": 14, "context": "Round stochastic: Another rounding scheme termed stochastic rounding was used by Gupta et al. (2015) for the weight updates of 16-bit neural networks. Gupta et al. (2015) define stochastic rounding as follows:", "startOffset": 81, "endOffset": 171}, {"referenceID": 5, "context": "We adopt the idea of previous work by Courbariaux et al. (2015) which uses full precision shadow weights.", "startOffset": 38, "endOffset": 64}, {"referenceID": 5, "context": "Further trimming of the same network uses as low as 7-bit multipliers (Courbariaux et al., 2014).", "startOffset": 70, "endOffset": 96}, {"referenceID": 39, "context": "A similar proposal represents the weights of the same network with +1, 0 and -1 values (Sung et al., 2015).", "startOffset": 87, "endOffset": 106}, {"referenceID": 11, "context": "Gupta et al. (2015) show that networks on datasets like CIFAR-10 (10 images classes) can be trained in 16-bit.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Further trimming of the same network uses as low as 7-bit multipliers (Courbariaux et al., 2014). Another approach by Courbariaux et al. (2015) uses only binary weights, again on the same network.", "startOffset": 71, "endOffset": 144}, {"referenceID": 16, "context": "The deep compression pipeline proposed by Han et al. (2016b) addresses this problem.", "startOffset": 42, "endOffset": 61}, {"referenceID": 6, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point.", "startOffset": 29, "endOffset": 55}, {"referenceID": 7, "context": "Combining the two previous ideas, \u2018Binarized Neural Network\u2019 (Courbariaux et al., 2016) uses binary weights and layer activations.", "startOffset": 61, "endOffset": 87}, {"referenceID": 5, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point. This approach reduces parameter size by factor 32X and removes the need of multiplications in the forward path. BinaryConnect achieves near-state-of-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN). A later work by Lin et al. (2016) takes this idea a step further by turning multiplications in the backward propagation into bit shifts.", "startOffset": 30, "endOffset": 402}, {"referenceID": 5, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point. This approach reduces parameter size by factor 32X and removes the need of multiplications in the forward path. BinaryConnect achieves near-state-of-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN). A later work by Lin et al. (2016) takes this idea a step further by turning multiplications in the backward propagation into bit shifts. Layer activations are approximated by integer power of 2 numbers, while error gradients are retained in full precision. This proposal significantly reduces the hardware requirements for accelerators. Combining the two previous ideas, \u2018Binarized Neural Network\u2019 (Courbariaux et al., 2016) uses binary weights and layer activations. These numbers are constraint to +1 and -1 for both forward and backward propagation. Convolutional neural networks mainly consist of multiply-accumulate operations. For a binarized network, these operations are replaced by binary XNOR and binary count. To improve training results, the proposed method uses a bit-shift-based batch normalization as well as a shift-based parameter update. Finally the work of Rastegari et al. (2016) applies the idea of binary networks to ImageNet data set.", "startOffset": 30, "endOffset": 1268}, {"referenceID": 24, "context": "Both are based on AlexNet (Krizhevsky et al., 2012) and use different degrees of binarization.", "startOffset": 26, "endOffset": 51}, {"referenceID": 32, "context": "The work by Rastegari et al. (2016) proposes two network architectures.", "startOffset": 12, "endOffset": 36}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation. They achieve a speedup of 2X for convolutional layers of AlexNet, compared to a non-optimized GPU implementation. Another work by Mathieu et al. (2013) achieves better results by replacing convolution through FFT.", "startOffset": 14, "endOffset": 238}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation. They achieve a speedup of 2X for convolutional layers of AlexNet, compared to a non-optimized GPU implementation. Another work by Mathieu et al. (2013) achieves better results by replacing convolution through FFT. Finally the neural network compression pipeline proposed by Han et al. (2016b) uses pruning and weight-sharing.", "startOffset": 14, "endOffset": 379}, {"referenceID": 37, "context": "(2016) uses OpenCL to implement whole VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board.", "startOffset": 42, "endOffset": 72}, {"referenceID": 33, "context": "Finally a recent Xilinxbased implementation (Qiu et al., 2016) achieves the start-of-art throughput of 137 GOPs.", "startOffset": 44, "endOffset": 62}, {"referenceID": 39, "context": "An approach by Zhang et al. (2015) uses Vivado HLS to accelerate the convolutional layers of AlexNet.", "startOffset": 15, "endOffset": 35}, {"referenceID": 36, "context": "A subsequent proposal by Suda et al. (2016) uses OpenCL to implement whole VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board.", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "A later implementation termed Eyeriss (Chen et al., 2016) can run the convolutional layers of AlexNet in forward path at 34 frames per second (74.", "startOffset": 38, "endOffset": 57}, {"referenceID": 2, "context": "DaDianNao by Chen et al. (2014) is a super-computer for machine learning at 28 nm technology.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.", "startOffset": 27, "endOffset": 46}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.", "startOffset": 27, "endOffset": 112}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.2 W 15.2 GOP/s/W Chen et al. (2016) Titan X 3.", "startOffset": 27, "endOffset": 172}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.2 W 15.2 GOP/s/W Chen et al. (2016) Titan X 3.23 TOP/s 250 W 12.9 GOP/s/W Han et al. (2016a)", "startOffset": 27, "endOffset": 229}, {"referenceID": 2, "context": "The ASIC design by Chen et al. (2016) is optimized for large networks and low power consumption.", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.", "startOffset": 76, "endOffset": 95}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.2 GOP/s/W vs 12.9 GOP/s/W). The FPGA implementation from Qiu et al. (2016) is an end-to-end implementation of the 16-layer version of VGG.", "startOffset": 76, "endOffset": 311}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.2 GOP/s/W vs 12.9 GOP/s/W). The FPGA implementation from Qiu et al. (2016) is an end-to-end implementation of the 16-layer version of VGG. The FPGA implementation uses 16-bit fixed point arithmetic to reduce memory and computation requirements. Moreover the authors use pruning in fully connected layers to reduce parameter size. Another work by Suda et al. (2016) achieves nearly the same throughput without weight pruning.", "startOffset": 76, "endOffset": 601}, {"referenceID": 25, "context": "LeNet was proposed by LeCun et al. (1998). This network consists of two convolutional and two fully connected layers and can be used to classify handwritten digits (MNIST dataset).", "startOffset": 22, "endOffset": 42}, {"referenceID": 23, "context": "The CIFAR-10 data set (Krizhevsky, 2009) has 10 image classes such as airplanes, bird, and truck.", "startOffset": 22, "endOffset": 40}, {"referenceID": 24, "context": "CaffeNet is the Caffe version of AlexNet (Krizhevsky et al., 2012) which is the winner of the 2012 ILSVRC competition.", "startOffset": 41, "endOffset": 66}, {"referenceID": 40, "context": "GoogLeNet was proposed by Szegedy et al. (2015) and won the 2014 ILSVRC competition.", "startOffset": 26, "endOffset": 48}, {"referenceID": 24, "context": "(2016) was developed with the goal of a small network with the accuracy of AlexNet (Krizhevsky et al., 2012).", "startOffset": 83, "endOffset": 108}, {"referenceID": 19, "context": "SqueezeNet by Iandola et al. (2016) was developed with the goal of a small network with the accuracy of AlexNet (Krizhevsky et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "As shown by Lin et al. (2015); Qiu et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 26, "context": "As shown by Lin et al. (2015); Qiu et al. (2016), it is a good approach to use mixed precision, i.", "startOffset": 12, "endOffset": 49}, {"referenceID": 5, "context": "Dynamic fixed point can be a good solution to overcome this problem, as shown by Courbariaux et al. (2014). In dynamic fixed point, each number is represented as follows:", "startOffset": 81, "endOffset": 107}, {"referenceID": 19, "context": "The SqueezeNet (Iandola et al., 2016) architecture was developed with the goal of a small CNN that performs well on the ImageNet data set.", "startOffset": 15, "endOffset": 37}, {"referenceID": 5, "context": "Some previous work (Courbariaux et al., 2014) concentrated on training with fixed point arithmetic from the start and shows little performance decline for as short as 7bit fixed point numbers on LeNet.", "startOffset": 19, "endOffset": 45}, {"referenceID": 41, "context": "This motivated previous research to eliminate all multipliers by using integer power of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008).", "startOffset": 100, "endOffset": 149}, {"referenceID": 29, "context": "This motivated previous research to eliminate all multipliers by using integer power of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008).", "startOffset": 100, "endOffset": 149}, {"referenceID": 22, "context": "For parameter updates we use the Adam rule by Kingma and Ba (2015). As an important observation, we do not quantize layer outputs during fine-tuning.", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "Since the choice of hyper parameters for retraining is crucial (Bergstra and Bengio, 2012), Ristretto relies on minimal human intervention in this step.", "startOffset": 63, "endOffset": 90}, {"referenceID": 14, "context": "This follows the thought of Gupta et al. (2015) and is conform with our description of the forward data path in subsection 2.", "startOffset": 28, "endOffset": 48}, {"referenceID": 34, "context": "2 Binary Networks The first published work to represent ImageNet networks with binary weights was by Rastegari et al. (2016). Their results show that very deep networks can be approximated with binary weights, although at an accuracy drop of around 3% for CaffeNet and 6% for GoogLeNet.", "startOffset": 101, "endOffset": 125}], "year": 2016, "abstractText": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks Convolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-ofart networks require billions of arithmetic operations and millions of parameters. To enable embedded devices such as smart phones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference. We present Ristretto, a fast and automated framework for CNN approximation. Ristretto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy. Since training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network. Given a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.", "creator": "LaTeX with hyperref package"}}}