{"id": "1605.06450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Query-Efficient Imitation Learning for End-to-End Autonomous Driving", "abstract": "One way to approach end-to-end autonomous driving is to learn a policy function that maps from a sensory input, such as an image frame from a front-facing camera, to a driving action, by imitating an expert driver, or a reference policy. This can be done by supervised learning, where a policy function is tuned to minimize the difference between the predicted and ground-truth actions. A policy function trained in this way however is known to suffer from unexpected behaviours due to the mismatch between the states reachable by the reference policy and trained policy functions. More advanced algorithms for imitation learning, such as DAgger, addresses this issue by iteratively collecting training examples from both reference and trained policies. These algorithms often requires a large number of queries to a reference policy, which is undesirable as the reference policy is often expensive. In this paper, we propose an extension of the DAgger, called SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous driving. We evaluate the proposed SafeDAgger in a car racing simulator and show that it indeed requires less queries to a reference policy. We observe a significant speed up in convergence, which we conjecture to be due to the effect of automated curriculum learning.", "histories": [["v1", "Fri, 20 May 2016 17:40:16 GMT  (1937kb,D)", "http://arxiv.org/abs/1605.06450v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["jiakai zhang", "kyunghyun cho"], "accepted": false, "id": "1605.06450"}, "pdf": {"name": "1605.06450.pdf", "metadata": {"source": "CRF", "title": "Query-Efficient Imitation Learning for End-to-End Autonomous Driving", "authors": ["Jiakai Zhang", "Kyunghyun Cho"], "emails": ["zhjk@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Imitation Learning for Autonomous Driving", "text": "In this section we describe imitation acquisition in connection with learning an automatic policy for driving."}, {"heading": "2.1 State Transition and Reward", "text": "An environment or a world is defined as a set of states. Each state is accompanied by a set of possible acts A (S). Each given state passes into another state s'S when an act A (S) is performed, according to a state transitional function \u03b4: S \u00b7 A (S) \u2192 S. This transitional function can be either deterministic or stochastical. For each sequence of state-action pairs, there is a (cumulative) reward r: r: (s0, a0), (s1, a1), (s2, a2),...), where st = \u03b4 (st \u2212 1, at \u2212 1).A reward can be implicit in the sense that the reward comes as a form of binary value, where 0 corresponds to each unsuccessful run (e.g. if the car crashes into another car, causing the car to crash), while a successful race (e.g. unlimited without a crash) does not receive the reward."}, {"heading": "2.2 Policies", "text": "A policy is a function that refers from a state observation \u03c6 (s) to one of the measures available to A (s) in the state. An underlying state perfectly describes the surrounding environment, while a policy often has only limited access to the state via its observation \u03c6 (s). In the context of full autonomous driving, s summarizes all the necessary information about the road (e.g. lane number, existence of other cars or pedestrians, etc.), for example, it is a picture frame captured by a front-facing camera. We have two different strategies. First, a primary policy \u03c0 is a policy that learns to drive a car. This policy does not observe a complete, underlying state, but only has access to the state observation systems that are a pixel-based picture frame in this paper."}, {"heading": "2.3 Driving", "text": "The policy in this essay looks at a frame from a front-facing camera and returns both the angle of a steering wheel (u-1, 1) and a binary indicator of braking (b-0, 1). We call this strategy of relying on a single fixed policy a naive strategy. Accessible States With a series of initial states (u-1, 1), each policy defines a subset of the accessible states (S-1). In other words, a car driven by a policy only visits the states (S-1). We use S-2 to be an achievable policy (S-2). In the case of learning to drive, this reference is intuitively smaller than the road that is led in a different direction by another policy (S-1) that probably does not lead to avoidance."}, {"heading": "2.4 Supervised Learning", "text": "The most obvious approach is supervised learning. In supervised learning, a car is first controlled by a reference policy, while the state observations \u03c6 (s) of the visited states are collected, resulting in D = {\u03c6 (s) 1, \u03c6 (s) 2,..., \u03c6 (s) N}. On the basis of this data set, we define a loss function aslsupervised (\u03c0, \u03c0, D) = 1N N N = 1 \u0445 \u03c0 (\u03c6 (s) n) \u2212 \u03c0 (\u03c6 (s) n). (1) Then a desired primary policy \u03c0 = arg mindness lsupervised (zipiert, \u03c0, D). A major problem of this supervised learning approach to imitation of learning results from the imperfection of the primary policy area. This imperfection probably leads the primary policy to a state that is not included in this primary policy."}, {"heading": "2.5 DAgger: beyond Supervised Learning", "text": "An essential feature of the approach described above of monitored learning is that it is only the reference policy par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par"}, {"heading": "3 SafeDAgger: Query-Efficient Imitation Learning with a Safety Policy", "text": "We propose an extension of DAgger that minimizes the number of queries to a reference policy both during the training and during the test. In this section, we describe this extension, called SafeDAgger, in detail."}, {"heading": "3.1 Safety Policy", "text": "In contrast to earlier approaches to imitated learning, often as learning-to-search [7, 16, 5], we present an additional policy \u03c0safe, which we refer to as security policy. This policy takes as input both the partial observation of a state \u03c6 (s) and a primary reward \u03c0 and returns a binary label indicating whether the primary policy \u03c0 is likely to deviate from a reference policy \u03c0 without questioning it. We define the deviation of a primary policy \u03c0 from a reference policy \u03c0 as (\u03c0) = primary reward indicating whether the primary policy \u03c0 is likely to deviate from a reference policy \u03c0 without questioning it. Note that the choice of the error metric can be flexibly chosen according to a target task. In this paper, we simply use the distance L2 between a reference steering angle and a predicted steering angle indicating the brake.Then, with this defined deviation, we ignore the optimal security policy."}, {"heading": "3.2 SafeDAgger: Safety Policy in the Loop", "text": "We describe here the proposed SafeDAgger Blue Fonts, which aim to reduce the number of queries to a reference policy during iterations. At the heart of SafeDAgger is the security policy that was introduced in this section. [1] There are two major modifications to the original DAgger strategy from Sec. 2.5.First, we use the secure strategy, rather than the naive strategy, to collect training examples (line 6 in Alg. 1), which allows an agent to simply give up when it is not safe to drive himself and pass control to the reference policy, collecting training examples with a much wider horizon without crashing. This would have been impossible with the original DAgger measures unless the manually enforced adoption of measures was taken. [17] It is certainly possible to set aside a subset of the original training sets for this purpose. [2] Such an intervention was carried out manually by a human driver [14] to highlight the differences between the SafeDAM Blue DAvanilla algorithms and DAV1 policy."}, {"heading": "4 Experimental Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Simulation Environment", "text": "We are using TORCS [1], a race car simulator, for empirical evaluation in this paper. We chose TORCS for the following reasons: First, it has been widely and successfully used as a platform for autonomous race research [10], although most of the previous work, with the exception of [9, 6], is not comparable because they use a radar instead of a camera to observe the condition. Second, TORCS is a lightweight simulator that can be run on a standard workstation. Third, since TORCS is open source software, it is easy to connect it to another software, which in our case is Torch. 3Tracks To simulate driving on motorways with multiple lanes, we modify the original TORCS surface structures by adding different road configurations, such as the number of lanes, the type of lanes and the lanes. 3 We will release a patch TORS between the TORS, allowing a seamless integration of TORS."}, {"heading": "4.2 Data Collection", "text": "We use a car in TORCS driven by a policy to collect data. For each training track, we add 40 cars driven by the reference policy to simulate traffic. In addition to the initial supervised learning phase, we perform up to three iterations. In the case of SafeDAgger, we collect 30k, 30k and 10k of training examples (according to the subset selection in line 6 of Alg. 1.) In the case of the original DAgger, we collect up to 390k of data per iteration and randomly select about 30k, 30k and 10k of training examples. 4.3 Political networks Primary political precision We use a deep revolutionary network followed by five revolutionary layers followed by a series of fully connected layers. This revolutionary network takes as input the pixel-level image taken by a front-facing camera. It predicts the angle of the steering wheel ([\u2212 1]) and whether it will be braked as the existence of a vehicle {1}, including the existence of a single connected policy."}, {"heading": "4.4 Evaluation", "text": "In this context, it should be noted that the measures taken in the past are not measures taken in the past, but measures taken in the past."}, {"heading": "6 Conclusion", "text": "First, we have introduced a security policy that prevents a primary policy from entering a dangerous state by automatically switching between a reference policy and primary policy without calling into question the reference policy. This security policy is used in data collection in the proposed SafeDAgger, which can collect a number of increasingly difficult examples and minimize the number of queries to a reference policy. Extensive experiments on simulated autonomous driving have shown that the SafeDAgger not only puts a reference policy less in question, but also educates a primary policy more efficiently. Imitation learning, in the form of the SafeDAgger, allows a primary policy to learn without catastrophic experience. However, the quality of a learned policy is limited by that of a reference policy. More research into fine-tuning a policy that the SafeDAgger has learned to outperform existing reference policy, for example through enhanced learning [18], must be pursued in the future."}, {"heading": "Acknowledgments", "text": "We thank the support of Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}, {"heading": "A Dataset and Collection Procedure", "text": "We use TORCS [1] to simulate autonomous driving in this paper. The control frequency for driving the car in the simulator is 30 Hz, sufficient for driving below 50 mph.Sensory input We use a front-side camera mounted on a racing car to collect picture frames while the car is driving. Each image is scaled and cut to 160 x 72 pixels with three color channels (R, G and B). In Fig. 4 we show the seven training paths and three test tracks with one sample image frame per lane. Labels How the car is driving, we collect the following twelve variables per frame: 1. Ill 0, 1}: if there is a lane to the left 2. Ir 0, 1}: if there is a lane to the right 3. Icl 0, 1}: if there is a car to the right."}, {"heading": "B Policy Networks and Training", "text": "Primary Policy Network We use a deep Convolutionary Network with five Constitutional Layers, followed by a group of fully connected layers. In Table 5 we detail the configuration of the network. Safe Frames0.987078 0.975920 0.965883 0.960750 0.9579860.954337 0.951110 0.948736 0.946532 0.944643Unsafe FramesSafety Policy Network To implement a security policy, an upstream network with two fully connected hidden layers of rectified linear units is used. This security network takes as input the activations of Conv5 of the primary political network (see Fig. 5.) Training Considering a number of training examples, we use stochastic gradient declines (SGD) with a batch size of 64, a dynamic of 0.9, a weight decrease of 0.001 and an initial learning rate of 0.001 to train a political network."}, {"heading": "C Sample Image Frames", "text": "In Fig. 6, we present twenty sample frames. The top ten frames were classified as safe by a trained security policy (0), while the bottom ten were classified as unsafe (1). It seems that security policy at this time determines the safety of a current condition monitoring according to two criteria: (1) the existence of other cars and (2) the entry into a sharp curve."}], "references": [{"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41\u201348. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "U.M. Mathew Monfort", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "venue": "arXiv preprint arXiv:1604.07316", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning, 28(1):41\u201375", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to search better than your teacher", "author": ["K.-w. Chang", "A. Krishnamurthy", "A. Agarwal", "H. Daume", "J. Langford"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2722\u20132730", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9 Iii", "J. Langford", "D. Marcu"], "venue": "Machine learning, 75(3):297\u2013325", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 315\u2013323", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Evolving large-scale neural networks for vision-based reinforcement learning", "author": ["J. Koutn\u00edk", "G. Cuccu", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 1061\u20131068. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "The wcci 2008 simulated car racing competition", "author": ["D. Loiacono", "J. Togelius", "P.L. Lanzi", "L. Kinnaird-Heether", "S.M. Lucas", "M. Simmerson", "D. Perez", "R.G. Reynolds", "Y. Saez"], "venue": "CIG, pages 119\u2013126. Citeseer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Off-road obstacle avoidance through end-to-end learning", "author": ["U. Muller", "J. Ben", "E. Cosatto", "B. Flepp", "Y.L. Cun"], "venue": "Advances in neural information processing systems, pages 739\u2013746", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["D.A. Pomerleau"], "venue": "Technical report, DTIC Document", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Progress in neural network-based vision for autonomous robot driving", "author": ["D.A. Pomerleau"], "venue": "Intelligent Vehicles\u2019 92 Symposium., Proceedings of the, pages 391\u2013396. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["S. Ross", "J.A. Bagnell"], "venue": "arXiv preprint arXiv:1406.5979", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "arXiv preprint arXiv:1011.0686", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 1765\u20131772. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 28. MIT press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 11, "context": "ALVINN by Pomerleau [13] was a neural network with a single hidden layer that takes as input an image frame from a front-facing camera and a response map from a range finder sensor and returns a quantized steering wheel angle.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "A similar approach was taken later in 2005 to train, this time, a convolutional neural network to drive an off-road mobile robot [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "[3] used a similar, but deeper, convolutional neural network for lane following based solely on a front-facing camera.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": ", [7, 16] and references therein.", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [7, 16] and references therein.", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": "More specifically, we focus on DAgger [16] which works in a setting where the reward is given only implicitly.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "We empirically evaluate the proposed SafeDAgger using TORCS [1], a racing car simulator, which has been used for vision-based autonomous driving research in recent years [9, 6].", "startOffset": 170, "endOffset": 176}, {"referenceID": 4, "context": "We empirically evaluate the proposed SafeDAgger using TORCS [1], a racing car simulator, which has been used for vision-based autonomous driving research in recent years [9, 6].", "startOffset": 170, "endOffset": 176}, {"referenceID": 5, "context": ", [7, 16, 5]), we introduce a concept of cost to a policy.", "startOffset": 2, "endOffset": 12}, {"referenceID": 14, "context": ", [7, 16, 5]), we introduce a concept of cost to a policy.", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": ", [7, 16, 5]), we introduce a concept of cost to a policy.", "startOffset": 2, "endOffset": 12}, {"referenceID": 5, "context": "The issue with supervised learning can however be addressed by imitation learning or learning-to-search [7, 16].", "startOffset": 104, "endOffset": 111}, {"referenceID": 14, "context": "The issue with supervised learning can however be addressed by imitation learning or learning-to-search [7, 16].", "startOffset": 104, "endOffset": 111}, {"referenceID": 14, "context": "DAgger is one such imitation learning algorithm proposed in [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 13, "context": "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 15, "context": "First, as noted in [17], a human operator cannot drive well without actual feedback, which is the case of DAgger as the primary policy drives most of the time.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy \u03c0safe, to which we refer as a safety policy.", "startOffset": 78, "endOffset": 88}, {"referenceID": 14, "context": "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy \u03c0safe, to which we refer as a safety policy.", "startOffset": 78, "endOffset": 88}, {"referenceID": 3, "context": "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy \u03c0safe, to which we refer as a safety policy.", "startOffset": 78, "endOffset": 88}, {"referenceID": 17, "context": "Relationship to a Value Function A value function V (s) in reinforcement learning computes the reward a given policy \u03c0 can achieve in the future starting from a given state s [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "This would have been impossible with the original DAgger unless the manually forced take-over measure was implemented [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "2 Such intervention has been done manually by a human driver [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "This has an effect of automated curriculum learning [2] with a mix strategy [20], where the safety policy selects training examples of appropriate difficulty at each iteration.", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "In AggreVaTe [15], for instance, the roll-out by a reference policy may be executed not from a uniform-randomly selected time point, but from the time step when the safety policy returns 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "A similar adaptation can be done with LOLS [5].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.", "startOffset": 149, "endOffset": 155}, {"referenceID": 4, "context": "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "We have found this multi-task approach to easily outperform a single-task network, confirming the promise of multi-task learning from [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "More research in finetuning a policy learned by the SafeDAgger to surpass existing, reference policies, for instance by reinforcement learning [18], needs to be pursued in the future.", "startOffset": 143, "endOffset": 147}], "year": 2016, "abstractText": "One way to approach end-to-end autonomous driving is to learn a policy function that maps from a sensory input, such as an image frame from a front-facing camera, to a driving action, by imitating an expert driver, or a reference policy. This can be done by supervised learning, where a policy function is tuned to minimize the difference between the predicted and ground-truth actions. A policy function trained in this way however is known to suffer from unexpected behaviours due to the mismatch between the states reachable by the reference policy and trained policy functions. More advanced algorithms for imitation learning, such as DAgger, addresses this issue by iteratively collecting training examples from both reference and trained policies. These algorithms often requires a large number of queries to a reference policy, which is undesirable as the reference policy is often expensive. In this paper, we propose an extension of the DAgger, called SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous driving. We evaluate the proposed SafeDAgger in a car racing simulator and show that it indeed requires less queries to a reference policy. We observe a significant speed up in convergence, which we conjecture to be due to the effect of automated curriculum learning.", "creator": "LaTeX with hyperref package"}}}