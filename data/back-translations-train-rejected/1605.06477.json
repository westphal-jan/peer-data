{"id": "1605.06477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Regression with n$\\to$1 by Expert Knowledge Elicitation", "abstract": "We consider regression under the \"extremely small $n$ large $p$\" condition. In particular, we focus on problems with so small sample sizes $n$ compared to the dimensionality $p$, even $n\\to 1$, that predictors cannot be estimated without prior knowledge. Furthermore, we assume all prior knowledge that can be automatically extracted from databases has already been taken into account. This setup occurs in personalized medicine, for instance, when predicting treatment outcomes for an individual patient based on noisy high-dimensional genomics data. A remaining source of information is expert knowledge which has received relatively little attention in recent years. We formulate the inference problem of asking expert feedback on features on a budget, present experimental results for two setups: \"small $n$\" and \"n=1 with similar data available\", and derive conditions under which the elicitation strategy is optimal. Experiments on simulated experts, both on simulated and genomics data, demonstrate that the proposed strategy can drastically improve prediction accuracy.", "histories": [["v1", "Fri, 20 May 2016 19:19:08 GMT  (132kb)", "http://arxiv.org/abs/1605.06477v1", null], ["v2", "Sat, 24 Sep 2016 21:58:12 GMT  (132kb)", "http://arxiv.org/abs/1605.06477v2", "In International Conference on Machine Learning and Applications (IEEE ICMLA'16)"], ["v3", "Tue, 7 Feb 2017 01:39:35 GMT  (132kb)", "http://arxiv.org/abs/1605.06477v3", "In Proceedings of the 15th IEEE International Conference on Machine Learning and Applications (IEEE ICMLA'16)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marta soare", "muhammad ammad-ud-din", "samuel kaski"], "accepted": false, "id": "1605.06477"}, "pdf": {"name": "1605.06477.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["first.last@aalto.fi"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.06 477v 1 [cs.L G] 20 May 2We look at regression under the \"extremely small n large p\" condition. In particular, we focus on problems with such small sample sizes n compared to dimensionality p, even n \u2192 1, that predictors without prior knowledge cannot be estimated. Furthermore, we assume that all prior knowledge that can be automatically extracted from databases has already been taken into account. In personalized medicine, for example, this constellation occurs when predicting treatment outcomes for a single patient based on noisy high-dimensional genomic data. A remaining source of information is expertise that has received relatively little attention in recent years. We formulate the consequence problem of obtaining expert feedback on characteristics of a budget, presenting experimental results for two arrangements: \"small n\" and \"n = 1 with similar available data,\" and deriving optimal conditions from it to show experiments simulated on both statistical and predictive strategies that can both improve the proposed by experts."}, {"heading": "1 Introduction", "text": "In the \"small n large p\" regression setting, the task is to make predictions based on a few noisy samples of high-dimensional data. It is impossible to address this problem without relying on prior knowledge. Normally, prior knowledge is represented by known structures in data, such as groupings of variables in paths or scarcity. However, in the extreme of n \u2192 1, none of them is sufficient and not even their combination. In this case, a potential source of additional useful knowledge comes from human expertise, which is usually expensive to extract. In this paper, we look at how to efficiently elicit expert knowledge under a limited feedback budget, creating the simplistic assumption that the user is able to provide accurate information to queries."}, {"heading": "1.1 Dealing with small n large p", "text": "\"Small n large p\" data (also known as \"fat data\") are characterized by a large number of predictors (p) that need to be estimated from a small number of data (small sample size n).This situation is typical of medical data where observations (such as drug reactions) are very rare, a very large number of potentially relevant covariants are available, e.g. from genomic measurements, and additional data can only be obtained at high costs.To address this problem and efficiently limit the selection of relevant characteristics, machine learning algorithms typically rely on known structures in the data (e.g. networks, pathways, linear structure of DNA).This type of prior knowledge can be taken into account with regulatory techniques (see e.g. [10, 17]) or priors in the Bayesian inference. Another efficient method to address the lack of data is the transfer between related tasks (see [15] for a survey)."}, {"heading": "1.2 Expert Knowledge Elicitation", "text": "Human judgment, especially expert knowledge, is often crucial for decision-making processes. Expertise is extensively studied in a wide range of application scenarios, from preference model development [1, 4] to medical science and shipping [6, 20], as well as interactive learning for student modelling [5, 12]. The methodological decisions required in defining expert feedback vary from application to application and depend on several conditions, such as (i) the expertise available (e.g.: How much should the expert be trusted?); (ii) the type of information to be obtained (e.g. learning a preference, estimating a quantity, identifying risky options); (iii) the knowledge constraints necessary to obtain the answer to the question of how many interactions / how much feedback the expert can provide."}, {"heading": "1.3 Contributions and Outline", "text": "In this paper, we propose to solve the \"extremely small n, large p\" problem for regression on the assumption that accurate expert knowledge is available but limited to a budget. As far as we know, this is the first study to elicit knowledge from this perspective and to sample sizes of n \u2192 1. This structure is particularly important for personalized medicine, but not limited to it. The rest of the paper is organized as follows: In Section 2, we offer a detailed description of the expert feedback scenario, in which we address the problem of lack of data in a regression task. In Section 4, we also present our assumptions on the use of expert feedback. Subsequently, we propose an effective algorithm to select which characteristics expert feedback is most asked for in order to reduce the loss, in Section 3. We analyze its optimality. In Section 4, we present simulations of the behavior of our expert feedback by demonstrating its validation strategy in a simple synthesis."}, {"heading": "2 Preliminaries", "text": "In this section, we present the problem structure and the first formulation for gaining expert knowledge in a predictive task. In particular, we explain our assumptions about the type of feedback the expert can give. The framework has been chosen as simple as possible, while still capturing the essential elements of large p, small n data. Specifically, we describe the problem with the terminology of predicting treatment efficacy, but the structure is of course more generally applicable."}, {"heading": "2.1 Problem Description", "text": "The aim is to improve the prediction of the effect of a treatment on a target patient by incorporating feedback from an expert. Let's assume a small set of observational data that can be used to learn a first predictor. The sentence consists of n observed treatment reactions y1,..., yn, stored in the Y-R1 \u00b7 n vector, originating from n different patients i (i = 1,.., n) who had previously received the same treatment. Let's label the matrix of genomic characteristics with X, where the size of X is n \u00b7 p, and on each line i = 1,... n we have the p-genomic characteristics corresponding to patient i, designated by xi. We focus on setups with p-n. For the new \"target patient,\" the same genomic measurements are available, designated x-R1 = [x-R1)..., and the goal is to predict the treatment response as accurately as possible."}, {"heading": "2.2 Data Assumptions", "text": "Linear Regression. We assume that there is a linear relationship between the genomic characteristics of the patient and the expected outcome of the treatment. More specifically, we choose a linear regression setting in which for each patient i, yi = xi\u03b8 + \u03b7, (1) in \u03b8-Rp is an unknown parameter underlying the linear function, and \u03b7 is i.i.d white noise quantifying the inherent noise in the measurements of drug effects for each patient. Coordinate juncture (i) of the parameter vector encodes the weight or relevance I have in calculating a patient's treatment response. Sparsity. We assume that the weight vectors \u03b8 s-sparse (s < < p), or in other words that many of the characteristics have zero weight or relevance in the drug response that I have in calculating a patient's treatment response."}, {"heading": "2.3 Expert Feedback", "text": "For clinical and behavioral variables, an expert can know how much they explain about risk1. For linear regression, the percentage of declared variance is the square of the correlation coefficient (under simplistic assumptions). If the expert is uncertain, he / she can give feedback on the importance of variables. In these cases, our formulation is an approximation of the patient-specific prior knowledge of the expert, and given the patient's treatment history, it is possible to make educated guesses about which hypotheses of disease mechanisms remain as potential hypotheses. In these cases, our formulation is an approximation of the patient-specific prior knowledge of the expert. Expert knowledge assumes that the expert is able to report the correct value of the prediction (i), but answering requires costs and therefore we cannot simply ask correct values for the complete process."}, {"heading": "2.4 Performance Measure", "text": "Let us define the loss of A as the expected quadratic loss for the target patient x *: LA = E [(x \u043a \u03b8, A \u2212 y *) 2] = E [(x \u043a, A \u2212 x \u043a) 2], (3) where the expectation is assumed about all sources of interference resulting from the noisy observations of the drug effect and the noisy selection of characteristics by the expert. In view of the m-interactions with the expert user, our goal is to obtain feedback on the most informative or relevant characteristics, so that we minimize the loss of the target. Thus, the optimal algorithm A * is defined as LA * = minA | comprehensive, init LA. It is important to emphasize that the overall performance of an algorithm depends strictly on the estimate obtained from the training data of other patients, which differ from the distribution of the possibly very imperfect data. In this work, the focus is on identifying experts who enable the maximum improvement of the target prediction."}, {"heading": "3 Algorithm", "text": "We propose to learn the regression parameters in two steps: Firstly, a first estimate \u03b8-init is learned from the \"small n large p\" training data with appropriate regulation and the information in this data set is efficiently recorded; the estimate is then improved by the use of knowledge by an expert; and the estimation error obtained with the first estimate \u03b8-init is the basis for comparing the potential improvement achieved by drawing on expert knowledge."}, {"heading": "3.1 Description", "text": "Given the assumed nature of the feedback (described in Section 2.3) and the budget constraints, the goal is to quickly identify the most useful feedback to reduce the loss. On closer inspection of the definition of loss, we easily obtain the following intuitions about the behavior of the algorithm: \u2022 Trivially, the algorithm should not seek feedback on the same feedback more than once, since the feedback from the expert is precise and we directly get the right value for the feedback (i). \u2022 By decomposing the loss as the sum of p-squared point-by-product, we can say that it is most useful to reduce the greatest feedback from the experts to 0 (through the feedback from the experts). \u2212 (I) \u00b7 (."}, {"heading": "3.2 Analysis", "text": "Next, we will specify conditions under which the sequence of decoding of Algorithm 1 is optimal. Denote. i = x. (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i).....). (i). (i). (i). (i). (i). (i). (i). (i). (i).). (i). (i). (i). (i). (i).). (i). (i).). (i). (i). (i).....). (i). (i). (i).). (i).). (i). (i). (i)...... (i). (). (i).). (i). (i). (i).)...... (i)."}, {"heading": "4 Experiments", "text": "We illustrate the performance of algorithm 1, which we refer to in the continuation of Largest Product Feature, in two experimental constellations. We start with a simple synthetic setting (described in Section 4.1) and then use a genomic dataset for a more elaborate simulation (as described in Section 4.2). In both constellations, we compare the loss of algorithm 1 with the loss of the following strategies: \u2022 No interaction: the base algorithm whose performance is given by the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error of the predictive error \u2022 Random: works by randomly (without repetition) selecting m features from which experts can obtain feedback \u2022 Largest Target Feature: calls for feedback on the largest coordinator of the target error of the target type and is not a target loss during the interval *."}, {"heading": "4.1 Synthetic Data", "text": "We randomly generated the training set X with k = 1000 lines and 150 feedbacks from a normal distribution with mean 0 and variance 1. We randomly assigned a sparse weight vector \u03b8, so that 5 of its feedbacks are not zero and come from a normal distribution with mean 0 and variance 1, while the remaining 145 feedbacks are 0. The output variable Y is then calculated using n noisy observations of the dot product between \u03b8 and randomly selected vectors x-X. We use the Glmnet package [7] to estimate subjects, and we vary the number of training samples from 5 to 30, while the number of expert feedbacks we assume can achieve growth from 0 to 10. We randomly select a target patient and calculate the corresponding loss for each feedback value. We record the average target loss over 100 randomly selected target patients, with the product feedbacks assumed by the target group 1 (the pre-performance scenario) assuming a different result in terms of small target performance."}, {"heading": "4.2 GDSC Dataset", "text": "We demonstrate the usefulness of our approach by applying it to the genomics data in the GDSC dataset, where we obtain results similar to those obtained from the synthetic data. In the following, we briefly describe the content of the dataset, then explain how we simulated the basic truth. The results obtained (Figure 2) follow the same trends as in the synthetic data, which in turn show an improvement in all strategies that use interaction for all values of the n.Genomics of Drug Sensitivity in Cancer (GDSC) data. We used data from the Genomics of Drug Sensitivity in Cancer Project by Wellcome Trust Sanger Institute (release 5.0, June 2014, www.cancerrxgene.org), consisting of 124 drugs and a panel of 124 human cancer cell lines for which full drug response measurements are available."}, {"heading": "5 Model Extensions", "text": "In order to gain more knowledge about the behaviour of the algorithm, we relax the model in two central aspects. These relaxations also make the model applicable to a much larger number of practical scenarios: In numerical simulations, unless otherwise specified, the setting for the \"n = 1\" scenario remains the same as described in Section 4.1."}, {"heading": "5.1 Feedback on a subset of features", "text": "We analyzed the predictive power of our approach when the expert can only give feedback on a subset of characteristics. To simulate this, we associate each characteristic with a value of either 1, which means that the expert has knowledge of that characteristic (and can give feedback), or 0, for the characteristics about which the expert has no prior knowledge. The Biggest Product (subset) characteristic algorithm selects the characteristics through which feedback is asked as before, but if the selected characteristic is not known by the expert (i.e., its associated value is 0), feedback on the next \"Biggest Product Feature\" with an associated value of 1 is asked. Instead of receiving feedback on the largest characteristics, we now receive expert feedback on the m \"Largest Product Features\" with associated value. The analysis in Section 3.2 also applies to this more general attitude, under the same assumptions. In fact, we again assume that expert feedback counts among the characteristics that allow the choice more than any other cost function."}, {"heading": "5.2 Noisy Expert", "text": "We simulated an experiment for the \"n = 1\" scenario, in which each feedback is influenced by normally distributed noise, centered and with a deviation between 0.1 and 0.5, with expert uncertainty (the range of (true) characteristic values being [0, 1]). Noisy feedback is used for all strategies. In personalized medicine, it is plausible to assume that expert feedback exhibits lower deviations in the weight of genomic characteristics commonly known as relevant, but much higher deviations for rarely encountered genes and their mutations. In Figure 4, we can see that the largest product property is even better than the baseline No interaction in the presence of noisy feedback. Although noise decreases the deviation between the prefabrication performance of our algorithm and the baseline, this is expected 2Note that the plot for \"feature subset = 100%\" will be the deviation from the curve (1) and the (1)."}, {"heading": "6 Conclusion", "text": "Based on loud estimates based on extremely small sample sizes, we demonstrated empirically how to improve the predictions that can be made by providing only a small amount of expert feedback. Specifically, we looked at a simplified problem formulation with strict budget constraints for accurate expert feedback; the simplified problem should be a starting point that opens up both new interesting theoretical questions and a series of applied work toward new solutions to the current very near-term problem of personalized medicine; the practically important goal of developing better predictions of treatment outcomes for individual patients is to estimate predictive results for the sample with n = 1, which will require creative solutions; new approaches to querying and incorporating available expertise will naturally be based on broad applicability."}, {"heading": "A Supplemental Empirical Results", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Preference elicitation for general random utility models", "author": ["Hossein Azari Soufiani", "David C Parkes", "Lirong Xia"], "venue": "In Uncertainty in Artificial Intelligence: Proceedings of the 29th Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The cancer cell line encyclopedia enables predictive modelling of anticancer drug sensitivity", "author": ["Jordi Barretina", "Giordano Caponigro", "Nicolas Stransky", "Kavitha Venkatesan", "Adam A Margolin", "Sungjoon Kim", "Christopher J Wilson", "Joseph Leh\u00e1r", "Gregory V Kryukov", "Dmitriy 13  Sonkin"], "venue": "Nature, 483(7391):603\u2013607,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Optimal expert elicitation to reduce interval uncertainty", "author": ["Nadia Ben Abdallah", "Sebastien Destercke"], "venue": "In AUAI press, editor, Uncertainty in Artificial Intelligence, Uncertainty In Artificial Intelligence (UAI)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Preference-based rank elicitation using statistical models: The case of mallows", "author": ["R\u00f3bert Busa-Fekete", "Eyke H\u00fcllermeier", "Bal\u00e1zs Sz\u00f6r\u00e9nyi"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Using bayesian networks to manage uncertainty in student modeling", "author": ["Cristina Conati", "Abigail Gertner", "Kurt Vanlehn"], "venue": "User modeling and user-adapted interaction,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Incorporating expert knowledge when learning Bayesian network structure: a medical case study", "author": ["M Julia Flores", "Ann E Nicholson", "Andrew Brunskill", "Kevin B Korb", "Steven Mascaro"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["Jerome Friedman", "Trevor Hastie", "Rob Tibshirani"], "venue": "Journal of statistical software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Systematic identification of genomic markers of drug sensitivity in cancer cells", "author": ["Mathew J Garnett", "Elena J Edelman", "Sonja J Heidorn", "Chris D Greenman", "Anahita Dastur", "King Wai Lau", "Patricia Greninger", "I Richard Thompson", "Xi Luo", "Jorge Soares"], "venue": "Nature, 483(7391):570\u2013575,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Quantifying expert opinion in linear regression problems", "author": ["Paul H Garthwaite", "James M Dickey"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1988}, {"title": "Statistical challenges of high-dimensional data", "author": ["Iain M Johnstone", "D Michael Titterington"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1906}, {"title": "Interactive elicitation of opinion for a normal linear model", "author": ["Joseph B Kadane", "James M Dickey", "Robert L Winkler", "Wayne S Smith", "Stephen C Peters"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "An interactive approach for Bayesian network learning using domain/expert knowledge", "author": ["Andr\u00e9s R Masegosa", "Seraf\u00edn Moral"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Uncertain Judgements. Eliciting Experts", "author": ["Anthony O\u2019Hagan", "Caitlin E. Buck", "Alireza Daneshkhah", "J. Richard Eiser", "Paul H. Garthwaite", "David J. Jenkinson", "Jeremy E. Oakley", "Tim Rakow"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Comparison of three expert elicitation methods for logistic regression on predicting the presence of the threatened brush-tailed rock-wallaby", "author": ["Rebecca A. O\u2019Leary", "Samantha Low Choy", "Justine V. Murray", "Mary Kynn", "Robert Denham", "Tara G. Martin", "Kerrie Mengersen"], "venue": "Petrogale penicillata. Environmetrics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Interactive intent modeling: Information discovery beyond", "author": ["Tuukka Ruotsalo", "Giulio Jacucci", "Petri Myllym\u00e4ki", "Samuel Kaski"], "venue": "search. Commun. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Genomics of Drug Sensitivity in Cancer (GDSC): a resource for therapeutic biomarker discovery in cancer cells", "author": ["Wanjuan Yang", "Jorge Soares", "Patricia Greninger", "Elena J Edelman", "Howard Lightfoot", "Simon Forbes", "Nidhi Bindal", "Dave Beare", "James A Smith", "I Richard Thompson"], "venue": "Nucleic acids research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Expert elicitation and Bayesian network modeling for shipping accidents: A literature review", "author": ["Guizhen Zhang", "Vinh V Thai"], "venue": "Safety Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": ", [10, 17]) or priors in Bayesian inference.", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": ", [10, 17]) or priors in Bayesian inference.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "Another efficient way to address the lack of data is to transfer knowledge across related tasks (see [15] for a survey).", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 140, "endOffset": 146}, {"referenceID": 3, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 140, "endOffset": 146}, {"referenceID": 5, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 188, "endOffset": 195}, {"referenceID": 18, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 188, "endOffset": 195}, {"referenceID": 4, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 251, "endOffset": 258}, {"referenceID": 11, "context": "Expert knowledge elicitation techniques have been widely studied in a wide range of application settings, from preference model elicitation [1, 4] to medical science and shipping industry [6, 20], as well as interactive learning for student modelling [5, 12].", "startOffset": 251, "endOffset": 258}, {"referenceID": 8, "context": "An important line of work [9, 11] studies methods of quantifying subjective opinion about the coefficients of linear regression models.", "startOffset": 26, "endOffset": 33}, {"referenceID": 10, "context": "An important line of work [9, 11] studies methods of quantifying subjective opinion about the coefficients of linear regression models.", "startOffset": 26, "endOffset": 33}, {"referenceID": 12, "context": "In the studies on prior elicitation [13], some also on regression [14], the focus has often been on how to elicit reliable prior knowledge, after which the Bayesian inference machinery takes care of the rest.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "In the studies on prior elicitation [13], some also on regression [14], the focus has often been on how to elicit reliable prior knowledge, after which the Bayesian inference machinery takes care of the rest.", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "We use the glmnet package [7] for estimating \u03b8\u0302init, and we vary the number of training samples from 5 to 30, while the number of expert feedbacks that we assume we can obtain grows from 0 to 10.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "org) [19, 8] consisting of 124 drugs and a panel of 124 human cancer cell lines for which complete drug response measurements are available.", "startOffset": 5, "endOffset": 12}, {"referenceID": 7, "context": "org) [19, 8] consisting of 124 drugs and a panel of 124 human cancer cell lines for which complete drug response measurements are available.", "startOffset": 5, "endOffset": 12}, {"referenceID": 1, "context": "To learn this \u201cpseudo-ground truth,\u201d we employed sparse linear regression using the glmnet package, which has been frequently used to identify genomic features of drug responses [2, 8].", "startOffset": 178, "endOffset": 184}, {"referenceID": 7, "context": "To learn this \u201cpseudo-ground truth,\u201d we employed sparse linear regression using the glmnet package, which has been frequently used to identify genomic features of drug responses [2, 8].", "startOffset": 178, "endOffset": 184}, {"referenceID": 0, "context": "5, encoding the expert uncertainty (the range of the (true) feature values is [0, 1]).", "startOffset": 78, "endOffset": 84}, {"referenceID": 2, "context": "For future work, a sensible formulation for the \u201csmall n, large p\u201d regression problem is to find the optimal expert queries for reducing the interval uncertainty for regression coefficients, with strategies recently studied and applied in reliability analysis problems [3].", "startOffset": 269, "endOffset": 272}, {"referenceID": 15, "context": "Similar expert interaction approaches were shown to be effective for user intent modelling in [16].", "startOffset": 94, "endOffset": 98}], "year": 2017, "abstractText": "We consider regression under the \u201cextremely small n large p\u201d condition. In particular, we focus on problems with so small sample sizes n compared to the dimensionality p, even n \u2192 1, that predictors cannot be estimated without prior knowledge. Furthermore, we assume all prior knowledge that can be automatically extracted from databases has already been taken into account. This setup occurs in personalized medicine, for instance, when predicting treatment outcomes for an individual patient based on noisy high-dimensional genomics data. A remaining source of information is expert knowledge which has received relatively little attention in recent years. We formulate the inference problem of asking expert feedback on features on a budget, present experimental results for two setups: \u201csmall n\u201d and \u201cn=1 with similar data available\u201d, and derive conditions under which the elicitation strategy is optimal. Experiments on simulated experts, both on simulated and genomics data, demonstrate that the proposed strategy can drastically improve prediction accuracy.", "creator": "LaTeX with hyperref package"}}}