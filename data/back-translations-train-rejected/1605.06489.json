{"id": "1605.06489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "abstract": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).", "histories": [["v1", "Fri, 20 May 2016 19:51:37 GMT  (1968kb,D)", "http://arxiv.org/abs/1605.06489v1", null], ["v2", "Tue, 29 Nov 2016 17:29:01 GMT  (1661kb,D)", "http://arxiv.org/abs/1605.06489v2", "Short workshop paper for EMDNN 2016"], ["v3", "Wed, 30 Nov 2016 15:32:03 GMT  (4435kb,D)", "http://arxiv.org/abs/1605.06489v3", "Updated full version of paper, in full letter paper two-column paper. Includes many textual changes, updated CIFAR10 results, and new analysis of inter/intra-layer correlation"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["yani ioannou", "duncan robertson", "roberto cipolla", "antonio criminisi"], "accepted": false, "id": "1605.06489"}, "pdf": {"name": "1605.06489.pdf", "metadata": {"source": "CRF", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "authors": ["Yani Ioannou", "Duncan Robertson", "Roberto Cipolla", "Antonio Criminisi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper describes a new method for generating computationally efficient and compact Convolutionary Neural Networks (CNNs) using a novel sparse connecting structure resembling a tree root, which allows for a significant reduction in computing costs and the number of parameters of state-of-the-art deep CNNs without compromising accuracy. It is known that the structure of a neural network is critical to its ability to learn and generalize from designated training data. While it has been shown that an infinitely broad hidden layer is a universal approximative [14], wide flat networks do not learn as well as thin deeper ones - as recent research shows, 35, 33, 10 - this does not appear to be a limitation of finite capacity, since deep networks are demonstrably represented by flat networks."}, {"heading": "2 Related Work", "text": "Reducing co-dependence in deep networks. Hinton et al. [13] introduced dropout networks to regulate deep networks. However, when training a network layer with dropout, a random subset of neurons is excluded from both forward and backward development. However, one limitation of the dropout network is that it increases the number of trainings required for convergence, typically reduced by a factor of two. Authors observe that this approach has some similarities to the use of model sets, but another effective method of increasing generalization is that it generally reduces the number of trainings required for convergence by a factor of two."}, {"heading": "3 Hierarchical Filter Groups", "text": "In this section, we present the main contribution of our work - an investigation of the use of hierarchical filter groups to reduce computational complexity and model size compared to modern deep image recognition networks."}, {"heading": "3.1 Reduced Co-adaption and Filter Groups", "text": "It has been shown that reducing co-adaptation of filters is beneficial for generalization in deep networks [13, 8, 3]. Co-adaptation occurs when hidden layer filters (or neurons) rely on each other to adjust training data and are highly correlated. However, instead of using a modified loss, regularization penalty or randomized network connectivity during training to prevent co-adaptation of characteristics, we take a much more direct approach. We use hierarchical filter groups (see \u00a7 2) to allow the network itself to learn independent filters. By limiting connectivity between filters on subsequent layers, the network is forced to learn filters of limited interdependence. This reduced connectivity also reduces the complexity of calculations and model size. Unlike methods to increase efficiency of deep networks by approximating pre-existing networks, this means that our model can only be trained on existing ones (see \u00a7 2)."}, {"heading": "3.2 Network Topology", "text": "Comprehensive research into the possible topologies for filter grouping within state-of-the-art deep networks is prohibitively expensive. Instead, we are based on a limited but interesting subset of hierarchical topologies, each of which is illustrated in Fig. 3. Each of these topologies assumes different basic assumptions about co-dependence between filters within each layer and the entire network architecture. columns. Column-like topologies assume that filters are consistently related to depth and that this co-dependence is relatively low across the network. AlexNet uses a two-column architecture for most layers. Tree-like topologies, such as those from Ioannou et al. [16] assume in conditional networks that either filters with depth become less co-dependent, or the number of filters grows with depth enough that there are many more filters in deep layers than required."}, {"heading": "4 Results", "text": "Here we present the results of a completely restructured (as described in \u00a7 3) state-of-the-art network architectures for image classification on the data sets CIFAR10 [23] and ILSVRC [32]."}, {"heading": "4.1 Improving Network in Network on CIFAR-10", "text": "Network in Network (NiN) [27] is an almost state-of-the-art network for CIFAR10 [23]. It consists of 3 spatial (5 x 5, 3 x 3) revolutionary layers with a large number of filters (192) interspersed with low-dimensional embedding (1 x 1) layers. We replicated the standard NiN network architecture as described by Lin et al. [27], but with state-of-the-art training methods. We trained with random 32 x 32 truncated and mirrored images from 4-pixel zero padded subtracted images, as described in [8, 10]. We also used the initialization of He et al. [11] and batch normalization [17]. With this configuration, the ZCA brightening was not needed to reproduce the validation accuracy of the image files obtained in [27]."}, {"heading": "4.2 Improving Residual Networks on ILSVRC", "text": "In fact, it is so that most of them are able to survive themselves if they do not survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is as if they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is as if they are able to survive themselves."}, {"heading": "4.3 Improving GoogLeNet on ILSVRC", "text": "We replicated the network as described by Szegedy et al. [35], except for the non-use of training extensions apart from random harvests and reflections (as supported by Caffe [20]). To train, we used the initialization of [11] modified for compound layers [16] and batch normalization without scale and bias [17]. At test time, we evaluated only the central image editing. While maintaining the original number of filters per shift, we trained networks with different degrees of filter grouping as described in Table 6. While the intonation1 See \u00a7 5 for an explanation of the GPU time disparity. 2 https: / github.com / facebook / fb.resnet.torchitecture is relatively complex, for simplicity we always use the same number of groups within each of the different filter sizes, although they had different cardinalities."}, {"heading": "5 GPU Implementation", "text": "Our experiments show that our method can achieve a significant reduction in CPU and GPU runtime for state-of-the-art CNNs without compromising accuracy. However, the reductions in GPU runtime were smaller than would have been expected based on theoretical predictions of computational complexity (FLOPs). However, we believe that this is largely a result of optimizing Caffe for existing network architectures (in particular AlexNet and GoogLeNet) that do not use a high degree of filter grouping. Caffe is currently paralleling across filter groups by using multiple CUDA streams to run multiple CuBLAS matrix multiplications simultaneously."}, {"heading": "6 Future Work", "text": "In this work, we focused on the use of homogeneous filter groups (with a uniform distribution of filters in each group), which may not be optimal. Heterogeneous filter groups can better reflect filter co-dependencies in deep networks."}, {"heading": "7 Conclusion", "text": "We have studied the effects of using complex hierarchical arrangements of filter groups in CNNs and demonstrated that a structured reduction in the degree of filter grouping with depth - a \"root\" topology - can allow us to obtain more efficient variants of state-of-the-art networks without compromising accuracy. Our method seems to complement existing methods such as low-dimensional embedding and can be used more efficiently to train deep networks than methods that only approximate the weights of a pre-trained model. We have validated our method by creating more efficient variants of the status-of-the-art Network-in-Network, Googlenet and ResNet architectures that were evaluated on the CIFAR10 and ILSVRC datasets. Our results show comparable accuracy with the base architecture with fewer parameters and much less calculation (measured by CPU and GPU timings). For Network-in-on CIF10, our net PILC value is lower than our net RILC value of 27%, and our net PILC value is lower than PILC-47%."}], "references": [{"title": "Do Deep Nets Really Need to be Deep", "author": ["L.J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Compressing Neural Networks with the Hashing Trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "JMLR Proceedings,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reducing Overfitting in Deep Networks by Decorrelating Representations", "author": ["M. Cogswell", "F. Ahmed", "R.B. Girshick", "L. Zitnick", "D. Batra"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Predicting Parameters in Deep Learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M.A. Ranzato", "N. deFreitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "R.Fergus: Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neocognitron: A self-organizing neural network model for a mechanish of pattern recognition unaffected by shifts in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Deformable part models are convolutional neural networks", "author": ["R. Girshick", "F. Iandola", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "JMLR Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Decision Forests, Convolutional Networks and the Models in- Between", "author": ["Y. Ioannou", "D. Robertson", "D. Zikic", "P. Kontschieder", "J. Shotton", "M. Brown", "A. Criminisi"], "venue": "Tech. Rep. MSR-TR-2015-58, Microsoft Research (Apr", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "author": ["Y. Ioannou", "D.P. Robertson", "J. Shotton", "R. Cipolla", "A. Criminisi"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32 nd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices", "author": ["C. Jhurani", "P. Mullowney"], "venue": "Journal of Parallel and Distributed Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "author": ["Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Tech. rep., University of Toronto (Apr", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "K.Q. (eds.) NIPS. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Simplifying convnets for fast learning", "author": ["F. Mamalet", "C. Garcia"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Fast training of convolutional networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Perceptrons. MIT press", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1988}, {"title": "Spectral Representations for Convolutional Neural Networks", "author": ["O. Rippel", "J. Snoek", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems. pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Very Deep Convolutional networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Learning Separable Filters", "author": ["A. Sironi", "B. Tekin", "R. Rigamonti", "V. Lepetit", "P. Fua"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 32, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 30, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 9, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 0, "context": "This does not appear to be a limitation of finite capacity, since deep networks have been shown to be representable by shallow networks [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 96, "endOffset": 99}, {"referenceID": 32, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 4, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 23, "context": "networks using large training datasets [24].", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "weight decay, dropout [13]) deep networks are susceptible to severe over-fitting (see \u00a72).", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Convolutional Neural Networks (CNNs) [6, 26] embody this idea, exploiting prior knowledge of the locality of natural image structure to design neural architectures with more limited, but salient, connectivity than a fully-connected neural network, that is in turn easier to learn.", "startOffset": 37, "endOffset": 44}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) [6, 26] embody this idea, exploiting prior knowledge of the locality of natural image structure to design neural architectures with more limited, but salient, connectivity than a fully-connected neural network, that is in turn easier to learn.", "startOffset": 37, "endOffset": 44}, {"referenceID": 15, "context": "More recently, learning CNNs with low rank filters was found to have a regularizing effect, improving generalization compared to a CNN with only full rank filters [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 28, "context": "Interestingly, this is in stark contrast to what we understand of biological neural networks, where we see \u201chighly evolved arrangements of smaller, specialized networks which are interconnected in very specific ways\u201d [30].", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": "[13] introduced dropout for regularization of deep networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] have suggested that dropout provides little incremental accuracy improvement compared to simply training using batch normalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] observe a correlation between the cross-covariance of hidden unit activations and overfitting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] use this loss on the fully connected layers at the end of deep networks such as AlexNet and demonstrate an increase in generalization accuracy comparable with that obtained by using dropout.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 17, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 31, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 24, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 15, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 103, "endOffset": 111}, {"referenceID": 29, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 116, "endOffset": 123}, {"referenceID": 20, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 116, "endOffset": 123}, {"referenceID": 23, "context": "[24] is the use of \u2018filter groups\u2019 in the convolutional layers of a CNN (see Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Also surprising, and not explicitly stated in [24], is the fact that the AlexNet network has approximately 57% fewer connection weights than the corresponding network without filter groups (see Fig.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "This method is used in most state of the art networks for image classification to reduce computation [35, 10].", "startOffset": 101, "endOffset": 109}, {"referenceID": 9, "context": "This method is used in most state of the art networks for image classification to reduce computation [35, 10].", "startOffset": 101, "endOffset": 109}, {"referenceID": 14, "context": "[15] describe conditional networks, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] propose a CNN architecture that is highly optimized for computational efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 24, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 20, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 17, "context": "[18] propose approximating the convolutional filters in a pre-trained network with representations that are low-rank both in the spatial and the channel domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 7, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 2, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 15, "context": "[16], in conditional networks, assume that either filters become less co-dependent with depth, or the number of filters grows with depth enough that there are many more filters in deep layers than required.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] evaluated their architecture on VGG networks, which are very large, and over-parametrized even compared to state-of-the-art networks such as residual networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "If we assume that filter responses identify parts (or more elemental features), then there should be more filter co-dependence with depth, as more complex relationships emerge [7].", "startOffset": 176, "endOffset": 179}, {"referenceID": 22, "context": "Here we present results of training from scratch re-structured (as described in \u00a73) state-of-the-art network architectures for image classification on the CIFAR10 [23] and ILSVRC [32] datasets.", "startOffset": 163, "endOffset": 167}, {"referenceID": 22, "context": "Network in Network (NiN) [27] is a near state-of-the-art network for CIFAR10 [23].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "We trained using random 32\u00d732 cropped and mirrored images from 4-pixel zero-padded mean-subtracted images, as in [8, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "We trained using random 32\u00d732 cropped and mirrored images from 4-pixel zero-padded mean-subtracted images, as in [8, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 10, "context": "[11] and batch normalization [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[11] and batch normalization [17].", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Residual networks (ResNets) [10] are the state-of-the art network for ILSVRC.", "startOffset": 28, "endOffset": 32}, {"referenceID": 30, "context": "ResNets are more computationally efficient than the VGG architecture [33] they are based on, due to the use of so called \u2018bottleneck\u2019 layers (low-dimensional embeddings [27]), but are also more accurate and quicker to converge, due to the use of identity mappings (shortcuts).", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "ResNet 50 We chose to apply root filter hierarchies within the \u2018ResNet 50\u2019 model [10], the largest residual network model to fit onto 8 GPUs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "[35], at test time the batch normalization layers/parameters may effectively be removed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "For this network we used code implementing full training augmentation to achieve state-of-the-art results, re-implementing more recent models [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "[35], with the exception of not using any training augmentation aside from random crops and mirroring (as supported by Caffe [20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[35], with the exception of not using any training augmentation aside from random crops and mirroring (as supported by Caffe [20]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "Jhurani and Mullowney [19] explore in depth the problem of using GPUs to accelerate the multiplication of very small matrices (smaller than 16\u00d716), and show it is possible to achieve high throughput with large batches, by implementing a more efficient interface than that used in the CuBLAS batched calls.", "startOffset": 22, "endOffset": 26}], "year": 2016, "abstractText": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).", "creator": "TeX"}}}