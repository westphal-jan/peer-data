{"id": "1605.06560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Functional Hashing for Compressing Neural Networks", "abstract": "As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.", "histories": [["v1", "Fri, 20 May 2016 23:44:19 GMT  (1995kb,D)", "http://arxiv.org/abs/1605.06560v1", "submitted to NIPS 2016"]], "COMMENTS": "submitted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["lei shi", "shikun feng", "zhifanzhu"], "accepted": false, "id": "1605.06560"}, "pdf": {"name": "1605.06560.pdf", "metadata": {"source": "CRF", "title": "Functional Hashing for Compressing Neural Networks", "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "emails": ["zhuzhifan}@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2 Background", "text": "In this paper, we express scalars at regular intervals (A or b), vectors in bold (x) and matrices in bold (X). In addition, we use xi to represent the i-th dimension of the vector x, and use Xij to represent the (i, j) -th input of the matrix X. We define the forward propagation of the \"-th layer asa.\" (1) For each \"-th layer is asa.\" (1) For each \"-th layer is the output dimension, d.\" (1) The output dimension is d. \""}, {"heading": "3 Functional Hashing for Neural Network Compression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Structure Formulation", "text": "The main difference between FunHashNN and HashedNets [7] lies in (i) the use of hash functions and (ii) the mapping from w to V: \u2022 Instead of adopting a pair of hash functions (h, q) into equation, we use a series of several pairs of independent random hash functions. Suppose there are U pairs of mappings {hu} Uu = 1, each hu (i, j) prints an integer within [1, K], and each group (i, j) selects a character factor. \u2022 Equation (3) of HashedNets uses an identity mapping {hu} Uu = 1, with each hu (i, j) selecting an integer within [1, K] and each group selecting a character factor. \u2022 Equation (3) of HashedNets uses an identity mapping between an element in V and a hacked value, i.e. Vij (wi), wh (j)."}, {"heading": "3.2 Training Procedure", "text": "The parameters that need to be updated include w in compression and \u03b1 in g (\u00b7) and the virtual matrix V.Forward Propagation. FunHashNN is synonymous with the formation of a standard neural network, except that we need to propagate values related to w through g (\u00b7) and the virtual matrix V.Forward Propagation forwards / backwards. If we replace equivalent (4) in equivalent. (1), we still leave the superscript \"and Getzi = bi + d; j = 1 ajVij = bi + d; j = 1 aj \u00b7 g ([1 (i, j) wh1 (i, j),..."}, {"heading": "3.3 Property Analysis", "text": "In this part, we will try to present the properties of our FunHashNN from several aspects to help understand them, especially compared to HashedNets [7]. It should be noted that both HashedNets and FunHashNN behavior hashing before training, i.e., in a two-phase analysis, would be insufficient if hashing collisions between important values happen. In the literature, several hash functions are known to perform better than a single function. In intuition, if we have multiple hash functions, the items that collide in one function will be different by other hash functions.Value reconstruction. In both HashedNets and FunHashNN, the HashNN trick can be considered a construction."}, {"heading": "3.4 Extensions", "text": "If we look at a linear model f (x; \u03b8) = \u03b8 > x, we can not only provide analyses like Bayesian or hashing on the input function space of x, but also on the dual space of \u03b8 [3]. We now return to the \"reconstruction network\" g (xij; \u03b1) in Eq. (4), where vector xij is the hashed values of u (i, j) whu (i, j) for u = 1,.., U. What we have done in Eq. (4) is actually hashing (i, j) by w to get the input function of g (\u00b7). By analogy, we can also use hash (i, j) for parameters of g (\u00b7), namely we have a new \"reconstruction network\" in the following form: Vij = g (xij; \u03b1ij), with [xij] u = (i, j] whu (i, j), j), j), and ij \u00b2 (ir)."}, {"heading": "4 Related Work", "text": "Recent studies have confirmed the redundancy in the parameters of deep neural networks. Denil et al. [11] dissected a matrix in fully interconnected layers as the product of two low tricks, so that the number of parameters decreases linearly as latent dimensionality decreases. More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to accelerate matrix multiplications. More recently, Han et al. [15, 16] suggested repeating circumcision retraining during training DNNs, using quantization and fine-tuning as a post-processing step. Huffman coding and hardware implementation were also considered. To maintain most accuracy, the authors suggested several rounds of circumcision retraining. That is, for low accuracy loss of d configuration we must cut slowly enough and therefore suffer from increased training time."}, {"heading": "5 Experiments", "text": "We are conducting extensive experiments to evaluate FunHashNN for DNN compression. Codes for full reproducibility will be open source soon after the necessary fine-tuning."}, {"heading": "5.1 Environment Descriptions", "text": "Datasets. Three benchmark datasets [24] are considered here, including (1) the original handwritten MNIST dataset with digits, (2) the BG-IMG dataset as a variant of MNIST, and (3) the binary image classification dataset CONVEX. For all datasets, we use predefined training and test splits. In particular, the original MNIST dataset has # train = 60,000 and # test = 10,000, while the remaining two datasets have # train = 12,000 and # test = 50,000. In addition, a large-scale dataset with billions of samples is used to learn DNNs for paired semantic ranking. We randomly split 20% samples from the training data to form the validation set. Methods and settings. In [7], the authors compared Nets with multiple DNN compression sets and showed hash compression combinations."}, {"heading": "5.2 Varying Compression Ratio", "text": "To test robustness, we vary the compression ratio with (1) a fixed virtual network size (i.e. the size of V 'in each layer) and then with (2) a fixed memory size (i.e. the size of w' in each layer), examining three-layer (1 hidden layer) and five-layer (3 hidden layers) networks. In experiments, we vary the compression ratio geometrically within {1, 12, 1 4,.. For FunHashNN, this comparison starts at 100 units per layer, using 4 hash functions, 3-layer g (\u00b7) and no double space hash."}, {"heading": "5.3 Varying Configurations of FunHashNN", "text": "For three-layer networks with a compression ratio of 1 / 8, we vary the configuration dimensions of FunHashNN, including the number of hash functions (U), the structure of the layers of the reconstruction network g (\u00b7), and whether dual space hashing is enabled. As it is impossible to enumerate all probable options, U limits itself to varying in {2, 4, 8, 16}. The structure of g (\u00b7) is selected from 2 x 4 layers, U x 1, U x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x, and a suffix -D shows double space hashing. Table 1 shows the performance of FunHashNN with different configurations on MNIST. The observations are summarized below: The order of index (0) to (1.x) fixes a 3-layer \u00b7 function (potentially three functions and three hash functions) and three-x functions."}, {"heading": "5.4 Pairwise Semantic Ranking", "text": "The data is collected from the logs of a commercial search engine, with each query url clicked being a positive sample and each unclicked a negative sample. In total, there are about 45B samples. We use a deep, convolutionally structured semantic model similar to [19, 31], which has a Siamese structure to describe the semantic similarity between a query and an Url title. The network is trained to optimize cross-entropy for each pair of positive and negative samples per query. Performance is evaluated by the ratio of correct and false pairing on the test set. In Figure 4, we present the performance of a base network as a training progress compared to FunHashNN and HashNet, both with 1 / 4 compression ratios. With U = 4 hash functions, FunHashNN performs better than HashedNet throughout the training, compared to FunHashN and HashNet, both of which are high hash words."}, {"heading": "6 Conclusion and Future Work", "text": "This paper introduces a novel approach to neural network compression, FunHashNN. In short, after adopting several low-cost hash functions to retrieve values in the compression space, FunHashNN uses a small reconstruction network to restore each entry in a matrix of the original network, plugging the reconstruction network into the entire network and learning together, and the recently proposed HashedNets [7] is presented as a degenerated special case of FunHashNN. Extensions to FunHashNN with dual space hashing and multihops are also discussed, and several datasets show promising high compression rates with low loss of predictive accuracy. In future work, we plan to systematically further analyze the properties and limitations of FunHashNN and its extensions, and other industrial applications are also expected, especially on mobile devices."}], "references": [{"title": "Balanced allocations", "author": ["Y. Azar", "A. Broder", "A. Karlin", "E. Upfal"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Oxford University Press, Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Multilevel adaptive hashing", "author": ["A. Broder", "A. Karlin"], "venue": "SODA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Using multiple hash functions to improve IP lookups", "author": ["A. Broder", "M. Mitzenmacher"], "venue": "INFOCOM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Compressing convolutional neural networks", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "ICML,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved data stream summary: The Count-Min sketch and its application", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "J. Algorithms, 55:29\u201338,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "8-bit approximations for parallelism in deep learning", "author": ["T. Dettmers"], "venue": "ICLR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain adaptation for large scale sentiment classification: a deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Sketch algorithms for estimating point queries in NLP", "author": ["A. Goyal", "H.I. Daume", "G. Cormode"], "venue": "EMNLP/CoNLL,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "ICLR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "CIKM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning to hash with multiple representations", "author": ["Y. Kang", "S. Kim", "S. Choi"], "venue": "IEEE ICDM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "ICLR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical evaluations of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q.V. Le", "T. Sarlos", "A.J. Smola"], "venue": "ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv:1312.4400,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Diversity networks", "author": ["Z. Mariet", "S. Sra"], "venue": "ICLR,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y.-L. Boureau", "Y. LeCun"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "CIKM,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "JMLR, 10:2615\u20132637,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to hash for indexing big data \u2013 a survey", "author": ["J. Wang", "W. Liu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of IEEE, 104(1):34\u201357,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "ICML,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "In ICCV,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss.", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 180, "endOffset": 183}, {"referenceID": 12, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 11, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 26, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 20, "context": "On the other hand the memory and energy consumption is usually, if not always, constrained in industrial applications [21, 35].", "startOffset": 118, "endOffset": 126}, {"referenceID": 34, "context": "On the other hand the memory and energy consumption is usually, if not always, constrained in industrial applications [21, 35].", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "Compressing the model size becomes more important for applications on mobile and embedded devices [15, 21].", "startOffset": 98, "endOffset": 106}, {"referenceID": 20, "context": "Compressing the model size becomes more important for applications on mobile and embedded devices [15, 21].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "[15, 16] for details).", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] for details).", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 20, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 34, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 17, "context": "[18] suggested to train a large network ahead, and distill a much smaller model on a combination of the original labels and the soft-output by the large net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "in network weights [7, 11], and exploits techniques to constrain or reduce the number of free-parameters in DNNs during learning.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "in network weights [7, 11], and exploits techniques to constrain or reduce the number of free-parameters in DNNs during learning.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 24, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 34, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 14, "context": "Moreover [15, 16] proposed a simple-yet-effective pruning-retraining iteration during training, followed by quantization and fine-tuning.", "startOffset": 9, "endOffset": 17}, {"referenceID": 15, "context": "Moreover [15, 16] proposed a simple-yet-effective pruning-retraining iteration during training, followed by quantization and fine-tuning.", "startOffset": 9, "endOffset": 17}, {"referenceID": 6, "context": "[7] proposed HashedNets to efficiently implement parameter sharing prior to learning, and showed notable compression with much less loss of accuracy than low-rank decomposition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "HashedNets was further deliberated in frequency domain for compressing convolutional neural networks in [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "Second, one single hash function is used to fetch a single value in the compression space, whose collision risk is larger than multiple hashes [4].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "Specifically, we use multiple hash functions [4] to map per virtual entry into multiple values in compression space.", "startOffset": 45, "endOffset": 48}, {"referenceID": 28, "context": "Typical choices of f(\u00b7) include rectified linear unit (ReLU) [29], sigmoid and tanh [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "Typical choices of f(\u00b7) include rectified linear unit (ReLU) [29], sigmoid and tanh [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 31, "context": "Feature Hashing has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection [32, 34].", "startOffset": 161, "endOffset": 169}, {"referenceID": 33, "context": "Feature Hashing has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection [32, 34].", "startOffset": 161, "endOffset": 169}, {"referenceID": 33, "context": "Following the definition in [34], the mapping \u03c6 is a composite of two approximate uniform hash functions h : N \u2192 {1, .", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "As shown in [34], a key property is its inner product preservation, which we quote and restate below.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "HashedNets in [7].", "startOffset": 14, "endOffset": 17}, {"referenceID": 33, "context": "Another independent hash function \u03be(i, j) : (d \u00d7 d)\u2192 \u00b11 outputs a sign factor, aiming to reduce the bias due to hash collisions [34].", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "(1) was kept as ReLU in [7] to further relieve the hash collision effect through a sparse feature space.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "In both [7] and this paper, the open source xxHash1 is adopted as an approximately uniform hash implementation with low cost.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "(a) HashedNets [7].", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The key difference between FunHashNN and HashedNets [7] lies in (i) how to employ hash functions, and (ii) how to map from w to V:", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "In this part, we try to depict the properties of our FunHashNN from several aspects to help understanding it, especially in comparison with HashedNets [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 3, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 31, "context": "\u2022 The maximum number of possible distinct values output by hashing intuitively explains the modelling capability [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "In line with previous work [32, 34], we compare HashedNets and FunHashNN in terms of feature hashing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 33, "context": "In line with previous work [32, 34], we compare HashedNets and FunHashNN in terms of feature hashing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 6, "context": "3 in [7]) zi = w >\u03c6 (1) i (a).", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "If considering a linear model f(x;\u03b8) = \u03b8>x, one can not only deliver analysis like Bayesian or hashing on input feature space of x, but also do similarly on the dual space of \u03b8 [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "[11] decomposed a matrix in a fully-connected layers as the product of two low-rank matrices, so that the number of parameters decreases linearly as the latent dimensionality decreases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to speed up matrix multiplications.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to speed up matrix multiplications.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "[15, 16] proposed to iterate pruning-retraining during training DNNs, and used quantization and fine-tuning as a post-processing step.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] proposed to iterate pruning-retraining during training DNNs, and used quantization and fine-tuning as a post-processing step.", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "Again, the most related work to ours is HashedNets [7], which was then extended in [6] to random hashing in frequency domain for compressing convolutional neural networks.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Again, the most related work to ours is HashedNets [7], which was then extended in [6] to random hashing in frequency domain for compressing convolutional neural networks.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "Extensive studies have been made on constructing and analyzing multiple hash functions, which have shown better performances over one single hash function [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "One multi-hashing algorithm, d-random scheme [1], uses only one hash table but d hash functions, pretty similar to our settings.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "One choice alternative to d-random is the d-left algorithm proposed in [5], used for improving IP lookups.", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "Hashing algorithms for natural language processing are also studied in [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Papers [32, 34] investigated feature hashing (a.", "startOffset": 7, "endOffset": 15}, {"referenceID": 33, "context": "Papers [32, 34] investigated feature hashing (a.", "startOffset": 7, "endOffset": 15}, {"referenceID": 23, "context": "Three benchmark datasets [24] are considered here, including (1) the original MNIST hand-written digit dataset, (2) dataset BG-IMG as a variant to MNIST, and (3) binary image classification dataset CONVEX.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "In [7], the authors compared HashedNets against several DNN compression approaches, and showed HashedNets performs consistently the best, including the low-rank decomposition [11].", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [7], the authors compared HashedNets against several DNN compression approaches, and showed HashedNets performs consistently the best, including the low-rank decomposition [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "We adopt a deep convolutional structured semantic model similar to [19, 31], which is of a siamese structure to describe the semantic similarity between a query and a url title.", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "We adopt a deep convolutional structured semantic model similar to [19, 31], which is of a siamese structure to describe the semantic similarity between a query and a url title.", "startOffset": 67, "endOffset": 75}, {"referenceID": 6, "context": "The recently proposed HashedNets [7] is shown as a degenerated special case of FunHashNN.", "startOffset": 33, "endOffset": 36}], "year": 2016, "abstractText": "As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.", "creator": "LaTeX with hyperref package"}}}