{"id": "1605.06651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Gambler's Ruin Bandit Problem", "abstract": "In this paper, we propose a new sequential decision making problem called {\\em gambler's ruin bandit problem} (GRBP). In each round of the GRBP the learner faces a gambler's ruin problem with two possible actions: a {\\em continuation action} that moves the learner randomly over the state space around the current state; and a {\\em terminal action} that moves the learner directly into one of the two terminal states (goal and dead-end state). The current round ends when a terminal state is reached. We first formulate GRBP as an optimization problem, and prove that the optimal policy is characterized by a simple threshold rule. The problem is solved for infinite time budget. Then, we consider the case when the state transition probabilities are unknown and provide logarithmic problem specific regret bounds. We also identify a condition under which the learner only incurs finite regret. Numerous applications including optimal medical treatment assignment can be formulated as a GRBP, in which the continuation action corresponds to the conservative treatment and the terminal action corresponds to the surgery.", "histories": [["v1", "Sat, 21 May 2016 14:43:44 GMT  (273kb,D)", "https://arxiv.org/abs/1605.06651v1", null], ["v2", "Mon, 11 Jul 2016 13:54:20 GMT  (416kb,D)", "http://arxiv.org/abs/1605.06651v2", null], ["v3", "Thu, 29 Sep 2016 03:51:57 GMT  (504kb,D)", "http://arxiv.org/abs/1605.06651v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nima akbarzadeh", "cem tekin"], "accepted": false, "id": "1605.06651"}, "pdf": {"name": "1605.06651.pdf", "metadata": {"source": "CRF", "title": "Gambler\u2019s Ruin Bandit Problem", "authors": ["Nima Akbarzadeh", "Cem Tekin"], "emails": [], "sections": [{"heading": null, "text": "The goal of the learner is to maximize his long-term abilities by selecting actions that yield high rewards. This is a non-trivial task, as the reward distributions are not previously known, and will receive a random reward. Countless order-optimal learning rules have been developed for the conventional learning rules. [4] These rules act myopically by selecting the action with the maximum index in each round. Situations that require multiple actions to be taken in each round cannot be done using conventional MABs. As an example, consider medical treatment management."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Gambler\u2019s Ruin Problem", "text": "If action F is removed from the GRBP, it becomes the player's ruin problem. In Hunter et al. [10] model of the player's ruin problem, in addition to the default result of moving a state to the left or right, two additional outcomes are also taken into account: one result immediately changes the state to G, while the other result immediately changes the state to D. These outcomes are called windfall or disaster outcomes, and the ruin and win probabilities and the duration of the game are calculated based on these additional outcomes. In another model [11], modifications such as the chance of being accepted in states other than G and D and staying in the same state are taken into account. Ruin and win probabilities are calculated according to the proposed transition model. Unlike GRBP, which is an MDP, the player's ruin problem is a Markov chain. Furthermore, the ruin and win probabilities in the above models can be precisely calculated, as the transition probabilities are assumed to be known."}, {"heading": "B. MDPs", "text": "GRBP is closely related to goal-oriented MDPs and stochastic problems with the shortest route [12]. For these problems, a measure must be taken in each state (or in each era) to reach the target state (G) at minimum cost. To this end, the optimal policy must be determined in advance using the known transition probabilities. Recently, progress has been made in achieving solutions for MDPs that have dead ends in addition to the target states (G) [7], [13]. These solutions require a value teration and heuristic search methods that must be carried out with knowledge of transition probabilities. To our knowledge, a reinforcement learning algorithm that works a priori without knowledge of transition probabilities and reaches logarithmic limits of regret has not yet been developed for these problems."}, {"heading": "C. Multi-armed Bandits", "text": "Over the past decade, many variations of the MAB problem have been studied and many different learning algorithms have been proposed, including Gittins index [16], confidence-based strategies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4] - [6], greedy strategies (greedy algorithm) [4], and Thompson samples [17] (see [8] for a comprehensive analysis of the MAB problem).The performance of a learning algorithm for a MAB problem is calculated using the term regret. In the stochastic MAB problem [3], regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle optimally based on complete knowledge of the problem parameters. It turns out that regret grows logarithmically in the number of rounds for this problem."}, {"heading": "III. PROBLEM FORMULATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Definition of the GRBP", "text": "In the GRBP, the system consists of a finite series of states S: = {D, 1,., G), where integer D = 0 denotes the dead end state and G denotes the target state. The initial state of each round results from a probability distribution q (s), s), s (S), S (S), so that 1 \u2212 q (1) > 0. The current round ends and the next round begins when the learner states D or G. Due to this fact, D and G are denoted as end states. Each round is divided into multiple time slots in which the learner takes action in each time span."}, {"heading": "C. Online Learning in the GRBP", "text": "As we have described in the previous subsection, if the state probabilities of transition are known, the optimal solution and its probability to reach the goal can be found by solving the Bellman equations of optimality. If the learner does not know pC and pF, the optimal policy cannot be calculated from the outset and must therefore be learned. We define the regret of the learner, who is a priori unaware of the optimal policy, in relation to an oracle that knows the optimal policy from the outset, as the regret given by reg (T): = TV-T-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "IV. FORM OF THE OPTIMAL POLICY", "text": "In this section we will demonstrate that the optimal policy for GRBP has a threshold form. The value of the threshold depends only on the transition probabilities and the number of states. Firstly, we give the definition of a stationary threshold policy. \u2212 \u2212 Definition 1. \u2212 Definition is a stationary threshold policy if there is a stationary threshold policy with a threshold value. \u2212 Definition 1. Definition 2. Definition 3. Definition 4. Definition 4. Definition 4. Definition 4. Definition 4. Definition 4. Definition 5. Definition 5. Definition 5. Definition 5. Definition 5. 5. Definition 5. 5. Definition 5. Definition 5. Definition 5. Definition 5. Definition 5. Definition 5. Definition 5. Definition 9."}, {"heading": "V. AN ONLINE LEARNING ALGORITHM AND ITS REGRET ANALYSIS", "text": "In this section, we propose a learning algorithm that minimizes regret when the likelihood of a state transition is unknown; the proposed algorithm forms estimates of the likelihood of a state transition based on the history of the state transition and then uses these estimates together with the form of the optimal policy obtained in Section IV to calculate an estimated optimal policy in each round."}, {"heading": "A. Greedy Exploitation with Threshold Based Exploration", "text": "The learning algorithm for the GRBP is called Greedy Exploitation with Threshold Based Exploration (GETBE) (GETBE) and its pseudo-code is specified in Algorithm 1. Unlike traditional MAB algorithms [3], [4], [6] which require all arms to be evaluated at least logarithmically, GETBE does not need to logarithmically select all strategies (arms) to find the optimal policy with a sufficiently high probability. GETBE achieves this by using the form of optimal policy derived from the previous section. Although GETBE does not require all strategies to be explored, it does require the exploration of measures F if the estimated optimal policy never selects action F. This forced exploration is done to ensure that GETBE does not get stuck in the suboptimal policy.GETBE holds opponents NGF (\u03c1), F (RT), C (GP) and NGP (N)."}, {"heading": "B. Regret Analysis", "text": "In this section, we have the (expected) regret of GETBE (the closed regret of GETBE). We show that GETBE achieves limited regret when the unknown transition probabilities are not in any exploration region and regrets logarithmically (in the number of rounds) when the unknown transition probabilities are in the exploration region. Based on Theorem 2, GETBE only needs to learn the optimal policy from the set of strategies. (Taking into account this fact and taking into account the expectation of (2), the expected regret of GETBE can be written. (Reg (T)] = 4, GETBE only needs to learn the optimal policy from the set of strategies. (13) Let it happen. (s): = V-tr1 (s) \u2212 V-tr1 (s) \u2212 V-tr0 (s), s-tr0 (s), s-S) |, s-S-S gaps when the suboptimal situation is different (if)."}, {"heading": "VI. NUMERICAL RESULTS", "text": "We are creating a synthetic medical treatment problem based on [21]. Each state is assumed to have one stage of gastric cancer (G = 4, D = 0). The target state is defined as at least three years of survival. Action C is accepted as chemotherapy and Action F is accepted as surgery. For Action C, pC is determined by using the average survival rates for young and old groups at different stages of cancer. ETC is used for each stage to achieve the survival rate at three years of age by taking continuous action. With this information, we set pC = 0.45. Also, the five-year survival rate of the surgery given in [22%] is used to pF = 0.3.The regrets shown in Figures 3 and 4 correspond to different variants of the GETBE, which are referred to as GETBE-SM, GETBE-PS and GETBE-UCB. Each variant updates the state transitions in different ways."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we presented the Gambler's Ruin Bandit Problem. We characterized the shape of the optimal policy for this problem, and then developed a learning algorithm called GETBE, which works on the GRBP to learn the optimal policy when the transition probabilities are unknown. We proved that the regret of this algorithm is either limited (finite) or logarithmic in the number of rounds based on the region where the true transition probabilities lie. In addition to the regret limits, we illustrated the performance of our algorithm through numerical experiments."}], "references": [{"title": "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges", "author": ["S.S. .Villar", "J. Bowden", "J. Wason"], "venue": "Statistical Science, vol. 30, no. 2, pp. 199\u2013215, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "RELEAF: An algorithm for learning and exploiting relevance", "author": ["C. Tekin", "M. van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 9, no. 4, pp. 716\u2013727, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics, vol. 6, no. 1, pp. 4\u201322, 1985.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "Cesa-bianchi", "N. o", "P. Fischer"], "venue": "Machine Learning, vol. 47, pp. 235\u2013256, 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Garivier", "Aurelien", "Cappe", "Olivier"], "venue": "COLT, 2011, pp. 359\u2013376.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica, vol. 61, no. 1-2, pp. 55\u201365, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory of goal-oriented mdps with dead ends", "author": ["A. Kolobov", "Mausam", "D. Weld"], "venue": "UAI, 2012, pp. 438\u2013447.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret analysis of stochastic and non-stochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1\u2013122, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "On the classical ruin problems", "author": ["L. Takacs"], "venue": "J. Amer. Statisistical Association, vol. 64, pp. 889\u2013906, 1969.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1969}, {"title": "Gambler\u2019s ruin with catastrophe and windfalls", "author": ["B. Hunter", "A.C. Krinik", "C. Nguyen", "J.M. Switkes", "H.F. von Bremen"], "venue": "Statistical Theory and Practice, vol. 2, no. 2, pp. 199\u2013219, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Maximum and minimum of modified gamblers ruin problem, arxiv:1301.2702", "author": ["T. van Uem"], "venue": "2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic programming and optimal control", "author": ["D. Bertsekas"], "venue": "Athena Scientific.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 0}, {"title": "Stochastic safest and shortest path problems", "author": ["F. Teichteil-Konigsbuch"], "venue": "AAAI, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["A. Tewari", "P. Bartlett"], "venue": "Advances in Neural Information Processing Systems, vol. 20, pp. 1505\u20131512, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["P. Auer", "T. Jaksch", "R. Ortner"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 89\u201396.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "Jones", "D.M."], "venue": "Progress in Statistics Gani, J. (ed.), pp. 241\u2013266, 1974.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1974}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "The Journal of Machine Learning Research, vol. 23, no. 39, pp. 285\u2013294, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences, vol. 78, no. 5, pp. 1404\u20131422, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic programming and modern control theory", "author": ["R. Bellman", "R.E. Kalaba"], "venue": "Citeseer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1965}, {"title": "On the gambler\u2019s ruin problem for a finite markov chain", "author": ["M.A. El-Shehawey"], "venue": "Statistics and Probability Letters, vol. 79, pp. 1590\u2013 1595, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Characteristics and prognosis of gastric cancer in young patients", "author": ["T. Isobe", "K. Hashimoto", "J. Kizaki", "M. Miyagi", "K. Aoyagi", "K. Koufuji", "K. Shirouzu"], "venue": "International Journal of Oncology, vol. 30, no. 1, pp. 43\u201349, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In the literature, this kind of feedback information is called bandit feedback [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP).", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP).", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "\u2022 We show that using conventional MAB algorithms such as UCB1 [4] in GRBP by enumerating all deterministic Markov policies is very inefficient and results in high regret.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "Unlike conventional MAB where the regret growth is at least logarithmic in the number of rounds [3], in GRBP regret can be either logarithmic or bounded, based on the values of the state transition probabilities.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "[10] of the Gambler\u2019s Ruin Problem, in addition to the standard outcome of moving one state to the left or right, two extra outcomes are also considered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In another model [11], modifications such as the chance of absorption in states other than G and D and staying in the same state are considered.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "MDPs GRBP is closely related to goal oriented MDPs and stochastic shortest path problems [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].", "startOffset": 126, "endOffset": 129}, {"referenceID": 12, "context": "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 164, "endOffset": 168}, {"referenceID": 3, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 241, "endOffset": 244}, {"referenceID": 5, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 246, "endOffset": 249}, {"referenceID": 3, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 288, "endOffset": 291}, {"referenceID": 16, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 314, "endOffset": 318}, {"referenceID": 7, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 324, "endOffset": 327}, {"referenceID": 2, "context": "For the stochastic MAB problem [3], the regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle which acts optimally based on complete knowledge of the problem parameters.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "In addition, GRBP model does not fit into the combinatorial models proposed in prior works [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 3, "context": "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "When UCB1 in [4] is used to select the policy to follow at the beginning of each round (with set of arms \u03a0), we have", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "Proof: See [4].", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Thus, the probability of hitting G from state s is (1\u2212 r)/(1\u2212 r) (10) for r 6= 1 and s/G for r = 1 [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "NUMERICAL RESULTS We create a synthetic medical treatment selection problem based on [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "For action C, p is determined by using the average survival rates for young and old groups at different stages of cancer given in [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "GETBEPS uses posterior sampling from the Beta distribution [17] to sample and update p and p .", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) \u223c Unif[0, 1], NC(1) = 1, N C(1) \u223c Unif[0, 1].", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) \u223c Unif[0, 1], NC(1) = 1, N C(1) \u223c Unif[0, 1].", "startOffset": 130, "endOffset": 136}], "year": 2016, "abstractText": "In this paper, we propose a new multi-armed bandit problem called the Gambler\u2019s Ruin Bandit Problem (GRBP). In the GRBP, the learner proceeds in a sequence of rounds, where each round is a Markov Decision Process (MDP) with two actions (arms): a continuation action that moves the learner randomly over the state space around the current state; and a terminal action that moves the learner directly into one of the two terminal states (goal and dead-end state). The current round ends when a terminal state is reached, and the learner incurs a positive reward only when the goal state is reached. The objective of the learner is to maximize its long-term reward (expected number of times the goal state is reached), without having any prior knowledge on the state transition probabilities. We first prove a result on the form of the optimal policy for the GRBP. Then, we define the regret of the learner with respect to an omnipotent oracle, which acts optimally in each round, and prove that it increases logarithmically over rounds. We also identify a condition under which the learner\u2019s regret is bounded. A potential application of the GRBP is optimal medical treatment assignment, in which the continuation action corresponds to a conservative treatment and the terminal action corresponds to a risky treatment such as surgery. I. INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2]. In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward. The goal of the learner is to maximize its long-term expected reward by choosing actions that yield high rewards. This is a non-trivial task, since the reward distributions are not known beforehand. Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6]. These rules act myopically by choosing the action with the maximum index in each round. Situations that require multiple actions to be taken in each round cannot be modeled using conventional MAB. As an example, consider medical treatment administration. At the beginning of each round a patient arrives to the intensive care unit (ICU) with a random initial health state. The goal state is defined as discharge and dead-end state is defined as death. Actions correspond to treatment options that move the patient randomly over the state space. The objective is to maximize the expected number of patients that are discharged by learning the optimal treatment policy using the observations gathered from the previous patients. In the example given above, each round corresponds to a goaloriented Markov Decision Process (MDP) with dead-ends Cem Tekin is supported by TUBITAK 2232 Fellowship (116C043). [7]. The learner knows the state space, goal and dead-end states, but does not know the state transition probabilities a priori. At each round, the learner chooses a sequence of actions and only observes the state transitions that result from the chosen actions. In the literature, this kind of feedback information is called bandit feedback [8]. Motivated by the application described above, we propose a new MAB problem in which multiple arms are selected in each round until a terminal state is reached. Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP). In GRBP, the system proceeds in a sequence of rounds \u03c1 \u2208 {1, 2, . . .}. Each round is modeled as an MDP (as in Fig. 1 ) with unknown state transition probabilities and terminal (absorbing) states. The set of terminal states includes a goal state G and a dead-end state D, and the non-terminal states are ordered between the goal and dead-end states. In each non-terminal state, there are two possible actions: a continuation action (action C) that moves the learner randomly over the state space around the current state; and a terminal action (action F ) that moves the learner directly into a terminal state. Starting from a random, non-terminal initial state, the learner chooses a sequence of actions and observes the resulting state transitions until a terminal state is reached. The learner incurs a unit reward if the goal state is reached. Otherwise, it incurs no reward. The goal of the learner is to maximize its cumulative expected reward over the rounds. If the state transition probabilities were known beforehand, an omnipotent oracle with unlimited computational power could calculate the optimal policy that maximizes the probability of hitting the goal state from any initial state, and then select its actions according to the optimal policy. We define the regret of the learner by round \u03c1 as the difference in the expected number of times the goal state is reached by the omnipotent oracle and the learner by round \u03c1. First, we show that the optimal policy for GRBP can be computed in a straightforward manner: there exists a threshold state above which it is always optimal to take action C and on or below which it is always optimal to take action F . Then, we propose an online learning algorithm for the learner, and bound its regret for two different regions that the actual state transition probabilities can lie in. The regret is bounded (finite) in one region, while it is logarithmic in the number of rounds in the other region. These bounds are problem-specific, in the sense that they are functions of the state transition probabilities. Finally, we illustrate the ar X iv :1 60 5. 06 65 1v 3 [ cs .L G ] 2 9 Se p 20 16 D\t\r   D+1\t\r   s-\u00ad\u20101\t\r   s G-\u00ad\u20101\t\r   G ... ... s+1", "creator": "LaTeX with hyperref package"}}}