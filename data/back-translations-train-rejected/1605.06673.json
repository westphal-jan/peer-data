{"id": "1605.06673", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces", "abstract": "Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target do- main. The problem is to learn a effective classifier for the target domain. In this paper, we propose a novel transfer learning method for this problem by learning a partially shared classifier for the target domain, and weighting the source domain data points. We learn some shared subspaces for both the data points of the two domains, and a shared classifier in the shared subspaces. We hope that in the shared subspaces, the distributions of two domain can match each other well, and to match the distributions, we weight the source domain data points with different weighting factors. Moreover, we adapt the shared classifier to each domain by learning different adaptation functions. To learn the subspace transformation matrices, the classifier parameters, and the adaptation parameters, we build a objective function with weighted clas- sification errors, parameter regularization, local reconstruction regularization, and distribution matching. This objective function is minimized by an itera- tive algorithm. Experiments show its effectiveness over benchmark data sets, including travel destination review data set, face expression data set, spam email data set, etc.", "histories": [["v1", "Sat, 21 May 2016 16:57:37 GMT  (35kb)", "http://arxiv.org/abs/1605.06673v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hongqi wang", "anfeng xu", "shanshan wang", "sunny chughtai"], "accepted": false, "id": "1605.06673"}, "pdf": {"name": "1605.06673.pdf", "metadata": {"source": "CRF", "title": "Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces", "authors": ["Hongqi Wang", "Sunny Chughtai", "Anfeng Xu", "Shanshan Wang"], "emails": ["wanghongqi@yahoo.com,", "anfengxu150@yahoo.com", "sunnychughtai313@yahoo.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.06 673v 1 [cs.L G] May 21 2Hongqi Wang, Anfeng Xu, and Shanshan Wang School of Management, Harbin University of Science and Technology, Harbin 150000, China Email: wanghongqi @ yahoo.com, anfengxu150 @ yahoo.com Anfeng Xu is the corresponding author. Sunny Chughtai Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan Email: sunnychughtai313 @ yahoo.com Keywords Transfer Learning \u00b7 Subspace Learning \u00b7 Distribution Matching \u00b7 Weighted Mean \u00b7 Travel Destination Review"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Transfer Representation and Classification Model", "text": "2.1 Shared Subspace RepresentationSuppose we have a source domain and a target domain, and each domain has a training set. The source domain training set is given as a set of n1 labeled data points, S = {(xs1, y s 1), \u00b7 \u00b7 \u00b7, (x s n1, ysn1)}, where x s i, ysn1) is the attribute vector of m dimensions of the i-th data point, and ysi, (x t n3, \u2212 1} is the binary name of the i-th data point. The target domain training set is given as a set of n2 partially labeled data points, T = {(xt1, y t 1), \u00b7 \u00b7 (x t n3, ytn3), x n3 + 1, \u00b7 \u00b7 \u00b7 \u00b7, xtn2}, where xtj, Rm is the attribute vector of the j data point, and tj-th data point."}, {"heading": "2.2.1 Partially Shared Classifiers", "text": "The first solution is to design the source and target domain classifiers by adapting the split classifier to the two domains, adding a linear function over the original feature vectors. For a source domain data point, xs, the classifier is specified as follows: f (xs) = g (xs) + x s (x s) = w xs, (3) where \"s (x s) = u xs\" is the source domain fit function and \"u Rm\" is the parameter vector of the fit function. For a target domain data point, xt, the classifier is obtained as a combination of g and a target domain function, h (xt) = g (x t) = w xt + v xt, (4) where \"t\" (x t) = v xt \"is the target domain, and\" v Rm is its parameter vector."}, {"heading": "2.2.2 Distribution Matching in the Subspaces by Weighting the Source Domain Data Points", "text": "The second solution to fill the gap is to match the distributions of the two domains in the subdomains. \u00b7 Since the subdomains are divided by both domains, we hope that in these subdomains the gap between the distributions of the source and target domains can be filled. \u2212 To this end, we have proposed to represent the distributions of the subdomains of the source and target domains as the mean of the vectors of their data points in the subdomains. \u2212 \u00b5s = 1n1n1 \u2211 xsi, and\u00b5t = 1n2n2 \u2211 j = 1\u0441xtj (5), where \u00b5s and \u00b5t are the mean vectors of the source and target domains. \u2212 Furthermore, we argue that actually different source domain data points play different roles in the allocation of two domains. It is not appropriate to treat all source domain data equally when they are aligned with the target domain."}, {"heading": "3 Parameter Estimation Method", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "3.2.1 Updating \u0398 and w", "text": "As we update the other parameters, we solve the following Minimum (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((Minimum) () (Minimum) (Minimum) (() (Minimum) () (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (() (Minimum) () (Minimum) (() (Minimum) (() (Minimum) (() (Minimum) (() (Minimum) ((() (Minimum) ((() (Minimum) ((() (Minimum) (() (Minimum) (() (Minimum) (() (Minimum) (() (Minimum) (Minimum) (Minimum) () (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((() (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((Minimum) (Minimum) (Minimum) (Minimum) (((Minimum) (Minimum) (Minimum) (((Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (((Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((Minimum) (Minimum) ((Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) (Minimum) ((Minimum) (Minimum) ((Minimum)"}, {"heading": "3.2.2 Updating \u03c6 and \u03d5", "text": "In this step we fix the other parameters and look only at F and F. If other parameters are fixed and the Terme, which do not contain the same, are removed, the minimization problem in (13) is reduced to the following problem, min.???, Q (?) = n1 \u00b2 i = 1L (???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2.3 Updating \u03c0", "text": "In this step we update and fix other parameters. For this purpose we have the following minimization problem (min \u03c0n1 \u2211 i = vid (ysi, \u03c6 xs) \u03c0i + C2n1 \u2211 i = 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4 Experimental Results", "text": "In fact, most of them are able to survive on their own."}, {"heading": "4.4.1 Classification Accuracy", "text": "The classification accuracy of the comparison methods across five benchmark datasets is shown in Table 1. The proposed method outperforms all comparison methods across four benchmark datasets. The only exception is the problem of classifying facial expressions compared to the GEMEP dataset, where the method used by Chu et al. [3] performs slightly better than the proposed method. However, even in the experiments with the GEMEP, the proposed method still performs second best. In the experiments with both the Destination Review dataset and the 20 newsgroup dataset, the proposed method significantly outperforms the other methods."}, {"heading": "4.4.2 Running Time", "text": "The runtime of the training process of the compared algorithms over different benchmark data sets is shown in Table 2. Reported results show that the proposed method has the lowest runtime. Furthermore, we can also see that the runtime is also relevant for the size of the data set. Thus, for example, in the two smallest data sets, the travel and GEMEP data sets, the runtime is also shorter than the runtime over other data sets."}, {"heading": "5 Conclusions", "text": "The features of this paper are as follows: - Instead of learning a common representation and classification directly for source and target domains, we proposed to learn common subspaces and classifiers and then adapt them to match source and target domains. - Instead of using the source domain data equally to estimate the distribution of source domain data, we proposed to weight the source domain data in the subdomains so that they match the distributions of the two domains. - We also proposed to regulate the weighting factors of the source domain data points and the classification responses of the target domain data points by local reconstruction coefficients. The minimization problem of our method is based on these characteristics and we solved it by an iterative algorithm. Experiments show their advantages over some other methods."}, {"heading": "6 Future Works", "text": "The first strategy is parallelization of the algorithm; the big data set can be divided into many small subsets and the algorithm can be parallelized to process these subsets at the same time; the second strategy is to apply the stochastic optimization method by using the data points individually and not using them all carefully; and we will extend the proposed algorithm to various applications such as computer mechanics [24,29,15,17,26,30], multimedia [21,9,14,22,23], nanotechnology [10,11,13,12,1], etc. We will also consider some other models to represent and construct the classifier, such as the Bayesian network [6,4,5]."}, {"heading": "Acknowledgements", "text": "This research was supported by the National Natural Science Foundation (71173062, 71203047), the Key Scientific and Technological Research Program of Heilongjiang Province (GB14D201), and the Construction Plan of the University Philosophy and Social Sciences Innovation Team of Heilongjiang Province (TD201203)."}], "references": [{"title": "Dual-color fluorescence cross-correlation spectroscopy on a planar optofluidic chip", "author": ["A. Chen", "M. Eberle", "E. Lunt", "S. Liu", "K. Leake", "M. Rudenko", "A. Hawkins", "H. Schmidt"], "venue": "Lab on a Chip 11(8), 1502\u20131506", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering low-rank shared concept space for adapting text mining models", "author": ["B. Chen", "W. Lam", "I. Tsang", "T.L. Wong"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(6), 1284\u20131297", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Selective transfer machine for personalized facial action unit detection", "author": ["W.S. Chu", "F.D.L. Torre", "J.F. Cohn"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 3515\u20133522", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding optimal bayesian network structures with constraints learned from data", "author": ["X. Fan", "B. Malone", "C. Yuan"], "venue": "Proceed. of the 30th Conf. on Uncertainty in Artificial Intelligence (UAI-2014)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An improved lower bound for bayesian network structure learning", "author": ["X. Fan", "C. Yuan"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI-2015)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tightening bounds for bayesian network structure learning", "author": ["X. Fan", "C. Yuan", "B. Malone"], "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-2014)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Transfer learning with reasonable boosting strategy", "author": ["L. La", "Q. Guo", "Q. Cao", "Y. Wang"], "venue": "Neural Computing and Applications 24(3-4), 807\u2013816", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation", "author": ["W. Li", "L. Duan", "D. Xu", "I. Tsang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 36(6), 1134\u20131148", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-kernel learning for multivariate performance measures optimization", "author": ["F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald"], "venue": "Neural Computing and Applications pp. 1\u201313", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Optofluidic devices with integrated solid-state nanopores", "author": ["S. Liu", "A.R. Hawkins", "H. Schmidt"], "venue": "Microchimica Acta 183(4), 1275\u20131287", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Electro-optical detection of single \u03bb-dna", "author": ["S. Liu", "T.A. Wall", "D. Ozcelik", "J.W. Parks", "A.R. Hawkins", "H. Schmidt"], "venue": "Chemical Communications 51(11), 2084\u20132087", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Effect of fabrication-dependent shape and composition of solid-state nanopores on single nanoparticle detection", "author": ["S. Liu", "T.D. Yuzvinsky", "H. Schmidt"], "venue": "ACS nano 7(6), 5621\u20135627", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlated electrical and optical analysis of single nanoparticles and biomolecules on a nanoporegated optofluidic chip", "author": ["S. Liu", "Y. Zhao", "J.W. Parks", "D.W. Deamer", "A.R. Hawkins", "H. Schmidt"], "venue": "Nano letters 14(8), 4816\u20134820", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications pp. 1\u20139", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale simulations and mechanics of biological materials pp. 301\u2013317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge adaptation with partiallyshared features for event detectionusing few exemplars", "author": ["Z. Ma", "Y. Yang", "N. Sebe", "A.G. Hauptmann"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 36(9), 1789\u20131802", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics", "author": ["B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu"], "venue": "Nanoscale research letters 10(1), 1\u20139", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning using the online fuzzy min-max neural network", "author": ["M. Seera", "C. Lim"], "venue": "Neural Computing and Applications pp. 1\u201312", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning using the online fuzzy minmax neural network", "author": ["M. Seera", "C. Lim"], "venue": "Neural Computing and Applications 25(2), 469\u2013480", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual domain adaptation via transfer feature learning", "author": ["J. Tahmoresnezhad", "S. Hashemi"], "venue": "Knowledge and Information Systems pp. 1\u201321", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "Tools with Artificial Intelligence (ICTAI), 2014 IEEE 26th International Conference on, pp. 853\u2013858. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pp. 1870\u20131875. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J.J.Y. Wang", "H. Bensmail"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pp. 1882\u20131888. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics 53(3), 403\u2013412", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature space independent semi-supervised domain adaptation via kernel matching", "author": ["M. Xiao", "Y. Guo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(1), 54\u201366", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Mechanical response of cardiovascular stents under vascular dynamic bending", "author": ["J. Xu", "J. Yang", "N. Huang", "C. Uhl", "Y. Zhou", "Y. Liu"], "venue": "Biomedical engineering online 15(1), 1", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust non-negative matrix factorization via joint sparse and graph regularization for transfer learning", "author": ["S. Yang", "C. Hou", "C. Zhang", "Y. Wu"], "venue": "Neural Computing and Applications 23(2), 541\u2013559", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A general framework for transfer sparse subspace learning", "author": ["S. Yang", "M. Lin", "C. Hou", "C. Zhang", "Y. Wu"], "venue": "Neural Computing and Applications 21(7), 1801\u20131817", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C 118(26), 14,586\u201314,594", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Mechanical properties of nanoworm assembled by dna and nanoparticle conjugates", "author": ["Y. Zhou", "S. Sohrabi", "J. Tan", "Y. Liu"], "venue": "Journal of Nanoscience and Nanotechnology 16(6), 5447\u20135456", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 18, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 17, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 26, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 27, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 19, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 1, "context": "[2] proposed a transfer learning framework for the adaptation of text mining models which is based on low-rank shared concept space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] proposed a transductive learning model, which is called selective transfer machine, for the problem of face expression recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] proposed a transfer learning which can adapt knowledge from a source domain to a target domain, and can handle the features of the source and target domains which are partially different.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "\u2013 Xiao and Guo [25] proposed a transfer learning method to handle the domain adaptation problem with completed feature spaces for the source and target domains.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "[8] proposed method for the heterogeneous domain adaptation problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] and Chu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] do not use the labels of the target domain data points.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Among all the methods mentioned above, only the work of Xiao and Guo [25] uses the local connection information to regularize the learning of the classifiers.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "[3] select the source domain data points for the", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We follow the splitting of source and target domain of NG14 data set of [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "[2] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "8012 Xiao and Guo [25] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "[8] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] 28.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 18.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] 21.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "18 Xiao and Guo [25] 19.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] 35.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Abstract Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target domain. The problem is to learn a effective classifier for the target domain. In this paper, we propose a novel transfer learning method for this problem by learning a partially shared classifier for the target domain, and weighting the source domain data points. We learn some shared subspaces for both the data points of the two domains, and a shared classifier in the shared subspaces. We hope that in the shared subspaces, the distributions of two domain can match each other well, and to match the distributions, we weight the source domain data points with different weighting factors. Moreover, we adapt the shared classifier to each domain by learning different adaptation functions. To learn the subspace transformation matrices, the classifier parameters, and the adaptation parameters, we build a objective function with weighted classification errors, parameter regularization, local reconstruction regularization, and distribution matching. This objective function is minimized by an iterative algorithm. Experiments show its effectiveness over benchmark data sets, including travel destination review data set, face expression data set, spam email data set, etc.", "creator": "LaTeX with hyperref package"}}}