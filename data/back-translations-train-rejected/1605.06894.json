{"id": "1605.06894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "DLAU: A Scalable Deep Learning Accelerator Unit on FPGA", "abstract": "As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1x speedup comparing to the Intel Core2 processors, with the power consumption at 234mW.", "histories": [["v1", "Mon, 23 May 2016 04:56:04 GMT  (910kb,D)", "http://arxiv.org/abs/1605.06894v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NE", "authors": ["chao wang", "qi yu", "lei gong", "xi li", "yuan xie", "xuehai zhou"], "accepted": false, "id": "1605.06894"}, "pdf": {"name": "1605.06894.pdf", "metadata": {"source": "CRF", "title": "DLAU: A Scalable Deep Learning Accelerator Unit on FPGA", "authors": ["Chao Wang", "Qi Yu", "Lei Gong", "Xi Li", "Xuehai Zhou"], "emails": ["cswang@ustc.edu.cn,", "llxx@ustc.edu.cn,", "xhzhou@ustc.edu.cn,", "yuiq@mail.ustc.edu.cn).", "yuanxie@ece.ucsb.edu."], "sections": [{"heading": null, "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "II. TILE TECHNIQUES AND HOT SPOT PROFILING", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "III. DLAU ARCHITECTURE AND EXECUTION MODEL", "text": "Fig. 1 describes the DLAU system architecture, which includes an embedded processor, a DDR3 memory controller, a DMA module, and the DLAU accelerator. The embedded processor is responsible for providing programming interfaces to users and communicates with them via JTAG-UART. Specifically, it transmits input data and the weight matrix to internal BRAM blocks, activates the DLAU accelerator, and returns the results to the user. As a standalone unit, the DLAU is flexible and adaptable to accommodate various applications with configurations.The DLAU consists of 3 processing units organized in the apipeline."}, {"heading": "A. TMMU architecture", "text": "TMMU uses an input FIFO buffer, which receives the data transmitted by DMA, and an output FIFO buffer, to send subtotals to PSAU. Figure 2 illustrates the TMMU schematic diagram, in which we set the tile size = 32 as an example. TMMU first reads the data of the weight matrix from the input buffer into different BRAMs in 32 based on the line number of the weight matrix (n = i% 32, where n refers to the number of BRAM, and i is the line number of the weight matrix). TMMU then starts buffering the data of the weight matrix in different BRAMs in 32 based on the line number of the weight matrix. The first time TMMU reads the tiled 32 values to register a registration and starts executing. Parallel to the compilation of the next weight matrix, the data of the TMU are stored in each MMU."}, {"heading": "B. PSAU architecture", "text": "Figure 3 shows the PSAU architecture that accumulates the subsum produced by TMMU. If the subsum is the final result, PSAU writes the value to the output buffer and sends the results to AFAU in a pipeline fashion. PSAU can accumulate one subsum per cycle, so the throughput of the PSAU accumulation corresponds to the generation of the subsum in TMMU."}, {"heading": "C. AFAU architecture", "text": "Finally, the Activation Function Acceleration Unit (AFAU) implements the activation function using piecemeal linear interpolation (y = ai * x + bi, x [x1, xi + 1). This method has been widely used to implement activation functions with negligible loss of precision when the interval between xi and xi + 1 is insignificant. Equation (1) shows the implementation of the sigmoid function. Overall, we divide the sigmoid function into four segments. f (x) = 0 if x \u2212 8 1 + a [b \u2212 xk c] x \u2212 b [b \u2212 x k c] and 0 < x \u2264 8 if different functions are configured. Overall, we divide the sigmoid function into four segments. f (x) = 0 if x \u2212 8 1 + a [b \u2212 xk c] x \u2212 b \u2212 k c] x \u2212 b \u2212 k c] x \u2212 u \u2212 b \u2212 b \u2212 c \u2212 c, if each unit is an acceleration."}, {"heading": "IV. EXPERIMENTS AND DATA ANALYSIS", "text": "To evaluate the performance and cost of the DLAU accelerator, we have implemented the hardware prototype on the Xilinx Zynq Zedboard development board, which features ARM Cortex-A9 processors with 667MHz and programmable textiles. For comparison, we use the Mnist dataset to train the 784 x M x N x 10 Deep Neural Networks in Matlab, and use M x N layer weights and node values for the input data from DLAU. For comparison, we use the Intel Core2 processor with 2.3GHz as the base unit. In the experiment, we use Tile size = 32 taking into account the hardware resources integrated in the Zedboard development board. The DLAU compiles 32 hardware neurons with 32 weights per cycle. DLAU clocking is 200MHz (one cycle takes 5ns). Three network sizes - 64 x 126 x 256 and 256 x 256 are tested."}, {"heading": "A. Speedup Analysis", "text": "We present the acceleration of DLAU and some other similar implementations of deep learning algorithms in Table II. Experimental results show that DLAU is able to achieve up to 36.1-fold acceleration at 256 x 256 network size. In comparison, Ly & Chows [5] and Kim et al work with limited Boltzmann machine algorithms [7], while the DLAU is much more scalable and flexible. DianNao [6] achieves up to 117.8-fold acceleration due to its high operating frequency at 0.98 GHz. Furthermore, since DianNao is not wired on an FPGA platform, it cannot efficiently adapt to different neural network sizes. Figure 4 illustrates the acceleration of DLAU at different network sizes - 64 x 64 x 128 x 256 x speed."}, {"heading": "B. Resource utilization and Power", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this article, we introduced DLAU, a scalable and flexible, FPGA-based deep learning accelerator. DLAU comprises three Pipelined Processing Units that can be reused for large neural networks. DLAU uses tile techniques to divide the input node data into smaller groups and repeatedly calculate it by dividing arithmetic logic. Experimental results on the Xilinx FPGA prototype show that DLAU can achieve 36.1-fold acceleration at reasonable hardware costs and low energy usage. Results are promising, but there are still some future directions, including optimization of the weight matrix and memory access. Also, the trade-off analysis between FPGA and GPU accelerators is a promising direction for large-scale acceleration of neural networks."}], "references": [{"title": "DjiNN and Tonic: DNN as a service and its implications for future warehouse scale computers", "author": ["J Hauswald"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks", "author": ["C Zhang"], "venue": "FPGA", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Data centers are the new polluters", "author": ["P. Thibodeau"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A high-performance FPGA architecture for restricted boltzmann machines, in FPGA", "author": ["D.L. Ly", "P. Chow"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["T Chen"], "venue": "ASPLOS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A highly scalable restricted boltzmann machine FPGA implementation", "author": ["Kim", "S.K"], "venue": "FPL", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A Deep Learning Prediction", "author": ["Qi Yu"], "venue": "Process Accelerator Based FPGA. CCGRID", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Going Deeper with Embedded FPGA Platform for Convolutional Neural Network", "author": ["Jiantao Qiu"], "venue": "FPGA", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Throughput-Optimized OpenCL-based FPGA Acceler-ator for Large-Scale Convolutional Neural Networks. FPGA 2016", "author": ["Naveen Suda"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Currently the most widely used neural models of deep learning are Deep Neural Networks (DNNs) [2] and Convolution Neural Networks (CNNs) [3], which have been proved to have excellent capability in solving picture recognition, voice recognition and other complex machine learning tasks.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Currently the most widely used neural models of deep learning are Deep Neural Networks (DNNs) [2] and Convolution Neural Networks (CNNs) [3], which have been proved to have excellent capability in solving picture recognition, voice recognition and other complex machine learning tasks.", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "billion kilowatt-hours annually by 2020 [4].", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "Chen et al presents a ubiquitous machine-learning hardware accelerator called DianNao [6], which opens a new paradigm to machine learning hardware accelerators focusing on neural networks.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "Currently around FPGA acceleration researches, Ly and Chow [5] designed FPGA based solutions to accelerate the Restricted Boltzmann Machine (RBM).", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "Similarly Kim et al [7] also developed a FPGA based accelerator for the restricted Boltzmann machine.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Other similar works also present FPGA based neural network accelerators [9], [10].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Other similar works also present FPGA based neural network accelerators [9], [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "present a FPGA based accelerator [8], but it cannot accommodate changing network size and network topologies.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "W[1][1] W[1][2] W[1][3] W[1][4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "W[1][1] W[1][2] W[1][3] W[1][4] .", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "W[1][1] W[1][2] W[1][3] W[1][4] .", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 1, "endOffset": 4}, {"referenceID": 0, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "W[2][1] W[2][2] W[2][3] W[2][4] .", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 1, "endOffset": 4}, {"referenceID": 1, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "W[3][1] W[3][2] W[3][3] W[3][4] .", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "W[31][1] W[31][2] W[31][3] W[31][4] .", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "W[31][1] W[31][2] W[31][3] W[31][4] .", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "W[31][1] W[31][2] W[31][3] W[31][4] .", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "W[32][1] W[32][2] W[32][3] W[32][4] .", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "W[32][1] W[32][2] W[32][3] W[32][4] .", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "W[32][1] W[32][2] W[32][3] W[32][4] .", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 1, "endOffset": 4}, {"referenceID": 2, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "W[4][1] W[4][2] W[4][3] W[4][4] .", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "In comparison, Ly&Chows work [5] and Kim et.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "als work [7] present the work only on Restricted Boltzmann Machine algorithms, while the DLAU is much more scalable and flexible.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "DianNao [6] reaches up to 117.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Work Network Clock Speedup Baseline Ly&Chow [5] 256\u00d7256 100MHz 32\u00d7 2.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "al [7] 256\u00d7256 200MHz 25\u00d7 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "4GHz Core2 DianNao [6] General 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "al [3] 256\u00d7256 100MHz 17.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Experimental results depict that our DLAU accelerator occupies similar number of FFs and LUTs to Ly&Chow\u2019s work [5], while it only consumes 35/257=13.", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Ly&Chow [5] XC2VP70 257 N/A 30403 29885", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "al [7] N/A 589824 18 11790 7662", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "work [7], the BRAM utilization of DLAU is insignificant.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1x speedup comparing to the Intel Core2 processors, with the power consumption at 234mW.", "creator": "LaTeX with hyperref package"}}}