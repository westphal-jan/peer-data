{"id": "1605.07026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Spontaneous vs. Posed smiles - can we tell the difference?", "abstract": "Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, `smiles' can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.", "histories": [["v1", "Mon, 23 May 2016 14:21:30 GMT  (2441kb)", "http://arxiv.org/abs/1605.07026v1", "10 pages, 5 figures, 6 tables, International Conference on Computer Vision and Image Processing (CVIP 2016)"]], "COMMENTS": "10 pages, 5 figures, 6 tables, International Conference on Computer Vision and Image Processing (CVIP 2016)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["bappaditya mandal", "nizar ouarti"], "accepted": false, "id": "1605.07026"}, "pdf": {"name": "1605.07026.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nizar Ouarti"], "emails": ["bmandal@i2r.a-star.edu.sg", "nizarouarti@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.07 026v 1 [cs.C V] 23 May 201 6keywords: pose, spontaneous smile, feature extraction, facial analysis."}, {"heading": "1 Introduction", "text": "In fact, it is a purely mental game, in which it is a question of finding a way to go, to find a way to go, to find a way to go, to find a way to walk, to walk."}, {"heading": "2 Feature Extraction from Various Face Components", "text": "We use the facial tracking algorithm developed by Nguyen et al. in [17] to determine the viewpoints. 21 tracking markers are labeled and placed in accordance with the convention, as shown in Fig. 2 (a). Markers are commented manually in the first frame of each video by user input, and then automatically track the remaining frames of the Smile video, it is of good accuracy and precision compared to other face tracking software [2]. Markers are placed on important viewpoints such as eyelids and lip angles for each subject. The convention we followed in selecting these markers is shown in Fig. 2 (a)."}, {"heading": "2.1 Face Normalization", "text": "In order to reduce the inaccuracy due to the head movement of the subject in the video, which can lead to a change in the angle in relation to rollers, yaw and pitch rotations, we use the normalization procedure for the face described in [5]. Let li represents each of the characteristic points used to represent the faces as in Fig. 2. Three non-collinear points (eye midpoints and nose tip) are used to form a plane. Eye midpoints are defined as c1 = l1 + l3 2 and c2 = l4 + l6 2. Angle between the positive normal vector N\u03c1 and the unit vectors U on X (horizontal), Y (vertical) and Z (vertical) axes give the relative head the following pose: \u03b8 = Arccos U."}, {"heading": "2.2 D-Marker Facial Features", "text": "In the first part of our strategy we focus on the extraction of the eyelid and the lips. We first construct an amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude lamplitude amplitude amplitude lamplitude amplitude lamplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude plitude amplitude amplitude amplitude amplitude amplitude amplitude amplitude amplituenplituenplituenplituenplituenplituenp.lnamplipliplipliplipliplipliplipliplipliplip.amplituenamplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplipipipipnnnnamplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituene amplituenpplituenplituene amplituenplituenplituene ampli"}, {"heading": "2.3 Features from Dense Optical Flow", "text": "In fact, most of them are able to determine for themselves what they want to do."}, {"heading": "3 Experimental Results", "text": "We test our proposed algorithm on the UvA-NEMO Smile Database [5], the largest and most comprehensive smile database (both posing and spontaneous) with videos of a total of 400 subjects (185 women, 215 men) between the ages of 8 and 76, giving us a total of 1240 individual videos. Each video consists of a short segment (3-8 seconds) of either posed or spontaneous smiles. Videos are extracted into frames at 50 frames per second. The extracted frames are also converted into grayscale and reduced to 480 x 270. In all experiments, we divide the database, where 80% are used as training samples and the remaining 20% as test samples. The binary classifier SVM in LIBSVM [12] is used to form a hyperplane based on the training samples. When a new test sample is transferred to the SVM, it uses the hyperplane to determine which class the new sample is at the age, subdividing the process by 5 times the differences with the help of a five-fold."}, {"heading": "3.1 Results using parameters from the facial components", "text": "Table 1 and in parentheses (\u00b7) show the accuracy rates in distinguishing spontaneous from positioned smiles by eye and lip characteristics. Results show that eye characteristics play a very important role in determining the posed smiles where lip characteristics are important for spontaneous smiles. Overall, we achieved an accuracy of 71.14% and 73.44%, respectively, when using eye and lip characteristics. Table 2 shows the classification performance based on combined features of eyes and lips. The table shows that these facial characteristics can be used to classify the pose smile better than the spontaneous one. Table 1: The overall accuracy (%) in classifying spontaneous and positioned spontaneous spontaneous eye characteristics using 60.1 (67.5) 39.9 (32.5) poses 17.5 (20.4) 82.5 (79.6) Table 1: The overall accuracy (%) for the classification of spontaneous and positioned spontaneous spontaneous eye characteristics using only 71.14 eye characteristics."}, {"heading": "3.2 Results using Dense Optical flow", "text": "The confusion matrices are shown in Tables 3, in parentheses (\u00b7) and 4. Tables show that the performance of the optical flow is lower compared to the component-based approach. However, the method of extracting facial components requires initialization of the user in order to find and track fiducial points where the dense optical flow functions are fully automatic. It does not require user intervention, so it is more useful for practical applications such as first-person views (FPV) or egocentric views on portable devices such as Google Glass to improve real-time social interactions [14,10]."}, {"heading": "3.3 Results using both Component based features and Dense Optical Flow", "text": "Table 5 shows the confusion matrix based on spontaneous and posed smiles. It can be seen that the performance of spontaneous smile classification has been improved by using features from dense optical flow. Experimental results in Table 5 show that both features from facial components and dense optical flows are important to improve accuracy. Features from facial components (as shown in the table) are useful for encoding information derived from muscle artefacts within a face, but the regulated dense optical flow features help encode fine-grained information for both micro and macro motion variations in facial smile videos."}, {"heading": "3.4 Comparison with Other Methods", "text": "The correct classification rate using different methods on UvA-NEMO is shown in Table 6."}, {"heading": "4 Conclusions", "text": "Distinguishing between spontaneous smiles and posed smiles is a difficult problem, since it requires extracting and learning subtle, tiny facial features. In this work, we analyzed features extracted from parameters based on facial components using fuducial points markers, and tracked them. We also obtained fully automatic features from dense optical flow on eyes and mouth plasters. It has been shown that the parameters of the facial component provide greater accuracy compared to dense optical flow characteristics for classifying smiles. However, the former require the initialization of fictional markers on the first image and therefore is not fully automatic. Dense optical flow has the advantage that the characteristics can be achieved without manual intervention. Combining the parameters of facial components and dense optical flow gives us the highest precision in classifying spontaneous and positioned smiles. Experimental results from the NEA MO database show the greatest effectiveness."}], "references": [{"title": "All smiles are not created equal: Morphology and timing of smiles perceived as amused, polite, and embarrassed/nervous", "author": ["Z. Ambadar", "J. Cohn", "L. Reed"], "venue": "Journal of Nonverbal Behavavior 33, 17\u201334", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental face alignment in the wild", "author": ["A. Asthana", "S. Zafeiriou", "S. Cheng", "M. Pantic"], "venue": "CVPR. Columbus, Ohio, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The timing of facial motion in posed and spontaneous smiles", "author": ["J. Cohn", "K. Schmidt"], "venue": "Intl J. Wavelets, Multiresolution and Information Processing 2, 1\u201312", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Eyes do not lie: Spontaneous versus posed smiles", "author": ["H. Dibeklioglu", "R. Valenti", "A. Salah", "T. Gevers"], "venue": "ACM Multimedia. pp. 703\u2013706", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you really smiling at me? spontaneous versus posed enjoyment smiles", "author": ["H. Dibeklioglu", "A.A. Salah", "T. Gevers"], "venue": "Proceedings of the IEEE ECCV. pp. 525\u2013538", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Telling lies: Cues to deceit in the marketplace, politics, and marriage", "author": ["P. Ekman"], "venue": "WW. Norton & Company, New York", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "The symmetry of emotional and deliberate facial actions", "author": ["P. Ekman", "J. Hager", "W. Friesen"], "venue": "Psychophysiology 18, 101\u2013106", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1981}, {"title": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System", "author": ["P. Ekman", "E. Rosenberg"], "venue": "second ed. Oxford Univ. Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring temporal patterns in classifying frustrated and delighted smiles", "author": ["H. et al."], "venue": "IEEE Trans. Affective Computing 3, 323\u2013334", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-sensor self-quantification of presentations", "author": ["T. Gan", "Y. Wong", "B. Mandal", "V. Chandrasekhar", "M. Kankanhalli"], "venue": "ACM Multimedia. pp. 601\u2013610. Brisbane, Australia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyses of the differences between posed and spontaneous facial expressions", "author": ["M. He", "S. Wang", "Z. Liu", "X. Chen"], "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction pp. 79\u201384", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A practical guide to support vector classification", "author": ["C. Hsu", "C. Chang", "C. Lin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The influence of temporal facial information on the classification of posed and spontaneous enjoyment smiles", "author": ["M.W. Huijser", "T. Gevers"], "venue": "Tech. rep., University of Amsterdam", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A wearable face recognition system on google glass for assisting social interactions", "author": ["B. Mandal", "S. Ching", "L. Li", "V. Chandrasekha", "C. Tan", "J.H. Lim"], "venue": "3 International Workshop on Intelligent Mobile and Egocentric Vision, ACCV. pp. 585\u2013599", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Verification of human faces using predicted eigenvalues", "author": ["B. Mandal", "X.D. Jiang", "A. Kot"], "venue": "19 International Conference on Pattern Recognition (ICPR). pp. 1\u20134. Tempa, Florida, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Oscillating patterns in image processing and in some nonlinear evolution equations", "author": ["Y. Meyer"], "venue": "The Fifteenth Dean Jacquelines B. Lewis Memorial Lectures, American Mathematical Society", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Tracking facial features under occlusions and recognizing facial expressions in sign language", "author": ["T. Nguyen", "S. Ranganath"], "venue": "International Conference on Automatic Face & Gesture Recognition. vol. 6, pp. 1\u20137", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Best basis denoising with non-stationary wavelet packets", "author": ["N. Ouarti", "G. Peyre"], "venue": "International Conferenc on Image Processing. vol. 6, pp. 3825\u20133828", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Method for highlighting at least one moving element in a scene, and portable augmented reality (Aug", "author": ["N. Ouarti", "A. SAFRAN", "B. LE", "S. PINEAU"], "venue": "wO Patent App", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Differentiating spontaneous from posed facial expressions within a generic facial expression recognition framework", "author": ["T. Pfister", "X. Li", "G. Zhao", "M. Pietikainen"], "venue": "ICCV Workshop. pp. 868\u2013875", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of deliberate and spontaneous facial movement in smiles and eyebrow raises", "author": ["K. Schmidt", "S. Bhattacharya", "R. Denlinger"], "venue": "Journal of Nonverbal Behavavior 33, 35\u201345", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "How to distinguish posed from spontaneous smiles using geometric features", "author": ["M. Valstar", "M. Pantic"], "venue": "In Proceedings of ACM ICMI. pp. 38\u201345", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "An eye detection and localization system for natural human and robot interaction without face detection", "author": ["X. Yu", "W. Han", "L. Li", "J. Shi", "G. Wang"], "venue": "TAROS pp. 54\u201365", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "A duality based approach for realtime tv-l1 optical flow", "author": ["C. Zach", "T. Pock", "H. Bischof"], "venue": "In Ann. Symp. German Association Patt. Recogn. pp. 214\u2013223", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence 31(1), 39\u201358", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "This means that, what an individual thinks, feels or understands, etc, deep inside the brain, get imitated into the outside world through its face [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 24, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 0, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 8, "context": "Facial smile expression undeniably plays a huge and pivotal role [25,1,9] in understanding social interactions within a community.", "startOffset": 65, "endOffset": 73}, {"referenceID": 5, "context": "Sometimes people would also pose a smile even when they are reluctantly or unwillingly do or perform something in front of their bosses/peers [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "demonstrate that spontaneous deliberately displayed facial behavior has differences both in utilized facial muscles and their dynamics as compared to posed ones [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 7, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 21, "context": "For example, spontaneous smiles ar smaller in amplitude, longer in duration, slower in onset and offset times than posed smiles [3,8,22].", "startOffset": 128, "endOffset": 136}, {"referenceID": 24, "context": "It is not surprising that in computer vision, algorithms developed for classifying such pose and spontaneous smiles usually fail to generalize to the subtlety and complexity of human pose and spontaneous affective behavior [25].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "Numerous researchers asserted that dynamic features such as duration and speed of the smile play a part in differentiating the nature of the smile [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "A spontaneous smile usually take longer time to reach from onset to apex and then offset as compared to a posed smile [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 20, "context": "On the other hand, the symmetry in (or the lack of) movement of spontaneous and posed smiles do not produce significant distinction in identifying them and is therefore not much useful [21].", "startOffset": 185, "endOffset": 189}, {"referenceID": 21, "context": "In [22] a multi-modal system using geometric features such as shoulder, head and inner facial movements are fused together and GentleSVM-sigmod is used to classify the posed and spontaneous smiles.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "in [11] proposed a technique for feature extraction and compared the performance using geometric and facial appearance features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Their comprehensive study shows that geometric features are generally more effective in detecting posed from spontaneous expressions [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "A spatiotemporal method involving both natural and infrared face videos to distinguish posed and spontaneous expressions is proposed in [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "in [4] used the dynamics of eyelid movements and defined distance based and angular features in the changes of the eye aperture.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Later in [5], they used dynamic characteristics of eyelid, cheek and lip corner movements for classifying posed and spontaneous smiles.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "Temporal facial information is obtained in [13] through segmenting the facial expression into onset, apex and offset which cover the entire duration of the smile.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "in [17] to obtain the fiducial points on the face.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "The markers are manually annotated in the first frame of each video by user input and thereafter it automatically tracks the remaining frames of the smile video, it is of good accuracy and precision as compared to other facial tracking software [2].", "startOffset": 245, "endOffset": 248}, {"referenceID": 4, "context": "To reduce inaccuracy due to the subject\u2019s head motion in the video that can cause change in angle with respect to roll, yaw and pitch rotations, we use the face normalization procedure described in [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 20, "context": "We compute the amplitude of eyelid and lip end movements during a smile using the procedure described in [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "In the second phase of the feature extraction, we use our own proposed dense optical flow [19] for capturing both global and local motions appearing in the smile videos.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 22, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 14, "context": "We use our previously developed face, integration of sketch and graph patterns (ISG) eyes and mouth detectors for face recognition on wearable devices and human-robot-interaction [14,23,15].", "startOffset": 179, "endOffset": 189}, {"referenceID": 4, "context": "3, top left, yellow ROI) with 100% accuracy on the entire UvA-NEMO smile database [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 23, "context": "where the attachment term is based on thresholding method [24] and the regularization term is based on the method developed by Meyer in [16], \u03b2 is a weight controlling the ratio between the end attachment and the term control.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "where the attachment term is based on thresholding method [24] and the regularization term is based on the method developed by Meyer in [16], \u03b2 is a weight controlling the ratio between the end attachment and the term control.", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "in [19] proposed to use a regularization that do not use an usual wavelet but a non-stationary wavelet packet [18], which generalize the concept of wavelet for extracting optical flow information.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "in [19] proposed to use a regularization that do not use an usual wavelet but a non-stationary wavelet packet [18], which generalize the concept of wavelet for extracting optical flow information.", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "We test our proposed algorithm on UvA-NEMO Smile Database [5], it is the largest and most extensive smile (both posed and spontaneous) database with videos from a total of 400 subjects, (185 female, 215 male) aged between 8 to 76 years old, giving us a total of 1240 individual videos.", "startOffset": 58, "endOffset": 61}, {"referenceID": 11, "context": "Binary classifier SVM in LIBSVM [12] is used to form a hyperplane based on the training samples.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "It does not require any user intervention, so it is more useful for practical applications like first-person-views (FPV) or egocentric views on wearable devices like Google Glass for improving real-time social interactions [14,10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 9, "context": "It does not require any user intervention, so it is more useful for practical applications like first-person-views (FPV) or egocentric views on wearable devices like Google Glass for improving real-time social interactions [14,10].", "startOffset": 223, "endOffset": 230}, {"referenceID": 19, "context": "[20] 73.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] 71.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Cohn & Schmidt [21] 77.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Mid-level fusion [5] 87.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Eyelid Features [5] 85.", "startOffset": 16, "endOffset": 19}], "year": 2016, "abstractText": "Smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways. Generally, it shows happy state of the mind, however, \u2018smiles\u2019 can be deceptive, for example people can give a smile when they feel happy and sometimes they might also give a smile (in a different way) when they feel pity for others. This work aims to distinguish spontaneous (felt) smile expressions from posed (deliberate) smiles by extracting and analyzing both global (macro) motion of the face and subtle (micro) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow. Specifically the eyes and lips features are captured and used for analysis. It aims to automatically classify all smiles into either \u2018spontaneous\u2019 or \u2018posed\u2019 categories, by using support vector machines (SVM). Experimental results on large database show promising results as compared to other relevant methods.", "creator": "LaTeX with hyperref package"}}}