{"id": "1605.07133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Towards Multi-Agent Communication-Based Language Learning", "abstract": "We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing", "histories": [["v1", "Mon, 23 May 2016 18:46:46 GMT  (2026kb,D)", "http://arxiv.org/abs/1605.07133v1", "9 pages, manuscript under submission"]], "COMMENTS": "9 pages, manuscript under submission", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["angeliki lazaridou", "nghia the pham", "marco baroni"], "accepted": false, "id": "1605.07133"}, "pdf": {"name": "1605.07133.pdf", "metadata": {"source": "CRF", "title": "Towards Multi-Agent Communication-Based Language Learning", "authors": ["Angeliki Lazaridou", "Marco Baroni"], "emails": ["angeliki.lazaridou@unitn.it", "thenghia.pham@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people are able to understand themselves and understand what they are doing. (...) It is not as if people are able to understand themselves. (...) It is not as if they are doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. \"(...) They are doing it as if they were doing it.\" (...) It is as if they were doing it. \"It is as if they were doing it.\" It is as if they were doing it."}, {"heading": "2 A two-agent referential game simulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The game", "text": "We propose a simple reference game with 2 agents, A1 and A2. The game is defined as follows: \u2022 A1 is shown a visual scene with two objects and it is said to describe the speaker with an attribute that constitutes the reference expression (RE). \u2022 A2 is shown the same visual scene without information about which speaker the speaker is, and the RE must \"point\" to the intended object \u2022 when A2 points to the right object, both agents get a game point. Note that this game is similar to the reference game (Kazemzadeh et al., 2014) played by humans and is designed to collect RE annotations on real scenarios. We observe that it is a cooperative game, i.e. our agents must work together to achieve game points. A1 should learn how to deliver precise phrases that distinguish the object from all others, and A2 should be good at pointing the RE in the presence of the object to interpret it."}, {"heading": "2.2 Visual Scenes", "text": "In fact, most of them are able to survive on their own if they do not put themselves in the position they are in."}, {"heading": "2.3 Agent Players", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 General Training Details", "text": "The parameters of the two agents are learned together during the game. The only monitoring is the communication success, i.e. whether the agents have agreed on the speaker. This constellation can be naturally modelled with Reinforcement Learning (Sutton and Barto, 1998). Within this framework, the agents \"parameters will implement a policy. by executing this policy, the agents execute actions, i.e., A1 selects an attribute and A2 selects an image. The loss function that the two agents minimize is \u2212 IEo1, o2, a) [R (o)], where o1 and o2 are the 2 objects in the visual scene, p (o | o1, o2, a) is the conditional probability that the two agents have the 2 objects as calculated by A2, with the objects and the attribute generated by A1 being the reward function that returns 1 iff (o)."}, {"heading": "3.2 Results", "text": "In fact, most of them are able to go in search of a solution that has its origins in the past."}, {"heading": "4 Discussion", "text": "In our proposed framework, we do not limit the number of agents or their role in the games, i.e. we imagine a community of agents who must all interact with each other and take turns performing different tasks, either cooperating or pitting themselves against each other in minmax-type zero-sum games, in which agents aim to minimize the gain of the opponent (e.g., as in the famous tic-tac-toe game). In our test case, we looked at the most basic act of communication, i.e. learning to relate to things, and we designed a \"grounded\" cooperative task that takes the form of reference games played by two agents. Initial experiments, while encouraging, have shown that it is essential to ensure that agents do not drift into their own language, but develop one that is played by two agents."}], "references": [{"title": "Trevor Darrell", "author": ["Jacob Andreas", "Marcus Rohrbach"], "venue": "and Dan Klein.", "citeRegEx": "Andreas et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Conceptual pacts and lexical choice in conversation", "author": ["Brennan", "Clark1996] Susan E Brennan", "Herbert H Clark"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "Brennan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Brennan et al\\.", "year": 1996}, {"title": "A Joseph Hoane", "author": ["Murray Campbell"], "venue": "and Feng-hsiung Hsu.", "citeRegEx": "Campbell et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Computational interpretations of the gricean maxims in the generation of referring expressions", "author": ["Dale", "Reiter1995] Robert Dale", "Ehud Reiter"], "venue": "Cognitive science,", "citeRegEx": "Dale et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dale et al\\.", "year": 1995}, {"title": "Lia-Ji Li", "author": ["Jia Deng", "Wei Dong", "Richard Socher"], "venue": "and Li Fei-Fei.", "citeRegEx": "Deng et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Simon Garrod", "author": ["Bruno Galantucci"], "venue": "and Gareth Roberts.", "citeRegEx": "Galantucci et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mark Matten", "author": ["Sahar Kazemzadeh", "Vicente Ordonez"], "venue": "and Tamara L Berg.", "citeRegEx": "Kazemzadeh et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ilya Sutskever", "author": ["Alex Krizhevsky"], "venue": "and Geoffrey Hinton.", "citeRegEx": "Krizhevsky et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Armand Joulin", "author": ["Tomas Mikolov"], "venue": "and Marco Baroni.", "citeRegEx": "Mikolov et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kees van Deemter", "author": ["Margaret Mitchell"], "venue": "and Ehud Reiter.", "citeRegEx": "Mitchell et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Antonoglou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": null, "citeRegEx": "Nham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nham et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "The Talking Heads experiment: Origins of words and meanings, volume 1", "author": ["Luc Steels"], "venue": null, "citeRegEx": "Steels.,? \\Q2015\\E", "shortCiteRegEx": "Steels.", "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Barto1998] Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Daniela Rus", "author": ["Stefanie Tellex", "Ross Knepper", "Adrian Li"], "venue": "and Nicholas Roy.", "citeRegEx": "Tellex et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Dialog-based language learning. arXiv preprint arXiv:1604.06045", "author": ["Jason Weston"], "venue": null, "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["Terry Winograd"], "venue": "Technical Report AI 235,", "citeRegEx": "Winograd.,? \\Q1971\\E", "shortCiteRegEx": "Winograd.", "year": 1971}], "referenceMentions": [], "year": 2016, "abstractText": "We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing.", "creator": "LaTeX with hyperref package"}}}