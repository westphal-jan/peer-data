{"id": "1605.07146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Wide Residual Networks", "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN.", "histories": [["v1", "Mon, 23 May 2016 19:27:13 GMT  (106kb,D)", "http://arxiv.org/abs/1605.07146v1", null], ["v2", "Mon, 28 Nov 2016 19:59:22 GMT  (106kb,D)", "http://arxiv.org/abs/1605.07146v2", null], ["v3", "Tue, 17 Jan 2017 15:35:14 GMT  (106kb,D)", "http://arxiv.org/abs/1605.07146v3", null], ["v4", "Wed, 14 Jun 2017 06:06:48 GMT  (106kb,D)", "http://arxiv.org/abs/1605.07146v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sergey zagoruyko", "nikos komodakis"], "accepted": false, "id": "1605.07146"}, "pdf": {"name": "1605.07146.pdf", "metadata": {"source": "CRF", "title": "Wide Residual Networks", "authors": ["Sergey Zagoruyko", "Nikos Komodakis"], "emails": ["sergey.zagoruyko@enpc.fr", "nikos.komodakis@enpc.fr"], "sections": [{"heading": null, "text": "Deep Residual Networks have been proven to scale up to thousands of layers and still have improved performance. However, each fraction of one percent of the improved accuracy costs almost double the number of layers, and therefore the formation of very deep residual networks has a problem of decreasing reuse of features, which means that these networks can only be trained very slowly. To address these problems, we are conducting a detailed experimental study on the architecture of ResNet blocks in this paper, on the basis of which we propose a novel architecture in which we reduce the depth and increase the width of residual networks. We refer to the resulting network structures as wide residual networks (WRNs) and show that these are far superior to their commonly used thin and very deep counterparts. For example, we show that even a simple 16-layer deep wide residual network exceeds all previous deep residual networks in terms of accuracy and efficiency, including a thousand-layer residual type of residual networks, by achieving new CFAR-https-CAR-state-of-https."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2 Wide residual networks", "text": "Residual block with identity mapping can be represented by the following formula: xl + 1 = xl + F (xl, Wl) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (1) where xl + 1 and xl are input and output of the l-th unit in the network, F is a residual function andWl are parameters of the block. Residual network consisting of sequentially stacked residual blocks.In [11] residual networks consists of two type of blocks: \u2022 basic - with two consecutive 3 \u00d7 3 convolutions with batch normalization and ReLU previous convolution: conventure3 \u00d7 3-convture3 \u00d7 3 Fig. \u2022 bottleneck - with one 3 \u00d7 3 convolution surrounding by dimensionality reduction and expansion 1 convolution layer: conven1 \u00d7 3-convertions 3 \u00d7 3-convertions 1 Fig.1 (b) Compared to the original architecture [9] in [11] the order of batch normalization, activation and convolution in dual resiis changed. \""}, {"heading": "2.1 Type of convolutions in residual block", "text": "Let's call B (M) a residual block structure, where M is a list of the core sizes of the revolutionary layers in a block. B (3,1), for example, refers to a residual block with 3 x 3 and 1 x 1 revolutionary layers (we always assume square space cores). Note that, since we do not consider \"bottlenecks\" as explained above, the number of feature layers across the block is always the same. We would like to answer the question of how important each of the 3 x 3 revolutionary layers of the \"basic\" residual architecture is, and whether it can be replaced by a less computationally intensive 1 x 1 layer or even a combination of 1 x 1 and 3 x 3 revolutionary layers, e.g. B (1,3) or B (1,3). This can increase or decrease the visual power of the block. Therefore, we experiment with the following combinations (note that the last combination, i.e. B (3,1,1) block is similar to the basic network architecture (1,3 x 1 x 1 x 3)."}, {"heading": "2.2 Number of convolutional layers per residual block", "text": "We are also experimenting with block deepening factor l to see how it affects performance, comparing networks with the same number of parameters, so in this case we need to build networks with different l and d (where d denotes the total number of blocks), while ensuring that network complexity remains roughly constant, which means, for example, that d should decrease when l increases."}, {"heading": "2.3 Width of residual blocks", "text": "In addition to the above modifications, we are experimenting with the expansion factor k of a block. While the number of parameters increases linearly with l (the deepening factor) and d (the number of ResNet blocks), the number of parameters and the computational complexity are square in k. However, mathematically it is more effective to expand the layers than to have thousands of small cores, since the GPU is much more efficient at parallel computations on large tensors, so we are interested in an optimal d-to-k ratio. An argument for broader residual networks would be that almost all architectures prior to residual networks, including the most successful inception [28] and VGG [24], would be much broader compared to [11]. Thus, residual networks WRN-22-8 and WRN-16-10 (see next paragraph to explain this notation) are very similar in the width, depth and number of parameters to VGG-Architecture Number. \"We continue to refer to a residual network of 1 and WR2.\""}, {"heading": "2.4 Dropout in residual blocks", "text": "As the extension increases the number of parameters, we would like to examine ways of regulation. Residual networks already have a batch normalization that provides a regularization effect, but requires a strong data expansion that we would like to avoid, and it'sblock type depth # params time, s CIFAR-10 B (1,3,1) 40 1,4M 85,8 6.06 B (3,1) 40 1,2M 67,5 5.78 B (1,3) 40 1,3M 72,2 6,42 B (3,1,1) 40 1,3M 82,2 5,86 B (3,3) 28 1,5M 67,5 5,73 B (3,1,3) 22 1,1M 59,9 5,78 Table 2: Test Errors (%, median over 5 runs) in CIFAR-10 residual networks with k = 2 and different block types. Time column measures a training period resistant.l CIFAR-10 1 6,69 2 5,43 3 5,65 4 5,93Table 2: Test Errors (%, median over 5 runs) with different block types of resistance."}, {"heading": "3 Experimental results", "text": "All of our experiments are based on [11] architecture with pre-activation residual blocks, and we use them as a starting point. For the experiments, we chose well-known CIFAR-10, CIFAR-100, and SVHN image classification datasets [15], which consist of 32 x 32 color images of 10 and 100 classes, divided into 50,000 tensile and 10,000 test images. For image preprocessing, we follow the methodology of [11] and [7], which perform global contrast normalization and ZCA brightening. For data augmentation, we do horizontal flips and take random snippets from images that are filled by 4 pixels on each side, and fill missing pixels with reflections of the original image. We do not use heavy data augmentation as suggested in [8]. SVHN is a dataset of Google's Street View House Numbers images and contains about 600,000 images that originate from a much harder real problem."}, {"heading": "Type of convolutions in a block", "text": "We use WRN-40-2 for blocks B (1,3,1), B (3,1), B (1,3) and B (3,1,1), as these blocks have only a 3 \u00d7 3 fold. To keep the number of parameters comparable, we trained other networks with fewer layers: WRN-28-2-B (3,3) and WRN-22-2B (3,1,3). We provide the results, including the test accuracy in the median of 5 runs and the time per training period in Table 2. Block B (3,3) proved to be the best by a small distance, and B (3,1) with B (3,1,3) are very close to B (3,3) in accuracy with fewer parameters and fewer layers. B (3,1,3) is faster than others with a small distance."}, {"heading": "Number of convolutions per block", "text": "Next, we will move on to the experiments on the variation of the deepening factor l (which represents the number of deepening layers per block). Indicative results are shown in Table 3, where in this case we used WRN-40-2 with 3 x 3 turns and trained several networks with different deepening factor l (1,2,3,4), the same number of parameters (2,2 x 106) and the same number of coil layers. As we can ascertain, B (3,3) proved to be the best, while B (3,3,3) and B (3,3) had the worst results. We suspect that this is probably due to the increased difficulty in optimizing due to the reduced number of residual connections in the last two cases. Moreover, B (3) turned out to be rather worse. The conclusion is that B (3,3) is optimal in terms of the number of turns per block. Therefore, in the remaining experiments we only consider large residual networks with a block type B (3,3)."}, {"heading": "Width of residual blocks", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to establish ourselves in the region."}, {"heading": "Dropout in residual blocks", "text": "We trained networks with dropouts that were inserted in residual blockages between convolutions on all datasets. We used cross-validation to determine dropout probability values, 0.3 on CIFAR and 0.4 on SVHN. Also, we did not have to increase the number of training periods without dropout.On CIFAR-10 there is almost no significant improvement, and on CIFAR-100 dropout successfully achieved errors of 0.5% with wide WRN-28-10 and 1.65% with thin ResNet50. To our knowledge, this is the first result to approach 20% errors on CIFAR-100, even surpassing methods with heavy data augmentation. There is a noticeable drop in accuracy with WRN-16-4 on CIFAR, which we speculate that the relatively small number of parameters and datasets go."}, {"heading": "16 4 5.37 24.53 1.85", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "16 4 X 5.55 25.76 1.64", "text": "28 10 4,17 20.50 - 28 10 X 4,39 20.04 - 52 1 6,83 29.88 2.08 52 1 X 6,76 28.23 1,70Table 6: Effect of failure in residual block.0 20 40 60 80 100 120 140 160101102tr a inin g l o ss012345te ste rro r (%) te ste rro r (%) ResNet-50 (Error 2.07%) WRN-16-4 (Error 1.85%) 0 20 40 60 80 100 120 140 160101102tr a inin g l o ss012345te ste rro r (%) te ste rro r (%) WRN-16-4 (Error 1.85%) WRN-16-4-dropout (Error 1.64%) Figure 3: Training curves for SVHN. On the left: thin and wide networks, on the right: Failure effect. Solid lines denote errors (y axis on the right, dashed axis on the left)."}, {"heading": "Computational efficiency", "text": "The rise of Convolutionary Neural Networks in deep learning is strongly attributable to very efficient parallel calculations of the GPU. Thin and deep residual networks with small cores, due to their sequential structure, contradict the nature of GPU calculations. Increasing breadth helps to balance calculations effectively in a much more optimal way, so that large networks are many times more efficient than thin ones, as our benchmarks show. We use cudnn v5 and Titan X to measure backward and forward update times at minibatch size 32 for multiple networks, the results are in Figure 4. We show that our best CIFAR-wide WRN-28-10 are 1.6 times faster than thin ResNet-1001. In addition, the broad WRN-40-4, which has roughly the same accuracy as ResNet-1001, is 8 times faster. We expect WRNs to be the same or even more efficient for other datasets."}, {"heading": "Implementation details", "text": "In all our experiments we use SGD with Nesterov impulse and cross-entropy loss. The initial learning rate is set to 0.1, the weight disintegration to 0.0005, the damping to 0, the impulse to 0.9 and the minibatch size to 128. On CIFAR the learning rate decreased by 0.2 to 60, 120 and 160 epochs and we train for a total of 200 epochs. On SVHN the initial learning rate is set to 0.01 and we lower it to 80 and 120 epochs by 0.1, training for a total of 160 epochs. Our implementation is based on Torch [6]. We use [19] to reduce the memory footprint of all our networks. Our code is available at https: / / github.com / szagoruyko / wide-residual-networks."}, {"heading": "4 Conclusions", "text": "We presented a study on the width of residual networks and showed current results on CIFAR-10, CIFAR-100 and SVHN only due to the increased width of residual networks. We show that large networks with only 16 layers are significantly more powerful than 1000-layer deep networks, which shows that the main performance of residual networks lies in residual blocks and not in extreme depth as previously claimed. Moreover, large residual networks are many times faster to train. We believe that these fascinating results will contribute to further advances in the study of deep neural networks."}], "references": [{"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Yoshua Bengio", "Xavier Glorot"], "venue": "In Proceedings of AISTATS 2010,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Scaling learning algorithms towards AI", "author": ["Yoshua Bengio", "Yann LeCun"], "venue": "Large Scale Kernel Machines. MIT Press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "On the complexity of shallow and deep neural network classifiers", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "European Symposium on Artificial Neural Networks, ESANN 2014, Bruges,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "In International Conference on Learning Representation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "CoRR, abs/1511.07289,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q. Weinberger"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Cifar-10 (canadian institute for advanced research)", "author": ["Alex Krizhevsky", "Vinod Nair", "Geoffrey Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Hugo Larochelle", "Dumitru Erhan", "Aaron Courville", "James Bergstra", "Yoshua Bengio"], "venue": "In Zoubin Ghahramani, editor, Proceedings of the 24th International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Optnet - reducing memory usage in torch neural networks, 2016", "author": ["Francisco Massa"], "venue": "URL https://github.com/fmassa/optimize-net", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems", "author": ["Guido F. Mont\u00fafar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann Lecun"], "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "FitNets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "Technical Report Arxiv report 1412.6550,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet [14], VGG [24], Inception [28] to Residual [9] networks, corresponding to improvements in many image recognition tasks.", "startOffset": 128, "endOffset": 132}, {"referenceID": 19, "context": "Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet [14], VGG [24], Inception [28] to Residual [9] networks, corresponding to improvements in many image recognition tasks.", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet [14], VGG [24], Inception [28] to Residual [9] networks, corresponding to improvements in many image recognition tasks.", "startOffset": 154, "endOffset": 158}, {"referenceID": 6, "context": "Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet [14], VGG [24], Inception [28] to Residual [9] networks, corresponding to improvements in many image recognition tasks.", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "The superiority of deep networks has been spotted in several works in the recent years [3, 20].", "startOffset": 87, "endOffset": 94}, {"referenceID": 15, "context": "The superiority of deep networks has been spotted in several works in the recent years [3, 20].", "startOffset": 87, "endOffset": 94}, {"referenceID": 0, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 7, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 21, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 178, "endOffset": 186}, {"referenceID": 3, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 207, "endOffset": 214}, {"referenceID": 17, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 207, "endOffset": 214}, {"referenceID": 18, "context": "Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies [1, 10], better optimizers [27], skip connections [17, 21], knowledge transfer [4, 22] and layer-wise training [23].", "startOffset": 239, "endOffset": 243}, {"referenceID": 6, "context": "The latest residual networks [9] had a large success winning ImageNet and COCO 2015 competition and achieving state-of-the-art in several benchmarks, including object classification on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and MS COCO.", "startOffset": 29, "endOffset": 32}, {"referenceID": 23, "context": "Also, follow-up work showed that residual links speed up convergence of deep networks [29].", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "Recent follow-up work explored the order of activations in residual networks, presenting identity mappings in residual blocks [11] and improving training of very deep networks.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "The problem of shallow vs deep networks has been in discussion for a long time in machine learning [2, 16] with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits.", "startOffset": 99, "endOffset": 106}, {"referenceID": 13, "context": "The problem of shallow vs deep networks has been in discussion for a long time in machine learning [2, 16] with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits.", "startOffset": 99, "endOffset": 106}, {"referenceID": 9, "context": "The authors of [12] tried to address this problem with the idea of randomly disabling residual blocks during training.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "This method can be viewed as a special case of dropout [25], where each residual block has an identity scalar weight on which dropout is applied.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Motivated by the above observation, our work builds on top of [11] and tries to answer the question of how wide deep residual networks should be and address the problem of training.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "In particular, we present wider deep residual networks that significantly improve over [11], having 50 times less layers and being more than 2 times faster.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Dropout was first introduced in [25] and then was adopted by many successful architectures as [14, 24] etc.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "Dropout was first introduced in [25] and then was adopted by many successful architectures as [14, 24] etc.", "startOffset": 94, "endOffset": 102}, {"referenceID": 19, "context": "Dropout was first introduced in [25] and then was adopted by many successful architectures as [14, 24] etc.", "startOffset": 94, "endOffset": 102}, {"referenceID": 10, "context": "It was then mainly substituted by batch normalization [13] which was introduced as a technique to reduce internal covariate shift in neural network activations by normalizing them to have specific distribution.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "Previously, dropout in residual networks was studied in [11] with dropout being inserted in the identity part of the block, and the authors showed negative effects of that.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "In [11] residual networks consisted of two type of blocks:", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Compared to the original architecture [9] in [11] the order of batch normalization, activation and convolution in residual block was changed from conv-BN-ReLU to BN-ReLUconv.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Compared to the original architecture [9] in [11] the order of batch normalization, activation and convolution in residual block was changed from conv-BN-ReLU to BN-ReLUconv.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Original architecture [11] is equivalent to k = 1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "There are essentially three simple ways to increase representational power of residual blocks: \u2022 to add more convolutional layers per block \u2022 to widen the convolutional layers by adding more feature planes \u2022 to increase filter sizes in convolutional layers As small filters were shown to be very effective in several works including [24, 29] we do not consider using filters larger than 3\u00d73.", "startOffset": 333, "endOffset": 341}, {"referenceID": 23, "context": "There are essentially three simple ways to increase representational power of residual blocks: \u2022 to add more convolutional layers per block \u2022 to widen the convolutional layers by adding more feature planes \u2022 to increase filter sizes in convolutional layers As small filters were shown to be very effective in several works including [24, 29] we do not consider using filters larger than 3\u00d73.", "startOffset": 333, "endOffset": 341}, {"referenceID": 22, "context": "One argument for wider residual networks would be that almost all architectures before residual networks, including the most successful Inception [28] and VGG [24], were much wider compared to [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "One argument for wider residual networks would be that almost all architectures before residual networks, including the most successful Inception [28] and VGG [24], were much wider compared to [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "One argument for wider residual networks would be that almost all architectures before residual networks, including the most successful Inception [28] and VGG [24], were much wider compared to [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 8, "context": "All of our experiments are based on [11] architecture with pre-activation residual blocks and we use it as baseline.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "CIFAR-10 and CIFAR-100 datasets [15] consist of 32\u00d732 color images drawn from 10 and 100 classes split into 50,000 train and 10,000 test images.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "For image preprocessing we follow the methodology of [11] and [7], performing global contrast normalization and ZCA whitening.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "For experiments on SVHN we don\u2019t do any image preprocessing, except dividing images by 255 to provide them in [0,1] range as input.", "startOffset": 110, "endOffset": 115}, {"referenceID": 8, "context": "To speed up training we run \u00abtype of convolutions in a block\u00bb and \u00abnumber of convolutions per block\u00bb experiments with k = 2 and reduced depth compared to [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 0, "context": "We show indicative results in table 3, where in this case we took WRN-40-2 with 3\u00d73 convolutions and trained several networks with different deepening factor l \u2208 [1,2,3,4], same number of parameters (2.", "startOffset": 162, "endOffset": 171}, {"referenceID": 1, "context": "We show indicative results in table 3, where in this case we took WRN-40-2 with 3\u00d73 convolutions and trained several networks with different deepening factor l \u2208 [1,2,3,4], same number of parameters (2.", "startOffset": 162, "endOffset": 171}, {"referenceID": 2, "context": "We show indicative results in table 3, where in this case we took WRN-40-2 with 3\u00d73 convolutions and trained several networks with different deepening factor l \u2208 [1,2,3,4], same number of parameters (2.", "startOffset": 162, "endOffset": 171}, {"referenceID": 3, "context": "We show indicative results in table 3, where in this case we took WRN-40-2 with 3\u00d73 convolutions and trained several networks with different deepening factor l \u2208 [1,2,3,4], same number of parameters (2.", "startOffset": 162, "endOffset": 171}, {"referenceID": 17, "context": "57 FitNet [22] 8.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "39 ELU [5] 6.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "28 original-ResNet[9] 110 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "82 stoc-depth[12] 110 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "pre-act-ResNet[11] 110 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "Results for [11] are shown with minibatch size 128 (as ours), and 64 in parenthesis.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "Thin 50-layer deep network even outperforms thin 152-layer deep network with stochastic depth [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Our implementation is based on Torch [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 14, "context": "We use [19] to reduce memory footprints of all our networks.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.", "creator": "LaTeX with hyperref package"}}}