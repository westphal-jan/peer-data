{"id": "1605.07246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Adaptive ADMM with Spectral Penalty Parameter Selection", "abstract": "The alternating direction method of multipliers (ADMM) is a versatile tool for solving a wide range of constrained optimization problems, with differentiable or non-differentiable objective functions. Unfortunately, its performance is highly sensitive to a penalty parameter, which makes ADMM often unreliable and hard to automate for a non-expert user. We tackle this weakness of ADMM by proposing a method to adaptively tune the penalty parameters to achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm, inspired by the successful Barzilai-Borwein spectral method for gradient descent, yields fast convergence and relative insensitivity to the initial stepsize and problem scaling.", "histories": [["v1", "Tue, 24 May 2016 00:48:28 GMT  (48kb,D)", "http://arxiv.org/abs/1605.07246v1", null], ["v2", "Fri, 10 Jun 2016 19:21:26 GMT  (168kb,D)", "http://arxiv.org/abs/1605.07246v2", null], ["v3", "Fri, 9 Sep 2016 19:51:17 GMT  (48kb,D)", "http://arxiv.org/abs/1605.07246v3", null], ["v4", "Wed, 25 Jan 2017 18:49:04 GMT  (403kb,D)", "http://arxiv.org/abs/1605.07246v4", "AISTATS 2017"], ["v5", "Wed, 19 Jul 2017 16:23:11 GMT  (240kb,D)", "http://arxiv.org/abs/1605.07246v5", "AISTATS 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NA", "authors": ["zheng xu", "mario a t figueiredo", "tom goldstein"], "accepted": false, "id": "1605.07246"}, "pdf": {"name": "1605.07246.pdf", "metadata": {"source": "CRF", "title": "Adaptive ADMM with Spectral Penalty Parameter Selection", "authors": ["Zheng Xu", "M\u00e1rio A. T. Figueiredo", "Thomas Goldstein"], "emails": ["1xuzh@cs.umd.edu,", "tomg@cs.umd.edu,", "2mario.figueiredo@tecnico.ulisboa.pt"], "sections": [{"heading": "1 Introduction", "text": "ADMM breaks down complex optimization problems into sequences of simpler sub-problems that can often be solved in a closed form; its simplicity, flexibility, and broad applicability have made ADMM a state-of-the-art solver in machine learning, signal processing, and many other areas [2]. It is well known that ADMM's efficiency depends on the careful selection of a penalty parameter that is often manually adjusted by users to their respective problem situations. In this paper, we propose to automate and accelerate ADMM by using adaptive (i.e. automated) step selection rules that essentially eliminate user oversight and dramatically increase performance [1, 7, 12, 23, 24]."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Alternating Direction Method of Multipliers", "text": "ADMM dates back to the 1970s [8, 10], and its convergence was evident in the 1990s [4], and convergence rates have been the subject of much recent work (see [11, 14, 18] and references therein). Over the past decade, ADMM has become a tool of choice to handle a variety of optimization problems in machine learning, signal processing, and many other areas (for a comprehensive review see [2]). \u2212 ADMM deals with problems in the form refills u Rn, v Rm H (u) + G (v), subject to Au + Bv = b, (1) ar Xiv: 160 5.07 246v 1 [cs.L G] on May 24, where H: Rn R and G: Rm R are closed, properly, convex functions, A Rp n, B Rp \u00b7 n, B Rp \u00b7 m, and b Rvp l \u2212 converk = \u2212 Rk \u2212 r \u2212 r \u2212 n convergence."}, {"heading": "2.2 Parameter tuning and adaptation", "text": "In the specific case where the target is a strictly convex quadratic function, criteria for the selection of an optimal constant penalty have recently been proposed [9, 19]. Residual balance (RB [2, 15]) is the only adaptive method available for general form problems (1). RB is based on the following observation: Increasing residual size strengthens the penalty value, resulting in smaller primary residuals but larger dual residuals. Conversely, reducing residual size leads to larger primary and smaller dual residuals. Since both residuals must be small in convergence, it makes sense to \"balance\" them, i.e. to coordinate both residuals of similar magnitude in order to keep both residuals of similar magnitude. A simple scheme for this goal is to optimize residual size when the residual size must be small."}, {"heading": "2.3 Dual interpretation of ADMM", "text": "We will now explain the close relationship between ADMM and Douglas-Rachdord splitting (DRS) [4, 6, 11], which plays a central role in the proposed approach: the initial observation is that the dual problem (1) has the formin conjugate of F, defined as F (y) = supx < x, y > \u2212 F (x) [20]. The DRS algorithm solves (7) by solving two sequences (k) k \u00b2 N and (k) k \u00b2 N according to the definition of F (y) = supx < x, y > \u2212 F (x) [20]. The DRS algorithm solves (7) by generating two sequences (k) k \u00b2 N and (k) k \u00b2 k \u00b2 n according to the definition of F (y) = supx < x, y > \u2212 F (x)."}, {"heading": "2.4 Spectral (Barzilai-Borwein) stepsize selection", "text": "The classical gradient descent step for the unrestricted minimization of a smooth function F: Rn \u2192 R has the form xk + 1 = xk \u2212 \u03c4k \u0445 F (xk). The spectral gradient method advanced by Barzilai and Borwein (BB) [1] adaptively selects the step size \u03c4k to achieve rapid convergence. In short, the standard method (there are variants) BB sets \u03c4k = 1 / \u03b1k, with \u03b1kI imitating the Hessen of F over the last step by aiming for a quasi-Newton step. Using a criterion of the smallest squares, the yield of K = argmin \u03b1, - or vice versa. \u2212 F (xk \u2212 1) \u2212 \u03b1 F (xk \u2212 1) \u2212 \u03b1 (xk \u2212 1) (xk \u2212 1), which represents an estimate of the curvature of F over the previous step of the algorithm."}, {"heading": "3 Spectral penalty parameters", "text": "Inspired by the BB method [1], we propose a method for selecting spectral penalty parameters for ADMM. We first derive a spectral step size rule for DRS and then adapt this rule to ADMM. Finally, we discuss securing methods to prevent unexpected behavior when curvature estimates are inaccurate."}, {"heading": "3.1 Spectral stepsize for Douglas-Rachford splitting", "text": "Considering the dual problem (7), and following the observation in (15) on the BB method, we model (approximate) H-question (approximate) and (16) on iteration k as linear functions of their arguments, namely either H-question (approximate) or K-question (approximate). Once we have obtained these curvature estimates, we will be able to use the following statement. Statement 1 (spectral DRS). Specify the DRS steps (8) - (9) are applied to problem (7), whereby (without the subscript k from the k question), \u03b2k question, k question, k question, k question, and K question referring to the question."}, {"heading": "3.2 Spectral stepsize estimation", "text": "Proposition 1 shows how to adaptively select the penalty parameters. We can start by obtaining linear estimates of the undergradients of the terms in the dual objective (7).The geometric mean of the optimal slope of the gradients for these two terms is then the optimal DRS step size and also the optimal penalty parameter for ADMM, thanks to the results in subsection 2.3.We now address the question of how to calculate the values k = \u03b1 \u2212 1k and \u03b2 \u2212 k = \u03b2 \u2212 1 k for the components G-k and H-K in the k iteration.The curvature parameters are calculated based on the results from iteration k and an older iteration k0 < k. Taking into account the identities (11), we define the values k: = Minimum quantity k-k: = Minimum quantity MK-S-K, MK-S-S-S-S-S: = Minimum size."}, {"heading": "3.3 Safeguarding: testing the quality of stepsize estimates", "text": "For some iterations, the linear models for one or both subgradients underlying the choice of the spectral increment may be very inaccurate. If this occurs, the smallest squares method may produce ineffective step sizes. However, for ADMM, there is no notion of \"stable\" step size (any constant step size is stable), so line search methods are not applicable. Rather, we propose to protect the method by evaluating the quality of the curvature estimates and updating the step size only if the curvature estimates meet a reliability criterion. The linear model (16) assumes that the change in the double gradients is proportional to the change in the ducal variables. < < < &ltK < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.4 Adaptive ADMM with spectral penalty parameter", "text": "The full adaptive ADMM (AADMM) is shown in algorithm 1. We propose to update the step size only every Tf iteration. The overhead of the proposed adaptivity scheme is modest and requires only a few internal products, as well as the memory required to maintain prior iteration. As mentioned in [15], convergence is guaranteed if the adaptivity is turned off after a limited number of iterations; however, in practice we have considered this unnecessary."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental setting", "text": "We will consider several applications to demonstrate the effectiveness of the proposed AADMM problem. We will focus on statistical problems with non-differentiable objectives: linear regression with elastic network regulation [11], square programming (QP) [2, 9, 11, 19], basic tracking [2, 11] and consensus \"1-regulated logistic regression [2]. We will use both synthetic and benchmark data sets (from the UCI repository and the LIBSVM side) used in [5, 16, 17, 21, 23, 25]. For comparison, we have implemented vanilla ADMM, fast ADMM with a restart strategy [11] and ADMM with residual accounting [2, 15] using \u00b5 = 10 and \u03b7 = 2 in (6), and switched off after 1000 algorithm 1 Adaptive ADMM (AMM) with penalty parameter selection (penalty parameter selection: 0.0 to 0.0)."}, {"heading": "4.2 Applications", "text": "We are a way to apply ADMM to this problem, is to rewrite it asmin u, v1 2)."}, {"heading": "4.3 Convergence results", "text": "Table 1 shows the convergence rate of ADMM and its variants for the applications described in Section 4.2. Fixed-step vanilla ADMM performs poorly in practice: in 9 of 17 realistic datasets, it fails to achieve convergence in the maximum number of iterations 2. Fast ADMM [11] often outperforms vanilla ADMM, but does not compete with the proposed AADMM, which also outperforms residual balancing in all test cases except the Rcv1 problem in terms of logistical consensus regression."}, {"heading": "4.4 Sensitivity to initial stepsize and problem scaling", "text": "Finally, we examine the sensitivity of the different ADMM variants to the initial choice of the penalty parameter (\u03c40) and the problem scaling. Figure 1 shows iteration counts for a wide range of \u03c40, for elastic net regression (left) and general QP (right) with synthetic data sets. Scaling sensitivity experiments were performed by multiplying the measurement vector c in the elastic net and QP by a scalar. Fast ADMM and vanilla ADMM use the fixed initial penalty parameter \u03c40 and react highly sensitively to this choice, as Figure 1 shows; in contrast, AADMM is stable with respect to the initial \u03c40 and the problem scale s."}, {"heading": "5 Conclusion", "text": "We have proposed Adaptive ADMM (AADMM), a new variant of the very popular ADMM algorithm that addresses one of its fundamental disadvantages: the critical dependence on a penalty parameter that requires careful coordination, a disadvantage that has made the use of ADMM more difficult for the layman, so that AADMM has the potential to contribute to a broader and easier applicability of this highly flexible and efficient optimization tool. Our approach imports and adapts the \"spectral\" step size method of Barzilai Borwein from the smooth optimization literature and adapts it to the more general class of problems addressed by ADMM. The cornerstone of our approach is the fact that ADMM with Douglas-Rachford splitting (DRS) is applied to the double problem for which we develop a spectral step size selection rule; this rule will then be converted into a criterion transferring the penalty parameter of MADM to a backup function, finally avoiding any problems with MADM."}], "references": [{"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J. Borwein"], "venue": "IMA J. Num. Analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. and Trends in Mach. Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D. Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Applications of Lagrangian-based alternating direction methods and connections to split Bregman", "author": ["E. Esser"], "venue": "CAM report,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "On the Barzilai-Borwein method. In Optimization and control with applications, pages 235\u2013256", "author": ["R. Fletcher"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "Optimal parameter selection for the alternating direction method of multipliers: quadratic problems", "author": ["E. Ghadimi", "A. Teixeira", "I. Shames", "M. Johansson"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e9mes de Dirichlet non lin\u00e9aires", "author": ["R. Glowinski", "A. Marroco"], "venue": "ESAIM: Mode\u0301lisation Mathe\u0301matique et Analyse Nume\u0301rique,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1975}, {"title": "Fast alternating direction optimization methods", "author": ["T. Goldstein", "B. O\u2019Donoghue", "S. Setzer", "R. Baraniuk"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A field guide to forward-backward splitting with a FASTA implementation", "author": ["T. Goldstein", "C. Studer", "R. Baraniuk"], "venue": "arXiv preprint arXiv:1411.3406,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adaptive primal-dual splitting methods for statistical learning and image processing", "author": ["T. Goldstein", "M. Li", "X. Yuan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On non-ergodic convergence rate of Douglas-Rachford alternating direction method of multipliers", "author": ["B. He", "X. Yuan"], "venue": "Numerische Mathematik,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities", "author": ["B. He", "H. Yang", "S. Wang"], "venue": "Jour. Optim. Theory and Appl.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Efficient L1 regularized logistic regression", "author": ["S.-I. Lee", "H. Lee", "P. Abbeel", "A. Ng"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "In ACM SIGKDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "A general analysis of the convergence of ADMM", "author": ["R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Alternating direction method of multipliers for strictly convex quadratic programs: Optimal parameter selection", "author": ["A. Raghunathan", "S. Di Cairano"], "venue": "In American Control Conf.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Convex Analysis", "author": ["R. Rockafellar"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1970}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "In ECML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Fast ADMM algorithm for distributed optimization with adaptive penalty", "author": ["C. Song", "S. Yoon", "V. Pavlovic"], "venue": "arXiv preprint arXiv:1506.08928,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Sparse reconstruction by separable approximation", "author": ["S. Wright", "R. Nowak", "M. Figueiredo"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Gradient methods with adaptive step-sizes", "author": ["B. Zhou", "L. Gao", "Y.-H. Dai"], "venue": "Computational Optimization and Applications,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "ADMM decomposes complex optimization problems into sequences of simpler subproblems, often solvable in closed form; its simplicity, flexibility, and broad applicability, made ADMM a state-of-the-art solver in machine learning, signal processing, and many other areas [2].", "startOffset": 267, "endOffset": 270}, {"referenceID": 0, "context": "automated) stepsize selection rules have been proposed, which essentially dispense with user oversight and dramatically boost performance [1, 7, 12, 23, 24].", "startOffset": 138, "endOffset": 156}, {"referenceID": 6, "context": "automated) stepsize selection rules have been proposed, which essentially dispense with user oversight and dramatically boost performance [1, 7, 12, 23, 24].", "startOffset": 138, "endOffset": 156}, {"referenceID": 11, "context": "automated) stepsize selection rules have been proposed, which essentially dispense with user oversight and dramatically boost performance [1, 7, 12, 23, 24].", "startOffset": 138, "endOffset": 156}, {"referenceID": 22, "context": "automated) stepsize selection rules have been proposed, which essentially dispense with user oversight and dramatically boost performance [1, 7, 12, 23, 24].", "startOffset": 138, "endOffset": 156}, {"referenceID": 23, "context": "automated) stepsize selection rules have been proposed, which essentially dispense with user oversight and dramatically boost performance [1, 7, 12, 23, 24].", "startOffset": 138, "endOffset": 156}, {"referenceID": 0, "context": "In this paper, we propose to automate and speed up ADMM by using stepsize selection rules imported from the gradient descent literature, namely the Barzilai-Borwein \u201cspectral\u201d method for smooth unconstrained problems [1, 7].", "startOffset": 217, "endOffset": 223}, {"referenceID": 6, "context": "In this paper, we propose to automate and speed up ADMM by using stepsize selection rules imported from the gradient descent literature, namely the Barzilai-Borwein \u201cspectral\u201d method for smooth unconstrained problems [1, 7].", "startOffset": 217, "endOffset": 223}, {"referenceID": 7, "context": "ADMM dates back to the 1970s [8, 10].", "startOffset": 29, "endOffset": 36}, {"referenceID": 9, "context": "ADMM dates back to the 1970s [8, 10].", "startOffset": 29, "endOffset": 36}, {"referenceID": 3, "context": "Its convergence was shown in the 1990s [4], and convergence rates have been the topic of much recent work (see [11, 14, 18] and references therein).", "startOffset": 39, "endOffset": 42}, {"referenceID": 10, "context": "Its convergence was shown in the 1990s [4], and convergence rates have been the topic of much recent work (see [11, 14, 18] and references therein).", "startOffset": 111, "endOffset": 123}, {"referenceID": 13, "context": "Its convergence was shown in the 1990s [4], and convergence rates have been the topic of much recent work (see [11, 14, 18] and references therein).", "startOffset": 111, "endOffset": 123}, {"referenceID": 17, "context": "Its convergence was shown in the 1990s [4], and convergence rates have been the topic of much recent work (see [11, 14, 18] and references therein).", "startOffset": 111, "endOffset": 123}, {"referenceID": 1, "context": "In the last decade, ADMM became one of the tools of choice to handle a wide variety of optimization problems in machine learning, signal processing, and many other areas (for a comprehensive review, see [2]).", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": "The convergence of the algorithm can be monitored using primal and dual \u201cresiduals,\u201d both of which approach zero as the iterates become more accurate, and which are defined as rk = b\u2212Auk \u2212Bvk, and dk = \u03c4kAB(vk \u2212 vk\u22121), (5) respectively [2].", "startOffset": 236, "endOffset": 239}, {"referenceID": 8, "context": "In the specific case where the objective is a strictly convex quadratic function, criteria for choosing an optimal constant penalty have been recently proposed [9, 19].", "startOffset": 160, "endOffset": 167}, {"referenceID": 18, "context": "In the specific case where the objective is a strictly convex quadratic function, criteria for choosing an optimal constant penalty have been recently proposed [9, 19].", "startOffset": 160, "endOffset": 167}, {"referenceID": 1, "context": "Residual balancing (RB [2, 15]) is the only available adaptive method for general form problems (1).", "startOffset": 23, "endOffset": 30}, {"referenceID": 14, "context": "Residual balancing (RB [2, 15]) is the only available adaptive method for general form problems (1).", "startOffset": 23, "endOffset": 30}, {"referenceID": 1, "context": "for parameters \u03bc > 1 and \u03b7 > 1 [2].", "startOffset": 31, "endOffset": 34}, {"referenceID": 21, "context": "RB has recently been adapted to distributed optimization [22] and other primal-dual splitting methods [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "RB has recently been adapted to distributed optimization [22] and other primal-dual splitting methods [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "ADMM with adaptive penalty is not guaranteed to converge, although convergence can be enforced by fixing \u03c4k = \u03c4 after a finite number of iterations [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 3, "context": "We now explain the close relationship between ADMM and Douglas-Rachdord Splitting (DRS) [4, 6, 11], which plays a central role in the proposed approach.", "startOffset": 88, "endOffset": 98}, {"referenceID": 5, "context": "We now explain the close relationship between ADMM and Douglas-Rachdord Splitting (DRS) [4, 6, 11], which plays a central role in the proposed approach.", "startOffset": 88, "endOffset": 98}, {"referenceID": 10, "context": "We now explain the close relationship between ADMM and Douglas-Rachdord Splitting (DRS) [4, 6, 11], which plays a central role in the proposed approach.", "startOffset": 88, "endOffset": 98}, {"referenceID": 19, "context": "with F \u2217 denoting the Fenchel conjugate of F , defined as F \u2217(y) = supx\u3008x, y\u3009 \u2212 F (x) [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "where we use the standard notation \u2202F (x) for the subdifferential of F evaluated at x [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "Spectral gradient methods, pioneered by Barzilai and Borwein (BB) [1], adaptively choose the stepsize \u03c4k to achieve fast convergence.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Spectral gradient methods dramatically outperform schemas with constant stepsize in many applications [7, 24] and have been generalized to handle non-differentiable problems via proximal gradient methods [23, 12].", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "Spectral gradient methods dramatically outperform schemas with constant stepsize in many applications [7, 24] and have been generalized to handle non-differentiable problems via proximal gradient methods [23, 12].", "startOffset": 102, "endOffset": 109}, {"referenceID": 22, "context": "Spectral gradient methods dramatically outperform schemas with constant stepsize in many applications [7, 24] and have been generalized to handle non-differentiable problems via proximal gradient methods [23, 12].", "startOffset": 204, "endOffset": 212}, {"referenceID": 11, "context": "Spectral gradient methods dramatically outperform schemas with constant stepsize in many applications [7, 24] and have been generalized to handle non-differentiable problems via proximal gradient methods [23, 12].", "startOffset": 204, "endOffset": 212}, {"referenceID": 0, "context": "Inspired by the BB method [1], we propose a spectral penalty parameter selection method for ADMM.", "startOffset": 26, "endOffset": 29}, {"referenceID": 19, "context": "An important property relating F and F \u2217 is that y \u2208 \u2202H(x) if and only if x \u2208 \u2202H\u2217(y) [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "As is typical in the spectral stepsize literature [24], the curvature of \u0124(\u03bb) is estimated via one of the two least squares problems", "startOffset": 50, "endOffset": 54}, {"referenceID": 23, "context": "where (following [24]) SD stands for steepest descent stepsize, and MG for minimum gradient stepsize.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Rather than choosing one or the other, we suggest the hybrid stepsize rule proposed in [12, 24], defined as", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "Rather than choosing one or the other, we suggest the hybrid stepsize rule proposed in [12, 24], defined as", "startOffset": 87, "endOffset": 95}, {"referenceID": 14, "context": "As noted in [15], convergence is guaranteed if the adaptivity is turned off after a finite number of iterations; however, we have found this to be unnecessary in practice.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 157, "endOffset": 171}, {"referenceID": 8, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 157, "endOffset": 171}, {"referenceID": 10, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 157, "endOffset": 171}, {"referenceID": 18, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 157, "endOffset": 171}, {"referenceID": 1, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 187, "endOffset": 194}, {"referenceID": 10, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 187, "endOffset": 194}, {"referenceID": 1, "context": "We focus on statistical problems involving non-differentiable objectives: linear regression with elastic net regularization [11], quadratic programming (QP) [2, 9, 11, 19], basis pursuit [2, 11], and consensus `1-regularized logistic regression [2].", "startOffset": 245, "endOffset": 248}, {"referenceID": 4, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 15, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 16, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 20, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 22, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 24, "context": "We use both synthetic and benchmark datasets (obtained from the UCI repository and the LIBSVM page) used in [5, 16, 17, 21, 23, 25].", "startOffset": 108, "endOffset": 131}, {"referenceID": 10, "context": "For comparison, we implemented vanilla ADMM, fast ADMM with a restart strategy [11], and ADMM with residual balancing [2, 15], using \u03bc = 10 and \u03b7 = 2 in (6), and turned off after 1000", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "For comparison, we implemented vanilla ADMM, fast ADMM with a restart strategy [11], and ADMM with residual balancing [2, 15], using \u03bc = 10 and \u03b7 = 2 in (6), and turned off after 1000", "startOffset": 118, "endOffset": 125}, {"referenceID": 14, "context": "For comparison, we implemented vanilla ADMM, fast ADMM with a restart strategy [11], and ADMM with residual balancing [2, 15], using \u03bc = 10 and \u03b7 = 2 in (6), and turned off after 1000", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "The initial penalty \u03c40 = 1/10 is used for all problems except the general QP problem, where \u03c40 is set to the value proposed for quadratic problems in [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "EN is a modification of the `1 (or LASSO) regularizer that helps preserving groups of highly correlated variables [11, 25], and requires solving", "startOffset": 114, "endOffset": 122}, {"referenceID": 24, "context": "EN is a modification of the `1 (or LASSO) regularizer that helps preserving groups of highly correlated variables [11, 25], and requires solving", "startOffset": 114, "endOffset": 122}, {"referenceID": 10, "context": "The synthetic dataset introduced in [11, 25] and realistic dataset introduced in [5, 25] are investigated.", "startOffset": 36, "endOffset": 44}, {"referenceID": 24, "context": "The synthetic dataset introduced in [11, 25] and realistic dataset introduced in [5, 25] are investigated.", "startOffset": 36, "endOffset": 44}, {"referenceID": 4, "context": "The synthetic dataset introduced in [11, 25] and realistic dataset introduced in [5, 25] are investigated.", "startOffset": 81, "endOffset": 88}, {"referenceID": 24, "context": "The synthetic dataset introduced in [11, 25] and realistic dataset introduced in [5, 25] are investigated.", "startOffset": 81, "endOffset": 88}, {"referenceID": 2, "context": "where z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 15, "context": "We study classification problems from [16, 21] with C = 1 and a random synthetic QP from [11], where Q \u2208 R500\u00d7500 with condition number approximately 4.", "startOffset": 38, "endOffset": 46}, {"referenceID": 20, "context": "We study classification problems from [16, 21] with C = 1 and a random synthetic QP from [11], where Q \u2208 R500\u00d7500 with condition number approximately 4.", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "We study classification problems from [16, 21] with C = 1 and a random synthetic QP from [11], where Q \u2208 R500\u00d7500 with condition number approximately 4.", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "An extended form with D\u0302 = [D, I] \u2208 Rm\u00d7(n+m) has been used to reconstruct occluded and corrupted faces [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 22, "context": "We also use a data matrix for face reconstruction from the Extended Yale B Face dataset [23], where each frontal face image is scaled to 32\u00d7 32.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "ADMM has become an important tool for solving distributed problems [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 15, "context": "Binary classification problems from [16, 17, 21] are also used to test the effectiveness of the proposed method.", "startOffset": 36, "endOffset": 48}, {"referenceID": 16, "context": "Binary classification problems from [16, 17, 21] are also used to test the effectiveness of the proposed method.", "startOffset": 36, "endOffset": 48}, {"referenceID": 20, "context": "Binary classification problems from [16, 17, 21] are also used to test the effectiveness of the proposed method.", "startOffset": 36, "endOffset": 48}, {"referenceID": 10, "context": "Fast ADMM [11] often outperforms vanilla ADMM, but does not compete with the proposed AADMM, which also outperforms residual balancing in all test cases except in the Rcv1 problem for consensus logistic regression.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Application Dataset #samples \u00d7 #features 3 Vanilla ADMM Fast ADMM [11] Residual balance [15] Adaptive ADMM", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Application Dataset #samples \u00d7 #features 3 Vanilla ADMM Fast ADMM [11] Residual balance [15] Adaptive ADMM", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "The alternating direction method of multipliers (ADMM) is a versatile tool for solving a wide range of constrained optimization problems, with differentiable or non-differentiable objective functions. Unfortunately, its performance is highly sensitive to a penalty parameter, which makes ADMM often unreliable and hard to automate for a non-expert user. We tackle this weakness of ADMM by proposing a method to adaptively tune the penalty parameters to achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm, inspired by the successful Barzilai-Borwein spectral method for gradient descent, yields fast convergence and relative insensitivity to the initial stepsize and problem scaling.", "creator": "LaTeX with hyperref package"}}}