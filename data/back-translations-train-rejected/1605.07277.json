{"id": "1605.07277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples", "abstract": "Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.", "histories": [["v1", "Tue, 24 May 2016 03:27:48 GMT  (350kb,D)", "http://arxiv.org/abs/1605.07277v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "ian goodfellow"], "accepted": false, "id": "1605.07277"}, "pdf": {"name": "1605.07277.pdf", "metadata": {"source": "CRF", "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples", "authors": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "emails": ["ngp5056@cse.psu.edu", "mcdaniel@cse.psu.edu", "ian@openai.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to play by the rules they have imposed on themselves."}, {"heading": "2. APPROACH OVERVIEW", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3. TRANSFERABILITY OF ADVERSARIAL", "text": "In this section, our working hypothesis is that intratechnical and transversal transferability of opposite samples are strong phenomena within the machine learning space. Therefore, we examine these two phenomena empirically across a wide range of machine learning techniques: deep neural networks (DNNs), logistic regression (LR), support vector machines (SVM), decision trees (DT), nearest neighbours (kNN) and ensembles (Ens). All models are susceptible to intratechnical, contrary transferability of samples - misclassification of samples by different models trained using the same machine learning technique, the phenomenon is stronger for differentiable models such as DNNs and LR than for non-differentiable models such as SVMs, DTs and kNs. Then, we find that DNNs and kNs exhibit resistance to transferable models that are distinct models such as SVTN and NN-sensitive."}, {"heading": "3.1 Experimental Setup", "text": "We describe here the data set and machine learning models used in this section to study both types of portability. Dataset - We use the underlying MNIST data set of handwritten digits [16]. This data set has been well studied in both machine learning communities and security communities. We chose it because its dimensionality is suitable for the range of machine learning techniques included in our study, all of which work at least reasonably well on this dataset. The task associated with the data set is to classify the images into one of the 10 classes corresponding to every possible digit, from 0 to 9. The data set includes 50, 000 training samples, 10, 000 validation samples and 10, 000 test samples. Each 28x28 pixel image is encoded as a vector of intensities whose real values range from 0 (black) to 1 (white)."}, {"heading": "3.2 Intra-technique Transferability", "text": "We show that differentiable models such as DNNs and LR are more susceptible to intratechnical transferability than non-differentiable models such as SVMs, DTs and kNNs. We measure intratechnical transferability between models i and j, both of which were learned with the same machine learning technique, as the proportion of opposing samples misclassified by model i is wrong. To train different models using the same machine learning method, we divide the training into vulnerable subsets A, B, C, D, E from 10,000 samples each to increase indexes. For each of the machine learning techniques (DNN, LR, SVM, kNN, kNN), we learn five different models called A, B, C, D, E. Model accuracy, i.e. the proportion of labels correctly predicted by the model for the test data, is reported in Figure 2a."}, {"heading": "3.3 Cross-technique Transferability", "text": "We define the transferability between the i and j models with different machine learning techniques, as a proportion of the opposing samples that we have produced. (This is a complex phenomenon as intratechnical transmission techniques, as these are models that may learn very different techniques such as DNNs and DTs.) Nevertheless, the transferability of different machine learning techniques is a surprising phenomenon to which techniques such as LR, SVM, DT and ensembles are prone, making it easy to opt for conciliatory models that are misclassified by different machine learning techniques. We study the transferability between the different techniques across models used in Section 3.2, which are described in Section 3.1 and 6. To these we add a 6th model: an ensemble f (~ x)."}, {"heading": "4. LEARNING CLASSIFIER SUBSTITUTES", "text": "In the previous section, we identified machine learning techniques (e.g. DNNs and LR) that provide adequate models for creating samples that have been misclassified between models with different techniques, i.e. opposing samples with strong transferability. So to create opposing samples that have been misclassified by a classifier whose underlying model is unknown, opponents can instead use a replacement model if it solves the same classification problem and its parameters are known. Therefore, efficient learning of substitutes is the key to black box attacks where opponents target remote classifiers whose model, parameters and training data are unknown to them. This is exactly the attack scenario being evaluated against commercial machine learning platforms in Section 5, while in this section we focus on predictive learning of machine learning substitutes."}, {"heading": "4.1 Dataset Augmentation for Substitutes", "text": "To get around this, we need to rely on a technique introduced in [20] to learn how to apply a dataset augmentation technology to learn the replacement modeling techniques. First, one collects a replacement training set of limited size (representative of the task solved by the oracle) and labels it by interrogating the oracle. Using this marked data, we train a first replacement model f probably poorly as the source of the samples used for the training."}, {"heading": "4.2 Deep Neural Network Substitutes", "text": "However, the authors concluded with preliminary results that suggest applicability to a nearby neighbor. Here, we show that the technique is generalizable and applicable to many machine learning techniques by transferring its performance to a DNN in 5 types of ML classifiers and obtaining a DNN that mimics the decision limits of the original classification techniques. Using the Jacobian-based methods we use to adjust the labels produced by 5 different labels, one for each of the mentioned ML techniques, these classifiers, which serve as oracles, are all trained on the models previously described in Section 3.1."}, {"heading": "4.3 Logistic Regression Substitutes", "text": "After generalized replacement learning with a demonstration of the capacity of DNNs to approximate any machine learning model, we now consider the replacement itself with another machine learning technique. Experiments in Section 3.3 led us to the conclusion that interdisciplinary transferability is not specific to opposing samples created on DNNs, but instead applies to many learning techniques. If we look at Figure 3, a natural candidate is a logistic regression, as it exhibits high interdisciplinary transmission rates that exceed DNNs unless they target themselves. Jacobs-based Dataset Augmentation Implementation for DNNs is easily adapted to multi-class logistic regression, as it has multi-class logistic regression analogous to the softmax layer, often used by deep neural networks to produce class probability vectors. We can easily comment the (i, j) component of the Jacary JR-Lx [)"}, {"heading": "4.4 Support Vector Machines Substitutes", "text": "After observing that deep learning and logistical regression were both relevant when approaching the classification oracles, we now turn to SVM for replacement learning.SVM-based datasets are motivated by the strong portability of the opposing samples observed with an SVM in Section 3. SVM-based datasets require a new augmentation technology. SVM-based datasets prove insufficient to investigate the specificity of SVM in analogy to future datasets. SVM-based datasets require a new augmentation technology. SVM-based datasets prove to be the specificity of SVM + 1 analogous to future datasets."}, {"heading": "5. BLACK-BOX ATTACKS OF REMOTE MACHINE LEARNING CLASSIFIERS", "text": "Intra-technical and cross-technical transferability of opposing samples, together with learning substitutes for classification oracles, enable a series of attacks on remote machine learning systems, the internals of which are unknown to opponents. To illustrate the feasibility of black box attacks on such remote systems, in an experiment we target two machine learning classifiers trained and hosted by Amazon and Google. We find that it is possible to create samples that have been misclassified by these commercial oracles at corresponding rates of 96.19% and 88.94%, after making 800 requests to learn replacement models that approach them."}, {"heading": "5.1 The Oracle Attack Method", "text": "The enemy threat model in this section is identical to the one used in learning substitution techniques in Section 4: enemies have oracle access to the remote classifier; their type, parameters, or training set are unknown to the opponent; the attack method uses Sections 3 and 4 of this paper and is a generalization of the approach described in Section 4; the opponent first trains a local replacement model to approximate the remote hosted classifier by queries to the oracle; we consider the use of deep learning and logistical regression to learn substitutes for classifiers; we apply the two refinements introduced in this paper: a periodic step size and a reservoir sampling; and since substitution models are locally trained, the opponent has complete knowledge of their model parameters."}, {"heading": "5.2 Amazon Web Services Oracle", "text": "In fact, it is a way in which people are able to put themselves at the centre. (...) It is not as if people are able to put themselves at the centre. (...) It is as if they are able to put themselves at the centre. (...) It is as if they are able to put themselves at the centre. (...) It is not as if they are able to put themselves at the centre. (...) It is as if they are able to put themselves at the centre. \"(...)"}, {"heading": "5.3 Google Cloud Prediction Oracle", "text": "To test whether this poor performance is limited to the Amazon Web Services platform, we are now using the Google Cloud Prediction API Service6 = Replacement Google Prediction API Service6. The procedure for training a classifier on Google's platform is similar to Amazon's. We first load the CSV method called \"prediction.trainedmodels.inserts\" into Google's cloud storage service, and then evaluate the resulting model using the API method prediction.trainedmodels.prediction and an uploaded CSV file of the MNIST test set. The API reports 92% accuracy on this test platform for the model trained.We now the use the Google Cloud Prediction Prediction API Prediction of PredictionsCSV PredictionsCuploaded."}, {"heading": "6. ADVERSARIAL SAMPLE CRAFTING", "text": "Building on previous work [22, 12, 19], which describes how adversaries can efficiently select disturbances that lead to deep neural networks to misclassify their input, we present new crafting algorithms for adversaries that target support vector machines (SVMs) and decision trees (DTs)."}, {"heading": "6.1 Deep Neural Networks", "text": "Deep Neural Networks (DNNs) learn hierarchical representations of high-dimensional inputs that are used to solve ML tasks [11], including classification. Each representation is modeled by a layer of neurons - elementary parameterized computational units - that behave like a multidimensional function. Input of each layer fi \u2212 1 multiplied by a series of weights that are part of the parameters of the layer successi. Thus, a DNN f can be considered a composition of parameterized functionsf: ~ x 7 \u2192 fn (\u03b8n,... f2 (\u03b82, f1 (\u03b81, ~ x))...), whose parameters are learned during training. For example, in the case of classification, the network will receive a large collection of known input-label pairs x."}, {"heading": "6.2 Multi-class Logistic Regression", "text": "Logistic regression is the generalization of logistic regression to classification problems with N > 2 classes [18]. Logistic regression attempts to find the hypothesis that best corresponds to the data within the hypothesis class, which represents a composition of a sigmoid function versus the class of linear functions. A multi-class logistic regression model f can be described as follows: f: ~ x 7 \u2192 [e ~ wj \u00b7 ~ x x] n l = 1 e ~ wl \u00b7 ~ x] j 1.. N (9), where \u03b8 = {w1,..., wN} is the set of parameters learned during training, e.g. by gradient descent or Newton methodology. Opponents can also produce contrary samples that have been misclassified by multi-class logistic regression models using the fast-gradient character method [12]. In the case of logistic regression, the method finds the most harmful disturbance (corresponding to the maximum norm rating of 8)."}, {"heading": "6.3 Nearest Neighbors", "text": "The k-nearest neighbor algorithm (kNN) is a non-parametric classifier [18] that does not require a familiarization phase. Predictions are made on invisible input, taking into account the k-points in the training sets that are closest depending on the distance. The estimated class of input is the one that is most frequently observed among these k-points. when k is set to 1, as is the case in this essay, the classifier: f: ~ x 7 \u2192 Y [arg min ~ z-z-x-22] (10), which outputs a line Y, is the matrix of indicator vectors, the labels for the training data X.Although the kNN algorithm is not parametric, it is still prone to conciliatory samples, as shown in [20, sc24]. In this essay, we used the method of fast gradient characters to generate reconciliatory samples to the next neighbors, which are lassified by the neighbors."}, {"heading": "6.4 Multi-class Support Vector Machines", "text": "For each class k of the machine learning task, a binary support vector machine classifier fk is trained with samples of class k that are marked as positive and samples from other classes as negative [8]. To classify a sample, each binary linear SVM classifier fk makes a prediction and the entire multiclass classifier f issues the class with the strongest confidence. Each of these underlying linear SVMs is a model fk that classifies invisible samples ~ x, using the following: fk: ~ x 7 \u2192 sgn (~ w [k] \u00b7 ~ x + bk) (12) We are now introducing an algorithm to find contradictory samples classified by a multi-class linear SVM f. To get the most out of our knowledge, this method is more computationally efficient than previous [4]: it does not require optimization."}, {"heading": "6.5 Decision Trees", "text": "Partitioning is done by selecting a trait and a corresponding condition threshold that best minimizes a cost function over the training data. Each node is an if-else statement with a threshold condition corresponding to one of the characteristics of the sample. A sample is classified by traversing the decision tree from its root to one of its leaves according to the conditions specified in intermediate tree nodes. The leaf reached indicates the class assigned.Opponents can also make contradictory inputs that are incorrectly classified by decision trees. To the best of our knowledge, this is the first contradictory sample to suggest an algorithm for decision trees. Intuition takes advantage of the underlying tree structure of the classification model. To find a contradictory sample and tree, we simply look for leaves with different classes in the neighborhood of the leaf corresponding to the initial preference of the decision tree for the sample."}, {"heading": "7. DISCUSSION AND RELATED WORK", "text": "After completing their training on known input-label pairs (~ x, ~ y), classifiers f (x) on invisible inputs ~ x [18]. Models extrapolate from knowledge extracted by processing input-label pairs during training to make label predictions. Several factors, including (1) shortcomings in training algorithms, (2) the linearity of many underlying components based on built-in machine learning models, and (3) the limited amount of training points, which do not always represent the entire plausible input domain, are characterized by adverse manipulations of their inputs."}, {"heading": "8. CONCLUSIONS", "text": "Our work first revealed the strong phenomenon of contrarian transferability of samples within the machine learning space. We found not only that contrary samples are misclassified within models trained using the same machine learning technique, but also between models trained using different techniques. We then improved accuracy and reduced the computational complexity of an existing algorithm for replacing classifiers for machine learning. We demonstrated that DNNs and LR can both be used effectively to learn a replacement model for many classifiers trained with a deep neural network, logistic regression, support vector machine, decision tree, and closest neighbor. In a final experiment, we demonstrated how all of these results could be used to target online classifiers trained and hosted by Amazon and Google, without any knowledge of the model design or parameters, but instead simply by creating label queries for 800 inputs to successfully classify these 894% and 819% of their attacks."}, {"heading": "9. REFERENCES", "text": "[1] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be safe? In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, pp. 16-25. ACM, 2006. [2] E. Battenberg, S. Dieleman, and al. Lasagne: Lightweight library to build and train neural networks in theano, 2015. [3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, and al. Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientific computing conference (SciPy), Volume 4, page 3. Biggio, 2010. [4] B. Biggio, I. Corona, and al. Evasion attacks against machine learning at test time."}, {"heading": "10. ACKNOWLEDGMENTS", "text": "The research was sponsored by the Army Research Laboratory and conducted under the collaboration agreement number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted to represent, either explicitly or implicitly, the official guidelines of the Army Research Laboratory or the U.S. Government."}], "references": [{"title": "Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "computer and communications security, pages 16\u201325. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "and al", "author": ["E. Battenberg", "S. Dieleman"], "venue": "Lasagne: Lightweight library to build and train neural networks in theano", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "and al", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin"], "venue": "Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientific computing conference (SciPy), volume 4, page 3. Austin, TX", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "and al", "author": ["B. Biggio", "I. Corona"], "venue": "Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases, pages 387\u2013402. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 26(4):984\u2013996", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML, pages 97\u2013112", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "L. Pavel"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Model compression", "author": ["C. Bucila", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "Proceedings of the 2016 International Conference on Learning Representations. Computational and Biological Learning Society", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "Proceedings of the 2015 International Conference on Learning Representations. Computational and Biological Learning Society", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Deep Learning and Representation Learning Workshop at NIPS 2014. arXiv preprint arXiv:1503.02531", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43\u201358. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 405\u2013412", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Machine Learning in Adversarial Settings", "author": ["P. McDaniel", "N. Papernot", "Z.B. Celik"], "venue": "IEEE Security & Privacy Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "and al", "author": ["N. Papernot", "P. McDaniel"], "venue": "The limitations of deep learning in adversarial settings. In Proceedings of the 1st IEEE European Symposium on Security and Privacy. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "and al", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha"], "venue": "Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever"], "venue": "Bruna, , et al. Intriguing properties of neural networks. In Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Random sampling with a reservoir", "author": ["J.S. Vitter"], "venue": "ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples [22, 12, 19]; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs.", "startOffset": 100, "endOffset": 112}, {"referenceID": 11, "context": "Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples [22, 12, 19]; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs.", "startOffset": 100, "endOffset": 112}, {"referenceID": 18, "context": "Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples [22, 12, 19]; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs.", "startOffset": 100, "endOffset": 112}, {"referenceID": 19, "context": "Adversarial samples can be used to, for example, subvert fraud detection, bypass content filters or malware detection, or to mislead autonomous navigation systems [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 18, "context": "Figure 1: An adversarial sample (bottom row) is produced by slightly altering a legitimate sample (top row) in a way that forces the model to make a wrong prediction whereas a human would still correctly classify the sample [19].", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "Adversarial sample transferability is the property that some adversarial samples produced to mislead a specific model f can mislead other models f \u2032\u2014even if their architectures greatly differ [22, 12, 20].", "startOffset": 192, "endOffset": 204}, {"referenceID": 11, "context": "Adversarial sample transferability is the property that some adversarial samples produced to mislead a specific model f can mislead other models f \u2032\u2014even if their architectures greatly differ [22, 12, 20].", "startOffset": 192, "endOffset": 204}, {"referenceID": 19, "context": "Adversarial sample transferability is the property that some adversarial samples produced to mislead a specific model f can mislead other models f \u2032\u2014even if their architectures greatly differ [22, 12, 20].", "startOffset": 192, "endOffset": 204}, {"referenceID": 18, "context": "trained a local deep neural network (DNN) using crafted inputs and output labels generated by the target \u201cvictim\u201d DNN [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "Note that this is distinct from knowledge transfer, which refers to techniques designed to transfer the generalization knowledge learned by a model f during training\u2014and encoded in its parameters\u2014to another model f \u2032 [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 21, "context": "Previous work on adversarial example transferability has primarily studied the case where at least one of the models involved in the transfer is a neural network [22, 12, 24], while we aim to more generally characterize the transferability between a diverse set of models chosen to capture most of the space of popular machine learning algorithms.", "startOffset": 162, "endOffset": 174}, {"referenceID": 11, "context": "Previous work on adversarial example transferability has primarily studied the case where at least one of the models involved in the transfer is a neural network [22, 12, 24], while we aim to more generally characterize the transferability between a diverse set of models chosen to capture most of the space of popular machine learning algorithms.", "startOffset": 162, "endOffset": 174}, {"referenceID": 23, "context": "Previous work on adversarial example transferability has primarily studied the case where at least one of the models involved in the transfer is a neural network [22, 12, 24], while we aim to more generally characterize the transferability between a diverse set of models chosen to capture most of the space of popular machine learning algorithms.", "startOffset": 162, "endOffset": 174}, {"referenceID": 21, "context": "This can be done by solving the following optimization problem [22]:", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "They yield adversarial samples effectively misleading non-linear and non-convex models like neural networks [22, 12, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 11, "context": "They yield adversarial samples effectively misleading non-linear and non-convex models like neural networks [22, 12, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 18, "context": "They yield adversarial samples effectively misleading non-linear and non-convex models like neural networks [22, 12, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 15, "context": "Models are trained on MNIST data [16] to solve the hand-written digit recognition task.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "As we validate the hypothesis throughout Sections 4 and 5, we operate under the specific threat model of an oracle, described in [20], which characterizes realistic adversarial settings.", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Dataset - We use the seminal MNIST dataset of handwritten digits [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "To train DNN, LR, and kNN models, we use Theano [3] and Lasagne [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "To train DNN, LR, and kNN models, we use Theano [3] and Lasagne [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "We enhance an algorithm introduced in [20] to learn a substitute model for a given classifier simply by querying it for labels on carefully chosen inputs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "To circumvent this, we build on a technique introduced in [20], which leverages a dataset augmentation technique to train the substitute model.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "Jacobian-based dataset augmentation - We use this augmentation technique introduced in [20] to learn DNN and LR substitutes for oracles.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "showed that substitute DNNs can approximate another DNNs [20].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Reservoir Sampling - We also introduce the use of reservoir sampling [23] as a mean to reduce the number of queries made to the oracle.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "In [20], the oracle classifier approximated was always a DNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The attack method leverages Sections 3 and 4 of this paper, and is a generalization of the approach introduced in [20].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "Previous work conducted such an attack using a substitute and targeted classifier both trained using deep learning, demonstrating that the attack was realistic using the MetaMind API providing Deep Learning as a Service [20].", "startOffset": 220, "endOffset": 224}, {"referenceID": 11, "context": "[12], shallow models like logistic regression are unable to cope with adversarial samples and learn a classifier resistant to them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "One could also try to deploy other defense mechanisms like defensive distillation [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Building on previous work [22, 12, 19] describing how adversaries can efficiently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).", "startOffset": 26, "endOffset": 38}, {"referenceID": 11, "context": "Building on previous work [22, 12, 19] describing how adversaries can efficiently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).", "startOffset": 26, "endOffset": 38}, {"referenceID": 18, "context": "Building on previous work [22, 12, 19] describing how adversaries can efficiently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).", "startOffset": 26, "endOffset": 38}, {"referenceID": 10, "context": "Deep Neural Networks (DNNs) learn hierarchical representations of high dimensional inputs used to solve ML tasks [11], including classification.", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "To craft adversarial samples misclassified by DNNs, an adversary with knowledge of the model f and its parameters \u03b8 can use the fast gradient sign method introduced in [12] or the Jacobian-based iterative approach proposed in [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "To craft adversarial samples misclassified by DNNs, an adversary with knowledge of the model f and its parameters \u03b8 can use the fast gradient sign method introduced in [12] or the Jacobian-based iterative approach proposed in [19].", "startOffset": 226, "endOffset": 230}, {"referenceID": 11, "context": "[12] proposed to compute the following perturbation:", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Multi-class logistic regression is the generalization of logistic regression to classification problems with N > 2 classes [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "Adversaries can also craft adversarial samples misclassified by multi-class logistic regression models using the fast gradient sign method [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The k nearest neighbor (kNN) algorithm is a lazy-learning non-parametric classifier [18]: it does not require a training phase.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "Although the kNN algorithm is non-parametric, it is still vulnerable to adversarial samples as pointed out in [20, 24].", "startOffset": 110, "endOffset": 118}, {"referenceID": 23, "context": "Although the kNN algorithm is non-parametric, it is still vulnerable to adversarial samples as pointed out in [20, 24].", "startOffset": 110, "endOffset": 118}, {"referenceID": 7, "context": "For each class k of the machine learning task, a binary Support Vector Machine classifier fk is trained with samples of class k labeled as positive and samples from other classes labeled as negative [8].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "To the best of our knowledge, this method is more computationally efficient than previous [4]: it does not require any optimization.", "startOffset": 90, "endOffset": 93}, {"referenceID": 17, "context": "Decision trees are defined by recursively partitioning the input domain [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "Upon completion of their training on collections of known input-label pairs (~x, ~y), classifiers f make label predictions f(x) on unseen inputs ~x [18].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "Our work builds on a practical method for attacks against black-box deep learning classifiers [20].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "Learning substitute models approximating the decision boundaries of targeted classifiers alleviates the need of previous attacks [22, 12, 19] for knowledge of the target architecture and parameters.", "startOffset": 129, "endOffset": 141}, {"referenceID": 11, "context": "Learning substitute models approximating the decision boundaries of targeted classifiers alleviates the need of previous attacks [22, 12, 19] for knowledge of the target architecture and parameters.", "startOffset": 129, "endOffset": 141}, {"referenceID": 18, "context": "Learning substitute models approximating the decision boundaries of targeted classifiers alleviates the need of previous attacks [22, 12, 19] for knowledge of the target architecture and parameters.", "startOffset": 129, "endOffset": 141}, {"referenceID": 8, "context": "Learning substitutes is an instance of knowledge transfer, a set of techniques to transfer the generalization knowledge learned by a model into another model [9, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 9, "context": "Learning substitutes is an instance of knowledge transfer, a set of techniques to transfer the generalization knowledge learned by a model into another model [9, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 16, "context": "The existence of such a threat vector calls for the design of defensive mechanisms [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "Unfortunately, we found that defenses proposed in the literature\u2014such as training with adversarial samples [12]\u2014were noneffective, or we were unable to deploy them because of our lack of access to the machine learning model targeted\u2014for instance distillation [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "Unfortunately, we found that defenses proposed in the literature\u2014such as training with adversarial samples [12]\u2014were noneffective, or we were unable to deploy them because of our lack of access to the machine learning model targeted\u2014for instance distillation [21].", "startOffset": 259, "endOffset": 263}, {"referenceID": 0, "context": "This work is part of a series of security evaluations of machine learning algorithms [1, 5].", "startOffset": 85, "endOffset": 91}, {"referenceID": 4, "context": "This work is part of a series of security evaluations of machine learning algorithms [1, 5].", "startOffset": 85, "endOffset": 91}, {"referenceID": 5, "context": "Unlike us, previous work in this field assumed knowledge of the model architecture and parameters [6, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 13, "context": "Unlike us, previous work in this field assumed knowledge of the model architecture and parameters [6, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 6, "context": "For instance poisoning the training data used to learn models was only considered in the context of binary SVMs whose training data is known [7] or anomaly detection systems whose underlying model is known [15].", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "For instance poisoning the training data used to learn models was only considered in the context of binary SVMs whose training data is known [7] or anomaly detection systems whose underlying model is known [15].", "startOffset": 206, "endOffset": 210}], "year": 2016, "abstractText": "Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.", "creator": "LaTeX with hyperref package"}}}