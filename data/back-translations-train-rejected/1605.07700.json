{"id": "1605.07700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Learning Purposeful Behaviour in the Absence of Rewards", "abstract": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \"just out of reach\" of the agent's current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.", "histories": [["v1", "Wed, 25 May 2016 01:33:34 GMT  (181kb,D)", "http://arxiv.org/abs/1605.07700v1", "Extended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33rd International Conference on Machine Learning, New York, NY, USA, 2016"]], "COMMENTS": "Extended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33rd International Conference on Machine Learning, New York, NY, USA, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["marlos c machado", "michael bowling"], "accepted": false, "id": "1605.07700"}, "pdf": {"name": "1605.07700.pdf", "metadata": {"source": "META", "title": "Learning Purposeful Behaviour in the Absence of Rewards", "authors": ["Marlos C. Machado", "Michael Bowling"], "emails": ["MACHADO@UALBERTA.CA", "MBOWLING@UALBERTA.CA"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to live, in which they are able to live, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they are able to"}, {"heading": "2. Background", "text": "In this section we present the problem of reinforcement learning (RL) and the options framework. We also discuss the problem of option discovery and some approaches that attempt to address it. We call convention random variables by uppercase letters (e.g. St, Rt), vectors by bold letters (e.g. \u03b8), functions by lowercase letters (e.g. v) and sentences by calligraphic script (e.g. S, A)."}, {"heading": "2.1. Reinforcement Learning and Options", "text": "In the RL Framework Directive (Sutton & Barto, 1998; Szepesva \u0301 ri, 2010 \u03b2), an agent aims to maximize a concept of cumulative reward by taking action in an environment; these actions can influence the next state in which the agent will be as good as any subsequent rewards he will experience. It is generally assumed that the tasks of interest satisfy the Markov property by being referred to as Markov decision-making processes (MDPs). An MDP is formally defined as a 5-tuple < S, A, Sutp, p, \u03b3 >. At the time, the agent is in the state in which he takes action leading to the next state st + 1 leading to the next state st + 1 \u0445S according to the transition probability."}, {"heading": "2.2. Option Discovery", "text": "The potential of options to dramatically influence learning by enhancing exploration is well known (e.g. McGovern & Sutton, 1998; McGovern & Barto, 2001; Kulkarni et al., 2016), but most of the work that benefits from options does not discover them, but assumes that they are provided or that there is a persistent notion of interest (reward) that can be used to discover options (Singh et al., 2004).The work that examines how options can be found falls into three different categories; the most common approach is to attempt to identify sub-target states through some heuristic ones, such as visitor frequency (McGovern & Barto, 2001), graph-related metrics such as ambivalence (S ims & Barto, 2008), or partition measurement criteria (Menache et al., 2002; Mannor et al., 2004; S ims."}, {"heading": "3. Option Discovery for Purposeful Agents", "text": "The approaches, based on intrinsic motivation and novelty, are some of the ways to circumvent the absence of rewards in the environment. (Schmidhuber (2010) summarizes several papers based on intrinsic motivation, which he defines as algorithms that maximize a reward signal derived from the agent's learning progress. (Lehman & Stanley (2011) has argued that agents should provide feedback from the environment that they apply even in more traditional environments such as seeking, maximizing novelty. Both ideas relate to our work. We discover options based on novelty that do not provide extrinsic rewards (Lehman & Stanley, 2011). These options are based on a very loose conception of the environment that aims to change fundamental components of a compressed environmental presentation (Schmidhuber, 2010).Our algorithms are based on four distinct concepts, namely, the changes in time (i)."}, {"heading": "4. Experimental Evaluation", "text": "We performed an empirical validation in a handmade domain that allows us to clearly represent the properties of our algorithm, namely: \u2022 With each new iteration of the algorithm, the options discovered become more and more complex. \u2022 More complex options are able to move the agent farther away in the state area. \u2022 As the agent moves further away with newly discovered options, he observes new self-purposes by discovering more options, creating a self-reinforcing gap. We evaluated our algorithm in a random passage in a toy domain that consists of moving around a ring of length 4096 with deterministic transitions. The agent starts with x-coordination 0 and selects each step he takes to the right or left. We use linear functional approximation with the two binary codes as representation (12 bits long). When the agent leaves the state, he goes to the state \u2212 1 (0000 0000000002 \u2192 11112 11112)."}, {"heading": "5. Conclusion", "text": "In this paper, we have presented a new algorithm that is capable of discovering options without feedback in the form of environmental rewards. Our algorithm discovers options that reproduce purposes arising from the states seen by a randomly acting agent. We presented experimental results that show how the discovered options significantly improve exploration by transferring determination to the agents and avoiding traditional aimless dithering. We also showed evidence that our approach can work well with partial observation. As a future work, we plan to investigate how this algorithm behaves in more complex environments, such as the Arcade Learning Environment (Bellemare et al., 2013). We can evaluate our algorithm in these large areas because it is able to behave closer to each other, unlike most other approaches to option discovery. Of course, we then need to be able to learn a policy by evaluating the discovered algorithm in these large areas, because it is able to behave closer to each other than most other approaches to option discovery."}, {"heading": "Acknowledgements", "text": "The authors thank Richard S. Sutton and Marc G. Bellemare for insightful discussions that have helped improve this work, and Csaba Szepesva'ri for pointing to the Neumann series that we used in our evidence, which was supported by grants from Alberta Innovates Technology Futures and the Alberta Innovates Centre for Machine Learning (AICML)."}], "references": [{"title": "Intrinsic Motivation and Reinforcement Learning", "author": ["Barto", "Andrew G"], "venue": "In Intrinsically Motivated Learning in Natural and Artificial Systems,", "citeRegEx": "Barto and G.,? \\Q2013\\E", "shortCiteRegEx": "Barto and G.", "year": 2013}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Skill Characterization Based on Betweenness", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2008}, {"title": "Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "The MAXQ Method for Hierarchical Reinforcement Learning", "author": ["Dietterich", "Thomas G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Dietterich and G.,? \\Q1998\\E", "shortCiteRegEx": "Dietterich and G.", "year": 1998}, {"title": "Discovering Hierarchy in Reinforcement Learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": "ArXiv e-prints,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone", "author": ["Lehman", "Joel", "Stanley", "Kenneth O"], "venue": "Evolutionary Computation,", "citeRegEx": "Lehman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lehman et al\\.", "year": 2011}, {"title": "Dynamic Abstraction in Reinforcement Learning via Clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "Roles of Macroactions in Accelerating Reinforcement Learning", "author": ["McGovern", "Amy", "Sutton", "Richard S"], "venue": "Technical report, University of Massachusetts,", "citeRegEx": "McGovern et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 1998}, {"title": "QCut - Dynamic Discovery of Sub-goals in Reinforcement Learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Proceedings of the European Conference on Machine Learning (ECML),", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Autonomous Inverted Helicopter Flight via Reinforcement Learning", "author": ["Ng", "Andrew Y", "Coates", "Adam", "Diel", "Mark", "Ganapathi", "Varun", "Schulte", "Jamie", "Tse", "Ben", "Berger", "Eric", "Liang"], "venue": "In Proceedings of the International Symposium on Experimental Robotics (ISER),", "citeRegEx": "Ng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2004}, {"title": "Intrinsic Motivation Systems for Autonomous Mental Development", "author": ["P.Y. Oudeyer", "F. Kaplan", "Hafner", "VV"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Oudeyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning", "author": ["Pickett", "Marc", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Pickett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pickett et al\\.", "year": 2002}, {"title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010)", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2010}, {"title": "Intrinsically Motivated Reinforcement Learning", "author": ["Singh", "Satinder P", "Barto", "Andrew G", "Chentanez", "Nuttapong"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": null, "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Temporal Difference Learning and TDGammon", "author": ["Tesauro", "Gerald"], "venue": "Comm. of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Finding Structure in Reinforcement Learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Thrun et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 14, "context": "Reinforcement learning (RL) has been successful in generating agents capable of intelligent behaviour in initially unknown environments; with accomplishments such as surpassing human-level performance in Backgammon (Tesauro, 1995), helicopter flight (Ng et al., 2004), and general competency in dozens of Atari 2600 games (Mnih et al.", "startOffset": 250, "endOffset": 267}, {"referenceID": 1, "context": "This effect is particularly pronounced when the agent operates at a fine time scale, as is common of video game platforms (Bellemare et al., 2013).", "startOffset": 122, "endOffset": 146}, {"referenceID": 18, "context": "Intrinsic motivation-based approaches (Singh et al., 2004; Oudeyer et al., 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al.", "startOffset": 38, "endOffset": 93}, {"referenceID": 15, "context": "Intrinsic motivation-based approaches (Singh et al., 2004; Oudeyer et al., 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al.", "startOffset": 38, "endOffset": 93}, {"referenceID": 18, "context": ", 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al., 2004).", "startOffset": 295, "endOffset": 315}, {"referenceID": 20, "context": "To construct purposeful agents, we appeal to the options framework (Sutton et al., 1999).", "startOffset": 67, "endOffset": 88}, {"referenceID": 20, "context": "Nevertheless, it is convenient to have agents encoding higher levels of abstraction, which also facilitate the learning process if properly defined (Dietterich, 1998; Sutton et al., 1999).", "startOffset": 148, "endOffset": 187}, {"referenceID": 7, "context": "The potential of options to dramatically affect learning by improving exploration is well known (e.g., McGovern & Sutton, 1998; McGovern & Barto, 2001; Kulkarni et al., 2016).", "startOffset": 96, "endOffset": 174}, {"referenceID": 18, "context": ", salient events (Singh et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 9, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 4, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 1, "context": "As future work, we plan to investigate how this algorithm behaves in more complex environments, such as the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 136, "endOffset": 160}], "year": 2016, "abstractText": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \u201cjust out of reach\u201d of the agents current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.", "creator": "LaTeX with hyperref package"}}}