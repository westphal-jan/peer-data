{"id": "1605.07785", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Geometry-aware stationary subspace analysis", "abstract": "In many real-world applications data exhibits non-stationarity, i.e., its distribution changes over time. One approach to handling non-stationarity is to remove or minimize it before attempting to analyze the data. In the context of brain computer interface (BCI) data analysis this may be done by means of stationary subspace analysis (SSA). The classic SSA method finds a matrix that projects the data onto a stationary subspace by optimizing a cost function based on a matrix divergence. In this work we present an alternative method for SSA based on a symmetrized version of this matrix divergence. We show that this frames the problem in terms of distances between symmetric positive definite (SPD) matrices, suggesting a geometric interpretation of the problem. Stemming from this geometric viewpoint, we introduce and analyze a method which utilizes the geometry of the SPD matrix manifold and the invariance properties of its metrics. Most notably we show that these invariances alleviate the need to whiten the input matrices, a common step in many SSA methods which often introduces errors. We demonstrate the usefulness of our technique in experiments on both synthesized and real-world data.", "histories": [["v1", "Wed, 25 May 2016 09:11:23 GMT  (97kb,D)", "http://arxiv.org/abs/1605.07785v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["inbal horev", "florian yger", "masashi sugiyama"], "accepted": false, "id": "1605.07785"}, "pdf": {"name": "1605.07785.pdf", "metadata": {"source": "CRF", "title": "Geometry-aware Stationary Subspace Analysis", "authors": ["Inbal Horev", "Florian Yger", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution that is capable, that we need to find a solution that puts us in a position, that we are able to find a solution that enables us to find a solution, that enables us to find a solution, that enables us to find a solution that enables us to find a solution, that enables us to find a solution that enables us to find a solution, and that enables us to find a solution that enables us to find a solution."}, {"heading": "2 Geometry-aware stationary subspace analysis", "text": "As discussed in the introduction, the task of extracting the stationary part from an observed mixture of stationary and non-stationary signals is essential in various applications. In this section, we present our approach to this problem. We call it geometry conscious SSA (gaSSA) because we use the geometric properties of the covariance matrices. To find a stationary subspace, SSA uses a cost function based on a matrix divergence. As a first step, we propose a formulation that uses a symmetrized version of the same matrix divergence. Subsequently, we show that this symmetrized matrix divergence is a distance between SPD matrices and offer a geometrical interpretation of the problem of SSA. We begin with a formal explanation of the problem. To this end, we present a review of the original SSA model and framework [30]. Next, we present the symmetrized matrix divergence and discuss its relationship to the belt geometry of SPD matrices."}, {"heading": "2.1 Stationary subspace analysis", "text": "Allow x (t) the SSA models to make relatively few assumptions about the torrential and n-sources. First, the SSA models are only available to a limited extent. (t) The SSA models are only available to a limited extent. (s) The SSA models are not able to identify the SLD sources. (s) The SSA models are only available to a limited extent. (s) The SSA models are only available to a limited extent. (s) The SSA models are not able to identify the SLD sources. (s) The SSA models are only available to a limited extent. (s) The SSA models are only available to a limited extent. (s) The SSA models of the SSA models are referred to as s-space and n-space. (s) The SSA model makes relatively few assumptions about the SSA sources. (s) The SSA models are only available to a limited extent."}, {"heading": "2.2 Symmetrized matrix divergence", "text": "SSA and its variants use in their cost function the KL divergence between Gaussian distributions = Y = = BLY distributions. Furthermore, we assume that these distributions have a zero meaning. Under this assumption, the KL divergence is a Bregman matrix divergence [4] between covariance matrices. (5) The family of Bregman matrix divergences is generally defined as asD\u03a6 (X, Y) divergence, hence it is often referred to as log-determinant divergence [18]. Bregman matrix divergences are useful in machine learning and have a number of useful properties [18], such as linearity and convexity in the first argument (and, in the case of KL divergence, also in the second)."}, {"heading": "2.3 A geometric interpretation", "text": "By replacing the cost function with one based on symmetrical divergence, we gain not only the advantageous properties of the Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Divergence, but also new insights into the problem of SSA. First, let us note that in Eq. (7) the problem is ultimately framed in terms of the distances between the SPD matrices. This indicates that a geometrical perspective is assumed, whereby the notion of stationary is captured by the scattering of the matrices [i]. In this view, the assumption that the covariance matrices of stationary signals between the epochs do not vary much vary, is caused by the fact that they have small distances between them. An illustration of this idea is shown in fig. 1. In this illustration, the matrices are seen as points on the SPD matrix matrix mannix S + D. The aim of our method is Q-Q Q Q Q-Q Q Q-Q Q Q-Q Q-Q-Q Q-Q Q-Q Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "2.4 A generic geometric formulation", "text": "In order to understand why it might be advantageous to use SPD matrices of the size n + n, one has to get used to a Euclidean matrix surface in this case, the similarity between SPD matrices can be measured simply by using the Euclidean distance derived from the Euclidean standard."}, {"heading": "2.5 Symmetries and invariance properties", "text": "We will now discuss the symmetries of our optimization problem and the invariance properties of our chosen metrics. These properties will allow us to make 2 other metrics such as the Euclidean metric or the log-Euclidean metric [3] significant. Let us simplify our problem. We will shortly give the results in relation to the AIRM, but the same is true for the log-determinant metric [7]. However, since our discussion is limited to real matrices, we have 2r (X, Y) = congruent transformations of the form X 7 \u2192 PHXP for the P-GLD (C) and PH = P > conjugated transpose [7]. Since our discussion is limited to real matrices, we have 2r (X, Y) = 2 r (P > XP > Y matrix)."}, {"heading": "Q\u0302 = argmin", "text": "The final transition is based on the observation that the solution to our optimization problem is not unique. Rather, because we are interested in restoring the stationary sub-space and not the exact sources themselves, the solution to each transformation is invariant (e.g. the scaling and rotation of sub-space). Indeed, this is true. The final transition is based on the observation that the solution to our optimization problem is not unique. Rather, since we are interested in restoring the stationary sub-space and not the exact sources themselves, the solution to each transformation is invariant (e.g. the scaling and rotation of sub-space). The final transition is based on the observation that matter is not unique."}, {"heading": "3 Experimental results", "text": "In this section we present experimental results on synthetic and data from a real BCI experiment. We compare the performance of gaSSA with the existing SSA and examine the effects of matrix lightening and the choice of metric."}, {"heading": "3.1 Toy data", "text": "For our first experiment, we generated data according to the SSA model as a mixture of stationary and non-stationary sources (\u00b1 0.00T = 0.00T). In order to generate non-stationary data, we used a slightly modified version of the scheme provided in the SSA toolbox [22] and in detail in its user manual. Here, we provide only a brief description: The elements of the mixing matrix A are uniformly standardized from the range [\u2212 0.5, 0.5] and their columns are normalized to 1. the distribution of s sources is constant across all epochs, namely ss (t), N (0, 0, s). In the SSA toolbox, we are used as an identity matrix, but we choose a random matrix of form, s = B > for an orthogonal matrix B > for an orthogonal matrix B and diagonal matrix."}, {"heading": "3.2 Brain-computer interface", "text": "Next, we applied our method to data from the BCI Competition IV dataset II. This dataset contains motor imaging (MI) EEG signals influenced by eye movement artifacts. It was collected in a multi-class environment, with subjects performing more than 2 different MI tasks. However, as in Lotte and Guan [21], we evaluate our algorithms for two-class problems by selecting only signals from the left- and right-wing MI trials. We applied the same pre-processing as described in Lotte and Guan [21]. EEG signals were filtered at 8 \u2212 30 Hz using a 5th order Butterworth filter. For each study, we extracted features from the time segment, which ranges from 0.5 s to 2.5 s after the keyword instructed by the subject, to MI.The data was initially divided into two parts: a training dataset and a test dataset similar to the 20% set set set of adjustment for the first two sets."}, {"heading": "4 Conclusion", "text": "We presented a covariance-based method for unattended stationary sub-space analysis. The problem was formulated in relation to the distance between matrices and not, as in SSA, the divergence between probability distributions. Based on the symmetries of the problem and the invariance properties of the geometries, we derived useful equivalence relationships. Experiments with both synthetic and BCI data supported our theoretical analysis and showed that our method exceeds SSA."}], "references": [{"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematica,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Multiclass brain\u2013computer interface classification by riemannian geometry", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Classification of covariance matrices using a riemannian-based kernel for bci applications", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Positive Definite Matrices", "author": ["R. Bhatia"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Manopt, a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Riemannian sparse coding for positive definite matrices", "author": ["A. Cherian", "S. Sra"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Efficient similarity search for covariance matrices via the Jensen-Bregman log-det divergence", "author": ["A. Cherian", "S. Sra", "A. Banerjee", "N. Papanikolopoulos"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Speech enhancement for non-stationary noise environments", "author": ["I. Cohen", "B. Berdugo"], "venue": "Signal processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Principal geodesic analysis for the study of nonlinear statistics of shape", "author": ["T.P. Fletcher", "C. Lu", "S.M. Pizer", "S. Joshi"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "From manifold to manifold: geometryaware dimensionality reduction for spd matrices", "author": ["M. Harandi", "M. Salzmann", "R. Hartley"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Intrinsic PCA for SPD matrices", "author": ["I. Horev", "F. Yger", "M. Sugiyama"], "venue": "In Asian Conference on Machine Learning (ACML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Independent Component Analysis, volume 46", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Nonlinear Time Series Analysis, volume 7", "author": ["H. Kantz", "T. Schreiber"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Low-rank kernel learning with Bregman matrix divergences", "author": ["B. Kulis", "M.A. Sustik", "I.S. Dhillon"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Stochastic covariance compression", "author": ["M.J. Kusner", "N.I. Kolkin", "S. Tyree", "K.Q. Weinberger"], "venue": "arXiv preprint arXiv:1412.1740,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A well-conditioned estimator for large-dimensional covariance matrices", "author": ["O. Ledoit", "M. Wolf"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Regularizing common spatial patterns to improve bci designs: unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "The stationary subspace analysis toolbox", "author": ["J.S. M\u00fcller", "P. v. B\u00fcnau", "F.C. Meinecke", "F.J. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Sided and symmetrized Bregman centroids", "author": ["F. Nielsen", "R. Nock"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A Riemannian framework for tensor computing", "author": ["X. Pennec", "P. Fillard", "N. Ayache"], "venue": "International Journal of Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Brain-computer interfacing in discriminative and stationary subspaces", "author": ["W. Samek", "K.-R. M\u00fcller", "M. Kawanabe", "C. Vidaurre"], "venue": "In 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Positive definite matrices and the s-divergence", "author": ["S. Sra"], "venue": "arXiv preprint arXiv:1110.1773,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Machine Learning in Non-Stationary Environments", "author": ["M. Sugiyama", "M. Kawanabe"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Region covariance: A fast descriptor for detection and classification", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Finding stationary subspaces in multivariate time series", "author": ["P. von B\u00fcnau", "F.C. Meinecke", "F.C. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "Physical Review Letters,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Stationary subspace analysis", "author": ["P. von B\u00fcnau", "F.C. Meinecke", "K.-R. M\u00fcller"], "venue": "In Independent Component Analysis and Signal Separation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Stationary common spatial patterns: towards robust classification of non-stationary eeg signals", "author": ["W. Wojcikiewicz", "C. Vidaurre", "M. Kawanabe"], "venue": "In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "When the stationarity assumption is violated, as is often the case in real-world applications such as speech enhancement [11] or neurological data analysis [25], specialized machine learning methods must be developed in order to maintain adequate prediction capabilities.", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "When the stationarity assumption is violated, as is often the case in real-world applications such as speech enhancement [11] or neurological data analysis [25], specialized machine learning methods must be developed in order to maintain adequate prediction capabilities.", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": "A relatively well-studied non-stationary setting is covariate-shift [26], in which the input distribution changes but the conditional distribution of the outputs does not.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "The problem of covariate-shift has received growing attention in recent years, and many theoretical and practical aspects have been addressed (see [28] for an in-depth exploration of this topic).", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "In the context of brain computer interface (BCI) data analysis, two such note-worthy methods are stationary subspace analysis (SSA) [30] and stationary common spacial patterns (sCSP) [32].", "startOffset": 132, "endOffset": 136}, {"referenceID": 31, "context": "In the context of brain computer interface (BCI) data analysis, two such note-worthy methods are stationary subspace analysis (SSA) [30] and stationary common spacial patterns (sCSP) [32].", "startOffset": 183, "endOffset": 187}, {"referenceID": 15, "context": "Similar in spirit to independent component analysis (ICA) [16], SSA statistically models the data as a mixture of stationary and non-stationary signals.", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "Although SSA is essentially an unsupervised method, variations of it exist which are useful for supervised tasks such as classification [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 193, "endOffset": 197}, {"referenceID": 23, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 213, "endOffset": 217}, {"referenceID": 5, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 240, "endOffset": 243}, {"referenceID": 6, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 256, "endOffset": 259}, {"referenceID": 18, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 277, "endOffset": 281}, {"referenceID": 12, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 311, "endOffset": 315}, {"referenceID": 29, "context": "a review of the original SSA model and framework [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "First, the s-sources are stationary only in the weak (or wide) sense [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "1This is also common practice in ICA [16]", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "Under this assumption the KL divergence is a Bregman matrix divergence [4] between covariance matrices.", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "DKL is obtained for \u03a6(X) = \u2212 log detX, so it is often called the log-determinant divergence [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "Bregman matrix divergences are useful in machine learning and have a number of useful properties [18], such as linearity and convexity in the first argument (and, in the case of the KL divergence, also in the second).", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "Subsequently, symmetrized versions of the Bregman matrix divergence, in particular Jensen-Bregman divergences, have been studied in recent years [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "and is called the Jensen-Bregman log-determinant (JBLD) divergence [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "The JBLD has many favorable properties (see [27]), primarily that its square root comprises a metric on the SPD matrix manifold.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "Moreover, in Sra [27] it has been shown that it is a close approximation to the affine invariant Riemannian metric (AIRM) [7] and shares many of its mathematical properties.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Moreover, in Sra [27] it has been shown that it is a close approximation to the affine invariant Riemannian metric (AIRM) [7] and shares many of its mathematical properties.", "startOffset": 122, "endOffset": 125}, {"referenceID": 26, "context": "In the context of SPD matrices, the JBLD is referred to as the symmetric Stein divergence or the (square of the) log-determinant metric [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "So, we may optimize Q over the Grassmann manifold [12], G = { span(Q) : Q \u2208 RD\u00d7m, Q>Q = I } , the set of all m-dimensional linear subspaces of RD\u00d7D.", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "[2] and implemented efficiently in Boumal et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Given the strong relation between the log-determinant metric and the AIRM [7], a natural progression is to incorporate the AIRM into the cost function.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "Despite its simplicity, the Euclidean geometry has several drawbacks and is not always well suited for SPD matrices [13, 3].", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "Despite its simplicity, the Euclidean geometry has several drawbacks and is not always well suited for SPD matrices [13, 3].", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "artifact referred to as the swelling effect [3], for a task as simple as averaging two matrices, it may occur that the determinant of the average is larger than any of the two matrices.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "[13], is the fact that this geometry forms a non-complete space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Of the possible Riemannian distances, the AIRM, due to its favorable mathematical properties, is widely used in many applications (see, for example [13, 24]).", "startOffset": 148, "endOffset": 156}, {"referenceID": 23, "context": "Of the possible Riemannian distances, the AIRM, due to its favorable mathematical properties, is widely used in many applications (see, for example [13, 24]).", "startOffset": 148, "endOffset": 156}, {"referenceID": 13, "context": "[14] and can be considered an unsupervised version of it.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Q of the cost function, used for the optimization, can be found in [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "2Other metrics such as the Euclidean metric or the log-Euclidean metric [3] may also be used.", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Our key observation stems from the fact that \u03b4r and \u03b4s are invariant to congruent transformations of the form X 7\u2192 PXP for P \u2208 GLD(C) and P = P > is the conjugate transpose [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 21, "context": "To generate non-stationarity in the data we used a slightly modified version of the scheme provided in the SSA toolbox [22] and detailed in its user manual.", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "[31], the n-space and s-sources are identifiable, while the s-space and n-sources are not.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The distance between sub-spaces is computed using \u03b4G , the metric on the Grassmann manifold [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "However, as in Lotte and Guan [21], we evaluate our algorithms on two-class problems by selecting only signals of left- and right-hand MI trials.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "We applied the same pre-processing as described in Lotte and Guan [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "[31] the first 20% of the test trials were set aside for adaptation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5]: Using the labels of the training set, we compute the mean (in the s-space) for each of the two classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Since the true number of stationary signals is unknown, we repeated the experiment for several values in the range m \u2208 [10, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Since the true number of stationary signals is unknown, we repeated the experiment for several values in the range m \u2208 [10, 18].", "startOffset": 119, "endOffset": 127}], "year": 2016, "abstractText": "In many real-world applications observed data exhibits non-stationarity, i.e., its distribution changes over time. One approach to handling nonstationarity is to remove or minimize it before attempting to analyze the data. In the context of brain computer interface (BCI) data analysis this may be done by means of stationary subspace analysis (SSA). The SSA method finds a matrix that projects the data onto a stationary subspace by optimizing a cost function based on a matrix divergence. In this work we present an alternative method for SSA based on a symmetrized version of this matrix divergence. We show that doing so frames the problem in terms of distances between symmetric positive definite (SPD) matrices, suggesting a geometric interpretation of the problem. Stemming from this geometric viewpoint, we introduce and analyze a method which utilizes the geometry of the SPD matrix manifold and the invariance properties of its metrics. We demonstrate the usefulness of our method in experiments on both synthesized and real-world data.", "creator": "LaTeX with hyperref package"}}}