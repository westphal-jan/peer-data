{"id": "1605.07805", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper we study this problem for finite state machines with inputs and outputs, and in particular for Moore machines. We introduce three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes that machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI always learns the right machine when the training set is a characteristic sample, which is generally not true for the other two algorithms. We also compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers, and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.", "histories": [["v1", "Wed, 25 May 2016 10:11:03 GMT  (36kb,D)", "http://arxiv.org/abs/1605.07805v1", null], ["v2", "Fri, 2 Sep 2016 09:27:40 GMT  (45kb,D)", "http://arxiv.org/abs/1605.07805v2", null]], "reviews": [], "SUBJECTS": "cs.FL cs.LG", "authors": ["georgios giantamidis", "stavros tripakis"], "accepted": false, "id": "1605.07805"}, "pdf": {"name": "1605.07805.pdf", "metadata": {"source": "CRF", "title": "Learning Moore Machines from Input-Output Traces", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A wealth of data from the Internet and other sources (e.g. sensors) is always revolutionizing many sectors of science, technology, and ultimately of our society. At the heart of this revolution lies machine learning, a broad range of techniques for deriving information from data. The objects studied by machine learning include classified, decision-making trees, and neural networks, with applications as diverse as artificial intelligence, marketing, finance, or medicine [19]. In the context of system design, an important problem is that with numerous applications, it automatically generates models from data. There are many variants of this problem, depending on what types of models and data are taken into account, as well as other assumptions or limitations. Examples include, but are limited to the classic field of system identification, as more recent work on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3]. In this paper, we look at a basic learning that a Moore machine learns from a set of input machines."}, {"heading": "2 Related Work", "text": "A pioneering work in the first category is Anglin's work on learning DFAs with membership and equivalence issues [4], which was subsequently extended to other types of machines, such as (extended) Mealy machines or register machines [24,14,13,6]. This work does not directly apply to the problem examined in this paper, as we explicitly prohibit both membership and equivalence queries. A pioneering work in the second category is the algorithm for learning DFAs from positive and negative examples [10]."}, {"heading": "3 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Finite state machines and automata", "text": "A finite state machine (FSM) is a tuple M of the form M = (I, O, Q = q) Q = Q = q (FA = q) Q = q (FA = q), where: - I is a finite series of input symbols. - O is a finite series of output symbols. - Q is a finite series of states. - Q0 \"Q\" Q is the initial state. - Z \"I\" Q is the transition function. - Z \"Q\" is the output function, which can be of two types: \u2022 Z \"O,\" in which the FSM is a complete function. - Z \"I\" O, \"in which the FSM is a mealy machine. If both functions are, let's say that the FSM is a complete function and that the QM is a partial function, let's say that the FSM is incomplete. Examples of a Moore machine and a Mealy machine are in Figure 1. Both FSMs are completed."}, {"heading": "3.2 Input-output traces and examples", "text": "Given a set of input and output symbols I and O, a Moore (I, O) trace is a pair of finite sequences (x1x2 \u00b7 \u00b7 xn, y0y1 \u00b7 \u00b7 \u00b7 yn), for a natural number n \u2265 0, so that the output sequence has one length more than the input sequence. Note that n can be a positive example, where the input sequence is empty (i.e., length 0), and the output sequence contains only one output symbol, so that the output sequence has one length more than the input sequence. Note that n can be a positive example, where the input sequence is empty (i.e., the output sequence contains only one output symbol)."}, {"heading": "3.3 Prefix tree acceptors and prefix tree acceptor products", "text": "Faced with a finite and non-empty set of positive examples over a given alphabet \u03a3, S + \u03a3 \u0445, we can construct in an ambiguous manner a treelike, incomplete DFA that accepts all the words in S + and rejects all the others. Such a DFA is called a prefix tree acceptor [7] (PTA) for S +. A PTA for S + = {b, aa, ab} is shown in Figure 3.We expand the concept of PTA to Moore machines. Suppose we have a set of moore (I, O) examples. Suppose N = dlog2 | O | e is the number of bits necessary to represent an element of O. Then, assuming a function f that maps elements of O to bit tuples of length N, we can map SIO to N pairs of positive and negative sample sets {(S1 +, S1 \u2212), (S2, SAP \u2212), SN \u00b7, (SN), (pair \u00b7 \u00b7 i), (SN)."}, {"heading": "4 Characteristic samples", "text": "An important concept in the theory of automatic learning is that of a characteristic sample [7]. A characteristic sample for a DFA is a set of words that capture all information about the states and behavior of this automaton. In this essay, we expand the concept of the characteristic sample to include Moore machines."}, {"heading": "4.1 Characteristic samples for Moore machines", "text": "Let us next define M = (I, Q, Q) as a minimum Moorean machine (SP). Let us call it a total order on input words (i.e., I, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II,"}, {"heading": "4.2 Computation, minimality, size, and other properties of characteristic samples", "text": "It is easy to see that adding more traces to a characteristic sample maintains the characteristic sample property, i.e. if SIO is a characteristic sample for a Moore machine M and S'IO, then S'IO is also a characteristic sample for M. The questions are then raised whether there are characteristic samples that are in a sense minimal, how many elements they comprise, what are the lengths of their elements, and how can we construct them. In the following, we sketch a simple procedure that, in the face of a minimal Moore machine M, returns a characteristic sample that is minimal in the sense that we remove any (I, O) ce from it or remove any number of letters at the end of an input word in it."}, {"heading": "5 Learning Moore machines from Input-Output Traces", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Problem definition", "text": "The problem of learning Moore machines from input-output traces (LMoMIO) is defined as follows: \"Considering an input alphabet I, an output alphabet O, and a set of Rtrain Moore (I, O) traces, called the training set, we want to automatically identify a deterministic, complete Moore machine M = (I, O, Q, q0, 3), so that M is consistent with consistency, i.e., we want to evaluate our learning technique. (I) = 1, complete Moore machine M = (I, O, q0, 3, 3) is assumed to be consistent itself, in the sense that it does not contain two different pairs with the same input word.) In addition to consistency, we want to evaluate various performance criteria, including: - Size of M, in terms of the number of states. Ideally, M, which is if M is another machine that is M., which is also consistent with no Rtrain, then we want to evaluate various performance criteria, including: - Size of M, in terms of the number of states."}, {"heading": "5.2 Algorithms to solve the LMoMIO problem", "text": "In fact, the fact is that most of them are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5.3 Properties of the algorithms", "text": "All three algorithms described above satisfy the consistency w.r.t. The input training set is replaced by the number of learned (PTAP and PRPNI). (This is a direct consequence of the properties of the PTAs, the basic RPNI algorithm, and the synchronous product.) The evidence for MooreMI may be more involved: Theorem 1. The output of the MooreMI algorithm is a complete Moore machine that is consistent with the training set. Formally, let SIO be the set of Moore (I, O) tracks used as input for the algorithm, and let M = (I, O, Q, q0, \u03b4, \u03bb) be the resulting Moore machine. Then the total functions and the executed SIO: (q0, O) tracks used as input for the algorithm."}, {"heading": "5.4 Performance optimizations", "text": "Compared to the pseudo-code, our implementation includes several optimizations. Firstly, to limit the copying effort involved in performing a merge operation, we perform the required state merges on the spot, while recording the actions required to undo them if the merge is not accepted. Secondly, the merge function must know the unique (due to the tree-like nature of the PTA) parent state of the blue state passed to it as an argument. Secondly, the naive approach of simply iterating over the states until we reach the parent state can seriously impair performance. Instead, in our implementation during the PTA construction and throughout the algorithm, we build a state mapping to their parents and consult them when needed. Thirdly, in the negative examples consistency tests, many of the acceptance checks involved are redundant. For example, suppose that starting from the initial state, it is only possible to achieve unmarked states (i.e., not blue states)."}, {"heading": "5.5 Complexity analysis", "text": "Let I and O be the input and output alphabets, and let SIO be the set of Moore (I, O) tracks provided as input for the learning algorithms. Let N = dlog2 (| O |) e be the number of bits required to encode the symbols in O. Let S1 +, S1 \u2212,..., SN \u2212 be the positive and negative sample sentences obtained by the preprocess step at the beginning of each algorithm. Let m + = \u2211 N i = 1 \u2211 w + |, m \u2212 = \u2211 N i = 1 \u0445 Si \u2212 | w |, and k = \u2211 (\u03c1I, \u03c1O) \u0644SIO | \u03c1I | 2. The time required for the preprocessing step O (N \u00b7 k), which is the same for all three algorithms. The time required for the rest of the phases of each algorithm - O (N \u2212 I | Total I + + \u00b7 AP + \u00b7 + \u00b7 N) (PRO \u00b7 N) and tzm (tzm)."}, {"heading": "6 Implementation & Experiments", "text": "All three algorithms presented in paragraph 5.2 were implemented in Python. Source code, including random Moore machine and characteristic sample generation, learning algorithms and tests, includes approximately 2,000 lines of code. Code and experiments are available upon request and will be made available online in a future version of this paper."}, {"heading": "6.1 Experimental comparison", "text": "From each of these machines, we generated a characteristic sample and ran each of the three algorithms on that characteristic sample. Then, we took the learned machines generated by the algorithms, and evaluated these machines in terms of size (# states) and accuracy, using a test set twice the size of the training set for accuracy, and the length of the words in the test set was twice the maximum word length, the results of which are shown in Tables 1 and 2. \"Algo 1,2,3\" refers to PTAP, PRPNI and MooreMI, respectively, and \"Time\" refers to the execution time of the learning algorithm (in seconds). \"States\" refers to the number of states of the learned machines. \"For accuracy, we used the three AEPs, Strong, Medium and Weak, defined in \u00a7 5.1."}, {"heading": "6.2 Comparison with OSTIA", "text": "OSTIA [21] is a well-known algorithm that learns successively converters, a class of more general converters than Moore and Mealy machines. Then, of course, the question arises whether it is possible to use OSTIA to learn Moore machines. In particular, we want to know what happens when entering OSTIA is a series of Moore (I, O) tracks: Will OSTIA learn a Moore machine? The answer here is negative, as an experiment we conducted shows. We constructed a characteristic example of the Moore machine in Figure 5a and then used the OSTIA algorithm (we used the open source implementation described in [1]. The resulting machine is shown in Figure 6. Note that there are transitions whose corresponding results are words with a length of more than 1 (e.g. transition label b / 0122), or even the empty word (output of the source state q0)."}, {"heading": "7 Conclusion & Future Work", "text": "We presented three algorithms for learning Moore machines using input-output traces. MooreMI, the most advanced of the three algorithms, has desirable performance characteristics both in theory (it meets the characteristic sample requirements) and in practice. Future work will include: (1) learning for Mealy and other types of state machines; (2) developing incremental versions of the learning algorithms presented here; (3) further implementation and experimentation; and (4) applying the methods presented here to learn models of different black box systems."}], "references": [{"title": "C", "author": ["H.I. Akram"], "venue": "de la Higuera, H. Xiao, and C. Eckert. Grammatical inference algorithms in matlab. In ICGI\u201910, Proceedings, pages 262\u2013266. Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The use of evolutionary programming based on training examples for the generation of finite state machines for controlling objects with complex behavior", "author": ["A.V. Aleksandrov", "S.V. Kazakov", "A.A. Sergushichev", "F.N. Tsarev", "A.A. Shalyto"], "venue": "J. Comput. Sys. Sc. Int., 52(3):410\u2013425,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Synthesizing Finite-state Protocols from Scenarios and Requirements", "author": ["R. Alur", "M. Martin", "M. Raghothaman", "C. Stergiou", "S. Tripakis", "A. Udupa"], "venue": "In HVC, volume 8855 of LNCS. Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Inf. Comput., 75(2):87\u2013106,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1987}, {"title": "Inducing finite state machines from training samples using ant colony optimization", "author": ["I.P. Buzhinsky", "V.I. Ulyantsev", "D.S. Chivilikhin", "A.A. Shalyto"], "venue": "J. Comput. Sys. Sc. Int., 53(2):256\u2013266,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning extended finite state machines", "author": ["S. Cassel", "F. Howar", "B. Jonsson", "B. Steffen"], "venue": "In SEFM 2014, Proceedings, pages 250\u2013264,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammatical Inference: Learning Automata and Grammars", "author": ["C. de la Higuera"], "venue": "CUP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Incremental regular inference", "author": ["P. Dupont"], "venue": "In ICGI-96, Proceedings, pages 222\u2013237,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control, 10(5):447\u2013474,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1967}, {"title": "Complexity of automaton identification from given data", "author": ["E.M. Gold"], "venue": "Information and Control, 37(3):302\u2013320,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1978}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["S. Gulwani"], "venue": "In 38th POPL, pages 317\u2013330,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Exact DFA identification using SAT solvers", "author": ["M.J.H. Heule", "S. Verwer"], "venue": "In J. M. Sempere and P. Gar\u0107\u0131a, editors, ICGI, pages 66\u201379. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Inferring canonical register automata", "author": ["F. Howar", "B. Steffen", "B. Jonsson", "S. Cassel"], "venue": "In VMCAI 2012, Proceedings, pages 251\u2013266,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning of automata models extended with data", "author": ["B. Jonsson"], "venue": "In SFM 2011, Advanced Lectures, pages 327\u2013349,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "ABCD-NL: Approximating continuous non-linear dynamical systems using purely boolean models for analog/mixed-signal verification", "author": ["A.V. Karthik", "S. Ray", "P. Nuzzo", "A. Mishchenko", "R. Brayton", "J. Roychowdhury"], "venue": "In ASP-DAC, pages 250\u2013255,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Switching and finite automata theory, 2nd ed", "author": ["Z. Kohavi"], "venue": "McGraw-Hill,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1978}, {"title": "Results of the abbadingo one DFA learning competition and a new evidence-driven state merging algorithm", "author": ["K.J. Lang", "B.A. Pearlmutter", "R.A. Price"], "venue": "In ICGI-98, Proceedings, pages 1\u201312,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "editor", "author": ["L. Ljung"], "venue": "System Identification (2nd Ed.): Theory for the User. Prentice Hall,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine Learning", "author": ["T.M. Mitchell"], "venue": "McGraw-Hill,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Identifying regular languages in polynomial time", "author": ["J. Oncina", "P. Garcia"], "venue": "In Advances in Structural and Syntactic Pattern Recognition, pages 99\u2013108,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning subsequential transducers for pattern recognition interpretation tasks", "author": ["J. Oncina", "P. Gar\u0107\u0131a", "E. Vidal"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 15(5):448\u2013458,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "A large scale study of programming languages and code quality in github", "author": ["B. Ray", "D. Posnett", "V. Filkov", "P. Devanbu"], "venue": "In ACM SIGSOFT, FSE \u201914,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Sciduction: Combining induction, deduction, and structure for verification and synthesis", "author": ["S.A. Seshia"], "venue": "In DAC, pages 356\u2013365, June", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Inferring mealy machines", "author": ["M. Shahbaz", "R. Groz"], "venue": "In FM 2009, pages 207\u2013222,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Program sketching", "author": ["A. Solar-Lezama"], "venue": "STTT, 15(5-6):475\u2013495,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A polynomial time algorithm to infer sequential machines", "author": ["K. Takahashi", "A. Fujiyoshi", "T. Kasai"], "venue": "Systems and Computers in Japan, 34(1):59\u201367,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Extended finite-state machine induction using SAT-solver", "author": ["V. Ulyantsev", "F. Tsarev"], "venue": "In ICMLA 2011, Volume 2: Special Sessions and Workshop, pages 346\u2013349,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 18, "context": "Traditionally, objects studied by machine learning include classifiers, decision trees, and neural networks, with applications to fields as diverse as artificial intelligence, marketing, finance, or medicine [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 200, "endOffset": 215}, {"referenceID": 10, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 200, "endOffset": 215}, {"referenceID": 22, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 200, "endOffset": 215}, {"referenceID": 21, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 200, "endOffset": 215}, {"referenceID": 2, "context": "Examples include, but are by no means limited to, the classic field of system identification [18], as well as more recent works on synthesizing programs, controllers, or other artifacts from examples [25,11,23,22,3].", "startOffset": 200, "endOffset": 215}, {"referenceID": 15, "context": "A Moore machine is a type of finite-state machine (FSM) with inputs and outputs, where the output always depends on the current state, but not on the current input [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 6, "context": "This is despite a large body of research on grammatical inference [7] which has studied similar, but not exactly the same problems, such as learning deterministic finite automata (DFA), which are special cases of Moore machines with a binary output, or subsequential transducers, which are more general than Moore machines.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "We also adapt the notion of characteristic sample, which is known for DFA [7], to the case of Moore machines.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "CSR is important, as it ensures identification in the limit: this is a key concept in automata learning theory which ensures that the learning algorithm will eventually learn the right machine when provided with a sufficiently large set of examples [9].", "startOffset": 249, "endOffset": 252}, {"referenceID": 20, "context": "We show that the well-known transducer-learning algorithm OSTIA [21] cannot generally learn a Moore machine, even in the case where the training set is a characteristic sample of a Moore machine.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "A seminal work in the first category is Angluin\u2019s work on learning DFAs with membership and equivalence queries [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 23, "context": "This work has been subsequently extended to other types of machines, such as (extended) Mealy machines or register automata [24,14,13,6].", "startOffset": 124, "endOffset": 136}, {"referenceID": 13, "context": "This work has been subsequently extended to other types of machines, such as (extended) Mealy machines or register automata [24,14,13,6].", "startOffset": 124, "endOffset": 136}, {"referenceID": 12, "context": "This work has been subsequently extended to other types of machines, such as (extended) Mealy machines or register automata [24,14,13,6].", "startOffset": 124, "endOffset": 136}, {"referenceID": 5, "context": "This work has been subsequently extended to other types of machines, such as (extended) Mealy machines or register automata [24,14,13,6].", "startOffset": 124, "endOffset": 136}, {"referenceID": 9, "context": "A seminal work in the second category is Gold\u2019s algorithm for learning DFAs from sets of positive and negative examples [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "Subsequent work includes the RPNI [20] algorithm (that learns DFAs), of which an incremental version also exists [8], and derivatives, like the EDSM [17] (that also learns DFAs, but unlike RPNI does not guarantee identification in the limit) and OSTIA [21] (that learns subsequential transducers) algorithms.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "Subsequent work includes the RPNI [20] algorithm (that learns DFAs), of which an incremental version also exists [8], and derivatives, like the EDSM [17] (that also learns DFAs, but unlike RPNI does not guarantee identification in the limit) and OSTIA [21] (that learns subsequential transducers) algorithms.", "startOffset": 113, "endOffset": 116}, {"referenceID": 16, "context": "Subsequent work includes the RPNI [20] algorithm (that learns DFAs), of which an incremental version also exists [8], and derivatives, like the EDSM [17] (that also learns DFAs, but unlike RPNI does not guarantee identification in the limit) and OSTIA [21] (that learns subsequential transducers) algorithms.", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "Subsequent work includes the RPNI [20] algorithm (that learns DFAs), of which an incremental version also exists [8], and derivatives, like the EDSM [17] (that also learns DFAs, but unlike RPNI does not guarantee identification in the limit) and OSTIA [21] (that learns subsequential transducers) algorithms.", "startOffset": 252, "endOffset": 256}, {"referenceID": 1, "context": "Apart from state merging algorithms like the above, learning from examples has also been attempted with techniques from artificial intelligence, such as genetic algorithms [2] and ant colony optimization [5], as well as with reduction to satisfiability [12,27].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Apart from state merging algorithms like the above, learning from examples has also been attempted with techniques from artificial intelligence, such as genetic algorithms [2] and ant colony optimization [5], as well as with reduction to satisfiability [12,27].", "startOffset": 204, "endOffset": 207}, {"referenceID": 11, "context": "Apart from state merging algorithms like the above, learning from examples has also been attempted with techniques from artificial intelligence, such as genetic algorithms [2] and ant colony optimization [5], as well as with reduction to satisfiability [12,27].", "startOffset": 253, "endOffset": 260}, {"referenceID": 26, "context": "Apart from state merging algorithms like the above, learning from examples has also been attempted with techniques from artificial intelligence, such as genetic algorithms [2] and ant colony optimization [5], as well as with reduction to satisfiability [12,27].", "startOffset": 253, "endOffset": 260}, {"referenceID": 25, "context": "[26] is close to our work, but the algorithm described there does not always yield a deterministic Moore machine, while ours does.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Related to ours is also the work in [15], which deals with learning finite state machine abstractions of non-linear analog circuits.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "The algorithm described in [15] is very different from ours, however, and uses the circuit\u2019s number of inputs to determine a subset of the states in the learned abstraction.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "Also, identification in the limit is not considered in [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Such a DFA is called a prefix tree acceptor [7] (PTA) for S+.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "An important concept in automata learning theory is that of a characteristic sample [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "We call an accuracy evaluation policy (AEP) any function that, given a Moore (I,O)-trace (\u03c1I , \u03c1O) and a Moore machine M = (I,O,Q, q0, \u03b4, \u03bb), will return a real number in [0, 1].", "startOffset": 171, "endOffset": 177}, {"referenceID": 8, "context": "CSR is important, as it ensures identification in the limit, a key concept in automata learning theory [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 19, "context": "The PRPNI algorithm starts by executing the RPNI DFA learning algorithm [20] on each pair, thus obtaining N learned DFAs.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "While not appearing in the original RPNI algorithm, the convention of marking states as red or blue was introduced later in [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "OSTIA [21] is a well-known algorithm that learns onward subsequential transducers, a class of transducers more general than Moore and Mealy machines.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "We constructed a characteristic sample for the Moore machine in Figure 5a and ran the OSTIA algorithm on it (we used the open source implementation described in [1]).", "startOffset": 161, "endOffset": 164}], "year": 2017, "abstractText": "The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper we study this problem for finite state machines with inputs and outputs, and in particular for Moore machines. We introduce three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of inputoutput traces into an incomplete Moore machine and then completes that machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI always learns the right machine when the training set is a characteristic sample, which is generally not true for the other two algorithms. We also compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers, and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.", "creator": "LaTeX with hyperref package"}}}