{"id": "1605.07824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Action Classification via Concepts and Attributes", "abstract": "Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford-40 Actions dataset, where the proposed method outperforms current state-of-the art and, combined with direct visual features, obtains an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.", "histories": [["v1", "Wed, 25 May 2016 11:06:58 GMT  (831kb,D)", "http://arxiv.org/abs/1605.07824v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "shimon ullman"], "accepted": false, "id": "1605.07824"}, "pdf": {"name": "1605.07824.pdf", "metadata": {"source": "CRF", "title": "Action Classification via Concepts and Attributes", "authors": ["Amir Rosenfeld", "Shimon Ullman"], "emails": ["amir.rosenfeld@weizmann.ac.il", "shimon.ullman@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In many pattern recognition tasks, especially in the field of computer vision, target classes follow a long tail distribution. In the field of action recognition, this is particularly true, since the product of actions and objects is much larger than each one and some examples may not be observed at all. Furthermore, this has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to solve this problem by building larger and larger datasets [14, 21]. However, the distribution of these datasets will inevitably also be lengthy. One way to address this problem is to borrow information from external data sources. For example, it has become popular to combine language and vision by means of common embedded spaces [13, 17] which allows invisible classes to be detected more reliably than by means of a purely visual approach. In this work, we propose to use an annotated concept dataset to learn how to assign images to concepts with visual meaning (e.g. objects and properties)."}, {"heading": "2 Previous Work", "text": "An early related work is ObjectBank [12], where the results of detectors for 200 common objects are aggregated over a spatial pyramid to serve as a feature-ure29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.ar Xiv: 160 5.07 824v 1 [cs.C V] 25 May 2representations. In the same spirit, ActionBank [22] learns detectors for different types of action in videos and uses them to represent other than weighted combinations of actions. The work of [11] learns object attributes to describe objects in a zero-shot learning environment, so that new classes (animals) can be correctly classified by matching them with human-generated lists of attributes. Recently, [20] one has learned different types of relationships between actions (e.g., part of / type of / mutually exclusive images) via visual and linguistic terms that can be hierarchically classified."}, {"heading": "3 Approach", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) It is not so that people are able to decide whether they want it or not. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...). \"\" (.) \"(.).\" (.) \"(.).\" (.) \"(.)\" (.) \"(.\" (.). \"(.).\" (.) \"(.).\" (.). \"(.).\" (.). \"(.)\" (.). \"(.).\" (.) \"(.)\" (.). \"(.)\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.\"). \"(.).\" (.). \").\" (.). \"(.).\" (.). \"(.).).\" (. \"(.).).\" (. \"(.).).\" (.). \"(.).\" (. \"(.).\" (.).).). \"(.).\" (.). \"(.).\" (.).). \"(.).).\" (. \"(.\"). \"(.).\" (.). \"(.).\" (.).). \"(.).\"). \").\" (. \"(.).\" (.). \").).\" (. \").\" (. \").\" (.). \"(.).\"). \".\" (. \".\"). \").\". \").\" (. \".\"."}, {"heading": "3.2 Refinement via Language", "text": "The objective attributes themselves have very long distributions in the VG datasets and contain many redundancies. There are many overlapping concepts that cannot be regarded as mutually exclusive classes."}, {"heading": "4 Experiments", "text": "To confirm our approach, we tested it on the Standford-40 action dataset [25], which contains 9532 images with a diverse set of 40 different action classes, 4000 images for training and the rest for testing. In addition, we test our method on the recently introduced HICO [3] dataset of 47774 images and 600 (not all mutually exclusive) action classes. Following are some technical details, followed by experiments testing different aspects of the method.As a starting point for purely visual categorization, we train and test several baseline approaches using feature combinations from different sources: (1) The global average summary of the VGG-GAP network [26] (2) the output of the second-to-last fully connected layer (referred to as fc6) from VGG-16 [24] and (3) the Pool 5 functions from the penultimate layer of ResNet-151 [9]."}, {"heading": "4.1 Visual vs Concept features", "text": "For the concept characteristics, we form concept detectors for both the objects and the object characteristics of the VG dataset. Direct use of these in their raw form is unfeasible, as in Sec. 3.2. To reduce the number of classes, we normalize each object name beforehand. The object name can be either a single word, such as \"dog,\" or a phrase, such as \"baseball bat.\" To normalize the names, we remove stop words, such as \"she,\" is \"a,\" as well as punctuation marks. We turn plural words into their singular form, as we have found that these easily impede performance, the word \"building\" usually refers to the structure and has a different meaning when they \"build.\""}, {"heading": "4.2 Describing Actions using Concepts", "text": "For each target class, the learned classifier assigns a weight to each of the concepts. The study of this weight vector reveals the concepts considered most relevant by the classifier. Remember that the weight vector of each learned classifier for class j (L is the set of target classes in F) sorts the values of the learned weight vector perspecj in descending order and lists the concepts corresponding to the obtained order as follows. Table 1 shows ten randomly selected classes from the Stanford 40 actions [25] data sets, with the five most ranked object concepts sorted according to the respective weight vector perspecj in descending order. In most cases, the classes are semantically significant. In some classes, however, we see unexpected concepts, such as holding a _ umbrella."}, {"heading": "4.3 Concept Visualization", "text": "To test which characteristics contribute most to the score, we use the Class Activation Map (CAM) approach of [26]. This method allows us to visualize which image regions contributed most to the score of each class. We can do this for the version of our method that uses only the VGG-GAP characteristics, since the method relies on the network-specific structure to project the scores of the scores onto the image (see [26] for details). We visualize the average CAMs of the top 5 keywords for different classes (as in the above section). We do this for two target classes for each image, one correct class and the other incorrectly to explain which image regions drive the method to decide on the image class. See Figure 2. If the method is \"forced\" to declare the equestrian image as \"feeding a horse,\" we see negative weights on the rider and strong positive weights on the lower part of the horse."}, {"heading": "4.4 Distribution of Weights", "text": "For a single weight vector, \"we define abs (\u03c9) = [| \u03c91 |, | \u03c92 |,.., | \u03c9N |] (8) \u03c9 = 1N L \u2211 j = 1 abs (\u03c9j) (9), i.e., the first ten concepts have lower weight classes because they are used too often to be discriminatory; the next hundred concepts have higher weights associated with object concepts, ordered by their frequency in VG; it is not surprising that the first ten concepts have lower weight classes because they are too common to be discriminatory; the next hundred concepts have higher weights, and finally, weights with reduced frequency become lower. This can be explained by concepts that have weaker classifiers because they have less positive examples, making them less reliable."}, {"heading": "5 Conclusions & Future Work", "text": "We have presented a method to learn to recognize actions in images by describing them as a weighted sum of recognized concepts (objects and object properties), using the annotations in the VG dataset to learn a wide range of concepts that are then used to recognize actions in still images. Along with visual features, we are able to achieve state-of-the-art classification performance and provide a visual and semantic explanation of the classifier's decisions. In the future, we will actually expand our work to capture object relationships that are also very important for action classification."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Weakly Supervised Deep Detection Networks", "author": ["Hakan Bilen", "Andrea Vedaldi"], "venue": "arXiv preprint arXiv:1511.02853,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "HICO: A Benchmark for Recognizing Human-Object Interactions in Images", "author": ["Yu-Wei Chao", "Zhan Wang", "Yugeng He", "Jiaxuan Wang", "Jia Deng"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning", "author": ["Ramazan Gokberk Cinbis", "Jakob Verbeek", "Cordelia Schmid"], "venue": "arXiv preprint arXiv:1503.00949,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham", "Luc Van Gool", "Christopher KI Williams", "John Winn", "Andrew Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Deep spatial pyramid: The devil is once again in the details", "author": ["Bin-Bin Gao", "Xiu-Shen Wei", "Jianxin Wu", "Weiyao Lin"], "venue": "arXiv preprint arXiv:1504.05277,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "ACD: Action Concept Discovery from Image-Sentence Corpora", "author": ["Jiyang Gao", "Chen Sun", "Ram Nevatia"], "venue": "arXiv preprint arXiv:1604.04784,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Visual Genome: Connecting Language and Vision Using Crowdsourced", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li"], "venue": "Dense Image Annotations. CoRR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification", "author": ["Li-Jia Li", "Hao Su", "Li Fei-Fei", "Eric P Xing"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Zero-shot image tagging by hierarchical semantic embedding", "author": ["Xirong Li", "Shuai Liao", "Weiyu Lan", "Xiaoyong Du", "Gang Yang"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering", "author": ["Arun Mallya", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1604.04808,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "WordNet: a lexical database for English", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Constrained convolutional neural networks for weakly supervised segmentation", "author": ["Deepak Pathak", "Philipp Krahenbuhl", "Trevor Darrell"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning semantic relationships for better action retrieval in images", "author": ["Vignesh Ramanathan", "Congcong Li", "Jia Deng", "Wei Han", "Zhen Li", "Kunlong Gu", "Yang Song", "Samy Bengio", "Chuck Rossenberg", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Action bank: A high-level representation of activity in video", "author": ["Sreemanananth Sadanand", "Jason J Corso"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning to share visual appearance for multiclass object detection", "author": ["Ruslan Salakhutdinov", "Antonio Torralba", "Josh Tenenbaum"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["Bangpeng Yao", "Xiaoye Jiang", "Aditya Khosla", "Andy Lai Lin", "Leonidas Guibas", "Li Fei-Fei"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1512.04150,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Capturing long-tail distributions of object subcategories", "author": ["Xiangxin Zhu", "Dragomir Anguelov", "Deva Ramanan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 22, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 13, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 153, "endOffset": 161}, {"referenceID": 20, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 153, "endOffset": 161}, {"referenceID": 12, "context": "For instance, it has become popular to combine language and vision using joint embedded spaces [13, 17], which allow recognizing unseen classes more reliably than using a purely visual approach.", "startOffset": 95, "endOffset": 103}, {"referenceID": 16, "context": "For instance, it has become popular to combine language and vision using joint embedded spaces [13, 17], which allow recognizing unseen classes more reliably than using a purely visual approach.", "startOffset": 95, "endOffset": 103}, {"referenceID": 9, "context": "Our concept dataset is the recently introduced Visual-Genome dataset [10], in which we leverage the rich region annotations.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "An early related work is ObjectBank [12], where the outputs of detectors for 200 common objects are aggregated via a spatial-pyramid to serve as feature", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "In the same spirit, ActionBank [22] learns detectors for various action types in videos and uses them to represent others as weighted combinations of actions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "The work of [11] learns object attributes to describe objects in a zero-shot learning setting, so that new classes (animals) can be correctly classified by matching them to human generated lists of attributes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "Recently, [20] learned various types of relations between actions (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Other works leverage information from natural language: in [17] an image is mapped to a semantic embedding space by a convex combination of word embeddings according to a pre-trained classifier on ImageNet [21], allowing to describe unseen classes as combinations of known ones.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Other works leverage information from natural language: in [17] an image is mapped to a semantic embedding space by a convex combination of word embeddings according to a pre-trained classifier on ImageNet [21], allowing to describe unseen classes as combinations of known ones.", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "[13] makes this more robust by considering the output of the classifier along with the WordNet [16] hierarchy, generating image tags more reliably.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[13] makes this more robust by considering the output of the classifier along with the WordNet [16] hierarchy, generating image tags more reliably.", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "The work of [8] mines a large image-sentence corpora for actor-verb-object triplets and clusters them into groups of semantically related actions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "Recently, [15] used detected or provided person-bounding boxes in a multiple-instance learning framework, fusing global image context and person appearance.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "[7] uses spatial-pyramid pooling on activations from a convolutional layer in a network and encodes them using Fisher Vectors [7], with impressive results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] uses spatial-pyramid pooling on activations from a convolutional layer in a network and encodes them using Fisher Vectors [7], with impressive results.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "To learn a broad enough range of concepts, we use the recently introduced Visual Genome (VG) dataset [10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "Each region spans an entire objects or an object part, and is annotated with a noun, an attribute, and a natural language description, all collected using crowd-sourcing (see [10] for full details).", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "Table 1: Highest ranking concepts linked to each action class according to the proposed method, for 10 arbitrarily selected actions from the Stanford-40 Actions dataset[25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "This is both simpler and more robust, as it can be validated from various benchmarks ([5, 21] and many others) that detecting the presence of objects in images currently works more accurately than correctly localizing them.", "startOffset": 86, "endOffset": 93}, {"referenceID": 20, "context": "This is both simpler and more robust, as it can be validated from various benchmarks ([5, 21] and many others) that detecting the presence of objects in images currently works more accurately than correctly localizing them.", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 3, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 17, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 18, "context": "To overcome this, we represent each concept by its representation produced by the GloVe method of [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "We perform K-means [1] clustering on the embedded word-vectors for a total of 100 clusters.", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "Denote by CI the set of concepts in an image I according to the ground-truth in the VG [10]dataset.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "To validate our approach, we have tested it on the Standford-40 Action dataset [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "In addition we test our method on the recently introduced HICO [3] dataset, with 47774 images and 600 (not all mutually exclusive) action classes.", "startOffset": 63, "endOffset": 66}, {"referenceID": 25, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 285, "endOffset": 289}, {"referenceID": 8, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 359, "endOffset": 362}, {"referenceID": 5, "context": "In all cases, we train a linear SVM [6] in a one-versus-all manner on `2 normalized features, or the `2 normalized concatenation of `2 features in case of using several feature types.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "To assign GloVe [19] vectors to object names or attributes, we use the pre-trained model on the Common-Crawl (42B) corpus, which contains a vocabulary of 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "Figure 1: (a) Object (red) and attributes (green) in the VG dataset [10] follow a long tail distribution.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "Except for the attribute-object concepts, we see that the concept based classification does nearly as well as the direct visual-based method, where the addition of ResNet-151 [9] clearly improves results.", "startOffset": 175, "endOffset": 178}, {"referenceID": 24, "context": "12% on Stanford-40 [25].", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "On the recent HICO [3] dataset we obtain a mean average precision of 31.", "startOffset": 19, "endOffset": 22}, {"referenceID": 14, "context": "[15] obtain higher results (36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "G: GlobalAverage-Pooling layer from [26].", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "V: fc6 from VGG-16 [24].", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "R: pool5 (penultimate layer) from ResNet-151[9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 24, "context": "Stanford-40[25](accuracy) Method \\ Features G G+V G+V+R", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "81 [7] HICO[3](mAP) G G+V G+V+R", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "81 [7] HICO[3](mAP) G G+V G+V+R", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "1\u2020[15]", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "Table 1 shows ten arbitrarily chosen classes from the Stanford-40 Actions [25] dataset, with the top 5 ranked object-concepts according to the respective weight-vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "To test what features contribute most to the classification result, we use the Class-Activation-Map (CAM) approach of [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We can do this for the version of our method which uses only the VGG-GAP features, as the method relies on the structure specific to this network to re-project the classification scores to the image (see [26] for details).", "startOffset": 204, "endOffset": 208}, {"referenceID": 25, "context": "We visualize (using [26]) highlighted regions contributing to the strongest concepts related to the correct vs the incorrect class .", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "2 we assign to each concept c \u2208 C a GloVe [19] representation Vc.", "startOffset": 42, "endOffset": 46}], "year": 2016, "abstractText": "Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford40 Actions dataset, where the proposed method outperforms current state-of-the art and, combined with direct visual features, obtains an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.", "creator": "LaTeX with hyperref package"}}}