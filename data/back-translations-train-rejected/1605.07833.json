{"id": "1605.07833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Effective Blind Source Separation Based on the Adam Algorithm", "abstract": "In this paper, we derive a modified InfoMax algorithm for the solution of Blind Signal Separation (BSS) problems by using advanced stochastic methods. The proposed approach is based on a novel stochastic optimization approach known as the Adaptive Moment Estimation (Adam) algorithm. The proposed BSS solution can benefit from the excellent properties of the Adam approach. In order to derive the new learning rule, the Adam algorithm is introduced in the derivation of the cost function maximization in the standard InfoMax algorithm. The natural gradient adaptation is also considered. Finally, some experimental results show the effectiveness of the proposed approach.", "histories": [["v1", "Wed, 25 May 2016 11:40:46 GMT  (681kb,D)", "https://arxiv.org/abs/1605.07833v1", "This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series"], ["v2", "Mon, 26 Sep 2016 14:17:27 GMT  (1769kb,D)", "http://arxiv.org/abs/1605.07833v2", "Revised version after review process. This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published soon as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series"]], "COMMENTS": "This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michele scarpiniti", "simone scardapane", "danilo comminiello", "raffaele parisi", "aurelio uncini"], "accepted": false, "id": "1605.07833"}, "pdf": {"name": "1605.07833.pdf", "metadata": {"source": "CRF", "title": "Effective Blind Source Separation Based on the Adam Algorithm", "authors": ["Michele Scarpiniti", "Simone Scardapane", "Danilo Comminiello", "Raffaele Parisi", "Aurelio Uncini"], "emails": ["aurelio.uncini}@uniroma1.it"], "sections": [{"heading": null, "text": "Keywords: Blind Source Separation, Stochastic Optimization, Adam Algorithm, InfoMax Algorithm, Natural Gradient."}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 The Blind Source Separation Problem", "text": "Let us consider a number of N unknown and statistically independent sources, which are referred to as s [n] = [s1] and [s2] separate signals.., sN [n] T, making the components si [n] null-mean and mutually independent. Signals received from an array of M sensors are described by x [n] [n] n [n] n [n] n [n] n different signals, xM [n] T and are referred to as mixtures, where the matrix A = (ai j) collects the mixing coefficients., i, j = 1,2,., N and v [n] = [v1]., vN [n] T is a noise vector, with correlations."}, {"heading": "3 The Adam Algorithm", "text": "The Adam algorithm performs the gradient descent (or ascent) optimization by evaluating the moving averages of the loud gradient mt and the square gradient vt [12]. These moment vectors are updated by using two scalar coefficients \u03b21 and \u03b22 that control the exponential drop rates: mt = \u03b21mt \u2212 1 + (1 \u2212 \u03b21) gt, (10) vt = \u03b22vt \u2212 1 + (1 \u2212 \u03b22) gt gt gt, (11) where \u03b21 \u2212 \u03b22 \u2212 0.1) and the element \u2212 gt \u2212 1 + (1 \u2212 \u03b21) gt, the pulse generator and the pulse generator are zero."}, {"heading": "4 Modified InfoMax algorithm", "text": "In this section we present the modified Bell and Sejnowski InfoMax algorithm, which is based on the Adam optimization method. Since the Adam algorithm uses a vectorization vector of parameters, we perform a vectorization of the gradient (6) or (7) w = vec (WH (y) - RN 2 \u00b7 1, (15), where vec (A) is the vectorization operator that forms a vector by stacking the columns of matrix A. The gradient vector is evaluated from the signals by a number of blocks NB. At this point, the mean value and the variance vectors are evaluated from the knowledge of the gradient w \u2212 t at a given time using equations (10) - (13). Then, the gradient vector (14) is evaluated from the signals using the gradient vector NB."}, {"heading": "5 Experimental Results", "text": "In this last section, we propose some experimental results to demonstrate the effectiveness of the proposed idea. We perform separation of mixtures of synthetic and real-world data = 1.1. The results are evaluated with respect to the Amari Performance Index (PI) [6], defined asPI = 1N (N \u2212 1) N \u2211 i = 1 N \u2211 k = 1 | maxj. [7] The results are evaluated with respect to matrix Q, while matrix Q is near zero if matrix Q is close to the product of a permutation matrix and a diagonal scaling matrix. [18), where qi j are the elements of matrix Q = WA. This index is close to zero if matrix Q is close to the product of a permutation matrix and a diagonal scaling matrix. The performance of the proposed algorithm has also been compared with the standard InfoMax algorithm [3] and the Momentum Max Algorithm [13]."}, {"heading": "6 Conclusions", "text": "This paper presented a modified InfoMax algorithm for blind separation of independent sources in a linear and immediate environment, based on a novel and advanced stochastic optimization method called Adam, which can benefit from the excellent properties of the Adam approach. In particular, it is easy to implement, computationally efficient, and suitable when the number of sources is high and poorly scaled, the mixing matrix is almost insufficiently conditioned, and additional noise is taken into account. Some experimental results evaluated in terms of the Amari performance index and compared with other state-of-the-art approaches have demonstrated the effectiveness of the proposed approach."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation 10(3), 251\u2013 276", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "The fundamental limitation of frequency domain blind source separation for convolutive mixtures of speech", "author": ["S. Araki", "R. Mukai", "S. Makino", "T. Nishikawa", "H. Saruwatari"], "venue": "IEEE Transactions on Speech and Audio Processing 11(2), 109\u2013116", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "An information-maximisation approach to blind separation and blind deconvolution", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Neural Computation 7(6), 1129\u20131159", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "On convolutive blind source separation in a noisy context and a total variation regularization", "author": ["T.Z. Boulmezaoud", "M. El Rhabi", "H. Fenniri", "E. Moreau"], "venue": "Proc. of IEEE Eleventh International Workshop on Signal Processing Advances in Wireless Communications (SPAWC2010). pp. 1\u20135. Marrakech", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Blind source separation and independent component analysis: a review", "author": ["S. Choi", "A. Cichocki", "H.M. Park", "S.Y. Lee"], "venue": "Neural Information Processing - Letters and Reviews 6(1), 1\u201357", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive Blind Signal and Image Processing", "author": ["A. Cichocki", "S. Amari"], "venue": "John Wiley", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Handbook of Blind Source Separation", "author": ["P. Comon", "Jutten", "C. (eds."], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaled natural gradient algorithm for instantaneous and convolutive blind source separation", "author": ["S.C. Douglas", "M. Gupta"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP2007). vol. 2, pp. 637\u2013640", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "H.E.", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12(7), 2121\u20132159", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Adaptive Filtering, vol", "author": ["Haykin", "S. (ed."], "venue": "2: Blind Source Separation. Wiley", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Wavelet-ICA methodology for efficient artifact removal from electroencephalographic recordings", "author": ["G. Inuso", "F. La Foresta", "N. Mammone", "F.C. Morabito"], "venue": "Proc. of International Joint Conference on Neural Networks", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive improved natural gradient algorithm for blind source separation", "author": ["J.Q. Liu", "D.Z. Feng", "W.W. Zhang"], "venue": "Neural Computation 21(3), 872\u2013889", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Probability, Random Variables and Stochastic Processes", "author": ["A. Papoulis"], "venue": "McGraw-Hill", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized splitting functions for blind separation of complex signals", "author": ["M. Scarpiniti", "D. Vigliano", "R. Parisi", "A. Uncini"], "venue": "Neurocomputing 71(10-12), 2245\u20132270", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind separation of convolved mixtures in the frequency domain", "author": ["P. Smaragdis"], "venue": "Neurocomputing 22(21\u201334)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Step-size control in blind source separation", "author": ["P. Thomas", "G. Allen", "N. August"], "venue": "International Workshop on Independent Component Analysis and Blind Source Separation. pp. 509\u2013514", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Flexible nonlinear blind signal separation in the complex domain", "author": ["D. Vigliano", "M. Scarpiniti", "R. Parisi", "A. Uncini"], "venue": "International Journal of Neural Systems 18(2), 105\u2013122", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 6, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 9, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 4, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 15, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 18, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 6, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 86, "endOffset": 91}, {"referenceID": 2, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 86, "endOffset": 91}, {"referenceID": 1, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 3, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 16, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 5, "context": "Some of these approaches perform separation by using high order statistics (HOS) while others exploit information theoretic (IT) measures [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "One of the well-know algorithms in this latter class is the InfoMax one proposed by Bell and Sejnowski in [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Moreover, in order to avoid numerical instability, a natural gradient modification to InfoMax algorithm has also been proposed [1,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "Moreover, in order to avoid numerical instability, a natural gradient modification to InfoMax algorithm has also been proposed [1,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 12, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 11, "context": "Recently, a novel algorithm for gradient based optimization of stochastic cost functions has been proposed by Kingma and Ba in [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "The authors have demonstrated in [12] that Adam is easy to implement, computationally efficient, invariant to diagonal rescaling of the gradients and well suited for problems with large data and parameters.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "The Adam algorithm combines the advantages of other state-of-the-art optimization algorithms, like AdaGrad [9] and RMSProp [19], outperforming the limitations of these algorithms.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "In addition, Adam can be related to the natural gradient (NG) adaptation [1], employing a preconditioning that adapts to the geometry of data.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "In this paper we propose a modified InfoMax algorithm based on the Adam algorithm [12] for the solution of BSS problems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "Some experimental results, evaluated in terms of the Amari Performance index (PI) [6], show the effectiveness of the proposed idea.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "The weights wi j can be adapted by maximizing or minimizing some suitable cost function [6,5].", "startOffset": 88, "endOffset": 93}, {"referenceID": 4, "context": "The weights wi j can be adapted by maximizing or minimizing some suitable cost function [6,5].", "startOffset": 88, "endOffset": 93}, {"referenceID": 2, "context": "A particularly good approach is to maximize the joint entropy of a single layer neural network [3], as shown in Figure 1, leading to the Bell and Sejnowski InfoMax algorithm.", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "With reference to Figure 1, using the equation relating the probability density function (pdf) of a random variable px (x) and nonlinear transformation of it py (y) [14], the joint entropy H (y) of the network output y[n] can be evaluated as", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "In order to avoid the numerical problems of the matrix inversion in (6) and the possibility to remain blocked in a local minimum, Amari has introduced the Natural Gradient (NG) adaptation [1] that overcomes such problems.", "startOffset": 188, "endOffset": 191}, {"referenceID": 11, "context": "The Adam algorithm performs the gradient descent (or ascent) optimization by evaluating the moving averages of the noisy gradient mt and the square gradient vt [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "The vector v\u0302t represents an approximation of the diagonal of the Fisher information matrix [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Hence Adam can be related to the natural gradient algorithm [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The results are evaluated in terms of the Amari Performance Index (PI) [6], defined as", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "The performances of the proposed algorithm were also compared with the standard InfoMax algorithm [3] and the Momentum InfoMax described in [13].", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "The performances of the proposed algorithm were also compared with the standard InfoMax algorithm [3] and the Momentum InfoMax described in [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "ICA is a common approach to deal with the problem of artifact removal from EEG [11].", "startOffset": 79, "endOffset": 83}], "year": 2016, "abstractText": "In this paper, we derive a modified InfoMax algorithm for the solution of Blind Signal Separation (BSS) problems by using advanced stochastic methods. The proposed approach is based on a novel stochastic optimization approach known as the Adaptive Moment Estimation (Adam) algorithm. The proposed BSS solution can benefit from the excellent properties of the Adam approach. In order to derive the new learning rule, the Adam algorithm is introduced in the derivation of the cost function maximization in the standard InfoMax algorithm. The natural gradient adaptation is also considered. Finally, some experimental results show the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}