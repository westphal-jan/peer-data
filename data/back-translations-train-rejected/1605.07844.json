{"id": "1605.07844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation", "abstract": "Taking advantage of top-ranked documents in response to a query for improving quality of query translation has been shown to be an effective approach for cross-language information retrieval. In this paper, we propose a new method for query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional representations of the words in the pseudo-relevant collections separately and then aim at finding a query-dependent transformation matrix between the vectors of translation pairs. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Persian demonstrate that the proposed method outperforms all competitive baselines in language modelling, particularly when it is combined with a collection-dependent translation model.", "histories": [["v1", "Wed, 25 May 2016 12:04:43 GMT  (864kb,D)", "https://arxiv.org/abs/1605.07844v1", null], ["v2", "Sat, 8 Oct 2016 11:19:10 GMT  (1166kb,D)", "http://arxiv.org/abs/1605.07844v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["javid dadashkarimi", "mahsa s shahshahani", "amirhossein tebbifakhr", "heshaam faili", "azadeh shakery"], "accepted": false, "id": "1605.07844"}, "pdf": {"name": "1605.07844.pdf", "metadata": {"source": "CRF", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation", "authors": ["Javid Dadashkarimi", "Mahsa S. Shahshahani", "Amirhossein Tebbifakhr", "Heshaam Faili", "Azadeh Shakery"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green"}, {"heading": "2 Related Works", "text": "It has long been used as a powerful method of estimating query language models in a large number of studies. Unlike CLRLM, which relies on parallel corpora and bilingual lexicographies, CLTRLM aims to find a number of bilingual topical variables from a comparable body to transfer the relevance of a term from one language to another. Unlike CLRLM, which relies on parallel corpora and bilingual lexicographies, these variables are used for query translations and show that CLTRLM is an effective method, especially for resource-lean languages. In CLTRLM, the top documents F s = {ds1, ds2, ds2,.., ds | F s |} are retrieved in response to the source sources."}, {"heading": "3 Linear Projection between Languages based on Pseudo-relevant Documents", "text": "In this section, we will present the proposed method in more detail. We will use an offline approach to learn bilingual representations of the words by evaluating pseudo-relevant documents in source and target languages. To this end, we will first learn word representations of the pseudo-relevant collections separately and then focus on finding a transformation matrix that minimizes a distance function between all the translation pairs appearing on the collections. As shown in Equation 1, our goal is to minimize f in relation to a transformation matrix W-Rn-n; f is defined as follows: f (W) = \u2211 (ws, wt) 1-2 | | WT-uws-vwt-2 (1), where wt is a translation pair of ws from F-s that appears in F-t; uws-Rn-1 and uwt-Rn-1 are the corresponding vectors or random vectors."}, {"heading": "3.1 Bilingual Representations and Translation Models", "text": "In this section, we present a method based on bilingual word representations to build a translation model for the query and then incorporate it into language modeling, the state retrieval frame. The new translation model is structured as follows: p (wt | ws) = e uws (3), in which WT uws transforms the source vector based on the source collection into the target space with small dimensional space.The new translation model is structured as follows: p (wt | ws) = e uws (4), in which T {ws} is the list of translations of ws."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "The source collection is a pool of Associated Press 1988-89, Los Angeles Times 1994 and Glasgow Herald 1995 collections used in previous TREC and CLEF ad hoc evaluation campaigns. In all experiments, we use the KL divergence retrieval model and the Dirichlet smoothing method to estimate the document language models where we set the dirichlet parameter \u00b5 to the typical value of 1000. To improve retrieval performance, we use the mixing model for pseudo-relevant feedback with the feedback coefficient of 0.5. The number of feedback documents and terms is set at the typical values of 10 and 50, respectively."}, {"heading": "4.2 Performance Comparison and Discussion", "text": "In this section, we will compare the effectiveness of a number of competitive methods in CLIR. We will consider the following dictionary-based CLIR methods for evaluating the proposed method: (1) the top-1 translation of each term in bilingual dictionaries (TOP-1), (2) all possible translations of each term with equal weighting (UNIFORM), (3) (BiCTM) proposed in [9], (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10]. As a basis for the comparisons, we also have the results of the monolingual runs in each collection (MONO) and the machine translator (MT), but our main focus is on investigating the superiority of the proposed method compared to the CLIR dictionaries."}, {"heading": "4.3 Parameter Sensitivity", "text": "We examine the sensitivity of the proposed method to two parameters \u03b1 and n in Figure 1. We first fix one parameter to its optimum value and then try to determine the optimum value of the other. It shows that both parameters function stable across Fr, ES and IT collections. Empirically, the optimum \u03b1 value is 0.6 and the optimum value of n in almost all collections is 10."}, {"heading": "5 Conclusion and Future Works", "text": "In this paper, we presented a translation model for interlingual information retrieval that uses source and target language feedback documents to generate word vectors in each, and then learned a projection matrix to project word vectors in the source language onto their translations in the target language. Then, we introduced a method to build a translation model that can easily be interpolated with other models, examining the performance of the proposed method on four European CLEF collections. Our method showed further improvements in FR, SP and IT, as the leading documents in DE are not as comparable as the previous ones; therefore, applying the proposed method to comparable companies is considered an interesting future work. In short queries, the proposed method achieves up to 87% of machine translation (MT) performance and significant improvements in verbose queries."}], "references": [{"title": "Cross-Lingual Topical Relevance Models", "author": ["D. Ganguly", "J. Leveling", "G. Jones"], "venue": "COLING \u201912", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["S. Gouws", "Y. Bengio", "G. Corrado"], "venue": "arXiv preprint:1410.2455", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-lingual relevance models", "author": ["V. Lavrenko", "M. Choquette", "W.B. Croft"], "venue": "SIGIR \u201902", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Relevance based language models", "author": ["V. Lavrenko", "W.B. Croft"], "venue": "SIGIR \u201901. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Positional relevance model for pseudo-relevance feedback", "author": ["Y. Lv", "C. Zhai"], "venue": "SIGIR \u201910", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting the divergence minimization feedback model", "author": ["Y. Lv", "C. Zhai"], "venue": "CIKM \u201914", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint:1309.4168", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Adv. in Neur. Inf. Proc. Sys., pp. 3111\u20133119", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative Translation Disambiguation for Cross-language Information Retrieval", "author": ["C. Monz", "B.J. Dorr"], "venue": "SIGIR \u201905", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vulic", "M. Moens"], "venue": "SIGIR \u201915", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications", "author": ["I. Vulic", "W.D. Smet", "J. Tang", "M. Moens"], "venue": "IP&M 51(1)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Pseudo-relevance feedback based on matrix factorization", "author": ["H. Zamani", "J. Dadashkarimi", "A. Shakery", "W.B. Croft"], "venue": "CIKM \u201916", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 11, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 2, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 2, "context": "To this end, cross-lingual topical relevance models (CLTRLM) aims to find a way to transform knowledge of the sets to the query model using bilingual topic modeling and a bilingual dictionary [3,1].", "startOffset": 192, "endOffset": 197}, {"referenceID": 0, "context": "To this end, cross-lingual topical relevance models (CLTRLM) aims to find a way to transform knowledge of the sets to the query model using bilingual topic modeling and a bilingual dictionary [3,1].", "startOffset": 192, "endOffset": 197}, {"referenceID": 9, "context": "Recently, bilingual word embedding is tailored effectively to this end where low-dimensional vectors are built after shuffling all the alignments [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "Unlike CLTRLM and the mixed word embedding translation model (MIXWETM) based on shuffling alignments in a comparable corpora ([10]), CLWETM considers quite sentence-level contexts of the words and therefore captures deeper levels of n-grams in both the languages.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "Furthermore, the obtained model can be incorporated within a language modeling framework, the state-of-the-art retrieval framework in the literature, and therefore the proposed method does not suffer from disadvantages of low-dimensional query-document similarity in ad-hoc retrieval [10].", "startOffset": 284, "endOffset": 288}, {"referenceID": 5, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 4, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 11, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 0, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 10, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 2, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 0, "context": "proposed to use these variables for query translation and demonstrated that CLTRLM is an effective method particularly for resource-lean languages [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": ", d|F t|} retrieved in response to a translation of the query (q) are assumed to be relevant documents and then we expect that each word w in target language is generated either from a target event or a source event as follows: p(w|q) = p(w|z)p(z|q)+ p(w|w)p(w|q) in which z is a topical variable on F t and w is a translation of w in the dictionary (see whole the details in [1]).", "startOffset": 376, "endOffset": 379}, {"referenceID": 7, "context": "Language modeling based on neural networks is a popular technique for capturing co-occurrences of the terms within a constant window c and thus, this model embeds semantic of the language as well as deeper levels of n-grams [8].", "startOffset": 224, "endOffset": 227}, {"referenceID": 1, "context": "On-line methods aim at finding a unified space for the languages during the learning process as follows [2]: L(\u03b8) = L(\u03b8)+L(\u03b8)+\u03bbf(\u03b8, \u03b8) where the last term is a regularization term.", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": ", introduced the shuffling-based word embedding method over comparable corpora in which the alignments play as a number of constraints on the vectors [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 6, "context": "Off-line methods learn the vectors separately and then find a transformation matrix minimizing a constraint function f as follows [7]: f(W) = \u2211 x,z ||W T x \u2212 z|| in which", "startOffset": 130, "endOffset": 133}, {"referenceID": 8, "context": "2) is estimated by a bi-gram coherence translation model (BiCTM) introduced in [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "uws and vwt are computed based on negative sampling skip-gram introduced in [8]; the size of the window, the number of negative samples, and the size of the vectors are set to typical values of 10, 45, and 50 respectively.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "As shown in [1] JCLTRLM outperforms CLTRLM and therefore we opted JCLTRLM as a baseline.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 268, "endOffset": 271}, {"referenceID": 0, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 309, "endOffset": 312}, {"referenceID": 9, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 330, "endOffset": 334}], "year": 2016, "abstractText": "Using top-ranked documents in response to a query has been shown to be an effective approach to improve the quality of query translation in dictionarybased cross-language information retrieval. In this paper, we propose a new method for dictionary-based query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional vectors of the words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of translation pairs appeared in the collections. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a querydependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Italian demonstrate that the proposed method outperforms a word embedding baseline based on bilingual shuffling and a further number of competitive baselines. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries.", "creator": "LaTeX with hyperref package"}}}