{"id": "1605.07918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling", "abstract": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.", "histories": [["v1", "Wed, 25 May 2016 14:59:46 GMT  (1464kb,D)", "http://arxiv.org/abs/1605.07918v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["byungsoo kim", "hwanjo yu", "gary geunbae lee"], "accepted": false, "id": "1605.07918"}, "pdf": {"name": "1605.07918.pdf", "metadata": {"source": "CRF", "title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling", "authors": ["Byungsoo Kim", "Hwanjo Yu", "Gary Geunbae Lee"], "emails": ["gblee}@postech.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2 Types of Relation", "text": "The first type of relationship is a relationship mediated by verb. < Boeing; a relationship of this kind is a verb phrase. It often forms an n-relationship. Consider as an example: \"Boeing announced the 747 ASB in 1986.\" The relationship \"announced\" has three arguments: \"Boeing,\" the 747 ASB \"and\" 1986. \"This n-relationship is presented as n-tuple: < Boeing; announced; the 747 ASB; 1986 >. However, since a binary relationship is a core concept of the semantic network and ontological KB, the n-turistic relationship must be transformed into binary relationships.\" This conversion involves addressing the problem of incomplete relationships. In the above example, simply covering all pairs of arguments, the triples < Boeing; announced; the 747 ASB >, < Boeing; announced; announced; announced; announced; announced: the full relationship between Boeing and ASB. \""}, {"heading": "3 Task Definition", "text": "We define Open IE as two tasks: recognizing arguments and classifying prepositions. In the face of a sentence, recognizing the argument involves considering a particular word (rel) as the keyword of a relationship, and then classifying other words (arg) to see if they are the correct keywords for the arguments for that relationship. As we enter, the classifier takes the shortest dependency path that connects rel with arg. We refer to this path as the path (rel, arg). By looking at the shortest dependency path that connects two words, we can focus on informative words that are useful for understanding the relationship between the two words (Bunescu and Mooney, 2005). In Figure 1 (a), \"Boeing,\" \"\" ASB, \"\" \"747\" are irrelevant for determining whether \"1986\" is a suitable argument for \"announced.\" We define four classes for recognizing arguments (arg1, \"arg2,\" argN \"and\")."}, {"heading": "4 Automatically Constructing the Training Set", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Highly Precise Tuple Extraction", "text": "Since Christensen et al. (2011) used semantic role designations (SRL) to find highly trustworthy relationships, we have used SRL1 to extract high-precision tuples with mediated ratios 2. We assign rel predicates and arg1, arg2, and argN to the labeled word with roles A0, A1, and AM. If the word is a preposition, we apply the assignment to its child while maintaining the problem of preposition for preposition classification. Consider the following example: \"In addition to the French Open, Nadal won 10 more singles titles in 2005.\" The SRL edition is \"Predicate: win, A0: Nadal, A1: title, AM-DIS: In, AM-TMP: in,\" and our assignment extracts the tuples as \"rel: win, arg1: Nadal, arg2: title, argN (in title.D)."}, {"heading": "4.2 Training Set Augmentation", "text": "Similar to OLLIE (Mausam et al., 2012) and ReNoun (Yahya et al., 2014), this augmentation process is based on seed-based remote monitoring: if arguments in a seed appear three times in a sentence, their relationship is likely to appear in the sentence. Magnification begins with the conversion of the tuple into triples. For tuples with a verb-mediated relationship, we convert each tuple into < arg1; arg2 >, arg1; arg1; arg2; arg2; argN >. For tuples with a noun-mediated relationship, we convert each tuple into < arg1; arg1; arg2 >. Among the converted triples, we acquire 55K seeds that satisfy the following limitations: (1) the arguments are reasonable; we are associated with numbers (not properly)."}, {"heading": "4.3 Feedback Negative Sampling", "text": "The samples from the previous stages only indicate which paths are arg1, arg2, and argN. They do not describe which paths are zero (negative), and one possible option for negative sensing is to consider non-positive paths as negative paths. However, there is a risk that uncaught positive paths will be treated as negative paths. Thus, for example, we have found that this path (spoke Moses) is an uncaught positive path labeled argN in the sentence: \"Their assumption was rejected by God, who confirmed Moses\" uniqueness as the one with whom the LORD spoke face to face. \"Our strategy for solving this problem begins with the observation that there are two characteristics to a highly negative path: (1) it contains a positive path or a positive path; and (2) the more similar it is to the positive path, the more negative the path is the more negative the path."}, {"heading": "5 Argument Detection", "text": "It is about the way in which people in the individual countries of the world get involved in the question of how far they are able to move in the world, and where they are able to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to think in the world, to think in the world, to think in the world, to think in the world, to get lost in the world, to get lost in the world, to get lost in the world, to get lost in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think, to think, to think, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think in the world, to think, to think in the world, to think in the world, to think, to think in the world, to think in the world, to think in the world, to think in the world, to think, to think, to think in the world, to think in the world, to think in the world, to think in the world, to think, to think in the world, to think in the world, to think in the world, to think,"}, {"heading": "6 Preposition Classification", "text": "The neural network used for preposition classification is almost identical to the model used in the previous section. There are only two modifications to the preposition classification model. Firstly, there is no penultimate fully connected layer in the model. We connect the max pooling layer directly to the softmax output layer. The second modification concerns the number of output classes. The number of classes for preposition classification depends on the number of prepositions occurring in the positive samples. With 88 prepositions in the positive samples and an additional class for non-prepositions, the neural network model has Hout-R89 for preposition classification."}, {"heading": "7 Triple Extraction", "text": "Triple extraction begins by aligning the prediction results as defined in the extraction template (see Table 1), which results in incomplete triples of arguments and relationships that are incomplete formulations. We extend over the aligned words arg1, rel, arg2, and argN to ensure that the triples contain sufficient information from the sentences. Previous Open IE systems assign a score for every tripled extracted. The score is used to indicate the degree of correctness, as extracted triples are not always correct.We define a scoring function as the following argument.score (t) = dep (s) \u00b7 \u2211 arg-args prob (arg) | args | (5), where t is a triple extracted from a sentence, dep (s) is the dependence that analyzes the confidence value of s, arg's probability is a set of arguments, and the output is conditional (prob)."}, {"heading": "8 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Evaluation Settings", "text": "We combed news articles on the web and sampled 100 sentences for evaluation. Since Open IE extracts entirely new relationships from the sentences, there are no basic truth extractions. For this reason, our natural choice for a performance metric was to calculate the precision of the number of extractions. This is a common metric in previous Open IE studies. The extractions were commented on manually for their accuracy and sorted by their results, in descending order. We set our system to output extractions with values above 0.75 to clarify our evaluation results."}, {"heading": "8.2 Comparison with State-of-the-Art Open", "text": "Unlike Open IE 4.2 and OLLIE, our system does not determine whether the extractions are factual. Therefore, we considered all extractions from Open IE 4.2 and OLLIE in comparison, without distinguishing the factuality of the extractions. Since there is no way to convert non-ary relationships into binary relationships, we discarded non-ary relationships from Open IE 4.2. Our proposed system produced more extractions than the other Open IE systems, and it achieved the highest precision in all areas in terms of the number of extractions (see Figure 5). Specifically, the proposed system resulted in extractions that were 1.62, 1.94 and 4.32 times more correct than Open IE 4.2, OLLIE and Reverb, outperforming the previous Open IE systems in terms of precision and extraction."}, {"heading": "8.3 Comparison with Different System Settings", "text": "Next, we analyzed how our system benefits from bidirectional LSTM networks (Figure 6 (a)). We compared two sets of extractions: extractions from a model that was trained with samples from high-precision tuples (without augmentation), and extractions from a method that uses high-precision tuples extraction (high-precision tuples). The first set contained extractions 1.25 times more accurate than the second set. In addition, the first set contained extractions that were more precise. We analyzed the effects of the augmentation of the training set by comparing two models: a model that was trained with the augmented samples (with augmentation), and samples from the high-precision tuples (without augmentation). Figure 6 (a) shows that the augmentation of the training set contributed to the production of extractions that were 1.12 times more correct than the extraction with a slight increase in precision."}, {"heading": "8.4 Extraction Error Analysis", "text": "We analyzed faulty extractions and investigated the source of error. According to our analysis, 20% of the errors were due to faulty dependency parsing. Since the proposed system receives a dependency path as input, errors in dependency parsing spread throughout the system. Among the faulty extractions without dependency parsing errors, 98% of the errors came from argument recognition and 4% from preposition classification."}, {"heading": "9 Conclusion", "text": "Our novel Open IE system with LSTM networks produced more precise and abundant extractions than modern Open IE systems. In particular, the proposed system extracted implicit relationships, unlike other Open IE systems. The advantages of the proposal are based on two contributions: a bi-directional, recurring architecture with LSTM units that allows the extraction of superordinate features that contain the contextual information in a sentence; and negative feedback that reduces discrepancies between positive and negative samples."}, {"heading": "A Bi-Directional Recurrent Layer with LSTM Units", "text": "It starts from the forward-facing recurrent layer with LSTM units containing input sequences from beginning to end (equations 6-11).ffwt (W fwt = 1).ffwt The BWW-S (W fwt = 1 + B fw f) (6) ifwt = 1 (W fwg = 1). (W fwg = 1 (W fwg = 1 + B fwi \u00b7 C fw = 1) (7) gfwt = 1 (W fw g g = 1) xt (W fwg = 1). (8) cfwt = i fw = 1 fw (9) ofwt = 1 (W fw = 2)."}, {"heading": "B Training Details", "text": "We set the threshold for the predictive value p to 0.9 during negative feedback samples. In addition, we set Dimwort to 300 and Dimpos, Dimdep and Dimne to 50. In addition, DimL was set to 450, corresponding to the dimensions of the input vector, and Dimpos, Dimdep and Dimne to 50. Similar to the control method used in Xu et al. (2015), we assigned a failure rate of 0.5 to the input vector. Since we use a Softmax operation for the final output, the natural choice for a training target is cross-entropy. J (\u03b8) = \u0445T log p (y (t) | x (t), \u03b8) (18) In the above equation, T is a series of training samples, and we use the predictive values: (Mpos, Mdep, Mne, MLSTM, Mhigher, M\u03b2out)."}], "references": [{"title": "Kraken: Nary facts in open information extraction", "author": ["Alan Akbik", "Alexander L\u00f6ser."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX), pages 52\u201356, Montr\u00e9al,", "citeRegEx": "Akbik and L\u00f6ser.,? 2012", "shortCiteRegEx": "Akbik and L\u00f6ser.", "year": 2012}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "Proceedings of the 6th International The Semantic Web and 2Nd Asian Conference on Asian Seman-", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Open information extraction from the web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan Bunescu", "Raymond Mooney."], "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724\u2013", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "An analysis of open information extraction based on semantic role labeling", "author": ["Janara Christensen", "Mausam", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the Sixth International Conference on Knowledge Capture, K-CAP \u201911, pages 113\u2013120,", "citeRegEx": "Christensen et al\\.,? 2011", "shortCiteRegEx": "Christensen et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Joachim Daiber", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes."], "venue": "Proceedings of the 9th International Conference on Semantic Systems, I-SEMANTICS \u201913, pages 121\u2013", "citeRegEx": "Daiber et al\\.,? 2013", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Identifying relations for open information extraction", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535\u20131545, Edinburgh, Scotland, UK.,", "citeRegEx": "Fader et al\\.,? 2011", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914,", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Dependency-based open information extraction", "author": ["Pablo Gamallo", "Marcos Garcia", "Santiago Fern\u00e1ndez-Lanza."], "venue": "Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP, pages 10\u201318, Avignon, France,", "citeRegEx": "Gamallo et al\\.,? 2012", "shortCiteRegEx": "Gamallo et al\\.", "year": 2012}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber."], "venue": "Proceedings of the IEEEINNS-ENNS International Joint Conference on Neural Networks, 2000. IJCNN 2000, volume 3, pages 189\u2013194 vol.3.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "The Journal of Neural Computation, 9(8):1735\u20131780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Open language learning for information extraction", "author": ["Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Mausam et al\\.,? 2012", "shortCiteRegEx": "Mausam et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating syntactic and semantic analysis into the open information extraction paradigm", "author": ["Andrea Moro", "Roberto Navigli."], "venue": "Proceedings of the 23rd International Joint Conference on Artifical Intelligence, IJCAI\u201913.", "citeRegEx": "Moro and Navigli.,? 2013", "shortCiteRegEx": "Moro and Navigli.", "year": 2013}, {"title": "Open information extraction to KBP relations in 3 hours", "author": ["Stephen Soderland", "John Gilmer", "Robert Bart", "Oren Etzioni", "Daniel S. Weld."], "venue": "Text Analysis Conference, TAC\u201913.", "citeRegEx": "Soderland et al\\.,? 2013", "shortCiteRegEx": "Soderland et al\\.", "year": 2013}, {"title": "Open information extraction using wikipedia", "author": ["Fei Wu", "Daniel S. Weld."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118\u2013127, Uppsala, Sweden, July. Association for Computational Linguis-", "citeRegEx": "Wu and Weld.,? 2010", "shortCiteRegEx": "Wu and Weld.", "year": 2010}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Renoun: Fact extraction for nominal attributes", "author": ["Mohamed Yahya", "Steven Whang", "Rahul Gupta", "Alon Halevy."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325\u2013335, Doha, Qatar,", "citeRegEx": "Yahya et al\\.,? 2014", "shortCiteRegEx": "Yahya et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Open IE has been successfully applied in many NLP tasks, such as question answering (Fader et al., 2014), knowledge base (KB) population (Soderland et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": ", 2014), knowledge base (KB) population (Soderland et al., 2013), and ontology extension (Moro and Navigli, 2013).", "startOffset": 40, "endOffset": 64}, {"referenceID": 15, "context": ", 2013), and ontology extension (Moro and Navigli, 2013).", "startOffset": 32, "endOffset": 56}, {"referenceID": 2, "context": "Traditional IE requires a pre-defined set of relations, whereas Open IE (Banko et al., 2007) does not.", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "Reverb (Fader et al., 2011) showed that simple parts-of-speech (POS) patterns can cover the majority of relationships.", "startOffset": 7, "endOffset": 27}, {"referenceID": 0, "context": "(2012) and KRAKEN (Akbik and L\u00f6ser, 2012) manually define extraction rules in dependency parse trees.", "startOffset": 18, "endOffset": 41}, {"referenceID": 17, "context": "Methods adopting this second approach include WOE (Wu and Weld, 2010), OLLIE (Mausam et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 13, "context": "Methods adopting this second approach include WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), and ReNoun (Yahya et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 19, "context": ", 2012), and ReNoun (Yahya et al., 2014).", "startOffset": 20, "endOffset": 40}, {"referenceID": 6, "context": "Reverb (Fader et al., 2011) showed that simple parts-of-speech (POS) patterns can cover the majority of relationships. Gamallo et al. (2012) and KRAKEN (Akbik and L\u00f6ser, 2012) manually define extraction rules in dependency parse trees.", "startOffset": 8, "endOffset": 141}, {"referenceID": 19, "context": "As described in Yahya et al. (2014), a noun-mediated relation is an attribute of an argument.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "By considering the shortest dependency path connecting two words, we can concentrate on informative words that are useful for understanding the relation between the two words (Bunescu and Mooney, 2005).", "startOffset": 175, "endOffset": 201}, {"referenceID": 4, "context": "As Christensen et al. (2011) leveraged semantic role labeling (SRL) to find n-ary relations, we used SRL1 to extract highly precise tuples with verbmediated relations2.", "startOffset": 3, "endOffset": 29}, {"referenceID": 13, "context": "Similar to OLLIE (Mausam et al., 2012) and ReNoun (Yahya et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": ", 2012) and ReNoun (Yahya et al., 2014), this augmentation process is based on seed-based distant supervision: if arguments in a seed triple appear in a sentence, their relation is likely to appear in the sentence.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Among the converted triples, we acquire 55K seeds satisfying the following constraints: (1) the arguments are proper nouns or cardinal numbers; (2) arguments with a proper noun are properly linked to entities in DBpedia (Auer et al., 2007); and (3) the lemma of a relation is not \u2018be\u2019 or \u2018do\u2019.", "startOffset": 220, "endOffset": 239}, {"referenceID": 6, "context": "We use DBpedia Spotlight (Daiber et al., 2013) for entity linking.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "We pre-trained word embeddings from the English Wikipedia corpus with the skip-gram model in word2vec (Mikolov et al., 2013).", "startOffset": 102, "endOffset": 124}, {"referenceID": 10, "context": "An RNN with long short-term memory (LSTM) units was first introduced by Hochreiter and Schmidhuber (1997) in order to tackle this problem with an adaptive gating mechanism.", "startOffset": 72, "endOffset": 106}, {"referenceID": 10, "context": "Among the many LSTM variants, we selected LSTM with peephole connections in the spirit of Gers and Schmidhuber (2000). Furthermore, we use both the forward and backward directional recurrent LSTM layer (see Appendix A).", "startOffset": 90, "endOffset": 118}, {"referenceID": 5, "context": "We then convert an arbitrary number of bidirectional output vectors to a path-level feature vector hpath through a max-over-time operation (Collobert et al., 2011).", "startOffset": 139, "endOffset": 163}], "year": 2016, "abstractText": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long shortterm memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.", "creator": "TeX"}}}