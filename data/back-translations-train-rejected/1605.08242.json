{"id": "1605.08242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Neighborhood Sensitive Mapping for Zero-Shot Classification using Independently Learned Semantic Embeddings", "abstract": "In a traditional setting, classifiers are trained to approximate a target function $f:X \\rightarrow Y$ where at least a sample for each $y \\in Y$ is presented to the training algorithm. In a zero-shot setting we have a subset of the labels $\\hat{Y} \\subset Y$ for which we do not observe any corresponding training instance. Still, the function $f$ that we train must be able to correctly assign labels also on $\\hat{Y}$. In practice, zero-shot problems are very important especially when the label set is large and the cost of editorially label samples for all possible values in the label set might be prohibitively high. Most recent approaches to zero-shot learning are based on finding and exploiting relationships between labels using semantic embeddings. We show in this paper that semantic embeddings, despite being very good at capturing relationships between labels, are not very good at capturing the relationships among labels in a data-dependent manner. For this reason, we propose a novel two-step process for learning a zero-shot classifier. In the first step, we learn what we call a \\emph{property embedding space} capturing the \"\\emph{learnable}\" features of the label set. Then, we exploit the learned properties in order to reduce the generalization error for a linear nearest neighbor-based classifier.", "histories": [["v1", "Thu, 26 May 2016 11:53:26 GMT  (192kb)", "http://arxiv.org/abs/1605.08242v1", "16 Pages"]], "COMMENTS": "16 Pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gaurav singh", "fabrizio silvestri", "john shawe-taylor"], "accepted": false, "id": "1605.08242"}, "pdf": {"name": "1605.08242.pdf", "metadata": {"source": "CRF", "title": "Neighborhood Sensitive Mapping for Zero-Shot Classification using Independently Learned Semantic Embeddings", "authors": ["Gaurav Singh", "Fabrizio Silvestri", "John Shawe-Taylor"], "emails": ["gaurav.singh.15@ucl.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.08 242v 1 [cs.L G] 26 MKeywords: Zero-Shot Classification, Textual Data, Property Embeddings, Learning Embeddings, Nearest Neighbor Classification"}, {"heading": "Introduction", "text": "This year it is more than ever before."}, {"heading": "Related Works", "text": "There is active research in the field of zero-shot classification in the recent past. Originally, the problem was defined in its current form by [18], where they address the problem by embedding the set of labels in a semantic space, which is then used to extract information about invisible labels. Considering an object in the data collection, they first predict the set of semantic features implicated (in the embedding of labels), which corresponds to this input, and then find the next class implicated in the labels. Authors also develop a generalization that is limited to errors in the zero-shot classification implicitly implied, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly"}, {"heading": "Proposed Method", "text": "In this section, we will present our two-step approach to zero-shot classification. First, we will describe the method for building learnable properties from semantic representation, and then how to minimize the generalization error in classifying multiple classes based on learned properties. In many cases, data have properties that cannot be learned in a linear way, for example, multi-class image classification has long included nonlinear folding meshes ([8,17]), neural meshes ([3]), and nonlinear SVMs ([1]). In other cases, instances are better classified using linear models. In this paper, we would focus our attention on instances that contain properties that can be learned with a linear model ([5, 16, 20]), especially in a zero-shot scenario."}, {"heading": "Learning Properties", "text": "In fact, it is not the case that one sees oneself in a position to abide by the rules as they have done in the past. (...) It is not the case that one has been able to abide by the rules. (...) It is not the case that one has been able to abide by the rules. (...) It is not the case that one has been able to change the rules. (...) It is the case that one has been able to change the rules. (...) It is not the case that one has been able to abide by the rules. (...) It is not the case that one has been able to change the rules. (...) It is not the case that one has been able to change the rules. (...) () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () ()) () () () () () ()) () () () () () () ()) () () () () ()) () () () ()) () () () () () ()) () () () ()) () () () ()) () () () () ()) () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () (() () () (() () (() () () (() () () (() () () (() () () (() () (() () () () (() ((() (() () (() (() () () ((() () ((() (() () () (("}, {"heading": "Experimental Evaluation", "text": "In this section, we will first describe the different datasets we used in the study, then ask various experimental questions and analyze the results in the light of the questions. Finally, we will discuss the results obtained on two larger tagging datasets."}, {"heading": "Dataset", "text": "We used three sets of data, namely fMRI data sets from [15] (as used in [18]), wiki10 + data sets as described in [26], and delicious data sets as described in [23] for experiments. We performed initial analysis on fMRI data sets and then computed results on wiki10 + data sets. - The fMRI data set is composed of neuronal activity observed by nine human participants while looking at 60 different concrete words. These 60 words are divided into 12 different categories, such as animals: bear, dog, cat, cow, horse and vehicles: truck, car, train, airplane, bicycle. Each participant received a word and a small line drawing of the concrete object representing the word. Participants were asked to think about the properties of these objects for several seconds while scans of their brain activity were recorded. Each sample measures the neural data sets at approximately 20,000 locations in the brain. Six fMRI data sets were generated for each word."}, {"heading": "Experiments", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "50 0.3590 0.0521 0.0825", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.1344 0.0392 0.0472", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.0685 0.0261 0.0283", "text": "In the end, we tested NSM-PB against LM and ConSE for 2 much larger datasets called wiki10 + ([26]) and another tagging dataset called delicious [23]. The wiki10 + dataset contains 20K + instances with more than 5000 unique labels. We created 100 different test strips with each test set consisting of 100 zero shot labels, while the pull set consists of all other labels. The classifier had to select the correct label from the set of all possible labels, that is, the classifier had no prior knowledge of whether or not a test instance was a zero shot label, which is close to the real life situation of a classifier. As a given Url can indeed have multiple correct labels, we decided to test the accuracy of the method with the predicted label x."}, {"heading": "5 0.0509 0.0320 0.0433", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.0924 0.0492 0.0822", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "20 0.1486 0.0721 0.1350", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Reproducibility", "text": "The codes used for the experiments are shared at http: / / bit.ly / 1RCNlwR. The values of \u03b1 and \u03bb in Equation (1), which provide the best results, are 0.1 and 0.5, respectively. In the case of ConSE, we use a multi-class regression classifier to predict the class probabilities. The values of parameter T (i.e. the number of closest embeddings of top T for a given instance) in ConSE, which gave the best result, were 5. The dimensions of the learned property embeddings in the experiments were 10."}, {"heading": "Conclusion", "text": "Most of these systems use semantic embeddings to learn the correlation between different embeddings. Recently, there has been some progress in the field of semantic embeddings, which has led to great popularity and ease of availability of pre-formed semantic embeddings. In this paper, we will use these pre-formed semantic embeddings to improve a linear zero-shot classifier in a multi-class classification framework. First, we will illustrate the problems with linear zero-shot classification systems that use semantic embeddings. We will also show problems of classifiers that are not sensitive to the neighborhood of a label in semantic space. Then, we will develop insights into the extraction of property embeddings that correspond better with learnable features in the data. We will show that NSM and LM both work better when supplied with such property vectors instead of sector-based vectors. Furthermore, we will show that we better integrate property-based PM and NSM on the annexes."}, {"heading": "Future Work", "text": "In this paper we have observed that re-learning semantic embedding can really improve the quality of classification results. Although we limited the analysis in this paper to the extraction of linear embedding of properties, in the future we will expand the work towards embedding properties from nonlinear data with maximum margin of nonlinear classification. We will also create a multi-core version of the proposed approach. We believe that this would drastically improve the quality of results for complex data with multiple nonlinearities."}], "references": [{"title": "Support vector ma- chines for histogram-based image classification", "author": ["Olivier Chapelle", "Patrick Haffner", "Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transac- tions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Pac nearest neighbor queries: Approximate and controlled search in high-dimensional and metric spaces", "author": ["Paolo Ciaccia", "Marco Patella"], "venue": "In Data Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Ciresan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Introducing a weighted non- negative matrix factorization for image classification", "author": ["David Guillamet", "Jordi Vitria", "Bernt Schiele"], "venue": "Pattern Recognition Letters,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Efficient max-margin multi-label classification with applications to zero-shot learning", "author": ["Bharath Hariharan", "SVN Vishwanathan", "Manik Varma"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Zero-shot recognition with unreliable attributes", "author": ["Dinesh Jayaraman", "Kristen Grauman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information pro- cessing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "Pattern Analysis and Ma- chine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Max-margin zero-shot learning for multi-class classi- fication", "author": ["Xin Li", "Yuhong Guo"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Semi-supervised zero-shot classifica- tion with label representation learning", "author": ["Xin Li", "Yuhong Guo", "Dale Schuurmans"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Co-occurrence statistics for zero-shot classification", "author": ["Thomas Mensink", "Efstratios Gavves", "Cees GM Snoek. Costa"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Dis- tributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Tom M Mitchell", "Svetlana V Shinkareva", "Andrew Carlson", "Kai-Min Chang", "Vi- cente L Malave", "Robert A Mason", "Marcel Adam Just"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Probabilistic matrix factorization", "author": ["Andriy Mnih", "Ruslan Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Zero- shot learning with semantic output codes", "author": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Relative attributes", "author": ["Devi Parikh", "Kristen Grauman"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["Marcus Rohrbach", "Michael Stark", "Bernt Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["Bernardino Romera-Paredes", "PHS Torr"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Effective and ef- ficient multilabel classification in domains with large number of labels", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": "In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Attribute dominance: What pops out", "author": ["Naman Turakhia", "Devi Parikh"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Designing category-level attributes for discriminative visual recognition", "author": ["Felix X Yu", "Liangliang Cao", "Rog\u00e9rio Schmidt Feris", "John R Smith", "Shih-Fu Chang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Enhancing navigation on wikipedia with social tags", "author": ["Arkaitz Zubiaga"], "venue": "arXiv preprint arXiv:1202.5469,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "setting [18], we are given a subset \u0176 \u2282 Y of the labels for which we do not observe any corresponding training instance.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Based on the definitions given by [18], we address the following general research question: \u201cGiven a semantic encoding of a large set of concept classes, can we build a classifier to recognize classes that were omitted from the training set?\u201d", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "[18] propose a method for zero-shot learning using a linear model based on creating semantic embeddings for words based on co-occurrences of labels in dictionaries and based on human feedback about labels properties.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The dramatic progress made in the area of semantic embeddings like word2vec ( [14]) have provided a method for encoding the semantic meaning into words.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "Secondly, we need to develop a neighborhood sensitive mapping that can help reduce the risk of error in the case of nearest neighbor based classifier for zero-shot classification as used by [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 17, "context": "Originally, the problem was defined in its current form by [18], where they address the problem by embedding the set of labels into a semantic space used then to extrapolate information about unseen labels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "In a following study, [6] propose a multi-label max-margin classifier with applications to zero-shot.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "[10] develop one such approach specifically based on learning attribute for animals (like color, eating habits etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] develop an approach specifically targeting images that focuses on exploiting co-occurrences of visual concepts in images for knowledge transfer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] propose an approach for zero-shot classification where image attributes are unreliable and use the error tendencies of the different attributes to develop a linear discriminant model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 125, "endOffset": 128}, {"referenceID": 18, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 130, "endOffset": 146}, {"referenceID": 19, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 130, "endOffset": 146}, {"referenceID": 22, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 130, "endOffset": 146}, {"referenceID": 23, "context": "There have been many such attribute based learning methods for visual recognition that have been developed in the past: [9], [4], [19, 21, 24, 25], to name a few.", "startOffset": 130, "endOffset": 146}, {"referenceID": 13, "context": "A general methodology to learn word embeddings is presented by [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "By using semantic embeddings learned using word2vec, [17] proposed ConSE a zero-shot image classification system specifically tested on image classification.", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "[22] proposed a linear regressor for zero-shot classification that also relies on independently learned semantic data, it is very similar to [18] in practise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[22] proposed a linear regressor for zero-shot classification that also relies on independently learned semantic data, it is very similar to [18] in practise.", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "[12] learns label representations from scratch without using independently generated semantic embeddings, which differentiates it from our work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] propose a max-margin multi-class zero-shot classifier with the assump-", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In this experiment, we test the ability of NSM-PB to distinguish between two novel classes as measured in [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "multi-class image classification has long involved non-linear convolution nets ( [8,17]), neural nets ( [3]), and non-linear SVMs ( [1]).", "startOffset": 81, "endOffset": 87}, {"referenceID": 16, "context": "multi-class image classification has long involved non-linear convolution nets ( [8,17]), neural nets ( [3]), and non-linear SVMs ( [1]).", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "multi-class image classification has long involved non-linear convolution nets ( [8,17]), neural nets ( [3]), and non-linear SVMs ( [1]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "multi-class image classification has long involved non-linear convolution nets ( [8,17]), neural nets ( [3]), and non-linear SVMs ( [1]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "In this work, we would focus our attention on instances that contain properties which can be learned using a linear model ( [5, 16, 20]), specifically in a zero-shot scenario.", "startOffset": 124, "endOffset": 135}, {"referenceID": 15, "context": "In this work, we would focus our attention on instances that contain properties which can be learned using a linear model ( [5, 16, 20]), specifically in a zero-shot scenario.", "startOffset": 124, "endOffset": 135}, {"referenceID": 17, "context": "[18] developed a novel generalization bound for zero shot classification using a linear classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "They used the analysis developed by [2] for nearest neighbor classifiers.", "startOffset": 36, "endOffset": 39}, {"referenceID": 17, "context": "They develop an upper bound (Equation (2) in [18]) on the accuracy of a linear zero-shot classifier using Gq, which is defined as follows: Gq(\u03c4q) = 1\u2212 (1\u2212Rq(\u03c4q)) n", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "Differently from [18], we start from the upper bound to the generalization error (see Equation(2) in [18]) and instead proceed to minimize Gq as it directly contributes to the generalization error.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Differently from [18], we start from the upper bound to the generalization error (see Equation(2) in [18]) and instead proceed to minimize Gq as it directly contributes to the generalization error.", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "We used three datasets, namely, fMRI dataset from [15] (as used in [18]), wiki10+ dataset as described in [26] and delicious dataset as described in [23] for experiments.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "We used three datasets, namely, fMRI dataset from [15] (as used in [18]), wiki10+ dataset as described in [26] and delicious dataset as described in [23] for experiments.", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "We used three datasets, namely, fMRI dataset from [15] (as used in [18]), wiki10+ dataset as described in [26] and delicious dataset as described in [23] for experiments.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "We used three datasets, namely, fMRI dataset from [15] (as used in [18]), wiki10+ dataset as described in [26] and delicious dataset as described in [23] for experiments.", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "We also used the same time-averaging described in [15, 18] to create a single average brain activity pattern for each of the 60 words, for each participant.", "startOffset": 50, "endOffset": 58}, {"referenceID": 17, "context": "We also used the same time-averaging described in [15, 18] to create a single average brain activity pattern for each of the 60 words, for each participant.", "startOffset": 50, "endOffset": 58}, {"referenceID": 24, "context": "\u2013 The wiki10+ dataset from [26] contains text of wikipedia articles and the tags assigned by users on delicious.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "\u2013 The delicious dataset [23] contains features of web pages from all over the Internet with tags generated by users on those web pages as the labels.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "The semantic embeddings used in this work are 300 dimensional trained using word2vec ( [14]) on google news dataset.", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "In these experiments, we refer to our method as NSM, that stands for Neighborhood Sensitive Mapping, [18] is referred to as LM and [17] is referred to as ConSE.", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "In these experiments, we refer to our method as NSM, that stands for Neighborhood Sensitive Mapping, [18] is referred to as LM and [17] is referred to as ConSE.", "startOffset": 131, "endOffset": 135}, {"referenceID": 17, "context": "We evaluated the proposed approach on three different research questions and compared our results to the methods in [18] and [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "We evaluated the proposed approach on three different research questions and compared our results to the methods in [18] and [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "In the end, we tested NSM-PB against LM and ConSE on 2 much bigger datasets called wiki10+ ( [26]) and another tagging dataset called delicious [23].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "In the end, we tested NSM-PB against LM and ConSE on 2 much bigger datasets called wiki10+ ( [26]) and another tagging dataset called delicious [23].", "startOffset": 144, "endOffset": 148}], "year": 2016, "abstractText": "In a traditional setting, classifiers are trained to approximate a target function f : X \u2192 Y where at least a sample for each y \u2208 Y is presented to the training algorithm. In a zero-shot setting we have a subset of the labels \u0176 \u2282 Y for which we do not observe any corresponding training instance. Still, the function f that we train must be able to correctly assign labels also on \u0176 . In practice, zero-shot problems are very important especially when the label set is large and the cost of editorially label samples for all possible values in the label set might be prohibitively high. Most recent approaches to zero-shot learning are based on finding and exploiting relationships between labels using semantic embeddings. We show in this paper that semantic embeddings, despite being very good at capturing relationships between labels, are not very good at capturing the relationships among labels in a datadependent manner. For this reason, we propose a novel two-step process for learning a zero-shot classifier. In the first step, we learn what we call a property embedding space capturing the \u201clearnable\u201d features of the label set. Then, we exploit the learned properties in order to reduce the generalization error for a linear nearest neighbor-based classifier.", "creator": "LaTeX with hyperref package"}}}