{"id": "1605.08325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Theano-MPI: a Theano-based Distributed Training Framework", "abstract": "We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning", "histories": [["v1", "Thu, 26 May 2016 15:13:46 GMT  (127kb,D)", "http://arxiv.org/abs/1605.08325v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["he ma", "fei mao", "graham w taylor"], "accepted": false, "id": "1605.08325"}, "pdf": {"name": "1605.08325.pdf", "metadata": {"source": "CRF", "title": "Theano-MPI: a Theano-based Distributed Training Framework", "authors": ["He Ma", "Fei Mao", "Graham W. Taylor"], "emails": ["hma02@uoguelph.ca", "gwtaylor@uoguelph.ca", "feimao@sharcnet.ca"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 Related Work", "text": "The idea is that people are able to decide for themselves what they want and what they want."}, {"heading": "3 Implementation", "text": "Our goal is to make the field of distributed deep learning more accessible by developing a scalable educational framework with two key components: firstly, Theano as a means of constructing an architecture and optimizing it through Stochastic Gradient Descent (SGD); secondly, Massage Passing Interface (MPI) as an in-process parameter exchanger; and, secondly, exploring various ways to reduce the communication burden associated with parallel SGD and identify some phenomena that affect convergence and acceleration when we train deep learning models in a distributed framework."}, {"heading": "3.1 The BSP Structure", "text": "Figure 1a shows a 4-GPU example of the proposed BSP structure, in which the same model is built and executed within four processes: P0, P1, P2, P3. Each process uses a CPU and a GPU. After the training graph of the model is compiled on the GPU, these parameters in the diagram become arrays in the GPU memory, whose values can be fetched from the device to the host and set from the host to the device. At the beginning of the training, the training data set is split into four parts. In each iteration, each work process takes a mini-batch of examples from its share and then performs SGD. Then, all workers are synchronized and the model parameters are exchanged between work processes in a collective way."}, {"heading": "3.2 CUDA-aware Parameter Exchanging", "text": "GPUDirect P2P technology enables parameters to be exchanged between GPUs without traversing the host memory, making MPI functions \"CUDA-aware.\" Based on this, we have explored various strategies that attempt to minimize data transfer and computation time, and have made more efficient use of QPI, PCIe, and network card bandwidth during data transfer. The basic strategy is to use the MPI Allreduce () function. The CUDAaware version of this in OpenMPI 1.8.7 does not make much improvement, as any collective MPI function with arithmetic operations still needs to copy data into the host memory. Functions such as Alltoall () and Allgather () do not include arithmetic, and therefore the CUDA-aware version of them (Fig. 1b) can avoid passing data through the host memory."}, {"heading": "3.3 Parallel Loading", "text": "For major visual detection applications such as ImageNet LSVRC, the data required for training is in the order of hundreds of gigabytes. Therefore, it is difficult to load all image data fully into memory after the training has started. Instead, the images are stored as batch files on local or remote hard drives and a file is loaded at a point in time when the data can be loaded from the hard drive. It is affected by various factors including file size, file format and network bandwidth when read from the remote hard drives."}, {"heading": "4 Benchmarking", "text": "In fact, it is true that most of them are in a position to trump themselves, both in the USA and in Europe. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5 Hardware and Software Environment", "text": "As shown in Fig. 6, each node in the cluster is a dual-socket system with two NVIDIA Tesla K80 GPUs on each socket, and the entire cluster is networked with Mellonox Infiniband FDR. We also tested on another cluster, Mosaic, which has distributed GPUs across nodes connected by Infiniband QDR. Each node has an NVIDIA K20m GPU. For high-level access to MPI functionality, we use its Python binding mpi4py, which was compiled against OpenMPI 1.8.7. All models mentioned in this report are designed in Theano 0.8, and their implementation is available in our Gift Subproject. Convolution and pooling operations in the computational graph depend on CUDA 7.0 and the cuDNN v4 library. We also support cudaconnet as an alternative backend."}, {"heading": "6 Discussion", "text": "The main technical characteristics of our framework are more efficient communication strategies between processes and parallel data loading techniques. Factors that influence frame acceleration can be linked to the model to be trained (i.e. architectural), training data loading strategy, computing curve synchronization, implementation of GPU cores, system memory and network bandwidth. Importantly, we do not seek to compromise the convergence of models trained in our framework, as measured speedup is based on the time used to achieve a certain error rate. However, the convergence achieved by a parallel framework also depends on the tuning of the bandwidth of that framework. Therefore, the convergence results in Table 1 can be improved if better hyper parameters are found. Factors that affect model convergence include the number of work processes, batch effective size, and learning parameters."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z Chen"], "venue": "ArXiv eprints", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N Wang"], "venue": "CoRR abs/1512.01274", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": "CoRR abs/1412.7024", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M Devin"], "venue": "Advances in Neural Information Processing Systems 25, pp. 1232\u20131240", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano-based large-scale visual recognition with multiple gpus", "author": ["W. Ding", "R. Wang", "F. Mao", "G.W. Taylor"], "venue": "CoRR abs/1412.2302", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR abs/1512.03385", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J Kim"], "venue": "Advances in Neural Information Processing Systems 26, pp. 1223\u20131231. Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Firecaffe: nearlinear acceleration of deep neural network training on compute clusters", "author": ["F.N. Iandola", "K. Ashraf", "M.W. Moskewicz", "K. Keutzer"], "venue": "CoRR abs/1511.00175", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J Long"], "venue": "CoRR abs/1408.5093", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R Sukthankar"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards effective use of training data in statistical machine translation", "author": ["P. Koehn", "B. Haddow"], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 317\u2013321. WMT \u201912, Association for Computational Linguistics, Stroudsburg, PA, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "CoRR abs/1404.5997", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Purine: A bi-graph based deep learning framework", "author": ["M. Lin", "S. Li", "X. Luo", "S. Yan"], "venue": "CoRR abs/1412.6249", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Computer Vision \u2013 ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P Perona"], "venue": "Springer International Publishing, Cham", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Developing a Scalable Deep Learning Framework Based on MPI", "author": ["H. Ma"], "venue": "Master\u2019s thesis, University of Guelph, Guelph, ON, CA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S Satheesh"], "venue": "International Journal of Computer Vision 115(3), 211\u2013252", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "Reed", "S.E"], "venue": "CoRR abs/1409.4842", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints abs/1605.02688", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A Bridging Model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM 33(8), 103", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28, pp. 685\u2013693. Curran Associates, Inc.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Object recognition [20], is now dominated by deep learning methods, which in many cases, rival human performance.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Recent success in areas such as activity recognition from video [13] and statistical machine translation [14] is an example of deep learning\u2019s ascent both in performance and at scale.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Recent success in areas such as activity recognition from video [13] and statistical machine translation [14] is an example of deep learning\u2019s ascent both in performance and at scale.", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "VGGNet [21]) and models that are as deep as 150 layers (c.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "ResNet [9]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "ImageNet [20] and MS-COCO [18], challenges artificial intelligence research and leads us to design deeper and more expressive models so that the complexity of models is sufficient for the task.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "ImageNet [20] and MS-COCO [18], challenges artificial intelligence research and leads us to design deeper and more expressive models so that the complexity of models is sufficient for the task.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Theano [23] is an open-source Python library for developing complex algorithms via mathematical expressions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "This includes the multi-GPU version of Caffe (FireCaffe [11]), Torch and Theano (Platoon).", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "To accelerate the training of a speech recognition model on distributed CPU cores, DownPour, an asynchronous parameter exchanging method [6], was proposed.", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "It was later found that controlling the maximum staleness of parameter updates received by the server leads to faster training convergence [10] on problems like topic modeling, matrix factorization and lasso regression compared to a purely asynchronous approach.", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "For accelerating image classification on the CIFAR and ImageNet datasets, an elastic averaging strategy between asynchronous workers and the server was later proposed [25].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "Krizhevsky proposed his trick on parallelizing the training of AlexNet [16] on multiple GPUs in a synchronous way [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Krizhevsky proposed his trick on parallelizing the training of AlexNet [16] on multiple GPUs in a synchronous way [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "Following his work, a Theano-based twoGPU synchronous framework [7] for accelerating the training of AlexNet was proposed, where both weights and momentum are averaged between two GPUs after each iteration.", "startOffset": 64, "endOffset": 67}, {"referenceID": 16, "context": "Purine [17] pipelines the propagation of gradients between iterations and overlaps the communication of large weights in fully connected layers with the rest of back-propagation, giving near 12\u00d7 data throughput speedup when training GoogLeNet [22] on 12 GPUs.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Purine [17] pipelines the propagation of gradients between iterations and overlaps the communication of large weights in fully connected layers with the rest of back-propagation, giving near 12\u00d7 data throughput speedup when training GoogLeNet [22] on 12 GPUs.", "startOffset": 243, "endOffset": 247}, {"referenceID": 1, "context": "Similarly, MXNet [2] also shows a super-linear data throughput speedup on training GoogLeNet under a distributed training setting.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "Bulk Synchronous Parallel (BSP) [24] is an intuitive way to implement parallel computing.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "Using low precision data types for weights or activations (or both) in the forward pass during training of deep neural networks has received much recent interest [4,5].", "startOffset": 162, "endOffset": 167}, {"referenceID": 4, "context": "Using low precision data types for weights or activations (or both) in the forward pass during training of deep neural networks has received much recent interest [4,5].", "startOffset": 162, "endOffset": 167}, {"referenceID": 7, "context": "It was shown that training Maxout [8] networks at 10 bits fixed point precision can still yield near state-of-art test accuracy [4].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "It was shown that training Maxout [8] networks at 10 bits fixed point precision can still yield near state-of-art test accuracy [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "Due to the limitation imposed by the Global Interpreter Lock (GIL) in Python, overlapping the communication with the gradient calculation as in [17] has not yet been implemented in our framework.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "Different from the multiprocessing and Queue messaging method in [7], we used the MPI Spawn function to start a child process from each training process and used the resulting MPI intra-communicator to pass messages between the training process and its child process.", "startOffset": 65, "endOffset": 68}, {"referenceID": 14, "context": "Averaging weights after gradient descent (AWAGD) [15,7] is a straightforward parallel SGD scheme.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "Averaging weights after gradient descent (AWAGD) [15,7] is a straightforward parallel SGD scheme.", "startOffset": 49, "endOffset": 55}, {"referenceID": 18, "context": "We have proved [19] that training a perceptron using this", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In this scheme, the learning rate is scaled with the number of GPUs used [15], namely k.", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "Recent work has applied low precision to weights and activations during training [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In the extreme, binary data types have been considered [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 21, "context": "The top-5 error is taken from [22].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "9 Tested on the ILSVRC14 dataset [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Referencing the implementation of EASGD in Platoon, a Theano-based multi-GPU framework that exploits data parallelism, we re-implemented the framework based on the CUDA-aware MPI SendRecv() function without the Round-Robin scheme [25].", "startOffset": 230, "endOffset": 234}], "year": 2016, "abstractText": "We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning.", "creator": "LaTeX with hyperref package"}}}