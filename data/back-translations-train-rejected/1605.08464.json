{"id": "1605.08464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance", "abstract": "We propose a low cost and effective way to combine a free simulation software and free CAD models for modeling human-object interaction in order to improve human &amp; object segmentation. It is intended for research scenarios related to safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial domain. The task of human and object modeling has been used for detecting activity, and for inferring and predicting actions, different from those works, we do human and object modeling in order to learn interactions in RGB-D data for improving segmentation. For this purpose, we define a novel density function to model a three dimensional (3D) scene in a virtual environment (VREP). This density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances. Using this function, we synthesize a large, realistic and highly varied synthetic RGB-D dataset that we use for training. We train a random forest classifier, and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields (CRF). Our evaluation shows that modeling these interactions improves segmentation performance by ~7\\% in mean average precision and recall over state-of-the-art methods that ignore these interactions in real-world data. Our approach is computationally efficient, robust and can run real-time on consumer hardware.", "histories": [["v1", "Thu, 26 May 2016 22:34:37 GMT  (2006kb,D)", "http://arxiv.org/abs/1605.08464v1", "accepted for publication at 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2016"]], "COMMENTS": "accepted for publication at 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.HC cs.RO", "authors": ["vivek sharma", "sule yildirim-yayilgan", "luc van gool"], "accepted": false, "id": "1605.08464"}, "pdf": {"name": "1605.08464.pdf", "metadata": {"source": "CRF", "title": "Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance", "authors": ["Vivek Sharma", "Luc Van Gool"], "emails": ["luc.vangool}@esat.kuleuven.be", "vangool@vision.ee.ethz.ch", "sule.yildirim@ntnu.no"], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "II. RELATED WORK", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "III. DATASET GENERATION USING A DENSITY FUNCTION", "text": "We used a virtual robot experimentation platform (VREP) [5] (fig. 2 and fig. 3) for modeling and dataset generation. VREP is a robot simulator with an embedded application for a robot in an integrated development environment (IDE) with potential field tasks without being physically dependent on the actual robot. VREP robot simulator can be used for 3D modeling and synthetic data generation. VREP simulator provides a virtual environment that is able to emulate realistic motion generation, translation, rotation of actual objects in the IDE. Each object / model can be controlled individually via a remote API client, making it an ambidextrous-like control architecture. It supports C / C + +, Python, Java, Lua, Matlab, Octave or Urbi. It is also publicly available for hobby academics, and research purposes."}, {"heading": "IV. ALGORITHM", "text": "Fig. 3 shows the schematic layout of our proposed hierarchical segmentation system. The synthesized RGB-D training dataset contains modeled H \u2212 O and O \u2212 O relationships and interactions achieved by means of density-based scene modeling. In the first step, the sampling, i.e. the number of frames and characteristics per object class are randomly selected to train an RDF classifier. Subsequently, individual extracted characteristics corresponding to an object class are passed on to the RDF classifier. The result is a trained classification forest in which each leaf node represents the class prediction of a tree. Now, a test depth map of the scene obtained from the real KINECT sensor is given as an input into the trained classification forest. The result is a pixel-by-type object class labeling. The probability of an object label assigned to a pixel from the classifier is indefinitely assigned to the CRF."}, {"heading": "A. Additive white gaussian noise in synthetic depth data", "text": "The synthetic depth data of the KINECT virtual sensor in the scene generally does not contain noise. Real-world sensing is usually a combination of ideal signal and noise. Noise generation may be due to a large number of sources, variations in detector sensitivity, environmental variations, the discrete nature of radiation, transmission or quantization errors, geometry-specific missing data due to shadows in the IR image of the object, discretization errors and noise gains with increasing depth, all of which add up to a noise model. Due to the noise in the real data and to handle invisible data samples more robustly in the test step, which means increasing the sound capability of the trained classifier, we add Gaussian white noise to the depth values (Fig. 4)."}, {"heading": "B. Feature Selection", "text": "We use the same depth characteristics as in [4]. For a given pixel position s of an object sample O from its depth map, we designate its depth value as mapping dO (s) and design a characteristic fO (s) by using two 2D offset positions u, v from s: fO (s) = dO (s + u) \u2212 dO (s + v) (5) The characteristic is depth invariant. We use a rectangular patch to extract depth values from an object sample. We calculate a fixed number of 300 characteristics for each object sample."}, {"heading": "C. Classifier", "text": "Choosing a good discrimination classifier is independent of the preceding sections, because our goal is to show that modeling using the density function helps improve segmentation performance in the real world. In this case, we use a Random Decision Forests (RDF) classifier for pixel-by-pixel object classification. The feature response function plays the most important and critical role in both training and testing random forests. In the internal node, the parameters of the split function are selected associated with each split node by optimizing a selected lens function defined on the training dataset. The lens function is based on maximizing information gain. The geometric primitives of the split function are used to partition data points. We use two geometric primitives, axis-oriented child criteria that were used during training to optimize node trees."}, {"heading": "D. Conditional Random Fields", "text": "The energy of the paired CRFs used for segmentation of the object class can be defined as the sum of simple and paired potential terms defined as: E (x) = \u2211 i (pi), j (pi, pj), (6), each vertex corresponding to the vertex set of a 2D grid, each vertex corresponding to the pixel p in the image and the neighborhood of the pixels i, j. x is an arbitrary label. The unary potential (pi) of the term in the CRF energy is a data term. It is the probability of the object name associated with the pixel i. Here, the unary term is determined from the RDF classifier for each pixel optimizer belonging to the object class. The PC's paired potential (pi), j (pi, pj) term in the CRF energy is encoded by the flexibility and the muglateness of the term."}, {"heading": "VI. EXPERIMENTAL EVALUATION", "text": "The aim of this work is to improve segmentation performance in real-world scenarios. Therefore, we only use real data, a random number of 65 depth maps, for evaluation. We obtain RGB-D data from a real-world RGB-D sensor placed on the ceiling in the middle of our shared workspace. See Figure 7 (Column 1) for real depth charts. We evaluate the impact on segmentation performance in three cases: (Section VI-B) what amount of noise synthetic data should be added to bring the synthetic data into line with the real world; (Section VI-C) the effect of the splitting function of RDF on classification performance; (Section VID) the comparison between modeled and non-modeled training data sets (i.e. with and without density function) by training an RDF classifier when evaluated in the real world; (Section VID) the comparison between modeled and non-modeled training data sets (i.e., with and without density function) by evaluating an RDF classifier on the real world, in order to evaluate the effect of the modeling of the object actions (the version of the human and the two variants)."}, {"heading": "A. Data Collection", "text": "A scene is a single frame in which there is a single object or a combination of several 3D objects arranged and oriented in a particular configuration, based on the density function. The scenes consist of complex configurations of people and objects. We generated an extensive synthetic data set of RGB-D data using 3D scene models of 4 different rooms based on industrial workspaces and office areas with a total of 10 object classes in the virtual environment [5] (see fig. 4). In the case of training we use exclusively synthetic depth data, and in the case of the evaluation stage test data we use synthetic and real depth data. The object classes are: human body parts (head, body, upper arm, forearm, hand and legs), table, chair, plant and storage. We generated the synthetic human body parts data using a real KINECT multisensor device with a 3D multi-coloured human model in the virtual environment."}, {"heading": "B. Noise Evaluation", "text": "Additive white Gaussian noise using a standard deviation (\u03c3) has been added to the synthetic training depth data to compensate for real-world noise by aligning the camera outputs as closely as possible and showing approximately good segmentation. Figure 5 (column 1) suggests that using this setting to generate synthetic data yields the best performance when using synthetic depth images with additive white Gaussian noise using a standard deviation of 15 cm."}, {"heading": "C. Feature Response Evaluation", "text": "The results in Fig. 5 (column 2) show that most increases occur in the linear feature response and are about 10% higher than in the axial feature response. The qualitative results from the axial feature response yielded exaggerated predictions, while they were also visually more satisfactory in the linear feature response."}, {"heading": "D. Comparison of Models", "text": "In order to achieve consistent modeling of H-O and O-O interactions, we preferred the occlusion of boundaries in order to obtain the most accurate possible relevance to the real world. This relationship between objects is based on a Euclidean threshold. We used a rank-based approach to the degree to which occlusion should be allowed, both in quantitative and qualitative terms. We found that test predictions rise monotonously up to 30 as the boundary overlap increases, and that shortly thereafter there is a sharp decrease in quantitative and qualitative performance. Therefore, we preferred the development of systems with boundary overlap, and our dataset was synthesized."}, {"heading": "E. Demonstration", "text": "Figure 7 (lines 2-3) shows the predictive results for the proposed CRF model based on real-world test data. Table I shows that the F1 measurement for the CRF model improves by about 8% compared to the non-modelled models [18], [19]. Training the model with our optimized RDF parameter setup takes 43 minutes, and testing the model takes 34 ms. Each block of our pipeline (i.e. scenario modelling using density function, fine-tuning of additive noise in the synthetic data, selecting the linear feed response for RDF tree training, modelling the labelling problem on a CRF to minimize label misation) contributes to the good results and plays a crucial role in improving segmentation performance. The resulting system is computerized, efficient, robust and supports real-time for our targeted application."}, {"heading": "VII. CONCLUSION", "text": "In this thesis, we have described a cost-effective and effective method for modeling the interaction between human and object and object to improve segmentation performance. To this end, we have proposed a density function for modeling a 3D scene in a virtual environment to synthesize a dataset that includes human-object and object-object interactions that are consistent with real scenarios. Our proposed density functions model the spatial arrangement of object, object position, object orientation, object arrangement, object interaction, and relationships between object classes, i.e. no, partial, and complete occlusion from each other. Our experiments are based on industrial workspaces and office areas with a total of 10 object classes. Our goal of modeling scenes using density function improves real-world segmentation performance by an average of 7% with medium precision, reminiscent of state-of-of-the-art, non-modeling segmentation methods."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank Jose'Oramas M, Michele Fenzi, Frank Dittrich and Stephan Irgenfried for their valuable suggestions."}], "references": [{"title": "Categorizing object-action relations from semantic scene graphs", "author": ["E.E. Aksoy", "A. Abramov", "F. Worgotter", "B. Dellen"], "venue": "In ICRA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Probabilistic modeling of realworld scenes in a virtual environment", "author": ["F. Dittrich", "S. Irgenfried", "H. W\u00f6rn"], "venue": "In International Conference on Computer Graphics Theory and Applications (GRAPP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Pixelwise object class segmentation based on synthetic data using an optimized training strategy", "author": ["F. Dittrich", "V. Sharma", "H. W\u00f6rn", "S. Yayilgan"], "venue": "ICNSC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Virtual robot experimentation platform v-rep: A versatile 3d robot simulator", "author": ["M. Freese", "S.P.N. Singh", "F. Ozaki", "N. Matsuhira"], "venue": "SIMPAR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Real time motion capture using a single time-of-flight camera", "author": ["V. Ganapathi", "C. Plagemann", "D. Koller", "S. Thrun"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Observing human-object interactions: Using spatial and functional compatibility for recognition", "author": ["A. Gupta", "A. Kembhavi", "L.S. Davis"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Framework for generation of synthetic ground truth data for driver assistance applications", "author": ["V. Haltakov", "C. Unger", "S. Ilic"], "venue": "In Pattern Recognition, Lecture Notes in Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Putting objects in perspective", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning object arrangements in 3d scenes using human context", "author": ["Y. Jiang", "M. Lim", "A. Saxena"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Structured class-labels in random forests for semantic image labelling", "author": ["P. Kontschieder", "S. Rota Bul\u00f3", "H. Bischof", "M. Pelillo"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Decision tree fields", "author": ["S. Nowozin", "C. Rother", "S. Bagon", "T. Sharp", "B. Yao", "P. Kohli"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Functional object descriptors for human activity modeling", "author": ["A. Pieropan", "C.H. Ek", "H. Kjellstrom"], "venue": "In ICRA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Some generalized order-disorder transformations", "author": ["R. Potts", "C. Domb"], "venue": "Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1952}, {"title": "Recognition using visual phrases", "author": ["M.A. Sadeghi", "A. Farhadi"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient real-time pixelwise object class labeling for safe human-robot collaboration in industrial domain", "author": ["V. Sharma", "F. Dittrich", "S. Yayilgan", "L.V. Gool"], "venue": "In ICML Workshops,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Improving human pose recognition accuracy using crf modeling", "author": ["V. Sharma", "F. Dittrich", "S. Yayilgan", "L. Van Gool"], "venue": "In CVPR Workshops,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Efficient human pose estimation from single depth", "author": ["J. Shotton", "R.B. Girshick", "A.W. Fitzgibbon", "T. Sharp", "M. Cook", "M. Finocchio", "R. Moore", "P. Kohli", "A. Criminisi", "A. Kipman", "A. Blake"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "In AAAI Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Auto-context and its application to high-level vision tasks", "author": ["Z. Tu"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Testing of image processing algorithms on synthetic data", "author": ["K. von Neumann-Cosel", "E. Roth", "D. Lehmann", "J. Speth", "A. Knoll"], "venue": "In International Conference on Software Engineering Advances,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "ware (VREP [5]) which is freely available.", "startOffset": 11, "endOffset": 14}, {"referenceID": 21, "context": "In [22], Neumann-Cosel et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "[8] proposes a framework for generation of synthetic data from a realistic driving simulator to create modeled traffic scenarios similar to real-world scenarios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "A number of works uses synthetic human data for pose and activity recognition [3], [4], [6], [10], [18], [19], [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "In [19], Shotton et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [4], [18], the authors use a simple synthetic human body representation in a virtual environment using a KINECT skeleton estimation for generating synthetic human pose data.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "In [4], [18], the authors use a simple synthetic human body representation in a virtual environment using a KINECT skeleton estimation for generating synthetic human pose data.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "In [19], the data is generated based on front, top, and side view, while in [4], [18], it is top view only.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [19], the data is generated based on front, top, and side view, while in [4], [18], it is top view only.", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "In [19], the data is generated based on front, top, and side view, while in [4], [18], it is top view only.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "In [9], Hoiem et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "[19] in the following aspects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "(a) In [19], all training data were thereby synthetically generated by using", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "tracking in real-world [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "In [10], Jiang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In [10], a camera is installed in the virtual environment, in the corner of the room, where two walls and the ceiling meet, or where a wall and the ceiling meet.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "Similar is done in [3], where Dittrich et al.", "startOffset": 19, "endOffset": 22}, {"referenceID": 18, "context": "[19] and Sharma et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], and we define the density function in VREP [5] in order to model object-object and human-object relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[18], and we define the density function in VREP [5] in order to model object-object and human-object relationships.", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "[19] and Sharma et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] only focuses on a single 3D object in a scene.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], Aksoy et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], and Pieropan et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] use the spatial relationship to perform activities recognition where humanobject and object-object relationships are encoded.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], \u201cDecision", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] or \u201cStructured ClassLabels in Random Forests for Semantic Image Labelling\u201d by Kontschieder et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], among others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Object classes from industrial domain is considered as the subset of object constellations which are consistent to the real-world (Source [3])", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "We used a Virtual Robot Experimentation Platform (VREP) [5] (Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "Refer to [5] for detailed information about how to import CAD models into the scene.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "The pairwise term is obtained from the Potts model [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "Thus, this labeling problem modeled on pairwise CRF is optimized using \u03b1expansion built on graph cuts [2] for finding a global optimal", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "We adopt the same depth features as specified in [4].", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "The pairwise potential \u03c6i,j(pi, pj) term in the CRF energy encodes a smoothness prior and encourages smooth segmentation by favoring neighboring pixels in a 2D grid having the same label [2].", "startOffset": 187, "endOffset": 190}, {"referenceID": 14, "context": "It takes the form of a Potts model [15], which is efficiently minimized by \u03b1-expansion [2] built on graph cuts.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "It takes the form of a Potts model [15], which is efficiently minimized by \u03b1-expansion [2] built on graph cuts.", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "In addition, we compare against state-of-the-art approaches [18], [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "In addition, we compare against state-of-the-art approaches [18], [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "We generated an extensive synthetic dataset of RGB-D data using the 3D scene models of 4 different rooms based on industrial workspace and office domain with a total of 10 object classes in the virtual environment [5] (see Fig.", "startOffset": 214, "endOffset": 217}, {"referenceID": 3, "context": "For more detailed information about human data generation, refer to previous work [4], [17].", "startOffset": 82, "endOffset": 85}, {"referenceID": 16, "context": "For more detailed information about human data generation, refer to previous work [4], [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "As a baseline, we implemented the same state-of-the-art (SOA) pipeline used in [18], [19] for the generation of the non-modeled dataset, based on top-view (see Fig.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "As a baseline, we implemented the same state-of-the-art (SOA) pipeline used in [18], [19] for the generation of the non-modeled dataset, based on top-view (see Fig.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The density function based modeling, substantially improves the performance by \u223c7% in mAR and mAP over state-of-the-art [18], [19].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "The density function based modeling, substantially improves the performance by \u223c7% in mAR and mAP over state-of-the-art [18], [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "Ground truth labels and its corresponding synthetic depth data generated using the same pipeline used in [4], [19].", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "Ground truth labels and its corresponding synthetic depth data generated using the same pipeline used in [4], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "Table I shows that the F1-measure for the CRF modeled improves by \u223c8% over the non-modeled ones [18], [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "Table I shows that the F1-measure for the CRF modeled improves by \u223c8% over the non-modeled ones [18], [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "F1-measure Avg Head Body UArm LArm Hand Legs Chair Plant Storage Table CRFNon\u2212Modeled [18], [19] 0.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "F1-measure Avg Head Body UArm LArm Hand Legs Chair Plant Storage Table CRFNon\u2212Modeled [18], [19] 0.", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "In future work, we would like to extend this work towards the semantic image understanding and image-sentence generation [11], [16] for manufacturing", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "In future work, we would like to extend this work towards the semantic image understanding and image-sentence generation [11], [16] for manufacturing", "startOffset": 127, "endOffset": 131}], "year": 2016, "abstractText": "We propose a low cost and effective way to combine a free simulation software and free CAD models for modeling human-object interaction in order to improve human & object segmentation. It is intended for research scenarios related to safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial domain. The task of human and object modeling has been used for detecting activity, and for inferring and predicting actions, different from those works, we do human and object modeling in order to learn interactions in RGBD data for improving segmentation. For this purpose, we define a novel density function to model a three dimensional (3D) scene in a virtual environment (VREP). This density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances. Using this function, we synthesize a large, realistic and highly varied synthetic RGB-D dataset that we use for training. We train a random forest classifier, and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields (CRF). Our evaluation shows that modeling these interactions improves segmentation performance by \u223c7% in mean average precision and recall over state-of-the-art methods that ignore these interactions in realworld data. Our approach is computationally efficient, robust and can run real-time on consumer hardware.", "creator": "LaTeX with hyperref package"}}}