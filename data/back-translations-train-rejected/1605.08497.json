{"id": "1605.08497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Universum Learning for SVM Regression", "abstract": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach.", "histories": [["v1", "Fri, 27 May 2016 03:11:41 GMT  (260kb,D)", "http://arxiv.org/abs/1605.08497v1", "10 pages,11 figures, Thesis:this http URL"]], "COMMENTS": "10 pages,11 figures, Thesis:this http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sauptik dhar", "vladimir cherkassky"], "accepted": false, "id": "1605.08497"}, "pdf": {"name": "1605.08497.pdf", "metadata": {"source": "CRF", "title": "Universum Learning for SVM Regression", "authors": ["Sauptik Dhar", "Vladimir Cherkassky"], "emails": ["sauptik.dhar@us.bosch.com", "cherk001@umn.edu"], "sections": [{"heading": "CCS Concepts", "text": "\u2022 Theory of calculation \u2192 Support of vector machines;"}, {"heading": "Keywords", "text": "Support of vector regression; Learning by contradiction; Universe learning"}, {"heading": "1. INTRODUCTION", "text": "The technique of universe learning or learning by classification, another common overarching problem, is assessed as a real estimation function. [18,19] provides a formal mechanism for the inclusion of a priori knowledge of the scope, in the form of additional (unmarked) universe samples. Universe learning was originally introduced for binary classification problems [18, 21] and it has been shown that most research on universe learning is limited to binary formulas with small sample size [6,17,21]. It is not clear how to extend or modify the universe learning approach to other types of learning problems. The majority of this work was carried out during S. Dhar's PhD and is available in his dissertation: http: / / conservancy.umn.edu / handle / 11299 / 162636 In addition to the classification, another common overarching learning problem is assessed as a function."}, {"heading": "2. SVM REGRESSION", "text": "This section contains a brief description of the standard SVM regression (SVR) formulation according to Vapnik [18,19]. This SVM regression setting uses a special margin-based loss function known as \u03b5 -insensitive loss formula. \u2212 implicit loss formula (f (x, w), y) = max (y \u2212 f (x, w) | sensitive output functions. Note that this loss function is in the space of the target values used y < as in Fig. 1. Then the SVM regression optimization formula (for linear parameters) can be defined as follows: Given i.i.d training samples (xi, yi) n i = 1; with x < d, y < the linear SVM model can be found by defining the optimization problem in w, b1 (w \u00b7 w) + C n: optimal training samples = 1 (b)."}, {"heading": "3. UNIVERSUM-SVM REGRESSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Universum-SVM Regression formulation", "text": "This section describes the proposed Universe SVM regression formulation, using new terms of falsification for regression setting, as explained below. Consider the regression setting, where available training data (xi, yi) n - 1 are modeled using linear SVR. As described in Section 2, for SVM regression, the concept of \"edge samples\" is implemented using \"insensitive zone\" (Fig. 1a). That is, training samples falling within an insensitive zone are \"explained\" by SVM model, and samples that outside \"falsify\" or \"contradict\" this model. Next, consider two SVR models that explain training samples equally well, e.g. both SVR models use the same value of principles and achieve the same empirical risk Remp (w) = n."}, {"heading": "3.2 Computational Implementation of U-SVR", "text": "The formula U-SVR (8) is non-convex due to the non-convex nature of the Universe loss U-VII (y-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-VII-"}, {"heading": "3.3 Generating Universum Data", "text": "Universe contains data samples from the same scope as available training data. For example, for the application of handwritten digital recognition, examples of handwritten letters can be used as universe data samples, along with examples of handwritten digits used as training data. Note that universe samples follow a different distribution than training data. Universe data is often available for most real-world application examples. However, selecting good universe data requires specific strategies for generating synthetic universe data and good engineering. Another strategy is to generate synthetic universe directly from available training data - which is often used for classification [6, 21]. Therefore, in this section, we present specific strategies for generating synthetic universe data under regression setting. Generating such synthetic universe samples requires minimal domain knowledge and should be more reversible for most life problems already being discussed as next to Universe.17"}, {"heading": "4. EMPIRICAL RESULTS", "text": "Most examples use synthetic datasets and linear SVM parameterization to illustrate the effects of the universe on the predictive power of SVR. All experiments follow the two-step experimental modeling strategy (presented in Section 3.2), which first estimates the optimal SVR model (using labeled training data only) and then estimates the U-SVR model (using both training data and universe data).This experimental strategy simplifies comparisons between standard SVR and U-SVR models and also allows for traceable model selection for USVR. In addition, all empirical comparisons use separate validation data for model selection, meaning that the regression model is estimated by adjusting the training dataset to target values, but the parameters for matching the target values (for each method) are selected using separate validation data, and the prediction performance (the final regression model is used for the regression experiment) is calculated among the universe selection."}, {"heading": "4.1 Hypercube dataset", "text": "Our first experiment uses a synthetic 30-dimensional hypercube dataset in which each input is uniformly distributed (0, 1).The results are: y = x1 +. + x5 \u2212 x6 \u2212 x6 \u2212.. \u2212 x10 +. + x21 +. SVS-1: input samples follow the same distribution as training samples, e.g. x < 30 and universally distributed in [0, 1].The output is generated as: y = \u2212 x1 \u2212. \u2212 x5 + x6 +. \u2212 x10 +. \u2212 x21 \u2212."}, {"heading": "4.2 Computer Hardware Dataset", "text": "Our next experiment uses the publicly available real computer hardware dataset [12]. The goal is to predict the published relative CPU performance using several other CPU properties. In this experiment, we use two types of synthetic universe generated directly from the training data: Strategy 1 (Universe 1) and Strategy 2 (Universe 2), as described in Section 3.3. The experimental setup is summarized next: - Number of training samples \u2212 Number of validation samples = 50. (These independent validation samples are used for model selection). - Number of test samples = 109. - Number of universe samples = 100. (Increasing the number of universe samples does not represent an additional improvement.) - Number of input variables = 36. As part of the pre-processed data, the required universe samples = 100."}, {"heading": "4.3 Vilmann\u2019s Rat Dataset", "text": "This dataset illustrates the effectiveness of U-SVR under nonlinear SVR parameterization. The real Vilmann's rat dataset contains the skull X-rays of 21 different rats, represented by 8 - symbols of 2-dimensional [1]. The skull X-rays are available for each rat at the ages of 7, 14, 21, 30, 40, 60, 90 and 150 days. The task is to predict the ontogenetic development (age) of a rat using the X-rays."}, {"heading": "Test Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Training Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Test Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Training Data", "text": "The processed data set contains 164 samples. In this experiment, we use two types of synthetic universe generated directly from the training data, following strategy 1 (universe 1) and strategy 2 (universe 2), as described in section 3.3. The experimental setup is summarized next: - Number of training samples and skull X-rays of 5 different rats. (= 40 images) - Number of validation samples and skull X-rays of 5 different rats (= 40 images. This independent validation data set is used for model selection.) - Number of test samples and skull X-rays of the remaining rats. (= 88 images) - Number of universe X-rays = 200 (increasing the number of universe samples does not provide any additional improvement). - Dimensionality of each sample = 16. (4 landmarks per 2D) For this data set, using the RF-Nucleus = K (Exxxi)."}, {"heading": "5. CONCLUSIONS", "text": "This paper extends the idea of universal learning to regression problems and offers a new optimization formulation called Universe Support Vector Regression (U-SVR), which is non-convex and cannot be solved with standard convex solvers typically used for existing SVM software packages, adopts the Convex Con-Cave Problem (CCCP) method and provides a new algorithm (Algorithm 1 & 2) to solve the proposed U-SVR formulation. Following this strategy, the current U-SVR formulation can be solved by multiple iterations of the standard SVM-like optimization problem. In addition, the proposed U-SVR formulation has 5 fully tunable parameters: C, CIS, and core parameters. Therefore, successful practical application of the U-SVR formulation depends on the optimal selection of these model parameters."}, {"heading": "6. REFERENCES", "text": "[1] F. L. Bookstein. Morphometric tools for landmarkdata: geometry and biology H. 76 networks. Cambridge University Press, 1997. [2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, 2004. [3] S. Chen and C. Zhang. Selecting informative universum sample for semi-supervised learning. In IJCAI, pp. 1016-1021, 2009. [4] S. Cherkassky. Predictive Learning. VCtextbook, 2013. [5] V. Cherkassky and S. Dhar. Simple method for interpretation of high-dimensional nonlinear svm classification models. In R. Stahlbock, S. F. Crone, M. Abou-Nasr, H. R. Arabnia, N. Kourentzes, P. Lenca, W.-M. Lippe, and G. M. Weiss, editors, DMIN, S. Daerky 2010."}], "references": [{"title": "Morphometric tools for landmark data: geometry and biology", "author": ["F.L. Bookstein"], "venue": "Cambridge University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Selecting informative universum sample for semi-supervised learning", "author": ["S. Chen", "C. Zhang"], "venue": "IJCAI, pages 1016\u20131021", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive Learning", "author": ["V. Cherkassky"], "venue": "VCtextbook", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Simple method for interpretation of high-dimensional nonlinear svm classification models", "author": ["V. Cherkassky", "S. Dhar"], "venue": "R. Stahlbock, S. F. Crone, M. Abou-Nasr, H. R. Arabnia, N. Kourentzes,  P. Lenca, W.-M. Lippe, and G. M. Weiss, editors, DMIN, pages 267\u2013272. CSREA Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical conditions for effectiveness of the universum learning", "author": ["V. Cherkassky", "S. Dhar", "W. Dai"], "venue": "Neural Networks, IEEE Transactions on, 22(8):1241\u20131255", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical selection of svm parameters and noise estimation for svm regression", "author": ["V. Cherkassky", "Y. Ma"], "venue": "Neural networks, 17(1):113\u2013126", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning from Data: Concepts", "author": ["V. Cherkassky", "F.M. Mulier"], "venue": "Theory, and Methods. Wiley-IEEE Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Large scale transductive svms", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "The Journal of Machine Learning Research, 7:1687\u20131712", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Development and evaluation of cost-sensitive universum-svm", "author": ["S. Dhar", "V. Cherkassky"], "venue": "Cybernetics, IEEE Transactions on, 45(4):806\u2013818", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Chris", "author": ["H. Drucker"], "venue": "B. L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In Advances in Neural Information Processing Systems 9, volume 9, pages 155\u2013161", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Uci machine learning repository. http://archive.ics.uci.edu/ml. Accessed: 2016-02-05", "author": ["M. Lichman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Weighted twin support vector machine with universum", "author": ["S. Lu", "L. Tong"], "venue": "Advances in Computer Science: an International Journal, 3(2):17\u201323", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparallel support vector machine for a classification problem with universum learning", "author": ["Z. Qi", "Y. Tian", "Y. Shi"], "venue": "Journal of Computational and Applied Mathematics, 263:288\u2013298", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "cal U} boost: Boosting with the universum", "author": ["C. Shen", "P. Wang", "F. Shen", "H. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(4):825\u2013832", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On psi-Learning", "author": ["X. Shen", "G.C. Tseng", "X. Zhang", "W.H. Wong"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "An analysis of inference with the universum. In Advances in neural information processing systems 20, pages 1369\u20131376", "author": ["F. Sinz", "O. Chapelle", "A. Agarwal", "B. Sch\u00f6lkopf"], "venue": "Red Hook, NY,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Estimation of Dependences Based on Empirical Data (Information Science and Statistics)", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley-Interscience", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Multi-view learning with universum", "author": ["Z. Wang", "Y. Zhu", "W. Liu", "Z. Chen", "D. Gao"], "venue": "Knowledge-Based Systems, 70:376\u2013391", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Inference with the universum", "author": ["J. Weston", "R. Collobert", "F. Sinz", "L. Bottou", "V. Vapnik"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 1009\u20131016. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "F", "author": ["D. Zhang", "J. Wang"], "venue": "W. 0001, and C. Zhang. Semi-supervised classification with universum. In SDM, pages 323\u2013333. SIAM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "This paper extends the idea of Universum learning [18, 19] to regression problems.", "startOffset": 50, "endOffset": 58}, {"referenceID": 18, "context": "This paper extends the idea of Universum learning [18, 19] to regression problems.", "startOffset": 50, "endOffset": 58}, {"referenceID": 17, "context": "The technique of Universum learning or learning through contradiction [18,19] provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional (unlabeled) Universum samples.", "startOffset": 70, "endOffset": 77}, {"referenceID": 18, "context": "The technique of Universum learning or learning through contradiction [18,19] provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional (unlabeled) Universum samples.", "startOffset": 70, "endOffset": 77}, {"referenceID": 17, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 85, "endOffset": 93}, {"referenceID": 5, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 16, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 20, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 2, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 9, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 12, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 13, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 14, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 19, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 21, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 3, "context": "edu/handle/ 11299/162636 Besides classification setting, another common supervised learning problem is regression or real-valued function estimation from noisy samples [4, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 7, "context": "edu/handle/ 11299/162636 Besides classification setting, another common supervised learning problem is regression or real-valued function estimation from noisy samples [4, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 17, "context": "This section provides brief description of standard SVM regression (SVR) formulation following Vapnik [18,19].", "startOffset": 102, "endOffset": 109}, {"referenceID": 18, "context": "This section provides brief description of standard SVM regression (SVR) formulation following Vapnik [18,19].", "startOffset": 102, "endOffset": 109}, {"referenceID": 3, "context": "For most SVR solvers, problem (6) is usually solved in its dual form (see [4] for details):", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "For more details see [4, 11].", "startOffset": 21, "endOffset": 28}, {"referenceID": 10, "context": "For more details see [4, 11].", "startOffset": 21, "endOffset": 28}, {"referenceID": 8, "context": "Recently, similar non-convex optimization problems have been addressed in [9,16] using the ConCave Convex Programming (CCCP) strategy.", "startOffset": 74, "endOffset": 80}, {"referenceID": 15, "context": "Recently, similar non-convex optimization problems have been addressed in [9,16] using the ConCave Convex Programming (CCCP) strategy.", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 6, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 7, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 5, "context": "Another strategy is to generate synthetic Universum directly from available labeled training data - which is often used for classification setting [6, 21].", "startOffset": 147, "endOffset": 154}, {"referenceID": 20, "context": "Another strategy is to generate synthetic Universum directly from available labeled training data - which is often used for classification setting [6, 21].", "startOffset": 147, "endOffset": 154}, {"referenceID": 2, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 5, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 16, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 20, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 10, "context": "Further, following [11], our empirical results for synthetic data use NRMS error calculated using the true output values t(x)(not corrupted by noise), whereas results for real-life data sets use NRMS errors calculated using noisy y -values.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Our first experiment uses a synthetic 30-dimensional hypercube data set, where each input is uniformly distributed in [0, 1] interval.", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": ", x \u2208 < and uniformly distributed in [0, 1].", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "For understanding of the U-SVR modeling results we adopt the technique known as \u2018histogram of projections\u2019 originally introduced for SVM classification setting [5, 6].", "startOffset": 160, "endOffset": 166}, {"referenceID": 5, "context": "For understanding of the U-SVR modeling results we adopt the technique known as \u2018histogram of projections\u2019 originally introduced for SVM classification setting [5, 6].", "startOffset": 160, "endOffset": 166}, {"referenceID": 4, "context": "near the boundaries of \u03b5-tube, which is similar to data piling at the margin borders for SVM classification [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 5, "context": "near the boundaries of \u03b5-tube, which is similar to data piling at the margin borders for SVM classification [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 1, "context": "This effect of data piling is typically observed for \u2018small-sample\u2019 regression data sets corresponding to very ill-posed estimation problems [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "Similar to classification settings [6, 17], the effectiveness of Universum for regression problems depends on the statistical characteristics of both the training data and the Universum data.", "startOffset": 35, "endOffset": 42}, {"referenceID": 16, "context": "Similar to classification settings [6, 17], the effectiveness of Universum for regression problems depends on the statistical characteristics of both the training data and the Universum data.", "startOffset": 35, "endOffset": 42}, {"referenceID": 2, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 5, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 16, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 11, "context": "Our next experiment uses the publicly available real-life Computer Hardware dataset [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Further the y- values have been scaled as log(1 + y) (see [12]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "The real-life Vilmann\u2019s Rat dataset contains the skull X-ray images of 21 different rats, represented using 8 - landmarks for 2-dimensions [1].", "startOffset": 139, "endOffset": 142}], "year": 2016, "abstractText": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach. CCS Concepts \u2022Theory of computation\u2192 Support vector machines;", "creator": "LaTeX with hyperref package"}}}