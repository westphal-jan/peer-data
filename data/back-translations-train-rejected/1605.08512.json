{"id": "1605.08512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "SNN: Stacked Neural Networks", "abstract": "It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this work is to generate better features for transfer learning from multiple publicly available pre-trained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this work is to make network fine- tuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called \"joint fine-tuning\" that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.", "histories": [["v1", "Fri, 27 May 2016 06:02:48 GMT  (2079kb)", "http://arxiv.org/abs/1605.08512v1", "8pages"]], "COMMENTS": "8pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["milad mohammadi", "subhasis das"], "accepted": false, "id": "1605.08512"}, "pdf": {"name": "1605.08512.pdf", "metadata": {"source": "CRF", "title": "S-NN: Stacked Neural Networks", "authors": ["Milad Mohammadi"], "emails": ["milad@cs.stanford.edu", "subhasis@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.08 512v 1 [cs.L G] 27 May 2The aim of this project is to generate better features for transfer learning from several publicly available upstream neural networks. To this end, we propose a novel architecture called Stacked Neural Networks, which takes advantage of the fast training time of transfer learning and is at the same time much more accurate. We show that the use of a stacked NN architecture can lead to an improvement in accuracy over modern techniques, in which only a pre-trained network is used for transfer learning.A second goal of this project is to maintain the generalisability of the base network to invisible tasks. To this end, we propose a new technique called Joint Finetuning, which is capable of delivering accuracy comparable to fine-tuning the same network across two sets of data."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2. Neural Networks Set", "text": "The network architectures and functions used for this study are outlined below. VGG 16 layers and 19 layers (VGG16, VGG19): This is an architecture proposed by Simonyan et al. [15], which uses a very deep network (16 and 19 layers, respectively) with smaller folding filters of 3 x 3 size to obtain state-of-the-art accuracy at the ImageNet 2014 Challenge. We use the fc7 layer of the VGG network as the feature layer. GoogLeNet: This is an architecture used by Szegedy etal. [16], which uses multiple \"receiver modules\" to create a deeper network with 22 layers, while it has much fewer parameters than other networks such as VGG and AlexNet. We use the pool5 / 5x5 s1 layer of GoogLeNet as the feature layer. Places: This is a network created by Zhou et al. [19] It has the same architecture as Alexbe used in Alexbe architecture, but has a better one in Alexbe architecture]."}, {"heading": "3. Datasets", "text": "Below we describe the attributes of each individual dataset used for our study evaluation.Caltech-UCSD Birds 200-2011 [17]: This is a dataset of 200 different bird species. The dataset consists of 11,788 images. Caltech256 [4]: This is a dataset of 256 object categories with 30,607 images. The dataset comes from Google images. Food-101 [2]: This is a dataset of 101 different food categories with 1,000 food per category. LISA Traffic Sign Dataset [10]: This dataset contains 7,855 annotations of 6,610 video images taken on US roads. Each image is labeled with the road signs visible on the images as well as the location of the sign. It covers 47 of the US traffic signs. MIT Scene [13]: This is a dataset of 15,620 indoor images with 67 categories each containing 100 or more images with 102 blossoms each of these [25] Oxford blossoms."}, {"heading": "4. Methodology", "text": "In this section, we will formally define Stacked Neural Networks and discuss the studies we have conducted to develop a novel deep learning framework to improve the state-of-the-art predictive accuracy of Deep Neural Networks (NN)."}, {"heading": "4.1. Feature Stacking", "text": "Stacked Neural Networks (S-NN) is defined as a combination of publicly available neural network architectures whose features are extracted on an intermediate layer of the network and then concatenated to form a larger feature set. Figure 2 illustrates this idea in detail. The concatenated feature vector is used to train a classification layer consisting of an optional failure layer, an affine layer and an SVM loss function. Section 5 discusses the effects of the failure layer in detail. While Figure 2 shows all five Convolutionary Neural Networks (CNN) as members of the S-NN, any combination of these CNN is also considered to be S-NN. For example, {GoogLeNet, VGG16} and {NIN, Places, VGG19} are examples of a 2-network and 3-network S-NN. We will evaluate the effect of various network combinations in Section 5, which show that S-N provide higher classification accuracy than singular structures."}, {"heading": "4.2. Ensemble of S-NN\u2019s", "text": "Each combination of S-NN's generates a new feature vector and a new set of scores. In order to further improve the classification accuracy, we investigated the effect of the ensemble mean value of the scores on the final network. Figure 3 shows a number of S-NN's whose scores are combined to an ensemble score. While any group of S-NN's can be used to generate an ensemble score, we opt for the calculation of this score by stacking an S-NN with all its network combination subgroups. For example, we take an S-NN containing three networks (NIN, VGG19, GoogLeNet, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEIN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NEN, NENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENE, NENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENENE"}, {"heading": "5. Results Discussion", "text": "Thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly, thirdly,"}, {"heading": "6. Joint Training of Multiple Tasks", "text": "As part of this work, we also investigated how the generalization of network characteristics suffers when a network is matched to a specific task. To perform these experiments, we looked at three tasks from different areas: Food-101 (task A), MIT Scene (task B), and Caltech256 (task C). In all of these experiments, we used the VGG16 network. We describe our experiments and observations below."}, {"heading": "6.1. Generalization Loss by Fine-Tuning", "text": "In our first experiment, we investigated the generalization loss through network tuning and ways to avoid this problem. To show the loss of generalizability through fine tuning, we first refined VGG16 at task A to create the VGG16A network, and independently at task B to create a network VGG16B. Subsequently, we drastically reduced the accuracy of transfer learning at task B at task B (43.0% compared to 72.2%).An interesting question is therefore whether it is possible to fine tune a single network to give accuracies similar to the individually fine-tuned networks at task A (VGG16 at task A and VGG2 at task B)."}, {"heading": "6.2. Generalization of Jointly-Tuned networks", "text": "In the previous section, we saw that fine-tuning a network to two tasks together can lead to accuracies comparable to individual fine-tuning. To answer this question, we took a third task C (Caltech256 in our experiments) and evaluated the transmission learning accuracy of task C via VGG16, VGG16A and VGG16AB. The results are shown in Table 3. It can be seen that the generalization capabilities of a finely tuned network are actually higher. Although the finely tuned network alone does not reach the transmission learning capability of the base network, it manages to mitigate much of the impairments caused by individual fine-tuning. We suspect that the reason for this phenomenon is that the fine-tuned network alone does not reach the transmission learning capability of the baseline network, but that it is only forced to fine-tune. \""}, {"heading": "7. Future Direction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Parallel Training of Networks", "text": "As shown in [18], training neural networks in deeper layers can improve classification accuracy. While the S-NNs, as explained in the previous sections, are geared toward agile training and high generalization accuracy, it is reasonable to assume that if they were trained together, their collective classification errors would decrease. So, instead of using the data for when each of these networks was independently pre-trained, we allow the reverse propagation training method to transmit the classification gradients to all networks. As all networks iterate through the same gradients in parallel, they all influence each other's weight and bias values. Due to the limited computing capacity available to us, we have not been able to generate results for this step. We will continue working on this scheme as soon as we have access to more powerful GPU machines."}, {"heading": "7.2. Weighted S-NN", "text": "Our analysis of the confusion matrices on the available data sets has made it clear to us that combining the network functions in S-NN helps to reduce errors, but it is also the case that some networks cause excessive confusion in the error rate of S-NN. To reduce such adverse effects of S-NN, we plan to combine features by weighting each network feature. To achieve this, the weight coefficients will depend on the predictive accuracy of each network in classifying a particular data set. For example, if GoogLetNet CNN shows weaker classification performance relative to CNN Places, the relative contribution of GoogLeNet features will be reduced. To achieve this, the characteristic vector of each network is multiplied by a scalar value in [0, 1], which is calculated by dividing the classification accuracy of each NN by the accuracy of the network by the accuracy of the best result."}, {"heading": "7.3. Data Augmentation", "text": "Data augmentation has improved classification accuracy by diversifying NN features. Literature [11, 2, 17] shows significant improvements in data sets from the MIT scene, CUB 200 and Oxford Flowers when their networks are trained in data augmentation. Although we have had little time to try this technique, we believe it will greatly improve our prediction accuracy for most, if not all, of the datasets."}, {"heading": "7.4. Parallel Wimpy Networks", "text": "Inspired by the idea of ensembles presented by Hinton et al. [6], this study demonstrates that combining multiple powerful networks leads to more substantial performance gains, and also demonstrates that building a single network on multiple datasets can lead to better generalization accuracy. We call it a stack of shaky neural networks. We are interested in evaluating the possibility of building numerous small, quick-to-train networks that are trained on multiple datasets and stacked as S-NN. We call it a stack of shaky neural networks. Such a technique is of interest to us on two fronts: firstly, determining whether multiple smart S-NN can perform as well (or better) a powerful network as VGG19. Secondly, determining whether this architecture can help reduce the computing burden of newer deep neural networks such as VGG19 by providing a much more parallel network architecture that allows multiple computing units (PUs) to be conveniently transferred to multiple computing units (GUs)."}, {"heading": "8. Conclusion", "text": "In this paper, we presented Stacked Neural Networks, a novel technique for extracting higher generalization accuracy from state-of-the-art neural networks in the public domain. We examined various NN stack combinations and found that while a five CNN stack provides the best accuracy, the stack of two CNN's can provide similar accuracy gains while consuming much less computing power. We also presented the improvement in classification accuracy when generating an ensemble of S-NN. Combining these techniques enabled us to increase classification accuracy beyond the state-of-the-art results presented in previous literacy studies. Furthermore, we investigated the effect of forming multiple datasets in a network. Interestingly, we concluded that it is possible to refine a single network across multiple datasets together and still achieve accuracies that are almost similar to the individual fine-grading of networks."}], "references": [{"title": "Food-101 \u2013 mining discriminative components with random forests", "author": ["L. Bossard", "M. Guillaumin", "L. Van Gool"], "venue": "European Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Dark knowledge", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Lecture,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Network In Network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv:1312.4400 [cs], Dec.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m- RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv:1412.6632 [cs], Dec.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Visionbased traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey", "author": ["A. Mogelmose", "M.M. Trivedi", "T.B. Moeslund"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, 13(4):1484\u2013 1497,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A visual vocabulary for flower classification", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1447\u20131454,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Computer Vision, Graphics & Image Processing, 2008. ICVGIP\u201908. Sixth Indian Conference on, pages 722\u2013729. IEEE,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Work- 7  shops (CVPRW), 2014 IEEE Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556 [cs], Sept.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv:1409.4842 [cs], Sept.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "[14] have shown that transfer learning on a pre-trained neural network can outperform traditional hand-tuned approaches in several tasks including coarse-grained detection, fine-grained detection and attribute detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "However, in some tasks such as Image-to-Sentence retrieval [9], a set of features is desirable instead of a score over some set of predefined classes.", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "[15] which uses a very deep network (16 and 19 layers respectively) with smaller convolution filters of 3\u00d73 size to obtain stateof-the-art accuracies on the ImageNet 2014 Challenge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[16], which uses several \u201cInception\u201d modules to create a deeper network with 22 layers while having much fewer parameters than other networks such as VGG and AlexNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "It has the same architecture as AlexNet [7] but trained on the Places dataset instead of ImageNet to enable better performance in scene-centric tasks.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "[8] which uses neural networks as the layer transfer function instead of a convolution followed by a non-linearity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Caltech256 [4]: This is a dataset of 256 object categories containing 30,607 images.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Food-101 [2]: This is a dataset of 101 distinct food categories with 1,000 foods per category.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "LISA Traffic Sign Dataset [10]: This dataset contains 7,855 annotations on 6,610 video frames captured on US roads.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "MIT scene [13]: This is an indoor scene dataset with 15,620 images with 67 categories each of which containing at least 100 images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Oxford flowers [12]: This is a collection of 102 groups of flowers each with 40 to to 256 flowers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 0, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 1, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 9, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 13, "context": "As shown in [18], training neural networks at deeper layers can improve the classification accuracy.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "Literatures [11, 2, 17] show substantial improvement in the MIT Scene, CUB 200, and Oxford Flowers datasets when their networks are trained using data augmentation.", "startOffset": 12, "endOffset": 23}, {"referenceID": 0, "context": "Literatures [11, 2, 17] show substantial improvement in the MIT Scene, CUB 200, and Oxford Flowers datasets when their networks are trained using data augmentation.", "startOffset": 12, "endOffset": 23}, {"referenceID": 2, "context": "[6], this study proves combining multiple powerful networks leads to more substantial performance gains.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this project is to generate better features for transfer learning from multiple publicly available pretrained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this project is to make network finetuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called \u201cjoint finetuning\u201d that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.", "creator": "LaTeX with hyperref package"}}}