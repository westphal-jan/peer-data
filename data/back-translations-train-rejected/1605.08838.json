{"id": "1605.08838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Dueling Bandits with Dependent Arms", "abstract": "We consider online content recommendation with implicit feedback through pairwise comparisons. We study a new formulation of the dueling bandit problems in which arms are dependent and regret occurs when neither pulled arm is optimal. We propose a new algorithm, Comparing The Best (CTB), with computational requirements appropriate for problems with few arms, and a variation of this algorithm whose computation scales to problems with many arms. We show both algorithms have constant expected cumulative regret. We demonstrate through numerical experiments on simulated and real dataset that these algorithms improve significantly over existing algorithms in the setting we study.", "histories": [["v1", "Sat, 28 May 2016 03:21:44 GMT  (317kb,D)", "https://arxiv.org/abs/1605.08838v1", null], ["v2", "Thu, 15 Jun 2017 01:30:11 GMT  (471kb,D)", "http://arxiv.org/abs/1605.08838v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bangrui chen", "peter i frazier"], "accepted": false, "id": "1605.08838"}, "pdf": {"name": "1605.08838.pdf", "metadata": {"source": "CRF", "title": "Dueling Bandits with Dependent Arms", "authors": ["Bangrui Chen", "Peter I. Frazier"], "emails": ["bc496@cornell.edu", "pf98@cornell.edu"], "sections": [{"heading": "1. Introduction", "text": "In Duelists Bandits et al., we are confronted with a collection of weapons, and draw a series of regrets, while observing noisy binary feedback indicates which arm is better for each pair drawn. As in the classic multi-armed bandit problem, we want to quickly learn which arm is best and minimize the number of draws to sub-optimal weapons. Dueling bandits were introduced by Yue and Joachims, whose results can be reliably derived from implicit feedback, for example by interleaved ranking in Radlinski et al. (2008), as opposed to cardinal ratings, which are usually difficult to obtain, and requires careful calibration."}, {"heading": "2. Problem Formulation", "text": "There are N \u2265 2 arms, and each arm i has an observable and unique d-dimensional function vector Ai > total vector Ai. Preferences between the armies i, j are characterized by fixed but unknown probabil-ities pi, j, where Pi, j = 1 \u2212 pj and pi, j 6 = 0.5, if i = j = j. We denote p = mini < j max (pi, j, pj, i). By constructing, p > 0.5.At any time we draw two arms Xt, 0 and Xt, 1 (this action is called a \"duel\") and we observe feedback Yt (0, 1} indicating the winning arm: Yt = 0, 0 won and Yt = 1 points to arm Xt, 1 won. We have pulled at the arms and the story (the arms drawn and the identity of the victor at times t drawn), Yt is equal to 0 with probability pi."}, {"heading": "4. Theoretical Results", "text": "In this section we prove the expected cumulative regret of KTB = = 1 = 2 = 1. The main idea of our proof is to show that for each cell Ci with B (i) 6 = 1, E [p] s with a greater probability of increase than decrease. The following problem, the proof of which is contained in the supplement, allows us to limit the number of times of this stochastic process by relating values that are less constant.Lemma 1. Let us leave the latte (0.5, 1).Suppose Z (t) is a stochastic process with filtration Ft, Z (0) = 0 and P (t + 1) = Z (t + 1) = Z (t) + 1 | Ft), then we have the latte p, then we have E [p), then we have the latte (t)."}, {"heading": "5. Computation for Decomposable mi", "text": "The CTB achieves a constant expected cumulative repentance. However, a naive implementation of algorithm 1 = requires a large amount of memory to store mi (t) for each cell, which makes it mathematically difficult for problems with many arms. In this section we will consider a specific case of CTB in which mi (0) can be expressed in terms of an initial assessment for each pair of arms. Specifically, we assume that there is an ri, j like thatmk (0) = a problem with many arms. (5) Here ri, j can be interpreted as a prior indication of the extent to which we believe that arm i is preferred over arm j. In this particular case, we describe an efficient calculation method that results in problems with many arms. Instead of storing mi (t), this method stores ri, j and qi, j (t) and uses it to reconstruct the equation mi (t) with equation."}, {"heading": "6. Bayesian Interpretation", "text": "Although our problem is formulated in a frequency setting, here we show that CTB has a qq interpretation (then q interpretation (then q interpretation). In this section, we construct a posterior cell based on a previous and given assumption that pi, j = q > 0.5 for all i < j, where q is the same or different (from p, and pi, j), may or may not be constant over i, j in reality. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 by integrating p0 over Ci \u00b7 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}, {"heading": "7. Numerical Experiments", "text": "In this section we compare the three variants of CTB described in Section 3, CTB-1, TB-2 and TB-3 with three benchmarks: Thompson Sampling, Relative Upper Confidence Bound (RUCB) and Winner-Stays (WS)."}, {"heading": "8. Conclusion", "text": "In this article, we look at the duel of bandits against weak remorse, applying to referral systems and recommendations of online content. We are formulating a new environment that differs from the traditional duels of bandits in which weapons are dependent. We propose an algorithm CTB and show that it constantly expects cumulative remorse and strong empirical performance."}, {"heading": "Appendix A.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 1", "text": "First we prove another lemma.Lemma 6. \u2212 Suppose Z (k) is a random gait with Z (0) = = martose (0) = 0, Z (k + 1) = Z (k + 1) = Z (k + 1) = Z (k) \u2212 1 with probability 1 \u2212 p. Then we have for S \u00b2 N E [\u221e t = 0 1 {Z (t) = 1 {Z (t) \u2264 S] = p + S (2p \u2212 1) (2p \u2212 1) (2p \u2212 1) 2. (8) Proof A = E [t: mint > 1 Z (t) = 0 | Z (t) = \u2212 1] and B = P (t) = p (t) = 0 | Z (1) = 1), then we know, then we know about E [s]. Denote A = E [t: mint > 1 Z (t) = 1 Z (t) = 0 (t)."}, {"heading": "Proof of Lemma 4", "text": "Proof: We prove it first for Yt = 0 and x-pt (Hi, j). This is becausept + 1 (x) = pt + 1 (\u03b8-x) = P (\u03b8-x | Yt = 0, pt (\u00b7)) = P (\u03b8-x, Yt = 0, pt (\u00b7))) P (Yt = 0, pt (\u00b7))) = P (\u03b8-x, Yt = 0, pt (\u00b7)) P (Yt = 0, pt (\u00b7) | \u03b8-Hi, j) P (\u03b8-Hi, j) + P (\u03b8-Hi, j) + P (Yt = 0, pt (\u00b7) | \u03b8-Hi, j) P (\u03b8 / -Hi, j) = pt (x-pt (Hi, j) q + (1 \u2212 pt (Hi, j))) (1 \u2212 q). The other three cases follow the same reasoning and we omit the proof."}, {"heading": "Proof of Lemma 5", "text": "This is obviously true if t = 0. Suppose that this is the case at t-1. Without loss of generality, we write \u2212 1 (Ck) = p0 (Ci) qmi (t-1) \u2212 mi (0) (1 \u2212 q) t \u2212 mi (1 \u2212 mi (t-1) + mi (0) M (t-1), where M (t-1) is a scaling constant. Suppose we choose Ai and Aj for comparison and Ai wins the duel. Name M (t) = M (t-1) \u0445 [pt \u2212 1 (Hij) \u0445 q + (Hi, j \u2212 1)) (1 \u2212 q Hi)) (Ck \u2212 0 = pt (Ck) = pt (Ck) (Ck) (k) (k) (k) (k) (k) (k) (k) (k) (k) (1) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (1) (k) (k) (k) (k) (k) (k) (k) (k) (k) (1 (k) (k) (k) (k) (k) (k) (k (k) (k) (k) (k) (k) (1 (k) (k) (k (k) (k (k) (k) (k) (k (k) (k) (k (k) (1 (k) (k) (k (k) (k (k) (k) (k (k) (k) (k (k) (k) (k (k) 1 (k (k)) (k (k (k)) (k (k) 1 (k (k) (k (k (k))) (k (k) 1 (k (k (k (k))) (k) (k (k (k (k (k)) (k (k))) (k (k (k (k (k)))) 1) (k (k (k (k (k (k (k)))) (k) (k (k (k (k (k (k (k))"}, {"heading": "Full Plot of Section 7", "text": "We include a diagram that contains all the information for RUCB. See Figure 3 for details."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["Nir Ailon", "Zohar Shay Karnin", "Thorsten Joachims"], "venue": "In ICML,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Dueling bandits with weak regret", "author": ["Bangrui Chen", "Peter Frazier"], "venue": "In Proceedings of the 34st International Conference on Machine Learning", "citeRegEx": "Chen and Frazier.,? \\Q2017\\E", "shortCiteRegEx": "Chen and Frazier.", "year": 2017}, {"title": "Econometric models in marketing, volume 16", "author": ["Philip Hans Franses", "Alan L Montgomery"], "venue": null, "citeRegEx": "Franses and Montgomery.,? \\Q2002\\E", "shortCiteRegEx": "Franses and Montgomery.", "year": 2002}, {"title": "Active ranking using pairwise comparisons", "author": ["Kevin G Jamieson", "Robert Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jamieson and Nowak.,? \\Q2011\\E", "shortCiteRegEx": "Jamieson and Nowak.", "year": 2011}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["Thorsten Joachims", "Laura Granka", "Bing Pan", "Helene Hembrooke", "Filip Radlinski", "Geri Gay"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["Junpei Komiyama", "Junya Honda", "Hisashi Kashima", "Hiroshi Nakagawa"], "venue": "In COLT,", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["Filip Radlinski", "Madhu Kurup", "Thorsten Joachims"], "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Mixed logit with repeated choices: households\u2019 choices of appliance efficiency level", "author": ["David Revelt", "Kenneth Train"], "venue": "Review of economics and statistics,", "citeRegEx": "Revelt and Train.,? \\Q1998\\E", "shortCiteRegEx": "Revelt and Train.", "year": 1998}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yisong Yue", "Thorsten Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yue and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yisong Yue", "Thorsten Joachims"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Yue and Joachims.,? \\Q2011\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["Masrour Zoghi", "Shimon Whiteson", "Remi Munos", "Maarten de Rijke"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Copeland dueling bandits", "author": ["Masrour Zoghi", "Zohar S Karnin", "Shimon Whiteson", "Maarten De Rijke"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012).", "startOffset": 159, "endOffset": 200}, {"referenceID": 12, "context": "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012).", "startOffset": 159, "endOffset": 200}, {"referenceID": 11, "context": "Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al.", "startOffset": 87, "endOffset": 111}, {"referenceID": 13, "context": "Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al., 2014) and RMED (Komiyama et al.", "startOffset": 118, "endOffset": 138}, {"referenceID": 6, "context": ", 2014) and RMED (Komiyama et al., 2015).", "startOffset": 17, "endOffset": 40}, {"referenceID": 7, "context": "Dueling bandits were introduced by Yue and Joachims (2009), motivated by interactive optimization of web search and other information retrieval systems.", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "The advantage of the dueling bandits formulation over the classical multi-armed bandits formulation in this application setting is that pairwise comparison results can be reliably inferred from implicit feedback, for example through interleaved rankings in Radlinski et al. (2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al.", "startOffset": 257, "endOffset": 281}, {"referenceID": 5, "context": "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012). Dueling bandits have been studied most frequently assuming strong regret, in which the regret is 0 if and only if both pulled arms are optimal. Several algorithms have been devised that assume the existence of a Condorcet winner, i.e., one that is preferred in comparison with each other arm. Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al., 2014) and RMED (Komiyama et al., 2015). Zoghi et al. (2015) points out points out that a Condorcet winner does not necessarily exist, and that its probability of existence decreases dramatically with the number of arms.", "startOffset": 160, "endOffset": 688}, {"referenceID": 8, "context": "This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002).", "startOffset": 65, "endOffset": 107}, {"referenceID": 12, "context": "This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002).", "startOffset": 65, "endOffset": 107}, {"referenceID": 3, "context": ", 2012) and probit models (Franses and Montgomery, 2002).", "startOffset": 26, "endOffset": 56}, {"referenceID": 9, "context": "Our exploitation of arm features is similar in spirit to work in the traditional (cardinal) multi-armed bandit setting on linear bandits (Rusmevichientong and Tsitsiklis, 2010; AbbasiYadkori et al., 2011).", "startOffset": 137, "endOffset": 204}, {"referenceID": 1, "context": "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm\u2019s utility and the utilities of the pulled arms.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm\u2019s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal.", "startOffset": 0, "endOffset": 400}, {"referenceID": 1, "context": "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm\u2019s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting.", "startOffset": 0, "endOffset": 697}, {"referenceID": 1, "context": "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm\u2019s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting.", "startOffset": 0, "endOffset": 795}, {"referenceID": 1, "context": "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm\u2019s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting. These bounds on binary weak regret have corresponding bounds on utility-based weak regret inflated by the difference in utility between the best and worst arms. We consider utility-based weak regret, in the total order setting, when the total order is induced by a utility which is in turn a function of observable arm features, an unknown latent preference vector, and a known utility function. This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002). We provide an algorithm, Comparing with the Best (CTB) that has expected cumulative utility-based weak regret that is constant in T , and that leverages the dependence between preferences over arms induced by the arm features and utility function to provide excellent empirical performance when prior information is available. While our regret bound\u2019s dependence on N is looser than Chen and Frazier (2017) (our dependence is 2N in the worst case, and is N2d when the utility function is linear over a d-dimensional space of preferences and arm features), our algorithm is more flexible in its ability to problem structure induced by the feature vectors, and outperforms it empirically by a substantial margin when N is small enough to allow computation that fully takes advantage of this problem structure.", "startOffset": 0, "endOffset": 1932}, {"referenceID": 8, "context": "For example, our framework includes the logit or Bradley-Terry model (Revelt and Train, 1998; Yue et al., 2012), in which d\u2032 = d, the utility function is u(\u03b8,Ai) = \u03b8 \u00b7Ai and pi,j = exp(u(\u03b8,Ai)) exp(u(\u03b8,Ai)+u(\u03b8,Aj)) .", "startOffset": 69, "endOffset": 111}, {"referenceID": 12, "context": "For example, our framework includes the logit or Bradley-Terry model (Revelt and Train, 1998; Yue et al., 2012), in which d\u2032 = d, the utility function is u(\u03b8,Ai) = \u03b8 \u00b7Ai and pi,j = exp(u(\u03b8,Ai)) exp(u(\u03b8,Ai)+u(\u03b8,Aj)) .", "startOffset": 69, "endOffset": 111}, {"referenceID": 3, "context": "Our framework also includes the probit model (Franses and Montgomery, 2002) in which d\u2032 = d and the utility function is the inner product as with the logit model, but pi,j = \u03a6(u(\u03b8,Ai)\u2212 u(\u03b8,Aj)) where \u03a6(\u00b7) is the standard normal cdf.", "startOffset": 45, "endOffset": 75}, {"referenceID": 4, "context": "Moreover, when the utility function is linear and d\u2032 = d, results in Jamieson and Nowak (2011) show M \u2032 is O(N2d).", "startOffset": 69, "endOffset": 95}, {"referenceID": 14, "context": "Though there are algorithms that outperform RUCB in some settings such as CCB and SCB (Zoghi et al., 2015), they typically work better when a Condorcet winner does not exist.", "startOffset": 86, "endOffset": 106}, {"referenceID": 12, "context": "\u2022 RUCB is as described in Zoghi et al. (2014). We choose it as our benchmark over other algorithms designed for strong regret from the literature because it works well relative to other algorithms designed for strong regret in previous literature when a Condorcet winner exists, and existence of a Condorcet winner is a consequence of our total order assumption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 2, "context": "\u2022 WS is as described in Chen and Frazier (2017), and is selectetd because it is designed for the weak regret setting.", "startOffset": 24, "endOffset": 48}], "year": 2017, "abstractText": "We study dueling bandits with weak utility-based regret when preferences over arms have a total order and carry observable feature vectors. The order is assumed to be determined by these feature vectors, an unknown preference vector, and a known utility function. This structure introduces dependence between preferences for pairs of arms, and allows learning about the preference over one pair of arms from the preference over another pair of arms. We propose an algorithm for this setting called Comparing The Best (CTB), which we show has constant expected cumulative weak utility-based regret. We provide a Bayesian interpretation for CTB, an implementation appropriate for a small number of arms, and an alternate implementation for many arms that can be used when the input parameters satisfy a decomposability condition. We demonstrate through numerical experiments that CTB with appropriate input parameters outperforms all benchmarks considered.", "creator": "LaTeX with hyperref package"}}}