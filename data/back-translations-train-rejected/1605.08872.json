{"id": "1605.08872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Online Bayesian Collaborative Topic Regression", "abstract": "Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix factorization (PMF) and topic modeling (e.g., LDA) for recommender systems, which has gained increasing successes in many applications. Despite enjoying many advantages, the existing CTR algorithms have some critical limitations. First of all, they are often designed to work in a batch learning manner, making them unsuitable to deal with streaming data or big data in real-world recommender systems. Second, the document-specific topic proportions of LDA are fed to the downstream PMF, but not reverse, which is sub-optimal as the rating information is not exploited in discovering the low-dimensional representation of documents and thus can result in a sub-optimal representation for prediction. In this paper, we propose a novel scheme of Online Bayesian Collaborative Topic Regression (OBCTR) which is efficient and scalable for learning from data streams. Particularly, we {\\it jointly} optimize the combined objective function of both PMF and LDA in an online learning fashion, in which both PMF and LDA tasks can be reinforced each other during the online learning process. Our encouraging experimental results on real-world data validate the effectiveness of the proposed method.", "histories": [["v1", "Sat, 28 May 2016 10:17:37 GMT  (40kb)", "http://arxiv.org/abs/1605.08872v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["chenghao liu", "tao jin", "steven c h hoi", "peilin zhao", "jianling sun"], "accepted": false, "id": "1605.08872"}, "pdf": {"name": "1605.08872.pdf", "metadata": {"source": "CRF", "title": "Online Bayesian Collaborative Topic Regression", "authors": ["Chenghao Liu", "Tao Jin", "Steven C.H. Hoi", "Peilin Zhao", "Jianling Sun"], "emails": ["twinsken@zju.edu.cn,", "taoj@zju.edu.cn,", "chhoi@smu.edu.sg,", "zhaop@i2r.a-star.edu.sg,", "sunjl@zju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.08 872v 1 [cs.L G] 28 May 2"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related work", "text": "In this section, we review two groups of studies related to our work, including (1) variants of CTR models and (2) online Bayesian inference.Variants of CTR model: Researchers have extended CTR models to various applications of recommender systems. Some researchers extend CTR models by integrating them with other ancillary information. In CTRsmf [Purushotham et al., 2012], authors integrate CTR with social matrix factoring models to account for social correlation between users. In LA-CTR [Kang and Lerman, 2013], they assume that users do not uniformly distribute their attention among other people. In HFT [McAuley and Leskovec, 2013], they have expanded hidden factors in product ratings with hidden topics in product reviews for product recommendations. Some researchers have expanded CTR to other recommendation tasks. In CSTR [Ding et al, 2013], authors have examined how to recommend network context in the social learning context."}, {"heading": "3 Collaborative Topic Regression: Revisited", "text": "Let us suppose that there are I User and J Items. Any sample of data is a three-stage model (i, j, rij) in which i, 2, \u00b7 II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II"}, {"heading": "4 Online Bayesian Collaborative Topic Regression", "text": "It is worth noting that this small modification 1 (b) does not break the main structure of the CTR, and our online parameter estimation method could be applied to the various variants of the CTR introduced in Section 2.CTR. It is worth noting that this small modification does not break the main structure of the CTR. In its parameter estimation methods we can be applied to the various variants of the CTR, but the flow of information is one-sided, which ignores that vj could guide feedback to the topics."}, {"heading": "5 Experimental Results", "text": "Our experiments were carried out using an extended MovieLens data set called \"MovieLens 10M Plot 2,\" resulting from the MovieLens 10M3. Specifically, the original2We will release the data set after the work has been accepted. 3http: / / grouplens.org / datasets / movielens / MovieLens 10M data set provides a total of 10,000,053 evaluation records for 10,681 movies (items) from 69,878 users. However, the original data set has very limited text information. We enrich the data set by collecting additional text content for each of the film objects. Specifically, we first used its identification number for each film object to find the movie listed on the IMDb4 website, and then collected its associated text of the \"action summary.\" We then combine the \"action summary\" text with the title and category text, which we use in MovieLens's document 10M, which is a distinction document for 2011."}, {"heading": "5.1 Experimental Setup and Metric", "text": "For each experiment, we randomly mix the evaluation records and then divide them into two parts: The first 90% of the mixed evaluation records are used as training data and the remaining 10% as a test set. In addition, we randomly draw 5% from the training data as a validation set for parameter selection. To make fair comparisons, all algorithms are performed over 5 experimental runs of different random permutations. To measure performance, we evaluate the performance of our proposed method of prediction task by measuring the Root Mean Square Error (RMSE). In the online learning experiments, we evaluate the RMSE performance on the test set according to 50,000 online iterations each. In addition, we evaluate the performance of topic modeling using the log probability of each word in the text collection [Hoffman et al., 2010]."}, {"heading": "5.2 Baselines for Comparison and Experimental Settings", "text": "In our experiments, we evaluate the proposed OBCTR algorithms for evaluating predictions by comparing them with some important baselines as follows: \u2022 PA-I: An online learning algorithm for solving online collaborative filter tasks by using the popular online passive-aggressive (PA) algorithm [Blondel et al., 2014]; \u2022 CTR: the existing Collaborative Topic Regression [Wang and Blei, 2011]. In our context, we replace the ALS algorithm [Hu et al., 2008] with the SGD algorithm [Koren et al., 2009], since the evaluation data is explicit and leaves the rest the same as the original CTR (note that the LDA step is still performed in batch manners); \u2022 OCTR: In order to evaluate the effectiveness of the joint optimization, we propose a simplified version of OBCTR: 1, namely 4.0,00,00,00,00,00,00,00,000.00 / 000,00,00,00,000,000,000,000,000,000,000 / 000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,"}, {"heading": "5.3 Evaluation of Online Rating Prediction Tasks", "text": "Figure 2 (a), 2 (b), 2 (c) compares the online performance of the above methods in K = 5, K = 10 and K = 20. We note that the CTR method took at least 6 hours to calculate the parameters by a batch variational inference algorithm. Figure 2 shows only their performance in the downstream collaborative filter phase. As we can see from Figure 2 (a), 2 (b), 2 (c), the CTRbased approaches exceed the online CF algorithm (PAI) in most cases, which are consistent with the experiments in [Wang and Lead, 2011] and the effectiveness of using additional text information to improve the performance of PMF for online rating prediction tasks. Secondly, among the various CTR-based approaches, the proposed OBCTR optimization is feasible in most cases."}, {"heading": "5.4 Performance on Online Topic modeling Tasks", "text": "Figure 2 (d) shows the results of the average online predictive probability for OBCTR and online LDA. Online learning allows us to make a large-scale comparison. We see that OBCTR consistently performs better than online LDA, which ignores valuation information regardless of how many topics we use. This is due to the use of valuation information to discover the low topic proportions in which OBCTR adds value to this task."}, {"heading": "5.5 Evaluation of Parameter Sensitivity", "text": "Figure 3 (a) shows how the increase in the RMSE results from the choice of two key parameters, \u03c3\u0442 and \u03c3r, in OBCTR.As shown in Figure 3 (a), an increase in the RMSE results in a rapid reduction in the RMSE. Once a certain optimum value is reached, a further increase in the RMSE can be achieved gradually. Secondly, we have found that the optimum value of the RMSE also largely depends on the definition of the parameter \u03c3r. If the RMSE is smaller, the optimum value is relatively smaller. However, once the optimum value is reached, the further performance changes are limited, suggesting that it is relatively easy to choose a good value of the RMSE due to its lower sensitivity within the range of the optimum values. Our results were consistent with the similar phenomena observed in [Wang and Lead, 2011]. Figure 3 (b) shows the effect of an increase in the model complexity K. This investigation is carried out by selecting the best achievable comparative parameters to compare the RMSE with the significant search parameters during the comparative period."}, {"heading": "6 Conclusion", "text": "This paper examined online learning algorithms for the practical implementation of Collaborative Topic Regression (CTR) techniques for real online recommendation systems. In contrast to CTR, which loosely combines LDA and PMF, we propose a novel online Bayesian CTR algorithm (OBCTR) that performs a joint optimization of LDA and PMF to achieve close coupling. Our encouraging results showed that OBCTR converges much faster than the other competing algorithms in online learning, thus achieving the best predictive performance among all comparable algorithms."}], "references": [{"title": "the Journal of machine Learning research", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics", "author": ["Mathieu Blondel", "Yotaro Kubo", "Naonori Ueda. Online passive-aggressive algorithms for non-negative matrix factorization", "completion"], "venue": "pages 96\u2013104,", "citeRegEx": "Blondel et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C Wilson", "Michael I Jordan. Streaming variational bayes"], "venue": "pages 1727\u20131735,", "citeRegEx": "Broderick et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 2612\u20132618", "author": ["Xuetao Ding", "Xiaoming Jin", "Yujia Li", "Lianghao Li. Celebrity recommendation with collaborative social topic regression. In Proceedings of the TwentyThird international joint conference on Artificial Intelligence"], "venue": "AAAI Press,", "citeRegEx": "Ding et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic collapsed variational bayesian inference for latent dirichlet allocation", "author": ["Foulds et al", "2013] James Foulds", "Levi Boyles", "Christopher DuBois", "Padhraic Smyth", "Max Welling"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Online variational bayesian learning", "author": ["Zoubin Ghahramani", "H Attias"], "venue": "Slides from talk presented at NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani and Attias. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "In advances in neural information processing systems", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei. Online learning for latent dirichlet allocation"], "venue": "pages 856\u2013864,", "citeRegEx": "Hoffman et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley. Stochastic variational inference"], "venue": "14(1):1303\u20131347,", "citeRegEx": "Hoffman et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "2008", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky. Collaborative filtering for implicit feedback datasets. In Data Mining"], "venue": "ICDM\u201908. Eighth IEEE International Conference on, pages 263\u2013272. IEEE,", "citeRegEx": "Hu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "La-ctr: A limited attention collaborative topic regression for social media", "author": ["Jeon-Hyung Kang", "Kristina Lerman"], "venue": "arXiv preprint arXiv:1311.1247,", "citeRegEx": "Kang and Lerman. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Computer", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky. Matrix factorization techniques for recommender systems"], "venue": "(8):30\u201337,", "citeRegEx": "Koren et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Content-based collaborative filtering for news topic recommendation", "author": ["Zhongqi Lu", "Zhicheng Dou", "Jianxun Lian", "Xing Xie", "Qiang Yang"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM,", "citeRegEx": "McAuley and Leskovec. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The population posterior and bayesian inference on streams", "author": ["James McInerney", "Rajesh Ranganath", "David M Blei"], "venue": "arXiv preprint arXiv:1507.05253,", "citeRegEx": "McInerney et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse stochastic inference for latent dirichlet allocation", "author": ["David Mimno", "Matt Hoffman", "David Blei"], "venue": "arXiv preprint arXiv:1206.6425,", "citeRegEx": "Mimno et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Advances in neural information processing systems", "author": ["Andriy Mnih", "Ruslan Salakhutdinov. Probabilistic matrix factorization"], "venue": "pages 1257\u20131264,", "citeRegEx": "Mnih and Salakhutdinov. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Collaborative topic regression with social matrix factorization for recommendation systems", "author": ["Sanjay Purushotham", "Yan Liu", "C-C Jay Kuo"], "venue": "arXiv preprint arXiv:1206.4684,", "citeRegEx": "Purushotham et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of The 31st International Conference on Machine Learning", "author": ["Tianlin Shi", "Jun Zhu. Online bayesian passive-aggressive learning"], "venue": "pages 378\u2013386,", "citeRegEx": "Shi and Zhu. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen. Deep content-based music recommendation"], "venue": "pages 2643\u20132651,", "citeRegEx": "Van den Oord et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Chong Wang", "David M Blei. Collaborative topic modeling for recommending scientific articles"], "venue": "pages 448\u2013456. ACM,", "citeRegEx": "Wang and Blei. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "pages 2719\u20132725", "author": ["Hao Wang", "Binyi Chen", "Wu-Jun Li. Collaborative topic regression with social regularization for tag recommendation. In Proceedings of the TwentyThird international joint conference on Artificial Intelligence"], "venue": "AAAI Press,", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "arXiv preprint arXiv:1409.2944,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Relational stacked denoising autoencoder for tag recommendation", "author": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Collaborative Topic Regression (CTR) has been actively explored in recent years [Wang and Blei, 2011].", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "More specifically, CTR combines the idea of probabilistic matrix factorization (PMF) [Mnih and Salakhutdinov, 2007] for predicting ratings, and the idea of probabilistic topic modeling, e.", "startOffset": 85, "endOffset": 115}, {"referenceID": 21, "context": "CTR has been shown as a promising method that produces more accurate and interpretable results and has been successfully applied in many recommender systems, such as tag recommendation [Wang et al., 2013; Lu et al., 2015], and social recommender systems [Purushotham et al.", "startOffset": 185, "endOffset": 221}, {"referenceID": 12, "context": "CTR has been shown as a promising method that produces more accurate and interpretable results and has been successfully applied in many recommender systems, such as tag recommendation [Wang et al., 2013; Lu et al., 2015], and social recommender systems [Purushotham et al.", "startOffset": 185, "endOffset": 221}, {"referenceID": 17, "context": ", 2015], and social recommender systems [Purushotham et al., 2012; Kang and Lerman, 2013].", "startOffset": 40, "endOffset": 89}, {"referenceID": 9, "context": ", 2015], and social recommender systems [Purushotham et al., 2012; Kang and Lerman, 2013].", "startOffset": 40, "endOffset": 89}, {"referenceID": 20, "context": "Despite being studied actively [Wang and Blei, 2011; Wang et al., 2013], the existing CTR techniques suffer from several critical limitations.", "startOffset": 31, "endOffset": 71}, {"referenceID": 21, "context": "Despite being studied actively [Wang and Blei, 2011; Wang et al., 2013], the existing CTR techniques suffer from several critical limitations.", "startOffset": 31, "endOffset": 71}, {"referenceID": 17, "context": "In CTRsmf [Purushotham et al., 2012], authors integrated CTR with social matrix factorization models to take social correlation between users into account.", "startOffset": 10, "endOffset": 36}, {"referenceID": 9, "context": "In LA-CTR [Kang and Lerman, 2013], they assumed that users divide their limited attention non-uniformly over other people.", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "In HFT [McAuley and Leskovec, 2013], they aligned hidden factors in product ratings with hidden topics in product reviews for product recommendations.", "startOffset": 7, "endOffset": 35}, {"referenceID": 3, "context": "In CSTR [Ding et al., 2013], authors explored how to recommend celebrities to general users in the context of social network.", "startOffset": 8, "endOffset": 27}, {"referenceID": 21, "context": "In CTR-SR [Wang et al., 2013], authors adapted CTR model by combining both item-tag matrix and item content information for tag recommendation tasks.", "startOffset": 10, "endOffset": 29}, {"referenceID": 22, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 23, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 19, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 20, "context": "However, all of these work follow the same parameter estimation scheme as [Wang and Blei, 2011] in a batch learning mode.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 7, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 10, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 2, "context": "To relax this assumption, researchers in [Broderick et al., 2013; Ghahramani and Attias, 2000] made streaming updates to the estimated posterior.", "startOffset": 41, "endOffset": 94}, {"referenceID": 5, "context": "To relax this assumption, researchers in [Broderick et al., 2013; Ghahramani and Attias, 2000] made streaming updates to the estimated posterior.", "startOffset": 41, "endOffset": 94}, {"referenceID": 2, "context": "For example, [Broderick et al., 2013] explored a mean-field variational Bayes algorithm for LDA inference.", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "In addition, [McInerney et al., 2015] introduced the population Variational Bayes (PVB) method which combines traditional Bayesian inference with the frequentist idea of the population distribution for streaming inference.", "startOffset": 13, "endOffset": 37}, {"referenceID": 18, "context": "[Shi and Zhu, 2014] proposed the Online Bayesian Passive-Aggressive (BayesPA) method for max-margin Bayesian inference of online streaming data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "CTR approximately infers posterior p(Z,\u03a6,\u0398|W) of LDA model via variational inference method [Blei et al., 2003].", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "CTR adopts the ALS algorithm [Hu et al., 2008] to solve an implicit feedback problem.", "startOffset": 29, "endOffset": 46}, {"referenceID": 11, "context": "In our context, we use the SGD algorithm [Koren et al., 2009] since ratings data are explicit.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "If we add the constant log p(W)p(R) to the objective, it is the minimization of KL(q(U,V,Z,\u03a6,\u0398)\u2016p(U,V,Z,\u03a6,\u0398|W,R)), which is similar with the variational formulation of original LDA [Blei et al., 2003].", "startOffset": 181, "endOffset": 200}, {"referenceID": 2, "context": "Inspired by streaming Bayesian inference [Broderick et al., 2013; Ghahramani and Attias, 2000], on the arrival of new data (i, j, rij ,wj), if we treat the posterior after observing t \u2212 1 samples as the new prior, the post-data posterior distribution qt+1(ui,vj , zj ,\u03a6,\u0398) is equivalent to the solution of the following optimization problem: min q KL[q(ui,vj , zj ,\u03a6,\u0398)\u2016qt(ui,vj , zj ,\u03a6,\u0398))]", "startOffset": 41, "endOffset": 94}, {"referenceID": 5, "context": "Inspired by streaming Bayesian inference [Broderick et al., 2013; Ghahramani and Attias, 2000], on the arrival of new data (i, j, rij ,wj), if we treat the posterior after observing t \u2212 1 samples as the new prior, the post-data posterior distribution qt+1(ui,vj , zj ,\u03a6,\u0398) is equivalent to the solution of the following optimization problem: min q KL[q(ui,vj , zj ,\u03a6,\u0398)\u2016qt(ui,vj , zj ,\u03a6,\u0398))]", "startOffset": 41, "endOffset": 94}, {"referenceID": 15, "context": "This hybird strategy has shown promising performance for LDA [Mimno et al., 2012; Shi and Zhu, 2014].", "startOffset": 61, "endOffset": 100}, {"referenceID": 18, "context": "This hybird strategy has shown promising performance for LDA [Mimno et al., 2012; Shi and Zhu, 2014].", "startOffset": 61, "endOffset": 100}, {"referenceID": 20, "context": "For detailed text preprocessing, we follow the same procedure as the one described in [Wang and Blei, 2011] to process text information.", "startOffset": 86, "endOffset": 107}, {"referenceID": 20, "context": "Note that we did not consider the CiteUlike dataset 5 as used in the previous study [Wang and Blei, 2011], because their dataset only provides \u201clike\u201d and \u201cdislike\u201d preference, which is kind of implicit feedback and thus unsuitable for our regression task.", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": "In addition, we also evaluate the performance of topic modeling via the log-likelihood of each word in text collection [Hoffman et al., 2010].", "startOffset": 119, "endOffset": 141}, {"referenceID": 1, "context": "\u2022 PA-I: An online learning algorithm for solving online collaborative filtering tasks by applying the popular online Passive-Aggressive (PA) algorithm [Blondel et al., 2014];", "startOffset": 151, "endOffset": 173}, {"referenceID": 20, "context": "\u2022 CTR: the existing Collaborative Topic Regression [Wang and Blei, 2011] .", "startOffset": 51, "endOffset": 72}, {"referenceID": 8, "context": "In our context, we replace the ALS algorithm [Hu et al., 2008] with SGD algorithm [Koren et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 11, "context": ", 2008] with SGD algorithm [Koren et al., 2009] since ratings data are explicit, and keep the rest same as the original CTR (note that the LDA step is still performed in a batch manner);", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Table 1: RMSE results after a single pass over training set OCTR, which runs online LDA [Hoffman et al., 2010] for LDA part and SGD for PMF part (but without joint optimization as OBCTR) sepearately.", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": "Besides, to evaluate the topic modeling performance, we also compare our method with the typical Online LDA method: \u2022 Online-LDA: an online Bayesian variational inference algorithom for LDA model[Hoffman et al., 2010].", "startOffset": 195, "endOffset": 217}, {"referenceID": 20, "context": "As we can see from Figure 2(a),2(b),2(c), the CTRbased approaches outperform the online CF algorithm (PAI) for most cases, which is in line the experiments in [Wang and Blei, 2011] and validates the efficacy of leveraging additional text information to improve the performance of PMF for online rating prediction tasks.", "startOffset": 159, "endOffset": 180}, {"referenceID": 20, "context": "Our results were consistent to the similar phenomena observed in [Wang and Blei, 2011].", "startOffset": 65, "endOffset": 86}], "year": 2016, "abstractText": "Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix factorization (PMF) and topic modeling (e.g., LDA) for recommender systems, which has gained increasing successes in many applications. Despite enjoying many advantages, the existing CTR algorithms have some critical limitations. First of all, they are often designed to work in a batch learning manner, making them unsuitable to deal with streaming data or big data in real-world recommender systems. Second, the document-specific topic proportions of LDA are fed to the downstream PMF, but not reverse, which is sub-optimal as the rating information is not exploited in discovering the lowdimensional representation of documents and thus can result in a sub-optimal representation for prediction. In this paper, we propose a novel scheme of Online Bayesian Collaborative Topic Regression (OBCTR) which is efficient and scalable for learning from data streams. Particularly, we jointly optimize the combined objective function of both PMF and LDA in an online learning fashion, in which both PMF and LDA tasks can be reinforced each other during the online learning process. Our encouraging experimental results on real-world data validate the effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}