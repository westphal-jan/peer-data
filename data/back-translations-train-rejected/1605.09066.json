{"id": "1605.09066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2016", "title": "Distributed Asynchronous Dual Free Stochastic Dual Coordinate Ascent", "abstract": "In this paper, we propose new Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA), and provide the proof of convergence rate for two cases: the individual loss is convex and the individual loss is non-convex but its expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model is a popular method and often has better performances than stochastic gradient descent methods in solving regularized convex loss minimization problems. Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, and can be applied to non-convex problem when its dual problem is meaningless. We extend Dual-Free Stochastic Dual Coordinate Ascent method to the distributed mode with considering the star network in this paper.", "histories": [["v1", "Sun, 29 May 2016 21:33:07 GMT  (7kb)", "http://arxiv.org/abs/1605.09066v1", null], ["v2", "Wed, 27 Jul 2016 03:29:57 GMT  (509kb)", "http://arxiv.org/abs/1605.09066v2", null], ["v3", "Sat, 19 Nov 2016 20:32:35 GMT  (511kb)", "http://arxiv.org/abs/1605.09066v3", null], ["v4", "Fri, 27 Oct 2017 03:06:07 GMT  (386kb)", "http://arxiv.org/abs/1605.09066v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhouyuan huo", "heng huang"], "accepted": false, "id": "1605.09066"}, "pdf": {"name": "1605.09066.pdf", "metadata": {"source": "CRF", "title": "Distributed Asynchronous Stochastic Dual Coordinate Ascent without Duality", "authors": ["Zhouyuan Huo"], "emails": ["zhouyuan.huo@mavs.uta.edu", "heng@uta.edu"], "sections": [{"heading": null, "text": "The results of the experimental analysis in [18] confirm that the SDCA method has strong theoretical convergence properties and often performs better than stochastic gradient lineage (SGD). In [6] the paper points out that the SDCA method is a variation of the SGD method and its updating is based on an unbiased estimate of gradients. Unlike most of the SGD methods, which directly solve primary problems as its name suggests, the SDCA is a dual problem of (1)."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach", "author": ["Mingyi Hong"], "venue": "arXiv preprint arXiv:1412.6058,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1604.03584,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1c", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Slow learners are fast", "author": ["John Langford", "Alexander Smola", "Martin Zinkevich"], "venue": "arXiv preprint arXiv:0911.0491,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Alex J Smola", "Kai Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "An asynchronous parallel randomized kaczmarz algorithm", "author": ["Ji Liu", "Stephen J Wright", "Srikrishna Sridhar"], "venue": "arXiv preprint arXiv:1401.4780,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Sdca without duality", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Mini-batch primal and dual methods for svms", "author": ["Martin Tak\u00e1\u010d", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1303.2314,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed mini-batch sdca", "author": ["Martin Tak\u00e1c", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1507.08322,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Tianbao Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Asynchronous distributed admm for consensus optimization", "author": ["Ruiliang Zhang", "James Kwok"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast distributed asynchronous sgd with variance reduction", "author": ["Ruiliang Zhang", "Shuai Zheng", "James T Kwok"], "venue": "arXiv preprint arXiv:1508.01633,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 14, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 1, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 11, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 12, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 13, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 15, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 17, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 17, "context": "Experimental results in [18] verify that SDCA method enjoys strong theoretical convergence guarantee properties and often has better performances than stochastic gradient descent (SGD) based methods.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "In [6], the paper points out that SDCA is a variation of SGD method, and its update is based on an unbiased estimate of gradient.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], a variation of SDCA was proposed and applied to problems in which individual \u03c6i is non-convex.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 20, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 10, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 7, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 0, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 2, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 9, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 3, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 19, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 6, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 18, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 4, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 16, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 18, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 4, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 16, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 0, "context": "References [1] Alekh Agarwal and John C Duchi.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Mingyi Hong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Zhouyuan Huo and Heng Huang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Martin Jaggi, Virginia Smith, Martin Tak\u00e1c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] John Langford, Alexander Smola, and Martin Zinkevich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mu Li, David G Andersen, Alex J Smola, and Kai Yu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ji Liu, Stephen J Wright, and Srikrishna Sridhar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Mark Schmidt, Nicolas Le Roux, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Martin Tak\u00e1\u010d, Avleen Bijral, Peter Richt\u00e1rik, and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Martin Tak\u00e1c, Peter Richt\u00e1rik, and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Lin Xiao and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Tianbao Yang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Ruiliang Zhang and James Kwok.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Ruiliang Zhang, Shuai Zheng, and James T Kwok.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "i=1 \u2016\u2207\u03c6i(w) \u2212\u2207\u03c6i(w )\u2016 \u2264 2L ( P (w)\u2212 P (w)\u2212 \u03bb 2 \u2016w \u2212 w\u2016 ) (9) This lemma was proved in [13].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we propose new Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA), and provide the proof of convergence rate for two cases: the individual loss is convex and the individual loss is non-convex but its expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model is a popular method and often has better performances than stochastic gradient descent methods in solving regularized convex loss minimization problems. Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, and can be applied to non-convex problem when its dual problem is meaningless. We extend Dual-Free Stochastic Dual Coordinate Ascent method to the distributed mode with considering the star network in this paper.", "creator": "LaTeX with hyperref package"}}}