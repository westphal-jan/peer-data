{"id": "1605.09082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "One-Pass Learning with Incremental and Decremental Features", "abstract": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically.", "histories": [["v1", "Mon, 30 May 2016 01:18:47 GMT  (287kb,D)", "http://arxiv.org/abs/1605.09082v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chenping hou", "zhi-hua zhou"], "accepted": false, "id": "1605.09082"}, "pdf": {"name": "1605.09082.pdf", "metadata": {"source": "CRF", "title": "One-Pass Learning with Incremental and Decremental Features", "authors": ["Chenping Hou", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1. Preliminaries", "text": "For the first time in a long time, I have been able to say that I have been able to do this for a long time, and that I have been able to do it for a long time, and that I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time, and I have been able to do it for a long time."}, {"heading": "2. The OPID Approach", "text": "It is a real application problem in two stages, and it is difficult to solve this problem."}, {"heading": "3. Optimization and Extension", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Optimization", "text": "C-stage: The optimization problem in Eq. (1) can be divided into T1 = > q (= > Identity) (T) (like Online ADMM [WB12] to solve it by scanning the data only once. Nevertheless, our goal is to learn h-q (s) to support the classification in E-level, and the most direct way is to use a linear classifier. In this case, we will provide more effective one-pass learning methods than ADMM.Proposition 1 The optimal solution for Eq. (2) can be achieved by solving A [T1] W (s) = B [T1], with B [T], T-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X), which is a linear classifier that can be used to scan scan scan scan scan only scan the data once."}, {"heading": "3.2. Extension", "text": "(3), the interaction between two classifiers is an equilibrium of classification results. (3), the interaction between two classifiers is an equilibrium of classification results by rotating the weights. (1), the more direct way is to combine all the features as in the stack. (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (2), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (), (1), (, (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1, (1), (1), (1), (), (), (1, (1), (1), (1), (), (1, (1), (1), (1), (), (1, (), (), (1, (1), (), (), (), (, (1), (1), (, (), (1), (1), (, (, (), (), (1), (, (1), (), (, (), (, (, (), (, (), (, (), (, (), (, (, (), (), (, (, (), (, (), (), (,), (), (, (), (, (, (), (), (, (,), (, (,), (), (,), (, (), (,), (, (), ("}, {"heading": "4. Experimental Results", "text": "There are two implementations (Eq. (3) and Eq. (16) of our algorithms (1). We call the OPID method with ensembles (Eq. (3) as OPIDe and the implementation in Eq. (16) is still referred to as OPID. For simplicity, in Eq. (16), we take examples (x) = x and thus the classifiers are linear. For fairness, the classifiers in Eq. (3) are also linear. We implement them using the LibLinear toolbox [FCH + 08] with L2-regulated logistic regressions and the parameters are matched by fivefold cross-validation. The multi-class problem is tackled by a vs-rest strategy. We will compare OPID with other related methods. To show if the classifiers are trained in Cstage, we will also practice linear logistic regression with L2."}, {"heading": "4.1. Classification Accuracy Comparison", "text": "In order to present the results both intuitively and qualitatively, we report the results of the first six data in the form of tables and the rest with numbers. The results of different methods on different data sets are presented in Table 2 and Figure 2, respectively. These results result in several observations. (1) If we use the C-level trained classifier to support learning in the E-level, the test performance will increase significantly, especially if the training points in the E-level are rare. This is in line with intuition, since the support from the C-level will be weaker with the increase in training points. (2) Compared with the accuracy of SVM (a), our results show a remarkable improvement. This confirms that our data may, to some extent, inherit the metric from the C-level. (3) It seems that the improvement of our method in relation to other approaches is much greater than the improvement of our approaches in multi-level scenarios. The reason for this may be that the data may inherit the metric from the C-level."}, {"heading": "4.2. The Influence of the Number of Survived Features", "text": "Unlike traditional problems, there are three types of characteristics in our settings. To illustrate the effectiveness of our methods, we vary the percentage of surviving characteristics and compare our methods with other related work. As in the previous section, we also conduct experiments on SensIT Vehicle and Gisette. Similar to the assignment of three types of characteristics, we select different percentages of traits in the middle as a surviving trait, and the rest is assigned as a vanished trait and an extended trait with the same number of characteristics. Comparative results are shown in Fig. (3). There are at least two observations from Fig. (3). (1) Our proposed methods outperform conventional methods, regardless of how high the percentage of surviving characteristics is. It confirms the effectiveness of our method in dealing with the problem in this setting. (2) With the increase in the number of surviving characteristics, OPID and OPIDe achieve higher accuracies in SensVehicle Data Total Features. Nevertheless, the reason for this may be that the number of surviving characteristics is low in the tower."}, {"heading": "5. Conclusion", "text": "In this paper, we examine the problem of learning with incremental and degressive traits and propose the one-pass learning approach, where not all data optimization needs to be retained. Our approach is particularly useful when both data and traits are evolving, while robust learning performance is required and highly scalable because it only needs to be scanned once every instance. In this paper, we focus on a one-time trait change, where the surviving and extended traits do not disappear. It will be interesting to expand to multi-layered trait changes, where the surviving and extended traits can disappear later."}], "references": [{"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Incremental and decremental support vector machine learning", "author": ["Gert Cauwenberghs", "Tomaso A. Poggio"], "venue": "NIPS", "citeRegEx": "Cauwenberghs and Poggio.,? \\Q2000\\E", "shortCiteRegEx": "Cauwenberghs and Poggio.", "year": 2000}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar.,? \\Q2006\\E", "shortCiteRegEx": "Demsar.", "year": 2006}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["David L. Donoho"], "venue": "In AMS Conference on Math Challenges of the 21st Century,", "citeRegEx": "Donoho.,? \\Q2000\\E", "shortCiteRegEx": "Donoho.", "year": 2000}, {"title": "Learning to classify with missing and corrupted features", "author": ["Ofer Dekel", "Ohad Shamir"], "venue": "ICML", "citeRegEx": "Dekel and Shamir.,? \\Q2008\\E", "shortCiteRegEx": "Dekel and Shamir.", "year": 2008}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "One-pass AUC optimization", "author": ["Wei Gao", "Rong Jin", "Shenghuo Zhu", "Zhi-Hua Zhou"], "venue": "ICML", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Nightmare at test time: Robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "ICML", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Accuracy and Stability of Numerical Algorithms", "author": ["Nicholas J. Higham"], "venue": "SIAM, Philadelphia, PA, USA,", "citeRegEx": "Higham.,? \\Q2002\\E", "shortCiteRegEx": "Higham.", "year": 2002}, {"title": "Classification with low rank and missing data", "author": ["Elad Hazan", "Roi Livni", "Yishay Mansour"], "venue": "ICML", "citeRegEx": "Hazan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2015}, {"title": "Overview of sensors and needs for environmental monitoring", "author": ["Clifford K. Ho", "Alex Robinson", "David R. Miller", "Mary J. Davis"], "venue": null, "citeRegEx": "Ho et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2005}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Advances in Kernel Methods: Support Vector Learning", "author": ["Bernhard Sch\u00f6lkopf", "Christopher J.C. Burges", "Alexander J. Smola", "editors"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "The mars\u2013a multiagent recommendation system for games on mobile phones", "author": ["Pavle Skocir", "Luka Marusic", "Marinko Marusic", "Ana Petric"], "venue": "In 6th KES International Conference,", "citeRegEx": "Skocir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Skocir et al\\.", "year": 2012}, {"title": "Sensors, chemical sensors, electrochemical sensors, and ecs", "author": ["Joseph R. Stetter", "William R. Penrose", "Sheng Yao"], "venue": "Journal of The Electrochemical Society,", "citeRegEx": "Stetter et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Stetter et al\\.", "year": 2003}, {"title": "Convex learning with invariances", "author": ["Choon Hui Teo", "Amir Globerson", "Sam T. Roweis", "Alexander J. Smola"], "venue": "NIPS", "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "ICML", "citeRegEx": "Wang and Banerjee.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Banerjee.", "year": 2012}, {"title": "One-pass multi-view learning", "author": ["Yue Zhu", "Wei Gao", "Zhi-Hua Zhou"], "venue": "ACML", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Zhi-Hua Zhou"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Zhou.", "year": 2012}, {"title": "Online bandit learning for a special class of non-convex losses", "author": ["Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou"], "venue": "AAAI", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically.", "creator": "LaTeX with hyperref package"}}}