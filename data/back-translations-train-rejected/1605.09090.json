{"id": "1605.09090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention", "abstract": "In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.", "histories": [["v1", "Mon, 30 May 2016 02:47:35 GMT  (162kb)", "http://arxiv.org/abs/1605.09090v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang liu", "chengjie sun", "lei lin", "xiaolong wang"], "accepted": false, "id": "1605.09090"}, "pdf": {"name": "1605.09090.pdf", "metadata": {"source": "CRF", "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention", "authors": ["Yang Liu", "Chengjie Sun", "Lei Lin"], "emails": ["yliu@insun.hit.edu.cn", "cjsun@insun.hit.edu.cn", "linl@insun.hit.edu.cn", "wangxl@insun.hit.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.09 090v 1 [cs.C L] 30 M"}, {"heading": "1 Introduction", "text": "There were three types of relationships in RTE, Entailment (derived to be true), Contradiction (derived to be false) and Neutral (truth unknown).A few examples were also mentioned in Table 1.Traditional methods in RTE, but not widely used because of its complexity and domain limitations. Recently published Stanford Natural Language Inference (SNLI1) Corpus makes it possible to use deep learning methods (Bos and Markert, 2005)."}, {"heading": "2 Our approach", "text": "In our thesis, we treated the RTE task as a supervised three-way classification problem. The overall architecture of our model is in Figure 1. The design of this model follows the idea of the Siamese network that the two identical sentence encoders share the same weights during training and the two sentence representations are then combined to create a \"relationship vector\" for classification. As we can see from the figure, the model essentially consists of three parts: (A). The sentence input module; (B). The sentence encoding module; (C). The sentence adaptation module. The last two parts are explained in detail in the following section. And the sentence input module is introduced in Section 3.3."}, {"heading": "2.1 Sentence Encoding Module", "text": "In order to generate better sentence representations, we used a two-step strategy for coding sentences. First, the average pooling layer was built on word-level biLSTMs to generate sentence vectors; this simple encoder, combined with the sentence matching module, formed the basic architecture of our model; with far fewer parameters, this basic model alone can easily outperform the type-of-state method (see Table 3); second, the attention mechanism was applied to the same sentence, rather than visiting words in the source sentence with target sentence representation, we used the representation generated in the previous stage to visit words that occurred in the sentence itself, resulting in a similar distribution with other attention mechanism weights; and more attention was paid to more important words. 2The idea of \"inner attention\" was inspired by the observation that when people read a sentence, they could roughly form an intuition about which part of the sentence is more important."}, {"heading": "2.2 Sentence Matching Module", "text": "After generating the sentence vectors, three matching methods were used to extract relationships between the premise and hypothesis. \u2022 Chain of the two representations \u2022 Element-wise product \u2022 Element-wise differenceThis matching architecture was first used by (Mou et al., 2015). Finally, we used a SoftMax layer over the output of a nonlinear projection of the generated matching vector for classification."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DataSet", "text": "To evaluate the performance of our model, we conducted our experiments at the corpus of the Stanford Natural Language Inference (SNLI) (Bos and Markert, 2005). For 570K pairs, SNLI is two orders of magnitude larger than any other resource of its type. The data set consists of crowdsourcing efforts, with each set of people written. Target markers include three classes: Entailment, Contraction, and Neutral2Recently (Yang et al., 2016) suggested a hierarchical attention model for the task of document classification, but whose mechanism is randomly initialized. (two irrelevant sets) We applied the standard move / validation / test split, which included 550k, 10k, and 10k samples, respectively."}, {"heading": "3.2 Parameter Setting", "text": "The training goal of our model is cross-entropy loss, and we use minibatch SGD with the Rmprop (Tieleman and Hinton, 2012) for optimization, the batch size is 128. To initialize the word embedding, a dropout layer with a dropout rate of 0.25 was applied. In our model, we used pre-formed 300D glove 840B vectors (Pennington et al., 2014) to initialize the word embedding. Words outside the vocabulary in the training set are randomly initialized by evenly scanning values (0.05, 0.05). None of these embedding will be updated during the training. We did not adjust the word embedding for two reasons: 1. To reduce the number of parameters needed for training. 2. Keep their representation in the inference time close to invisible similar words, which improves the generating ability of the model's open source kernel."}, {"heading": "3.3 The Input Strategy", "text": "In this part, we examined four strategies for modifying the inputs to our basic model that help us increase performance, the four strategies are: \u2022 Inverting Premises (Sutskever et al., 2014) \u2022 Doubling Premises (Zaremba and Sutskever, 2014) \u2022 Doubling Hypothesis \u2022 Differentiating Inputs (removing identical words appeared in premises and hypotheses) Experimental results were illustrated in Table 2. As we can see from this, duplicating the hypotheses and differentiating the inputs improved both the performance of our model. While the hypotheses are usually much shorter than premises, the duplication hypothesis can absorb this difference and emphasize the importance of this strategy twice. Differentiating the input strategy forces the model to focus on another part of the two sentences that can help classify neutral and contradiction examples, as we have observed that our model tended to result in insecure conversation instances of the original T-T: and T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T."}, {"heading": "3.4 Comparison Methods", "text": "In this part, we compared our model with the following kind-of-the-state basic approaches: \u2022 LSTM enc: 100D LSTM encoders + MLP. (Bowman et al., 2015) \u2022 GRU enc: 1024D GRU encoders + skip-thoughts + cat, -. (Vendrov et al., 2015) \u2022 TBCNN enc: 300D Tree-based CNN encoders + cat, -. (Mou et al., 2015) \u2022 SPINN enc: 300D SPINN-NP encoders + cat, \u0445, -. (Bowman et al., 2016) \u2022 Static attention: 100D LSTM + static attention. (Rockta \ufffd schel et al., 2015) \u2022 WbW attention: 100D LSTM + word-by-word attention. (Rockta \ufffd schel et al., 2015) The cat refers to linking, - and denote elements in terms of product difference, easier and easier to understand."}, {"heading": "3.5 Results and Qualitative Analysis", "text": "Although the classification of RTE examples is not based solely on representations achieved through attention, it is nevertheless instructive for the analysis of the mechanism of inner attention, as we observed a large increase in performance after application. We hand-picked several examples from the data set to visualize them. To make the weights more differentiated, we did not use uniform Color Atlas crosses. That is, each sentence has its own color batla, the lightest color and the darkest color denoting the least attention weight or the greatest value within the set. Visualizations of inner attention on these examples are shown in Figure 2. We observed that more attention was paid to nons, verbs and adjectives. This is consistent with our experience that these words are more semantic than functional words. While the mean pooling considered each word of equal importance, the attention mechanism helps to re-weight words according to their importance, and more focused and precise phrases were generated based on attention-producing agents."}, {"heading": "4 Conclusion and Future work", "text": "In this paper, we have proposed a bi-directional LSTM-based model with inner-attention to solve the RTE problem. We have an idea to use attention mechanisms within a sentence that can teach itself to read words without the information of another. The inner-Attention mechanism helps to generate more precise sentence representations through attention vectors. Furthermore, the simple effective distraction strategy we have introduced further increases our results, and this model can easily be adapted to other sentence comparison models. Our future work includes: 1. Use this architecture for other sentence-making tasks such as question-answer, paraphrase and sentence-text similarity, etc. 2. Try more heuristic matching methods to fully exploit the sentence vectors."}, {"heading": "Acknowledgments", "text": "We thank all anonymous reviewers for their hard work!"}], "references": [{"title": "Recognising textual entailment with logical inference", "author": ["Bos", "Markert2005] Johan Bos", "Katja Markert"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Bos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2005}, {"title": "Christopher Potts", "author": ["Samuel R Bowman", "Gabor Angeli"], "venue": "and Christopher D Manning.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Christopher D Manning", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta"], "venue": "and Christopher Potts.", "citeRegEx": "Bowman et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rui Yan", "author": ["Lili Mou", "Men Rui", "Ge Li", "Yan Xu", "Lu Zhang"], "venue": "and Zhi Jin.", "citeRegEx": "Mou et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "and Phil Blunsom.", "citeRegEx": "Rockt\u00e4schel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bing Xiang", "author": ["Ming Tan"], "venue": "and Bowen Zhou.", "citeRegEx": "Tan et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Lecture 6.5-rmsprop. COURSERA: Neural networks for machine learning", "author": ["Tieleman", "Hinton2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Sanja Fidler", "author": ["Ivan Vendrov", "Ryan Kiros"], "venue": "and Raquel Urtasun.", "citeRegEx": "Vendrov et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alex Smola", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He"], "venue": "and Eduard Hovy.", "citeRegEx": "Yang et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a firststage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence\u2019s first-stage representation to attend words appeared in itself, which is called \u201dInner-Attention\u201d in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \u201dInner-Attention\u201d mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.", "creator": "LaTeX with hyperref package"}}}