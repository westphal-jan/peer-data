{"id": "1605.09186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Does Multimodality Help Human and Machine for Translation and Image Captioning?", "abstract": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.", "histories": [["v1", "Mon, 30 May 2016 11:47:00 GMT  (218kb,D)", "https://arxiv.org/abs/1605.09186v1", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16)"], ["v2", "Thu, 2 Jun 2016 13:52:45 GMT  (218kb,D)", "http://arxiv.org/abs/1605.09186v2", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16) v2: Added softmax equations for attentions and revised conclusion"], ["v3", "Mon, 13 Jun 2016 15:33:11 GMT  (218kb,D)", "http://arxiv.org/abs/1605.09186v3", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16) v2: Added softmax equations for attentions and revised conclusion v3: Small revisions"], ["v4", "Tue, 16 Aug 2016 12:11:29 GMT  (209kb,D)", "http://arxiv.org/abs/1605.09186v4", "7 pages, 2 figures, v4: Small clarification in section 4 title and content"]], "COMMENTS": "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ozan caglayan", "walid aransa", "yaxing wang", "marc masana", "mercedes garc\\'ia-mart\\'inez", "fethi bougares", "lo\\\"ic barrault", "joost van de weijer"], "accepted": false, "id": "1605.09186"}, "pdf": {"name": "1605.09186.pdf", "metadata": {"source": "CRF", "title": "Does Multimodality Help Human and Machine for Translation and Image Captioning?", "authors": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer", "Le Mans"], "emails": ["FirstName.LastName@lium.univ-lemans.fr", "joost@cvc.uab.es", "mmasana@cvc.uab.es", "yaxing@cvc.uab.es", "ocaglayan@gsu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al., 2014) competed successfully in last year's WMT evaluation campaign (Bojar et al., 2015). In the same trend, it was proposed by (Elliott et al., 2015) to generate descriptions of images using DNNs. Several attempts were made to integrate functions from different modalities to help the automated system better model the task (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a). This paper describes the systems developed by LIUM and CVC that participated in the two proposed tasks for the multimodal translation campaign."}, {"heading": "2 Multimodal Machine Translation", "text": "This task consists of translating an English sentence which translates an image into German in view of the English sentence itself and the image described therein."}, {"heading": "2.1 Phrase-based System", "text": "Our basic system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA + + (Och and Ney, 2003), which is trained using data provided by the organizers and coordinated with MERT (Och, 2003) to BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) results on the validation set.ar Xiv: 160 5.09 186v 4 [cs.C L] 16 Aug 201 6We also have Continuous Space Language Model1 (Schwenk, 2010) with the auxiliary functions support for the of (Aransa et al., 2015)."}, {"heading": "2.2 Neural MT System", "text": "The basic model we experimented2 is an attention-based encoder decoder approach (Bahdanau et al., 2014), except for some notable changes to the recursive decoder called Conditional GRU.1github.com / hschwannk / cslm-toolkit 2github.com / nyu-dl / dl4mt-tutorialWe define by X and Y, a source set of length N and a target set of length M. Each source and target word is represented by an embedding vector of the dimension EX and EY respectively: X = (x1, x2,..., xN), xi REX (1) Y = (y1, y2,..., yM), yj REY (2) A bi-directional recursive encoder reads an input sequence EX forward and backward to generate two sets of hidden states based on the current input and previous state."}, {"heading": "2.2.1 Training", "text": "We chose the following hyperparameters for all NMT systems for both task 1 and task 2. All embedding and repetition levels have a dimensionality of 620 and 1000, respectively. We used Adam as a stochastic optimizer with a minibatch size of 32, Xavier weight initialization (Glorot and Bengio, 2010) and L2 regularization with \u03bb = 0.0001, with the exception of the monomodal Task-1 system, for which the choice fell to Adadelta, samples from N (0, 0.01) and L2 regularization with \u03bb = 0.0005. Network performance is evaluated using BLEU validation splitting after each 1000 minibatch update, and training is stopped if BLEU does not improve for 20 evaluation periods. Training times were 16 and 26 hours for monomodal and multimodal systems on a Tesla K40 GPU, respectively. Finally, we used a classic beam search to the left of a set of 12 beam sizes."}, {"heading": "2.3 Data", "text": "Phrase-based and NMT systems for task 1 are trained using the data set provided by the organizers and described in Table 1. This data set consists of 29K parallel sentences (direct translations of image descriptions from English into German) for training, 1014 for validation, and finally 1000 for the test set. We pre-processed the data set using Moses punctuation normalization, tokenization, and reduction scripts. To better generalize the composition structures in German, we applied a compound splitter 3 (Sennrich and Haddow, 2015) to the German vocabulary of training and validation sentences, reducing the target vocabulary from 18670 to 15820 unique tokens. During translation generation, the splitted connections are sewn back together."}, {"heading": "2.4 Results and Analysis", "text": "The results of our phrase-based baseline and of the four systems submitted are presented in Table 2. The BL + 4Features system is the rescoring of the 1000 best baseline values using all the features described in 2.1, while BL + 3Features has the same but no FC7 image features. Overall, we were able to improve the values of the test sets for METEOR and BLEU by about 0.4 and 0.8, respectively, using a strong phrase-based baseline with auxiliary features. In the NMT systems, the monomodal NMT achieved a comparative BLEU value of 32.50 compared to 33.45 for the phrase-based baseline. The multimodal NMT system described in Section 3.2 achieved relatively lower values when trained on the data of task 1."}, {"heading": "3 Multimodal Image Description Generation", "text": "The aim of Task 2 is to create German image descriptions using the image itself and one or more English descriptions as input.3github.com / rsennrich / wmt2014-scripts."}, {"heading": "3.1 Visual Data Representation", "text": "In a groundbreaking paper, Krizhevsky et al. (Krizhevsky et al., 2012) convincingly demonstrated that CNNs provide a far superior image representation compared to previously used handmade image features. Based on this success, more intensive research efforts began to further improve representation on CNNs. Simonyan and Zisserman's work (Simonyan and Zisserman, 2014) improved the network by splitting large revolutionary features into several layers of small revolutionary features that allowed to train a much deeper network. Organizers make these features available to all participants. Specifically, they offer the features from the fifth convolutionary layer and the features from the second fully interconnected layer of VGG-19. Recently, residual networks (ResNet et al, 2015) were proposed (ResNet et al), which learn residual features that allow network layers (or projection layers) to be added to prevent the network layers (or network layers) from being built deeper (ResNet)."}, {"heading": "3.2 Multimodal NMT System", "text": "The multimodal NMT system is an extension of (Xu et al., 2015) and the monomodal NMT system described in Section 2.2.The first GRU layer calculates an intermediate representation of s \u2032 j as follows: s \u2032 j = (1 \u2212 z \u2032 j) s \u2032 j + z \u2032 j sj \u2212 1 (3) s \u2032 j = tanh (W \u2032 E [yj \u2212 1] + r \u2032 j (U \u2032 sj \u2212 1)) (4) r \u2032 j = (W \u2032 rE [yj \u2212 1]) (W \u2032 rsj \u2212 1) (5) z \u2032 j = (W \u2032 zetj [yj \u2212 1] + U \u2032 zsj \u2212 1) (6), where E \u2032 sj \u2212 1) is the target word embedding, s \u2032 j \u2032 j = the hidden state (r \u2032 j \u2212 1) and the hidden layers of Uzmodation (r \u2032 j)."}, {"heading": "3.2.1 Generation", "text": "Since we get 5 source descriptions for each image in order to generate a single German description, we have the NMT generate a German description for each image and select the ones with the highest probability and preferably without UNK tokens."}, {"heading": "3.3 Data", "text": "The organizers provided an extended version of the Flickr30K entities dataset (Elliott et al., 2016), which, in addition to the 5 original English descriptions contained in the dataset, contains 5 independently produced German descriptions for each image. It is possible to use this dataset either by taking into account the cross-product of 5 source and 5 target descriptions (a total of 25 description pairs for each image) or by taking only the 5 pairs of descriptions, each resulting in 725K and 145K training pairs, respectively. We opted for the smaller subset of 145K sets. Preprocessing is exactly the same as task 1, except that we only use sentence pairs with sentence lengths [3, 50] and a ratio of no more than 3. This results in a final training dataset of 131sets (Table 4). We selected the most common German 10K words and replaced the rest with a UNK token for the target page."}, {"heading": "3.4 Results and Analysis", "text": "As we see in Table 5, the multimodal system does not outperform the monomodal NMT system. Several explanations can explain this behavior. First, the architecture is not well suited to integrate image and text representations, which is possible because we did not explore all the possibilities to benefit from both modalities. Another explanation is that the image context contains too much irrelevant information that cannot be distinguished by the lone attention mechanism, which would require a deeper analysis of attention weights to be answered."}, {"heading": "4 Human multimodal description", "text": "In order to evaluate the significance of the different modalities for the task of image description generation, we conducted an experiment in which we replaced the computer algorithm with human participants, the two modalities being the five English description sentences and the image. The result is a single description sentence in German. In the experiment, participants were asked to perform the following tasks: \u2022 Both the image and the English descriptions are given: \"Describe the image in one sentence in German. You can get help from the English sentences provided.\" \u2022 The experiment was performed by 16 native German speakers aged 23 to 54 (from Austria, Germany and Switzerland, 10 of whom are female and 6 male). The experiment is performed on the first 80 sentences of the validation group: \"Translate the English sentence into German.\" The participants completed 10 repetitions for each task, and not the repetition of the same image of the tasks. The results of the experiments are presented in Table 6."}, {"heading": "5 Conclusion and Discussion", "text": "The results show that integrating image features into a multimodal neural MT system with a shared attention mechanism does not yet exceed the performance of a text-only monomodal system. However, our multimodal systems improve a picture signature system (which was expected), and the phrase-based system can benefit from resorting with a multimodal neural speech model as well as resorting with a neural MT system. We have also presented the results of a human evaluation that performs the same tasks as the challenge. The results are fairly clear: captions can benefit from multimodality."}, {"heading": "Acknowledgments", "text": "We thank KyungHyun Cho and Orhan Firat for providing the DL4MT tutorial as open source and Kelvin Xu for the arcticcaptions5 system."}], "references": [{"title": "Improving continuous space language models using auxiliary features", "author": ["Walid Aransa", "Holger Schwenk", "Loic Barrault."], "venue": "Proceedings of the 12th International Workshop on Spoken Language Translation, pages 151\u2013158, Da Nang, Vietnam, Decem-", "citeRegEx": "Aransa et al\\.,? 2015", "shortCiteRegEx": "Aransa et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709. m2cr.univ-lemans.fr github.com/kelvinxu/arctic-captions", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, United Kingdom, July.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel."], "venue": "Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 595\u2013603. JMLR Work-", "citeRegEx": "Kiros et al\\.,? 2014a", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel."], "venue": "CoRR, abs/1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014b", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT \u201907, pages 228\u2013231, Strouds-", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Comput. Linguist., 29:19\u201351, March.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013 167, Stroudsburg, PA, USA. Association for Com-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Continuous space language models for statistical machine translation", "author": ["Holger Schwenk."], "venue": "The Prague Bulletin of Mathematical Linguistics, (93):137\u2013146.", "citeRegEx": "Schwenk.,? 2010", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "A joint dependency model of morphological and syntactic structure for statistical machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 114\u2013", "citeRegEx": "Sennrich and Haddow.,? 2015", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002), pages 901\u2013904.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A Ehinger", "Aude Oliva", "Antonio Torralba."], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485\u20133492. IEEE.", "citeRegEx": "Xiao et al\\.,? 2010", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of The 32nd International Con-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al.", "startOffset": 70, "endOffset": 126}, {"referenceID": 23, "context": "Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al.", "startOffset": 70, "endOffset": 126}, {"referenceID": 1, "context": ", 2014) and (Bahdanau et al., 2014) competed successfully in the last year\u2019s WMT evaluation campaign (Bojar et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 3, "context": "In the same trend, generating descriptions from images using DNNs has been proposed by (Elliott et al., 2015).", "startOffset": 87, "endOffset": 109}, {"referenceID": 3, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 11, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 10, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 22, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 15, "endOffset": 30}, {"referenceID": 8, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 38, "endOffset": 54}, {"referenceID": 16, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 67, "endOffset": 86}, {"referenceID": 17, "context": "This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al.", "startOffset": 86, "endOffset": 97}, {"referenceID": 18, "context": "This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores on the validation set.", "startOffset": 115, "endOffset": 138}, {"referenceID": 13, "context": ", 2002) and METEOR (Lavie and Agarwal, 2007) scores on the validation set.", "startOffset": 19, "endOffset": 44}, {"referenceID": 19, "context": "We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al.", "startOffset": 53, "endOffset": 68}, {"referenceID": 0, "context": "We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 21, "context": "\u2022 VGG19-FC7 image features: The auxiliary feature used in the first CSLM are the image features provided by the organizers which are extracted from the FC7 layer (relu7) of the VGG-19 network (Simonyan and Zisserman, 2014).", "startOffset": 192, "endOffset": 222}, {"referenceID": 14, "context": "\u2022 Source side sentence representation vectors: We used the method described in (Le and Mikolov, 2014) to compute continuous space representation vector for each source (i.", "startOffset": 79, "endOffset": 101}, {"referenceID": 15, "context": "The two other scores used for n-best reranking are the log probability computed by our NMT system that will be described in the following section and the score obtained by a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010).", "startOffset": 222, "endOffset": 244}, {"referenceID": 1, "context": "The fundamental model that we experimented2 is an attention based encoder-decoder approach (Bahdanau et al., 2014) except some notable changes in the recurrent decoder called Conditional GRU.", "startOffset": 91, "endOffset": 114}, {"referenceID": 2, "context": "We use Gated Recurrent Unit (GRU) (Chung et al., 2014) activation function for both recurrent encoders and decoders.", "startOffset": 34, "endOffset": 54}, {"referenceID": 6, "context": "We used Adam as the stochastic optimizer with a minibatch size of 32, Xavier weight initialization (Glorot and Bengio, 2010) and L2 regularization with \u03bb = 0.", "startOffset": 99, "endOffset": 124}, {"referenceID": 20, "context": "In order to generalize better over the compound structs in German, we trained and applied a compound splitter3 (Sennrich and Haddow, 2015) over the German vocabulary of training and validation sets.", "startOffset": 111, "endOffset": 138}, {"referenceID": 12, "context": "(Krizhevsky et al., 2012) convincingly show that CNNs yield a far superior image representation compared to previously used hand-crafted image features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 21, "context": "The work of Simonyan and Zisserman (Simonyan and Zisserman, 2014) improved the network by breaking up large convolutional features into multiple layers of small convolutional features, which allowed to train a much deeper network.", "startOffset": 35, "endOffset": 65}, {"referenceID": 7, "context": "Recently, Residual Networks (ResNet) have been proposed (He et al., 2015).", "startOffset": 56, "endOffset": 73}, {"referenceID": 24, "context": "To select the optimal layer for image representation we performed an image classification task on a subsection of images from SUN scenes (Xiao et al., 2010).", "startOffset": 137, "endOffset": 156}, {"referenceID": 25, "context": "(Xu et al., 2015).", "startOffset": 0, "endOffset": 17}, {"referenceID": 25, "context": "The multimodal NMT system is an extension of (Xu et al., 2015) and the monomodal NMT system described in Section 2.", "startOffset": 45, "endOffset": 62}, {"referenceID": 24, "context": "Figure 1: Classification accuracy on a subset of SUN scenes (Xiao et al., 2010) for ResNet-50: The colored groups represent the building blocks while the bars inside are the stacked blocks (He et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 7, "context": ", 2010) for ResNet-50: The colored groups represent the building blocks while the bars inside are the stacked blocks (He et al., 2015).", "startOffset": 117, "endOffset": 134}, {"referenceID": 25, "context": "(Xu et al., 2015).", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "A shared attention layer similar to (Firat et al., 2016) that consists of a fully-connected feedforward network is used to compute a set of modality specific attention coefficients emod ij at each timestep j:", "startOffset": 36, "endOffset": 56}, {"referenceID": 4, "context": "The organizers provided an extended version of the Flickr30K Entities dataset (Elliott et al., 2016) which contains 5 independently crowd-sourced German descriptions for each image in addition to the 5 English descriptions originally found in the dataset.", "startOffset": 78, "endOffset": 100}], "year": 2016, "abstractText": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.", "creator": "TeX"}}}