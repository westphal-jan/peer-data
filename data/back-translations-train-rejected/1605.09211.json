{"id": "1605.09211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Going Deeper for Multilingual Visual Sentiment Detection", "abstract": "This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO). The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release, adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet, providing comparable or better performance with reduced network parameter cost.", "histories": [["v1", "Mon, 30 May 2016 12:57:44 GMT  (694kb,D)", "http://arxiv.org/abs/1605.09211v1", "technical report, 7 pages"]], "COMMENTS": "technical report, 7 pages", "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.CV", "authors": ["brendan jou", "shih-fu chang"], "accepted": false, "id": "1605.09211"}, "pdf": {"name": "1605.09211.pdf", "metadata": {"source": "CRF", "title": "Going Deeper for Multilingual Visual Sentiment Detection", "authors": ["Brendan Jou", "Shih-Fu Chang"], "emails": [], "sections": [{"heading": null, "text": "Since the image pool in MVSO1 can be corrupted by user noise from social interactions, we have divided a subcorpus of MVSO images based on tag-limited queries for higher loyalty label. We show that as a result of this higher loyalty label, more powerful AlexNet styled2 ANP detectors can be trained with the tag-limited image subset compared to models in the whole body. We publish all of these newly trained models for public research along with the list of tag-restricted images from the MVSO dataset."}, {"heading": "1. INTRODUCTION", "text": "Following the trend in many other areas, the advent of high-volume and poorly monitored data is the increased interest in large-scale sentiment studies in affective computing.5 However, they tend to explore the problem of data economy, since such specialized psychology terminology is less likely than that found in large quantities in computer vision and multimedia. Several works have since achieved affective representations of the middle level to bridge the affective gap between the low levels and the high levels."}, {"heading": "2. BRIEF OVERVIEW OF MVSO", "text": "Multilingualism Visual Sentiment Ontology (MVSO), 1 a significant extension and enhancement of Visual Sentiment Ontology (VSO), 6 consists of over 15.6K sentimental middle-level visual concepts using the semantic construct of adjective-noun pairs (ANPs). Ontology was constructed by using seeds of emotion keywords from \"Plutchik's Wheel of Emotions\" 9, where they were translated into 12 different languages by native speakers. For each language, these keywords were used to query the Flickr API to retrieve a large corpus of images with related tags and other metadata."}, {"heading": "3. DEEP VISUAL SENTIMENT CONCEPT DETECTOR BANKS", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "4. EXPERIMENTS & DISCUSSION", "text": "All of the new experiments used a single NVIDIA GeForce GTX Titan X GPU and were implemented with Caffe.11 \u2020 See https: / / github.com / BVLC / caffe / tree / master / models / bvlc _ reference _ caffenet for details on the parameters. \u2021 Note that we are using the first iteration of GoogLeNet / Inception, sometimes referred to as Inception-v1, although there have been several suggestions for improvements since that use duplicate 3 x 3 layers or integrate residual learning."}, {"heading": "4.1 Hybrid-pool Multilingual ANP Detectors", "text": "This year, we will be able to go in search of a solution that is capable of finding a solution that meets the needs of the people, \"he said."}, {"heading": "4.2 Tag-pool Multilingual ANP Detectors", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "5. CONCLUSION", "text": "In this technical report, we introduced new ANP detector banks for MVSO for six major languages that use a more modern network architecture than in the original MVSO release. These detectors have reduced memory space compared to their predecessors and achieve highly competitive or even significantly better ANP detection performance, e.g. a 35.05% improvement in English ANP detection. In addition, we presented ANP detectors that use a limited subset of images and ANPs that come only from tag-based Flickr API retrieval results, resulting in significantly higher ANP classification performance due to improved label quality. We also released the new inception-based ANP detector banks, tag-pool CaffeNet ANP detector banks, and the list of image URLs that correspond to the tag-restricted MVSO subset used in this report. In the future, we would like to have further fine-tune the MVSO detector systems, for example, that we have determined in this technical report that MVSO's original training systems are actually fine-tuned."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Miriam Redi, Mercan Topkara, Nikolaos Pappas and Tao Chen from the Multilingual Visual Sentiment Ontology (MVSO) team for their continued support and insightful discussions, and we would also like to thank Margaret Yuying Qian for doing some of the early tag-restricted partitioning and statistics."}], "references": [{"title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology", "author": ["B. Jou", "T. Chen", "N. Pappas", "M. Redi", "M. Topkara", "Chang", "S.-F."], "venue": "[ACM Multimedia (MM) ], (2015).", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "[Neural Information Processing Systems (NIPS) ], (2012).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "T. Darrell", "Chang", "S.-F."], "venue": "arXiv preprint arXiv:1410.8586 (2014).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "[Computer Vision and Pattern Recognition (CVPR) ], (2015).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Affective Computing", "author": ["R.W. Picard"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "Chang", "S.-F."], "venue": "[ACM Multimedia (MM) ], (2013).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Complura: Exploring and leveraging a large-scale multilingual visual sentiment ontology", "author": ["H. Liu", "B. Jou", "T. Chen", "M. Topkara", "N. Pappas", "M. Redi", "Chang", "S.-F."], "venue": "[Intl Conf. on Multimedia Retrieval (ICMR) ], (2016).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Multilingual visual sentiment concept matching", "author": ["N. Pappas", "M. Redi", "M. Topkara", "B. Jou", "H. Liu", "T. Chen", "Chang", "S.-F."], "venue": "[Intl Conf. on Multimedia Retrieval (ICMR) ], (2016).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion: A Psychoevolutionary Synthesis", "author": ["R. Plutchik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1980}, {"title": "SentiCart: Cartography & geo-contextualization for multilingual visual sentiment", "author": ["B. Jou", "M.Y. Qian", "Chang", "S.-F."], "venue": "[ACM International Conference on Multimedia Retrieval (ICMR) ], (2016).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "[ACM Multimedia (MM) ], (2014).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "SentiBank: Large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["D. Borth", "T. Chen", "R. Ji", "Chang", "S.-F."], "venue": "[ACM Multimedia (MM) ], (2013).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "Intl Journ. of Computer Vision (IJCV) 115(3) (2015).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400 (2014).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO).1 The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release,1 adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture2 by fine-tuning from DeepSentiBank.3 Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet,4 providing comparable or better performance with reduced network parameter cost. In addition, since the image pool in MVSO1 can be corrupted by user noise from social interactions, we partitioned out a sub-corpus of MVSO images based on tag-restricted queries for higher fidelity labels. We show that as a result of these higher fidelity labels, higher performing AlexNet-styled2 ANP detectors can be trained using the tag-restricted image subset as compared to the models in full corpus. We release all these newly trained models for public research use along with the list of tag-restricted images from the MVSO dataset.", "creator": "LaTeX with hyperref package"}}}