{"id": "1605.09221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent", "abstract": "This paper presents research in progress investigating the viability and adaptation of reinforcement learning using deep neural network based function approximation for the task of radio control and signal detection in the wireless domain. We demonstrate a successful initial method for radio control which allows naive learning of search without the need for expert features, heuristics, or search strategies. We also introduce Kerlym, an open Keras based reinforcement learning agent collection for OpenAI's Gym.", "histories": [["v1", "Mon, 30 May 2016 13:23:04 GMT  (585kb,D)", "http://arxiv.org/abs/1605.09221v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["timothy j o'shea", "t charles clancy"], "accepted": false, "id": "1605.09221"}, "pdf": {"name": "1605.09221.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent", "authors": ["Timothy J O\u2019Shea", "Charles Clancy", "T. C. Clancy"], "emails": [], "sections": [{"heading": null, "text": "IntroductionRadios are ubiquitous in modern society.Between mobile devices, portable devices, computer devices, medical devices and other devices we transport and operate on a daily basis, radio frequency communication has become the most ubiquitous and convenient way we communicate information in our daily lives. Unfortunately, our frequency resources are limited and our data needs are growing in an apparently unlimited way.Despite this wireless frequency scarcity, our methods of allocating, adjusting and optimizing frequency usage remain very much in the dark even today.Spectrum is still statically allocated, and devices are unaware of the use or availability of resources in their immediate environment.The field of cognitive radio and dynamic frequency access has attempted to address this by introducing expert systems that attempt to perform frequency sensing and some degree of characterization of their environment, and devices are unaware of the effects of this environment, but its effects have been greatly limited by their inability to spread, new regions, and general protocols."}, {"heading": "1 Reinforcement Learning Policy", "text": "We introduce KeRLym [9], a Python-written set of deep gain learning tools that implements GPU-optimized deep neural networks based on Theano [2] and TensorFlow [12]. OpenAI recently released Gym [8], a collection of learning benchmark environments for amplification, an easy-to-use API, and a web-based high-score algorithm comparison board. We use this API in our reinforcement tutorial to provide a standard agent interface and quickly provide a wide range of tasks to test their performance and alignment."}, {"heading": "1.1 Policy Learning", "text": "Since Google Deepmind's Nature Paper on Deep-Q Networks [3], there has been a steep increase in interest in the capabilities of enhanced learning, we are using an approximation of the politics of the deep neural network. This is an exciting and growing area with much potential for improvement for future learning algorithms. For the scope of this work, we are implementing a parametric version of the Deep Q learning algorithm along with a Double Q Learning [7] implementation in the KeRLym toolbox. We are implementing a variety of functional approximation networks that can be used within these networks, including dense, fully connected networks, conventional networks similar to those in the Atari paper, and recurring networks that use LSTM that can improve sequence learning in POMDPs, as discussed in [6]. Approaches are similar for both algorithms, a value function Q, a form of a network; an upgraded; a conventional one; and a conventional one."}, {"heading": "1.2 Deep-Q Network Implementation", "text": "Our Q function Q (s, a, \u03b8) is a deep neural network with random initial parameters. We begin with an architecture similar to the Convolutionary Network used by Mnih et al in [3], but make changes that show improvements in our range and take into account the input information form. As we pass both scalar stored variables with sensor information and contiguous frequency domain values into the value function as the current state, we treat each input configuration value as independent, separate input with fully connected logic, while reducing the parameter space and allowing frequency domain filters to form on the power spectrum and use shift-immutable by using a series of revolutionary layers, similar to what we do for raw time domain samples in [10]. Ultimately, we link the activations of these two activation layers for output of the contiguous layers to a complete resource."}, {"heading": "2 Radio Search Environment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Environment Overview", "text": "Typical electronic devices, such as mobile phones, contain highly flexible Radio Frequency Integrated Circuits (RFIC) at this point, enabling frequency tuning and digitization of relatively large, arbitrary bands. Typically, they are programmed by a carrier in a very simple way to exert brute force through a small list of carrier-assigned channels and bandwidths, even though they are actually able to adjust to relatively arbitrary center frequencies between 100 MHz and 6 GHz, and often provide performance of two decimations of a bandwidth of 10-20 MHz. Instead of brute force when searching for signals on multiple carrier-centric bands, we suggest instead that machine learning provide a general search strategy to identify signals that provide useful connectivity while optimizing them for minimal search time, battery consumption, and potential power consumption."}, {"heading": "2.2 Environment Implementation", "text": "We start by building an environment for the Gym Reinforcement Learning environment to try to mirror our problem, and a reasonable set of assumptions for what a real system could do and feel, but with a relatively small amount of complexity for the initial work. We simulate a single radio receiver sampling at a bandwidth of 20 MHz, which can be decimated and retuned by discrete actions in 2. The discrete actions we allow are as follows, referring to the variables: medium frequency (fc), bandwidth (bw), maximum bandwidth (bwmax), minimum bandwidth (bwmin), maximum medium frequency (fcmax) and minimum medium frequency (fcmin).- Freq Down: fc \u2212 = max (bw / 2, MHcmin), MHcmin) - Freq Up: fc + = min (fcmax / 2, fcmax) - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: - end frequency: -: - end frequency: - end frequency: -: - end frequency: -: - end frequency: -: - end frequency: - end frequency: -: - end frequency: -: - end frequency: - end frequency: -: - end frequency: - end frequency: -: - end frequency: -: - end frequency: - end frequency: -: - end frequency: - end frequency: -: - end frequency: -: - end frequency: - end frequency: - end frequency: -: - end frequency: -: - end frequency: -: - end frequency: - end frequency: - end frequency: -: - end frequency: -: - end frequency: -: - end frequency: - end frequency:"}, {"heading": "3 Training Considerations", "text": "There are numerous ways to define penalties and rewards for this search process within the environment, which represent a number of different considerations for the training process. Below, we suggest three possible reward systems. Our agent's goal at runtime is to detect the signal that exists somewhere within the band, and locate the signal by zooming in on it using SW-L and SW-R actions. These rewards and penalties are intended to reflect this. Scheme A may lead to the fastest training rate and a simpler approach to directly rewarding good actions, Scheme B provides a strong deterrent effect for false positive actions, but slows learning, and Scheme C provides a simple end result that requires a style of delayed learning."}, {"heading": "4 Conclusions and Future Work", "text": "From this chart, it is clear that we are learning a relatively clear distinction between good and bad actions, as evidenced by the separation in the third chart, and that our reward is growing and our completion time is long enough to be successful part of the time. \"In future work, we hope to offer a more comprehensive trade between the goals described above, learn a policy that works at a more satisfactory reward level, and compare the impact of reward / punishment systems on the recipient's traditional operating characteristics, ROC, on performance curves. We are excited about the potential in this area, and positively, this approach will be fruitful."}], "references": [{"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: a method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "arXiv preprint arXiv:1509.06461", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Kerlym: keras reinforcement learning gym agents, https:// github.com/osh/kerlym, 2016", "author": ["T. O\u2019Shea"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Convolutional radio modulation recognition networks", "author": ["T.J. O\u2019Shea", "J. Corgan", "T.C. Clancy"], "venue": "arXiv preprint arXiv:1602.04105,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., \u201cMastering the game of go with deep neural networks and tree search\u201d, Nature, vol. 529, no. 7587, pp. 484\u2013489", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensorflow: large-scale machine learning on heterogeneous systems, 2015", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Software available from tensorflow. org,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Recent Work by Minh [3], Silver [11], Sutton [1], and others has begun to demonstrate the ability to learn exceedingly complex and varied tasks using deep neural network based policy function approximation to implement Q-Learning.", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "Recent Work by Minh [3], Silver [11], Sutton [1], and others has begun to demonstrate the ability to learn exceedingly complex and varied tasks using deep neural network based policy function approximation to implement Q-Learning.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "Recent Work by Minh [3], Silver [11], Sutton [1], and others has begun to demonstrate the ability to learn exceedingly complex and varied tasks using deep neural network based policy function approximation to implement Q-Learning.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "We also implement a general purpose open source Deep Neural network based Q-Learning function approximation learner for Gym using Keras primitives to learn a policy for rapidly exploring this environment through its set of discrete actions and observations [5].", "startOffset": 257, "endOffset": 260}, {"referenceID": 8, "context": "We introduce KeRLym [9], an open source deep reinforcement learning agent collection written in python using Keras to implement GPU optimized deep neural networks on top of Theano [2] and TensorFlow [12].", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "We introduce KeRLym [9], an open source deep reinforcement learning agent collection written in python using Keras to implement GPU optimized deep neural networks on top of Theano [2] and TensorFlow [12].", "startOffset": 180, "endOffset": 183}, {"referenceID": 11, "context": "We introduce KeRLym [9], an open source deep reinforcement learning agent collection written in python using Keras to implement GPU optimized deep neural networks on top of Theano [2] and TensorFlow [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 7, "context": "OpenAI recently released Gym [8], a collection of reinforcement learning benchmark environments, an API to easily use them, and a web based high-score board for algorithm comparison.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Since Google Deepmind\u2019s Nature paper on Deep-Q Networks [3], there has been a surge of interest in the capabilities of reinforcement learning use deep neural network policy network approximation.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "For the scope of this work we implement a parametric version of the Deep Q-Learning algorithm along with a Double Q-Learning [7] implementation in the KeRLym toolbox.", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "We implement a variety of function approximation networks which can be used inside them including dense fully connected networks, convolutional networks similar to those used in the Atari paper, and recurrent networks leveraging LSTM which may improve sequence learning in POMDPs as discussed in [6].", "startOffset": 296, "endOffset": 299}, {"referenceID": 3, "context": "001 in a Keras Adam [4] solver, and a discount rate of \u03b3 = 0.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "We start with a similar architecture to the convolutional network used by Mnih et al in [3], but make changes which show improvement in our domain and account for the input information form.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "Since we are passing both scalar stored variables containing sensor information, and contiguous frequency domain values into the value function as the current state, we treat each input configuration value as an independent discrete input with fully connected logic, while we reduce the parameter space and allow frequency domain filters to form and be used shift-invariantly on the power spectrum by using a set of convolutional layers, similar to our approach on raw time-domain samples in [10].", "startOffset": 492, "endOffset": 496}], "year": 2016, "abstractText": "This paper presents research in progress investigating the viability and adaptation of reinforcement learning using deep neural network based function approximation for the task of radio control and signal detection in the wireless domain. We demonstrate a successful initial method for radio control which allows naive learning of search without the need for expert features, heuristics, or search strategies. We also introduce Kerlym, an open Keras based reinforcement learning agent collection for OpenAI\u2019s Gym.", "creator": "LaTeX with hyperref package"}}}