{"id": "1605.09227", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Learning Combinatorial Functions from Pairwise Comparisons", "abstract": "A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support.", "histories": [["v1", "Mon, 30 May 2016 13:38:47 GMT  (436kb,D)", "http://arxiv.org/abs/1605.09227v1", "1 figure"]], "COMMENTS": "1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["maria-florina balcan", "ellen vitercik", "colin white"], "accepted": false, "id": "1605.09227"}, "pdf": {"name": "1605.09227.pdf", "metadata": {"source": "CRF", "title": "Learning Combinatorial Functions from Pairwise Comparisons", "authors": ["Maria-Florina Balcan", "Ellen Vitercik", "Colin White"], "emails": [], "sections": [{"heading": null, "text": "With this motivation in mind, we look at an alternative learning model in which the algorithm must learn the underlying function through to pairwise comparisons from pairwise comparisons. In this model, we present a number of novel algorithms that learn about a variety of combinatorial function classes, ranging from graph functions to broad classes of assessment functions that are fundamental to microeconomic theory, social networking analysis, and machine learning, such as coverage, submodular, XOS, and subadditive functions, and functions with sparse Fourier support."}, {"heading": "1 Introduction", "text": "The problem of ranking, based on pairs of comparisons, is present in many fields of application, ranging from algorithmic game theory [19] to computerized finance [15] to social networks [7]. Thus, a company may want to learn the combinatorial evaluation functions of its consumers, since this enables them to set better prices, choose which goods are sold as bundles, and set warehouse levels. Previous work on the learning evaluation functions may have focused on the model in which the learning algorithm gains access to a set of examples (bundles of goods) characterized by the underlying evaluation function [3, 1, 10, 2, 16]. However, for real evaluation functions, this cardinal data may not be accessible. In fact, it may be difficult for a consumer to offer a number that matches their evaluation for a bundle of goods. Instead, it may be more natural for them as if they like a bundle of goods or not."}, {"heading": "1.1 Our Results", "text": "Our formal definition of what it means to learn a function up to comparisons is similar to the PAC setting: We say that a class of functions is learnable if there is an efficient algorithm that outputs a comparison function that is highly likely to have the choice of examples that have a small error about the distribution. For some function classes, we require that the function of two sets can be sufficiently far apart to guarantee that the learned comparator accurately responds to these sets.Formally, in Section 3, we assume that for a fixed class F of combination functions that can be multiplied in F to a factor of \u03b1 (n), we can learn any function in F up to comparisons whose values differ by at least one multiplicative factor of \u03b1 (n)."}, {"heading": "1.2 Our Techniques", "text": "(Our techniques differ significantly from learning with real evaluated examples. When we try to learn a combinatorial function from cardinal data and not from ordinal data, the existence of an approximate linear function implies a natural learning algorithm, through a reduction to learning linear separators. In our model, where we are only allowed to ask comparison questions in pairs, we need a substantially different algorithm and a fundamentally different analysis. At a high level, the existence of an approximate linear function still implies a useful structure: Once we know that such a function ~ w exists and approximates the underlying function f to an \u03b1 (n) factor, then we will put two sentences S and S so that f (S) and f (S) are separated, we can learn a linear separator that classifies all sentences with a value less than f (S) as negative and classifies all sentences with a value greater than f (S) as positive."}, {"heading": "1.3 Related Work", "text": "Previous work has examined the learning ability of submodular and related functions by giving them access to a series of random examples characterized by the underlying function. Goemans et al. showed how to approach a submodular function within a multiplicative O-factor [12] in the query model, i.e. the queries are selected adaptively by the algorithm. Balcan and Harvey showed how to efficiently learn a function that provides the given submodular factor up to a fraction of the test inputs on 1 \u2212 fraction of the test inputs, with probability 1 \u2212 \u03b4, in the supervised learning environment [3]. They call this model the PMAC model of learning, where PMAC stands for \"probably mostly correct,\" due to its similarity to the PAC model of learning. They also show a lower limit in this model."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Combinatorial Functions", "text": "In the course of this work, we examine various classes of combinatorial functions. All functions we examine are defined by subsets of a basic set [n] = {1,..., n} and set 2 [n] to R. We use the indicator function of the set S, so a function f is subadditive if and only if (S) i = 1, otherwise (S) i = 0.We define three important function classes here and move the rest of the definitions to their respective sections. Subadditive functions. A function f is subadditive if and only if f (S) \u2264 f (S) + f (S), for all S, S \u2032 [n]. Intuitively, the value of a set is at most the sum of its parts. Submodular functions. A function f is submodular if and only if f (T) ulare, if and only if f (Xi) \u2212 f (S {i}) \u2212 f (S) for all S T functions that represent a submodular function."}, {"heading": "2.2 Learning Model", "text": "Let f be an unknown function from any class F (for example, the class of submodular functions) and assume that sentences are drawn from any distribution D over 2 [n]. Let's also assume that we have access to a pair-by-pair comparison oracle that yields 1 when typing S, S \u00b2 2 [n], if f (S) \u2264 f (S \u00b2) and 0 are different. We cannot hope to learn f (S) and f (S \u00b2) well in absolute terms in this model. Rather, our goal is to produce a hypothesis g that for most pairs S, S \u00b2 2 [n], either g accurately predicts which of the f (S) or f (S \u00b2) additive abilities are greater, or f (S) and f (S \u00b2) are separated by less than a multiplicative factor. We formally define this learning model as the following F (comparator-learnable separation)."}, {"heading": "3 General Framework for Learning with Comparisons", "text": "In this section, we present a general algorithm for learning combinatorial functions up to pairwise comparisons. We guarantee that for an underlying combinatorial function f, with a high probability, our algorithm produces a hypothesis where there is a random S, S and the probability that f (S) and f (S) differ by a large multiplicative factor. We then describe our algorithm for a large family of general function classes, each of which can be approximated in the class by the p-th root of the linear function f (n). We question this algorithm for many classes of combinatorial functions. To make this idea more concrete, we define the following characterization of a class of functions.Definition 2 (n) -approximable."}, {"heading": "3.1 Lower Bounds", "text": "Now we show that the class of non-negative, monotonous, submodular functions is not comparable to the class of non-negative, submodular functions that we can only learn in practice. - In addition, we use a special family of matroid functions that form a subset of the class of monotonous submodular functions specified in [3] and a family of matroid M = {MB] with the following properties. - For all k-8 with k = 2o (n1 / 3) there is a family of sets A 2 [n] and a family of matroid M = {MB} with the following properties. - For each k-8 with k and for each A-3. \u2022 For each B-3 there is a family of sets A A, rankMB (A) = {8 log kMB."}, {"heading": "4 Additive Separation Analysis", "text": "Let us f be a monotonous submodular function with range in [0, 1] and fix D to be the uniform distribution over the n-dimensional boolean cube. A minor variant of algorithm 1 allows us to learn an approximate factor, but not as a multiplicative factor as in Section 3. In this case, we say that f (S) is greater than f (S) or vice versa, whenever f (S) and f (S) is a sufficiently large additive factor \u03b2, as a multiplicative factor as in Section 3. In this case, we say that f is comparator-learnable with additive separation \u03b2.This result is based on important insights into the submodular function spectrum. In particular, we use the fact that each monotonous submodular function f is with range in [0, 1]."}, {"heading": "5 Application to Other Combinatorial Functions", "text": "We can expand Algorithm 1 to learn about many other classes of combinatorial functions up to pairwise comparisons, including evaluation functions with limited nonlinear interactions [24], Fourier functions with sparse quantities [23], and coverage functions [1, 9]. We will summarize the function classes we study in the following sections and give a brief motivation and description of our guarantees."}, {"heading": "5.1 Valuation Functions with Limited Nonlinear Interactions", "text": "We show that if the underlying evaluation function intuitively expresses nonlinear interactions between magnitudes to at most k (i.e. it is a function with k-limited nonlinear interactions), then algorithm 1 learns that f (S) and f (S) are sufficiently far apart to guarantee that the learned comparator will correctly predict with high probability. This is in contrast to previous results, where the guarantee that the learned comparator will correctly predict that we cf (S) < f (S) for c is sufficiently large. To define what we mean by limited nonlinear interactions, we use the notion of an interaction function. Let f: 2 [n] \u2192 R is an evaluation and we leave it."}, {"heading": "5.2 Fourier Sparse Set Functions", "text": "We can extend algorithm 1 to general combinatorial functions with Fourier support, which are contained in a set P '2. In addition, we achieve an even better complexity of the sample if we are guaranteed that the size of the Fourier support is limited by a constant k \u2264 | P |. Again, we do not need to require that f (S) and f (S) be separated by a sufficiently large multiplicative factor to ensure that the learned comparator predicts correctly with high probability. An important example of a function with sparse Fourier support is the intersection function of a graph G = (V, E) equipped with a weight function w: E \u2192 R. This function is defined as fG (A) = comparator with high probability."}, {"heading": "5.3 Coverage Functions", "text": "Coverage functions form a subset of the class of submodular functions and are used in combinatorial optimization, machine learning, and algorithmic game theory. A coverage function f is defined as [n], and each element in [n] corresponds to a subset of a universe U whose elements have nonnegative weights. The value from f to S [n] is the weight of the union of the corresponding subsets in U. We combine structural results that are specific to coverage functions from [1] and [9] to prove that coverage functions can be learned comparably by multiplicative separation (1 +) by using O queries (n3 / 5) that provide access to an ERM oracle for learning linear separators. In particular, we prove the following theorem, the proof of which can be found in Appendix D. Theorem 10. The class of coverage functions is comparable by multiplicative separation (+ 1 / 4) and example complexity (4) in Theorem 10."}, {"heading": "6 Conclusion", "text": "In this paper, we examine the learning model, in which the goal is to learn an underlying combinatorial function through to pairwise comparisons from pairwise comparisons. We present several general algorithms that can be used to learn about a variety of combinatorial function classes, including those consisting of submodular, XOS, subadditive, coverage, and Fourier sparse functions. We also demonstrate that the functions in a class F can be approximated by the p-th root of a linear function within a multiplicative factor (s), and for submodular functions with limited curvature. Specifically, we show that if the functions in a class F can be approximated by the p-th root of a linear function, within a multiplicative factor (s), then we can learn a comparison function that for most pairs S, S-D, either correctly predicts which of the f (S) or f (s) function is greater, S (f), and S (f)."}, {"heading": "Acknowledgments", "text": "We thank Peter Bartlett for the insightful initial discussions that led to the development of this research project, which was supported in part by NSF grants CCF-1451177, CCF-1422910, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) Fellowship."}, {"heading": "B Proofs from Section 3", "text": "The proof of claim 2 is: First, we will show the probability of the bad event happening for each (i, j) or each (i, j) or each (i, j), and then we will have all possible pairs (i, j) and P2 = PrS (i, j)."}, {"heading": "C Proofs from Section 4", "text": "S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S1, S1, S1, S1, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2,"}, {"heading": "D Proofs from Section 5", "text": "The interaction function allows us to express f-Fk as a linear function in nk-dimensional space. (Especially if we define S1 = 0 and 0 as a vector in Rn k (S) to be a vector in Rn kn (S). (This indicates a simple adjustment to algorithm 1: If we know that the underlying weighting function is f in Fk, then we can map each sample S (S) to a sample S (S) and try to learn linear threshold functions."}], "references": [{"title": "Sketching valuation functions", "author": ["Ashwinkumar Badanidiyuru", "Shahar Dobzinski", "Hu Fu", "Robert Kleinberg", "Noam Nisan", "Tim Roughgarden"], "venue": "In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning valuation functions", "author": ["Maria-Florina Balcan", "Florin Constantin", "Satoru Iwata", "Lei Wang"], "venue": "In The 25th Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning submodular functions", "author": ["Maria-Florina Balcan", "Nicholas JA Harvey"], "venue": "In Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "The modern theory of consumer behavior: Ordinal or cardinal", "author": ["William Barnett"], "venue": "The Quarterly Journal of Austrian Economics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "author": ["Ralph Allan Bradley", "Milton E Terry"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1952}, {"title": "Pairwise ranking aggregation in a crowdsourced setting", "author": ["Xi Chen", "Paul N Bennett", "Kevyn Collins-Thompson", "Eric Horvitz"], "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Networks, Crowds, and Markets", "author": ["David Easley", "Jon Kleinberg"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Open problem: The statistical query complexity of learning sparse halfspaces", "author": ["Vitaly Feldman"], "venue": "In Conference on Learning Theory, pages 1283\u20131289,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning coverage functions and private release of marginals", "author": ["Vitaly Feldman", "Pravesh Kothari"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Optimal bounds on approximation of submodular and xos functions by juntas", "author": ["Vitaly Feldman", "Jan Vondrak"], "venue": "In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Tight bounds on low-degree spectral concentration of submodular and XOS functions", "author": ["Vitaly Feldman", "Jan Vondr\u00e1k"], "venue": "In IEEE 56th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Approximating submodular functions everywhere", "author": ["Michel X Goemans", "Nicholas JA Harvey", "Satoru Iwata", "Vahab Mirrokni"], "venue": "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Privately releasing conjunctions and the statistical query barrier", "author": ["Anupam Gupta", "Moritz Hardt", "Aaron Roth", "Jonathan Ullman"], "venue": "SIAM Journal on Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Using the method of pairwise comparison to obtain reliable teacher assessments", "author": ["Sandra Heldsinger", "Stephen Humphry"], "venue": "The Australian Educational Researcher,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Options, Futures, and Other Derivatives", "author": ["John C. Hull"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Curvature and optimal algorithms for learning and minimizing submodular functions", "author": ["Rishabh K Iyer", "Stefanie Jegelka", "Jeff A Bilmes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Active ranking using pairwise comparisons", "author": ["Kevin G Jamieson", "Robert Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Sparse solutions for linear prediction problems", "author": ["Tyler Neylon"], "venue": "PhD thesis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Algorithmic Game Theory", "author": ["Noam Nisan", "Tim Roughgarden", "Eva Tardos", "Vijay V. Vazirani"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Learning pseudo-boolean k-dnf and submodular functions", "author": ["Sofya Raskhodnikova", "Grigory Yaroslavtsev"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence", "author": ["Nihar B. Shah", "Sivaraman Balakrishnan", "Joseph K. Bradley", "Abhay Parekh", "Kannan Ramchandran", "Martin J. Wainwright"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Absolute identification by relative judgment", "author": ["Neil Stewart", "Gordon DA Brown", "Nick Chater"], "venue": "Psychological review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Learning fourier sparse set functions", "author": ["Peter Stobbe", "Andreas Krause"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "The problem of ranking based on pairwise comparisons is present in many application domains ranging from algorithmic game theory [19] to computational finance [15] to social networks [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 2, "context": "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].", "startOffset": 219, "endOffset": 236}, {"referenceID": 0, "context": "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].", "startOffset": 219, "endOffset": 236}, {"referenceID": 9, "context": "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].", "startOffset": 219, "endOffset": 236}, {"referenceID": 1, "context": "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].", "startOffset": 219, "endOffset": 236}, {"referenceID": 15, "context": "Previous work on learning valuation functions has concentrated on the model in which the learning algorithm is given access to a set of examples (bundles of goods) which are labeled by the underlying valuation function [3, 1, 10, 2, 16].", "startOffset": 219, "endOffset": 236}, {"referenceID": 3, "context": "After all, it is well-known that humans are significantly better at comparing than scoring [4, 22].", "startOffset": 91, "endOffset": 98}, {"referenceID": 21, "context": "After all, it is well-known that humans are significantly better at comparing than scoring [4, 22].", "startOffset": 91, "endOffset": 98}, {"referenceID": 13, "context": "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].", "startOffset": 201, "endOffset": 214}, {"referenceID": 5, "context": "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].", "startOffset": 201, "endOffset": 214}, {"referenceID": 4, "context": "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].", "startOffset": 201, "endOffset": 214}, {"referenceID": 3, "context": "Research on judgment elicitation through pairwise comparisons is a fundamental problem in fields outside of computer science, ranging from psychology to economics to statistics, as well as many others [14, 6, 5, 4].", "startOffset": 201, "endOffset": 214}, {"referenceID": 11, "context": "Using existing approximation results [12, 2], we immediately conclude that several broad classes of combinatorial functions are comparator-learnable, including many that are ubiquitous in microeconomic theory.", "startOffset": 37, "endOffset": 44}, {"referenceID": 1, "context": "Using existing approximation results [12, 2], we immediately conclude that several broad classes of combinatorial functions are comparator-learnable, including many that are ubiquitous in microeconomic theory.", "startOffset": 37, "endOffset": 44}, {"referenceID": 15, "context": "We also rely on results from [16] and [2] to achieve stronger bounds for submodular functions if the curvature is small.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "We also rely on results from [16] and [2] to achieve stronger bounds for submodular functions if the curvature is small.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "We show in Section 4 that if the underlying distribution over subsets of [n] is uniform, then we can take advantage of key insights regarding the Fourier spectrum of monotone submodular functions with range in [0, 1], presented in [11], to learn such a function up to comparisons on pairs of sets whose values differ by a sufficiently large additive factor.", "startOffset": 210, "endOffset": 216}, {"referenceID": 10, "context": "We show in Section 4 that if the underlying distribution over subsets of [n] is uniform, then we can take advantage of key insights regarding the Fourier spectrum of monotone submodular functions with range in [0, 1], presented in [11], to learn such a function up to comparisons on pairs of sets whose values differ by a sufficiently large additive factor.", "startOffset": 231, "endOffset": 235}, {"referenceID": 0, "context": "We extend this result to XOS functions with range in [0, 1] as well.", "startOffset": 53, "endOffset": 59}, {"referenceID": 22, "context": "In particular, we present results for functions with sparse Fourier support [23] and functions with bounded nonlinear interactions [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Finally, for coverage functions [9, 1], we achieve \u03b1(n) = 1 + .", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "Finally, for coverage functions [9, 1], we achieve \u03b1(n) = 1 + .", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "XOS functions with distributional assumptions and range in [0,1] \u03b2 \u2208 (0, 1) \u00d5 ( n O ( 1 \u03b3 ) / 3 ) ,", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "Submodular functions with distributional assumptions and range in [0,1] \u03b2 \u2208 (0, 1) \u00d5 (", "startOffset": 66, "endOffset": 71}, {"referenceID": 11, "context": "learn an approximation of a submodular function within a multiplicative \u00d5( \u221a n) factor [12] in the membership query model, i.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "Balcan and Harvey showed how to efficiently learn a function that approximates the given submodular factor up to a \u221a n factor on a 1\u2212 fraction of the test inputs, with probability 1\u2212 \u03b4, in the supervised learning setting [3].", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "show near tight bounds on the PMAC learnability of subadditive functions and XOS (fractionally subadditive) functions [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 12, "context": "gave an algorithm with runtime nO(log(1/\u03b4)/ 2) which learns an approximation h to a submodular function f such that with high probability, |f(x)\u2212h(x)| \u2264 [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "show an algorithm with runtime 2\u00d5(1/ 4/5) \u00b7 n2 for approximating a submodular function with L2 error [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "showed that submodular functions always have an approximate function with a small sketch [1], and Iyer et al.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "showed parameterized bounds based on the curvature of the submodular function (how close the function is to being fully additive) [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "They show an algorithm to learn the rank using O(d log n) queries on average [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "study the ranking problem by assuming that the ranking reflects the inherent \u201cqualities\u201d of the objects, as defined by a vector ~ w\u2217 \u2208 Rn [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).", "startOffset": 202, "endOffset": 212}, {"referenceID": 1, "context": "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).", "startOffset": 202, "endOffset": 212}, {"referenceID": 15, "context": "By using structural results for submodular, XOS, and subadditive functions, as well as submodular functions with bounded curvature and XOS functions with a polynomial number of SUM trees, compiled from [3, 2, 16] we immediately obtain the following corollary to Theorem 1 (the formal proof is in Appendix B).", "startOffset": 202, "endOffset": 212}, {"referenceID": 2, "context": "To prove this result, we use a special family of matroid rank functions, which form a subset of the class of monotone submodular functions, presented in [3] and described as follows.", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "Theorem 2 (Theorem 7 in [3]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "As in [3], we use the family of matroids presented in Theorem 2 to show that for a superpolynomial sized set of k points in {0, 1}n, and for any partition of those points into High and Low, we can construct a matroid where the points labeled High have rank rhigh and the points labeled Low have rank rlow, and that rhigh/rlow = \u03a9\u0303(n 1/3).", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "The proof of Theorem 4 makes use of a special family of XOS functions presented in [2] and follows the same logic as in the proof of Theorem 3.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "Let f be a monotone submodular function with range in [0, 1] and fix D to be the uniform distribution over the n-dimensional boolean cube.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "In particular, we use the fact that any monotone submodular function f with range in [0, 1] is \u03b3-close to", "startOffset": 85, "endOffset": 91}, {"referenceID": 10, "context": "\u221a E[(f(x)\u2212 p(x))2] < \u03b3 [11].", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Let F be the class of monotone submodular functions with range in [0, 1].", "startOffset": 66, "endOffset": 72}, {"referenceID": 0, "context": "Moreover, even in the model where the learning algorithm has access to realvalued function labels, for a monotone submodular function f with range in [0, 1], the best known results for learning a function h such that ||f \u2212 h||2 \u2264 require running time 2\u00d5(1/ 4/5) \u00b7 n2 and 2\u00d5(1/ 4/5) log n random examples [11].", "startOffset": 150, "endOffset": 156}, {"referenceID": 10, "context": "Moreover, even in the model where the learning algorithm has access to realvalued function labels, for a monotone submodular function f with range in [0, 1], the best known results for learning a function h such that ||f \u2212 h||2 \u2264 require running time 2\u00d5(1/ 4/5) \u00b7 n2 and 2\u00d5(1/ 4/5) log n random examples [11].", "startOffset": 304, "endOffset": 308}, {"referenceID": 10, "context": "For ease of notation, we set k = 25 \u03b34/5 log 3 2 \u03b3 for the remainder of this section, where the constants come from the analysis in [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "Let f be a monotone submodular function with range in [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "Let f : 2[n] \u2192 [0, 1] be a monotone submodular function, D be the uniform distribution over 2[n], and \u03b3 \u2208 (0, 1).", "startOffset": 15, "endOffset": 21}, {"referenceID": 10, "context": "[11] proved that for \u03b3 \u2208 (0, 1), if we set \u03ba = 1 \u03b34/5 log 1 \u03b3 , then there exists L \u2286 [n], |L| \u2264 24 \u03b34/5 log 3 2 \u03b3 , such that if p(T ) = \u2211 S:|S\\L|\u2264\u03ba f\u0302(S)\u03c7S(T ), then ||f \u2212 p||2 = \u221a ES\u223cD[(f(S)\u2212 p(S))2] < \u03b3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Let F be the class of XOS functions with range in [0, 1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "This follows from the same line of reason as in the proof of Theorem 6 and the fact that for any XOS function f : 2[n] \u2192 [0, 1], there is a polynomial p of degree O(1/\u03b3) such that ||f \u2212p||2 \u2264 \u03b3 [11].", "startOffset": 121, "endOffset": 127}, {"referenceID": 10, "context": "This follows from the same line of reason as in the proof of Theorem 6 and the fact that for any XOS function f : 2[n] \u2192 [0, 1], there is a polynomial p of degree O(1/\u03b3) such that ||f \u2212p||2 \u2264 \u03b3 [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 22, "context": "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].", "startOffset": 215, "endOffset": 219}, {"referenceID": 0, "context": "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].", "startOffset": 244, "endOffset": 250}, {"referenceID": 8, "context": "We can extend Algorithm 1 to learn over many other classes of combinatorial functions up to pairwise comparisons, including valuation functions with limited nonlinear interactions [24], Fourier sparse set functions [23], and coverage functions [1, 9].", "startOffset": 244, "endOffset": 250}, {"referenceID": 22, "context": "Stobbe and Krause show that the Fourier support of f is contained in P = {S | |S| = 2} \u222a \u2205 [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "We combine structural results specific to coverage functions from [1] and [9] to prove that coverage functions are comparator-learnable with multiplicative separation (1 + ), using \u00d5(n3/ 5) queries, given access to an ERM oracle for learning linear separators.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "We combine structural results specific to coverage functions from [1] and [9] to prove that coverage functions are comparator-learnable with multiplicative separation (1 + ), using \u00d5(n3/ 5) queries, given access to an ERM oracle for learning linear separators.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "In this case, we require that the underlying distribution be uniform and that the underlying function be XOS or monotone submodular with range in [0, 1].", "startOffset": 146, "endOffset": 152}], "year": 2016, "abstractText": "A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support.", "creator": "LaTeX with hyperref package"}}}