{"id": "1605.09293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Internal Guidance for Satallax", "abstract": "We propose a new internal guidance method for automated theorem provers based on the given-clause algorithm. Our method influences the choice of unprocessed clauses using positive and negative examples from previous proofs. To this end, we present an efficient scheme for Naive Bayesian classification by generalising label occurrences to types with monoid structure. This makes it possible to extend existing fast classifiers, which consider only positive examples, with negative ones. We implement the method in the higher-order logic prover Satallax, where we modify the delay with which propositions are processed. We evaluated our method on a simply-typed higher-order logic version of the Flyspeck project, where it solves 26% more problems than Satallax without internal guidance.", "histories": [["v1", "Mon, 30 May 2016 16:01:51 GMT  (193kb,D)", "http://arxiv.org/abs/1605.09293v1", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["michael f\\\"arber", "chad brown"], "accepted": false, "id": "1605.09293"}, "pdf": {"name": "1605.09293.pdf", "metadata": {"source": "META", "title": "Internal Guidance for Satallax", "authors": ["Michael F\u00e4rber", "Chad Brown"], "emails": ["michael.faerber@uibk.ac.at"], "sections": [{"heading": "1 Introduction", "text": "It is the result of experiments that show a method to either fail or succeed in a particular situation. Mathematicians solve problems through experience. When solving a problem, mathematicians gain experience that will help them solve harder problems that they could not have solved in order to find more evidence in less time."}, {"heading": "2 Naive Bayesian classifier with monoids", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Motivation", "text": "Many automated theory testers have a state of evidence in which they make decisions by ranking available decisions (e.g., which proposal they want to process) and selecting the best one. This is related to the classification problem in machine learning, which takes data on previous decisions, i.e., which situation has led to which choice, and then arranges decisions according to usefulness for the current situation. For example, let us assume that the state of the theory tester is modelled by the set of constants that occur in the previously processed propositions or in the conjecture. Let our conjecture be x + y = y + x and let our assumptions include Equation 1. [P (0) = \u21d2 (x.P (x) = \u21d2 P (s (x))) = \u21d2 x.P (x))] = \u21d2 x.P (x)], (1) x + 0 = x. (2) If we first process Equation 1, the state of proof is characterized by F = {+, s, 0} if we then work out the prediction 2 and it turns out to be similar."}, {"heading": "2.2 Classifiers with positive examples", "text": "A classifier takes pairs (F, l) that correlate a set of features F with a label l, and produces a function that predicts a label given a set of features. Classifiers can be characterized by a function r (l, F) that represents the relevance of a label with a set of features. For internal orientation, we use r to estimate the relevance of a clause l for the process in the current test state F. A Bavarian classifier estimates the relevance of a label by its probability to occur with a set of features, i.e. P (l | F). Using the naive Bavarian assumption that features are conditionally independent, the conditional probability is: P (l | F) = P (F | l) P (F) = P (F) = P (l) = P (l) (l))."}, {"heading": "2.3 Generalised classifiers", "text": "In our experiments, we found negative educational examples to be crucial for internal leadership. Therefore, we generalized the classifier to represent the nature of the processes as commutative monoids. 3 We omitted several constant factors. In addition, FEMaLeCoP also considers characteristics of educational examples that are not part of characteristics F, even if this is another derivative of the theoretical model. Definition 1: A pair (M, +) is a monoid if there is a neutral element 0, so for all x, y, z, z and z = x + (y + z) and x + x = x. If more x + y + x, then the monoid is mutative.The generalized classifier is instantiated with a commutative monoid (M, +) and reads the triple (F, l, o) which is added to characteristics and labeled."}, {"heading": "3 Learning scenarios", "text": "In this section, we still consider ATPs as black boxes by entering a problem and classification data for internal orientation and returning them as output training data (blank if the ATP has not found evidence).We propose two different scenarios to generate training data and use it for subsequent correction searches, see Figure 1: - Online Learning: We run ATP with classification data for every problem solved by ATP. For each problem solved by ATP, we update the classifier with the training data from the ATP evidence. - Offline Learning: We first run ATP without classification data for all problems and store training data for each problem solved. We then generate classification data from the training data and re-run ATP with the classifier for all problems.While the second scenario can be paralleled and thus requires less conversion time, in the worst case (namely if each problem fails), each problem must be dealt with twice, thus doubling the CPU time of the first scenario."}, {"heading": "4 Internal guidance for given-clause provers", "text": "The variants of the clause algorithm are usually used in refutation-based ATPs, such as Vampire [KV13] or E [Sch13]. 4 We run a simple version of the 4 Technically, our reference auditor Satallax does not implement a clause algorithm, since Satallax treats terms instead of clauses and associates the choice of unedited terms with other commands. However, for the sake of internal guidance, we can consider implementing a version of the clause algorithm. We describe the differences in more detail in Section 6.Algorithm: In view of an initial set of clauses to be refuted, the set of unedited clauses is initialized with the initial set of clauses, and the set of edited clauses is the empty sentence. Each iteration of the algorithm selects a given clause from the unedited clauses and transfers it to the edited clause."}, {"heading": "4.1 Recording training data", "text": "The recording of training data can be done in several ways: - In situ: information on the use of clauses is recorded every time an unedited clause is processed; - Post mortem: information on the use of clauses is reconstructed only when evidence is found; - Since this method does not involve any effort in searching for evidence, we resort to post mortem records that are still sufficiently meaningful for our purposes; - For each piece of evidence, we store: guesses (if one is given), axioms A (premises given in the problem), edited clauses C and clauses C + used in the final proof (C + C)."}, {"heading": "4.2 Postprocessing training data", "text": "In our experiments, we often come across the same clauses, which differ only in that they contain different Skolem constants. To this end, we process the training data before generating classification data from it. We have tried various techniques to treat Skolem constants and other post-processing methods: - Skolem filtering: We discard clauses that contain any Skolem constants. - Consistent scolemisation: We normalise Skolem constants within clauses, similar to [UV\u016011]. That is, a sentence P (x, y, x) where x and y are Skolem constants becomes P (c1, c2, c1, c1). - Consistent normalization: Similar to consistent scolemisation, we normalise all symbols of a set. That is, P (x, y, x) as above becomes such constants P (c2, c2)."}, {"heading": "4.3 Transforming training data to classification data", "text": "For a given training date with the processed clauses C and the proof clauses C +, we define the corresponding classification data: {(F (c), c, (1, 0) | c (F (c), c (0, 1)) | c (C\\ C +), where F (c) refers to the characteristics of a clause. We use the monoid introduced in section 2 (N \u00b7 N, + 2, (0, 0))), which stores positive and negative examples. The classification data of the entire training data are then the (multiset) union of the classification data of the individual training data."}, {"heading": "4.4 Clause ranking", "text": "This section describes how our internal guidance method influences the choice of unprocessed clauses using a previously constructed classifier. At the beginning of the search for evidence, the ATP loads the classifier. Some learning ATPs, such as E / TSM [Sch00], select and process knowledge relevant to the current problem. However, since we store classification data in a hash table, filtering irrelevant knowledge to the problem at hand would require a relatively slow traversal of the entire table, while looking up the knowledge is quick even with a large number of irrelevant facts.For this reason, we do not filter the classification data per problem. Then, at each selection point, i.e. each time it selects a sentence from the unprocessed clauses C, the ATP selects a sentence c \u00b2 C that maximizes clauses rank R (c, F), with R (c, F) = rATP (N (c), F) and: - rATP (c) (a function of relevance of the ATP (c), which is the (4) function of the traditional (4) of Alc, the function of the (4)."}, {"heading": "5 Tuning of guidance parameters", "text": "We used two different methods to automatically find good internal guidance parameters, such as c, cp and cn from Section 2."}, {"heading": "5.1 Off-line tuning", "text": "Offline Tuning analyzes existing training data and tries to find parameters that give a high ranking to verifiable clauses from the training data and a low ranking to verifiable clauses. For each training date, we evaluate the following formula, which adds for each verifiable clause the number of verifiable clauses that are rated higher: \u2211 c + \u0445 C + | {c | R (c +, F +) > R (c +, F +), c \u00b2 C\\ C +} |, where C and C + are from the training date (see subsection 4.1), F and F + are the characteristics of the evidence states when c or c + have been processed (we reconstruct them from the training date), and R is the ranking list from subsection 4.4. At the end, we sum up the results of the above formula for all training data and take the guideline parameters that minimize this sum."}, {"heading": "5.2 Particle Swarm Optimisation", "text": "Particle swarm optimization [KE95] (PSO) is a standardized optimization algorithm that can be applied to minimize the output of a function f (x), where x is a vector for continuous values. A particle is then computed by a location x (a candidate solution to the optimization problem) and a velocity v. First, p-particles are generated with random locations and velocities. At each iteration of the algorithm, a new velocity is then calculated for each particle and the particle is moved by that amount. The new velocity of a particle is: v (t + 1) = \u03c9 \u00b7 v (t) + inspp \u00b7 rp \u00b7 (bp (t) \u2212 x (t) \u00b7 rg \u00b7 (bg (t) \u2212 x (t)), where: - v (t) is the old velocity of the particle, - bp (t) is the old velocity of the particle, - p - (t) is the solution \u2212 (t) where \u2212 p (t) is the old velocity of the particle."}, {"heading": "6 Implementation", "text": "We implement our internal instructions in Satallax version 2.8. Satallax is an automatic procedure that often implements other terms such as word processing in the work environment and word processing in the work environment. It is about the way in which word processing in the work environment and the way in which word processing in the work environment and the way in which word processing in the work environment is carried out and the way in which word processing in the work environment is carried out. The priorities assigned to these commands are the settings that Satallax uses for evidence research are called. A set of flag settings is called mode (in other ATPs often called strategies) and can be chosen by users after starting Satallax."}, {"heading": "7 Evaluation", "text": "This year it is so far that it will only take a few days until it is so far again."}, {"heading": "8 Conclusion", "text": "We have demonstrated the usefulness of this method experimentally and demonstrated that we can solve up to 26% more problems with a given set of tests. Internal guided ATPs could be integrated into hammer systems such as Sledgehammer (which can already reconstruct satallax evidence [SBP13]) or HOL (y) Hammer [KU14] and continuously improve their success rate, with minimal overhead. It might also be interesting to learn internal guidance for ATPs from targets that the user has indicated in previous evidence. Currently, we only learn from problems for which we have found evidence, but in the future we could also benefit from taking into account evidence that does not provide evidence. Furthermore, it would be interesting to see the impact of negative examples on existing ATPs with internal guidance, such as FEMaLeCoP. We believe that identifying good features that characterise the state of evidence is important to further improve learning outcomes."}, {"heading": "Acknowledgements", "text": "We would like to thank Sebastian Joosten and Cezary Kaliszyk for reading the first drafts of the paper and in particular Josef Urban for the stimulating discussions and invitation of the authors to Prague. Furthermore, we would like to thank the anonymous IJCAR speakers for their valuable comments. This work was supported by the Austrian Science Fund (FWF) P26201 and the European Research Council (ERC) AI4REASON."}], "references": [{"title": "Satallax: An automatic higher-order prover", "author": ["Chad E. Brown"], "venue": "Automated Reasoning - 6th International Joint Conference,", "citeRegEx": "Brown.,? \\Q2012\\E", "shortCiteRegEx": "Brown.", "year": 2012}, {"title": "A formal proof of the Kepler conjecture", "author": ["Ky Khac Vu", "Roland Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "Vu and Zumkeller.,? \\Q2015\\E", "shortCiteRegEx": "Vu and Zumkeller.", "year": 2015}, {"title": "Sine qua non for large theory reasoning", "author": ["Krystof Hoder", "Andrei Voronkov"], "venue": "International Conference on Automated Deduction,", "citeRegEx": "Hoder and Voronkov.,? \\Q2011\\E", "shortCiteRegEx": "Hoder and Voronkov.", "year": 2011}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "Kennedy and Eberhart.,? \\Q1995\\E", "shortCiteRegEx": "Kennedy and Eberhart.", "year": 1995}, {"title": "System description: E.T", "author": ["Cezary Kaliszyk", "Stephan Schulz", "Josef Urban", "Jir\u00ed Vyskocil"], "venue": "25th International Conference on Automated Deduction, Berlin,", "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2014\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2014}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "Logic for Programming, Artificial Intelligence, and Reasoning - 20th International Conference,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "First-order theorem proving and Vampire", "author": ["Laura Kov\u00e1cs", "Andrei Voronkov"], "venue": "Computer Aided Verification - 25th International Conference,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "Machine Learning for Automated Reasoning", "author": ["Daniel A. K\u00fchlwein"], "venue": "PhD thesis, Radboud Universiteit Nijmegen,", "citeRegEx": "K\u00fchlwein.,? \\Q2014\\E", "shortCiteRegEx": "K\u00fchlwein.", "year": 2014}, {"title": "leanCoP 2.0 and ileanCoP 1.2: High performance lean theorem proving in classical and intuitionistic logic (system descriptions)", "author": ["Jens Otten"], "venue": "Automated Reasoning, 4th International Joint Conference,", "citeRegEx": "Otten.,? \\Q2008\\E", "shortCiteRegEx": "Otten.", "year": 2008}, {"title": "Automated reasoning in higher-order logic using the TPTP", "author": ["Geoff Sutcliffe", "Christoph Benzm\u00fcller"], "venue": "THF infrastructure. J. Formalized Reasoning,", "citeRegEx": "Sutcliffe and Benzm\u00fcller.,? \\Q2010\\E", "shortCiteRegEx": "Sutcliffe and Benzm\u00fcller.", "year": 2010}, {"title": "LEO-II and Satallax on the Sledgehammer test bench", "author": ["Nik Sultana", "Jasmin Christian Blanchette", "Lawrence C. Paulson"], "venue": "J. Applied Logic,", "citeRegEx": "Sultana et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sultana et al\\.", "year": 2013}, {"title": "Learning Search Control Knowledge for Equational Deduction", "author": ["S. Schulz"], "venue": "Number 230 in DISKI. Akademische Verlagsgesellschaft Aka GmbH Berlin,", "citeRegEx": "Schulz.,? \\Q2000\\E", "shortCiteRegEx": "Schulz.", "year": 2000}, {"title": "Logic for Programming, Artificial Intelligence, and Reasoning - 19th International Conference, LPAR-19, Stellenbosch", "author": ["Stephan Schulz"], "venue": "South Africa, December 14-19,", "citeRegEx": "Schulz.,? \\Q2013\\E", "shortCiteRegEx": "Schulz.", "year": 2013}, {"title": "BliStr: The Blind Strategymaker", "author": ["Josef Urban"], "venue": "GCAI", "citeRegEx": "Urban.,? \\Q2015\\E", "shortCiteRegEx": "Urban.", "year": 2015}, {"title": "MaLeCoP machine learning connection prover", "author": ["Josef Urban", "Ji\u0159\u00ed Vysko\u010dil", "Petr \u0160t\u011bp\u00e1nek"], "venue": "Automated Reasoning with Analytic Tableaux and Related Methods - 20th International Conference,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "Using hints to increase the effectiveness of an automated reasoning program: Case studies", "author": ["Robert Veroff"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Veroff.,? \\Q1996\\E", "shortCiteRegEx": "Veroff.", "year": 1996}], "referenceMentions": [], "year": 2016, "abstractText": "We propose a new internal guidance method for automated theorem provers based on the given-clause algorithm. Our method influences the choice of unprocessed clauses using positive and negative examples from previous proofs. To this end, we present an efficient scheme for Naive Bayesian classification by generalising label occurrences to types with monoid structure. This makes it possible to extend existing fast classifiers, which consider only positive examples, with negative ones. We implement the method in the higher-order logic prover Satallax, where we modify the delay with which propositions are processed. We evaluated our method on a simply-typed higher-order logic version of the Flyspeck project, where it solves 26% more problems than Satallax without internal", "creator": "LaTeX with hyperref package"}}}