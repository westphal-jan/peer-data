{"id": "1605.09332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning.", "histories": [["v1", "Mon, 30 May 2016 17:16:40 GMT  (78kb)", "http://arxiv.org/abs/1605.09332v1", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v2", "Tue, 31 May 2016 19:24:04 GMT  (75kb)", "http://arxiv.org/abs/1605.09332v2", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v3", "Fri, 18 Nov 2016 20:26:25 GMT  (208kb,D)", "http://arxiv.org/abs/1605.09332v3", "Submitted to International Conference on Learning Representations (ICLR) 2017"]], "COMMENTS": "Submitted to Neural Information Processing Systems 2016 (NIPS)", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["ludovic trottier", "philippe gigu\\`ere", "brahim chaib-draa"], "accepted": false, "id": "1605.09332"}, "pdf": {"name": "1605.09332.pdf", "metadata": {"source": "CRF", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "emails": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.09 332v 1 [cs.L G] 30 May 201 6"}, {"heading": "Parametric Exponential Linear Unit for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Deep Convolutional Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Ludovic Trottier Philippe Gigu\u00e8re Brahim Chaib-draa", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Department of Computer Science and Software Engineering", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Laval University, Qu\u00e9bec, Canada", "text": "ludovic.trottier.1 @ ulaval.caphilippe.giguere @ ift.ulaval.cabrahim.chaib-draa @ ift.ulaval.caMay 31, 2016The activation function of Deep Neural Networks (DNNs) has undergone many changes in recent decades. Since the advent of the known unsaturated linear unit (ReLU), many have attempted to further improve the performance of networks with more sophisticated functions. Examples are the leaky ReLU (LReLU) to remove gradients, and the exponential linear unit (ELU) to reduce bias shifts. In this paper, we introduce the parametric ELU (PELU), an adaptive activation function that allows DNNs to adopt different nonlinear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the flexibility of the network, increasing the NET by increasing several levels (T)."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own, without having to take responsibility for themselves."}, {"heading": "2. Parametric Exponential Linear Unit (PELU)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Definition", "text": "The Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) is still negative for all pre-activations h, meaning a unit greater than zero. A recent analysis by Clevert et al. (2015) has shown that units with non-zero mean activation act as a bias for the next layer (known as the bias problem), causing oscillations and impeding learning. Reducing bias corrects the gradient toward unity, accelerating learning. To this effect, Clevert et al. (2015) proposes the exponential linear unit (ELU), which is defined as the identity for positive arguments and expeditions (h) \u2212 1. Since ELU outputs negative values for negative arguments, the network can shift the mean activation toward zero, which reduces bias."}, {"heading": "2.2. Analysis", "text": "To understand the effects of the proposed parameterization, we now examine the vanishing gradient for the following simple network containing a neuron in each of its L layers: x = h0, hl = wlhl \u2212 1, zl = f (hl), E = (zL, y) (1 \u2264 l \u2264 L) (4), where we have omitted the tendencies for simplicity without loss of generality. In (4), the loss function between the network forecast zL and the label y is the value E assuming x. In this case, it can be shown by the chain rule of the derivative that the derivative of E is k in relation to any possible weight type k: x = hk \u2212 1f. \"L = k\" (hj) wj \"(hj) wj\" zL where f. \""}, {"heading": "2.3. Optimization", "text": "PELU is trained simultaneously with all network parameters using the standard back propagation algorithm and the updates for a and b can be derived by the chain rule of derivation to target E. The gradient of a and b for one layer is: \u2202 E \u2202 a = \u2211 i \u2202 E \u2202 f (xi) \u2202 f (xi) \u2202 a, \u2202 E \u2202 b = \u2211 i \u2202 f (xi) \u2202 f (xi) \u2202 b, (6) where i summarizes all elements of the tensor to which f is applied. The terms E \u2202 f (xi) are the gradients propagated from the above layers, whereas f (x) 8,000 f (x) \u2202 f (x) \u2202 f (x) 20s are the gradients of f in relation to a, b: \u2202 f (x) - a case to which f is applied. Terms E (x x / xi) are the gradients propagated from the above layers."}, {"heading": "3. Experimentations", "text": "In this section we present our experiments in both unsupervised and supervised learning with tasks MNIST, CIFAR-10 / 100 and ImageNet. Our goal is to compare the performance of different CNN architectures using ReLU, ELU or PELU. Since Clevert et al. (2015) achieved higher performance improvements with ELU alone than with a combination of BN followed by ReLU, we also removed the batch normalization before PELU."}, {"heading": "3.1. MNIST", "text": "Unsupervised learning is the task of learning a model to extract a feature representation from unsupervised undescribed observations, which is generally useful for problems such as deep learning data fusion (Srivastava and Salakhutdinov, 2012). To this end, we first followed Desjardins et al. (2015) and defined an encoder consisting of four fully connected layers of sizes 1000, 500, 250, 30, with a symmetrical decoder (weights are not bound). To this end, we also added BN (for ReLU) and Dropout (Srivastava et al., 2014) with a probability of 0.2 before and after activation, with the exception of the last layer we held linear."}, {"heading": "3.2. CIFAR-10/100", "text": "This year, it has come to the point where you feel you can go to the top without being able to go to the top."}, {"heading": "3.3. ImageNet", "text": "Finally, we tested the proposed PELU on ImageNet 2012 task (ILSVRC2012) with four different network architectures: ResNet18 (Shah et al., 2016; He et al., 2015a), Network in Network (NiN et al., 2013), All-CNN (Springenberg et al., 2014) and Overfeat (Sermanet et et al., 2013). As with the other experiments, we removed BN before ELU and PELU activations. However, due to the relatively complex architecture of NiN, we used BN after each maximum pool layer (all three layers) to control the disappearance of gradients. Each network was represented with a dynamics-based gradient descent (0.9) below the standard training programs. Figure 2 presents the error rates of all four networks on ImageNet 2012 validation."}, {"heading": "4. PELU Training Progression", "text": "In this section, we add a visual evaluation of the non-linear behaviors adopted during the training to the experimental results of Section 3. To this end, we trained a Vgg network (Simonyan and Zisserman, 2014), following Zagoruyko, replacing the ReLU activations with the proposed PELU."}, {"heading": "5. Conclusion", "text": "In this thesis, we proposed the Parametric Exponential Linear Unit (PELU), an adaptive activation function that allows networks to adopt different nonlinear behaviors during the training phase. Based on a parameterization of the previously proposed ELU activation, PELU allows networks to change the saturation point for negative arguments and the peaks of the linear part for positive ones. In this way, the networks can have more control over bias shift and vanishing gradients. We conducted several experiments in unattended learning on the MNIST dataset and monitored learning on CIFAR10 / 100 and ImageNet datasets. Results showed that our proposed activation either improves the convergence rate or the error rate for multiple network architectures, such as All-CNN, Vgg, ResNet, NiN and relative overfeat. We observed an improvement of up to 7.28%, without a hyline-parameter adjusting the network architecture to the other ELL tuning behavior of the network architecture."}, {"heading": "A. Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1. Figures for Proof", "text": "In this section, we present Figure 3, which shows the graph of the functions used in the analysis of the vanishing gradient in Section 2.2 of the paper. In the upper part of Figure 3, the graph of f \u2032 (wh) w is shown, while the lower part shows the graph of l (w). Both graphs assume that w \u2265 b / a is as specified by the theorem. As seen in the uppermost graph of Figure 3, the length of the values h < 0, for which f \u2032 (wh) w \u2265 1 is represented as a red segment l (w) whose value is represented by the absolute value of zero f \u2032 (wh) w \u2212 1. From the lower part of Figure 3, we can see that l (w) is pseudo-concave for w \u2265 b / a and therefore has a unique optimum."}, {"heading": "A.2. PELU Training Progression", "text": "In this section we present the number of activations of Vgg PELU during the entire training phase on CIFAR-10 as described in Section 4 of the paper. Figure 4 shows the learned activations after training, while Figure 5 shows the progression of the PELU parameters for each shift. Learned activations for Vgg on CIFAR \u2212 10 Task Activation parameter progression for Vgg on CIFAR \u2212 10 task"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning.", "creator": "LaTeX with hyperref package"}}}