{"id": "1605.09410", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "End-to-End Instance Segmentation with Recurrent Attention", "abstract": "While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset and KITTI vehicle segmentation dataset.", "histories": [["v1", "Mon, 30 May 2016 20:40:20 GMT  (8852kb,D)", "http://arxiv.org/abs/1605.09410v1", null], ["v2", "Tue, 6 Sep 2016 15:09:06 GMT  (8891kb,D)", "http://arxiv.org/abs/1605.09410v2", null], ["v3", "Sun, 27 Nov 2016 17:41:57 GMT  (8221kb,D)", "http://arxiv.org/abs/1605.09410v3", null], ["v4", "Mon, 16 Jan 2017 23:08:35 GMT  (8221kb,D)", "http://arxiv.org/abs/1605.09410v4", null], ["v5", "Thu, 13 Jul 2017 00:53:33 GMT  (5962kb,D)", "http://arxiv.org/abs/1605.09410v5", "CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mengye ren", "richard s zemel"], "accepted": false, "id": "1605.09410"}, "pdf": {"name": "1605.09410.pdf", "metadata": {"source": "CRF", "title": "End-to-End Instance Segmentation and Counting with Recurrent Attention", "authors": ["Mengye Ren", "Richard S. Zemel"], "emails": ["mren@cs.toronto.edu", "zemel@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "2 Recurrent attention model", "text": "Our proposed model consists of four main components: A) an external 2D memory that tracks the state of the segmented objects; B) a box proposal network that is responsible for locating objects of interest; C) a segmentation network for segmenting image pixels within the frame; and D) a scoring network that determines whether an object instance has been found and also determines when it should be stopped. See Figure 2 to illustrate these components. Note: We use the following notation to describe the model architecture: x-x-RH-W is the input image; t indexes the iterations of the model and \u03c4 indexes the views of the inner RNN; yt, y-t-t-t-t-t (0, 1) H-W is the segmentation output / Ground-Truth sequence; st, s-t-t-( 0, affdence 1) is the Confidence-T sequence."}, {"heading": "2.1 Part A: Model input and external 2D memory", "text": "We are exploring three variants of our model, which are located in the first component."}, {"heading": "2.2 Part B: Box network", "text": "The box network locates objects of interest. The CNN network in the box network gives an H '\u00b7 W' \u00b7 L feature map ubox, t. Since a single glance cannot provide enough information to the upper network to decide where exactly the field should be drawn, we allow the volatile LSTM to look at different places. It is initialized to be uniform across all places, and it indexes the fleeting views. ubox, t = CNN (dt), zt, \u03c4 = LSTM (\u2211 h, w \u03b1h, wt \u2212 n, p, w, l box, t, zt, \u03c4 \u2212 1), \u03b1t, \u03a3 + 1 = MLP (zt, T, Y) (3) We run the hidden state of the LSTM through a linear layer to get predicted box coordinates. We parameterise the field according to its normalized center gX, Y and size Y (zt, T, Y). (Z, T) We seduce the hidden state of the box to get a linear coordinate LM by predicting the box state."}, {"heading": "2.3 Part C: Segmentation network", "text": "The remaining task is to segment the pixels belonging to the dominant object within the window. In the segmentation network, we adopt a variant of DeconvNet [13] with skip connections that adds deconvolution (or folding transposition) layers by folding plane to elevate the low-resolution feature map to complete segmentation. After the fully twisted layers, we obtain a patch-level segmentation prediction for the heat map y-t. We then project this patch prediction onto the original image using the transposition of the previously calculated Gauss filters. The learned method for increasing the signal inside the delimitation box and a constant \u03b2 to suppress the pixels outside the box. Finally, the sigmoid function generates final segmentation values between 0 and 1, yt = sigmoid (\u03b3 \u00b7 Extract (y, FTY, FTX) \u2212 \u03b2 (9)."}, {"heading": "2.4 Part D: Scoring network", "text": "To estimate the number of objects in the image and finish our sequential process, we insert a scoring network similar to that shown in [18]. Our scoring network takes information from the field and segmentation network to generate a score between 0 and 1.st = sigmoid (W (zt, end) + W (usegm)) (10). We train the entire model with a sequence length determined by the maximum number of objects plus one. During the inference, we interrupt iterations as soon as the output score falls below 0.5. The loss function (described below) encourages the scores to decrease monotonously."}, {"heading": "2.5 Loss functions", "text": "The total loss function is a sum of three losses: the segmentation corresponding to IoU loss Ly, the box IoU loss Lb, and the score cross-sectional task, the IoU loss Ls: L (y, b, s) = Ly (y, y) + Lb (b, b) + Ls (s, s) (11) (a) Matching IoU loss (mIOU). A primary challenge of instance segmentation involves matching model and soil-truth instants. we calculate a maximum weighted two-part graph that matches between the output instances and the soil-truth instances (22) and [18]. By matching, the loss is insensitive to the order of the soil-truth instances. In contrast to the coverage scores proposed in [21], both false positive and false negative segmentation are directly punished."}, {"heading": "2.6 Training procedure and post-processing", "text": "Bootstrap Training. The box and segmentation networks rely on each other's output to make decisions for the next time step. Due to the coupled nature of the two networks, we propose a bootstrap training: these networks are pre-trained with ground-truth segmentation or boxes, and in later stages we replace the ground truth with the predicted model values. To smooth the transition between phases, we examine the idea of \"planned sampling\" [2], where we gradually eliminate the dependence on ground truth segmentation at the network's input. As shown in Figure 2, there is a dynamic switch in the input of the external memory during the training to use either the maximum overlapping ground truth instance segmentation or the output of the network from the previous time step."}, {"heading": "3 Related Work", "text": "In fact, it is not that we will be able to find a solution that will solve people's problems, \"he said.\" But it is not that we will be able to solve them. \"He added,\" It is not that we will have to solve them. \"He added,\" But it is not that we will get a grip on them. \"He added,\" But it is not that we will have to rein them in, that they will have to rein themselves in. \"He added,\" But it is not that we will get a grip on them. \"He added,\" It is not that we will have to rein them in, that they will have to rein themselves in. \""}, {"heading": "4 Experiments", "text": "An example of the segmentation of model components is the CVPPP segmentation of plant leaves, which was developed based on the importance of plant leaf segmentation. We performed the A1 subset of CVPPP segmentation of plant leaves. We trained our model on 128 marked images and reported on the results of the 33 images tested. We compare our performance to [18] and other top approaches published at the CVPPP conference; see the Collation Study [19] for details of these other approaches. KITTI segmentations of motor vehicles, which also provide rich information in the context of autonomous driving. Following [27, 26, 23] we also evaluated model performance on KITTI car segmentation data. We trained the model with 3712 training images and reported on the performance of 144 test images. We also examine the relative importance of model components via degradation studies."}, {"heading": "4.1 Results & discussion", "text": "In the task of leaf segmentation, our best model surpasses the previous state of the art, but by a wide margin both in segmentation and counting. Table 1 shows that the models match FCN and have lower values than the simpler version. This is useful because the size of the dataset is small and the FCN significantly increases the input dimension and number of parameters. [23] One possible explanation is the inclusion of depth information during training, which can help the model detect distant object boundaries. In addition, their method of bottom-up instance fusion plays a crucial role (without this leading to a steep drop in performance). One possible explanation is the inclusion of depth information during training."}, {"heading": "5 Conclusion", "text": "In this work, we borrow the intuition of human counting and formulate instance segmentation and counting as a recurring, attentive process. Our end-to-end architecture shows significant improvements over previous studies on the use of RNN for the same tasks, and shows current results on demanding leaf and auto-segmentation data sets. We address the classic problem of object occlusion with a recurring external memory, and the attention structure brings segmentation with fine resolution. Our model also shows promising counting performance on a portion of the MS-COCO data set."}, {"heading": "A Training procedure specification", "text": "We used the Adam Optimizer with a learning rate of 0.001 and a lot size of 8. The learning rate is multiplied by 0.85 for all 5000 training steps.A.1 Planned samplesWe call \u03b8t the probability of having the greatest overlap with the previous prediction, as opposed to the model output. \u03b8t follows an exponential decay during the workout, and for larger t the decay occurs later: \u03b8t = min (\u0442t exp (\u2212 epoch \u2212 SS2), 1) (22) \u0442t = 1 + log (1 + Kt) (23), where the epoch is the training index, S, S2 and K are constants. In the experiments reported here, their values are 10000, 2885 and 3."}, {"heading": "B Model architecture", "text": "B.1 Foreground + Orientation FCNWe resize the image to a uniform size. For CVPPP and MS-COCO datasets, we take a uniform size of 224 x 224, and for KITTI, we take 128 x 448. Table 4 lists the specification of all layers. Table 5: External Storage Specification Name Filterspec Size CVPPP / MS-COCO Size KITTIConvLSTM 3 x 3 224 x 224 x 9 128 x 448 x 9B.2 External Storage NetworkThe box network records 9 channels. Either directly from the output of the FCN or from the hidden state of the ConvLSTM. It goes through a CNN structure again and uses the attention vector predicted by the LSTM to perform dynamic pooling in the last layer. CNN hyperparameters are listed in Table 6 and the MLFCM hyperparameters are listed in Table 7."}], "references": [{"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Counting everyday objects in everyday scenes", "author": ["P. Chattopadhyay", "R. Vedantam", "R.S. Ramprasaath", "D. Batra", "D. Parikh"], "venue": "CoRR, abs/1604.03505,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CoRR, abs/1512.04412,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to count leaves in rosette plants", "author": ["M.V. Giuffrida", "M. Minervini", "S. Tsaftaris"], "venue": "Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "ICML,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to count objects in images", "author": ["V.S. Lempitsky", "A. Zisserman"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Proposal-free network for instance-level object segmentation", "author": ["X. Liang", "Y. Wei", "X. Shen", "J. Yang", "L. Lin", "S. Yan"], "venue": "CoRR, abs/1509.02636,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Finely-grained annotated datasets for imagebased plant phenotyping", "author": ["M. Minervini", "A. Fischbach", "H. Scharr", "S.A. Tsaftaris"], "venue": "Pattern Recognition Letters,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "3-d histogram-based segmentation and leaf detection for rosette plants", "author": ["J. Pape", "C. Klukas"], "venue": "ECCV Workshops,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to decompose for object detection and instance segmentation", "author": ["E. Park", "A.C. Berg"], "venue": "CoRR, abs/1511.06449,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R.S. Zemel"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R.B. Girshick", "J. Sun"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent instance segmentation", "author": ["B. Romera-Paredes", "P.H.S. Torr"], "venue": "CoRR, abs/1511.08250,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Leaf segmentation in plant phenotyping: A collation study", "author": ["H. Scharr", "M. Minervini", "A.P. French", "C. Klukas", "D.M. Kramer", "X. Liu", "I. Luengo", "J. Pape", "G. Polder", "D. Vukadinovic", "X. Yin", "S.A. Tsaftaris"], "venue": "Mach. Vis. Appl., 27(4):585\u2013606,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "D. Yeung", "W. Wong", "W. Woo"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance segmentation of indoor scenes using a coverage loss", "author": ["N. Silberman", "D. Sontag", "R. Fergus"], "venue": "ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end people detection in crowded scenes", "author": ["R. Stewart", "M. Andriluka"], "venue": "CoRR, abs/1506.04878,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixel-level encoding and depth layering for instance-level semantic labeling", "author": ["J. Uhrig", "M. Cordts", "U. Franke", "T. Brox"], "venue": "CoRR, abs/1604.05096,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-leaf tracking from fluorescence plant videos", "author": ["X. Yin", "X. Liu", "J. Chen", "D.M. Kramer"], "venue": "ICIP,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Instance-level segmentation with deep densely connected MRFs", "author": ["Z. Zhang", "S. Fidler", "R. Urtasun"], "venue": "CoRR, abs/1512.06735,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Monocular object instance segmentation and depth ordering with CNNs", "author": ["Z. Zhang", "A.G. Schwing", "S. Fidler", "R. Urtasun"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset [12] and KITTI vehicle segmentation dataset [5].", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset [12] and KITTI vehicle segmentation dataset [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "Traditionally, counting is performed in a task-specific setting, either by detection followed by regression, or by learning discriminatively with a counting distance metric [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "Studies in applications such as image question answering [1, 16] also reveal that counting, especially on everyday objects, is a very challenging task on its own [3].", "startOffset": 57, "endOffset": 64}, {"referenceID": 15, "context": "Studies in applications such as image question answering [1, 16] also reveal that counting, especially on everyday objects, is a very challenging task on its own [3].", "startOffset": 57, "endOffset": 64}, {"referenceID": 2, "context": "Studies in applications such as image question answering [1, 16] also reveal that counting, especially on everyday objects, is a very challenging task on its own [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 16, "context": "Classical object detection pipelines [17] is composed of four stages: proposals, scoring, refinement, and non-maximal suppression (NMS).", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "convolutional networks (FCN) [11] will have trouble directly outputting all instance labels in a single shot.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "Recent work on instance segmentation [21, 27, 26] formulates complex graphical models, which results in a complex and time-consuming pipeline.", "startOffset": 37, "endOffset": 49}, {"referenceID": 26, "context": "Recent work on instance segmentation [21, 27, 26] formulates complex graphical models, which results in a complex and time-consuming pipeline.", "startOffset": 37, "endOffset": 49}, {"referenceID": 25, "context": "Recent work on instance segmentation [21, 27, 26] formulates complex graphical models, which results in a complex and time-consuming pipeline.", "startOffset": 37, "endOffset": 49}, {"referenceID": 11, "context": "We evaluate our model on a number of challenging datasets: 1) CVPPP leaf segmentation dataset [12]; 2) KITTI car segmentation dataset [5]; and 3) on MS-COCO [10] images, where we train two different models, for \u201cperson\u201d and \u201czebra\u201d categories, and test the model with images that contain at least one instance of the chosen category.", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "We evaluate our model on a number of challenging datasets: 1) CVPPP leaf segmentation dataset [12]; 2) KITTI car segmentation dataset [5]; and 3) on MS-COCO [10] images, where we train two different models, for \u201cperson\u201d and \u201czebra\u201d categories, and test the model with images that contain at least one instance of the chosen category.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "We evaluate our model on a number of challenging datasets: 1) CVPPP leaf segmentation dataset [12]; 2) KITTI car segmentation dataset [5]; and 3) on MS-COCO [10] images, where we train two different models, for \u201cperson\u201d and \u201czebra\u201d categories, and test the model with images that contain at least one instance of the chosen category.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "The first is a pixel-level foreground segmentation, produced by a variant of the DeconvNet [13] with skip connections.", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "[23] by producing a 2-d object angle map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Convolutional LSTM [20] is a form of RNN that uses convolution as its recurrent operator and thus is able to efficiently process a 2D image input and store a 2D hidden state.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "We follow DRAW [7] and use a Gaussian interpolation kernel to extract an N \u00d7 N patch from the x\u0303, a concatenation of the original image with dt.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "In the segmentation network, we adopt a variant of the DeconvNet [13] with skip connections, which appends deconvolution (or convolution transpose) layers after convolution layers to upsample the low-resolution feature map to a full-size segmentation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "To estimate the number of objects in the image, and to terminate our sequential process, we incorporate a scoring network, similar to the one presented in [18].", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "we compute a maximum-weighted bipartite graph matching between the output instances and ground-truth instances [22] and [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "we compute a maximum-weighted bipartite graph matching between the output instances and ground-truth instances [22] and [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "Unlike coverage scores proposed in [21] it directly penalizes both false positive and false negative segmentation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "To smooth out the transition between stages, we explore the idea of \u201cscheduled sampling\u201d [2] where we gradually remove the reliance on ground-truth segmentation at the input of the network.", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "into a segmentation tree [21].", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "[26] formulated a dense CRF for instance segmentation; .", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] used a CNN to generate pixel-level object size information, and used clustering as a post-processing step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] proposed a pipeline-based approach and won the MS-COCO instance segmentation challenge.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[23] presented another approach with FCN, and achieved very impressive results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22, 15, 18] employs endto-end recurrent neural networks (RNN) to perform object detection and segmentation.", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[22, 15, 18] employs endto-end recurrent neural networks (RNN) to perform object detection and segmentation.", "startOffset": 0, "endOffset": 12}, {"referenceID": 17, "context": "[22, 15, 18] employs endto-end recurrent neural networks (RNN) to perform object detection and segmentation.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "A permutation agnostic loss function based on maximum weighted bipartite matching was proposed by [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "Similarly, our box proposal network also uses an RNN to generate box proposals: instead of running the image 300 times through the RNN, we only run it once by using a soft attention mechanism [24].", "startOffset": 192, "endOffset": 196}, {"referenceID": 17, "context": "RomeraParedes and Torr [18] use convolutional LSTM (ConvLSTM) [20] to produce instance segmentation directly.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "RomeraParedes and Torr [18] use convolutional LSTM (ConvLSTM) [20] to produce instance segmentation directly.", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Previous work on object counting in images has mainly focused on crowds of pedestrians and biological cells [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "[3] focused on counting questions in VQA and proposed detector approaches as well as a regression based method (\u201cassociative subitizing\u201d) that works on a 3\u00d7 3 field of CNN features level.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "One instance segmentation benchmark is the CVPPP plant leaf dataset [12], which was developed due to the importance of instance segmentation in plant phenotyping.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "We compare our performance to [18], and other top approaches that were published with the CVPPP conference; see the collation study [19] for details of these other approaches.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "We compare our performance to [18], and other top approaches that were published with the CVPPP conference; see the collation study [19] for details of these other approaches.", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "Following [27, 26, 23], we also evaluated the model performance on KITTI car segmentation dataset.", "startOffset": 10, "endOffset": 22}, {"referenceID": 25, "context": "Following [27, 26, 23], we also evaluated the model performance on KITTI car segmentation dataset.", "startOffset": 10, "endOffset": 22}, {"referenceID": 22, "context": "Following [27, 26, 23], we also evaluated the model performance on KITTI car segmentation dataset.", "startOffset": 10, "endOffset": 22}, {"referenceID": 17, "context": "RIS+CRF [18] 66.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "9) MSU [19] 66.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "6) Nottingham [19] 68.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "0) Wageningen [25] 71.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "6) IPK [14] 74.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "8) PRIAn [6] 1.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "detect [3] Person 3.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "22 aso-sub [3] Person 1.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "detect [3] Zebra 2.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "56 aso-sub [3] Zebra 1.", "startOffset": 11, "endOffset": 14}, {"referenceID": 26, "context": "DenseCRFv1 [27] 77.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "736 DenseCRFv2 [26] 78.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "833 FCN+Depth [23] 84.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "In the zebra counting task, we found that our model outperforms the detector and NMS method, and associative-subitizing methods [3], but we are not doing as well in the person category.", "startOffset": 128, "endOffset": 131}], "year": 2016, "abstractText": "While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset [12] and KITTI vehicle segmentation dataset [5].", "creator": "LaTeX with hyperref package"}}}