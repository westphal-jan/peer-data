{"id": "1605.09432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "abstract": "Given a supervised/semi-supervised learning scenario where multiple annotators are available, we consider the problem of identification of adversarial or unreliable annotators.", "histories": [["v1", "Mon, 30 May 2016 22:05:36 GMT  (1126kb,D)", "http://arxiv.org/abs/1605.09432v1", "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012"]], "COMMENTS": "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["ramanathan subramanian", "romer rosales", "glenn fung", "jennifer dy"], "accepted": false, "id": "1605.09432"}, "pdf": {"name": "1605.09432.pdf", "metadata": {"source": "CRF", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "authors": ["Ramanathan Subramanian", "R\u00f3mer Rosales"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The distributed nature of this phenomenon has contributed to the development of many crowdsourcing projects, and this scenario is prevalent in most forms of expert / non-expert group opinions and assessment tasks (including many forms of Internet or online user behavior), where aggregation of observations and opinions from multiple sources is far from a solved.From the perspective of machine learning, we treat observations and opinions as data points or subscriber labels / comments. In this sense, multiple participants (anchors) can assign conflicting labels to the same data point and obtain knowledge from multiple distributed sources. From the perspective of machine learning, observations and opinions can be considered data points or subscriber labels / comments. In this sense, multiple participants (anchors) can assign contradictory labels to the same data point."}, {"heading": "2 Problem Setup", "text": "Let the available data gathered by multiple annotators be specified by a series of dots X = {x1,.., xN}, drawn independently of an input distribution. Some of these data points may have been labeled by one or more annotators. We designate y (t) i as the label for the i-th data point given by the annotator t, and allow Y = {y (t) i) to do so. We allow xi and y (t) i to be random variables in an input or label space, respectively, and introduce additional random variables Z = {z1,., zN} to represent the true but not observed label for the data points. Annotators can label certain data points more reliably than others due to the characteristics of the data point itself. For example, a loud data point may be more difficult to be labeled by all annotators, but a data pointer Xiv: 160 x 5.09 432v [s.C on May 30th] may also be a confidential annotator."}, {"heading": "3 Evaluating Annotators", "text": "Consider any single data point. If we knew the basic truth, we could easily evaluate the accuracy of the annotator. What if we do not have access to the basic truth (it does not exist or is expensive to obtain)? The following proposed distribution provides a way to evaluate an annotator without dependence on the basic truth. The basic idea is to evaluate a conditional distribution that measures how predictably an annotator label is bound to other annotators: p (y (k) | [y (t\\ k)}}, x) = p (z | x) p (y (t\\ k)} | x) = x (x) p (x) p ({y (t\\ k)} | z, x) = x) p (z (z (z\\ k)} (1) We find that if the basic truth (along with the input data) is given, the anotators are mutually independent and p (t\\ k) (otator) (z |), x (p) is expected."}, {"heading": "4 Experiments", "text": "In this year, it has come to the point where it will be able to take the lead, \"he said in an interview with\" Welt am Sonntag, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in which he took care of the\" world, \"in the\" world, \"in the\" world, \"in the\" world, \"in the\" world, \"in the\" world, \"in the\" world, \"in the\" in the \"world,\" in the \"in the\" in the \"world,\" in the \"in the\" world, \"in the\" in the \"world,\" in the \"in the\" world, \"in the\" in the \"in the\" world, \"in the\" in the \"in the\" world, \"in the\" in the \"in the\" in the \"world,\" in the \"in the\" in the \"in the\" in the \"world,\" in the \"in the\" in the \"in the\" world, \"in the\" in the \"in the\" in the \"in the\" world, \"in the\" in the \"in the\" in the \"in the\" world, \"in the\" in the \"in the\" in the \"in the\" in the \"in,\" in the \"in the\" in the \"in the\" in the \"in, in the\" in the \"in the\" in, in the \"in the\" in the \"in the\" in the \"in, in the\" in the \"in, in the\" in the \"in the\" in the \"in, in the\" in the \"in the\" in the \"in the\" in the \"in, in the\" in the \"in the\" in the \"in the\" in, in the \"in the\" in the \"in the\" in the \"in, in the\" in the \"in the\" in the \"in the\" in the \"in the\" in, in the \"in the\" in the \"in the\" in the \"in the\" in the \"in the\" in the \"in the\" in the"}], "references": [{"title": "Learning from crowds", "author": ["V.C. Raykar"], "venue": "J. Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Evaluating non-expert annotations for natural language tasks", "author": ["R. Snow"], "venue": "In Proc. EM-NLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan"], "venue": "In Proc. Int. Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "This is in contrast with the related work in [1,2] where annotators are assumed uniformly accurate/inaccurate irrespective of the data point being labeled.", "startOffset": 45, "endOffset": 50}, {"referenceID": 1, "context": "This is in contrast with the related work in [1,2] where annotators are assumed uniformly accurate/inaccurate irrespective of the data point being labeled.", "startOffset": 45, "endOffset": 50}, {"referenceID": 2, "context": "We instead follow the model introduced in [3] where the dependency x\u2192 y \u2190 z is explicit.", "startOffset": 42, "endOffset": 45}], "year": 2016, "abstractText": "Data can be acquired, shared, and processed by an increasingly larger number of entities, in particular people. The distributed nature of this phenomenon has contributed to the development of many crowdsourcing projects. This scenario is prevalent in most forms of expert/non-expert group opinion and rating tasks (including many forms of internet or on-line user behavior), where a key element is the aggregation of observations-opinions from multiple sources.", "creator": "LaTeX with hyperref package"}}}