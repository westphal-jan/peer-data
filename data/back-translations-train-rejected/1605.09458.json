{"id": "1605.09458", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Training Auto-encoders Effectively via Eliminating Task-irrelevant Input Variables", "abstract": "Auto-encoders are often used as building blocks of deep network classifier to learn feature extractors, but task-irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network. In this paper,via dropping the task-irrelevant input variables the performance of auto-encoders can be obviously improved .Specifically, an importance-based variable selection method is proposed to aim at finding the task-irrelevant input variables and dropping them.It firstly estimates importance of each variable,and then drops the variables with importance value lower than a threshold. In order to obtain better performance, the method can be employed for each layer of stacked auto-encoders. Experimental results show that when combined with our method the stacked denoising auto-encoders achieves significantly improved performance on three challenging datasets.", "histories": [["v1", "Tue, 31 May 2016 00:58:47 GMT  (324kb,D)", "http://arxiv.org/abs/1605.09458v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hui shen", "dehua li", "hong wu", "zhaoxiang zang"], "accepted": false, "id": "1605.09458"}, "pdf": {"name": "1605.09458.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hui Shen", "Dehua Li", "Hong Wu", "Zhaoxiang Zang"], "emails": ["shenhui@hust.edu.cn", "lidehua1946@sina.com;", "wu@hust.edu.cn", "zxzang@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Feature Learning; Deep Learning; Neural Network; Auto Encoder; Stacked Autoencoder; Variable Selection; Feature Selection; Unattended Training Biographical Notes: Hui Shen is currently a PhD student at the School of Automation, Huazhong University of Science and Technology, China. She received her bachelor's degree from Wuhan University of Science and Technology, China. Her main interests are Neural Network, Deep Learning and Machine Learning. DeHua Li is a professor at the School of Automation, Huazhong University of Science and Technology, China. He earned his bachelor's degree from Wuhan University in 1970. And he spent one year as a Senior Visiting Scholar at the AI Department of the University of Edinburgh, UK. His research interests include AI, Neotic Science and Machine Learning. Hong Wu earned his bachelor's degree in Computer Science and Technology at Wuhan University, Monazhering. His Master's degree in Huazang Intelligence at Huazang University of Technology and Huazang University of China are his main research interests at Huazong University Automatic Science and Technology, Huong University of China."}, {"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 Preliminaries", "text": "It is a neural network consisting of three levels, an input layer, a hidden layer and an output layer. It can also be considered a network consisting of an encoder and a decoder. Processing input data to hidden layers can be seen as an encoder, while from hidden layers an output layer to the decoder. First, during training, it can use the input data x for hidden representation h by the encoder: hq = fq x) = sf (wTq x + bq), where wTq is a weight vector, bq is the activation of the input data x for hidden representation h by the encoder."}, {"heading": "3 Importance-based Variable Selection", "text": "The hidden representation of an autocoder is a description of all the input data. AEs do not identify any useful or useless information for classification. Therefore, they try to collect all the information of the input data, not only task-relevant information, but also task-relevant information when there is no relevant information. To solve this problem, a meaning-based selection method is proposed to find the task-irrelevant variables and drop them. In short, the method is to evaluate the meaning of each variable for classification and to drop the variables with less significance than a threshold. We use the sensitivity of the discriminatory hyperlevel to a variable to evaluate the meaning of the variable. we argue that the variables with higher sensitivity are more important for classification."}, {"heading": "4 Experiments and Results", "text": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is referred to as DAEIVS, and compared performances obtained by stacked DAE-ITS (SDAE-ITS), stacked DAE (SDAE) Vincent et al. (2010), PGBM Sohn et al. (2013) and aNN Wang et al. (2014). The baseline was SDAE with standard training strategy, which was described in Vincent et al. (2010). We tested different depths of SDAE and SDAEIVS from 1 to 3 layers. Each auto-encoder had 1000 hidden units and used bound weights. Both encoders and decoders used sigmoid function and took cross-entropy errors as reconstruction errors. The results of feature extractors learned by one layer were used as upper layer input variables."}, {"heading": "4.1 Effect of importance-based variable selection", "text": "While applying algorithms, we will calculate the value of each variable on each dataset (Equation (2), (3) and (4). Examples and visualization of the meaning of variables for each dataset are shown in Figure 1. In the figure, the lighter and darker areas will correspond with higher importance. It can be seen that variables of high importance focus on the central area where the digits are mainly occupied."}, {"heading": "4.2 Classification performance comparison", "text": "In Table 1, we show the error rate in the test classification of SDAE and SDAE-ITS with varying depth. It is evident that the performance generated by SDAE-ITS significantly exceeds the performance of SDAE at any depth. These results suggest that our method can effectively help auto encoders learn more and better task-relevant feature extractors to achieve better task performance."}, {"heading": "5 Conclusion", "text": "Auto-encoders try to capture as much information as possible in the input data and must expend part of their capacity to learn task-irrelevant information, if any. More importantly, task-irrelevant information can lead the eventual classification to overmatch, leading to poor performance. The proposed method is a simple and effective method of variable selection to deal with this problem. By several rounds of variable selection, the remaining input variables are fed into an auto-encoder to learn feature extractors. As this method is used for each layer of stacked auto-encoders, it not only eliminates task-irrelevant information, but also prints the deep network to some degree to efficiently control model complexity to achieve better performance. Experimental results show that the method can efficiently drop task-irrelevant variables and help auto-encoders learn more and better feature extractors."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the Doctoral Startup Foundation of China Three Gorges University (grant no. KJ2013B064), the Natural Science Foundation of Hubei (grant no. 2015CFB336) and the National Natural Science Foundation of China (grant no. 61502274)."}], "references": [{"title": "Neuro-fuzzybased hybrid controller for stable temperature of liquid in heat exchanger", "author": ["S. Admuthe", "R. Chile"], "venue": "International Journal of Computational Science and Engineering, 10(1):220 \u2013 230.", "citeRegEx": "Admuthe and Chile,? 2015", "shortCiteRegEx": "Admuthe and Chile", "year": 2015}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["E. Arisoy", "A. Sethy", "B. Ramabhadran", "S. Chen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5421\u2013", "citeRegEx": "Arisoy et al\\.,? 2015", "shortCiteRegEx": "Arisoy et al\\.", "year": 2015}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Dediu, A.-H., Mart\u00c3n-Vide, C., Mitkov, R., and Truthe, B., editors, Statistical Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science, pages 1\u201337.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "H. Larochelle", "D. Popovici", "U. Montreal"], "venue": "In NIPS, pages 153\u2013160.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y.-l", "Cun", "Y. L"], "venue": null, "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K.Q. Weinberger", "F. Sha", "Y. Bengio"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1476\u20131484.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642\u20133649. IEEE.", "citeRegEx": "Ciresan et al\\.,? 2012", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Applied logistic regression, volume 398", "author": ["D.W. Hosmer Jr", "S. Lemeshow", "R.X. Sturdivant"], "venue": "John Wiley & Sons.", "citeRegEx": "Jr et al\\.,? 2013", "shortCiteRegEx": "Jr et al\\.", "year": 2013}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 473\u2013480. ACM.", "citeRegEx": "Larochelle et al\\.,? 2007", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in neural information processing systems, pages 873\u2013 880.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "An intelligent oil reservoir identification approach by deploying quantum levenberg-marquardt neural network and rough set", "author": ["N. Liu", "F. Zheng", "K. Xia"], "venue": "International Journal of Computational Science and Engineering, 6(1-2):76 \u2013 85.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lecture notes, 72.", "citeRegEx": "Ng,? 2011", "shortCiteRegEx": "Ng", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems, pages 2294\u20132302.", "citeRegEx": "Rifai et al\\.,? 2011a", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 645\u2013 660. Springer.", "citeRegEx": "Rifai et al\\.,? 2011b", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 833\u2013840.", "citeRegEx": "Rifai et al\\.,? 2011c", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning and selecting features jointly with point-wise gated boltzmann machines", "author": ["K. Sohn", "G. Zhou", "C. Lee", "H. Lee"], "venue": "International Conference on Machine Learning, pages 217\u2013225.", "citeRegEx": "Sohn et al\\.,? 2013", "shortCiteRegEx": "Sohn et al\\.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "The Journal of Machine Learning Research, 11:3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Attentional neural network: Feature selection using cognitive feedback", "author": ["Q. Wang", "J. Zhang", "S. Song", "Z. Zhang"], "venue": "ArXiv e-prints.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Neural networks are widely applied in various fields, such as oil exploration in Liu et al. (2011), speech recognition in Arisoy et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 0, "context": "(2011), speech recognition in Arisoy et al. (2015), temperature control in Admuthe and Chile (2015), and so on.", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": "(2015), temperature control in Admuthe and Chile (2015), and so on.", "startOffset": 31, "endOffset": 56}, {"referenceID": 13, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 6, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 15, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 3, "context": "Details of auto-encoders and their applications in deep learning can be found in Hinton et al. (2006); Bengio (2013); Ciresan et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al. (2012); Bengio et al.", "startOffset": 8, "endOffset": 45}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al. (2012); Bengio et al. (2013). Auto-encoders are usually implemented with neural networks, but over-complete (higher dimensional hidden layer than the input layer) and unconstrained autoencoders may learn identical mapping, which results in useless features.", "startOffset": 8, "endOffset": 67}, {"referenceID": 21, "context": "Because of the unsupervised training, auto-encoders attempts to capture everything in input data, including task-irrelevant information if there exits(Vincent et al., 2010).", "startOffset": 150, "endOffset": 172}, {"referenceID": 4, "context": "explored supervised pretraining (Bengio et al., 2007), and they concluded that partially supervised pre-training (alternately perform supervised and unsupervised training of an autoencoder) can lead auto-encoders to learn better feature extractors when much task-irrelevant information is contained in training data.", "startOffset": 32, "endOffset": 53}, {"referenceID": 19, "context": "proposed pointwise gated Boltzmann machines (PGBM) (Sohn et al., 2013).", "startOffset": 51, "endOffset": 70}, {"referenceID": 22, "context": "proposed the attentional neural network (aNN) (Wang et al., 2014).", "startOffset": 46, "endOffset": 65}, {"referenceID": 15, "context": "Gradient descent can be utilized to optimized the error function during training. When the training is completed, the output layer with the weights of hidden to output are dropped and the learned representation feature holds in the hidden layer, which can be used for classification or used as the input of an other autoencoder to learn more abstract feature. However, an AE may learn identity, which leads to obtain no useful featrues. In order to prevent this situation, AEs often utilize the configuration called \u201cbottleneck\u201d of which the quantity of hidden units lower than input units. An other approch is adding regular terms on the objective function to constrain the weights. Otherwise, using disturbed input data for training is also an effective means, like denoising autoencoder. Denoising auto-encoder(DAE) is a variant of standard auto-encoder. It attempts to reconstruct the input x from the encoded representation h of noisy input x\u0303 via a decoder. By disturbing the input x, denoising auto-encoder tries to learn robust features that can successfully recover the perturbed values to reconstruct the original input data. If a DAE can recover the original input data from the code of corruped input data, it can be said that the DAE has leanred robust and stable features. Stacked denoising auto-encoder(SDAE) is formed by stacking multiple single-layer DAEs for learning more abstract representations. SDAE can be used to effectively pre-train deep networks. In the process of pre-training by SDAE, the hidden features learned by lower-layer DAE are used as inputs for training next (upper-layer) DAE, and the encoders of DAEs are used to initialize weights in the deep network. See Vincent et al. (2010) for details.", "startOffset": 69, "endOffset": 1720}, {"referenceID": 5, "context": "see Bishop (2006); Hosmer Jr et al.", "startOffset": 4, "endOffset": 18}, {"referenceID": 5, "context": "see Bishop (2006); Hosmer Jr et al. (2013) for MLR in detail.", "startOffset": 4, "endOffset": 43}, {"referenceID": 18, "context": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is called DAEIVS, and compared performances produced by stacked DAE-IVS (SDAE-IVS), stacked DAE (SDAE) Vincent et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 18, "context": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is called DAEIVS, and compared performances produced by stacked DAE-IVS (SDAE-IVS), stacked DAE (SDAE) Vincent et al. (2010), PGBM Sohn et al.", "startOffset": 61, "endOffset": 215}, {"referenceID": 18, "context": "(2010), PGBM Sohn et al. (2013), and aNN Wang et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 15, "context": "(2013), and aNN Wang et al. (2014) .", "startOffset": 18, "endOffset": 35}, {"referenceID": 15, "context": "(2013), and aNN Wang et al. (2014) . The baseline was SDAE trained with standard training strategy described in Vincent et al. (2010). We tested different depth of SDAE and SDAEIVS from 1 layer to 3 layers.", "startOffset": 18, "endOffset": 134}, {"referenceID": 12, "context": "Our datasets were several variants of MNIST dataset for recognizing images of handwritten digits Larochelle et al. (2007), including MNIST with random background(bgrand) or with image background (bg-img) and the combination of rotated digits with image background (rot-bg-img).", "startOffset": 97, "endOffset": 122}], "year": 2016, "abstractText": "Auto-encoders are often used as building blocks of deep network classifier to learn feature extractors, but task-irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network. In this paper,via dropping the task-irrelevant input variables the performance of auto-encoders can be obviously improved .Specifically, an importance-based variable selection method is proposed to aim at finding the task-irrelevant input variables and dropping them.It firstly estimates importance of each variable,and then drops the variables with importance value lower than a threshold. In order to obtain better performance, the method can be employed for each layer of stacked auto-encoders. Experimental results show that when combined with our method the stacked denoising auto-encoders achieves significantly improved performance on three challenging datasets.", "creator": "LaTeX with hyperref package"}}}