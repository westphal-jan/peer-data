{"id": "1605.09507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "abstract": "Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.", "histories": [["v1", "Tue, 31 May 2016 07:11:18 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v1", "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016"], ["v2", "Fri, 18 Nov 2016 08:54:57 GMT  (1693kb,D)", "http://arxiv.org/abs/1605.09507v2", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016"], ["v3", "Mon, 26 Dec 2016 12:29:26 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v3", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This is initial submission version. Fully edited version is available atthis http URL"]], "COMMENTS": "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016", "reviews": [], "SUBJECTS": "cs.SD cs.CV cs.LG cs.NE", "authors": ["yoonchang han", "jaehun kim", "kyogu lee"], "accepted": false, "id": "1605.09507"}, "pdf": {"name": "1605.09507.pdf", "metadata": {"source": "CRF", "title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "authors": ["Yoonchang Han", "Jaehun Kim", "Kyogu Lee"], "emails": ["han@snu.ac.kr,", "eldrin@snu.ac.kr,", "kglee@snu.ac.kr)."], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. PROLIFERATION OF DEEP NEURAL NETWORKS IN", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "III. SYSTEM ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Audio Preprocessing", "text": "The Convolutionary Neural Network is one of the representation methods that allows a machine to be fed raw data and automatically determine the representations required for classification or recognition [10]. However, proper pre-processing of the input data is still an important issue in order to improve the performance of the system.In the first pre-processing step, the stereo input tone is converted to mono by taking the mean of the left and right channels, and then it is sampled down from the original sampling frequency of 44,100 Hz to 22,050 Hz. This allows us to use frequencies up to 11,025 Hz, the nylquist frequency, and it is sufficient to cover most of the harmonics produced by musical instruments, while removing sounds that may be contained in the frequencies above that range. In addition, all audios are normalized by dividing the time domain signal by its maximum value."}, {"heading": "B. Network Architecture", "text": "ConvNets can be seen as a combination of feature extractor and classifier. Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet structure, which includes very deep architecture with repeated multiple convolution layers, followed by max pooling, as shown in Figure 1. This method of using smaller receptive window size and smaller increments for ConvNet is widely used, especially in the field of image processing, as in the study by Zeiler and Fergus [33] and Sermanet al. [34], which provides superior performance in ILSVRC-2013.Although the general architectural style is similar to that of other successful ConvNets in the field of image processing, the proposed ConvNet is designed according to our input data. Weuse filters with a very small 3 \u00d7 3 receptive field, with a fixed strict size of 1, and spatial abstractions with a maximum abstraction of 8 \u00b0 C, are used by other successful Nvets in the image processing area proposed by our ConvNets, where the NConvNets are conceived."}, {"heading": "C. Training Configuration", "text": "The training was performed by optimizing the categorical crossentropy between predictions and targets. We used Adam [36] as an optimizer with a learning rate of 0.001 and the minibatch size was set to 128. To speed up the learning process with parallelization, we used a GTX 970 GPU that has 4GB of disk space. Training was regulated by dropout at a rate of 0.25 after each maximum pooling shift. Dropout is a technique that prevents overadjustment of units to training data by randomly dropping some units from the neural network during the training phase. [37] In addition, we added dropout after a fully connected layer as well as at a rate of 0.5, as a fully connected layer easily suffers from overadjustment. In addition, we conducted an experiment with different time resolutions to find the optimal analysis size. Since our training data was a fixed 3-audio training layer, we divided the 1.5-audio layer for each 0.5, and the 3.0-layer for each one, we performed an experiment with different time resolutions to find the optimal analysis size."}, {"heading": "D. Activation Function", "text": "In this section, we present several activation functions used in the experiment for comparison. The traditional way to model the activation of a neuron is to use a hyperbolic tangent (tanh) or a sigmoid function. However, we have shown that non-saturating nonlinearities such as the rectified linear unit (ReLU) allow much faster learning than these saturating nonlinearities, especially for models trained on large datasets [18]. Furthermore, we have shown that the performance of ReLU is better than that of Sigmoid and tanh activation [39]. Most modern studies on ConvNets use ReLU to model the output of neurons [28], [33]. ReLU was first introduced by Nair and Hinton in their work on restricted Boltzmann machines."}, {"heading": "IV. EVALUATION", "text": "A. IRMAS dataset The IRMAS dataset contains musical audio extracts with annotations on the dominant instruments and is intended to be used for automatic identification of the dominant instruments in music. This dataset was used in the essay on the classification of dominant instruments by Bosch et al. [44] and covers music from different decades of the last century, which greatly varies the audio quality. In addition, the dataset covers a wide variation in the types of musical instruments, articulations, recording and production styles and performers. The dataset is divided into training and test data, and all audio files are in 16-bit stereo waves with 44,100 Hz sampling rate. The training data consisted of 6705 audio files with extracts of 3 s from more than 2000 different recordings. Two subjects were paid to obtain the data for 11 tuned instruments, as shown in Table II from selected music tracks, with the aim of having additional training data included in the training data, with the goal of having a single music extract to be present."}, {"heading": "B. Testing Configuration", "text": "In the training phase, we used a fixed length window, because the input data for ConvNet should be in a certain fixed form. However, our audios tests had variable lengths between 5 s and 20 s, which were much longer than that of the training audio. Developing a system that can handle variable length of input data is valuable because music in real life varies in its length. We performed short-term analyses with overlapping windows to obtain local instrument information in the audio excerpts. As there is one comment per audio clip, we observed several sigmoid outputs and aggregated them to make a click-by-click decision. We tried two different strategies for aggregation, which are the average and the normalized sum of instruments, which are referred to as S1 and S2 throughout the paper. For S1, we simply took an average of sigmoid outputs and aggregated them to make the sigmoid outputs class-by-class (i.e., over-class threshold values)."}, {"heading": "C. Performance Evaluation", "text": "Using the evaluation method widely used in the instrument recognition task, we calculated the precision and retrieval, defined as asP = tptp + fp (4), where tp is true positive, fp is false positive and fn is false negative. In addition, we used the F1 measure to calculate the total performance of the system, which is the harmonic mean between precision and retrieval: F1 = 2PRP + R (6) Since the number of annotations for each class (i.e. 11 musical instruments) was not the same, we calculated the precision, retrieval and F1 measure for both the micro and macro averages. For the micro average, we calculated the metrics globally independent of classes, giving more weight to the instrument with a higher number of appearances. On the other hand, we calculated the metrics for each label and found its unweighted average for the macro averages; therefore, it is not related to the number of instances, but rather we repeated the overall performance three times over each experiment."}, {"heading": "V. RESULTS", "text": "We used LReLU (\u03b1 = 0.33) for the activation function, 1 s for the analysis window, S2 for the aggregation strategy and 0.50 for the identification threshold as the default setting of the experiment where this showed the best performance. Experimental variables are in Table III.First, we compared the performance of the proposed ConvNet with that of the existing algorithm on the IRMAS dataset. The effect of the activation function, analysis window, aggregation strategy and identification threshold on detection performance was analyzed separately in the following subsections."}, {"heading": "A. Comparison to Existing Algorithms", "text": "The existing algorithm of Fuhrmann and Herrera [45] used typical handmade Timbral audio features with their frame and variance statistics to train SVMs, and Bosch et al. [44] improved this algorithm with the source division called FASST (Flexible Audio Source Separation Framework) [46] in a pre-processing step. In terms of precision, the Fuhrmann and Herrera algorithm performed best for both the micro and macro measure. However, its memory was very low, about 0.25, resulting in a low F1 measurement. Our proposed ConvNet architecture outperformed existing algorithms on the IRMAS dataset for both the micro and macro F1 measurement, as shown in Figure 3. From this result, it can be determined that the function learned from the input data classified via ConvNet works better than the traditional handmade features with SVMs."}, {"heading": "B. Effect of Activation Function", "text": "In the case of the use of corrected units as an activation function, it was possible to observe a significant improvement in performance compared to the Tanh baseline as expected, as shown in Table IV. Contrary to the result presented in the ImageNet classification work by He et al. [42], PReLU showed no improvement in performance, but only a performance consistent with ReLU in our task. On the other hand, LReLU with a very leaky alpha setting (\u03b1 = 0.33) showed the best identification performance, which was consistent with the result of the empirical evaluation work on ConvNet activation function by Xu et al. [43] This result shows that the suppression of the negative part of the activation certainly improved performance compared to ReLU, as the entire part that caused this activity function by Xu et al. [43] also had no negative effects on the activity of the task."}, {"heading": "C. Effect of Analysis Window Size", "text": "As mentioned above, we conducted an experiment with different analysis window sizes such as 3.0, 1.5, 1.0, and 0.5 s to find the optimal analysis resolution. Figure 4 shows the micro and macro F1 measurement with different analysis window sizes depending on the identification threshold, and it can be noted that using the longest 3.0 s window was clearly inferior to using shorter window sizes regardless of identification threshold. However, shortening the analysis window to 0.5 s reduced overall performance again, which indicates that 1.0 s is the optimal analysis window size for our task."}, {"heading": "D. Effect of Identification Threshold", "text": "On the contrary, a lower threshold leads to better memory with lower precision. Therefore, we used the F1 measure, which is the harmonic mean of precision and memory, to evaluate overall performance. In terms of the F1 measure, we found that 0.5 is the most appropriate threshold, since it shows the best performance for the macro F1 measure, as shown in Figure 4. The current system uses a certain identification threshold for all instruments. However, we think there could be room for improvement by using different thresholds for each instrument, as different types of instruments are included in the experiment. For example, the amplitude of the piano sound in a number of music segments was relatively low because it is normally used as an accompanying instrument. On the other hand, the flute sound in the music was usually louder than others because it is normally used as a lead instrument."}, {"heading": "E. Effect of Aggregation Strategy", "text": "We conducted an experiment with two different strategies, S1 and S2, for aggregating ConvNet outputs as described in the Testing Configuration section. Performance of S1 and S2 is shown in Table V with a threshold \u03b8, which yielded the highest F1 measurement for each strategy. As a result, S2 overall showed better identification performance than S1. There was only a small performance gap between S1 and S2 for micro F1 measurement, but the difference was remarkable for the macro F1 measurement. This result shows that performing a class-by-sum followed by normalization is a better aggregation method for the prevailing instrument identification than performing class-by-means. This is probably due to the fact that training and test audios vary greatly in quality depending on the recording and production time, and the normalization of audio extracts helped minimize the effects of quality differences between extracts, which would lead to a more general output."}, {"heading": "F. Analysis of Instrument-Wise Identification Performance", "text": "The results are based on the overall performance. In this section, we analyze and discuss the results of the study in detail."}, {"heading": "G. Qualitative Analysis with Visualization Methods", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we described how to use ConvNet to identify dominant instruments in the real world of music. In addition, we trained the network with fixed-length single-labeled data, and identified an arbitrary number of dominant instruments in a music clip with a variable length. Our results showed that very deep ConvNet is able to achieve good performance by automatically learning the corresponding feature from the input data. Our proposed ConvNet architecture surpassed all previous approaches to processing instruments with a prevailing instrument threshold on the IRMAS dataset. However, Mel's spectrogram was used as input for the ConvNet, and we did not use source separation in pre-processing as opposed to existing works. We conducted multiple experiments with different activation functions for ConvNet. Tanh and ReLU were used as the baseline, and the recently introduced LReLU functions were used as input for the ConvNet, and the results were assessed differently with different LU activation functions, which differed from existing works."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported partly by the MSIP (Ministry of Science, ICT and Future Planning), Korea, under the ITRC (Information Technology Research Center) funding programme (IITP-2016-H8501-16-1016) under the direction of the IITP (Institute for Information & Communications Technology Promotion) and partly by a National Research Foundation of Korea funded by the MSIP (NRF-2014R1A2A2A04002619)."}], "references": [{"title": "Musical instrument recognition using cepstral coefficients and temporal features", "author": ["A. Eronen", "A. Klapuri"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on, vol. 2. IEEE, 2000, pp. II753\u2013II756.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Modified group delay feature for musical instrument recognition", "author": ["A. Diment", "P. Rajan", "T. Heittola", "T. Virtanen"], "venue": "10th International Symposium on Computer Music Multidisciplinary Research (CMMR). Marseille, France, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse cepstral codes and power scale for instrument identification", "author": ["L.-F. Yu", "L. Su", "Y.-H. Yang"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 7460\u20137464.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Music instrument recognition: from isolated notes to solo phrases", "author": ["A. Krishna", "T.V. Sreenivas"], "venue": "Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP\u201904). IEEE International Conference on, vol. 4. IEEE, 2004, pp. iv\u2013265.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Musical instrument recognition on solo performances", "author": ["S. Essid", "G. Richard", "B. David"], "venue": "Signal Processing Conference, 2004 12th European. IEEE, 2004, pp. 1289\u20131292.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Musical instrument recognition in polyphonic audio using source-filter model for sound separation.", "author": ["T. Heittola", "A. Klapuri", "T. Virtanen"], "venue": "in ISMIR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Instrument identification in polyphonic music: Feature weighting to minimize influence of sound overlaps", "author": ["T. Kitahara", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2007, no. 1, pp. 155\u2013155, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A novel cepstral representation for timbre modeling of sound sources in polyphonic mixtures", "author": ["Z. Duan", "B. Pardo", "L. Daudet"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 7495\u20137499.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Rwc music database: Music genre database and musical instrument sound database.", "author": ["M. Goto", "H. Hashiguchi", "T. Nishimura", "R. Oka"], "venue": "in ISMIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3\u20134, pp. 197\u2013387, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Investigation of recurrentneural-network architectures and learning methods for spoken language understanding.", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent neural networks for language understanding.", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning algorithms for classification: A comparison on handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "C. Cortes", "J.S. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger", "P. Simard"], "venue": "Neural networks: the statistical mechanics perspective, vol. 261, p. 276, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Handwritten digit recognition using convolutional neural networks and gabor filters", "author": ["A. Calder\u00f3n", "S. Roa", "J. Victorino"], "venue": "Proc. Int. Congr. Comput. Intell, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "A novel hybrid cnn\u2013svm classifier for recognizing handwritten digits", "author": ["X.-X. Niu", "C.Y. Suen"], "venue": "Pattern Recognition, vol. 45, no. 4, pp. 1318\u20131325, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2010, pp. 1279\u20131287.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio.", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "in ISMIR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "S. Bock"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6979\u20136983.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking automatic chord recognition with convolutional neural networks", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "Machine Learning and Applications (ICMLA), 2012 11th International Conference on, vol. 2. IEEE, 2012, pp. 357\u2013362.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio chord recognition with recurrent neural networks.", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ISMIR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Boundary detection in music structure analysis using convolutional neural networks.", "author": ["K. Ullrich", "J. Schl\u00fcter", "T. Grill"], "venue": "in ISMIR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Music boundary detection using neural networks on combined features and two-level annotations", "author": ["T. Grill", "J. Schl\u00fcter"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Malaga, Spain, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Musical instrument sound classification with deep convolutional neural network using feature fusion approach", "author": ["T. Park", "T. Lee"], "venue": "arXiv preprint arXiv:1512.07370, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic instrument recognition in polyphonic music using convolutional neural networks", "author": ["P. Li", "J. Qian", "T. Wang"], "venue": "arXiv preprint arXiv:1511.05520, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Y. Hoshen", "R.J. Weiss", "K.W. Wilson"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4624\u20134628.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["D. Palaz", "R. Collobert", "M.M. Doss"], "venue": "arXiv preprint arXiv:1304.1018, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning sparse feature representations for music annotation and retrieval.", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J.O. Smith"], "venue": "in ISMIR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from over-  JOURNAL OF  LATEX CLASS FILES, VOL. 14, NO. 8, MAY 2016  13 fitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1929}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "arXiv preprint arXiv:1512.07108, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, 2013, p. 1.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "arXiv preprint arXiv:1505.00853, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of sound segregation techniques for predominant instrument recognition in musical audio signals.", "author": ["J.J. Bosch", "J. Janer", "F. Fuhrmann", "P. Herrera"], "venue": "in ISMIR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Polyphonic instrument recognition for exploring semantic similarities in music", "author": ["F. Fuhrmann", "P. Herrera"], "venue": "Proc. of 13th Int. Conference on Digital Audio Effects DAFx10, 2010, pp. 1\u20138.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Harmonic and percussive sound separation and its application to mir-related tasks", "author": ["N. Ono", "K. Miyamoto", "H. Kameoka", "J. Le Roux", "Y. Uchiyama", "E. Tsunoo", "T. Nishimoto", "S. Sagayama"], "venue": "Advances in music information retrieval. Springer, 2010, pp. 213\u2013236.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "To name a few, Eronen used cepstral coefficients and temporal features to classify 30 orchestral instruments with several articulation styles and achieved a classification accuracy of 95% for instrument family level and about 81% for individual instruments [1].", "startOffset": 257, "endOffset": 260}, {"referenceID": 1, "context": "used a modified group delay feature that incorporates phase information together with mel-frequency cepstral coefficients (MFCCs) and achieved a classification accuracy of about 71% for 22 instruments [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "on cepstrum with temporal sum-pooling and achieved an Fmeasure of about 96% for classifying 50 instruments [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Some previous works such as Krishna and Sreenivas [4] experimented with a classification for solo phrases rather than for isolated notes.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "[5] reported that a classification system with MFCCs and GMM along", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] used a non-negative matrix factorization (NMF)-based source-filter model with MFCCs and GMM for synthesized polyphonic sound and achieved a recognition rate of 59% for six polyphonic notes randomly generated from 19 instruments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used various spectral, temporal, and modulation features with PCA and linear discriminant analysis (LDA) for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed the uniform discrete cepstrum (UDC) and mel-scale UDC (MUDC) as a spectral representation with a radial basis function (RBF) kernel support vector machine (SVM) to classify 13 types of Western instruments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The classification accuracy of randomly mixed chords of two and six polyphonic notes, generated using isolated note samples from the RWC musical instrument sound database [9], was around 37% for two polyphony notes and 25% for six polyphony notes.", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "However, it is getting more common to design the system to automatically discover the higherlevel representation from the raw data by stacking several layers of nonlinear modules, which is called deep learning [10].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "DNN-based approaches have outperformed previous state-of-the-art methods in speech applications such as phone recognition, largevocabulary speech recognition, multi-lingual speech recognition, and noise-robust speech recognition [11].", "startOffset": 229, "endOffset": 233}, {"referenceID": 11, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "On the other hand, ConvNet is useful for data with local groups of values that are highly correlated, forming distinctive local characteristics that might appear at different parts of the array [10].", "startOffset": 194, "endOffset": 198}, {"referenceID": 14, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "In addition, it has been reported that it has outperformed state-of-the-art approaches for several computer vision benchmark tasks such as object detection, semantic segmentation, and category-level object recognition [11], and also for speech-recognition tasks", "startOffset": 218, "endOffset": 222}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "These hierarchical network structures of ConvNets are highly suitable for representing music audio, because music tends to present a hierarchical structure in time and different features of the music might be more salient at different time scales [21].", "startOffset": 247, "endOffset": 251}, {"referenceID": 21, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 24, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 218, "endOffset": 222}, {"referenceID": 25, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 224, "endOffset": 228}, {"referenceID": 26, "context": "[27] and Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28], although it is still an ongoing work and is not a predominant instrument recognition method; hence, there are no other instruments but only target instrument sounds exist.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Our research differs from [27] because we deal with polyphonic music, while their work is based on the studio recording of single tones.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In addition, our research also differs from [28] because we use single-label data for training and estimate multi-label data, while they used multilabel data from the training phase.", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "on an end-to-end approach, which is promising in that using raw audio signals makes the system rely less on domain knowledge and preprocessing, but usually it shows a slightly lower performance than using spectral input such as melspectrogram in recent papers [29], [30].", "startOffset": 260, "endOffset": 264}, {"referenceID": 29, "context": "on an end-to-end approach, which is promising in that using raw audio signals makes the system rely less on domain knowledge and preprocessing, but usually it shows a slightly lower performance than using spectral input such as melspectrogram in recent papers [29], [30].", "startOffset": 266, "endOffset": 270}, {"referenceID": 9, "context": "The convolutional neural network is one of the representation learning methods that allow a machine to be fed with raw data and to automatically discover the representations needed for classification or detection [10].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "[31] and Hamel et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], which is a reasonable setting that sufficiently preserves the harmonic characteristics of the music while greatly reducing the dimensionality of the input data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet [32] structure, which contains very deep architecture using repeated several convolution layers followed by max-pooling, as shown in Figure 1.", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet [32] structure, which contains very deep architecture using repeated several convolution layers followed by max-pooling, as shown in Figure 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "from Zeiler and Fergus [33] and Sermanet et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 33, "context": "[34], which has shown superior performance in ILSVRC-2013.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "performance for image processing datasets such as CIFAR-10 and MNIST [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "We used Adam [36] as an optimizer with a learning rate of 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "Dropout is a technique that prevents the overfitting of units to the training data by randomly dropping some units from the neural network during the training phase [37].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "We used a uniform distribution with zero biases for both convolutional and fully connected layers following Glorot and Bengio [38].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "However, non-saturating nonlinearities such as the rectified linear unit (ReLU) allow much faster learning than these saturating nonlinearities, particularly for models that are trained on large datasets [18].", "startOffset": 204, "endOffset": 208}, {"referenceID": 38, "context": "the performance of ReLU is better than that of sigmoid and tanh activation [39].", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 107, "endOffset": 111}, {"referenceID": 39, "context": "on restricted Boltzmann machines [40].", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "[41], compresses the negative part rather than make it all zero, which might cause some initially inactive units to remain inactive.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42], is basically similar to LReLU in that it compresses the negative part.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] reported that the performance of LReLU is better than those of ReLU and PReLU, but sometimes it is worse than that of basic ReLU, depending on the dataset and the value for \u03b1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "01) were used, because it has been reported that the performance of LReLU considerably differs depending on the value and that very leaky ReLU works better [43].", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "[44] and includes music from various decades from the past century, hence differing in audio quality to a great extent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The existing algorithm from Fuhrmann and Herrera [45] used typical hand-made timbral audio features with their frame-", "startOffset": 49, "endOffset": 53}, {"referenceID": 43, "context": "[44] improved this algorithm with source separation called FASST (Flexible Audio Source Separation Framework) [46] in a preprocessing step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[44] improved this algorithm with source separation called FASST (Flexible Audio Source Separation Framework) [46] in a preprocessing step.", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Performance comparison of the predominant instrument recognition algorithm from Fuhrmann and Herrra [45], Bosch et al.", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "[44], and our proposed ConvNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42], PReLU did not show any performance improvement, but just showed a matching performance with ReLU in our task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "We selected the t-distributed stochastic neighbor embedding (t-SNE) [47] algorithm, which is a technique for dimensionality reduction of high-dimensional data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Second, we exploited the deconvolution [33], [48] method to identify the functionality of each unit in the proposed ConvNet model by visual analysis.", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "This method is highly effective especially in a dataset where its dimension is very high [47].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "The main principle of this method is to inverse every stage of operations reaching to the target unit, to generate a visually inspectable image that has been, as a consequence, filtered by the trained subfunctionality of the target unit [33].", "startOffset": 237, "endOffset": 241}], "year": 2017, "abstractText": "Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}