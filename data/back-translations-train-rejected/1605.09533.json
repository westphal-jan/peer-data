{"id": "1605.09533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems", "abstract": "This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled night-time road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings.", "histories": [["v1", "Tue, 31 May 2016 09:00:33 GMT  (2494kb,D)", "http://arxiv.org/abs/1605.09533v1", "8 pages, 12 figures, submitted to ITSC 2016"]], "COMMENTS": "8 pages, 12 figures, submitted to ITSC 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["matthias limmer", "julian forster", "dennis baudach", "florian sch\\\"ule", "roland schweiger", "hendrik p a lensch"], "accepted": false, "id": "1605.09533"}, "pdf": {"name": "1605.09533.pdf", "metadata": {"source": "CRF", "title": "Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems", "authors": ["Matthias Limmer", "Julian Forster", "Dennis Baudach", "Florian Sch\u00fcle", "Roland Schweiger", "Hendrik P.A. Lensch"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "II. Related Work", "text": "The precision of common GPS sensors of up to 10 m in length for localization meets the needs of regular navigation systems, but not of precise road estimation, especially for extended reality navigation. In order to achieve greater precision in terms of slowness and lateral localization, various sensors with longer and shorter perception are used. In the following, a few approaches are presented that are more closely related to the scope of this system. Tsogas et al. [5] fuse measurements of a camera, laser scanners and a digital map based on the clothoid road model. A Sugeno-fuzzy system determines appropriate weights for each of the different sensors that depend on predicting the distance of the ego vehicle and the range of the sensor."}, {"heading": "III. Framework", "text": "This paper uses a derivative of the framework [1] and is presented in Fig. 2. The modules of the framework are as follows: First, radar data is combined with a tracked first-person motion estimate to form a grid map; second, initialized by the GPSposition, the rough position is determined in a commercially available map database and the map parameters for that location are transformed into a compliant digital map model; third, the grid map and the digital map are merged into a longitudinal digital map; and fourth, the street detection module performs a corresponding camera image and generates an optical map."}, {"heading": "A. Grid Mapping", "text": "A grid map is a 2D map that quantifies the local environment into cells of equal size representing the occupancy (see Figure 3). Each cell integrates the corresponding sensor measurements of a distance sensor over time, thereby reducing the inherent noise and uncertainties of individual measurements. In the proposed framework, data from an automotive imaging radar that returns both the distances of the reflections and their velocities are stored in the grid map. As the ego vehicle moves, its relative position on the grid map must be determined by estimating the ego motion. An advanced Kalman filter with a CTRV model (constant speed and velocity [12]) uses the wheel speed and yaw velocity measurements to make an ego motion estimate."}, {"heading": "B. Digital Mapping", "text": "A commercial map database typically stores its information in the form of commented discrete points using the Universal Transverse Mercator (UTM) coordinate system. The number of points per street, the accuracy of such points, and the meta-information per point vary greatly as major roads are better scanned and maintained by database providers. To obtain a consistent local digital road model, the mold points around the current ego vehicle location are interpolated by a cubic hermit spline, creating the digital map that serves as the basis for the following fusion modules."}, {"heading": "C. Map Matching", "text": "In order to estimate the orientation and longitudinal position of the first person vehicle on the digital map, the grid map is inserted into the digital map using a particle filter. Each particle in the filter represents the position and orientation of the vehicle and is weighted according to various characteristics to how well the digital map and grid map fit. [13] The capture of the particles is initialized by the previous position or GPS position if no previous position is available."}, {"heading": "D. Road Detection", "text": "The original module for optical lane detection [3] in the framework of [1] is replaced by the lane-independent module for lane detection proposed in this essay. In this processing step, pixels in a camera image belonging to the currently used road are identified and the road boundaries are determined from these captured pixels, which are then transformed into the optical map and tracked. Further details on this processing step are described in section IV."}, {"heading": "E. Lane Course Fusion", "text": "To increase the accuracy of lateral localization, the optical map is merged with the digital map. Therefore, the lateral coordinates in the first-person vehicle coordinate system are scanned from both maps along the longitudinal paths of roadways or roadsides. Corresponding lateral positions are interpolated linearly by weighting each sensor according to its reliability for different distances from the first-person vehicle. The optical map is very reliable for narrow distances, while the digital map is more reliable for longer distances. [1] describes the specific weighting scheme."}, {"heading": "IV. Road Detection", "text": "The recognition module described below identifies the currently used road in a camera image by performing a pixel classification using deep learning techniques. It then extracts and tracks the left and right road boundaries taking into account the uncertainties and border closures of other road users. It then calculates the optical map that can be merged with the digital map."}, {"heading": "A. Scene Labeling", "text": "In fact, most of them are able to survive on their own, without having to move to another world."}, {"heading": "B. Road Segmentation", "text": "A CNN, as sketched above, generates a class membership card in which each pixel is assigned 1Validated turns. Fig. 6 shows a street segmentation generated by the following steps. Assuming that the largest contiguous group of classified street pixels approaches the actual contiguous group of street pixels, separated street pixel clusters can be neglected. Holes in the contiguous group of pixels are then filled with a flood fill algorithm. The algorithm is seeded at the bottom of the image, as this part of the road is supposed to be in most cases. A contour is extracted from the segmented street pixels, which is determined by the snake algorithm of [18] as a contour. The left and right road boundaries are then converted into pixels by dividing the contour at the highest central point of our street contact."}, {"heading": "C. Road Border Shaping", "text": "In order to calculate the optical map required in the following merging step, the estimated roadsides must be inserted into a conclusive road model. All estimates are bundled lengthwise, with each container representing a point for the shape of the roadsides. Values of each container are analyzed to calculate a reliability measurement for this particular point for the shape of the roadsides. Average values of the containers are used as mold points for the installation of a spline, while the interquartile area determines whether a point is used for the spline calculation. Using the meta information contained in the digital map, other lane edges, such as roadsides, can be interpolated from the left and right edges. If previous processing steps continuously fail to provide usable measurements for a road boundary (e.g. in sharp curves), this road boundary can be extrapolated by the other road boundary and the other edges."}, {"heading": "V. Experiments", "text": "The experiments carried out are twofold: firstly, different network topologies were redundantly trained and evaluated in terms of classification performance (Section V-B.) The data set for training and evaluation of classifiers consists of 7095 scenetypically labelled images of an NIR camera with nocturnal sequences of rural roads with a wide variety of weather conditions, seasons and landscapes. Secondly, the optical map of the most powerful classifiers was compared with the standard optical map of [3] using the fusion frame of [1] (Section V-C. The evaluation is based on five nocturnal sequences leading to a distance of 13.5 km, using a ground-truth curve recorded by a D-GPS sensor and a high-precision IMU. Figure 8 shows sample images of these sequences."}, {"heading": "A. Training of the CNN", "text": "Only certain combinations of the parameters mentioned in Section IV-A. The influence of the number of pyramid layers (nl), the depth of the network while maintaining the input field size (nc, kc), and the initial number of filters of the first convolution block (nf) were evaluated. Therefore, the topologies are called topo-nl-nc-nf, with a parameter range of nl, (nc, kc), (1, 7), (3, 3), and nf, (16, 32). Parameters nc and kc must be selected so that the input field size remains the same, which applies to the tuples defined above. The core size of the maximum pooling layers is set to 2 x 2 pixels, nf layers to all topology variants. Topo-4-1-32, for example, has the following parameters: 4, nc = All learning layers are divided to different levels (nc)."}, {"heading": "B. Scene Labeling Results", "text": "Table I shows the performance of the classifier in relation to several measures. These measures are the MCC [20], the total accuracy (ACC), the average across all classes (IUglobal) and specifically for the road (IUroad) and vehicle class (IUveh). The IU measure is defined as: IU = TPTP, FP, FN (1), where TP (true positive) is the number of correctly classified pixels and FP-FN (false positive and false negative) is the amount of incorrectly classified pixels in relation to a particular class. To ensure that identical topologies work similarly, each topology is trained three times. The table shows the average result for a topology of individually rated classifiers. Topology Topo-1-1-16 is comparable to the best rated topology of [9]. According to Table I, this topology achieves the lowest performance."}, {"heading": "C. Lane Course Prediction Results", "text": "In the following, the performance of the fusion system is evaluated using the best performing classifier for each topology variant q with respect to the optical track detection of [3] and the system without the optical map. Performance measurement is derived from [1]. It compares the deviations of road process assessments from the ground truth curve for different distances to the first-person vehicle. [1] has shown that its fusion algorithm primarily favors short-distance assessments, only the average performance of the five sequences for short-distance assessments (0-30m) is shown in Table II. The last line shows the error rates for lane detection (percentage of frames in which no road markings are detected). It should be noted that the average performance of the five sequences for short-distance assessments (0-30m) is calculated exclusively for frames containing detected road markers. Table II shows that the CNN-based map set is not detected by optical road markings (5 cm)."}, {"heading": "VI. Conclusion", "text": "This paper presented an in-depth, multi-scale approach to camera-based prediction and localization of road paths based on Convolutionary Neural Networks. Various network topologies have been trained to reliably detect road and vehicle pixels from which an optical map is extracted. Lower-lying topologies with a higher number of filters per folding layer perform better, while increasing the pyramid levels after level three does not significantly increase performance. However, unlike the baseline, our approach performs well in various weather conditions, even when road markings are missing, demonstrating that state-of-the-art performance can be achieved while increasing robustness and application in situations where traditional road markings are not possible."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Markus Thom and Oliver Hartmann for their valuable support."}], "references": [{"title": "Augmenting night vision video images with longer distance road course information", "author": ["F. Sch\u00fcle", "R. Schweiger", "K. Dietmayer"], "venue": "IEEE Intelligent Vehicles Symposium, 2013, pp. 1233\u20131238.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-sensor self-localization based on maximally stable extremal regions", "author": ["H. Deusch", "J. Wiest", "S. Reuter", "D. Nuss", "M. Fritzsche", "K. Dietmayer"], "venue": "IEEE Intelligent Vehicles Symposium, 2014, pp. 555\u2013560.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust lane recognition embedded in a real-time driver assistance system", "author": ["R. Risack", "P. Klausmann", "W. Kr\u00fcger", "W. Enkelmann"], "venue": "IEEE Intelligent Vehicles Symposium, 1998, pp. 35\u201340.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Recent progress in road and lane detection: a survey", "author": ["A. Bar Hillel", "R. Lerner", "D. Levi", "G. Raz"], "venue": "Machine Vision and Applications, vol. 25, no. 3, pp. 727\u2013745, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Combined lane and road attributes extraction by fusing data from digital map, laser scanner and camera", "author": ["M. Tsogas", "N. Floudas", "P. Lytrivis", "A. Amditis", "A. Polychronopoulos"], "venue": "Information Fusion, vol. 12, no. 1, pp. 28 \u2013 36, 2011, special Issue on Intelligent Transportation Systems.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic fusion of rural road course estimations", "author": ["F. Sch\u00fcle", "C. Koch", "O. Hartmann", "R. Schweiger", "K. Dietmayer"], "venue": "Proceedings of the IEEE International Conference on Intelligent Transportation Systems, 2013, pp. 1701\u20131706.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection and tracking of boundary of unmarked roads", "author": ["Y.W. Seo", "R.R. Rajkumar"], "venue": "International Conference on Information Fusion, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparative analysis of decision trees based classifiers for road detection in urban environments", "author": ["C. Fern\u00e1ndez", "R. Izquierdo", "D.F. Llorca", "M.A. Sotelo"], "venue": "Proceedings of the IEEE International Conference on Intelligent Transportation Systems, 2015, pp. 719\u2013724.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Road scene segmentation from a single image", "author": ["J.M. Alvarez", "T. Gevers", "Y. LeCun", "A.M. Lopez"], "venue": "Proceedings of the European Conference on Computer Vision. Springer Berlin Heidelberg, 2012, pp. 376\u2013389.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Comparison and evaluation of advanced motion models for vehicle tracking", "author": ["R. Schubert", "E. Richter", "G. Wanielik"], "venue": "International Conference on Information Fusion, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Global positioning using a digital map and an imaging radar sensor", "author": ["M. Szczot", "M. Serfling", "O. L\u00f6hlein", "F. Sch\u00fcle", "M. Konrad", "K. Dietmayer"], "venue": "IEEE Intelligent Vehicles Symposium, 2010, pp. 406\u2013411.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations, 2014, arXiv:1509.01951.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1915}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Giusti", "D.C. Ciresan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Proceedings of the IEEE International Conference on Image Processing, 2013, pp. 4034\u2013 4038.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A theory for rapid exact signal scanning with deep multi-scale convolutional neural networks", "author": ["M. Thom", "F. Gritschneder"], "venue": "Tech. Rep. arXiv:1508.06904, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Algorithms for Graphics and Image Processing, 1982", "author": ["T. Pavlidis"], "venue": "ch. Contour Tracing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1982}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278 \u20132324, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "A comparison of mcc and cen error measures in multi-class prediction", "author": ["G. Jurman", "S. Riccadonna", "C. Furlanello"], "venue": "PLoS ONE, vol. 7, no. 8, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25. Curran Associates, Inc., 2012, pp. 1097\u20131105.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1], [2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "Evaluation results show state-of-the-art performance compared to a baseline approach [3], but no failures when lane markings are not available.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "localization satisfies the needs of regular navigation systems but not those of a precise road course estimation, especially for augmented reality navigation [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "overview of different sensor fusion approaches is collated in [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "[5] fuse measurements of a camera, laser", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] is not dependent on the clothoid model and the digital map.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] describe a framework that fuses a NIR camera sensor, a radar sensor and a digital map.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Precise lateral localization is then accomplished by fusing the longitudinally mapped digital map with an optical lane recognition algorithm [3] in the camera image.", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "subsequent works [6], a Bayesian fusion system that performs the final road course estimation is introduced.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "[7] describe a road boundary estimator based on intensity distribution thresholding from camera images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] perform road detection by training decision trees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] describe a road scene segmentation from single images using a convolutional neural network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], [11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10], [11]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "The framework proposed in this paper is based on [1], but replaces the optical lane detection module from [3] with a road segmentation module based on deep multiscale CNNs.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "The framework proposed in this paper is based on [1], but replaces the optical lane detection module from [3] with a road segmentation module based on deep multiscale CNNs.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "This paper leverages a derivation of the framework from [1] and is depicted in Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 11, "context": "An extended Kalman filter with a CTRV-model (constant turn rate and velocity [12]) leverages the wheel speeds and yaw rate measurements to accomplish an ego-motion estimation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "particle of the filter represents the position and orientation of the vehicle and is weighted by how well the digital map and the grid map fit using various features [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "The original Optical Lane Detection module [3] in the", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "framework of [1] is replaced with the lane-independent Road Detection module proposed in this paper.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "The specific weighting scheme is described in [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "It combines the approach of [14] with the multi-scale scheme of [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "It combines the approach of [14] with the multi-scale scheme of [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "[14] introduces network topologies characterized by many convolution layers with small convolution kernels and comparatively", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Since realtime performance is needed for augmented reality applications, techniques from [16] and [17] are implemented", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "Since realtime performance is needed for augmented reality applications, techniques from [16] and [17] are implemented", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "Applying the CNN efficiently to complete images while retaining the full image resolution requires slight changes in various layers and the introduction of several helper layers into the network (see [16], [17]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "Applying the CNN efficiently to complete images while retaining the full image resolution requires slight changes in various layers and the introduction of several helper layers into the network (see [16], [17]).", "startOffset": 206, "endOffset": 210}, {"referenceID": 17, "context": "by using the snake algorithm of [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "performing classifiers were compared to the standard optical map from [3] using the fusion framework from [1] (Section V-C).", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "performing classifiers were compared to the standard optical map from [3] using the fusion framework from [1] (Section V-C).", "startOffset": 106, "endOffset": 109}, {"referenceID": 18, "context": "To train the topologies, multinomial logistic regression performs a stochastic gradient descent leveraging the backpropagation algorithm [19] with linear learn rate annealing.", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "fully connected layers are adjusted such that the multiclass extension of the Matthews Correlation Coefficient (MCC) [20] is optimized.", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "All trainings were conducted with cuda-convnet [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "These measures are the MCC [20], the overall accuracy (ACC), the intersection over union (IU) as an average over all classes (IUglobal) and specifically for the road (IUroad) and vehicle class (IUveh).", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "Topology topo-1-1-16 is comparable to the best performing topology of [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "In the following, the fusion system performance is evaluated using the best performing classifier for each topology variant in relation to the optical lane recognition from [3] and the system without the optical map.", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "The performance measure is taken from [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "Since [1] have shown that their fusion algorithm benefits primarily short range estimations, only", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Lane based estimation [3] performs better on scenes where the lane is clearly visible (A, B) but has a significant failure in all other conditions (C-E).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "lane-based [3] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Further, it should be noted that the best performing classifiers are not necessarily the best performing road course estimators, although a trend can be detected, when comparing topologies topo-[1,2]-*-* and", "startOffset": 194, "endOffset": 199}, {"referenceID": 1, "context": "Further, it should be noted that the best performing classifiers are not necessarily the best performing road course estimators, although a trend can be detected, when comparing topologies topo-[1,2]-*-* and", "startOffset": 194, "endOffset": 199}], "year": 2016, "abstractText": "This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled nighttime road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings.", "creator": "LaTeX with hyperref package"}}}