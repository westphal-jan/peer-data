{"id": "1605.09564", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler", "abstract": "Specialized dictionaries are used to understand concepts in specific domains, especially where those concepts are not part of the general vocabulary, or having meanings that differ from ordinary languages. The first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question. Classical methods for detecting this vocabulary involve gathering a domain corpus, calculating statistics on the terms found there, and then comparing these statistics to a background or general language corpus. Terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain. Here we present two tools, a directed crawler, and a distributional semantics package, that can be used together, circumventing the need of a background corpus. Both tools are available on the web.", "histories": [["v1", "Tue, 31 May 2016 10:31:16 GMT  (399kb)", "http://arxiv.org/abs/1605.09564v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["gregory grefenstette", "lawrence muchemi"], "accepted": false, "id": "1605.09564"}, "pdf": {"name": "1605.09564.pdf", "metadata": {"source": "CRF", "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler", "authors": ["Gregory Grefenstette", "Lawrence Muchemi"], "emails": ["gregory.grefenstette@inria.fr", "lawrence.githiari@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "Specialized dictionaries (Caruso, 2011) and domain-specific taxonomies are useful for describing the specific way a language is used in a domain, and for general applications such as domain-specific annotations or classifications. In order to create a specialized dictionary, it is first necessary to determine the characteristic vocabulary to include. These are words that are either domain-specific, or common words that have specialized uses within the domain. Recent advances in machine learning in natural language processing have led to the development of distributional semantic tools, such as word2vec, which use unattended training over a large corpus of text to embed words in an N-dimensional vector space (Goldberg and Levy, 2014). These vectors have the desirable property that words that are substitutional or found in similar contexts are closely related to this vector space."}, {"heading": "2. Building a Directed Crawler", "text": "A guided crawl is a web crawler for collecting text for a specific topic. A web crawler is a program that constantly retrieves web pages, starting with a list of seed URLs1. Each web page that is downloaded contributes new URLs written in a particular language or that contain specific keywords. In our dedicated crawl, we start with a list of seed URLs from the Open Directory Project2 (ODP), whose crowdsourced classification of web pages is used in many lexical semantic projects (e.g., Osi\u0144ski and Weiss, 2004; Lee at al, 2013; \u0160eva et al, 2015)."}, {"heading": "3. Word2vec", "text": "In fact, it is in such a way that it is a matter of a manner and manner in which most people who are in a position to understand the world and understand what it is about: a world in which the world is not about itself, but about a world in which the world is not about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, about itself, itself, about itself, about itself, itself, about itself, about itself, about itself, about itself"}, {"heading": "4. Combining a directed crawl and word2vec", "text": "In fact, it is not as if it were a pure conspiracy, but a conspiracy in which it is a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a, a conspiracy, a conspiracy, a, a conspiracy, a conspiracy, a conspiracy, a conspiracy, a, a, a conspiracy, a conspiracy, a conspiracy, a conspiracy,"}, {"heading": "4.1 Word2vec Trick", "text": "Word2vec can also be used to discover the distinctive vocabulary of a domain when using a domain text such as the one searched by a directed crawler, and a larger background text that does not come from the same domain. Without changing the word2vec code, you can make that vocabulary visible by using a q \"trick\" to insert an explicit label into the domain text and learn a word vector for that explicit label. Words that come closest to that explicit, inserted label are the words most predictable by the domain label. So we do this insertion: 1. Let's take the domain corpus generated by a directed crawl (as described in section 2 above), and remove stopwords8 and punctuation from the text. uppercase is the resulting text. 2. Let's insert a new uppercase text between each word."}, {"heading": "4.2 Examples", "text": "For example, the corpus we recognized for Vitiligo9 comes from 1000 websites and contains the following text:... Individuals with Vitiligo feel self-confident about their appearance and have a poor self-image that comes from fear of public rejection and psychosexual concerns... After this step we inserted an explicit label, for example VVV, between every word in the domain text.... Individuals VVVV vitiligo consciously feel VVVV as a poor VVV image. In step 2, we insert an explicit label, VVV, between every word in the domain text."}, {"heading": "4.3 From words to phrases", "text": "Using the technique described in Section 4.1, we collect a certain number of individual words that are candidates for the characteristic vocabulary. We can also extract multi-word phrases from our original domain body using a parser such as the Stanford parser or heuristic methods such as retaining sequences between stopwords. These multi-word phrases and their frequency are illustrated by the list of single words9 Vitiligo is a chronic skin disease characterised by parts of the skin losing pigment. 10 -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15 candidates to maintain each phrase that contains one of the single word candidates. We derive both from the single words and multi-word phrases before this filtering is performed. The most common multi-word phrase filtered from our 1000 site Vitiligo-Corpilgo vitiligo is vitiligo-vitiligo, which includes vitiligo-44-di44 white pattility 1269-69-white skin treatment."}, {"heading": "4.4 Filtering candidates", "text": "Since we intend to establish a taxonomy, we further filter these candidates by retaining only those candidates who appear in the domain corpus with another candidate, a random step that reduces the term candidate list to 990."}, {"heading": "4.5 Building a loose taxonomy", "text": "With the \"dog and poodle intuition,\" which means that one term > > > > Cell Therapy > > > occurs frequently in the same sentence, and that if one term occurs much more frequently (e.g. dog) than the other (e.g. poodle), then the more common term is the hypernym of the other, we also implement the string inclusion hypothesis, i.e., a sub-term of a long-term term term term is the hypernym of the longer-term term. These two strategies were sufficient to take first place in the SemEval 2015 taxonomy task (Grefenstette, 2015b). These two strategies generate a paired order of the received terms as hypernames and hyponyms."}, {"heading": "5. Conclusion", "text": "In this article, we explain how we have developed a controlled crawler that collects domain-specific text using open source tools, and also show how the collected body of word2vec can be used to discover the basic vocabulary for a particular domain."}, {"heading": "5.1 Acknowledgments", "text": "This work is supported by an Advanced Research Scholarship from Inria."}, {"heading": "6. References", "text": "In the second half of 2011, in the second half of 2011, pp. 66-75. 2011. Church, Kenneth and Patrick Hanks. \"Word association norms, mutual information, and lexicography.\" Word Association, and Richard K. Landauer. \"Indexing by latent semantic analysis.\" In Proceedings of the 27th ACL, pp. 76-83, 1989Deerwester, Scott; Susan T. Dumais; George W. Furnas; Thomas K. Landauer; and Richard Harshman; and lexicography and lexicography of the 27th ACL, pp. Gregagan Deerwester, Scott. \"Finding Experts Using Wikipedia.\" FEWS 290: 33-41. 2007. Chakrabarti, Soumen, Martin Van den Berg, and ByDom. \"Folcused Demartini: a new approach to specific resource discovery.\""}], "references": [{"title": "Online specialised dictionaries: a critical survey.\" In Electronic lexicography in the 21st century: New Applications for New Users", "author": ["Caruso", "Valeria"], "venue": "Proceedings of eLex 2011,", "citeRegEx": "Caruso and Valeria.,? \\Q2011\\E", "shortCiteRegEx": "Caruso and Valeria.", "year": 2011}, {"title": "Word association norms, mutual information, and lexicography.", "author": ["Church", "Kenneth", "Patrick Hanks"], "venue": "In Proceedings of the 27th ACL,", "citeRegEx": "Church et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Church et al\\.", "year": 1989}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "JASIS 41:391-407", "citeRegEx": "Scott et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Scott et al\\.", "year": 1990}, {"title": "Finding Experts Using Wikipedia.\" FEWS", "author": ["Demartini", "Gianluca"], "venue": "Computer Networks 31,", "citeRegEx": "Demartini and Gianluca.,? \\Q1999\\E", "shortCiteRegEx": "Demartini and Gianluca.", "year": 1999}, {"title": "Papers in linguistics, 1934-1951", "author": ["Firth", "John Rupert"], "venue": null, "citeRegEx": "Firth and Rupert.,? \\Q1957\\E", "shortCiteRegEx": "Firth and Rupert.", "year": 1957}, {"title": "Word2vec explained: Deriving Mikolov et al.'s Negative-sampling Word-embedding Method.", "author": ["Goldberg", "Yoav", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Explorations in automatic thesaurus discovery", "author": ["Gregory Grefenstette"], "venue": "Kluwer International Series in Engineering and Computer Science,", "citeRegEx": "Grefenstette.,? \\Q1994\\E", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "Personal Semantics.\" In Language Production, Cognition, and the Lexicon, pp. 203-219", "author": ["Grefenstette", "Gregory"], "venue": null, "citeRegEx": "Grefenstette and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette and Gregory.", "year": 2015}, {"title": "INRIASAC: Simple hypernym extraction methods", "author": ["Grefenstette", "Gregory"], "venue": null, "citeRegEx": "Grefenstette and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette and Gregory.", "year": 2015}, {"title": "Distributional structure.", "author": ["Harris", "Zellig S"], "venue": "Word 10,", "citeRegEx": "Harris and S.,? \\Q1954\\E", "shortCiteRegEx": "Harris and S.", "year": 1954}, {"title": "The Sketch Engine", "author": ["Kilgarriff", "Adam", "Pavel Rychly", "Pavel Smrz", "David Tugwell"], "venue": "In Proceedings of Euralex", "citeRegEx": "Kilgarriff et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kilgarriff et al\\.", "year": 2004}, {"title": "Semantic contextual advertising based on the open directory project.", "author": ["Lee", "Jung-Hyun", "Jongwoo Ha", "Jin-Yong Jung", "Sangkeun Lee"], "venue": "ACM Transactions on the Web (TWEB)", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Neural Word Embeddings as Implicit Matrix Factorization.", "author": ["Levy", "Omer", "Yoav Goldberg"], "venue": "Proceedings of NIPS", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings.\" Transactions of the Association for", "author": ["Levy", "Omer", "Yoav Goldberg", "Ido Dagan"], "venue": "Computational Linguistics", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Lin", "Dekang"], "venue": "In Proceedings of COLING-ACL,", "citeRegEx": "Lin and Dekang.,? \\Q1998\\E", "shortCiteRegEx": "Lin and Dekang.", "year": 1998}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Conceptual clustering using lingo algorithm: Evaluation on open directory project data.", "author": ["Osi\u0144ski", "Stanislaw", "Dawid Weiss"], "venue": "In Intelligent Information Processing and Web Mining,", "citeRegEx": "Osi\u0144ski et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Osi\u0144ski et al\\.", "year": 2004}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Pennington", "Jeffrey", "Richard Socher", "Christopher D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Experiments on linguistically based term associations.", "author": ["Ruge", "Gerda"], "venue": "In RIAO\u201991,", "citeRegEx": "Ruge and Gerda.,? \\Q1991\\E", "shortCiteRegEx": "Ruge and Gerda.", "year": 1991}, {"title": "Open Directory Project based universal taxonomy for Personalization of Online (Re) sources.", "author": ["\u0160eva", "Jurica", "Markus Schatten", "Petra Grd"], "venue": "Expert Systems with Applications 42,", "citeRegEx": "\u0160eva et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u0160eva et al\\.", "year": 2015}, {"title": "From frequency to meaning: vector space models of semantics", "author": ["Turney", "Peter D", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Learning Word MetaEmbeddings by Using Ensembles of Embedding Sets", "author": ["Yin", "Wenpeng", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "In our directed crawler, we begin our crawl using a list of seed URLs from the Open Directory Project (ODP) whose crowdsourced classification of web pages has been used in many lexical semantic projects (e.g., Osi\u0144ski and Weiss, 2004; Lee at al, 2013; \u0160eva et al., 2015).", "startOffset": 203, "endOffset": 270}, {"referenceID": 6, "context": "Frequency of other syntactic relations were used later (Grefenstette, 1994; Lin, 1998), including frequency of appearance in the same lists (Kilgarriff at al.", "startOffset": 55, "endOffset": 86}, {"referenceID": 15, "context": "Word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": ", 2013) and GloVe (Pennington et al., 2014) are two recent tools, among many others (Yin and Sch\u00fctze, 2015), for creating word embeddings.", "startOffset": 18, "endOffset": 43}], "year": 2016, "abstractText": "Specialized dictionaries are used to understand concepts in specific domains, especially where those concepts are not part of the general vocabulary, or having meanings that differ from ordinary languages. The first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question. Classical methods for detecting this vocabulary involve gathering a domain corpus, calculating statistics on the terms found there, and then comparing these statistics to a background or general language corpus. Terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain. Here we present two tools, a directed crawler, and a distributional semantics package, that can be used together, circumventing the need of a background corpus. Both tools are available on the web.", "creator": "Word"}}}