{"id": "1606.00025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Implementing a Reverse Dictionary, based on word definitions, using a Node-Graph Architecture", "abstract": "In this paper, we outline an approach to build graph based reverse dictionaries using word definitions. A reverse dictionary takes a phrase as an input and outputs a list of words semantically similar to that phrase, and is a solution to the Tip of the Tongue problem. We use a distance based similarity measure, computed on the graph, to assess the similarity between a word and the input phrase. We compare the performance of our approach with the Onelook Reverse Dictionary and a distributional semantics method based on word2vec, and show that our approach is much better than the distributional semantics method, and as good as Onelook's.", "histories": [["v1", "Tue, 31 May 2016 20:09:59 GMT  (277kb,D)", "https://arxiv.org/abs/1606.00025v1", "9 pages, 4 figures, 4 tables, submitted towards *SEM and EMNLP"], ["v2", "Fri, 15 Jul 2016 22:57:17 GMT  (562kb,D)", "http://arxiv.org/abs/1606.00025v2", "Added a new section titled 'Recommendations'. Polished multiple arguments. 10 pages, 4 figures, 4 tables, submitted at EMNLP and COLING"], ["v3", "Tue, 19 Jul 2016 08:55:35 GMT  (562kb,D)", "http://arxiv.org/abs/1606.00025v3", "Added a new section titled 'Recommendations'. Polished multiple arguments. 10 pages, 4 figures, 4 tables, submitted at EMNLP and COLING. Correction of abstract"], ["v4", "Mon, 26 Sep 2016 13:48:31 GMT  (294kb,D)", "http://arxiv.org/abs/1606.00025v4", "Accepted for publication at COLING, 2016. Minor changes. 10 pages, 4 figures, 4 tables"], ["v5", "Sat, 17 Dec 2016 22:36:15 GMT  (294kb,D)", "http://arxiv.org/abs/1606.00025v5", "Included publication information"]], "COMMENTS": "9 pages, 4 figures, 4 tables, submitted towards *SEM and EMNLP", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sushrut thorat", "varad choudhari"], "accepted": false, "id": "1606.00025"}, "pdf": {"name": "1606.00025.pdf", "metadata": {"source": "CRF", "title": "Implementing a Reverse Dictionary, based on word definitions, using a Node-Graph Architecture", "authors": ["Sushrut Thorat", "Varad Choudhari"], "emails": ["sushrut.thorat94@gmail.com", "varad.choudhari@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "A forward-looking dictionary (FD) is primarily a solution to the problem. A backward-looking dictionary (RD) from 1974 (Sierra, 2000), also known as an inverted dictionary, or a search term that includes a dictionary (Calvo et al., 2016), uses sentences for individual words that approximate the meaning of these terms. In the proceedings of the 26th International Conference on \"Brother,\" a definition of \"Brother\" is a boy or a man who has the same mother and father as another person. An inverted dictionary will not only use this phrase to refer to \"Brother,\" but also phrases such as \"ofson 1In the proceedings of the 26th International Conference on Computational Linguistics (COLING 2016), pages 2797-2806. Test data and a demo code can be found at: https: / / github.com / novelmartis / RD16demo2Accessed: February, 2016my Parents\" is primarily a solution to the problem. \""}, {"heading": "2 System Description", "text": "The block diagram of how the RD works is shown in Fig. 1. We will now discuss the concept of reverse map, which is central to the structure of our diagram, and the process of obtaining the connectivity matrix on which our diagram is based."}, {"heading": "2.1 The Reverse Map", "text": "In an inverted map, words branch off to the words contained in their definitions. In an inverted map, words branch off to the words in whose definitions they are included. An example of an inverted map 3 becomes the required word in Fig. 2. If the input phrase is a definition, a search depth of one (extending from the words of the input phrase to the definitions in which they are included) of the inverted map leads to the required word. An additional search depth provides us with semantic information about words whose definitions include or refer to concepts. An inverted map suggests semantic convergence in a flat search, although convergence may occur with multiple words, which is acceptable as they may be sectarian."}, {"heading": "2.2 Connectivity Matrix of the Reverse Map", "text": "The steps in building the connectivity matrix based on the reverse map are as follows: Our input is a forward dictionary, a list of function words, and a lemmatizer. We process the forward dictionary to keep content words in their basic form. We then construct the forward linked list, transform it into a backward linked list, and then construct the backward linked connectivity matrix. Likewise, we can construct the forward linked connectivity matrix."}, {"heading": "2.2.1 Processing the Forward Dictionary", "text": "A forward dictionary (FD) can be viewed as a two-dimensional list. The lines of the first column contain the words and the lines of the second column contain the corresponding definitions. We reduce all the words in column one to their lemmas, their basic form4. We then delete all the function words 5 (Fromkin et al., 2011) and the corresponding definitions in column two. For our purposes, we bundle all the definitions of a particular word in a single cell, parse them through the lemmmatrizers and delete all the function words contained therein. We refer to the resulting list as a forward linked list. We now create the backward linked list."}, {"heading": "2.2.2 The Back-linked list", "text": "We numbered the words in column one of the forward-linked list in alphabetical order (wordid) and replaced all the words in column two with their word Ids. The sequential linked list is generated by 4Using the pattern lemmatizer (Smedt and Daelemans, 2012) and wordnet morphy (Bird et al., 2009).5The functional words are from Higgins, 2014: http: / / myweb.tiscali.co.uk / wordscape / museum / funcword.htmlder the following algorithm: for i in [1, length (Fs)] do: for j in Fs (i, 2) do: Bs (j, 2).append (i) where Fs is the forward-linked list, and Bs is the back-linked list. We created a list that refers a word to the words within its definitions."}, {"heading": "2.2.3 The Back-Linked Matrix", "text": "We now create the matrix that represents the connections (weights) between the nodes (words, in this case).The rear matrix (BLM) is generated by the following algorithm: for i in [1, length (Fs)] do: for j in Bs (i, 2) do: BLM (j, i) = 1 BLM (i, i) = 0We will see in Section 3 that many words do not appear in any definition in a dictionary and therefore cannot contact all words in the word list through the back of the word. but we want to get the similarity between any two words in the word. as a simple measure to ensure complete connectivity, we build a mixed back matrix (mBLM) that has forward connections for words that do not have sufficient feedback connections. mBLM is generated by the following algorithm: mBLM = BLM = length (BLM) for i, [l] do = BS (Li = BLM)"}, {"heading": "2.3 The Node-Graph Architecture", "text": "Each word is represented by a node. Each node has two states {0, 1}. They respond to incoming signals by processing their state and transmitting the signal to downstream nodes. If S is the state of the node population, then n denotes the number of time steps to be taken, Iin the input signals to the nodes, Iext the external preload signals to the nodes, and Iout the output signals of the nodes, then the dynamics of the states of the nodes calculated by the algorithm: t = 0while t \u2264 n do: Iout = SIin = BLM Iout + Iext for i in [1, length (BLM)] do: if Iin (i) \u2265 1 then do: S (i) = 1 if Iin (i) = 0 then do: S (i) = 0t + 1We create a graph for i in [1, length (BLM)] do: if Iin (i) \u2265 1 then do: Iin (i)."}, {"heading": "2.4 The Similarity Measure", "text": "We use a distance-based measure of similarity. We define the distance dY, X from a word X to another word Y as the depth of the search required to develop a state with only SX = 1, to the first state with SY = 1. Note that dY, X 6 = dX, Y. We calculate the frequency of phenomena, {\u03bdZ} in all definitions, for all words {Z} in the word list. We define the similarity measure EW, P of a word W to an input set P that contains the input words {Pi} as: EW, P = \u2211 i (\u03bdPi \u00d7 dW, Pi) \u2212 1 \u2211 i \u03bd \u2212 1 PiWe weighted the inversion of the distances between words with the inversion of the frequencies of the input words. Thus, the similarity measure includes a measure of \"semantic importance\" of each input word in the input phrase. We calculate the similarity of each word with the input phrase, leaving the similarity in the size and the input phrase."}, {"heading": "2.5 System Summary", "text": "The user enters a phrase. Input (content) words are extracted from the phrase. Graphs are created for each input word, and in each graph the node corresponding to the input word is activated. Graphs are further developed to the maximum non-redundant search depth (see Section 3.). The similarity measure to the input phrase is calculated for each word in the lexicon, and the words are graded according to their similarity measures, which results in output."}, {"heading": "3 Graph exploration", "text": "We construct BLMs and mBLMs based on the prozess7 Oxford 3000 wordlist8 = deeper BLM for the entire WordNet (Miller, 1995) lexicon (WL). We use the Oxford Learner (OLD), MerriamWebster dictionary9 (MW) and WordNet (WN) dictionaries as forward-looking dictionaries for the Oxford 3000 word list, and WordNet for the WordNet lexicon (WL). We also build a BLM and mBLM by pooling definitions (Fusion BLM) from the three forward-looking dictionaries for the 3k word list to check the effect of using multiple dictionaries for performance. 7words that appeared in Oxford Learner's dictionary definitions but were not part of the BLariy word list were added to the BLaritivity word list to ensure consistency."}, {"heading": "4 Performance Analysis", "text": "The only online reverse dictionary available is the Onelook Reverse Dictionary (Beeferman, 2003), with which we compare the performance of our algorithm. Onelook is a commercial application, and its architecture is proprietary. We know that it indexes 1061 dictionaries and resources such as Wikipedia and WordNet. Onelook's lexicon is much larger than 3k. In performance comparison, we give the performance with (referred to as \"Corr\") and without adapting the results to the 3k lexicon. We also compare our approach with a distribution semantics method based on word2vec, which represents words as vectors in a linear structure that allows analog thinking. In this vector space, the \"King + Woman - Man\" vector bears a great resemblance to the vector for \"Queen\" (Mikolov et al., 2013a; Mikolov et al., 2013b)."}, {"heading": "4.1 Performance Test", "text": "We introduced users to the concept of the inverted dictionary and asked them to generate phrases11Based on Daniel Rodriguez's implementation of word2vec at https: / / github.com / danielfrg / word2vec, trained on a body of 15.8 million words and a vocabulary of 98 k, they would use it to get to a specific word if they forgot the word but retained the concept. 25 such users generated 179 phrases, a sample of which is shown in Table 2. Performance is measured by ranking the words in the outputs of their user-generated phrases."}, {"heading": "4.2 Performance results", "text": "This year, as never before, it will be able to retaliate, to retaliate."}, {"heading": "5 Recommendations", "text": "It is as if it were a reactionary project that will be able to retaliate."}, {"heading": "6 Concluding Remarks", "text": "In fact, it is as if it were a reactionary project that had been able to retaliate."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we outline an approach to build<lb>graph-based reverse dictionaries using word<lb>definitions. A reverse dictionary takes a<lb>phrase as an input and outputs a list of words<lb>semantically similar to that phrase. It is a so-<lb>lution to the Tip-of-the-Tongue problem. We<lb>use a distance-based similarity measure, com-<lb>puted on a graph, to assess the similarity be-<lb>tween a word and the input phrase. We com-<lb>pare the performance of our approach with the<lb>Onelook Reverse Dictionary and a distribu-<lb>tional semantics method based on word2vec,<lb>and show that our approach is much better<lb>than the distributional semantics method, and<lb>as good as Onelook, on a 3k lexicon. This sim-<lb>ple approach sets a new performance baseline<lb>for reverse dictionaries.1", "creator": "LaTeX with hyperref package"}}}