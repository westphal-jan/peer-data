{"id": "1606.00075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Applications of Probabilistic Programming (Master's thesis, 2015)", "abstract": "This thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking.", "histories": [["v1", "Tue, 31 May 2016 23:48:55 GMT  (2695kb)", "http://arxiv.org/abs/1606.00075v1", "Supervisor: Frank Wood. The thesis was prepared in the Department of Engineering Science at the University of Oxford"]], "COMMENTS": "Supervisor: Frank Wood. The thesis was prepared in the Department of Engineering Science at the University of Oxford", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yura n perov"], "accepted": false, "id": "1606.00075"}, "pdf": {"name": "1606.00075.pdf", "metadata": {"source": "CRF", "title": "Applications of Probabilistic Programming", "authors": ["Yura Perov"], "emails": [], "sections": [{"heading": null, "text": "This thesis describes the work on two examples of the application of probability programs: the learning of probability programs that specialize in specifications, in particular the program code of one-dimensional samplers; and the enabling of Monte Carlo sequential inferences with the help of data-driven suggestions. The latter is presented with experimental results on a linear Gaussian model and a non-parametric program code that depends on Dirichlet objects, which are a model for mixing objects and tracking.We begin this work by giving a brief introduction to probability programs, which we present an approach for the automatic discovery of samplers in the form of probability programs, in the form of probability programs. Specifically, we learn the process code of samplers for one-dimensional distributions. We formulate an approach for the automatic discovery of samplers in the form of probability programs, in the form of probability programs."}, {"heading": "Acknowledgement", "text": "The initial description of the approach and the initial experimental results for learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014) and in my bachelor thesis (Perov, 2014), recognizing that part of the work on data-driven proposals was done in collaboration with Tuan Anh Le under the direction of Prof. Frank Wood. Specifically, we brainstormed and developed the ideas jointly, wrote probabilistic program codes for linear Gaussian model and DPMO model experiments, and jointly conducted and analyzed the proposal for the experiment for the scientific rediscovery of classical genetic laws was prepared under the guidance of Prof. Joshua Tenenbaum. For initial experiments, a framework for stochastic simulation in Java (L'Ecuyeret al., 2002) was useful."}, {"heading": "1 Introduction to probabilistic programming 1", "text": "1.1 Another example of a probabilistic program and its execution track..... 2 Another example of a probabilistic program and its execution track........................................ 31.3. Existing probabilistic programming platforms and statistical inferencing in them............................................................................................................................................................................................................................................................................................................... 6."}, {"heading": "2 Learning probabilistic programs 10", "text": "The personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database, the personal data will be entered into the database."}, {"heading": "3 Data-driven proposals in probabilistic programming 50", "text": "3.0.1 Suggestions in a row Monte Carlo Inference............ 50 3.0.2 Use of a discriminatory model for data-driven suggestions... 51 3.0.3 Experiments with the linear Gaussian model........ 543.0.3.1 Functions for generating training and test episodes {y1: T} 55 3.0.3.2 Comparison of sequential Monte Carlo runs without and with data-driven suggestions...................... 58x3.0.4 Application of the approach to the DPMO model......... 603.0.4.1 DPMO model in Anglican.............. 3.0.4.2 Conjugates........"}, {"heading": "A Corpus of sampler code 78", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B The DDPMO and GPU code in Anglican 83", "text": "B.1 The DDPMO Code........................ 83 B.2 The GPU Code.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "1.1 An example of a probabilistic program, and its exe-", "text": "A basic probability model in the form of a probability program is presented below: (query (leave [unknown-mean-t1 (sample (normal 2 1)) unknown-mean-t2 (sample (normal unknown-mean-t1))) noise 0.1] (observe (normal unknown-mean-t1 noise) 3) (observe (normal unknown-mean-t2 noise) 3.1) (predict unknown-mean-t1) (predict unknown-mean-t2))) Equation 1.1 contains two latent variables, x1 and x2, and two observed data points, y1 and y2. Each run of a probability program yields a single execution track. An execution track is a map of random decisions to their specific values. An execution track defines the execution of the probability program completely before executing a probability program, where the execution sequence is probability-yn and the execution is x-1."}, {"heading": "1.2 Another example of a probabilistic program, and", "text": "Another simple example of a probabilistic program is a procedure that takes samples from the geometric distribution: for the sake of simplicity, this second example of a probabilistic program contains no observations and is therefore unconditional. In other words, its predecessor is the same as the one at the bottom. Examples of valid execution traces of this program are: 31. true; 2. false, true; 3. false, false, false, false, false, true.A valid execution track for this program is an arbitrary, possibly empty, sequence of false drags terminated by a true drag.The program is a good example because it shows that the number of random decisions made in each execution track for programs ending with probability 1 must always be finite, but this number is not necessarily limited by a constant."}, {"heading": "1.3 Existing probabilistic programming platforms and", "text": "As a rule, a new related engine has been developed for each probable programming language to perform some kind of statistical inference. Statistical inference methods are typically selected and implemented in such a way that automatic inferences are possible for all probable programs that can be expressed in a probable programming language. Thus, the expressiveness of the language varies and depends on the inference methods used. Specific languages and implementations include functional probable programming languages such as Church (Goodman et al., 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al., 2014). Logic, probable programming languages (De Raedt and Kimmig, 2013) such as ProbLog et al., 2011); and domain-specific PPLs such as (Kiselyov and Shan, 2009). Other languages and implementations are also favorable explanations for the probability programming languages."}, {"heading": "1.4 Ways to improve general-purpose statistical infer-", "text": "In fact, it is not as if they have been able to find a solution that they have been able to find. (...) It is not as if they have been able to find a solution. (...) It is as if they have been able to find a solution. (...) It is as if they have been able to find a solution. (...) It is not as if they have been able to find a solution. (...) It is as if they have been able to find a solution. \"(...)\" It is as if they have been able to find a solution. (...) \"It is not possible to find it.\" (...) \"(...) It is not as if they have been able to find a solution.\" (...)"}, {"heading": "2.1 Related work", "text": "Our work on learning probabilistic programs is related to both automatic programming and the generalization of data. The former, automatic programming, deals with the synthesis of program code from specifications. These specifications are often incomplete and include input / output examples. The latter, the generalization of data, is one of the main goals of machine learning in its entirety."}, {"heading": "2.1.1 Automatic programming", "text": "An up-to-date overview of automatic programming can be found in (Gulwani et al., 2014) and their references. Approaches to programming synthesis include work in the field of inductive logical programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), conclusions on grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998). Seen from a very abstract standpoint, automatic programming concerns searching in the complex space of the program code for programs that meet a certain specification. Automatic programming approaches differ from each other in the way the specifications are formulated, in the expressiveness of the search space and in which search algorithm is used. Automatic programming is often paired with the field of programming languages and verification to find statistical programs that best meet the search algorithms used in the search algorithms and the search expressions used in the search algorithms."}, {"heading": "2.1.2 Generalising from data and automated modelling", "text": "The approach we describe in this chapter is related to the density estimation (Silverman, 1986), which concerns the estimation of an unobservable probability density function taking into account some observed data. However, there is a significant difference between the density estimation and our approach. While most density estimation methods generate only a set of parameters (e.g. weights), we learn to present the observed data in a structural and potentially interpretable form of the generative model program code. Our approach is also related to learning probabilistic model structures (Grosse et al., 2012) and learning probabilistic relational models (Friedman et al., 1999) and Bayesian network structure (Mansinghka et al., 2012). Also worth noting is current work in the search for generative probable model structures (Grosse et al., 2012) and core compositions (Duvenaud et al., 2013)."}, {"heading": "2.2 Approach", "text": "We are interested in finding probability programs that generate samples that statistically resemble the distribution of interest F\u03bb with parameters 13Vector \u03bb. At the moment, we assume that the parameter vector \u03bb is fixed and omit it to simplify the notation. Any probability program that we consider to be a potential match is presented as its program text T. We define the grammar before the program text p (T), the details of which are described in Section 2.4. The distribution of interest F can be specified in various forms, e.g. as a set of samples X = {xi \u0445 F} or as characteristics of this distribution F (e.g. its moments). For each program candidate T, we want to evaluate how well it coincides with the distribution of interest F. There are no general ways to analytically verify this by looking at its program text. Therefore, we apply the methods of approximate Bayesian calculation. Specifically, we draw samples from the program X (X = T, X, T = T) before analyzing the program X (X, T = T)."}, {"heading": "2.2.1 Basics of approximate Bayesian computation (ABC)", "text": "s start with the standard structure for Bayean inference. There are some parameters of interest \u03b8, an intermediate hidden distribution pattern and an observation. We are able to model the previous distribution p (\u03b8), the conditional distribution p (\u03b8) for the intermediate hidden distribution pattern (IA). We are interested in the posteriorp distribution pattern p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV distribution pattern) p (IV) p (IV distribution pattern) p (IV) p (IV) p (IV distribution pattern) p (IV-IV) p (IV-IV) p (IV) p (IV-IV) p (IV-IV) p (IV-IV)"}, {"heading": "2.2.2 Matching distributions using ABC", "text": "\"Before describing a few other aspects of ABC that are relevant to our work, let us begin to grasp our problem in relation to ABC.\" We should remember that we approach this problem in the manner in which we have a prior distribution of interest in F (T), and we need to find the distribution T in the form of a probability program in which we want to compare two distributions, one given as probability program code and another by its properties. One could imagine doing this by making an infinite number of samples of T or by some kind of code analysis, but this is not feasible in the general case to compare the best of our knowledge."}, {"heading": "2.3 Noisy ABC", "text": "Another concept in ABC, which is indispensable for our work, is the suggestion of the \"noisy ABC\" by Wilkinson in (Wilkinson, 2013). He proposes replacing the fixed threshold with a general noisy kernel function K (y, z), which should be a valid probability density function in the light of a certain observation y. The kernel could be interpreted as a measurement or model error. It is expected to have high values if y \u2248 z, and low values otherwise. The common ABC target distribution is as follows (Marin et al., 2012): pK, (\u03b8, z | y) = ABC samples p (z | \u03b8) K (y, z) \u2022 p nominal values of the statistical distribution K (y, z) dz nominal nominal values of the nominal values are as follows (Marin et al., 2012): pK, (\u03b8, z | y) = ABC nominal values of the nominal values of the nominal values of the nominal values of the nominal values of the nominal values (z, z) nominal values of the nominal values of the nominal values of the nominal values of the nominal values of the nominal values."}, {"heading": "2.3.1 Moments matching as an example of statistic \u03b71", "text": "An example of such a statistic is the set of moments of a distribution. The nth moment, such as a fixed value c, the one-dimensional probability distribution, with existing probability density functions f, is known as: \u00b5n = (x \u2212 c) n samples (x) n f (x) dx, where the constant c could be equal, for example to zero (e.g. for the mean that is the first raw moment) or to the mean (e.g. for the variance that is the second central moment).Moments are widely studied in statistics and have long been used to estimate distribution parameters. Ideally, we would agree an infinite number of moments to achieve at least some limited distributions. Unfortunately, in practice we are limited and can only verify the finite number of moments."}, {"heading": "2.3.2 Use of hypothesis test statistics for \u03b71", "text": "Figure 2.2 shows pseudo-code that aims to find a sampler for the Bernoulli distribution family that is parameterized using the G-test statistic (McDonald, 2009): Gn, \u03bb = 2 x 0.1 # [X-n = i] ln (# [X-n = i] ln (# [X-n = i] ln) (1 \u2212 i) \u00b7 | X-n |), where # [X-n = i] is the number of samples in X-n that assume the value i. We calculate the p-value of the G-test 3 of the erroneous rejection of a zero hypothesis H0: X-x-Bernoulli (n). This p-value is incorporated into the core of the X value by observing a coin that is inverted and confronted with the probability corresponding to the p-value of the test."}, {"heading": "2.4 Prior over program code", "text": "We used a grammar similar to one introduced in our previous paper. (Perov and Wood, 2014; Perov, 2014) It complements the adapter grammar (Johnson et al., 2007) before it is used in (Liang et al., 2010) by using local environments. (The basic element in functional languages is an expression.) To generate a probabilistic program, we apply the production rules listed below recursively, with the type being the desired output signature of the production program. Production rules are applied stochastically with some probabilities: pi = 1 (covered later in this section). The set of types used in our experiments is a mapping of typed symbols to values (including such values as primitive and compound procedures), but these values are not evaluated / applied until the actual execution of a probabilistic program."}, {"heading": "2.5 Experiments", "text": "In the current work, we show in Section 2.5.1 that our approach is comparable to evolutionary algorithms, a common method of program synthesis. Subsequently, we reimplement our method into new probabilistic programming systems, namely the Anglican and probabilistic schemes, which gave us a tenfold improvement in speed, as reported in Section 2.5.2. Finally, in Section 2.5.3, we report on new experimental results, similar (Perov, 2014; Perov and Wood, 2014), but with thinner inning.2930"}, {"heading": "2.5.1 Evaluation of our approach versus evolutionary algorithms", "text": "Genetic programming is an evolutionary metaheuristic optimization algorithm that is used to generate a program in accordance with the specification. For a recent introduction to the field of genetic programming, see (Poli et al., 2008). The very similar grammar we \u2212 \u2212 described in Section 2.4 was written in Python in the evolutionary framework DEAP (Fortin et al., 2012). The fitness function was selected as the log probability, which was omitted from Equation 2.5 with the term p (X-X-T), in accordance with the assumption that searched probability programs will repeatedly appear in the results of program searches. An alternative would be marginalization via X-X. However, this requires more program runs and is therefore more computationally expensible."}, {"heading": "2.5.2 Engines comparison", "text": "Figure 2.7 shows a speed comparison between different probabilistic programming motors: Anglican (Tolpin et al., 2015b) 5, Interpreted Anglican (Wood et al., 2014) 6 and Probabilistic Scheme (Paige and Wood, 2014) Our previous work (Perov and Wood, 2014; Perov, 2014) was performed in Interpreted Anglican (Wood et al., 2014). Interpreted Anglican engine is written in Clojure and interprets Anglican code. Anglican engine is written in and integrated with Clojure. It deals with control structures and translates them into Clojure code. Thus, Clojure is a Compilistic target for Anglican. Probabilistic Scheme engine is based on Scheme compiler \"Stalin\" 7, which is written in C, with included Probabilistic C (Paige and Wood, 2014) library. Probabilistic program Scheme code is also a Compilistic target for Probabilistic Scheme compiler. \""}, {"heading": "2.5.3 Learning sampler code", "text": "Given the improvement in speed, we were able to reproduce the initial experiments much faster and report the results with better accuracy. In particular, in Figure 2.8 we show the results of the learning sampler program code for six common one-dimensional distributions Bernoulli (p), Poisson (\u03bb), Gamma (a, 1.0), Beta (a, 1), Normal (0, 1), Normal (\u00b5, \u03c3). As before, we marginalized using the parameter space with a small randomly composed set of \u03bb1,.., \u03bbS. Figure 2.9 shows repeated experiments on learning independent onedimensional samplers aimed at matching any one-dimensional real-world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013)."}, {"heading": "2.5.4 Example of learning a standard Normal sampler", "text": "We illustrate the process of learning a sampler for the standard normal distribution by providing a few examples of probabilistic program text from the Markov chain, which is aimed at the rear end of such a sampler. In Figure 2.10 we provide examples, which are generated by evaluating these probabilistic programs. This section provides program text for each corresponding sub-plot in the order: from left to right, from top to bottom. One of the first derived programs, from the back via the program text, is a program that always delivers a deterrent 0.0. This very short program text has a very high probability in view of the production rules, and it corresponds exactly to the first moment, i.e. the mean: 1 (lambda (stack _ level) 0.0 next reasonable approximations, from the Markov chain, are probabilistic programs, the sample from a uniform continuous distribution with fixed boundaries: 1 (lambda) lambda-safe c-safe c-safe c-safe c-level (lambda-safe c uack) c uc c c uc c-safe c c-lamuda-level a (a-safe c-level-2.0) a-safe a-a-a-lamuack (a-safe) a-level a-a-a-a-a-safe a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-( lambda-safe) (lambda-level-a-a-a-a-a-a-a-a-safe-a-a-a-a-a-a-a-a-a-a-a-a-safe-a-a-a-a-a-a-a-a-a-a-a-a-a-level-a-a-a-a-a-a)."}, {"heading": "2.6 Outline of a motivating example for the scientific re-", "text": "Discovery in Classical Genetics Our approach allows us to automatically infer from generative models. A motivation for this conclusion is to induce laws of nature. As an example, we outline a series of experiments aimed at a scientific rediscovery of the laws of classical genetics, which could be induced in the form of probability programs. The proposed experiments are designed to follow Mendel's original experiments (Mendel, 1985). In this work, we merely outline the potential number of experiments and leave the experiments themselves as potential future work of ourselves or others."}, {"heading": "2.6.1 Aims of the proposing experiment", "text": "We aim to induce three laws of classical genetics in the form of probability programs (Wikipedia, 2015): 1. Law of segregation: (a) \"an individual contains for each particular trait a pair of alleles that separate or separate for this trait during cell division,\" (b) and \"each parent passes a randomly selected copy (allele) to his offspring.\" 2. Law of dominance: \"There is an idea of dominant and recessive alleles, and a dominant allele is always masked by a dominant allele.\" 3. Law of independent sorting: \"separate genes for separate traits are passed independently from parent to offspring.\""}, {"heading": "2.6.2 Brief introduction to Mendel\u2019s work", "text": "He believes that the people who are able to survive themselves are able to survive themselves by going in search of themselves. He believes that the people who are able to survive themselves are not able to survive themselves. He believes that the people who are able to survive themselves are not able to survive themselves. He believes that the people who are able to survive themselves are not able to survive themselves. He believes that the people who are able to survive themselves are not able to survive themselves. He believes that the people who are able to survive themselves are not able to survive themselves. He believes that the people who are able to survive themselves are not able to survive themselves."}, {"heading": "2.6.3 Abstractions to induce", "text": "The aim of the experiment is to set in motion three probabilistic procedures which represent (a) an abstraction of a plant, (b) an abstraction of an observable characteristic of a plant, and (c) an abstraction of the hybridization of plants. In the following, they are explained in more detail and briefly described how these abstractions can be interpreted together as the three laws of Mendel's genetics."}, {"heading": "2.6.4 Inducing a probabilistic procedure", "text": "For the purpose of generating a probabilistic program code, we refer to an abstract procedure sample-probabilistic-procedure that randomly generates a probabilistic program by sampling from any grammar via program text that is similar to the one described in Section 2.4. This procedure takes two arguments: 1. The list of input signatures of the inductive probabilistic procedure. 2. \"The output signature of the inductive probabilistic procedure.\" If, for example, we want to induce the program code of a probabilistic procedure that takes three real values and returns the list of two lists that both contain the pair of integers, \"we must call the procedure sample-probabilistic-procedure as follows: 42 (\" we define a new program (sample-probabilistic int-procedure) (listen \"real\" real \"sample) c (list),\" (list) (list \"int\" int \"int\" int \"int) (list),\" list safe, \"(\" list \"list\" list \"sample\" int \"int\" int), \"(\" list \"real,\") (\"safe,\" (\"list\" list \"list\"): 42 (\"(\") a new program (sample-probabilistic int-int-procedure) int-int (((\"int) int),\" (c) real (list, \"safe,\" (list \"),\" (list (\"safe),\" (list (\"list),\" safe, \"(\" list \"(\" list \"list\" safe, \"list\")."}, {"heading": "2.6.5 An abstraction of an individual plant", "text": "First of all, we assume that a single plant is only a few data. In other words, the assumption is that to describe a real natural object it is sufficient to represent that object in the form of finite data. We have to create a product-random-individual that produces a random single plant (like a factory), this process does not need arguments and provides an unknown, but a specific data structure. It is specific because we assume that all our plants and their offspring belong to a single species, even if they may belong to different varieties of that species: 43 (define produce-random-individual (induce-probabilistic process' (); no inputs. 'any; arbitrary, but fixed output signature.)))))) By calling the process produce-random-individual, we try indigenous plants: (define plant-1 (produce-random-individual)) (define plant-2 (produce-randomly-life cycle), before we actually call it an abstraction of a plant, including its entire life cycle)."}, {"heading": "2.6.6 An abstraction of an observable plant feature", "text": "Secondly, we need a probabilistic process trait that scans an observable trait based on the data of a single plant. This process should take an individual and return a trait. Following Mendel's work, we work with highly distinct binary characteristics (such as the color being only violet or white), which means that we can assume that the output signature of the inductive procedure is Boolean: 44 (define the individual data signature (create the output signature randomly-individually)) (define the get characteristic (generate the inductive probabilistic procedure (list the individual data signature)); an input:; the individual data. 'bool; The output is Boolean.; We rely on the fact that we work with; a highly differentiable trait that has no gradient:; it is just one or the other.) The output signature of the process is just a language function that gives back the language function."}, {"heading": "2.6.7 An abstraction of the plant hybridisation process", "text": "Thirdly, we need a probabilistic process that hybridizes plants so that a new plant emerges. We strive to induce two completely independent probabilistic processes: the process produce-hybrid-sexual and the process produce-hybrid-sexual takes as input two single plants and produces a new plant: (define produce-hybrid-sexual (induce-probabilistic process (list (output-signature produce-random-individual)) (output-signature produce-random-individual)) (output-signature produce-random-individual))))) Process produce-hybrid-sexual takes as input a single plant and produces a new one: 45\\ begin {lstlisting} (define produce-hybrid-asexually (list (output-signature produce-random-individual)))) (output-signature produce-random-individual)))))"}, {"heading": "2.6.8 Procedures that are expected to be learnt", "text": "Using our current understanding of classical genetics, we provide the source code for probabilistic programs in Figures 2.11, 2.12, 2.13 and 2.14 below, which one might expect to trigger the basic laws of classical genetics. That is, we provide the source code for four probabilistic programs: produce-random-individual, get-trait, produce-hybrid-sexual, produce-hybrid-asexual. For simplicity, we only treat one trait (define produce-random-individual (lambda) () (define get-feature (lambda (me) 46 (define produce-hybrid-sexual (lambda (pollen-parents)) (define produce-hybrid-asexual (lambda (parents) produce-hybrid-sexual-parent))))) Figure 2.14: Interpretation: \"asexual hybrid-sexual (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda)) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) (lambda) lambda) (lambda) (lambda) (lambda) (lambda) (lambda-lambda) (lambda) lambda) (lambda) (lambda) (lambda-lambda) (lambda) (lambda) lambda-lambda) (lambda) (lambda) (lambda) (lambda-lambda) (lambda) (lambda-lambda) (lambda) (lambda) (lambda-lambda) (lambda) (lamb"}, {"heading": "2.7 Conclusion", "text": "In this sense, it is also important that people are able to identify themselves and understand what they are doing. (...) It is not so that they are able to identify themselves. \"(...)\" It is not so that they do it. \"(...)\" It is so. \"(...)\" It is so. \"(...)\" It is so. \"(...)\" It is so. \"(...)\" \"It is so.\" \"\" It is so. \"(...)\" (...) \"(...)\" (\")\" (... \")\" (\")\" (... \")\" (\")\" (... \")\" (\")\" (\")\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()\" () \"()."}, {"heading": "3.1 Conclusion", "text": "This year, the time has come for it to be able to get to grips with the problems that have been mentioned."}, {"heading": "Appendix A", "text": "body of the sampler code7879808182"}, {"heading": "Appendix B", "text": "The DPMO code in AnglicanB.1 The DPMO code 1 (ns ddpmo.ddpmo 2 (: use [anglican emit runtime] 3 [anglib xrp utils new-dists] 4 ddpmo header) 5 (:: r.7) 3 (: r.7) 3 (: r.7) 3 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).7 (: 0).8 (.8).8 (.8).8 (.8).8 (.8).8 (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8) (.8).8) (.8) (.8) (.8).8) (.8) (.8) (.8) (.8) (.8) (.8).8).8 (.8) (.8) (.8) (.8).8) (.8).7 (.7).7 (.7).7 (.7).7).7 (: 7).7 (: 7).7 (: 7).7).7).7 (: 7).7 (: 7).7).7).7 (: 7).7 (: 7) (: 7).7).7) (: 7).7).7).7).7 (: 7).7 (: 7).7).7).7).7 (: 7).7).7) (: 7).7).7).7)"}], "references": [{"title": "UCI Machine Learning Repository", "author": ["K. Bache", "M. Lichman"], "venue": "http:// archive.ics.uci.edu/ml.", "citeRegEx": "Bache and Lichman,? 2013", "shortCiteRegEx": "Bache and Lichman", "year": 2013}, {"title": "A note on the generation of random normal deviates", "author": ["G.E. Box", "M.E. Muller"], "venue": "The Annals of Mathematical Statistics, 29(2):610\u2013611.", "citeRegEx": "Box and Muller,? 1958", "shortCiteRegEx": "Box and Muller", "year": 1958}, {"title": "Generalized P\u00f3lya urn for time-varying Dirichlet process mixtures", "author": ["F. Caron", "M. Davy", "A. Doucet"], "venue": "Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI 2007).", "citeRegEx": "Caron et al\\.,? 2007", "shortCiteRegEx": "Caron et al\\.", "year": 2007}, {"title": "Probabilistic programming concepts", "author": ["L. De Raedt", "A. Kimmig"], "venue": "arXiv eprint arXiv:1312.4328.", "citeRegEx": "Raedt and Kimmig,? 2013", "shortCiteRegEx": "Raedt and Kimmig", "year": 2013}, {"title": "Bootstrap learning via modular concept discovery", "author": ["E. Dechter", "J. Malmaud", "R.P. Adams", "J.B. Tenenbaum"], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013).", "citeRegEx": "Dechter et al\\.,? 2013", "shortCiteRegEx": "Dechter et al\\.", "year": 2013}, {"title": "Non-uniform random variate generation", "author": ["L. Devroye"], "venue": "Springer-Verlag.", "citeRegEx": "Devroye,? 1986", "shortCiteRegEx": "Devroye", "year": 1986}, {"title": "A semiautomatic system for ground truth generation of soccer video sequences", "author": ["T. D\u2019Orazio", "M. Leo", "N. Mosca", "P. Spagnolo", "P.L. Mazzeo"], "venue": "In Advanced Video and Signal Based Surveillance,", "citeRegEx": "D.Orazio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "D.Orazio et al\\.", "year": 2009}, {"title": "Probability: theory and examples", "author": ["R. Durrett"], "venue": "Cambridge University Press.", "citeRegEx": "Durrett,? 2010", "shortCiteRegEx": "Durrett", "year": 2010}, {"title": "Structure discovery in nonparametric regression through compositional kernel search", "author": ["D. Duvenaud", "J.R. Lloyd", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML 2013), pages 1166\u20131174.", "citeRegEx": "Duvenaud et al\\.,? 2013", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2013}, {"title": "Just-in-time learning for fast and flexible inference", "author": ["S.M.A. Eslami", "D. Tarlow", "P. Kohli", "J. Winn"], "venue": "Advances in Neural Information Processing Systems (NIPS 2014).", "citeRegEx": "Eslami et al\\.,? 2014", "shortCiteRegEx": "Eslami et al\\.", "year": 2014}, {"title": "A repository for generative models (composite authors, edited by andreas stuhlm\u00fcller)", "author": ["ForestDB"], "venue": "Available at http://forestdb.org/.", "citeRegEx": "ForestDB,? 2016", "shortCiteRegEx": "ForestDB", "year": 2016}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["Fortin", "F.-A.", "De Rainville", "F.-M.", "Gardner", "M.-A.", "M. Parizeau", "C. Gagn\u00e9"], "venue": "Journal of Machine Learning Research, 13:2171\u20132175.", "citeRegEx": "Fortin et al\\.,? 2012", "shortCiteRegEx": "Fortin et al\\.", "year": 2012}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI 2013), volume 99, pages 1300\u20131309.", "citeRegEx": "Friedman et al\\.,? 1999", "shortCiteRegEx": "Friedman et al\\.", "year": 1999}, {"title": "The principles and practice of probabilistic programming", "author": ["N.D. Goodman"], "venue": "Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201913, pages 399\u2013402, New York, NY, USA. ACM.", "citeRegEx": "Goodman,? 2013", "shortCiteRegEx": "Goodman", "year": 2013}, {"title": "Church: a language for generative models", "author": ["N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "Proceedings of the Twenty95", "citeRegEx": "Goodman et al\\.,? 2008", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Probabilistic programming", "author": ["A.D. Gordon", "T.A. Henzinger", "A.V. Nori", "S.K. Rajamani"], "venue": "Proceedings of the on Future of Software Engineering, pages 167\u2013 181. ACM.", "citeRegEx": "Gordon et al\\.,? 2014", "shortCiteRegEx": "Gordon et al\\.", "year": 2014}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N.J. Gordon", "D.J. Salmond", "A.F. Smith"], "venue": "IEE Proceedings F (Radar and Signal Processing), volume 140, pages 107\u2013113. IET.", "citeRegEx": "Gordon et al\\.,? 1993", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Exploiting compositionality to explore a large space of model structures", "author": ["R. Grosse", "R.R. Salakhutdinov", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML 2012).", "citeRegEx": "Grosse et al\\.,? 2012", "shortCiteRegEx": "Grosse et al\\.", "year": 2012}, {"title": "Neural adaptive sequential Monte Carlo", "author": ["S. Gu", "R.E. Turner", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems (NIPS 2015).", "citeRegEx": "Gu et al\\.,? 2015", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Approaches and Applications of Inductive Programming (Dagstuhl Seminar 13502)", "author": ["S. Gulwani", "E. Kitzelmann", "U. Schmid"], "venue": "Dagstuhl Reports, 3(12):43\u201366.", "citeRegEx": "Gulwani et al\\.,? 2014", "shortCiteRegEx": "Gulwani et al\\.", "year": 2014}, {"title": "Learning to pass expectation propagation messages", "author": ["N. Heess", "D. Tarlow", "J. Winn"], "venue": "Advances in Neural Information Processing Systems (NIPS 2013), pages 3219\u20133227.", "citeRegEx": "Heess et al\\.,? 2013", "shortCiteRegEx": "Heess et al\\.", "year": 2013}, {"title": "Incremental learning in inductive programming", "author": ["R. Henderson"], "venue": "Approaches and Applications of Inductive Programming, pages 74\u201392. Springer.", "citeRegEx": "Henderson,? 2010", "shortCiteRegEx": "Henderson", "year": 2010}, {"title": "Accelerating inference: towards a full language, compiler and hardware stack", "author": ["B."], "venue": "arXiv e-print arXiv:1212.2991.", "citeRegEx": "B.,? 2012", "shortCiteRegEx": "B.", "year": 2012}, {"title": "Notes on approximate bayesian computation (private correspondence)", "author": ["J. Huggins"], "venue": null, "citeRegEx": "Huggins,? \\Q2013\\E", "shortCiteRegEx": "Huggins", "year": 2013}, {"title": "Consensus message passing for layered graphical models", "author": ["V. Jampani", "S.M.A. Eslami", "D. Tarlow", "P. Kohli", "J. Winn"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS 2015).", "citeRegEx": "Jampani et al\\.,? 2015a", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "The informed sampler: A discriminative approach to Bayesian inference in generative computer vision models", "author": ["V. Jampani", "S. Nowozin", "M. Loper", "P.V. Gehler"], "venue": "Special Issue on Generative Models in Computer Vision, Computer Vision and Image Understanding, 136:32\u201344.", "citeRegEx": "Jampani et al\\.,? 2015b", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "Just-in-time kernel regression for expectation propagation", "author": ["W. Jitkrittum", "A. Gretton", "S.M.A. Eslami", "C.B. Lakshminarayanan", "D. Sejdinovic", "Z. Szab\u00f3"], "venue": "Large-Scale Kernel Learning: Challenges and New Opportunities workshop at International Conference on Machine Learning (ICML 2015).", "citeRegEx": "Jitkrittum et al\\.,? 2015", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "Adaptor grammars: A framework for specifying compositional nonparametric bayesian models", "author": ["M. Johnson", "T.L. Griffiths", "S. Goldwater"], "venue": "Advances in Neural Information Processing Systems (NIPS 2007), 19:641.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv e-print arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "An inductive logic programming approach to statistical relational learning", "author": ["K. Kersting"], "venue": "Proceedings of the Conference on An Inductive Logic Programming Approach to Statistical Relational Learning 2005, pages 1\u2013228. IOS Press.", "citeRegEx": "Kersting,? 2005", "shortCiteRegEx": "Kersting", "year": 2005}, {"title": "On the implementation of the probabilistic logic programming language ProbLog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V.S. Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming, 11(2-3):235\u2013262.", "citeRegEx": "Kimmig et al\\.,? 2011", "shortCiteRegEx": "Kimmig et al\\.", "year": 2011}, {"title": "Embedded probabilistic programming", "author": ["O. Kiselyov", "Shan", "C.-C."], "venue": "Domain-Specific Languages, pages 360\u2013384. Springer.", "citeRegEx": "Kiselyov et al\\.,? 2009", "shortCiteRegEx": "Kiselyov et al\\.", "year": 2009}, {"title": "The art of computer programming, volume 2: Seminumerical algorithms (3rd edition)", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "Knuth,? \\Q1998\\E", "shortCiteRegEx": "Knuth", "year": 1998}, {"title": "Genetic programming: on the programming of computers by means of natural selection, volume 1", "author": ["J.R. Koza"], "venue": "MIT Press.", "citeRegEx": "Koza,? 1992", "shortCiteRegEx": "Koza", "year": 1992}, {"title": "Fully automatic variational inference of differentiable probability models", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "NIPS Workshop on Probabilistic Programming 2014.", "citeRegEx": "Kucukelbir et al\\.,? 2014", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2014}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["T.D. Kulkarni", "P. Kohli", "J.B. Tenenbaum", "V. Mansinghka"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015, pages 4390\u20134399. 98", "citeRegEx": "Kulkarni et al\\.,? 2015", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "SSJ: A framework for stochastic simulation in Java", "author": ["P. L\u2019Ecuyer", "L. Meliani", "J. Vaucher"], "venue": "In Proceedings of the 2002 Winter Simulation Conference,", "citeRegEx": "L.Ecuyer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "L.Ecuyer et al\\.", "year": 2002}, {"title": "Learning programs: a hierarchical Bayesian approach", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pages 639\u2013646.", "citeRegEx": "Liang et al\\.,? 2010", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Bias reformulation for one-shot function induction", "author": ["D. Lin", "E. Dechter", "K. Ellis", "J.B. Tenenbaum", "S.H. Muggleton"], "venue": "Proceedings of the 23rd European conference on Artificial Intelligence (ECAI 2014), pages 525\u2013530.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Particle Gibbs with ancestor sampling", "author": ["F. Lindsten", "M.I. Jordan", "T.B. Sch\u00f6n"], "venue": "Journal of Machine Learning Research, 15(1):2145\u20132184.", "citeRegEx": "Lindsten et al\\.,? 2014", "shortCiteRegEx": "Lindsten et al\\.", "year": 2014}, {"title": "The BUGS project: evolution, critique and future directions", "author": ["D. Lunn", "D. Spiegelhalter", "A. Thomas", "N. Best"], "venue": "Statistics in Medicine, 28(25):3049.", "citeRegEx": "Lunn et al\\.,? 2009", "shortCiteRegEx": "Lunn et al\\.", "year": 2009}, {"title": "Structured priors for structure learning", "author": ["V. Mansinghka", "C. Kemp", "T. Griffiths", "J. Tenenbaum"], "venue": "arXiv e-print arXiv:1206.6852.", "citeRegEx": "Mansinghka et al\\.,? 2012", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2012}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V. Mansinghka", "D. Selsam", "Y. Perov"], "venue": "arXiv e-print arXiv:1404.0099.", "citeRegEx": "Mansinghka et al\\.,? 2014", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2014}, {"title": "Approximate Bayesian computational methods", "author": ["Marin", "J.-M.", "P. Pudlo", "C.P. Robert", "R.J. Ryder"], "venue": "Statistics and Computing, 22(6):1167\u20131180.", "citeRegEx": "Marin et al\\.,? 2012", "shortCiteRegEx": "Marin et al\\.", "year": 2012}, {"title": "Factorie: probabilistic programming", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Handbook of biological statistics, volume 2", "author": ["J.H. McDonald"], "venue": "Sparky House Publishing Baltimore, Maryland.", "citeRegEx": "McDonald,? 2009", "shortCiteRegEx": "McDonald", "year": 2009}, {"title": "Experiments in plant hybridization (in German)", "author": ["G. Mendel"], "venue": "Verhandlungen des naturforschenden Vereins Br\u00fcnn. http://www.mendelweb.org/Mendel. html (translated in 1996).", "citeRegEx": "Mendel,? 1985", "shortCiteRegEx": "Mendel", "year": 1985}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Statistical Relational Learning, page 373.", "citeRegEx": "Milch et al\\.,? 2007", "shortCiteRegEx": "Milch et al\\.", "year": 2007}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI 2001), pages 362\u2013369. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "Advances in Inductive Logic Programming, 32:254\u2013264.", "citeRegEx": "Muggleton,? 1996", "shortCiteRegEx": "Muggleton", "year": 1996}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT Press.", "citeRegEx": "Murphy,? 2012", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "The dependent Dirichlet process mixture of objects for detection-free tracking and object modeling", "author": ["W. Neiswanger", "F. Wood", "E. Xing"], "venue": "Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), pages 660\u2013668. 100", "citeRegEx": "Neiswanger et al\\.,? 2014", "shortCiteRegEx": "Neiswanger et al\\.", "year": 2014}, {"title": "Inductive functional programming using incremental program transformation", "author": ["R. Olsson"], "venue": "Artificial Intelligence, 74(1):55\u201381.", "citeRegEx": "Olsson,? 1995", "shortCiteRegEx": "Olsson", "year": 1995}, {"title": "A compilation target for probabilistic programming languages", "author": ["B. Paige", "F. Wood"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Paige and Wood,? 2014", "shortCiteRegEx": "Paige and Wood", "year": 2014}, {"title": "Generative probabilistic programming (in Russian)", "author": ["Y. Perov"], "venue": "B.S. Thesis, Department of Mathematics and Computer Science, Siberian Federal University.", "citeRegEx": "Perov,? 2014", "shortCiteRegEx": "Perov", "year": 2014}, {"title": "Efficient, envelope-based multicore Markov chain inference for Church", "author": ["Y. Perov", "V. Mansinghka"], "venue": "NIPS Workshop on Probabilistic Programming 2012.", "citeRegEx": "Perov and Mansinghka,? 2012", "shortCiteRegEx": "Perov and Mansinghka", "year": 2012}, {"title": "Learning probabilistic programs", "author": ["Y. Perov", "F. Wood"], "venue": "arXiv e-print arXiv:1407.2646.", "citeRegEx": "Perov and Wood,? 2014", "shortCiteRegEx": "Perov and Wood", "year": 2014}, {"title": "IBAL: a probabilistic rational programming language", "author": ["A. Pfeffer"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI 2001), pages 733\u2013740. Citeseer.", "citeRegEx": "Pfeffer,? 2001", "shortCiteRegEx": "Pfeffer", "year": 2001}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu.", "citeRegEx": "Poli et al\\.,? 2008", "shortCiteRegEx": "Poli et al\\.", "year": 2008}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International journal of Man-Machine Studies, 27(3):221\u2013234.", "citeRegEx": "Quinlan,? 1987", "shortCiteRegEx": "Quinlan", "year": 1987}, {"title": "Probabilistic inductive logic programming \u2013 theory and applications, volume", "author": ["L.D. Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Improving inference performance in probabilistic programming", "author": ["R. Ranca"], "venue": null, "citeRegEx": "Ranca,? \\Q2014\\E", "shortCiteRegEx": "Ranca", "year": 2014}, {"title": "Slice sampling for probabilistic programming", "author": ["R. Cambridge. Ranca", "Z. Ghahramani"], "venue": "guages. Master\u2019s thesis, Department of Engineering,", "citeRegEx": "Ranca and Ghahramani,? \\Q2015\\E", "shortCiteRegEx": "Ranca and Ghahramani", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["S. Reed", "N. de Freitas"], "venue": null, "citeRegEx": "Reed and Freitas,? \\Q2016\\E", "shortCiteRegEx": "Reed and Freitas", "year": 2016}, {"title": "Bayesianly justifiable and relevant frequency calculations for the applied statistician", "author": ["Rubin", "D. B"], "venue": "The Annals of Statistics, 12(4):1151\u20131172.", "citeRegEx": "Rubin and B,? 1984", "shortCiteRegEx": "Rubin and B", "year": 1984}, {"title": "Induction of recursive program schemes", "author": ["U. Schmid", "F. Wysotzki"], "venue": "Machine Learning: ECML-98, pages 214\u2013225. Springer.", "citeRegEx": "Schmid and Wysotzki,? 1998", "shortCiteRegEx": "Schmid and Wysotzki", "year": 1998}, {"title": "Density estimation for statistics and data analysis, volume 26", "author": ["B.W. Silverman"], "venue": "CRC Press.", "citeRegEx": "Silverman,? 1986", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Sequential Monte Carlo methods in practice", "author": ["A. Smith", "A. Doucet", "N. de Freitas", "N. Gordon"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Modeling Cognition with Probabilistic Programs: Representations and Algorithms", "author": ["A. Stuhlm\u00fcller"], "venue": "PhD thesis, Massachusetts Institute of Technology and Stanford University.", "citeRegEx": "Stuhlm\u00fcller,? 2015", "shortCiteRegEx": "Stuhlm\u00fcller", "year": 2015}, {"title": "Biips: Software for Bayesian inference with interacting particle systems", "author": ["A. Todeschini", "F. Caron", "M. Fuentes", "P. Legrand", "P. Del Moral"], "venue": "arXiv e-print arXiv:1412.3779.", "citeRegEx": "Todeschini et al\\.,? 2014", "shortCiteRegEx": "Todeschini et al\\.", "year": 2014}, {"title": "Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs", "author": ["D. Tolpin", "J.W. van de Meent", "Paige", "F. Brooks Wood"], "venue": "In ECML PKDD", "citeRegEx": "Tolpin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tolpin et al\\.", "year": 2015}, {"title": "Probabilistic programming in Anglican", "author": ["D. Tolpin", "J.W. van de Meent", "F. Wood"], "venue": "In Machine Learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "Tolpin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tolpin et al\\.", "year": 2015}, {"title": "Maximum a posteriori estimation by search in probabilistic programs", "author": ["D. Tolpin", "F. Wood"], "venue": "Eighth Annual Symposium on Combinatorial Search.", "citeRegEx": "Tolpin and Wood,? 2015", "shortCiteRegEx": "Tolpin and Wood", "year": 2015}, {"title": "Image segmentation by data-driven Markov chain Monte Carlo", "author": ["Z. Tu", "Zhu", "S.-C."], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(5):657\u2013 673.", "citeRegEx": "Tu et al\\.,? 2002", "shortCiteRegEx": "Tu et al\\.", "year": 2002}, {"title": "Particle Gibbs with ancestor sampling for probabilistic programs", "author": ["J.W. van de Meent", "H. Yang", "V. Mansinghka", "F. Wood"], "venue": "arXiv e-print arXiv:1501.06769", "citeRegEx": "Meent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meent et al\\.", "year": 2015}, {"title": "Mendelian inheritance\u201d \u2014 Wikipedia, the free encyclopedia", "author": ["Wikipedia"], "venue": "https://en.wikipedia.org/wiki/Mendelian_inheritance.", "citeRegEx": "Wikipedia,? 2015", "shortCiteRegEx": "Wikipedia", "year": 2015}, {"title": "Approximate Bayesian computation (ABC) gives exact re103", "author": ["R.D. Wilkinson"], "venue": null, "citeRegEx": "Wilkinson,? \\Q2013\\E", "shortCiteRegEx": "Wilkinson", "year": 2013}, {"title": "Automated variational inference in probabilistic programming", "author": ["D. Wingate", "T. Weber"], "venue": "arXiv e-print arXiv:1301.1299.", "citeRegEx": "Wingate and Weber,? 2013", "shortCiteRegEx": "Wingate and Weber", "year": 2013}, {"title": "A new approach to probabilistic programming inference", "author": ["F. Wood", "J.W. van de Meent", "V. Mansinghka"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (ICML", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}, {"title": "Generating efficient MCMC kernels from probabilistic programs", "author": ["L. Yang", "P. Hanrahan", "N.D. Goodman"], "venue": "Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), pages 1068\u20131076. 104", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 56, "context": "The initial description of the approach and initial experimental results on learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014), and in my Bachelor\u2019s thesis (Perov, 2014).", "startOffset": 146, "endOffset": 168}, {"referenceID": 54, "context": "The initial description of the approach and initial experimental results on learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014), and in my Bachelor\u2019s thesis (Perov, 2014).", "startOffset": 198, "endOffset": 211}, {"referenceID": 36, "context": "For initial experiments, a framework for Stochastic Simulation in Java (L\u2019Ecuyer et al., 2002) was usefully employed.", "startOffset": 71, "endOffset": 94}, {"referenceID": 13, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 15, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 61, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 78, "context": "The probability of an execution trace can be defined, in a similar but more restrictive way to (Wood et al., 2014), as p(y,x) \u2261Nn=1 p(yn|\u03b6tn ,xn)p(xn|xn\u22121), where yn is the n-th output data point (i.", "startOffset": 95, "endOffset": 114}, {"referenceID": 14, "context": "Particular languages and implementations include functional probabilistic programming languages such as Church (Goodman et al., 2008), Anglican (Wood et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 78, "context": ", 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 42, "context": ", 2014) and Venture (Mansinghka et al., 2014); logic probabilistic programming languages (De Raedt and Kimmig, 2013) such as ProbLog (Kimmig et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 30, "context": ", 2014); logic probabilistic programming languages (De Raedt and Kimmig, 2013) such as ProbLog (Kimmig et al., 2011); and domain-specific PPLs, such as (Kiselyov and Shan, 2009).", "startOffset": 95, "endOffset": 116}, {"referenceID": 57, "context": "Other languages and implementations also endorse declarative definitions of probabilistic models and include IBAL (Pfeffer, 2001),", "startOffset": 114, "endOffset": 129}, {"referenceID": 47, "context": "Stan (Stan Development Team, 2014), BLOG (Milch et al., 2007), BUGS (Lunn et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 40, "context": ", 2007), BUGS (Lunn et al., 2009), FACTORIE (McCallum et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 44, "context": ", 2009), FACTORIE (McCallum et al., 2009), Markov Logic networks (Richardson and Domingos, 2006), and Infer.", "startOffset": 18, "endOffset": 41}, {"referenceID": 15, "context": "More detailed overviews are given in (Roy, 2016; De Raedt and Kimmig, 2013; Gordon et al., 2014; Mansinghka et al., 2014).", "startOffset": 37, "endOffset": 121}, {"referenceID": 42, "context": "More detailed overviews are given in (Roy, 2016; De Raedt and Kimmig, 2013; Gordon et al., 2014; Mansinghka et al., 2014).", "startOffset": 37, "endOffset": 121}, {"referenceID": 47, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 14, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 42, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 40, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 47, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 78, "context": ", 2007), sequential Monte Carlo (Wood et al., 2014), Hamiltonian Monte Carlo (Stan Development Team, 2014), variational inference (Mansinghka et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 42, "context": ", 2014), Hamiltonian Monte Carlo (Stan Development Team, 2014), variational inference (Mansinghka et al., 2014), belief propagation (Hershey et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 48, "context": "NET probabilistic program into a finite graphical model and an application of expectation propagation inference method (Minka, 2001), which seriously restricts the range of models that may be written in it.", "startOffset": 119, "endOffset": 132}, {"referenceID": 61, "context": "On the other hand, while languages like Church, Anglican and Venture are some of the most flexible and expressive (Ranca, 2014), their statistical inference performance is slower by at least the factor of 10x in comparison to languages like Infer.", "startOffset": 114, "endOffset": 127}, {"referenceID": 67, "context": "For example, two new probabilistic programming languages have recently been introduced, employing sequential Monte Carlo methods (Smith et al., 2013): Anglican (Wood et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 78, "context": ", 2013): Anglican (Wood et al., 2014) and Biips (Todeschini et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 69, "context": ", 2014) and Biips (Todeschini et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 39, "context": "In 2014 a general-purpose implementation of the particle Gibbs with ancestor sampling method (Lindsten et al., 2014) was introduced in (van de Meent et al.", "startOffset": 93, "endOffset": 116}, {"referenceID": 77, "context": "Variational inference has been employed in probabilistic programming since 2013 in Stochastic MATLAB (Wingate and Weber, 2013), Venture (Mansinghka et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 42, "context": "Variational inference has been employed in probabilistic programming since 2013 in Stochastic MATLAB (Wingate and Weber, 2013), Venture (Mansinghka et al., 2014) and Stan (Kucukelbir et al.", "startOffset": 136, "endOffset": 161}, {"referenceID": 34, "context": ", 2014) and Stan (Kucukelbir et al., 2014).", "startOffset": 17, "endOffset": 42}, {"referenceID": 62, "context": "Finally, slice sampling for probabilistic programming has been proposed in (Ranca and Ghahramani, 2015).", "startOffset": 75, "endOffset": 103}, {"referenceID": 72, "context": "In particular, an approximation search algorithm for maximum a posteriori probability estimation has been presented in (Tolpin and Wood, 2015).", "startOffset": 119, "endOffset": 142}, {"referenceID": 42, "context": "the implementation of Venture in C++ (Mansinghka et al., 2014) works faster than its very early prototype implementation in Clojure (Perov and Mansinghka, 2012)), by an intermediate compilation instead of a continuous interpretation (e.", "startOffset": 37, "endOffset": 62}, {"referenceID": 55, "context": ", 2014) works faster than its very early prototype implementation in Clojure (Perov and Mansinghka, 2012)), by an intermediate compilation instead of a continuous interpretation (e.", "startOffset": 77, "endOffset": 105}, {"referenceID": 78, "context": ", 2015b), with a program compilation into a Clojure function, works faster than the previous Anglican interpreter (Wood et al., 2014)), and by an utilisation of just-in-time compilation (examples include engines, described in (Perov and Mansinghka, 2012; Tolpin et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 55, "context": ", 2014)), and by an utilisation of just-in-time compilation (examples include engines, described in (Perov and Mansinghka, 2012; Tolpin et al., 2015b), which are implemented in Clojure).", "startOffset": 100, "endOffset": 150}, {"referenceID": 53, "context": "Another related work worth mentioning is \u201cProbabilistic C\u201d (Paige and Wood, 2014), where authors present a C library that allows sequential Monte Carlo and Particle Gibbs inference in any C and C++ program with just two added C functions, OBSERVE and PREDICT, to condition executions and get particle smoothing predictions correspondingly.", "startOffset": 59, "endOffset": 81}, {"referenceID": 55, "context": "by exploring conditional dependencies and making incremental updates only on a part of the execution trace, as in Venture (Perov and Mansinghka, 2012; Mansinghka et al., 2014) and Shred (Yang et al.", "startOffset": 122, "endOffset": 175}, {"referenceID": 42, "context": "by exploring conditional dependencies and making incremental updates only on a part of the execution trace, as in Venture (Perov and Mansinghka, 2012; Mansinghka et al., 2014) and Shred (Yang et al.", "startOffset": 122, "endOffset": 175}, {"referenceID": 79, "context": ", 2014) and Shred (Yang et al., 2014), where the latter is a tracing interpreter for Church language.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "For example, while in old implementations of Church (Goodman et al., 2008) N sweeps1 of Metropolis-Hastings inference in a hidden Markov model with T 1A sweep, in the context of doing Metropolis-Hastings inference on the probabilistic program with T random choices, consists of T local MH proposals on those random choices.", "startOffset": 52, "endOffset": 74}, {"referenceID": 20, "context": "NET have included neural networks (Heess et al., 2013), just-in-time random forests (Eslami et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 9, "context": ", 2013), just-in-time random forests (Eslami et al., 2014) and just-in-time kernel-based regression (Jitkrittum et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": ", 2014) and just-in-time kernel-based regression (Jitkrittum et al., 2015).", "startOffset": 49, "endOffset": 74}, {"referenceID": 24, "context": "NET for graphical models with many layers, which are often used in computer vision, a consensus message passing method has been proposed in (Jampani et al., 2015a).", "startOffset": 140, "endOffset": 163}, {"referenceID": 51, "context": "The approach is illustrated by experiments on the existing Bayesian generative nonparametric model, \u201cthe Dependent Dirichlet Process Mixture of Objects\u201d (Neiswanger et al., 2014).", "startOffset": 153, "endOffset": 178}, {"referenceID": 43, "context": "To assess whether the distribution of samples generated by a program candidate matches the given distribution of interest, we use approximate Bayesian computation methods (Marin et al., 2012).", "startOffset": 171, "endOffset": 191}, {"referenceID": 56, "context": "This argument is supported by the fact that we were able to successfully learn an exact sampler for the Bernoulli distribution family (Perov and Wood, 2014; Perov, 2014), given only an adaptor grammar-based prior learnt from a corpus of sampler code that did not include Bernoulli sampler code.", "startOffset": 134, "endOffset": 169}, {"referenceID": 54, "context": "This argument is supported by the fact that we were able to successfully learn an exact sampler for the Bernoulli distribution family (Perov and Wood, 2014; Perov, 2014), given only an adaptor grammar-based prior learnt from a corpus of sampler code that did not include Bernoulli sampler code.", "startOffset": 134, "endOffset": 169}, {"referenceID": 33, "context": "Finally, our approach holds its own in comparison to state-of-the-art genetic programming methods (Koza, 1992; Poli et al., 2008).", "startOffset": 98, "endOffset": 129}, {"referenceID": 58, "context": "Finally, our approach holds its own in comparison to state-of-the-art genetic programming methods (Koza, 1992; Poli et al., 2008).", "startOffset": 98, "endOffset": 129}, {"referenceID": 19, "context": "One recent overview of automatic programming is presented in (Gulwani et al., 2014) and its references.", "startOffset": 61, "endOffset": 83}, {"referenceID": 49, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 29, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 60, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 38, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 33, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 34, "endOffset": 46}, {"referenceID": 52, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 72, "endOffset": 86}, {"referenceID": 65, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 115, "endOffset": 142}, {"referenceID": 37, "context": "Our approach is similar to the work on learning programs using a hierarchical Bayesian prior (Liang et al., 2010).", "startOffset": 93, "endOffset": 113}, {"referenceID": 66, "context": "The approach we describe in this chapter is related to density estimation (Silverman, 1986), which concerns the estimation of an unobservable probability density function given some observed data.", "startOffset": 74, "endOffset": 91}, {"referenceID": 12, "context": "Our approach is also related to probabilistic model learning, for example to learning probabilistic relational models (Friedman et al., 1999) and Bayesian network structure (Mansinghka et al.", "startOffset": 118, "endOffset": 141}, {"referenceID": 41, "context": ", 1999) and Bayesian network structure (Mansinghka et al., 2012).", "startOffset": 39, "endOffset": 64}, {"referenceID": 17, "context": "Also worthwhile of mentioning are recent works in search over generative probabilistic model structures (Grosse et al., 2012) and kernel compositions (Duvenaud et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": ", 2012) and kernel compositions (Duvenaud et al., 2013).", "startOffset": 32, "endOffset": 55}, {"referenceID": 43, "context": ") Before describing further details of our approach, we provide a brief outline of approximate Bayesian computation that is based on (Marin et al., 2012) and uses algorithms and equations there contained1.", "startOffset": 133, "endOffset": 153}, {"referenceID": 23, "context": "This means that we 1In addition, notes (Huggins, 2013) from Jonathan Huggins were quite helpful.", "startOffset": 39, "endOffset": 54}, {"referenceID": 43, "context": "Approximate Bayesian computation (ABC) framework (Marin et al., 2012) is different from the standard setup of Bayesian inference due to the impossibility (or intractability) to precisely calculate the likelihood p(y|\u03b8), even if we can sample from p(\u00b7|\u03b8).", "startOffset": 49, "endOffset": 69}, {"referenceID": 43, "context": "Algorithm 1 Likelihood-free rejection sampler (Marin et al., 2012) for i = 1 to N do repeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until z = y set \u03b8i = \u03b8\u2032, end for", "startOffset": 46, "endOffset": 66}, {"referenceID": 43, "context": "Algorithm 2 Likelihood-free approximate rejection sampler (Marin et al., 2012) for i = 1 to N do repeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until \u03c1{\u03b7(z), \u03b7(y)} \u2264 set \u03b8i = \u03b8\u2032, zi = z\u2032, end for", "startOffset": 58, "endOffset": 78}, {"referenceID": 43, "context": "The above algorithm samples from the joint distribution (Marin et al., 2012)", "startOffset": 56, "endOffset": 76}, {"referenceID": 76, "context": "Another concept in ABC, that is essential for our work, is the proposal of \u201cnoisy ABC\u201d, made by Wilkinson in (Wilkinson, 2013).", "startOffset": 109, "endOffset": 126}, {"referenceID": 43, "context": "The joint ABC target distribution becomes as follows (Marin et al., 2012):", "startOffset": 53, "endOffset": 73}, {"referenceID": 76, "context": "The important point, which was made by Wilkinson himself in (Wilkinson, 2013), is that if the model already includes the error as part of its generative model (i.", "startOffset": 60, "endOffset": 77}, {"referenceID": 7, "context": "One of counterexamples may be found in (Durrett, 2010).", "startOffset": 39, "endOffset": 54}, {"referenceID": 45, "context": "2 shows pseudocode that aims to find a sampler for the Bernoulli distribution family parametrised by \u03bb with the help of G-test statistic (McDonald, 2009):", "startOffset": 137, "endOffset": 153}, {"referenceID": 56, "context": "We used grammar prior that is similar to one that was introduced in our preceding work (Perov and Wood, 2014; Perov, 2014).", "startOffset": 87, "endOffset": 122}, {"referenceID": 54, "context": "We used grammar prior that is similar to one that was introduced in our preceding work (Perov and Wood, 2014; Perov, 2014).", "startOffset": 87, "endOffset": 122}, {"referenceID": 27, "context": "It complements the adaptor grammar (Johnson et al., 2007) prior that is used in (Liang et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 37, "context": ", 2007) prior that is used in (Liang et al., 2010) by the use of local environments4 and type signatures.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 1, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 32, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 54, "context": "The initial experiments had been described in our prior work (Perov, 2014; Perov and Wood, 2014).", "startOffset": 61, "endOffset": 96}, {"referenceID": 56, "context": "The initial experiments had been described in our prior work (Perov, 2014; Perov and Wood, 2014).", "startOffset": 61, "endOffset": 96}, {"referenceID": 54, "context": "3, we report new experimental results, similar to (Perov, 2014; Perov and Wood, 2014), but with thinner binning.", "startOffset": 50, "endOffset": 85}, {"referenceID": 56, "context": "3, we report new experimental results, similar to (Perov, 2014; Perov and Wood, 2014), but with thinner binning.", "startOffset": 50, "endOffset": 85}, {"referenceID": 33, "context": "Our approach was evaluated against genetic programming (Koza, 1992), one of stateof-the-art methods to search in the space of programs.", "startOffset": 55, "endOffset": 67}, {"referenceID": 58, "context": "For a recent introduction into the field of genetic programming, see (Poli et al., 2008).", "startOffset": 69, "endOffset": 88}, {"referenceID": 11, "context": "4, was reproduced in the evolutionary computation framework DEAP (Fortin et al., 2012) written in Python.", "startOffset": 65, "endOffset": 86}, {"referenceID": 78, "context": ", 2015b)5, Interpreted Anglican (Wood et al., 2014)6 and Probabilistic Scheme (Paige and Wood, 2014).", "startOffset": 32, "endOffset": 51}, {"referenceID": 53, "context": ", 2014)6 and Probabilistic Scheme (Paige and Wood, 2014).", "startOffset": 34, "endOffset": 56}, {"referenceID": 56, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al.", "startOffset": 15, "endOffset": 50}, {"referenceID": 54, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al.", "startOffset": 15, "endOffset": 50}, {"referenceID": 78, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al., 2014).", "startOffset": 89, "endOffset": 108}, {"referenceID": 53, "context": "Probabilistic Scheme engine is based on Scheme compiler \u201cStalin\u201d 7, which is written in C, with included Probabilistic C (Paige and Wood, 2014) library.", "startOffset": 121, "endOffset": 143}, {"referenceID": 59, "context": "9 shows repeated experiments for learning independent onedimensional samplers that aim to match arbitrary one-dimensional real world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013).", "startOffset": 180, "endOffset": 220}, {"referenceID": 0, "context": "9 shows repeated experiments for learning independent onedimensional samplers that aim to match arbitrary one-dimensional real world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013).", "startOffset": 180, "endOffset": 220}, {"referenceID": 46, "context": "The proposed experiments are designed to follow Mendel\u2019s original experiments (Mendel, 1985).", "startOffset": 78, "endOffset": 92}, {"referenceID": 75, "context": "We aim to induce three laws of classical genetics in the form of probabilistic programs (Wikipedia, 2015):", "startOffset": 88, "endOffset": 105}, {"referenceID": 46, "context": "presented his results in his classic paper \u201cExperiments in Plant Hybridisation\u201d (1865) (Mendel, 1985).", "startOffset": 87, "endOffset": 101}, {"referenceID": 22, "context": "presented his results in his classic paper \u201cExperiments in Plant Hybridisation\u201d (1865) (Mendel, 1985).", "startOffset": 67, "endOffset": 87}, {"referenceID": 21, "context": "Better inference techniques and hierarchical/cumulative learning methods (Henderson, 2010; Dechter et al., 2013) are essential to finding more complex probabilistic programs, including human-interpretable programs as in Appendix A that are theoretically in the prior of our grammar but were not identified during our experiments.", "startOffset": 73, "endOffset": 112}, {"referenceID": 4, "context": "Better inference techniques and hierarchical/cumulative learning methods (Henderson, 2010; Dechter et al., 2013) are essential to finding more complex probabilistic programs, including human-interpretable programs as in Appendix A that are theoretically in the prior of our grammar but were not identified during our experiments.", "startOffset": 73, "endOffset": 112}, {"referenceID": 68, "context": "to automatically induce problem-specific generative models that are similar to, for example, hidden Markov or latent Dirichlet allocation models; or even more sophisticated probabilistic problems that aim to describe our world and agents in it, for example as in (Stuhlm\u00fcller, 2015) and in (ForestDB, 2016)).", "startOffset": 263, "endOffset": 282}, {"referenceID": 10, "context": "to automatically induce problem-specific generative models that are similar to, for example, hidden Markov or latent Dirichlet allocation models; or even more sophisticated probabilistic problems that aim to describe our world and agents in it, for example as in (Stuhlm\u00fcller, 2015) and in (ForestDB, 2016)).", "startOffset": 290, "endOffset": 306}, {"referenceID": 28, "context": "as in (Karpathy et al., 2015), and potentially in combination with (Reed and de Freitas, 2016)), similar to ones that are described in the following Chapter.", "startOffset": 6, "endOffset": 29}, {"referenceID": 50, "context": "This is known as a \u201cgenerate and test\u201d approach (Murphy, 2012) since we just sample values xt from the generative model and only then evaluate how good they fit a data point yt.", "startOffset": 48, "endOffset": 62}, {"referenceID": 16, "context": "Another name for this approach is \u201cbootstrap particle filter\u201d (Gordon et al., 1993).", "startOffset": 62, "endOffset": 83}, {"referenceID": 50, "context": "This is the optimal proposal because for any given xt\u22121 the new weight w s t will have the same value independently of the value of xt (Murphy, 2012).", "startOffset": 135, "endOffset": 149}, {"referenceID": 51, "context": "For our further experiments we chose a dependent Dirichlet Process mixture of objects (DDPMO) model (Neiswanger et al., 2014).", "startOffset": 100, "endOffset": 125}, {"referenceID": 2, "context": "The model is based on a generalised P\u00f3lya urn (GPU) for timevarying Dirichlet process mixtures (Caron et al., 2007).", "startOffset": 95, "endOffset": 115}, {"referenceID": 51, "context": "The generative process of the DDPMO is described in (Neiswanger et al., 2014).", "startOffset": 52, "endOffset": 77}, {"referenceID": 51, "context": "The comparison of the object recognition and tracking performance of Bayesian statistical inference in the DDPMO model against the performance of some others state-of-the-art models and methods (not necessarily Bayesian) is also provided in (Neiswanger et al., 2014).", "startOffset": 241, "endOffset": 266}, {"referenceID": 51, "context": "The DDPMO model, as presented in (Neiswanger et al., 2014), uses conjugate priors.", "startOffset": 33, "endOffset": 58}, {"referenceID": 14, "context": "Conjugate priors may be implemented in languages like Church (Goodman et al., 2008), Anglican (Wood et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 78, "context": ", 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 42, "context": ", 2014) and Venture (Mansinghka et al., 2014) in the form of exchangeable random procedures (XRPs).", "startOffset": 20, "endOffset": 45}, {"referenceID": 6, "context": "For our experiments, we chose a soccer video dataset (D\u2019Orazio et al., 2009), for which there already exists a human-authored ground truth.", "startOffset": 53, "endOffset": 76}, {"referenceID": 25, "context": "The work on using discriminative proposals for Markov Chain Monte Carlo in parametric generative models include (Tu and Zhu, 2002) and (Jampani et al., 2015b), with applications in computer vision.", "startOffset": 135, "endOffset": 158}, {"referenceID": 18, "context": "Recent work with sequential Monte Carlo includes neural adaptive SMC (Gu et al., 2015), where authors also adapt proposals by descending the inclusive Kullback-Leibler divergence between the proposal and the true posterior distributions on hidden variables given observations.", "startOffset": 69, "endOffset": 86}, {"referenceID": 35, "context": "Another related recent work is a new probabilistic programming language called Picture (Kulkarni et al., 2015), for which authors propose and describe the use of data-driven proposals in the context of models for computer vision.", "startOffset": 87, "endOffset": 110}], "year": 2016, "abstractText": "This thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking. We begin this work by providing a brief introduction to probabilistic programming. In the second Chapter we present an approach to automatic discovery of samplers in the form of probabilistic programs. Specifically, we learn the procedure code of samplers for one-dimensional distributions. We formulate a Bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code. We use an approximate Bayesian computation method to learn the programs, whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest. In our experiments we leverage different probabilistic programming systems, including Anglican and Probabilistic C, to perform Markov chain Monte Carlo sampling over the space of programs. Experimental results have demonstrated that, using the proposed methodology, we can learn approximate and even some exact samplers. Finally, we show that our results are competitive with regard to genetic programming methods.", "creator": "LaTeX with hyperref package"}}}