{"id": "1606.00189", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Neural Network Translation Models for Grammatical Error Correction", "abstract": "Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, without explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system.", "histories": [["v1", "Wed, 1 Jun 2016 09:31:00 GMT  (45kb,D)", "http://arxiv.org/abs/1606.00189v1", "Accepted for presentation at IJCAI-16"]], "COMMENTS": "Accepted for presentation at IJCAI-16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shamil chollampatt", "kaveh taghipour", "hwee tou ng"], "accepted": false, "id": "1606.00189"}, "pdf": {"name": "1606.00189.pdf", "metadata": {"source": "META", "title": "Neural Network Translation Models for Grammatical Error Correction", "authors": ["Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng"], "emails": ["shamil@u.nus.edu,", "nght}@comp.nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "Grammar error correction (GEC) is a challenging task due to the variability of the error type and the syntactic and semantic dependencies of the errors from the surrounding context. Most grammatical error correction systems use classification and rule-based approaches to correct certain error types. However, these systems use several linguistic keywords as characteristics. Standard linguistic analysis tools such as Part-of-Speech (POS) taggers and parsers are often trained on well-shaped text and work poorly on ungrammatic text. This leads to further errors and limits the performance of rule-based and classifying approaches on GEC. As a result, the phrase-based approach of machine translation (SMT) has gained popularity because it is able to learn text formations from erroneous text to correct text from erroneous text corrections from erroneous parallel corrections, not limited to specific word types."}, {"heading": "2 Related Work", "text": "The popularity of this problem in natural language processing research continued to grow due to Helping Our Own (HOO) and the CoNLL common tasks [Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; 2014]. Most of the published work in GEC aimed to build specific classifiers for different types of errors and then use them to build hybrid systems [Dahlmeier et al., 2012; Rozovskaya et al., 2014]. One of the first approaches to using SMT for GEC focused on correcting the countability errors of mass terms (e.g. a lot of information) [Brockett et al., 2006]. They had to use an artificially constructed parallel corpus to form their SMT system. Later, the availability of large-scale error-correct data we improved [SMT network]."}, {"heading": "3 A Machine Translation Framework for Grammatical Error Correction", "text": "In this thesis, the task of grammatical error correction is formulated as a translation task from the language of \"bad\" English into the language of \"good\" English. That is, the source sentence is written by a learner of the second language and potentially contains grammatical errors, while the target sentence is the corrected fluent sentence. The best translation is selected according to the following equation: T \u0445 = argmax T P (T | S) = argmax T N \u2211 i = 1 \u03bbihi (T, S), where N is the number of features, hi and source code function and feature weight, respectively. We use the standard features used in phrase-based translation without reordering, which results in monotonous translations."}, {"heading": "4 Neural Network Global Lexicon Model", "text": "A global lexicon model is used to predict the presence of words in the corrected output. The model estimates the overall probability of a target hypothesis (i.e. a corrected sentence of a candidate) taking into account the source sentence using the probability calculated for each word in the hypothesis. The individual word probabilities can be calculated using training density estimation models such as maximum entropy [Mauser et al., 2009] or probabilistic neural networks [Ha et al., 2014]. Following [Ha et al., 2014] we formulate our global lexicon model using a preceding neural network. The model and training algorithm are described below."}, {"heading": "4.1 Model", "text": "The probability of a target hypothesis is calculated using the following equation: P (T | S) \u2248 | T | i = 1 P (ti | S) (1), where S and T are the source sentence and / or the target hypothesis, and | T | denotes the number of words in the target hypothesis. P (ti | S) is the probability of the target word ti in view of the source sentence S. P (ti | S) is the output of the neural network. The architecture of the neural network is shown in Figure 1. P (ti | S) is calculated by: P (ti | S) = \u03c3i (W2 \u00b7 O1 + b2), where O1 is the output of the hidden layer, and W2 and b2 are the output layer weights and distortions. \u03c3i is the elementary sigmoid function that scales the output to (0, 1 + b2). O1 is calculated by the following equation: O1 = (W1 \u00b7 S + b1), which is represented by the absence of the source of the absent layer on top 1 and the absent layer of the absent layer of the absent one and the absent layer of the absent layer."}, {"heading": "4.2 Training", "text": "We use the binary cross entropy (Eq.2) as a cost function: E = \u2212 1 | Vt | | Vt | \u2211 i = 1 [T-i log p (ti | S) + (1 \u2212 T-i) log (1 \u2212 p (ti | S)))] (2), where T-i refers to the binary wordbag representation of the reference target sentence and Vt is the target vocabulary. Each mini-batch consists of a fixed number of sentence pairs (S, T). The training algorithm repeatedly minimizes the cost function calculated for a given mini-batch by updating the parameters according to the gradients."}, {"heading": "4.3 Rescaling", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "5 Neural Network Joint Model", "text": "Common models in translation supplement the context information in language models with words from the starting sentence. A Neural Network Joint Model (NNJM) [Devlin et al., 2014] uses a neural network to model the probabilities of words in a context composed of source and target words. NNJM can scale to a large order of n-grams and still function well because it is able to capture semantic information through continuous space representations of words and to learn non-linear relationships between source and target words. Unlike the global lexicon model, NNJM uses a fixed window from the source side and takes sequence information from words to estimate the probability of the target word. The model and training method are described below."}, {"heading": "5.1 Model", "text": "The probability of the target hypothesis T in view of the source sentence S is estimated by the following equation: P (T | S) = max (T | S) = 1 P (ti | hi) (5), where | T \u2212 is the number of words in the target sentence, ti is the ith target word, and hi is the context (history) for the target word ti. The context hi consists of a series of m source words preceded by (sai \u2212 12, \u00b7 \u00b7, sai + m \u2212 12) and n \u2212 1 words preceding the target word ti, represented by (ti \u2212 n + 1, \u00b7, ti \u2212 1). The context words from the source page are the words in the window of sizem surrounding the source word sai, which is aligned with the target word ti. The output of the neural network P (ti | hi) is the output of the last softmax layer given by the following equation: P ti | hi and the last softword, which is given by the following layer hi | max."}, {"heading": "5.2 Training", "text": "To avoid the costly Softmax layer and thus speed up both training and decoding, we use Noise Contrastive Estimation (NCE) according to [Vaswani et al., 2013]. During the training, the negative log likelihood function is modified to a probable binary classifier that learns to distinguish between the actual target word and the k random words (noisy samples) per selected training instance. The two classes are C = 1, indicating that the word is the target word, and C = 0, indicating that the word is a loud sample. Conditional probabilities for C = 0 and C = 1, given a target word and context, are: P (C = 1 | ti, hi) = 1 k + 1P (ti | hi) 1k + 1P (ti | hi) 1K (hi) + 1P (hi) + 1P (P), given an equation = hi (iq)."}, {"heading": "6 Experiments", "text": "We describe our experimental setup including the description of the data we use, the configuration of our base system and neural network components, and the evaluation method in Section 6.1, followed by the results and discussion in Section 6.2."}, {"heading": "6.1 Setup", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "6.2 Results and Discussion", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "7 Conclusion", "text": "Our experiments show that using the two neural network translation models improves the performance of a phrase-based SMT approach to GEC. To our knowledge, this is the first work to use these two neural network models for SMT-based GEC. Neural networks \"ability to model words and phrases in continuous space and capture nonlinear relationships allows them to better generalize and make more precise grammatical errors."}, {"heading": "Acknowledgments", "text": "This research is supported by the Singapore Ministry of Education Academic Research Fund's Tier 2 funding MOE2013-T2-1150."}], "references": [{"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In Proc", "author": ["Chris Brockett", "William B. Dolan", "Michael Gamon. Correcting ESL errors using phrasal SMT techniques"], "venue": "of ACL,", "citeRegEx": "Brockett et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proc", "author": ["Daniel Dahlmeier", "Hwee Tou Ng. Better evaluation for grammatical error correction"], "venue": "of NAACL,", "citeRegEx": "Dahlmeier and Ng. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "NUS at the HOO 2012 shared task", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Eric Jun Feng Ng"], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,", "citeRegEx": "Dahlmeier et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Building a large annotated corpus of learner English: The NUS corpus of learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu"], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Dahlmeier et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Helping Our Own: The HOO 2011 pilot shared task", "author": ["Robert Dale", "Adam Kilgarriff"], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation,", "citeRegEx": "Dale and Kilgarriff. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "HOO 2012: A report on the preposition and determiner error correction shared task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway"], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,", "citeRegEx": "Dale et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proc", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul. Fast", "robust neural network joint models for statistical machine translation"], "venue": "of ACL,", "citeRegEx": "Devlin et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar. Grammatical error correction using hybrid systems", "type filtering"], "venue": "of CoNLL Shared Task,", "citeRegEx": "Felice et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lexical translation model using a deep neural network architecture", "author": ["Thanh-Le Ha", "Jan Niehues", "Alex Waibel"], "venue": "Proceedings of the 11th International Workshop on Spoken Language Translation,", "citeRegEx": "Ha et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation", "author": ["Junczys-Dowmunt", "Grundkiewicz", "2014] Marcin Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": "In Proc. of CoNLL Shared Task,", "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2014}, {"title": "In Proc", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu. Statistical phrase-based translation"], "venue": "of NAACL,", "citeRegEx": "Koehn et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In Proc", "author": ["Arne Mauser", "Sa\u0161a Hasan", "Hermann Ney. Extending statistical machine translation with discriminative", "trigger-based lexicon models"], "venue": "of EMNLP,", "citeRegEx": "Mauser et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proc", "author": ["Tomoya Mizumoto", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto. Mining revision log of language learning SNS for automated Japanese error correction of second language learners"], "venue": "of IJCNLP,", "citeRegEx": "Mizumoto et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "venue": "Proc. of CoNLL Shared Task,", "citeRegEx": "Ng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant"], "venue": "Proc. of CoNLL Shared Task,", "citeRegEx": "Ng et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Illinois-Columbia system in the CoNLL-2014 shared task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash"], "venue": "Proc. of CoNLL Shared Task,", "citeRegEx": "Rozovskaya et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for correcting English article errors", "author": ["Chengjie Sun", "Xiaoqiang Jin", "Lei Lin", "Yuming Zhao", "Xiaolong Wang"], "venue": "Proceedings of the 4th CCF Conference on Natural Language Processing and Chinese Computing,", "citeRegEx": "Sun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proc", "author": ["Raymond Hendy Susanto", "Peter Phandi", "Hwee Tou Ng. System combination for grammatical error correction"], "venue": "of EMNLP,", "citeRegEx": "Susanto et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang. Decoding with largescale neural language models improves translation"], "venue": "of EMNLP,", "citeRegEx": "Vaswani et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "NTHU at the CoNLL2014 shared task", "author": ["Jian-Cheng Wu", "Tzu-Hsi Yen", "Jim Chang", "Guan-Cheng Huang", "Jimmy Chang", "Hsiang-Ling Hsu", "YuWei Chang", "Jason S. Chang"], "venue": "Proc. of CoNLL Shared Task,", "citeRegEx": "Wu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Zheng Yuan", "Ted Briscoe. Grammatical error correction using neural machine translation"], "venue": "of NAACL,", "citeRegEx": "Yuan and Briscoe. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction [Susanto et al., 2014; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014].", "startOffset": 105, "endOffset": 188}, {"referenceID": 8, "context": "Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction [Susanto et al., 2014; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014].", "startOffset": 105, "endOffset": 188}, {"referenceID": 9, "context": "We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems [Ha et al., 2014; Devlin et al., 2014].", "startOffset": 137, "endOffset": 175}, {"referenceID": 7, "context": "We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems [Ha et al., 2014; Devlin et al., 2014].", "startOffset": 137, "endOffset": 175}, {"referenceID": 5, "context": "The popularity of this problem in natural language processing research grew further through Helping Our Own (HOO) and the CoNLL shared tasks [Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; 2014].", "startOffset": 141, "endOffset": 210}, {"referenceID": 6, "context": "The popularity of this problem in natural language processing research grew further through Helping Our Own (HOO) and the CoNLL shared tasks [Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; 2014].", "startOffset": 141, "endOffset": 210}, {"referenceID": 14, "context": "The popularity of this problem in natural language processing research grew further through Helping Our Own (HOO) and the CoNLL shared tasks [Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; 2014].", "startOffset": 141, "endOffset": 210}, {"referenceID": 3, "context": "Most published work in GEC aimed at building specific classifiers for different error types and then use them to build hybrid systems [Dahlmeier et al., 2012; Rozovskaya et al., 2014].", "startOffset": 134, "endOffset": 183}, {"referenceID": 16, "context": "Most published work in GEC aimed at building specific classifiers for different error types and then use them to build hybrid systems [Dahlmeier et al., 2012; Rozovskaya et al., 2014].", "startOffset": 134, "endOffset": 183}, {"referenceID": 1, "context": ", many informations\u2192much information) [Brockett et al., 2006].", "startOffset": 38, "endOffset": 61}, {"referenceID": 13, "context": "Later, the availability of large-scale error corrected data [Mizumoto et al., 2011] further improved SMT-based GEC systems.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": "Specifically, addition of monolingual neural network language models [Bengio et al., 2003; Vaswani et al., 2013], neural network joint models (NNJM) [Devlin et al.", "startOffset": 69, "endOffset": 112}, {"referenceID": 19, "context": "Specifically, addition of monolingual neural network language models [Bengio et al., 2003; Vaswani et al., 2013], neural network joint models (NNJM) [Devlin et al.", "startOffset": 69, "endOffset": 112}, {"referenceID": 7, "context": ", 2013], neural network joint models (NNJM) [Devlin et al., 2014], and neural network global lexicon models (NNGLM) [Ha et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 9, "context": ", 2014], and neural network global lexicon models (NNGLM) [Ha et al., 2014] have been shown to be useful for SMT.", "startOffset": 58, "endOffset": 75}, {"referenceID": 17, "context": "2014] and as a classifier for article error correction [Sun et al., 2015].", "startOffset": 55, "endOffset": 73}, {"referenceID": 21, "context": "Recently, a neural machine translation approach has been proposed for GEC [Yuan and Briscoe, 2016].", "startOffset": 74, "endOffset": 98}, {"referenceID": 11, "context": "We use a phrase-based machine translation framework [Koehn et al., 2003] for translation, which employs a log-linear model to find the best translation T \u2217 given a source sentence S.", "startOffset": 52, "endOffset": 72}, {"referenceID": 2, "context": "5 measure [Dahlmeier and Ng, 2012], which weights precision twice as much as recall, is the evaluation metric widely used for GEC and was the official evaluation metric adopted in the CoNLL 2014 shared task [Ng et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 15, "context": "5 measure [Dahlmeier and Ng, 2012], which weights precision twice as much as recall, is the evaluation metric widely used for GEC and was the official evaluation metric adopted in the CoNLL 2014 shared task [Ng et al., 2014].", "startOffset": 207, "endOffset": 224}, {"referenceID": 9, "context": "Additionally, we augment the feature set by adding two neural network translation models, namely a neural network global lexicon model [Ha et al., 2014] and a neural network joint model [Devlin et al.", "startOffset": 135, "endOffset": 152}, {"referenceID": 7, "context": ", 2014] and a neural network joint model [Devlin et al., 2014].", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": "The individual word probabilities can be computed by training density estimation models such as maximum entropy [Mauser et al., 2009] or probabilistic neural networks [Ha et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 9, "context": ", 2009] or probabilistic neural networks [Ha et al., 2014].", "startOffset": 41, "endOffset": 58}, {"referenceID": 9, "context": "Following [Ha et al., 2014], we formulate our global lexicon model using a feed-forward neural network.", "startOffset": 10, "endOffset": 27}, {"referenceID": 7, "context": "A neural network joint model (NNJM) [Devlin et al., 2014] uses a neural network to model the word probabilities given a context composed of source and target words.", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "To avoid the costly softmax layer and thereby speed up both training and decoding, we use Noise Contrastive Estimation (NCE) following [Vaswani et al., 2013].", "startOffset": 135, "endOffset": 157}, {"referenceID": 4, "context": "NUCLE [Dahlmeier et al., 2013], which is the official training data for the CoNLL 2013 and 2014 shared tasks, is used as the parallel text for training.", "startOffset": 6, "endOffset": 30}, {"referenceID": 13, "context": "0 [Mizumoto et al., 2011], which consists of texts written by ESL (English as Second Language) learners on the language learning platform Lang82.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": "Our baseline system uses exactly the same training data as [Susanto et al., 2014] for training the translation model and the language model.", "startOffset": 59, "endOffset": 81}, {"referenceID": 18, "context": "our baseline system and the SMT components of [Susanto et al., 2014] is that we tune with F0.", "startOffset": 46, "endOffset": 68}, {"referenceID": 19, "context": "To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) [Vaswani et al., 2013].", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "2 [Dahlmeier and Ng, 2012], for evaluation.", "startOffset": 2, "endOffset": 26}, {"referenceID": 21, "context": "We compare our system to the top 3 systems in the CoNLL 2014 shared task and to the best published results [Yuan and Briscoe, 2016; Susanto et al., 2014] on the test data of the CoNLL 2014 shared task.", "startOffset": 107, "endOffset": 153}, {"referenceID": 18, "context": "We compare our system to the top 3 systems in the CoNLL 2014 shared task and to the best published results [Yuan and Briscoe, 2016; Susanto et al., 2014] on the test data of the CoNLL 2014 shared task.", "startOffset": 107, "endOffset": 153}, {"referenceID": 21, "context": "Our final system including both neural network models outperforms the best system [Yuan and Briscoe, 2016] by 1.", "startOffset": 82, "endOffset": 106}, {"referenceID": 21, "context": "It should be noted that this is despite the fact that the system proposed in [Yuan and Briscoe, 2016] uses much larger training data than our system.", "startOffset": 77, "endOffset": 101}, {"referenceID": 21, "context": "[Yuan and Briscoe, 2016] - - 39.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "90 [Susanto et al., 2014] 53.", "startOffset": 3, "endOffset": 25}], "year": 2016, "abstractText": "Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, without explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system.", "creator": "TeX"}}}