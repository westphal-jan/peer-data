{"id": "1606.00253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "On a Topic Model for Sentences", "abstract": "Probabilistic topic models are generative models that describe the content of documents by discovering the latent topics underlying them. However, the structure of the textual input, and for instance the grouping of words in coherent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA, an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections.", "histories": [["v1", "Wed, 1 Jun 2016 12:34:50 GMT  (77kb,D)", "http://arxiv.org/abs/1606.00253v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["georgios balikas", "massih-reza amini", "marianne clausel"], "accepted": false, "id": "1606.00253"}, "pdf": {"name": "1606.00253.pdf", "metadata": {"source": "CRF", "title": "On a Topic Model for Sentences", "authors": ["Georgios Balikas", "Massih-Reza Amini", "Marianne Clausel"], "emails": ["FirstName.LastName@imag.fr"], "sections": [{"heading": "Keywords", "text": "Text Mining; Topic modeling; Unsupervised learning"}, {"heading": "1. INTRODUCTION", "text": "Statistical topic models are generative, unattended models that describe the content of documents in large collections of text. Previous research has examined the use of topic models such as Latent Dirichlet Allocation (LDA) [2] in a variety of areas ranging from image analysis to political science. However, most work on topic models is based on word interchangeability and treats documents in a bag manner. As a result, grouping the words into coherent text segments such as sentences or phrases is repellent. However, the inner structure of documents is generally useful when it comes to identifying topics. For example, one would expect that in each sentence, after standard pre-processing steps such as stop-word removal, only a very limited number of latent topics would appear. Therefore, we argue that contiguous text segments should represent \"constraints\" on the topics that appear within these segments."}, {"heading": "2. THE PROPOSED MODEL", "text": "A statistical model represents the words in a collection of D documents as mixtures of K \"topics,\" referring to a vocabulary of size V. In the case of LDA, a complete topic on topics from a dirichlet with previous parameters is examined for each document. The probability p \"(w\" s) of a term w \"(w\" s) is represented by the topic k \"s. We refer to the complete K\" V \"matrix of wordtopic probabilities as a result. The multinomial parameters are pulled back from a dirichlet previously parameterized by \u03b2. Each observed term w in the collection is represented by a discrete hidden indicator zi. For simplicity in mathematical development and notation, we assume symmetrical priors, but the extension to the asymmetrical case is manageable."}, {"heading": "3. EMPIRICAL RESULTS", "text": "In this context, it has to be said that these two cases are two different types who have behaved in different ways. (...) It is not that they behave in a certain way. (...) It is that they interact with each other in different ways. (...) It is that they interact with each other in different ways. (...) It is that they interact with each other in different ways. (...) It is as if they interact with each other in different ways. (...) It is as if they interact with each other in different ways. (...) It is as if they interact with each other in different ways. (...) It is as if they interact with each other in different ways. (...) It is as if they meet with different people. \"(... (...).\""}, {"heading": "LDA senLDA senLDA+", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LDA senLDA senLDA+", "text": "To investigate the correlation between the topic model representations learned at different text levels, we report on the classification performance by using the concatenation of the topic distributions of a document by LDA and senLDA as document representations. For example, the concatenated vector representation of a document performs better if K = 125 is a vector of 250 dimensions for each model. The resulting concatenated representations are referred to by \"senLDA +\" in Figure 3. As can be seen, \"senLDA +\" performs better in comparison to both LDA and senLDA. Its performance combines the advantages of both models: during the first iterations it is as steep as the senLDA representations, and in later iterations the senLDA convergence benefits in order to surpass the simple senLDA representation."}, {"heading": "4. CONCLUSION", "text": "We proposed senLDA, an extension of senLDA, in which topics are sampled per contiguous text span, resulting in very fast convergence and good classification and perplexity. LDA and senLDA differ in that the second assumes a very strong dependence of latent topics between words of sentences, while the first assumes independence between words of documents in general. In our future research, our goal is to investigate this dependence and further adapt the sampling process of topic models to the rich text structure."}, {"heading": "5. ACKNOWLEDGEMENTS", "text": "This work is partially supported by CIFRE N 28 / 2015."}, {"heading": "6. REFERENCES", "text": "[1] L. Azzopardi, M. Girolami, and K. van Risjbergen.Investigating the relationship between language model perplexity and IR precision-recall measures. In SIGIR, pp. 369-370, 2003. [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of Machine Learning research, 3: 993-1022, 2003. [3] J. L. Boyd-Graber and D. M. Blei. Syntactic Topic Models. In Advances in Neuronal Information Processing Systems, pp. 185-192, 2009. [4] R.-C. Chen, R. Swanson, and A. S. Gordon. An adaptation of topic modeling to sentences. 2010. [5] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, S. Isics, 101 (suppl 1): 5228-5235, 2004."}], "references": [{"title": "Investigating the relationship between language model perplexity and IR precision-recall measures", "author": ["L. Azzopardi", "M. Girolami", "K. van Risjbergen"], "venue": "In SIGIR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Syntactic topic models", "author": ["J.L. Boyd-Graber", "D.M. Blei"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "An adaptation of topic modeling to sentences", "author": ["R.-C. Chen", "R. Swanson", "A.S. Gordon"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Technical report, Technical report,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "LSHTC: A benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "CoRR, abs/1503.08581,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition", "author": ["G. Tsatsaronis", "G. Balikas", "P. Malakasiotis", "I. Partalas", "M. Zschunke", "M.R. Alvers", "D. Weissenborn"], "venue": "BMC bioinformatics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Multi-document summarization using sentence-based topic models", "author": ["D. Wang", "S. Zhu", "T. Li", "Y. Gong"], "venue": "In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Prior research has investigated the application of topic models such as Latent Dirichlet Allocation (LDA) [2] in a variety of domains ranging from image analysis to political science.", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "For inference, we use a collapsed Gibbs sampling method [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "After standard manipulations as in the paradigm of [6] one arrives at:", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "For instance, in [9] in the context of summarization the authors combine the unigram language model with topic models over sentences so that the latent topics are represented by sentences instead of terms.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "In [4] the notion of sentence topics is introduced and they are sampled from separate topic distributions and co-exist with the word topics.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "[3] propose an adaptation of topic models to the text structure obtained by the parsing tree of a document.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "They come from the publicly available collections of Wikipedia [7] and PubMed [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "They come from the publicly available collections of Wikipedia [7] and PubMed [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "For intrinsic evaluation we report here perplexity [1], which is probably the dominant measure for topic models evaluation in the bibliography.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Extrinsic evaluation Previous studies have shown that perplexity does not always agree with human evaluations of topic models [1] and it is recommended to evaluate topic models on real tasks.", "startOffset": 126, "endOffset": 129}], "year": 2016, "abstractText": "Probabilistic topic models are generative models that describe the content of documents by discovering the latent topics underlying them. However, the structure of the textual input, and for instance the grouping of words in coherent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA, an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections.", "creator": "LaTeX with hyperref package"}}}