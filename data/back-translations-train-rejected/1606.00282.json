{"id": "1606.00282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Multi-Label Zero-Shot Learning via Concept Embedding", "abstract": "Zero Shot Learning (ZSL) enables a learning model to classify instances of an unseen class during training. While most research in ZSL focuses on single-label classification, few studies have been done in multi-label ZSL, where an instance is associated with a set of labels simultaneously, due to the difficulty in modeling complex semantics conveyed by a set of labels. In this paper, we propose a novel approach to multi-label ZSL via concept embedding learned from collections of public users' annotations of multimedia. Thanks to concept embedding, multi-label ZSL can be done by efficiently mapping an instance input features onto the concept embedding space in a similar manner used in single-label ZSL. Moreover, our semantic learning model is capable of embedding an out-of-vocabulary label by inferring its meaning from its co-occurring labels. Thus, our approach allows both seen and unseen labels during the concept embedding learning to be used in the aforementioned instance mapping, which makes multi-label ZSL more flexible and suitable for real applications. Experimental results of multi-label ZSL on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label ZSL model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model.", "histories": [["v1", "Wed, 1 Jun 2016 13:38:04 GMT  (770kb)", "http://arxiv.org/abs/1606.00282v1", "15 pages. Technical Report 2016-06-01. School of Computer Science. The University of Manchester. (Submitted to a Journal)"]], "COMMENTS": "15 pages. Technical Report 2016-06-01. School of Computer Science. The University of Manchester. (Submitted to a Journal)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ubai sandouk", "ke chen"], "accepted": false, "id": "1606.00282"}, "pdf": {"name": "1606.00282.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Zero-Shot Learning via Concept Embedding", "authors": ["Ubai Sandouk", "Ke Chen"], "emails": [], "sections": [{"heading": null, "text": "Most research in ZSL focuses on single-label classification, only a few studies have been conducted in multi-label ZSL, where an instance is associated with a number of labels at the same time, due to the difficulty of modelling complex semantics conveyed by a number of labels. In this paper, we propose a novel approach to multi-label ZSL by embedding concepts we have learned from public multimedia users \"comments. Thanks to the embedding of concepts, multi-label ZSL can be performed by efficiently embedding an instance of multi-label ZSL into the concept space, similar to what is the case with single-label ZSL. Furthermore, our semantic learning model is able to embed a label outside the vocabulary by deriving its meaning from its jointly occurring labels. Thus, our approach allows both visible and invisible labels to be embedded during the concept of multi-label embedding, learning the multi-label concept that Sulmapping makes possible."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a purely mental game, in which the aim is to find a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "2 RELATED WORKS", "text": "In this case, it is an experiment that it carries within itself. It is an experiment that it carries within itself. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist. It is an experiment that it does not exist."}, {"heading": "3 CONCEPT EMBEDDING BASED MULTI-LABEL ZSL", "text": "In this section we present our concept for the embedding of a multi-level ZSL framework (CE-ML-ZSL). First we describe our problem definition and main idea. Afterwards we present our technical solutions in detail."}, {"heading": "3.1 Overview", "text": "It is about the question to what extent it is about a way and a way, in which it is about the question whether and to what extent it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way, in which it is about a way and a way in which it is about a way, in which it is about a way and a way in which it is about a way, in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a"}, {"heading": "3.2 Concept Embedding Learning", "text": "To be self-contained, we briefly describe our approach to learning HI: \u0442 \u00d7 \u0394 \u2192 2 (L), which we have developed in our recent work, and further details can be found in [19]."}, {"heading": "3.2.1 Label, Context and Document Representation", "text": "Our CE learning approach [19] is based on raw label, context, and document representations. A label M value (E) is described by analyzing its global usage pattern in a semantic learning dataset by aggregation [23]. Consequently, the * o = o weights of each label's usage are first extracted to highlight rare but informative labels. Subsequently, dot products are used on label pairs to detect pairs of common usage patterns. Finally, each label is described by its common usage pattern against all other labels in the training kit. The resulting feature vector r (M) is dimensional in nature and summarizes the global usage of each label. The local context of a label M, which consists of a document, a set of commonly occurring labels, is captured via latent dirichlet allocation (LDA) [11], which characterizes the local context with a histogram across a series of topics as a latent representation (resulting in a low cost)."}, {"heading": "3.2.2 Siamese Neural Architecture", "text": "As shown in Fig. 4, a subnet consists of y successive layers of non-linear units and is fed with the input:; (E) (M,) = r (M),? u () 1 formed by 1 To distinguish IM learning, we apply the high sentence (T) to the notation of training data used in CE learning. Chain of label and local context features. Such a subnet is used to predict the vwx () of; (E) (M,). Therefore, the activations of the penultimate layer, the so-called coding layer, are used to provide CE representations. To improve CE, two identical subnetworks are coupled via their remote learning coding layers, ensuring that the Euclidean distance between two concepts in CE space adequately reflects their setical similarity."}, {"heading": "3.2.3 Learning Algorithm", "text": "To predict z (E) = vwx () of; (E) (M) (M),) a sub-network is initialized using the layered learning method proposed in [24]; (E), a variant of cross-entropy loss (measuring the difference between z (E) and the predicted results, zY (E) is used for this learning task: (E), zY (E), zY (E), (1), where there is a collective notation of all parameters in the sub-network, z (E), 5 + zY (E), 8 + |: | O (E), 1 \u2212 z (E), 5 log), which is a divergence between all parameters in the sub-network, z (E), 8 is the element of z (E) and =: z (E) 5j8 s is a correction that reduces the influence of costs by highlighting the confusion."}, {"heading": "3.3 CE-Based Instance Mapping Learning", "text": "In this section we present our approach to learn how to assign instances to the CE representations VW: 2 (3) \u2192 T."}, {"heading": "3.3.1 Training Example Generation", "text": "If there is no OOV mark in = MN NO | P | in connection with an instance, the CE mark for MN in its local context is obtained directly via the CE model: HI (M,). If there are OOV marks in the instance, we use the CE mark to derive the CE mark of the OOV mark from those of other markings in the vocabulary (IV) in [19]. Since simultaneous occurring markings should be semantically coherent, the CE mark of an OOV mark in the instance can be estimated as a CE mark."}, {"heading": "3.3.2 SVR-Based Instance Mapping Learning", "text": "Supportive vector regression (T \u00a8 \u00a9) [26] proves to be an effective regression tool. In our work, we use SVR to learn a regression model. Since the CE representation target X is multivariate for one instance, we train (E) T \u00a8 \u00a9 models, each SVR managing the regression of; to one of = (E) CE characteristics. In view of an exemplary training data set of R examples, (N, XN5k8) \"O 2 (L) NO, \u00ae \u2212 T \u00a8 \u00a9 learning is defined as [27]: Minimize \u00b1 (\") B \u00b1 (\") (\") + (\") \u00b2 (\") \u00b2 (\") + \u2211 \u00b3 + \u00b2 'N (\") (\") N (\") \")\") \")\"). \""}, {"heading": "3.4 Deployment in Multi-Label ZSL", "text": "During the test, the trained IM model provides a predicted CE target XZ = VW (; Y; \u00c9) for a test instance; Y. Subsequently, a standardized semantic priming method [22] is used to achieve kinship using (2), which measures the distance between XZ and the known embedded concepts defined by all the examples in our semantic learning and instance training datasets (cf. Fig. 2 (b)). While a label has multiple CE representations as used in different label groups to describe different instances, the ultimate goal of the multi-label ZSL expects a single mapping value for each label. Using the CE nature, we solve the problem by defining the following rule: For a label MN, the relationship between; Y and MN is measured over the minimum distance between XZ and all known CE representations of M, i.e. XZ, M = MZ (XZ = XHI) and XHN (XHI = minimum)."}, {"heading": "4 EXPERIMENTAL SETTINGS", "text": "In order to thoroughly evaluate our approach, we apply it to both image and music areas. In this section, we describe data sets, experimental protocols, and evaluation criteria used in this work."}, {"heading": "4.1 Dataset", "text": "We use two benchmark records in each domain: MagTag5K [28] and Million Song Dataset (MSD) [29] for music tracks and HSUN [14] and LabelMe [30] for image tracks. MagTag5K is a controlled version of MagnaTune, which is the result of an online annotation game in which players evaluate the appropriateness of label sets for music tracks [31]. MagTag5K contains 5,259 tracks commented with a vocabulary of 136 labels. The average number of labels in a series of labels describing a single track, i.e. the document length, is necessarily five in MagTag5K. MSD is a record of one million songs, some of which are commented online by the crowd via last.fm, a crowd-sharing website for users to freely comment on music tracks, i.e., the records of records relating to labels are one million songs, some of which are commented on by the crowd via last.fm, a crowd-sharing site for users to freely comment on music tracks, i.e. the records of records relating to labels are one set of which are one set of records for each of five days."}, {"heading": "4.2 Instance Input Representation", "text": "To establish the IM model, we use commonly used instance features to represent an image or a music track. Acoustic information is extracted from a music track using short-term spectral analysis (e.g. Echo Nest Timbre (ENT) features [32], which characterize audio segments with 12 MFCC-like basic functions [33]. It is worth noting that these basic functions are kept secret by EchoNest, but allow each piece of music to be seamlessly encoded by its API [32]. Datasets such as MSD are often distributed using ENT features instead of raw music tracks to circumvent copyright restrictions. As a result, a track is automatically divided into segments in which each segment is characterized by 12 ENT features via the API. In our experiments, the ENT features of a segment are presented together with the 1st and 2nd derivative features of a segment."}, {"heading": "4.3 Experimental Protocol", "text": "For a thorough performance assessment, we have designed a series of experiments in different environments and further compared our approach to COSTA [18]. To filter out the best of our knowledge, this is the only model that uses contextualized semantics for multilingual learning. However, other approaches are not comparable due to their technical limitations, such as dependence on other techniques required in their approach, such as semantic image segmentation, which must be applied before ZSL learning. Furthermore, the work in [9] applies only to the image domain, while our experiments cover both image and music domains. In our experiments, we use two different settings for semantic learning.The first setting is similar to that used in COSTA [18], where a single dataset is used to simulate ZSL scenarios."}, {"heading": "5 EVALUATION", "text": "In this section we first describe our evaluation criteria and report on the results of various experimental settings."}, {"heading": "5.1 Evaluation Criteria", "text": "The example-based evaluation evaluates the ability of a model to predict a set of suitable labels for a test instance, while the concept-based evaluation evaluates the ability of a model to correctly identify the applicability of individual labels to test instances. In contrast to COSTA [18], where only the concept-based evaluation is used, we adopt both evaluation criteria in our experiments. In the face of a test instance; Y, a model returns the order of relative to all known labels:? [=?] [5 |; Y8 NO |: Where? [5], if < Y8, if < as described in Sect. 3.4. In the example-based evaluation, we first measure precision against \u00f3 [39, pp. 151-162], i.e. the proportion of correctly predicted labels at the top positions."}, {"heading": "5.2 Results on Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Results on CE Learning", "text": "During CE learning, we specify the number of topics to be used in context modeling with the hierarchical Dirichlet process [40], which yields 19 and 30 topics for MagTag5K and HSUN, respectively. Optimal hyperparameters in the deep subnetworks are found via the web search based on the resume described in Sekt. 4.3. Consequently, the optimal subnetwork in Siamese architecture has a structure: ji * \u2192 100 \u2192 100 \u2192 = (E) \u2192 (i * ji * for MagTag5K and ji * 200 \u2192 = (E) \u2192 (i * ji * for HSUN. We set = 0.5 and = (E) in (3), \u00a1= 1 in (4) for both datasets. The initial learning rates are set to 10 \u00fa for MagTag5K and 5 \u00d7 10 for HSUN and the learning rates are decomposed with a factor of 0.95 per 200 epochs."}, {"heading": "5.2.2 Results on IM Learning", "text": "In our experiments we observe that the optimal hyperparameters depend on the dimensionality of the CE space and are considered within a range of 50, 1,0,48, '1 and 1 for all = (E) dimensions. The IM model is evaluated by measuring the average error resulting from regression on a test data set, \u00e8. \"The property XZ (; Y,) \u2212 X (; Y,) \u2212 X-X-X (; Y,) \u2212 Scan-S is defined."}, {"heading": "5.3 WCT Results", "text": "This year it is more than ever before."}, {"heading": "5.4 CCT Results", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "6 DISCUSSION", "text": "Dre eeisrVnlrsrteeaeeteeteeteerteerterrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "7 CONCLUSION", "text": "This book is about how it works and how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works, it's about how it works. It's about how it works, it's about how it works. It's about how it works."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Zero Shot Learning (ZSL) enables a learning model to classify instances of an unseen class during training. While most research in ZSL focuses on single-label classification, few studies have been done in multi-label ZSL, where an instance is associated with a set of labels simultaneously, due to the difficulty in modeling complex semantics conveyed by a set of labels. In this paper, we propose a novel approach to multi-label ZSL via concept embedding learned from collections of public users\u2019 annotations of multimedia. Thanks to concept embedding, multi-label ZSL can be done by efficiently mapping an instance input features onto the concept embedding space in a similar manner used in single-label ZSL. Moreover, our semantic learning model is capable of embedding an out-of-vocabulary label by inferring its meaning from its co-occurring labels. Thus, our approach allows both seen and unseen labels during the concept embedding learning to be used in the aforementioned instance mapping, which makes multi-label ZSL more flexible and suitable for real applications. Experimental results of multilabel ZSL on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label ZSL model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model.", "creator": "PrimoPDF http://www.primopdf.com"}}}