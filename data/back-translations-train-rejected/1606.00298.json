{"id": "1606.00298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Automatic tagging using deep convolutional neural networks", "abstract": "We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.", "histories": [["v1", "Wed, 1 Jun 2016 14:18:08 GMT  (125kb,D)", "http://arxiv.org/abs/1606.00298v1", "Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2016"]], "COMMENTS": "Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2016", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["keunwoo choi", "george fazekas", "mark sandler"], "accepted": false, "id": "1606.00298"}, "pdf": {"name": "1606.00298.pdf", "metadata": {"source": "CRF", "title": "AUTOMATIC TAGGING USING DEEP CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "emails": ["mark.sandler}@qmul.ac.uk"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2. CNNS FOR MUSIC SIGNAL ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Motivation for using CNNs for audio analysis", "text": "In this section, we examine the characteristics of CNNs in terms of music signals. [14] The development of CNNs has been motivated by biological vision systems, in which information from local regions is repeatedly captured by many sensory cells and used to collect higher information. [14] CNNs are therefore designed to learn robust characteristics that respond to certain visual objects with local translation and distortion invariances. These advantages often also work with audio signals, although the topology of audio signals (or their 2D representation) is not identical to that of a visual image. CNNs have been used for various audio analysis tasks, mostly on the assumption that auditory events can be detected or detected by perceiving their time frequency representations. Although the advantage of deep learning is to learn the characteristics, the architecture of the networks should be carefully designed, taking into account the extent to which the characteristics (e.g. inventories) of such characteristics are desirable."}, {"heading": "2.2 Design of CNNs architectures", "text": "There are many variants of applying CNNs to audio signals: they differ in the type of input representation, folding axes, size and number of twisted cores or subsamplings, and the number of hidden layers."}, {"heading": "2.2.1 TF-representation", "text": "The use of the mel scale is supported by expertise in human hearing [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27]. Constant Q transformation (CQT) is mainly used where the fundamental frequencies of notes should be accurately identified, e.g. in chord recognition [10] and transcription [22]. Direct use of short-term Fourier transformation coefficients (STFT) is preferred when inverse transformation is necessary [3, 23]. It was used, for example, in limit detection [7], but is less popular compared to its ubiquitous use in digital signal processing. Compared to CQT, the frequency resolution of the STFT transformation is required in comparison to the human frequency ranges [7], which are required for both the recognition of the basic frequency and the basic frequency of the STimel finel scale."}, {"heading": "2.2.2 Convolution - kernel sizes and axes", "text": "Each folding layer of size H x W x D learns D characteristics of H x W, where H and W refer to the height or width of the learned cores, respectively. Therefore, the size of the core determines the maximum size of a component that it can accurately capture. If the kernel size is too small, the layer would not learn a meaningful representation of the shape (or distribution) of the data. Therefore, relatively large cores such as 17 x 5 are proposed in [10]. This is also justified by the task (chord recognition), where a small change in the distribution along the frequency axis should lead to different results and therefore no frequency invariance should be assigned. However, using large cores can have two disadvantages. Firstly, it is known that the number of parameters per representation capacity increases as the size of the core increases. For example, 5 x 5 folds should be replaced by two stacked 3 x 3 folds, which results in a smaller number of parameters."}, {"heading": "2.2.3 Pooling - sizes and axes", "text": "Pooling reduces the size of the feature map with one operation, usually a maximum function. It has been adopted by the majority of work that relies on CNN structures. Essentially, pooling uses subsampling to reduce the size of the feature map while preserving the information of an activation in the region, rather than information about the entire input signal. This nonlinear behavior of subsampling also provides distortion and translation invariances by discarding the original position of the selected values. As a result, the pooling size determines the tolerance of position variance within each plane and represents a compromise between two aspects that affect network performance. If the pool size is too small, the network does not have enough distortion invariance, if it is too large, the positioning of features can be overlooked when they are needed. Generally, the pooling axes coincide with the convential axes, although this is not necessarily the case. More important is the axis we need to consider, for example, when invoking the inventory."}, {"heading": "3. PROBLEM DEFINITION", "text": "Automatic tagging is a multi-label classification task, i.e. a clip can have multiple tags attached to it. It differs from other audio classification problems, such as multi-label classification, which are often formalised as a single-label classification problem. If there are the same number of labels, the output space of multi-label classification can increase exponentially compared to single-label classification. Accordingly, classifying multiple labels requires more data to solve a model with greater capacity and efficient optimization methods. If there are K-exclusive labels, the classifier only needs to be able to predict one of K's different vectors, which are a hot vector. However, with multiple labels, the number of cases increases to 2K. For crowd-sourced music tag records [2,13], most of the tags are wrong (0) for most clips, making accuracy or square error inappropriate."}, {"heading": "4. PROPOSED ARCHITECTURE", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5. EXPERIMENTS AND DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Overview", "text": "Two sets of data were used to evaluate the proposed system, the MagnaTagune records [13] and the Million Song records (MSD) [2]. The MagnaTagune records were relatively popular for content-based tagging, but similar performances from recent work seem to suggest that the performances are saturated, i.e. a glass ceiling was not reached by the noise in the annotation. MSD contains more songs than MagnaTagune, it has different types of annotations up to 1M songs. There was not much work to compare our approach as audio signals do not come with the records."}, {"heading": "5.2 Experiment I: MagnaTagATune", "text": "The MagnaTagune dataset consists of 25,856 clips of 29.1-s, 16 kHz sampled mp3 files with 188 tags. We only use top 50 tags covering genres (classical, rock), instruments (piano, vocals, drums) and other descriptions (slow, Indian). The dataset is not balanced, the most common tag is used 4,851 times, while the 50th is used most frequently. The data set designations consist of 7,644 unique vectors in a 50-dimensional vector space. The results of the proposed architecture and its variants are summarized in Table 3. There is little difference in performance between FCN and FCN-5 and FCN-5."}, {"heading": "5.3 Experiment II: Million Song Dataset", "text": "We evaluated the proposed structures using the Million Song Dataset (MSD) with last.fm tags. We selected the top 50 tags covering genres (rock, pop, jazz, funk), epochs (60s-00s) and moods (sad, happy, chill). 214,284 (201,680 for training and 12,605 for validation) and 25,940 clips were selected from the training / test kits provided by filtering out elements without top 50 tags. The number of tags ranges from 52,944 (rock) to 1,257 (happy) and there are 12,348 unique tag vectors. Note that the size of the MSD is more than 9 times larger than the MagnaTagune dataset.The results of the proposed architectures with different layers are summarized in Table 5."}, {"heading": "6. CONCLUSION", "text": "In Experiment I (Section 5.2), the proposed architectures with different input representations and layer counts were compared with the MagnaTagATune dataset with results reported in previous work with competitive results. In terms of audio input representation, the use of Mel spectrograms resulted in better performance compared to STFTs and MFCCs. In Experiment II (Section 5.3), different layer counts were evaluated with the million song dataset, which contains nine times as many music clips. In this experiment, deeper networks were found to benefit most from the availability of large training data. In the future, automatic tagging algorithms with variable input lengths will be investigated."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This work was partly funded by the FAST IMPACt EPSRC Grant EP / L019981 / 1 and the H2020 Research and Innovation grant AudioCommons (688382) from the European Commission. Sandler acknowledges the support of the Royal Society as recipient of a Wolfson Research Merit Award."}, {"heading": "8. REFERENCES", "text": "[1] Fre \u00d3ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features and speed improvements. arXiv preprint arXiv: 1211.5590, 2012. [2] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. In Proceedings of the 12th International Society for Music Retrieval Conference, ISMIR 2011, Miami, Florida, USA, October 24-28, 2011, pages 591- 596 Ellis, 2011. Keunwoo Choi, George Fazekas, Mark Sandler, and Jeonghee Kim. Auralization of deep convolutional neural networks: Listening to learned features. In Proceedings of the 16th International Society for Music Retval Conference, ISMIR 2015."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In Proceedings of the 12th International Society for Music Information Retrieval Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Auralisation of deep convolutional neural networks: Listening to learned features", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler", "Jeonghee Kim"], "venue": "In Proceedings of the 16th International Society for Music Information Retrieval Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Keras: Deep learning library for theano and tensorflow", "author": ["Fran\u00e7ois Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Multiscale approaches to music audio feature learning", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Proceedings of the 14th International Society for Music Information Retrieval Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "End-toend learning for music audio", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Music boundary detection using neural networks on spectrograms and selfsimilarity lag matrices", "author": ["Thomas Grill", "Jan Schl\u00fcter"], "venue": "In Proceedings of the 23rd European Signal Processing Conference (EUSPICO", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio", "author": ["Philippe Hamel", "Simon Lemieux", "Yoshua Bengio", "Douglas Eck"], "venue": "In Proceedings of the 12th International Society for Music Information Retrieval Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Rethinking automatic chord recognition with convolutional neural networks", "author": ["Eric J Humphrey", "Juan P Bello"], "venue": "In Machine Learning and Applications, 11th International Conference on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Evaluation of algorithms using games: The case of music tagging", "author": ["Edith Law", "Kris West", "Michael I Mandel", "Mert Bay", "J Stephen Downie"], "venue": "In Proceedings of the 10th International Society for Music Information Retrieval Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Content-aware collaborative music recommendation using pre-trained neural networks", "author": ["Dawen Liang", "Minshu Zhan", "Daniel PW Ellis"], "venue": "In Proceedings of the 16th International Society for Music Information Retrieval Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "An introduction to the psychology of hearing", "author": ["Brian CJ Moore"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A deep bag-of-features model for music auto-tagging", "author": ["Juhan Nam", "Jorge Herrera", "Kyogu Lee"], "venue": "arXiv preprint arXiv:1508.04999,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["Tara N Sainath", "Abdel-rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Tara N Sainath", "Ron J Weiss", "Andrew Senior", "Kevin W Wilson", "Oriol Vinyals"], "venue": "In Proc. Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["Jan Schluter", "Sebastian Bock"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on. IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "An end-to-end neural network for polyphonic music transcription", "author": ["Siddharth Sigtia", "Emmanouil Benetos", "Simon Dixon"], "venue": "arXiv preprint arXiv:1508.01774,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network", "author": ["Andrew JR Simpson", "Gerard Roma", "Mark D Plumbley"], "venue": "arXiv preprint arXiv:1504.04658,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1929}, {"title": "Musical genre classification of audio signals", "author": ["George Tzanetakis", "Perry Cook"], "venue": "Speech and Audio Processing, IEEE transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Boundary detection in music structure analysis using convolutional neural networks", "author": ["Karen Ullrich", "Jan Schl\u00fcter", "Thomas Grill"], "venue": "In Proceedings of the 15th International Society for Music Information Retrieval Conference,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Deep content-based music recommendation", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Transfer learning by supervised pretraining for audio-based music classification", "author": ["A\u00e4ron Van Den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Proceedings of the 15th International Society for Music Information Retrieval Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Audio music genre classification using different classifiers and feature selection methods", "author": ["Yusuf Yaslan", "Zehra Cataltepe"], "venue": "In 18th ICPR 2006,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}], "referenceMentions": [{"referenceID": 27, "context": "Although feature selection have been widely used to solve this problem [29], clear recommendations which", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "Aggregating hand-crafted features for music tagging was introduced in [25].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Since these are frame-level features, their statistics such as mean and variance are computed [25], or they are clustered and vector quantised [15] to obtain clip-level features.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Since these are frame-level features, their statistics such as mean and variance are computed [25], or they are clustered and vector quantised [15] to obtain clip-level features.", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "As alternative to the above systems, DNNs have recently become widely used in audio analysis, following their success in computer vision, speech recognition [19] and auto-tagging [6, 8, 18, 28].", "startOffset": 157, "endOffset": 161}, {"referenceID": 5, "context": "As alternative to the above systems, DNNs have recently become widely used in audio analysis, following their success in computer vision, speech recognition [19] and auto-tagging [6, 8, 18, 28].", "startOffset": 179, "endOffset": 193}, {"referenceID": 7, "context": "As alternative to the above systems, DNNs have recently become widely used in audio analysis, following their success in computer vision, speech recognition [19] and auto-tagging [6, 8, 18, 28].", "startOffset": 179, "endOffset": 193}, {"referenceID": 16, "context": "As alternative to the above systems, DNNs have recently become widely used in audio analysis, following their success in computer vision, speech recognition [19] and auto-tagging [6, 8, 18, 28].", "startOffset": 179, "endOffset": 193}, {"referenceID": 26, "context": "As alternative to the above systems, DNNs have recently become widely used in audio analysis, following their success in computer vision, speech recognition [19] and auto-tagging [6, 8, 18, 28].", "startOffset": 179, "endOffset": 193}, {"referenceID": 13, "context": "In computer vision, deep convolutional neural networks (CNNs) have been introduced because they can simulate the behaviour of the human vision system and learn hierarchical features, allowing object local invariance and robustness to translation and distortion in the model [14].", "startOffset": 274, "endOffset": 278}, {"referenceID": 17, "context": "CNNs have been introduced in audio-based problems for similar reasons, showing state-of-the-art performance in speech recognition [19] and music segmentation [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "CNNs have been introduced in audio-based problems for similar reasons, showing state-of-the-art performance in speech recognition [19] and music segmentation [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 4, "context": "In [5] and [28], spherical kmeans and multi-layer perceptrons are used as feature extractor and classifier respectively.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "In [5] and [28], spherical kmeans and multi-layer perceptrons are used as feature extractor and classifier respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Multi-resolution spectrograms are used in [5] to leverage the information in the audio signal on different time scales.", "startOffset": 42, "endOffset": 45}, {"referenceID": 26, "context": "In [28], pretrained weights of multilayer perceptrons are transferred in order to predict tags for other datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "A two-layer convolutional network is used in [6] with mel-spectrograms as well as raw audio signals as input features.", "startOffset": 45, "endOffset": 48}, {"referenceID": 16, "context": "In [18], bagof-features are extracted and input to stacked Restricted Boltzmann machines (RBM).", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The development of CNNs was motivated by biological vision systems where information of local regions are repeatedly captured by many sensory cells and used to capture higher-level information [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "Mel-spectrograms have been one of the widespread features for tagging [5], boundary detection [26], onset detection [21] and latent feature learning [27].", "startOffset": 70, "endOffset": 73}, {"referenceID": 24, "context": "Mel-spectrograms have been one of the widespread features for tagging [5], boundary detection [26], onset detection [21] and latent feature learning [27].", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "Mel-spectrograms have been one of the widespread features for tagging [5], boundary detection [26], onset detection [21] and latent feature learning [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "Mel-spectrograms have been one of the widespread features for tagging [5], boundary detection [26], onset detection [21] and latent feature learning [27].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 165, "endOffset": 184}, {"referenceID": 16, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 165, "endOffset": 184}, {"referenceID": 19, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 165, "endOffset": 184}, {"referenceID": 24, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 165, "endOffset": 184}, {"referenceID": 25, "context": "The use of the mel-scale is supported by domain knowledge about the human auditory system [17] and has been empirically proven by performance gains in various tasks [6, 18, 21, 26, 27].", "startOffset": 165, "endOffset": 184}, {"referenceID": 9, "context": "chord recognition [10] and transcription [22].", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "chord recognition [10] and transcription [22].", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "The direct use of Short-time Fourier Transform (STFT) coefficients is preferred when an inverse transformation is necessary [3, 23].", "startOffset": 124, "endOffset": 131}, {"referenceID": 21, "context": "The direct use of Short-time Fourier Transform (STFT) coefficients is preferred when an inverse transformation is necessary [3, 23].", "startOffset": 124, "endOffset": 131}, {"referenceID": 6, "context": "It has been used in boundary detection [7] for example, but it is less popular in comparison to its ubiquitous use in digital signal processing.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "These are called end-to-end models and applied both for music [6] and speech [20].", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "These are called end-to-end models and applied both for music [6] and speech [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The performance is comparable to the mel-spectrogram in speech recognition [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "It is also noteworthy that the learned filter banks in both [6] and [20] show similarities to the mel-scale, supporting the use of the known nonlinearity of the human auditory system.", "startOffset": 60, "endOffset": 63}, {"referenceID": 18, "context": "It is also noteworthy that the learned filter banks in both [6] and [20] show similarities to the mel-scale, supporting the use of the known nonlinearity of the human auditory system.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "For this reason, relatively largesized kernels such as 17\u00d75 are proposed in [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "For tagging, 1D convolution along the time axis is used in [6] to learn the temporal distribution, assuming that different spectral band have different distributions and therefore features should be learned per fre-", "startOffset": 59, "endOffset": 62}, {"referenceID": 20, "context": "In contrast, 2D convolution can learn both temporal and spectral structures and has already been used in music transcription [22], onset detection [21], boundary detection [26] and chord recognition [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "In contrast, 2D convolution can learn both temporal and spectral structures and has already been used in music transcription [22], onset detection [21], boundary detection [26] and chord recognition [10].", "startOffset": 147, "endOffset": 151}, {"referenceID": 24, "context": "In contrast, 2D convolution can learn both temporal and spectral structures and has already been used in music transcription [22], onset detection [21], boundary detection [26] and chord recognition [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "In contrast, 2D convolution can learn both temporal and spectral structures and has already been used in music transcription [22], onset detection [21], boundary detection [26] and chord recognition [10].", "startOffset": 199, "endOffset": 203}, {"referenceID": 1, "context": "In crowd-sourced music tag datasets [2,13], most of the tags are false(0) for most of the clips, which makes accuracy or mean square error inappropriate as a measure.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "In crowd-sourced music tag datasets [2,13], most of the tags are false(0) for most of the clips, which makes accuracy or mean square error inappropriate as a measure.", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "Rectified Linear Unit (ReLU) is used as an activation function in every convolutional layer except the output layer, which uses Sigmoid to squeeze the output within [0, 1].", "startOffset": 165, "endOffset": 171}, {"referenceID": 10, "context": "Batch Normalisation is added after every convolution and before activation [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "5 is added after every max-pooling layer [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "Compared to [28] and [18], the proposed system takes advantages of convolutional networks, which do not require any pre-training but fully trained in a supervised fashion.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "Compared to [28] and [18], the proposed system takes advantages of convolutional networks, which do not require any pre-training but fully trained in a supervised fashion.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "The architecture of [6] may be the most similar to ours.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "Results from many 3s clips are averaged in [6] to obtain the final prediction.", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "Two datasets were used to evaluate the proposed system, the MagnaTagATune dataset [13] and the Million Song Dataset (MSD) [2].", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "Two datasets were used to evaluate the proposed system, the MagnaTagATune dataset [13] and the Million Song Dataset (MSD) [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "The MagnaTagATune dataset has been relatively popular for content-based tagging, but similar performances from recent works [5,6,18,28] seem to suggest that performances are saturated, i.", "startOffset": 124, "endOffset": 135}, {"referenceID": 5, "context": "The MagnaTagATune dataset has been relatively popular for content-based tagging, but similar performances from recent works [5,6,18,28] seem to suggest that performances are saturated, i.", "startOffset": 124, "endOffset": 135}, {"referenceID": 16, "context": "The MagnaTagATune dataset has been relatively popular for content-based tagging, but similar performances from recent works [5,6,18,28] seem to suggest that performances are saturated, i.", "startOffset": 124, "endOffset": 135}, {"referenceID": 26, "context": "The MagnaTagATune dataset has been relatively popular for content-based tagging, but similar performances from recent works [5,6,18,28] seem to suggest that performances are saturated, i.", "startOffset": 124, "endOffset": 135}, {"referenceID": 11, "context": "We used ADAM adaptive optimisation [12] on Keras [4] and Theano [1] framework during the experiments.", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "We used ADAM adaptive optimisation [12] on Keras [4] and Theano [1] framework during the experiments.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "We used ADAM adaptive optimisation [12] on Keras [4] and Theano [1] framework during the experiments.", "startOffset": 64, "endOffset": 67}, {"referenceID": 16, "context": "2015, Bag of features and RBM [18] .", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "2014, 1D convolutions [6] .", "startOffset": 22, "endOffset": 25}, {"referenceID": 26, "context": "2014, Transferred learning [28] .", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "2012, Multi-scale approach [5] .", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "2011, Pooling MFCC [8] .", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "This result is aligned with the preferences of mel-spectrograms over STFT on automatic tagging [5,6,18,27].", "startOffset": 95, "endOffset": 106}, {"referenceID": 5, "context": "This result is aligned with the preferences of mel-spectrograms over STFT on automatic tagging [5,6,18,27].", "startOffset": 95, "endOffset": 106}, {"referenceID": 16, "context": "This result is aligned with the preferences of mel-spectrograms over STFT on automatic tagging [5,6,18,27].", "startOffset": 95, "endOffset": 106}, {"referenceID": 25, "context": "This result is aligned with the preferences of mel-spectrograms over STFT on automatic tagging [5,6,18,27].", "startOffset": 95, "endOffset": 106}, {"referenceID": 8, "context": "The structures of DNNs need to be designed for easier training when there are a larger number of layers [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "In practice, this approach is limited by computational resources and therefore very deep structures may need to be designed to motivate efficient training, for instance, using deep residual networks [9].", "startOffset": 199, "endOffset": 202}], "year": 2016, "abstractText": "We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.", "creator": "LaTeX with hyperref package"}}}