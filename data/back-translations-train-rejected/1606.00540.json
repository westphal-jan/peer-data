{"id": "1606.00540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Multi-pretrained Deep Neural Network", "abstract": "Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned.", "histories": [["v1", "Thu, 2 Jun 2016 05:39:54 GMT  (317kb)", "http://arxiv.org/abs/1606.00540v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["zhen hu", "zhuyin xue", "tong cui", "shiqiang zong", "chenglong he"], "accepted": false, "id": "1606.00540"}, "pdf": {"name": "1606.00540.pdf", "metadata": {"source": "CRF", "title": "Multi-pretrained Deep Neural Network", "authors": ["Zhen Hu", "Zhuyin Xue", "Tong Cui", "Shiqiang Zong", "Chenglong He"], "emails": ["49859211@qq.com"], "sections": [{"heading": null, "text": "Model is Deep Belief Network (DBN). Optimization formulas differ during the pre-training process for different pre-training models. In this paper, we investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as a pre-training model. Experimental results show that DBN gets a better initial model. However, after the fine-tuning process, the model converges to a relatively inferior model. However, after being pre-trained for the second time by SDA, it converges to a better model when it is refined."}, {"heading": "Introduction", "text": "In the neeisrcnlhteeSrteeeeteeteeteerrrrrrrllrrrlrrlrrlrlrrrlrrlrrrrlrrrrlrrrlrrrrlrrrlrrrrlrrrrlrrrrlrrrlrrrlrrrlrrrlrrrlrrrrlrrrrlrrrrlrrrlrrrlrrrlrrlrlrrlrrlrrlrrrlrrlrrlrrlrrlrlrrlrlrrrlrlrlrrlrlrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrlrlrlrrrlrlrrlrrlrrrrlrlrlrrlrrrrrlrrrrlrrrrrlrrrrrrlrrrrrrrrrlrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "Model", "text": "In this section we presented our proposed model. Since our model was based on RBM and PCS, we first presented these two models."}, {"heading": "Restricted Boltzmann Machine", "text": "In the RBM, the visible nodes were divided into visible nodes and hidden nodes. The visible nodes recorded the original data as input and the hidden nodes were not directly connected to the input. The value of the visible and hidden nodes was defined by v and h. In the RBM, an energy function was defined as E (,) = hT (1), where W was the weight matrix, b and c were the visible and hidden distortions. The probability function was defined as L (v) = log (,) (,) (,) (,) (2). It is easy to see that the values of the hidden nodes were conditionally independent of visible nodes and the values of the visible nodes were conditionally independent of the condition (,) (10) hidden nodes."}, {"heading": "De-noising Auto-encoder", "text": "For AE, the values of hidden nodes were calculated using the following equation: = Sigm (), (3) where Sigm denotes the nonlinear sigmoid function, the reconstruction process was accomplished by = Sigm (+ c), (4) The optimization problem for AE was defined as Asmin,,, | | 2. (5) To obtain the corrupt input, some elements first set to 0. After the reconstructed input was encoded and decrypted as AE, it was compared with the reconstructed input, and their experiment showed that the network was more general by corrupting the input."}, {"heading": "Multi-pretrained Deep Neural Network", "text": "Our proposed model, MPDNN, was based on RBM and PCS. We use these models to pre-train our network twice, and the experimental results showed the difference. The optimization problem for RBM was a problem of probability maximization and the problem for PCS was a problem of error minimization. Therefore, PCS was more loyal to the original input and RBM can be varied. This difference cannot be compensated by the fine-tuning process."}, {"heading": "Experiment", "text": "In this section we report on the results of our experiments."}, {"heading": "Experiment Setup", "text": "Our proposed model, MPDNN, was based on RBM and DAE. We used these models to pre-train our network twice, and the experimental results showed the difference. The optimization problem for RBM was a problem of probability maximization, and the problem for DAE was a problem of error minimization. So DAE was more faithful to the original input, and RBM can be varied. This difference cannot be erased by the fine-tuning process. In our paper, we considered a widespread benchmark problem, the problem of handwritten number recognition of the MNIST. In this problem, we chose 50,000 images as a training set, 10,000 images as a validation set, and 10,000 as a testset.We proposed a 4-layer network to detect the digits. The number of hidden nodes in all 3 hidden layers was 1,000. In our experiment, we implemented four models, -MPDNN-SS, the network was pre-trained twice by DAE, by the network -MPDND-DAM, the -MPD-DAM was."}, {"heading": "Experiment Results", "text": "The results showed that MPDNN-DD was boring in the initial state, as the variance was the lowest for all 50 runs. MPDNN-DS performed best in terms of accuracy among all models. Once pre-trained by PCS, whether PCS or RBM, the network performed worse, either with higher error rate or greater variance. Since the pre-training process ran in layers, MPDNN-DD was equivalent to RBM in subsequent training periods, and MPDNN-SS was equivalent to DAE in more training periods. Figure 1 showed the error rate curve along with the fine-tuning of the iteration. We found that MPDNN-DX (X = D, S) performed better from the beginning of fine-tuning to the end than MPDNN-SX (X = D, S), although MPDNN-DS was worse than MPN-SS in the second phase."}, {"heading": "Summary", "text": "The experimental results showed that RBM was good at finding a better starting state for the neural network, while this starting state can get stuck with fine-tuning the model. By pre-training the model for the second time with PCS, the model can perform worse than the model that was pre-trained at the beginning of RBM fine-tuning, and eventually overtake the pre-trained model as a fine-tuning process. In our work, we compared only two models, namely RBM and PCS. We will be testing more models in the near future to find the advantage of different models before training, so we can select the most suitable model before training when we train our network."}, {"heading": "Acknowledgement", "text": "The research was supported by the National Natural Science Foundation of China under grant number 61402426, training program of the Major Research Plan of the National Natural Science Foundation of China (number 91546103) and partially supported by the Collaborative Innovation Center of Social Safety Science and Technology."}], "references": [{"title": "Convex optimization. Cambridge university press", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On gibbs sampling for state space models", "author": ["C.K. Carter", "R. Kohn"], "venue": "Biometrika 81(3),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "A pendulum swung too far", "author": ["K. Church"], "venue": "Linguistic Issues in Language Technology 6(5),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Global optimization of statistical functions with simulated annealing", "author": ["W.L. Goffe", "G.D. Ferrier", "J. Rogers"], "venue": "Journal of econometrics 60(1-2),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Support vector machines", "author": ["M.A. Hearst", "S.T. Dumais", "E. Osman", "J. Platt", "B. Scholkopf"], "venue": "Intelligent Systems and their Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets. Neural computation", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences 79(8),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1982}, {"title": "Neural networks for control systemsa survey", "author": ["K.J. Hunt", "D. Sbarbaro", "R. Z \u0307 bikowski", "P.J. Gawthrop"], "venue": "Automatica 28(6),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the 24th international conference on Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "State-of-the-art surveythe traveling salesman problem: A neural network perspective", "author": ["J.Y. Potvin"], "venue": "ORSA Journal on Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Learning internal representation by back propagation. Parallel distributed processing: exploration in the microstructure of cognition", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Adaptive probabilities of crossover and mutation in genetic algorithms", "author": ["M. Srinivas", "L.M. Patnaik"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on 24(4),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Genetic algorithms and neural networks: Optimizing connections and connectivity. Parallel computing", "author": ["D. Whitley", "T. Starkweather", "C. Bogart"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1990}], "referenceMentions": [{"referenceID": 6, "context": "In 1982 Hopfield proposed the Hopfield Network[8] and proved that neural network can be used to simulate the XOR function.", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 222, "endOffset": 225}, {"referenceID": 12, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "However, the training process of neural network is a non-convex problem while the BP algorithm is intrinsically a gradient-descent algorithm which is guaranteed to converge to a global optima when solving convex problems[1].", "startOffset": 220, "endOffset": 223}, {"referenceID": 3, "context": "Some researchers proposed improved algorithms such as simulated annealing[4], Genetic algorithm[19] to reduce the training epoches.", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Some researchers proposed improved algorithms such as simulated annealing[4], Genetic algorithm[19] to reduce the training epoches.", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "What\u2019s more, as the powerful expressive ability of neural network, a network in the global optimal state maybe severely over-fitting[17], and even worse than a network in the local optimal state.", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "As the fast development fast-learnable model such as Support Vector Machine[6], researchers were more and more considerable about the training data[3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "As the fast development fast-learnable model such as Support Vector Machine[6], researchers were more and more considerable about the training data[3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 10, "context": "al introduced the parameter bonding strategy into the training process of neural network[12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "al proposed the model named as LeNet5[12] which was a 8-layer neural network.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "al proposed the pre-training strategy[7].", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "al proposed the convolutional RBM[13] while Larochelle et.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "al proposed the auto-encoder (AE) [11].", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Some researchers improved AE, such as De-noising Auto-encoder (DAE)[18], Contractive Autoencoder (CAE)[15] and so on.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Some researchers improved AE, such as De-noising Auto-encoder (DAE)[18], Contractive Autoencoder (CAE)[15] and so on.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "It\u2019s easy to see that values of hidden nodes were conditionally independent on condition of visible nodes and values of visible nodes were conditionally independent on condition of hidden nodes[10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": "So equation (2) can be optimized by gibbs sampling[2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "Hinton proposed that the gibbs sampling process can be only run for once to get a well enough initial state[7].", "startOffset": 107, "endOffset": 110}], "year": 2016, "abstractText": "Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned.", "creator": "Microsoft\u00ae Word 2010"}}}