{"id": "1606.00575", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks", "abstract": "In recent year, parallel implementations have been used to speed up the training of deep neural networks (DNN). Typically, the parameters of the local models are periodically communicated and averaged to get a global model until the training curve converges (denoted as MA-DNN). However, since DNN is a highly non-convex model, the global model obtained by averaging parameters does not have guarantee on its performance improvement over the local models and might even be worse than the average performance of the local models, which leads to the slow-down of convergence and the decrease of the final performance. To tackle this problem, we propose a new parallel training method called \\emph{Ensemble-Compression} (denoted as EC-DNN). Specifically, we propose to aggregate the local models by ensemble, i.e., the outputs of the local models are averaged instead of the parameters. Considering that the widely used loss functions are convex to the output of the model, the performance of the global model obtained in this way is guaranteed to be at least as good as the average performance of local models. However, the size of the global model will increase after each ensemble and may explode after multiple rounds of ensembles. Thus, we conduct model compression after each ensemble, to ensure the size of the global model to be the same as the local models. We conducted experiments on a benchmark dataset. The experimental results demonstrate that our proposed EC-DNN can stably achieve better performance than MA-DNN.", "histories": [["v1", "Thu, 2 Jun 2016 08:10:10 GMT  (59kb,D)", "https://arxiv.org/abs/1606.00575v1", null], ["v2", "Tue, 18 Jul 2017 08:50:05 GMT  (100kb,D)", "http://arxiv.org/abs/1606.00575v2", "ECML 2017"]], "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.NE", "authors": ["shizhao sun", "wei chen", "jiang bian", "xiaoguang liu", "tie-yan liu"], "accepted": false, "id": "1606.00575"}, "pdf": {"name": "1606.00575.pdf", "metadata": {"source": "CRF", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks", "authors": ["Shizhao Sun", "Wei Chen", "Jiang Bian", "Xiaoguang Liu", "Tie-Yan Liu"], "emails": ["sunshizhao@mail.nankai.edu.cn,", "wche@microsoft.com,", "jiabia@microsoft.com,", "liuxg@nbjl.nankai.edu.cn,", "tyliu@microsoft.com."], "sections": [{"heading": null, "text": "Keywords: Parallel Machine Learning, Distributed Machine Learning, Deep Learning, Ensemble Method."}, {"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "2 Preliminary: Parallel Training of DNN", "text": "In the sequence of this paper, we designate a DNN model as f (w) 3. where w represents the parameters of this DNN model. In addition, we designate the results of the model f (w) at input x as f (w; x) = (f (w; x, 1),.., f (w; x, C), where C denotes the number of classes and f (w; x, c) denotes the performance (i.e., the score) for the c-th class. DNN is a highly non-convex model due to the non-linear activations and poolings after many layers. In the parallel training of DNN, we assume that there are K workers and each of them holds a local dataset Dk = {xk, 1, yk, 1),. (xk, mk, yk, mk)} with the size mk."}, {"heading": "3 Model Aggregation: MA vs. Ensemble", "text": "In this section we will first show why the MA method can not guarantee to produce a global model with better performance than local models. However, if the model is convex w.r.t. the parameters and the loss are convex w.r.t., the performance of the global model produced by MA is guaranteed to be no worse than the average performance of local DNG models. This is because if f (\u00b7) is a convex model, we have a convex model, L (f) convex; x), y) = L (1K-K-k = 1 f (wtk; x), y. In addition, if the loss is also convex w.r.t. the model models we have, L (1K-K-k = 1 f) that we have."}, {"heading": "4 EC-DNN", "text": "In this section we first present the EC-DNN framework that ensemble uses for model aggregation, then we introduce a specific implementation of EC-DNN that uses distillation for compression, and finally we discuss the temporal complexity of EC-DNN and the comparison with traditional ensemble methods."}, {"heading": "4.1 Framework", "text": "The individual models of the local model of the EC-DNN are the same as those of the MA-DNN, in which the local model of SGD is updated. Specifically, it is the local model of the local model of the local model from Wtk to W + 1. The local model of the local model of the EC-DNN is the same as the local model in which the local model of SGD is updated. Specifically, this means that workers update their local model from Wtk to W + 1 by minimizing the training process with SGD, i.e., wt + 1k = w t t (L (f), xk), where it is the learning rate."}, {"heading": "4.2 Implementations", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "4.3 Time Complexity", "text": "We compare the temporal complexity of MA-DNN and EC-DNN from two aspects: 1. Communication time: Parallel DNN training processes are usually sensitive to communication frequency \u03c4. Different parallelization frameworks lead to different optimal \u03c4. In particular, EC-DNN prefers larger \u03c4 compared to MADNN. In essence, less frequent communication between workers can lead to more diverse local models, which will lead to better overall performance for EC-DNN. On the other hand, very different local models may indicate a higher probability that local models are in the neighborhood of different local optimizations, so that the global model in MA-DNN is more likely to perform worse than local models. The poor performance of the global model slows down convergence and harms the performance of the final model. Therefore, EC-DNN provides less communication time than MA-DNN.2. Calculation time: According to the analysis in Sec 4.2, EC-DNN does not consume any additional compression time as the compression process is controlled roughly in time."}, {"heading": "4.4 Comparison with Traditional Ensemble Methods", "text": "Traditional ensemble methods for DNN [25,5] typically train multiple DNN models independently of each other, without communication, and ultimately turn them into an entire ensemble. We call this method E-DNN. E-DNN has been proposed to improve the accuracy of DNN models by reducing variance, and there is no need to train base models with parallelization frameworks. In contrast, EC-DNN is a parallel algorithm that aims to train DNN models faster without losing accuracy by using a machine cluster. Although E-DNN can be considered a special case of EC-DNN, with only one final communication and no compression process, it becomes more powerful than E-DNN through intermediate communication in EC-DNN E-DNN. The reasons are the following: 1) Local workers have different local data, communication during training will contribute to making local models compatible with the entire training data, by allowing the EC-DNN to be continuously optimized according to each local process."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "Our experiments are conducted on a GPU cluster connected to an InfiniBand network, from which each machine is equipped with two Nvdia-K20 GPU processors, one GPU processor corresponding to a local worker. Data. We conducted experiments on public data sets selected by CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge), the training images are rotated horizontally but not truncated, and the test data is neither flipped nor cropped. on CIFAR-10 and CIFAR-100, we use NiN [19], a 9-layer convolutionary network. On ImageNet, we use GoogLeNet, a 22-layer convolutionary network."}, {"heading": "5.2 Compared Methods", "text": "We perform performance comparisons for four methods: - S-DNN refers to the sequential training on a GPU until convergence [19,25]. - E-DNN refers to the method that trains local models independently and only interacts with local models at the end of the training [25,5]. - MA-DNN refers to the parallel DNN training frame with aggregation by averaging model parameters [27,20,6,26,2,3]. - EC-DNN refers to the parallel DNN training frame with aggregation by averaging model outputs. EC-DNN applies compression to the compression of all experiments in this paper. Furthermore, we use EC-DNNL, MA-DNNL and E-DNNL to designate the corresponding methods that consider the local model with the lowest training loss as the final model and use EC-DNNG, MA-DNNG and E-DNNG to represent the respective methods that represent the final model for the local model (i.e. the EC for the local model and the DNN)."}, {"heading": "5.3 Experimental Results", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we propose EC-DNN, a new parallel formation system for DNN based on ensemble compression. Compared to the traditional MA-DNN approach, in which the parameters of different local models are averaged, our proposed method uses the ensemble method to aggregate local models. In this way, we can guarantee that the error of the global model in EC-DNN is above the limit due to the average error of the local models and can consistently achieve better performance than MA-DNN. In the future, we plan to consider other compression methods for EC-DNN. We will also examine the theoretical properties of the ensemble method, the compression method and the entire EC-DNN framework."}, {"heading": "7 Acknowledgments", "text": "This work is partially supported by NSF in China (grant number: 61373018, 61602266, 11550110491) and NSF in Tianjin (grant number: 4117JCYBJC15300)."}], "references": [{"title": "Model compression", "author": ["C. Bucilua", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM Conference on Knowledge Discovery and Data Mining. pp. 535\u2013 541. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. pp. 5880\u20135884. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "Proceedings of the 32st International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642\u20133649. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Le", "Q.V"], "venue": "Advances in Neural Information Processing Systems. pp. 1223\u20131231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51(1), 107\u2013113", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N de Freitas"], "venue": "Advances in Neural Information Processing Systems. pp. 2148\u20132156", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems. pp. 1269\u20131277", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems 28. pp. 1135\u20131143", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dual learning for machine translation", "author": ["D. He", "Y. Xia", "T. Qin", "L. Wang", "N. Yu", "T. Liu", "W.Y. Ma"], "venue": "Advances in Neural Information Processing Systems 29, pp. 820\u2013828", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 1026\u20131034", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. rep., University of Toronto", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Measures of diversity in classifier ensembles", "author": ["L. Kuncheva", "C. Whitaker"], "venue": "Machine Learning, pp. 181\u2013207", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.Y. Su"], "venue": "11th USENIX Symposium on Operating Systems Design and Implementation. pp. 583\u2013598", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Network in network", "author": ["L. Min", "C. Qiang", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. pp. 2754\u20132761. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV) 115(3), 211\u2013252", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with ensembles: How overfitting can be useful", "author": ["P. Sollich", "A. Krogh"], "venue": "Advances in Neural Information Processing Systems, vol. 8, pp. 190\u2013196", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28, pp. 685\u2013693", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving deep neural network acoustic models using generalized maxout networks", "author": ["X. Zhang", "J. Trmal", "D. Povey", "S. Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 215\u2013219. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Recent rapid development of deep neural networks (DNN) has demonstrated that its great success mainly comes from big data and big models [25,13].", "startOffset": 137, "endOffset": 144}, {"referenceID": 12, "context": "Recent rapid development of deep neural networks (DNN) has demonstrated that its great success mainly comes from big data and big models [25,13].", "startOffset": 137, "endOffset": 144}, {"referenceID": 6, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 77, "endOffset": 80}, {"referenceID": 17, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 101, "endOffset": 107}, {"referenceID": 26, "context": ", by averaging the identical parameter of each local models [27,20].", "startOffset": 60, "endOffset": 67}, {"referenceID": 19, "context": ", by averaging the identical parameter of each local models [27,20].", "startOffset": 60, "endOffset": 67}, {"referenceID": 24, "context": "Empirical evidence in [25,5] even show that the ensemble model of DNN, i.", "startOffset": 22, "endOffset": 28}, {"referenceID": 4, "context": "Empirical evidence in [25,5] even show that the ensemble model of DNN, i.", "startOffset": 22, "endOffset": 28}, {"referenceID": 16, "context": "According to previous theoretical and empirical studies [17,24], ensemble model tend to yield better results when there exists a significant diversity among local models.", "startOffset": 56, "endOffset": 63}, {"referenceID": 23, "context": "According to previous theoretical and empirical studies [17,24], ensemble model tend to yield better results when there exists a significant diversity among local models.", "startOffset": 56, "endOffset": 63}, {"referenceID": 0, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 21, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 13, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 26, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 5, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 25, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 1, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "NG-SGD [20] proposes an approximate and efficient implementation of Natural Gradient for SGD (NGSGD) to improve the performance of MA-DNN.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "EASGD [26] improves MA-DNN by adding an elastic force which links the weights of the local models with the weights of the global model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "BMUF [3] leverages data parallelism and blockwise model-update filtering to improve the speedup of MA-DNN.", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "3 and previous studies [20,3] also revealed such problem.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "3 and previous studies [20,3] also revealed such problem.", "startOffset": 23, "endOffset": 29}, {"referenceID": 1, "context": "3 As shown in [2], MA-DNN in synchronous case converges faster and achieves better test accuracy than that in asynchronous case.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "In order to improve the performance of ensemble, it is necessary to generate diverse local models other than merely accurate ones [17,24].", "startOffset": 130, "endOffset": 137}, {"referenceID": 23, "context": "In order to improve the performance of ensemble, it is necessary to generate diverse local models other than merely accurate ones [17,24].", "startOffset": 130, "endOffset": 137}, {"referenceID": 0, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 21, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 13, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 9, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 7, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 8, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 20, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 10, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 11, "context": "dual learning [12], to improve the performance of both tasks simultaneously.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Traditional ensemble methods for DNN [25,5] usually first train several DNN models independently without communication and make ensemble of them in the end.", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Traditional ensemble methods for DNN [25,5] usually first train several DNN models independently without communication and make ensemble of them in the end.", "startOffset": 37, "endOffset": 43}, {"referenceID": 15, "context": "We conducted experiments on public datasets CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge) [23].", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "We conducted experiments on public datasets CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge) [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "On CIFAR-10 and CIFAR-100, we employ NiN [19], a 9-layer convolutional network.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "On ImageNet, we use GoogLeNet [25], a 22-layer convolutional network.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "All the experiments are implemented using Caffe [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "\u2013 S-DNN denotes the sequential training on one GPU until convergence [19,25].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "\u2013 S-DNN denotes the sequential training on one GPU until convergence [19,25].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "\u2013 E-DNN denotes the method that trains local models independently and makes ensemble of the local models merely at the end of the training [25,5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "\u2013 E-DNN denotes the method that trains local models independently and makes ensemble of the local models merely at the end of the training [25,5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 26, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 19, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 5, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 25, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 1, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 2, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}], "year": 2017, "abstractText": "Parallelization framework has become a necessity to speed up the training of deep neural networks (DNN) recently. Such framework typically employs the Model Average approach, denoted as MA-DNN, in which parallel workers conduct respective training based on their own local data while the parameters of local models are periodically communicated and averaged to obtain a global model which serves as the new start of local models. However, since DNN is a highly non-convex model, averaging parameters cannot ensure that such global model can perform better than those local models. To tackle this problem, we introduce a new parallel training framework called Ensemble-Compression, denoted as EC-DNN. In this framework, we propose to aggregate the local models by ensemble, i.e., averaging the outputs of local models instead of the parameters. As most of prevalent loss functions are convex to the output of DNN, the performance of ensemble-based global model is guaranteed to be at least as good as the average performance of local models. However, a big challenge lies in the explosion of model size since each round of ensemble can give rise to multiple times size increment. Thus, we carry out model compression after each ensemble, specialized by a distillation based method in this paper, to reduce the size of the global model to be the same as the local ones. Our experimental results demonstrate the prominent advantage of EC-DNN over MA-DNN in terms of both accuracy and speedup.", "creator": "LaTeX with hyperref package"}}}