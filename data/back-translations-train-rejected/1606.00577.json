{"id": "1606.00577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge sources", "abstract": "A popular approach to topic modeling involves extracting co-occurring n-grams of a corpus into semantic themes. The set of n-grams in a theme represents an underlying topic, but most topic modeling approaches are not able to label these sets of words with a single n-gram. Such labels are useful for topic identification in summarization systems. This paper introduces a novel approach to labeling a group of n-grams comprising an individual topic. The approach taken is to complement the existing topic distributions over words with a known distribution based on a predefined set of topics. This is done by integrating existing labeled knowledge sources representing known potential topics into the probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. In the inference these modified distributions guide the convergence of the latent topics to conform with the complementary distributions. This approach ensures that the topic inference process is consistent with existing knowledge. The label assignment from the complementary knowledge sources are then transferred to the latent topics of the corpus. The results show both accurate label assignment to topics as well as improved topic generation than those obtained using various labeling approaches of Latent Dirichlet allocation (LDA) when compared by pointwise mutual information (PMI) assessment.", "histories": [["v1", "Thu, 2 Jun 2016 08:15:15 GMT  (1031kb)", "http://arxiv.org/abs/1606.00577v1", null], ["v2", "Fri, 4 Nov 2016 05:15:36 GMT  (258kb,D)", "http://arxiv.org/abs/1606.00577v2", null], ["v3", "Wed, 17 May 2017 21:03:06 GMT  (259kb,D)", "http://arxiv.org/abs/1606.00577v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["justin wood", "patrick tan", "wei wang", "corey arnold"], "accepted": false, "id": "1606.00577"}, "pdf": {"name": "1606.00577.pdf", "metadata": {"source": "CRF", "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge sources", "authors": ["Justin Wood"], "emails": ["juwood03@ucla.edu"], "sections": [{"heading": null, "text": "Although the distribution of themes in a single label has been shown to be useful and generally representative of a particular topic, the assignment of terms to themes can often be limited to handling. Identifying theme names in a general sense is useful to capture different terms in a single label. Terms such as pencil, pack, eraser, and books can be assigned to the label. \"School Supplies\" Adding descriptive semantics to each topic, especially those without domain knowledge, to understand themes that are achieved through themes."}, {"heading": "II. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dirichlet Distribution", "text": "The dirichlet distribution is a distribution of probability mass functions with a certain number of atoms and is often used in Bayesian models. A property of the dirichlet, which is often used in inferences to Bayesian models, is conjugation to multinomial distribution, which allows the back of a random variable with a multinomial probability and a dirichlet before it is also a dirichlet distribution. The parameters are given as a vector, which is denoted by \u03b1. The probability density function for a given probability mass function (PMF) and the parameter vector \u03b1 of length J is defined as: (,) = () () () A sample from the dirichlet distribution produces a PMF, which is parameterized by \u03b1. Choosing a certain set of \u03b1 values affects the result of the generated PMF. If all (symmetric values are the same (symmetric parameters), then all approaches to the PMF distribution are considered uniform. \""}, {"heading": "B. Latent Dirichlet Allocation", "text": "As we extend the LDA model in our proposed approach, it is worth giving a brief overview of the algorithm and model of LDA.LDA is a hierarchical Bayes model that uses Dirichlet Priors to estimate the model's persistent latent variables. At a high level, LDA is based on a generative model in which each word of an input document is selected from a corpus by first selecting a topic that corresponds to that word, and then the word from a topic to the word distribution. Each topic to the word distribution and word to the topic distribution is taken from its respective dirichlet distribution. The formal definition of the generative algorithm over a corpus is: 1. For each of the K topics applies:"}, {"heading": "2. Choose \ud835\udf19\ud835\udc58\u223c Dir(\u03b2)", "text": "3. For each of the D documents (d):"}, {"heading": "4. Choose Nd \u223c Poisson(\u03be)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5. Choose \u03b8d \u223c Dir(\u03b1)", "text": "For each of the words in the list, there is a separate term."}, {"heading": "A. Bijective Mapping", "text": "In the simplest approach, the source LDA model assumes that there is a 1-to-1 mapping between a known set of topics and the topics used to create a corpus. The generative model then assumes that instead of matching the topics to word distributions based on the sample from the Dirichlet distribution, a set of K distributions is given as input and scanned for a specific symbol position after each topic assignment. The generation process for a corpus that was adapted during the creation of distributions from the traditional LDA generative model looks like this (for the sake of brevity, only the relevant parts of the existing LDA algorithm are displayed): 1. For each of the K topics applies: 2. Expk (Xk, 1, Xk, 2,..., Xk, V)"}, {"heading": "3. Choose \ud835\udf19\ud835\udc58 \u223c Dir(\u03b4k)", "text": "Where (Xk, 1, Xk, 2,..., Xk, V) represents the knowledge source hyperparameter for the kth knowledge source document. The generative model differs from the traditional LDA model only in how each \u03c6 is built. Therefore, the derivative for inferences is also a simple factor. In order to approximate the distributions for \u03b8 and \u03c6, a collapsed Gibbs sampler can approximate the z-mappings as follows: P (w | zi = j, z \u2212 i, wi \u2212 i) P (zi = j, w \u2212 i) c c (c = j, c \u2212 i) P (zi = j | z \u2212 i) \"c\" c c (c \u2212 i) b (z \u2212 i b (z \u2212 i) b (z \u2212 i) c c c (c) c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c) c (c) c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c) c (c) c) c) c) c (c) c (c) c) c) c) c (c) c) c (c) c) c) c) c (c) c) c) c (c) c) c) c (c) c) c) c) c (c) c) c) c) c) c) c)"}, {"heading": "B. Known Mixture of Topics", "text": "The next model assumes that the theme model knows how many themes are source themes (as well as their source distributions) and how many regular non-source themes are. The previous approach works quite well in this situation, as a non-source theme has a symmetrical beta parameter that captures assignments that have not been assigned due to a small number of source themes.The resulting model helps to solve the existing problems of the bijective model and requires only a minor input into the existing generative model. The resulting model works quite well with the bijective model, as the symmetrical dirichlet can be used beforehand to guide a theme towards a general unlabeled theme or source theme.The model changes, as shown below, with a slight change in the generative algorithm and collapsed Gibbs patterns. 1. For each of the K themes: 2. if k \u2264 T then3."}, {"heading": "6. Choose \u03c6k \u223c Dir(\u03b4k)", "text": "The change required for the collapsed Gibbs sample is then: andThis approach has the advantage of allowing a mix of known and unknown topics, but problems still arise, as the Dirichlet distributions may be too restrictive for source distribution."}, {"heading": "C. Source-LDA", "text": "With the help of the counts as hyperparameters, the resulting divergence in the distribution of knowledge from the source of the distribution of knowledge is attributed to the form of the distribution of knowledge. However, this may run counter to the objective of improving existing subject modeling. In order to influence the distribution of knowledge, it is entirely plausible that there may be a divergence between the two distributions. In other words, there does not necessarily have to be a need to strictly monitor the respective distribution of knowledge. Supplementing the existing generative model only slightly changes the existing generative model and allows a variation for each individual \u03b2i, which frees us from an overly restrictive link to the associated distribution of knowledge."}, {"heading": "5. Choose \u03bbt \u223c N(\u00b5,\u03c3)", "text": "6. \u03b4t \u2190 [(Xt, 1) g (\u03bbt), (Xt, 2) g (\u03bbt),..., (Xt, V) g (\u03bbt)]"}, {"heading": "7. Choose \u03c6t \u223c Dir(\u03b4t)", "text": "8. For each of the D documents d:"}, {"heading": "9. Choose Nd \u223c Poisson(\u03be)", "text": "For each of the Nd words wn, d: 12. select zn, d \u0445 multinomial (\u03b8) 13. choose yourself, d \u0445 multinomial (\u03c6zn, d) The complete broken Gibbs sampling algorithm is inalgorithm 1.4) Analysis: By using a clustering algorithm or the threshold of the topic, the broken Gibbs algorithm is guaranteed to produce K topics. Runtime is a function of the number of iterations I, average words per document davg, number of documents D, number of topics T and number of approximation steps A, and is O (I \u00d7 Davg \u00d7 D \u00d7 T \u00d7 A). This differs only from the traditional collapsed Gibbs sampling in LDA by an increase in (T \u2212 K) A. Butda we have built the approach to potentially have a large T \u2011 K difference that can have a significant impact on current events."}, {"heading": "IV. EVALUATION", "text": "In order to test the results of the source LDA algorithm, we set up an experiment that will be tested against traditional LDA. Below, we describe in more detail the test setups and metrics used to compare the results."}, {"heading": "A. Reuters Newswire Analysis", "text": "1) Experimental Setup: Source-LDA, LDA, EDA, and CTM were conducted against the Reuters-21578 Newswire Collection. This collection contains documents from the 1987 Reuters Newswire. The data set contains 21,578 articles, under a large set of categories. An important feature of the data set are a number of predetermined categories that we can use for our topic labeling. We select 80 different categories as our superset for possible knowledge bases, including broad categories such as shipping, interest rates, and trade, as well as more refined categories such as rubber, zinc, and coffee. Our choice to apply our topic labeling method to this data set is due to the fact that the Reuters dataset is commonly used for information retrieval and text categorization applications. Due to its widespread use, it can significantly help us compare our results with other studies."}, {"heading": "B. Wikipedia Corpus", "text": "1) Experimental Setup: A corpus of Wikipedia vocabulary articles was created by following the steps of the generative model for source LDA, where the K topics chosen are a subset of a larger collection of Wikipedia topics. The number of topics (K) was given as 100, selected from a total collection of 578 topics (B), then the number of documents (D) as 200, and the average number of words (Davg) as 300. After these 200 documents were generated, the topic mappings were recorded and used as the basis for truth measurement, the word mappings were used as a corpus, and three different topic models were applied to these documents. The first was the bijective mapping model, in which only the topics in the generative model were used as input, the second topic model was LDA, and the third was source LDA model. The SourceLDA model was set to 1.0 and set to 0.0."}, {"heading": "V. RELATED WORK", "text": "There is much available literature on the approach proposed in this paper. These methods are mainly extensions of the LDA and supplement the original model with improvements such as topic labeling, integration with context information, and hierarchical modeling."}, {"heading": "A. Topic Labeling", "text": "In the early research phase, labels were often hand-generated [20] - [23]. Although manual labeling can produce a more comprehensible and precise semantics of a topic, it takes a lot of human effort and is prone to subjectivity [24]. The model is interpreted by selecting the top words in the distribution [1], [20], [24], [25]. The Topics over Time (TOT) model implements continuous topics with each topic [24]. The model was applied in three types of datasets, and the results show more precise topics and better timestamp predictions. However, the interpretation of topics manually and post-hoc labeling is time consuming and subjective."}, {"heading": "B. Supervised Labeling", "text": "The approach includes a response variable in the LDA model to obtain latent topics that potentially provide an optimal prediction of the response variable of a new, unlabeled document. Similar to sLDA, Discriminative LDA (DiscLDA), which attempts to solve the same problem as sLDA but differs in approach [14], focuses on the introduction of a class-dependent linear transformation of the topic mix proportions. This transformation matrix was learned using a conditional probability criterion, which has the advantage that both the dimension of the documents in the corpus and the smaller dimensions of the LDA documents are labeled. Both sLDA and DiscLDA allow only one superordinate input that marks a single topic."}, {"heading": "C. Contextual Integration", "text": "An existing approach that takes into account concepts supplied by external sources requires a manual entry of relevant terms [30], and in the topic model, these concepts are then applied to the allocation of topics to a token in a document. Besides this concept, the modeling of a hierarchical approach can also be used to incorporate concepts into a hierarchical structure. [6] This work demonstrates the benefit of incorporating external knowledge into the topic modeling process. An approach that integrates Wikipedia information into the topic modeling differs from the supervised approach in that it only requires an existing Wikipedia article [6]. The assumption in this paper is that in the generative process the topics are selected from the Wikipedia word distributions, and the results show that Wikipedia articles can be used as effective topics in the topic modeling process. Wikipedia has again been shown as the basis for the topic modeling, but for a tangential approach, entity disambiguity."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have described a new methodology for unattended topic modeling with meaningful n-gram labels and provided parallel algorithms to accelerate the follow-up process, using existing marked knowledge sources to influence a topic model so that the labels from these external sources can be used for topics generated by a corpus of interest. In addition, this approach leads to more significant topics generated based on the quality of the external knowledge source. We have tested our methodology against the Reuters-21578 Newswire Collection Corpus for labeling and Wikipedia as external knowledge sources. Analyzing the quality of topic models using PMI shows SourceLDA to improve existing topic models."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "A popular approach to topic modeling involves extracting co-occurring n-grams of a corpus into semantic themes. The set of n-grams in a theme represents an underlying topic, but most topic modeling approaches are not able to label these sets of words with a single n-gram. Such labels are useful for topic identification in summarization systems. This paper introduces a novel approach to labeling a group of n-grams comprising an individual topic. The approach taken is to complement the existing topic distributions over words with a known distribution based on a predefined set of topics. This is done by integrating existing labeled knowledge sources representing known potential topics into the probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. In the inference these modified distributions guide the convergence of the latent topics to conform with the complementary distributions. This approach ensures that the topic inference process is consistent with existing knowledge. The label assignment from the complementary knowledge sources are then transferred to the latent topics of the corpus. The results show both accurate label assignment to topics as well as improved topic generation than those obtained using various labeling approaches of Latent Dirichlet allocation (LDA) when compared by pointwise mutual information (PMI) assessment.", "creator": "Microsoft\u00ae Word 2013"}}}