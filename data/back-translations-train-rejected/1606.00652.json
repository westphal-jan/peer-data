{"id": "1606.00652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Death and Suicide in Universal Artificial Intelligence", "abstract": "Reinforcement learning (RL) is a general paradigm for studying intelligent behaviour, with applications ranging from artificial intelligence to psychology and economics. AIXI is a universal solution to the RL problem; it can learn any computable environment. A technical subtlety of AIXI is that it is defined using a mixture over semimeasures that need not sum to 1, rather than over proper probability measures. In this work we argue that the shortfall of a semimeasure can naturally be interpreted as the agent's estimate of the probability of its death. We formally define death for generally intelligent agents like AIXI, and prove a number of related theorems about their behaviour. Notable discoveries include that agent behaviour can change radically under positive linear transformations of the reward signal (from suicidal to dogmatically self-preserving), and that the agent's posterior belief that it will survive increases over time.", "histories": [["v1", "Thu, 2 Jun 2016 12:48:39 GMT  (21kb)", "http://arxiv.org/abs/1606.00652v1", "Conference: Artificial General Intelligence (AGI) 2016 13 pages, 2 figures"]], "COMMENTS": "Conference: Artificial General Intelligence (AGI) 2016 13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jarryd martin", "tom everitt", "marcus hutter"], "accepted": false, "id": "1606.00652"}, "pdf": {"name": "1606.00652.pdf", "metadata": {"source": "CRF", "title": "Death and Suicide in Universal Artificial Intelligence", "authors": ["Jarryd Martin Tom Everitt", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 6.00 652v 1 [cs.A I] 2 JKeywordsAIXI, universal intelligence, algorithmic information theory, semimass, Solomonoff induction, AI security, death, suicide, suicide attacks"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Preliminaries 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Definitions of Death 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Known Environments: AI\u00b5 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Unknown Environments: AIXI and AI\u03be 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Conclusion 12", "text": "A shorter version of this essay will be presented at AGI-16. [6] \"That suicide is often consistent with interest and our duty to ourselves, no one can question who allows age, illness, or misfortune to make life a burden and make it even worse than annihilation.\" - Hume, Of Suicide (1777)"}, {"heading": "1 Introduction", "text": "A good theoretical understanding of these agents is valuable for several reasons. Firstly, it can lead to principled XI attempts to construct such agents. [10] Secondly, once such agents are constructed, however, it can serve to make their reasoning and behavior more transparent and comprehensible to humans. Thirdly, it can help in the development of strategies to control these agents. The latter challenge has recently received considerable attention in the context of the potential risks these agents pose to human security. [2] It has even been argued that control strategies should be developed before universally intelligent agents are first built. [8] In this context - where we need to think about the behavior of agents in the absence of a complete specification of their implementation - a theoretical understanding of their general characteristics seems indispensable."}, {"heading": "2 Preliminaries", "text": "Let the alphabet X be a finite set of symbols converted to x. (1) Let the alphabet X be a finite set of symbols converted to x. (2) Let the alphabet X be converted to x. (2) Let the alphabet X be converted to x. (2) Let the alphabet X be converted to x. (4) Let the alphabet X be converted to x. (4) Let the alphabet X be converted to x. (4) Let the alphabet X be converted to x. (4) Let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let us let the alphabet X be converted to x. (4) Let the alphabet X be converted to x."}, {"heading": "3 Definitions of Death", "text": "Death as semi-authoritative loss. < < < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as semi-authoritative loss.\" < \"Death as authoritative loss.\" < \"Death as authoritative loss.\" < \"Death as authoritative loss.\" < \"Death as authoritative loss.\" < \"Death as authoritative loss.\""}, {"heading": "4 Known Environments: AI\u00b5", "text": "In this section, we show that the behavior of a universal agent can depend on the reward margin. This is a surprising result, because in a standard RL setup in which the environment is modeled as an appropriate probability quantity (not half measures), the relative value of two measures under positive linear transformations of reward is invariant [3, 4]. Here, we focus on the Agent AI\u00b5, who knows the actual distribution of our environment. < This simplifies the analysis and makes clear that the above measures are ineffective. < If the two formalizations predict different behavior or are only applicable in incomparable environmental classes, we may be concerned that our results are more reflective than any general characteristics of intelligent agents. < Change in behavior results solely because the environmental model is a semimacroeconomic benchmark. In the following examples, we denounce the AI\u00b5 policy through actions. We also assume that there is always a story <"}, {"heading": "5 Unknown Environments: AIXI and AI\u03be", "text": "We first consider the agents AIXI and AIXI, who do not know the true surroundings, and model them instead according to a mixture distribution, which is distributed over a creditable class M of semimeasure. < < < < < < < < < < < < < < < < < < < < < < < < < < < <; < < < < < <;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"}, {"heading": "6 Conclusion", "text": "In this paper, we have given a formal definition of death for intelligent agents in the sense of semi-authoritative loss, the definition being applicable to any universal agent using an environmental class M with semi-authoritative measures. Additionally, we have shown that this definition corresponds to an alternative formalism in which the environment is modeled as an appropriate measure and death is a state of death without reward. We have shown that agents seek or avoid death, depending on whether rewards are represented by positive or negative real numbers, and that survival, despite positive probability of death, actually increases a Bayesian agent's confidence that he is in a safe environment. We maintain that these results have an impact on issues relating to the safety of artificial intelligence, in particular on the so-called \"shutdown problem.\" [8] The shutdown problem arises when the self-preservation instinct of an intelligent agent prompts him to defy the scope of this analysis, but that [7] the scope of the report exceeds the scope of our own."}, {"heading": "Acknowledgements", "text": "We thank John Aslanides and Jan Leike for reading drafts and providing valuable feedback."}], "references": [{"title": "Anthropic Bias: Observation Selection Effects in Science and Philosophy", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "On the computability of AIXI", "author": ["J. Leike", "M. Hutter"], "venue": "In: UAI-15", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "P.M.B.: An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "Vit\u00e1nyi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Death and suicide in universal artificial intelligence", "author": ["J. Martin", "T. Everitt", "M. Hutter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The basic AI drives. In: AGI-08", "author": ["S.M. Omohundro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "IEEE Transactions on Information Theory IT-24,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1978}, {"title": "A monte carlo AIXI approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "W. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research 40(1),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "6 Conclusion 12 A shorter version of this paper will be presented at AGI-16 [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "1 Introduction Reinforcement Learning (RL) has proven to be a fruitful theoretical framework for reasoning about the properties of generally intelligent agents [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "Firstly, it can guide principled attempts to construct such agents [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "The latter challenge has recently received considerable attention in the context of the potential risks posed by these agents to human safety [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "The universally intelligent agent AIXI constitutes a formal mathematical theory of artificial general intelligence [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Originally devised for Solomonoff induction, they are necessary for universal artificial intelligence because the halting problem prevents the existence of a (lower semi-)computable universal measure for the class of (computable) measures [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 3, "context": "Using this formalism we proceed to reason about the behaviour of several generally intelligent agents in relation to death: AI\u03bc, which knows the true environment distribution; AI\u03be, which models the For example, Leike and Hutter [4] proved that since \u03be is a mixture over semimeasures, the iterative and recursive formulations of the value function are non-equivalent.", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "environment using a universal mixture; and AIXI, a special case of AI\u03be that uses the Solomonoff prior [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "In Algorithmic Information Theory, a semimeasure over an alphabet X is a function \u03bd : X \u2217 \u2192 [0, 1] such that (1) \u03bd(\u01eb) \u2264 1, and (2) \u03bd(x) \u2265", "startOffset": 92, "endOffset": 98}, {"referenceID": 7, "context": "Any semimeasure \u03bd can be turned into a measure \u03bdnorm using Solomonoff normalisation [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "We define the value (expected total future reward) of a policy \u03c0 in an environment \u03bd given a history \u00e6<t [4]: V \u03c0 \u03bd (\u00e6<tat) = 1 \u0393t \u2211", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "This is a surprising result, because in a standard RL setup in which the environment is modelled as a proper probability measure (not a semimeasure), the relative value of two policies is invariant under positive linear transformations of the reward [3, 4].", "startOffset": 250, "endOffset": 256}, {"referenceID": 3, "context": "This is a surprising result, because in a standard RL setup in which the environment is modelled as a proper probability measure (not a semimeasure), the relative value of two policies is invariant under positive linear transformations of the reward [3, 4].", "startOffset": 250, "endOffset": 256}, {"referenceID": 0, "context": "For the standard choice of reward range, rt \u2208 [0, 1], death is the worst possible outcome for the agent, whereas if rt \u2208 [\u22121, 0], it is the best.", "startOffset": 46, "endOffset": 52}, {"referenceID": 6, "context": "In a certain sense, therefore, the reward range parameterises a universal agent\u2019s selfpreservation drive [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "We argue that it could form the basis of a \u201ctripwire mechanism\u201d[2] that would lead an agent to terminate itself upon reaching a level of intelligence that would constitute a threat to human safety.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "There is an \u201cobservation selection effect\u201d[1] at work: AIXI only experiences history sequences on which it remains alive, and infers that a safe environment is more likely.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "We prove convergence of \u03be to \u03bcnorm by showing that (with respect to the true environment \u03bc), the total expected squared distance between \u03be and \u03bcnorm is finite [3]:", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "The shutdown problem arises if an intelligent agent\u2019s self-preservation drive incentivises it to resist termination [2, 7, 8].", "startOffset": 116, "endOffset": 125}, {"referenceID": 6, "context": "The shutdown problem arises if an intelligent agent\u2019s self-preservation drive incentivises it to resist termination [2, 7, 8].", "startOffset": 116, "endOffset": 125}, {"referenceID": 1, "context": "This suggests a potentially robust \u201ctripwire mechanism\u201d [2] that could decrease the risk of intelligence explosion.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "It seems doubtful that such a condition could ever be made robust against subversion by a sufficiently intelligent agent [2].", "startOffset": 121, "endOffset": 124}], "year": 2016, "abstractText": "Reinforcement learning (RL) is a general paradigm for studying intelligent behaviour, with applications ranging from artificial intelligence to psychology and economics. AIXI is a universal solution to the RL problem; it can learn any computable environment. A technical subtlety of AIXI is that it is defined using a mixture over semimeasures that need not sum to 1, rather than over proper probability measures. In this work we argue that the shortfall of a semimeasure can naturally be interpreted as the agent\u2019s estimate of the probability of its death. We formally define death for generally intelligent agents like AIXI, and prove a number of related theorems about their behaviour. Notable discoveries include that agent behaviour can change radically under positive linear transformations of the reward signal (from suicidal to dogmatically self-preserving), and that the agent\u2019s posterior belief that it will survive increases over time.", "creator": "LaTeX with hyperref package"}}}