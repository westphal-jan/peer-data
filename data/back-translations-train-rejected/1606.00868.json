{"id": "1606.00868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Unified Framework for Quantification", "abstract": "Quantification is the machine learning task of estimating test-data class proportions that are not necessarily similar to those in training. Apart from its intrinsic value as an aggregate statistic, quantification output can also be used to optimize classifier probabilities, thereby increasing classification accuracy. We unify major quantification approaches under a constrained multi-variate regression framework, and use mathematical programming to estimate class proportions for different loss functions. With this modeling approach, we extend existing binary-only quantification approaches to multi-class settings as well. We empirically verify our unified framework by experimenting with several multi-class datasets including the Stanford Sentiment Treebank and CIFAR-10.", "histories": [["v1", "Thu, 2 Jun 2016 20:42:31 GMT  (530kb,D)", "http://arxiv.org/abs/1606.00868v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aykut firat"], "accepted": false, "id": "1606.00868"}, "pdf": {"name": "1606.00868.pdf", "metadata": {"source": "CRF", "title": "Unified Framework for Quantification", "authors": ["Aykut Firat"], "emails": ["aykut@crimsonhexagon.com"], "sections": [{"heading": "1 Introduction", "text": "However, in business applications such as social media opinion monitoring, accurate prediction of the share of each opinion class is more important than accurate prediction of each opinion. For example, understanding the customer's intention to watch a movie before and after the release of a trailer requires accurate quantification of class proportions, which can vary widely from day to day and are very different from the training time proportions. Naively aggregated probability factors generally result in greatly distorted estimates of class proportions when test and training class distributions differ significantly from each other, precisely when something interesting happens. In this case, the probability results are also not optimal for classification, because the assumed previous class probabilities, namely training proportions, no longer match the actual priors that correspond to the test class proportions."}, {"heading": "2 Related Work", "text": "Although their history dates back to at least 1966 (Gart and Buck, 1966), quantification, as a name, has first appeared in a number of Forman publications (Forman, 2005; Forman, 2007; Forman, 2008). Forman recognized the importance of the problem at HP when she had to monitor the rapid divergence of support problems over time, using the approaches of the classical quantification method as the first estimate of the actual positive rate (tpr) and the false positive threshold (fpr). Then, using an adjustment formula for the proportioning of proportions, the adjusted class proportioning of p is estimated as if it were using the actual positive rate (fpr) and false threshold (fpr). Forman himself uses the differences in relation to the distribution of proportions, proportions and proportions."}, {"heading": "3 Unifying Quantification Methods", "text": "We now show that existing threshold and blending model-based quantification approaches can be modeled as a limited regression problem by identifying their trait transformation and loss functions. As the framework is multivariable, existing binary-only approaches are automatically extended to the multi-class situation under this model. We assume that the distribution of traits assigned to a class is identical in the test and training data. Any quantification task that does not largely fulfill this basic assumption is beyond the scope of this paper."}, {"heading": "3.1 Notation", "text": "We will use a similar notation used in (Friedman, 2014): T = {yi, xi} NT1 is the labeled training data of the size NT = \u2211 K i = 1Ni with K classes, where Ni is the number of training examples for class i, and y = {c1, c2,..., cK} is the labeling variable of the class, and x is the labeling variable of size V; yi and xi refer to the actual values of these variables in the data. F = {xi} NF1, are the future unlabeled data of size NFI (z) is an indicator function: 1 if z is true, 0 other labeling variable of size Tk = 1NT, i = ck), ratio of class kin training.Fk is the estimated ratio of class k to the unlabeled future size NFI (z) is an indicator function: 1 if z is true, 0 other labeling variable of size Tk = 1T."}, {"heading": "3.2 Constrained Regression", "text": "The general restricted regression model for quantification is: y = X\u03c0-F + s.t. Due to the slight variability, the distribution rate is influenced only by quantification. Hopkins & King use a function transformation function that directly aims to approximate the distribution of characteristics using repeated random samples. Broadly speaking, they produce vectors by stamping n characteristics first, tabulating all existing permutations and normalizing them to derive a distribution. Gonza \u0301 lezCastro in their HDx method, on the other hand, derives a distribution for each characteristic separately. In these two methods, the maximum value of the subscript l refers to the number of repeated samples and the number of characteristics to derive a distribution from it. The remaining approaches use a function transformation that affects the probability output, Pr (y = cl | x) of a basic classification."}, {"heading": "3.3 Solution for Different Loss Functions", "text": "The problem of restricted regression in Section 3.2 can be solved with the help of various loss functions. In the literature three different loss functions have been used, namely the smallest squares, the least absolute deviation and the Hellinger divergence, so we will outline the solution method for these three functions."}, {"heading": "3.3.1 Least-Squares", "text": "The problem of the smallest squares can be formulated as follows: min (y \u2212 X\u03c0-F) T (y \u2212 X\u03c0-F) s.t. \u03c0 F > = 0 and \u2211 \u03c0-F = 1The objective function can be extended as follows: \u03c0-TFX-TX\u03c0-F \u2212 2\u03c0-TFXTy + yTyLet D = 2XTX and d = \u2212 2XTy and ignore the constant yTythe objective function becomes 1 2 \u03c0-T FD\u03c0-F + d T \u03c0-FFurthermore, the constraints can easily be expressed as follows: A\u03c0-F \u2264 b This reformulation fits the quadratic programming with linear constraints. Therefore, the problem of the smallest squares can be solved with quadratic programming. We have used the quadrog package in R for our experiments."}, {"heading": "3.3.2 Least-Absolute-Deviation", "text": "The problem of the least absolute deviation can be formulated as follows: min \u2211 | y \u2212 X\u03c0 \u2212 F | s.t. \u03c0 \u0445 F > = 0 and \u2211 \u03c0 \u0445 F = 1This problem can be converted into a linear programming problem and solved by standard techniques, first defining artificial variables, vector u and rewriting the objective function as follows: min \u2211 us.t. u \u2265 y \u2212 X\u03c0 \u0445Fu \u2265 \u2212 (y \u2212 X\u03c0 \u0432\u0438F) \u0445 F \u2265 1 \u2212 (\u0445 \u03c0 \u0432\u0438\u0441\u0438\u0441F) \u2265 \u2212 1\u03c0 \u0441F > = 0We used the Linprog package in R to formulate and solve this problem as a linear programming problem."}, {"heading": "3.3.3 Hellinger Divergence", "text": "The Hellinger divergence problem can be formulated as follows: min 1 \u2212 \u2211 i \u221a yi (X\u03c0-F) is.t. \u03c0 \u0445F > = 0 and \u0445 \u03c0-F = 1A reduced gradient optimization approach to solve the dual problem formulated above is explained in (Klafszky et al., 1989).Since the regression problem is quite small, we found that the direct solution of this non-linear optimization problem with linear limitations was good enough for testing purposes."}, {"heading": "4 Experiments", "text": "We tested the advanced versions of the most important quantification algorithms against four different sets of data: Stanford Sentiment Treebank, CIFAR-10 image data, marketing data presented in (Hastie et al., 2009), and 69 Twitter opinion analysis data sets created by us. Our goal is not to create a definitive ranking of existing methods, but to show an embodiment of our unified framework. At the same time, we demonstrate empirically that the multi-class extensions claimed in this work well. Although the rankings we receive from the experiments contain indications of the feasibility of the methods, we caution the reader when generalizing our results to arrive at a definitive ranking of the quantification methods.Our R code is made publicly available and can be used to replicate the experiments.1"}, {"heading": "4.1 Stanford Sentiment Treebank", "text": "We used the training and test split used by (Socher et al., 2013) with an order of magnitude of 8544 or 3311 steps. We worked with a binary Document-Term-Matrix with 3770 characteristics and 5 classes: very negative, negative, neutral, positive, very positiv.Instead of a sophisticated classifier like Sochers RNTN (Socher et al., 2013) we used a logistic classifier as the basis classifier for quantification. The accuracy of RNTN and logistic regression in this task was 44% or 38%. This was a practical choice for us, since the execution of cross validations would take much longer if we used RNTN. It is also a test to see whether we can achieve good results with average classifiers 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0."}, {"heading": "4.2 CIFAR-10 dataset", "text": "CIFAR-10 has 60,000 color images in 10 classes (plane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). We used 55,000 of these images as training data in a bare-bones Convolutional Neural Network (CNN) algorithm that reached an accuracy of about 80%. CNN's state-of-the-art algorithms in this dataset can achieve an accuracy of more than 95% through data augmentation and use of ensembles. However, our goal was to experiment with an imperfect classifier, as quantification is not required when we have a perfect classifier (for example, the famous MNIST dataset). We used the remaining 5,000 images as test data. We re-created all possible test case options with a step ratio of 0.10. However, for practical reasons, we randomly sampled 1001 of a total of 92378 cases and used our tests on this sample.We used HDX and HDVA performance for this very important experimental property."}, {"heading": "4.3 Marketing dataset", "text": "This dataset is an excerpt from a survey and consists of 14 demographic attributes with a good mix of categorical and continuous variables with a lot of missing data. We added this data for experiments as Friedman had previously used it to test his quantifier. We used the same settings Friedman used, namely a classification problem with five occupational classes (Professional / Managerial, Factory Worker / Laboratory / Driver, Homemaker, Student, Retired), 5043 lines and 75 binarized characteristics. We used two thirds of the data as training and constructed all possible (1001) test combinations with an increment of 0.1 from the remaining test data."}, {"heading": "4.4 Twitter dataset", "text": "The Twitter dataset consists of 69 different test cases constructed from actual hand-coded tweets. Each test case deals with a different topic, and the number of classes ranges from 3 to 12. The data size of each test varies from 700 to 4000, and the number of characteristics varies from 150 to 4200. For this dataset, we have created two types of tests.In the first test set, we follow the previous test settings by generating all possible proportions with a variable step size, depending on the number of classes. Since the number of test cases may be too many, we randomly select 20 test proportions for each of the 69 test topics for a total of 1380 tests.In the second test set, we also generate 20 tests from each test topic. This time, however, we start with the rough training data proportions and generate random behavior by taking samples from a dirichlet distribution of the previous proportions as input."}, {"heading": "4.5 Results", "text": "We measure the performance of the methods using the mean absolute deviation (MAD) of the predictions from the true proportions and also taking into account the accuracy after the quantification. For classification after the quantification, we use the rule after the quantification (Saerens et al., 2002): the average MAD results in Table 2 and the average precision after the quantification in Table 3. The results are the averages of the results of all experiments per data set. Accuracy is simply the number of correct classifications divided by the total number of classifications. In both tables, \"Naive\" shows the results of naive quantification, a baseline for performance. In Table 3, \"Truth\" shows the accuracy of the test series, the accuracy of which is achievable if we know the test proportions accurately, an upper limit for what is achievable."}, {"heading": "4.5.1 Post-Quantification Classification Performance", "text": "An important advantage of quantification is improved classification performance. In Figure 4, we show how different quantification approaches increase classification accuracy by adjusting a regression curve for classification accuracy because they differ in terms of MAD test proportions of training portions. Naive classification performance deteriorates as MAD grows."}, {"heading": "4.5.2 Performance Sensitivity to Training Proportions", "text": "We show in Figure 3 how the performance of the quantification methods changes as the test proportions differ from the training proportions. We adjust a regression line for each quantification method by using the test proportions from the training proportions as an independent variable and the MAD of the regression lines: MAD from truth, MAD from training proportions Post - Quantification Classification Accuracy (smoothed) predicted test proportions from the true test proportions as a dependent variable. The best quantifiers are expected to be insensitive to the training proportions. The test and MM variations exhibit remarkable insensitivity to training proportions, while the Hellinger divergence-based methods exhibit higher sensitivity."}, {"heading": "5 Conclusion", "text": "Within this framework, the quantification approaches differ primarily in the choice of a function transformation function, which should be approximately constant and sufficiently different across classes in the training and test data. We also used mathematical programming to offer solutions for least squares, least absolute deviation, and Hellinger divergence loss functions for the limited regression problem. In addition, we extended the purely binary quantification approaches to include multi-class settings and presented the results of our experiments with four different data sets to verify that our multi-class extensions work in practice as expected. Our code is made available for replication and further experiments."}], "references": [{"title": "Quantification-oriented learning based on reliable classifiers", "author": ["Jorge Dez", "Juan Jos del Coz"], "venue": "Pattern Recognition,", "citeRegEx": "Barranquero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barranquero et al\\.", "year": 2015}, {"title": "Quantification via Probability Estimators", "author": ["Bella et al.2010] A. Bella", "C. Ferri", "J. Hernandez-Orallo", "M.J. Ramirez-Quintana"], "venue": "In 2010 IEEE 10th International Conference on Data Mining (ICDM),", "citeRegEx": "Bella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bella et al\\.", "year": 2010}, {"title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "author": ["Esuli", "Sebastiani2015] Andrea Esuli", "Fabrizio Sebastiani"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Esuli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Esuli et al\\.", "year": 2015}, {"title": "Counting positives accurately despite inaccurate classification", "author": ["George Forman"], "venue": "In Machine Learning: ECML", "citeRegEx": "Forman.,? \\Q2005\\E", "shortCiteRegEx": "Forman.", "year": 2005}, {"title": "Quantifying counts, costs, and trends accurately via machine learning", "author": ["George Forman"], "venue": "Technical report,", "citeRegEx": "Forman.,? \\Q2007\\E", "shortCiteRegEx": "Forman.", "year": 2007}, {"title": "Quantifying counts and costs via classification", "author": ["George Forman"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Forman.,? \\Q2008\\E", "shortCiteRegEx": "Forman.", "year": 2008}, {"title": "Class Counts In Future Unlabeled Samples (Detecting and dealing with concept drift), Presentation at MIT CSAIL Big Data Event, November", "author": ["Jerome Friedman"], "venue": null, "citeRegEx": "Friedman.,? \\Q2014\\E", "shortCiteRegEx": "Friedman.", "year": 2014}, {"title": "Comparison of a screening test and a reference test in epidemiologic studies. II. A probabilistic model for the comparison of diagnostic tests", "author": ["Gart", "Buck1966] J.J. Gart", "A.A. Buck"], "venue": "American Journal of Epidemiology,", "citeRegEx": "Gart et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Gart et al\\.", "year": 1966}, {"title": "Class distribution estimation based on the Hellinger distance", "author": ["Rocio Alaiz-Rodriguez", "Enrique Alegre"], "venue": "Information Sciences,", "citeRegEx": "Gonzalez.Castro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonzalez.Castro et al\\.", "year": 2013}, {"title": "Classifier Technology and the Illusion of Progress", "author": ["David J. Hand"], "venue": "Statistical Science,", "citeRegEx": "Hand.,? \\Q2006\\E", "shortCiteRegEx": "Hand.", "year": 2006}, {"title": "The Elements of Statistical Learning", "author": ["Hastie et al.2009] Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "A Method of Automated Nonparametric Content Analysis for Social Science", "author": ["Hopkins", "King2010] Daniel Hopkins", "Gary King"], "venue": "American Journal of Political Science,", "citeRegEx": "Hopkins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2010}, {"title": "Linearly constrained estimation by mathematical programming", "author": ["Jnos Mayer", "Tams Terlaky"], "venue": "European Journal of Operational Research,", "citeRegEx": "Klafszky et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Klafszky et al\\.", "year": 1989}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure", "author": ["Patrice Latinne", "Christine Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts Potts"], "venue": "In EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Does quantification without adjustments work? arXiv preprint arXiv:1602.08780", "author": ["Dirk Tasche"], "venue": null, "citeRegEx": "Tasche.,? \\Q2016\\E", "shortCiteRegEx": "Tasche.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Despite all the advances in machine learning, however, we do not often have access to perfect classifiers (Hand, 2006), especially when training data is scarce.", "startOffset": 106, "endOffset": 118}, {"referenceID": 3, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 4, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 5, "context": "Although its history goes back to at least 1966 (Gart and Buck, 1966), quantification, as a name, first occurred in a series of papers by Forman (Forman, 2005; Forman, 2007; Forman, 2008).", "startOffset": 145, "endOffset": 187}, {"referenceID": 6, "context": "While these thresholds are empirically chosen, Friedman uses the optimum threshold that minimizes the variance of proportion estimates (Friedman, 2014).", "startOffset": 135, "endOffset": 151}, {"referenceID": 8, "context": "A similar idea is used by (Gonzalez-Castro et al., 2013) in their HDy method with basically two differences: they replace Forman\u2019s PP-Area metric with Hellinger distance, and use probability mass function (PMF) instead of the CDF to characterize the distributions.", "startOffset": 26, "endOffset": 56}, {"referenceID": 1, "context": "The method used by (Bella et al., 2010) can also be viewed as a mixture model.", "startOffset": 19, "endOffset": 39}, {"referenceID": 8, "context": "The HDx method outlined in (Gonzalez-Castro et al., 2013) similarly works directly with features without a classifier.", "startOffset": 27, "endOffset": 57}, {"referenceID": 0, "context": "A completely different approach is pursued in (Esuli and Sebastiani, 2015) and (Barranquero et al., 2015).", "startOffset": 79, "endOffset": 105}, {"referenceID": 15, "context": "A recent study, (Tasche, 2016), claims that \u201cquantification without adjustments\u201d methods work only in the degenerate case, i.", "startOffset": 16, "endOffset": 30}, {"referenceID": 6, "context": "We will be adopting a similar notation used in (Friedman, 2014):", "startOffset": 47, "endOffset": 63}, {"referenceID": 6, "context": "Because the probability output is only affected by the same variables, the distribution of probability output given a class is also the same in test and training data by an extension of the fundamental assumption (see (Friedman, 2014) for an illustration).", "startOffset": 218, "endOffset": 234}, {"referenceID": 12, "context": "A reduced gradient optimization approach to solve the dual of the above formulated problem is given in (Klafszky et al., 1989).", "startOffset": 103, "endOffset": 126}, {"referenceID": 10, "context": "We tested the extended versions of the major quantification algorithms using four different datasets: Stanford Sentiment Treebank, CIFAR-10 image data, marketing data presented in (Hastie et al., 2009) and 69 Twitter opinion analysis data sets we have created.", "startOffset": 180, "endOffset": 201}, {"referenceID": 14, "context": "We used the training and test split used by (Socher et al., 2013), of size 8544 and 3311 respectively.", "startOffset": 44, "endOffset": 65}, {"referenceID": 14, "context": "Instead of using a sophisticated classifier such as Socher\u2019s RNTN (Socher et al., 2013), we used a logistic classifier as our base classifier for quantification.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": "We included this data for experiments as it was used previously by Friedman to test his quantifier. We adopted the same settings Friedman used, namely a classification problem with five occupation classes (Professional/Managerial, Factory Worker/Laborer/Driver, Homemaker, Student, Retired), 5043 rows and 75 binarized features. We used a random two thirds of the data as training, and constructed all possible (1001) test combinations with a step size of 0.", "startOffset": 67, "endOffset": 418}, {"referenceID": 13, "context": "For post-quantification classification we use the post-quantification classification rule by (Saerens et al., 2002):", "startOffset": 93, "endOffset": 115}], "year": 2016, "abstractText": "Quantification is the machine learning task of estimating test-data class proportions that are not necessarily similar to those in training. Apart from its intrinsic value as an aggregate statistic, quantification output can also be used to optimize classifier probabilities, thereby increasing classification accuracy. We unify major quantification approaches under a constrained multi-variate regression framework, and use mathematical programming to estimate class proportions for different loss functions. With this modeling approach, we extend existing binary-only quantification approaches to multi-class settings as well. We empirically verify our unified framework by experimenting with several multi-class datasets including the Stanford Sentiment Treebank and CIFAR-10.", "creator": "LaTeX with hyperref package"}}}