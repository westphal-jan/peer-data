{"id": "1606.00930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Comparison of 14 different families of classification algorithms on 115 binary datasets", "abstract": "We tested 14 very different classification algorithms (random forest, gradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer neural nets, extreme learning machines, k-nearest neighbors and a bagging of knn, naive Bayes, learning vector quantization, elastic net logistic regression, sparse linear discriminant analysis, and a boosting of linear classifiers) on 115 real life binary datasets. We followed the Demsar analysis and found that the three best classifiers (random forest, gbm and RBF SVM) are not significantly different from each other. We also discuss that a change of less then 0.0112 in the error rate should be considered as an irrelevant change, and used a Bayesian ANOVA analysis to conclude that with high probability the differences between these three classifiers is not of practical consequence. We also verified the execution time of \"standard implementations\" of these algorithms and concluded that RBF SVM is the fastest (significantly so) both in training time and in training plus testing time.", "histories": [["v1", "Thu, 2 Jun 2016 23:01:25 GMT  (61kb,D)", "http://arxiv.org/abs/1606.00930v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jacques wainer"], "accepted": false, "id": "1606.00930"}, "pdf": {"name": "1606.00930.pdf", "metadata": {"source": "CRF", "title": "Comparison of 14 different families of classification algorithms on 115 binary datasets", "authors": ["Jacques Wainer"], "emails": ["wainer@ic.unicamp.br"], "sections": [{"heading": null, "text": "Keywords: Classification Algorithms; Comparison; Binary Problems; Demsar Procedure; Bayesian Analysis"}, {"heading": "1 Introduction", "text": "For practitioners, this form of research is even more important. (2014) 179 different implementations of classification algorithms (from 17 different families of algorithms) on 121 public datasets. We believe that such highly empirical research is very important for both machine learning researchers and practitioners in particular. It is probably more useful to develop a large forest of data (which the family of classification algorithms does best according to Ferna-Delgado et al. (2014) than to do it for Bayesian networks (mainly naive Bayes) or even to develop Nearest Neighbors methods that work worse than random forests.For practitioners, this form of research is even more important."}, {"heading": "2 Data and Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Experimental procedure", "text": "In general, the experimental approach followed by this research is as follows: Each data set Di (the data sets are discussed in Section 2.2) is divided into two subgroups S1i and S2i, each with an equal class share. For each subset Sji, we used a 5-fold cross-validation method to select the best set of hyperparameters for the procedure a. Subsequently, we trained the entire subset Sji using the procedure a with hyperparameters, calculating the error rate for the subset S-i, (where 1% = 2 and 2% = 1). We call the error rate of the algorithm a, when we learn about the subset Sji, hyperparameters."}, {"heading": "2.2 Datasets", "text": "We started with the 121 sets of data collected, processed and converted into a uniform format by the authors of Ferna \u0301 ndez-Delgado et al. (2014), from the 165 that were available on the UCI website in March 2013, where 56 were excluded by the authors of Ferna \u0301 ndez-Delgado et al. (2014). For the remaining 121, all categorical variables were converted to numerical data, and each feature was standardized to zero mean and standard deviation equal to 1.We downloaded the data set that was pre-processed by the authors of Ferna \u0301 ndez-Delgado et al. (2014). \u2022 We performed the following transformations: \u2022 65 of the data sets were multi-class problems and standard deviations. We converted them into a binary file problem by sorting the data sets by their names and alternately assigning the original class to the positive and negative classes."}, {"heading": "2.3 Classification algorithms", "text": "In the introduction, we argued that one of the possible types of criticism of the work of the European Commission and the European Central Bank (ECB), which deals with the question of whether the European Commission is really able to correct the deficits and to correct the deficits that it has, should be identified and corrected. We decided to use algorithms that do not require hyperparameters, such as Linear Discriminant Analysis (LDA) and Logistic Regression (LDA). We have enabled the algorithms to better adapt the algorithms, and so we believe that they are not able to use hyperparameters. We have the other algorithms able to understand and understand the algorithms."}, {"heading": "2.4 Hyperparameters ranges", "text": "Most implementations of rf use two or three hyperparameters: the mtry, the number of trees, and possibly some limit on the complexity of the trees, either the minimum size of a terminal node, or the maximum depth, or a maximum number of terminal nodes. We have not set a range of possible values for the hyperparameters that limit the complexity of the trees. Mtry is the number of random properties that are used to construct a particular tree. There is a general suggestion (we do not know where the source or other papers that have tested this suggestion lie) that this number should be the square root of the properties of the dataset. If nfeat is the number of properties of the dataset, we set the possible values to {0.5 \u00d7 3000."}, {"heading": "2.5 Reproducibility", "text": "The data described above, the R programs that tested the 14 algorithms, the results of the execution of these algorithms, the R programs used to analyze these results and generate the tables and numbers in this essay, and the stored interactions of the MCMC algorithm are available at https: / / figshare. com / s / d0b30e4ee58b3c9323fb."}, {"heading": "3 Statistical procedures", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Demsar procedure", "text": "We follow the procedure proposed by Demsar (2006), which suggests that we should first use a Friedman test (which can be considered an anti-parametric version of the repeated ANOVA test) to determine whether there is sufficient evidence that the error rate measurements for each method are not samples from the same distribution. If the p-value is below 0.05 (for a 95% certainty), then it can be said that the error rates are unlikely to be all \"equal\" and that one can continue to compare each algorithm with the others. In comparing all algorithms with each other, as is the case here, Demsar proposed the Nemenyi test, which calculates the p-value of all pair comparisons. Also, a p-value below 0.05 indicates that this comparison is statistically significant, meaning that it is unlikely that two sets of error rates are samples from the same distribution."}, {"heading": "3.2 Bayesian comparison of multiple groups", "text": "A standard zero hypothesis assumes the zero hypothesis, usually that the two samples come from the same population and the probability (p-value) is calculated of two samples from the same population showing as much difference in mean (or median) as the one encountered in the data. If the p-value is not low, one cannot claim that the zero hypothesis is true and that the two sets of data come from the same population (and thus all observed differences are due to \"happiness\"). Since the p-value is too high, one cannot claim that the proof of the zero hypothesis is the same. Equivalence tests are a form of \"proof\" of the usual zero hypothesis. Equivalence tests assume as a zero hypothesis that the difference between the mean (or median) of the two propositions is above a certain threshold, and if the p-value is low enough, one can claim that the difference between the zero hypothesis and the means is smaller than the mean."}, {"heading": "3.3 Threshold of irrelevance", "text": "We propose two different ways of defining the threshold of irrelevance for differences in error rates. We will calculate the two thresholds and use the lowest of the two measurements as the threshold of irrelevance. The first proposal is to compare the two measurements of error for each data set and for each classification algorithm. In Eq.1, they were the two terms (i, 1 | a, \u03b8 2, i, 2), that is, the error of the classifier a when it learns from the subset S2i and is tested on the subset S1i, and the dual thereof. The difference is that the two measurements S1i, 2 | a, \u03b8 2, i, 1) \u2212 (i, 1) the error of the classifier a when learning on the subset S2i and on the subset S1i is tested."}, {"heading": "3.4 Computational costs", "text": "The first is the one-stroke test, which measures the total time it takes to train the algorithm a on a subset of Sji and test it on a subset of Sji. So this is the time to train the algorithm and run it on two equal-sized data. However, all algorithms must search for the corresponding hyperparameters, so the total time can also be calculated to search for the right hyperparameter (which we discuss over a five-fold resume), but different algorithms can have a different size of preliminary hyperparameters (since some hyperparameters depend on the properties of the data set), so we divide the total time to search for the best hyperparameter by the number of hyperparameter combinations being tested. We call it the execution time per hyperparameter time, which differs greatly from different data sets, so we use the mean of the execution time to determine the execution time."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Error rates of the different algorithms", "text": "Table 1 lists the mean ranking of all algorithms, the number of times each algorithm was among the peaks of a dataset, followed by SVM with Gaussian cores, followed by gradient-increasing machines. The three most powerful algorithms in terms of the middle rank were the increase in linear classifiers naive Bayes and L1-regulated LDA. We make no claim that these three algorithms are inherently \"bad.\" It is possible that our choice of hyperparameters was outside the range of more useful values, or the implementation used was particularly unreliable. In particular, both nb and sda did not run at all for 20 subsets, which may partially explain their poor performance. The ranking of the algorithm is dense, meaning that all the most powerful algorithms get rank 1, all the second best algorithms get rank 2, and so on. Also, for the ranking we rounded the error rates to a significant difference of 3 percentage points, so that their relevance is less than 0.05 percent of the error rate."}, {"heading": "4.2 Demsar procedure", "text": "The p-values below 0.05 indicate that the differences between the corresponding classifiers are statistically significant. The Demsar analysis shows that there is no statistical significance between the top 3 classification algorithms - the p-values of the paired comparisons between rf, svmRadial and gbm are all well above 0.05. nnet, the next algorithm in the ranking, is statistically significantly different from each of the first 3. As discussed above, the failure to prove that the top three algorithms are not statistically different does not prove that they are equivalent. For this, we will need the Bayesian ANOVA."}, {"heading": "4.3 Irrelevance thresholds", "text": "The median values of \u03b4ia and \u03b4 cv ia for all data sets and for the three best algorithms for each data set are: median (\u03b4ia) = 0.0112 median (\u03b4cvia) = 0.0134. So the two measures are comparable, slightly more than 1%, and we will use the lower of the two as the exclusion threshold of irrelevance (0.0112). The fact that the median of \u03b4ia is smaller than the other is somewhat surprising, since \u03b4ia measures the error of learning (and testing) with two different samples of the data population, while \u03b4cvia represents the difference of learning in essentially the same subset, but tests in two different samples of different size from the same data population."}, {"heading": "4.4 Bayesian ANOVA analysis", "text": "Table 3 shows the paired probability that the differences in the error rates of the classifiers are within our limits of irrelevance (of -0.0112 or 0.0112).The comparison between the three best algorithms shows that the differences are most likely (0.83 to 0.64) within our range of irrelevance, i.e. there is no \"practical difference\" in the error rates of the three algorithms. In particular, there is a stronger argument that the best algorithm, rf, is equivalent to svmRadial for practical purposes. To our surprise, the assertion of equivalence between the gbm and the other two is less strong relative to the second best svmRadial."}, {"heading": "4.5 Computational costs of the algorithms", "text": "Table 4 shows the mean rank for the 1-move test and the hyperparameter times. The result for the nb is surprising. Also surprising, at least for the author, is how expensive RF is compared to the other two \"equivalent\" classifiers, svmRadial and gbm. Table 5 is the Demsar procedure p-values for the paired comparisons of the 1-move test costs of the top 6 algorithms (B contains the full p-value table). Note that the difference between svmRadial and gbm is not significant; the difference between rf and svmRadial and gbm are significant. Table 6 lists the p-values of the paired data parameters of the top 6 algorithms. B contains the full p-value of the hyperparameters table. svmRadial is therefore faster than all other hyperparameters."}, {"heading": "5 Discussion", "text": "In fact, most of us are in a position to go to another world, in which they can go to another world, in which they can go to another world, in which they go to another world, in which they find themselves to another world, in which they find themselves to another world, in which they find themselves to another world, in which they live to another world, in which they live to another world, in which they live to themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, in which they can, they can, in which they can, in which they can, they can, in which they can, in which they can, they can, in which they can, in which they can, in which they can, in which they can, live in which they can, in which they can, in which they can, in which they can, they can, in which they can, they can, in which they can, live in which they can, in which they can, in which they can, in which they can, live in which they"}, {"heading": "5.1 Limits on this research", "text": "One limitation of this research is that its conclusions should only be generalized to data sets that are \"similar\" to those used. Specifically, our data sets did not contain very large or very sparse or data sets with many more features than data points (as is common in some bioinformatics applications). Furthermore, our experiments with binary classification tasks were conducted. In light of these data limitations, if the data sets in the UCI repository can be assumed to be samples of \"real life problems,\" our results should be generalizable. A second limitation of this research is based on our decisions regarding hyperparameter search for each algorithm. There is very little research on the range of possible or useful values of hyperparameters for each of the algorithms discussed, so that our decisions are questionable. And if our decisions were particularly bad, an algorithm might be ranked in the comparisons. The reader should be aware of the limited usefulness of the timely implementation."}, {"heading": "5.2 Future research", "text": "Given the finding that rf, svmRadial, and gbm are probably the best functioning algorithms, it is interesting to examine further variations and different implementations of these algorithms. We discussed that more modern implementations can change the algorithms \"timing result, but variations in algorithms can also change the ranking and apparent equivalence of the three algorithms. Variations of the Random Forest, such as Rotation Forest (Rodriguez et al., 2006), Extremely Randomized Forest (Geurts et al., 2006), Random Forest of Inference Trees (Hothorn et al., 2006; Strobl et al., 2007), should be compared with the standard algorithm. The smallest square SVM (Suykens and Vandewalle, 1999) should be compared with the SVM standard, and various models for promoting conditional inference trees (Hothorn et al., 2006; Strobl al, 2007) should be compared with the algorithm."}, {"heading": "6 Conclusion", "text": "We have shown that random forests, RBF SVM, and gradient enhancers are classification algorithms that will most likely lead to highest accuracy, and that there is likely to be no significant difference in error rates between these three algorithms, especially between random forests and RBF SVM. In terms of training and test run times, SVM with an RBF kernel is faster than the other two competitors. We believe that this work also makes important methodological contributions to machine learning research, mainly in defining a threshold of irrelevance below which changes in error rates should be considered practically meaningless. We have argued that this threshold should be 0.0112. Another important methodological contribution is the use of a Bayesian variance analysis method to verify that the differences between the three most powerful algorithms were very likely to be less than the 0.0112 threshold of practical irrelevance, and we have shown that the Bayesian model used is suitable to use similar error rates between different algorithms."}, {"heading": "A Datasets", "text": "This year, the number of people of working age capable of falling by 20 per cent in the first half of the year has multiplied."}, {"heading": "B Full p-value tables for the pairwise compari-", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Bayesian model verification", "text": "Figure 2 shows the histogram of error rates over the 115 data sets for the various algorithms superimposed with the most suitable Gaussian. It can be argued that the distribution of error rates can reasonably be considered normal, which would be consistent with the assumptions of equation 2e. Similarly, Figure 3 shows the distribution of error rates for 20 random data sets, and again it is argued that the assumption in equation 2f is reasonable. A more formal verification of the model is the method of posterior prediction testing proposed by Gelman et al. (1996). The method calculates how improbable the true data is compared to the data generated by the model, given the posterior distribution of the hyperparameters of the model. The data (true and generated) are summarized by the discrepancy between the two models (Gelman et al., 1996). Figure 4 shows the relationship between the deviations generated by the MC and the true data, or the probability that the 67 data produced by the MC is the least likely to correspond to the deviations generated by the MC."}, {"heading": "D Convergence of the MCMC", "text": "There were no attempts to optimize the number of simulations of the MCMC algorithm. We used JAGS with 5000 Burnin steps and 5000 adaptive steps. Finally, we performed a total of 100,000 interactions on 4 separate chains. Below, we find the diagnosis Gelman and Rubin (Brooks and Gelman, 1998), which compares the variance within and between the chains, as reported by the function gelman.diag from the R packet coda, where the variables stored in Equation 2b are: b0 is \u03b1 and b1 [a] are \u03b2a for each algorithm, b2 [d] are \u03b4d for each dataset and ySigma is \u03c30 from Equation 2a, a1SD and a2SD are \u03c3a and \u03c3a from 2g and 2h. [a] Not all lines referring to the b2 [d] variables are shown, [1] [1] [d] variables, but they all have the same values."}, {"heading": "100541.69 98992.43 98427.81 100000.00 97006.48 97467.65 94816.02 100000.00", "text": "b2 [2] b2 [3] b2 [4] b2 [5] b2 [6] b2 [7] b2 [8] b2 [9] 99043.72 97966.91 99380.08 100000.00 99530.18 101053.74 100087.01 100000.00... b2 [114] b2 [115] ySigma a1SD a2SD"}, {"heading": "100204.87 100216.75 55353.48 16108.44 51977.11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E Results with the robust Bayesian model", "text": "This section presents the results of the Bayesian analysis based on the robust model. Table 12 is the complete probability table of the Bayesian ANOVA. We cannot verify the model by posterior predictive verification, because the \u03c72 discrepancy requires the variance of the data. In the robust model, the variance of the data also depends on the degrees of deviation of the student-t distribution, and in the case of the robust simulations, the variance of the student-t distribution is 1.12, and the variance of the student-t distribution is not defined for degrees of freedom below 2."}], "references": [{"title": "Fast kernel classifiers with online and active learning", "author": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Bordes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2005}, {"title": "General methods for monitoring convergence of iterative simulations", "author": ["S. Brooks", "A. Gelman"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "Brooks and Gelman,? \\Q1998\\E", "shortCiteRegEx": "Brooks and Gelman", "year": 1998}, {"title": "sparseLDA: Sparse Discriminant Analysis. R package version 0.1-6", "author": ["L. Clemmensen"], "venue": "URL http://CRAN.R-project.org/package=sparseLDA", "citeRegEx": "Clemmensen,? \\Q2012\\E", "shortCiteRegEx": "Clemmensen", "year": 2012}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Demsar,? \\Q2006\\E", "shortCiteRegEx": "Demsar", "year": 2006}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Polytomous logistic regression", "author": ["J. Engel"], "venue": "Statistica Neerlandica", "citeRegEx": "Engel,? \\Q1988\\E", "shortCiteRegEx": "Engel", "year": 1988}, {"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["M. Fern\u00e1ndez-Delgado", "E. Cernadas", "S. Barro", "D. Amorim"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Fern\u00e1ndez.Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fern\u00e1ndez.Delgado et al\\.", "year": 2014}, {"title": "Multi-class support vector machine", "author": ["V Franc"], "venue": "International Conference on Pattern Recognition", "citeRegEx": "Franc,? \\Q2002\\E", "shortCiteRegEx": "Franc", "year": 2002}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of statistical software", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Posterior predictive assessment of model fitness via realized discrepancies", "author": ["A. Gelman", "Meng", "X.-L", "H. Stern"], "venue": "Statistica Sinica,", "citeRegEx": "Gelman et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 1996}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine Learning", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "elmNN: Implementation of ELM (Extreme Learning Machine) algorithm for SLFN (Single Hidden Layer Feedforward Neural Networks). R package version 1.0", "author": ["A. Gosso"], "venue": null, "citeRegEx": "Gosso,? \\Q2012\\E", "shortCiteRegEx": "Gosso", "year": 2012}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R Tibshirani"], "venue": "The Annals of Statistics", "citeRegEx": "Hastie and Tibshirani,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1998}, {"title": "Unbiased recursive partitioning: A conditional inference framework", "author": ["T. Hothorn", "K. Hornik", "A. Zeileis"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "Hothorn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hothorn et al\\.", "year": 2006}, {"title": "Extreme learning machine: theory and applications", "author": ["Huang", "G.-B", "Zhu", "Q.-Y", "Siew", "C.-K"], "venue": "Neurocomputing", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Learning Vector Quantization", "author": ["T. Kohonen"], "venue": null, "citeRegEx": "Kohonen,? \\Q1995\\E", "shortCiteRegEx": "Kohonen", "year": 1995}, {"title": "Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan", "author": ["J. Kruschke"], "venue": null, "citeRegEx": "Kruschke,? \\Q2014\\E", "shortCiteRegEx": "Kruschke", "year": 2014}, {"title": "rknn: Random KNN Classification and Regression. R package version 1.2-1", "author": ["S. Li"], "venue": "URL https://cran.r-project.org/web/packages/rknn/index.html", "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "Classification and regression by randomforest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News", "citeRegEx": "Liaw and Wiener,? \\Q2002\\E", "shortCiteRegEx": "Liaw and Wiener", "year": 2002}, {"title": "e1071: Misc Functions of the Department of Statistics (e1071), TU Wien. R package version 1.6-4", "author": ["D. Meyer", "E. Dimitriadou", "K. Hornik", "A. Weingessel", "F. Leisch"], "venue": null, "citeRegEx": "Meyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2014}, {"title": "gbm: Generalized Boosted Regression Models. R package version 2.1. URL http://CRAN.R-project.org/package=gbm", "author": ["G. Ridgeway"], "venue": null, "citeRegEx": "Ridgeway,? \\Q2013\\E", "shortCiteRegEx": "Ridgeway", "year": 2013}, {"title": "Rotation forest: A new classifier ensemble method", "author": ["J.J. Rodriguez", "L.I. Kuncheva", "C.J. Alonso"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Rodriguez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2006}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Bias in random forest variable importance measures: Illustrations, sources and a solution", "author": ["C. Strobl", "Boulesteix", "A.-L", "A. Zeileis", "T. Hothorn"], "venue": "BMC Bioinformatics", "citeRegEx": "Strobl et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Strobl et al\\.", "year": 2007}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters", "citeRegEx": "Suykens and Vandewalle,? \\Q1999\\E", "shortCiteRegEx": "Suykens and Vandewalle", "year": 1999}, {"title": "Modern Applied Statistics with S, 4th Edition", "author": ["W.N. Venables", "B.D. Ripley"], "venue": null, "citeRegEx": "Venables and Ripley,? \\Q2002\\E", "shortCiteRegEx": "Venables and Ripley", "year": 2002}, {"title": "bst: Gradient Boosting. R package version 0.3-4", "author": ["Z. Wang"], "venue": "URL http://CRAN.R-project.org/package=bst", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "klar analyzing german business cycles", "author": ["C. Weihs", "U. Ligges", "K. Luebke", "N. Raabe"], "venue": null, "citeRegEx": "Weihs et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weihs et al\\.", "year": 2005}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks", "citeRegEx": "Wolpert,? \\Q1992\\E", "shortCiteRegEx": "Wolpert", "year": 1992}, {"title": "ranger: A fast implementation of random forests for high dimensional data in C++ and R. arXiv:1508.04409", "author": ["M.N. Wright", "A. Ziegler"], "venue": null, "citeRegEx": "Wright and Ziegler,? \\Q2015\\E", "shortCiteRegEx": "Wright and Ziegler", "year": 2015}, {"title": "What is better: gradient-boosted trees, or a random forest", "author": ["Z. Zajac"], "venue": null, "citeRegEx": "Zajac,? \\Q2016\\E", "shortCiteRegEx": "Zajac", "year": 2016}, {"title": "Regularization and variable selection via the elas", "author": ["T. Hastie"], "venue": null, "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}, {"title": "The procedure calculates how unlikely is the true data when compared to data generated by the model, given the posterior distribution of the hyperparameters of the model", "author": ["A more formal model verification is the procedure of posterior predictive check proposed by Gelman"], "venue": "The data (true and generated) are summarized by the \u03c72 discrepancy (Gelman", "citeRegEx": "Gelman,? 1996", "shortCiteRegEx": "Gelman", "year": 1996}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction Fern\u00e1ndez-Delgado et al. (2014) evaluated 179 different implementations of classification algorithms (from 17 different families of algoritms) on 121 public datasets.", "startOffset": 15, "endOffset": 47}, {"referenceID": 4, "context": "Of course there are meta-extensions of such algorithms to multi-class problems, for example, one-vs-one, one-vs-all, error correcting output coding (ECOC) (Dietterich and Bakiri, 1995), stacked generalization (Wolpert, 1992), pairwise coupling (Hastie et al.", "startOffset": 155, "endOffset": 184}, {"referenceID": 30, "context": "Of course there are meta-extensions of such algorithms to multi-class problems, for example, one-vs-one, one-vs-all, error correcting output coding (ECOC) (Dietterich and Bakiri, 1995), stacked generalization (Wolpert, 1992), pairwise coupling (Hastie et al.", "startOffset": 209, "endOffset": 224}, {"referenceID": 4, "context": "For example, if a researcher is interested in developing algorithms for very large classification problems, it is probably more useful to develop a big-data random forest (which is the family of classification algoritms with best performance according to Fern\u00e1ndez-Delgado et al. (2014)) than to do it for Bayesian networks (mainly naive Bayes) or even Nearest Neighbors methods, which perform worse than random forests.", "startOffset": 255, "endOffset": 287}, {"referenceID": 4, "context": "For example, if a researcher is interested in developing algorithms for very large classification problems, it is probably more useful to develop a big-data random forest (which is the family of classification algoritms with best performance according to Fern\u00e1ndez-Delgado et al. (2014)) than to do it for Bayesian networks (mainly naive Bayes) or even Nearest Neighbors methods, which perform worse than random forests. For practitioners, this form of research is even more important. Practitioners in machine learning will have limited resources, time, and expertise to test many different classification algorithms on their problem, and this form of research will allow them to focus on the most likely useful algorithms. Despite its importance and breath, we believe that Fern\u00e1ndez-Delgado et al. (2014) had some \u201cimperfections\u201d which we address in this research.", "startOffset": 255, "endOffset": 808}, {"referenceID": 4, "context": "Of course there are meta-extensions of such algorithms to multi-class problems, for example, one-vs-one, one-vs-all, error correcting output coding (ECOC) (Dietterich and Bakiri, 1995), stacked generalization (Wolpert, 1992), pairwise coupling (Hastie et al., 1998), among others. Also, there are alternative formulations for specific binary classifiers to deal with multiclass, for example, Franc et al. (2002) for SVM, Engel (1988) for logistic regression, and so on.", "startOffset": 156, "endOffset": 412}, {"referenceID": 4, "context": "Of course there are meta-extensions of such algorithms to multi-class problems, for example, one-vs-one, one-vs-all, error correcting output coding (ECOC) (Dietterich and Bakiri, 1995), stacked generalization (Wolpert, 1992), pairwise coupling (Hastie et al., 1998), among others. Also, there are alternative formulations for specific binary classifiers to deal with multiclass, for example, Franc et al. (2002) for SVM, Engel (1988) for logistic regression, and so on.", "startOffset": 156, "endOffset": 434}, {"referenceID": 6, "context": "On the issue of binary classifiers, Fern\u00e1ndez-Delgado et al. (2014) did not include in their comparisons the gradient boosting machine (gbm) algorithm, considered a very competitive algorithm for classification problems because, as reported in Zajac (2016), the implementation did not work in multiclass problems.", "startOffset": 36, "endOffset": 68}, {"referenceID": 6, "context": "On the issue of binary classifiers, Fern\u00e1ndez-Delgado et al. (2014) did not include in their comparisons the gradient boosting machine (gbm) algorithm, considered a very competitive algorithm for classification problems because, as reported in Zajac (2016), the implementation did not work in multiclass problems.", "startOffset": 36, "endOffset": 257}, {"referenceID": 6, "context": "On the issue of binary classifiers, Fern\u00e1ndez-Delgado et al. (2014) did not include in their comparisons the gradient boosting machine (gbm) algorithm, considered a very competitive algorithm for classification problems because, as reported in Zajac (2016), the implementation did not work in multiclass problems. We included gbm in our comparison. \u2022 We reduced the number of classifiers to only a few classes/algorithms and not different implementations of the same algorithm. Fern\u00e1ndezDelgado et al. (2014) compared an impressing 179 different classification programs, but it was unclear how many were just different implementations of the same algorithm, and how many were variations within the same \u201cfamily\u201d of algorithms.", "startOffset": 36, "endOffset": 509}, {"referenceID": 6, "context": "On the issue of binary classifiers, Fern\u00e1ndez-Delgado et al. (2014) did not include in their comparisons the gradient boosting machine (gbm) algorithm, considered a very competitive algorithm for classification problems because, as reported in Zajac (2016), the implementation did not work in multiclass problems. We included gbm in our comparison. \u2022 We reduced the number of classifiers to only a few classes/algorithms and not different implementations of the same algorithm. Fern\u00e1ndezDelgado et al. (2014) compared an impressing 179 different classification programs, but it was unclear how many were just different implementations of the same algorithm, and how many were variations within the same \u201cfamily\u201d of algorithms. We believe that for practitioner and research communities, it is more useful to have an understanding of how different families of algorithms rank in relation to each other. For the practitioner, which should have more limited access to the different algorithms, this knowledge allow them to order which algorithms should be applied first to their particular problem. In fact, Fern\u00e1ndez-Delgado et al. (2014) also perform an analysis of their results based on the algorithm\u2019s \u201cfamily\u201d, but they have difficulty of extracting useful information from this analysis, since in most cases, different \u201cimplementations\u201d in the same family have widely different results.", "startOffset": 36, "endOffset": 1136}, {"referenceID": 6, "context": "Given Fern\u00e1ndez-Delgado et al. (2014) daunting task of testing 179 programs, they had to rely on default values for the hyperparameters which may lead to suboptimal results.", "startOffset": 6, "endOffset": 38}, {"referenceID": 5, "context": "Fern\u00e1ndez-Delgado et al. (2014) follow the standard null hypothesis significant test in analyzing their result, but even within this framework, their analysis is not as rigorous as it should have been.", "startOffset": 0, "endOffset": 32}, {"referenceID": 3, "context": "The NIST standard for comparing many classifiers across different datasets was proposed by Demsar (2006) and it is discussed in section 3.", "startOffset": 91, "endOffset": 105}, {"referenceID": 3, "context": "The NIST standard for comparing many classifiers across different datasets was proposed by Demsar (2006) and it is discussed in section 3.1. In particular, when testing the different algorithms for statistical significant differences, the Demsar procedure requires one to use the Nemenyi test, which is a nonparametric test that performs the appropriate multiple comparison p-value correction. But Fern\u00e1ndez-Delgado et al. (2014) used a paired t-test (a parametric test) between the first ranked and the following 9 top ranked algorithms, apparently without multiple comparions corrections.", "startOffset": 91, "endOffset": 430}, {"referenceID": 6, "context": "Fern\u00e1ndez-Delgado et al. (2014) first two ranked programs are two different implementation of the same algorithm - parallel implementation of random forest (first ranked) and a non-parallel implementation (second ranked).", "startOffset": 0, "endOffset": 32}, {"referenceID": 6, "context": "2 Datasets We started with the 121 datasets collected from the UCI site, processed, and converted by the authors of Fern\u00e1ndez-Delgado et al. (2014) into a unified format.", "startOffset": 116, "endOffset": 148}, {"referenceID": 6, "context": "2 Datasets We started with the 121 datasets collected from the UCI site, processed, and converted by the authors of Fern\u00e1ndez-Delgado et al. (2014) into a unified format. The datasets is derived from the 165 available at UCI website in March 2013, where 56 were excluded by the authors of Fern\u00e1ndez-Delgado et al. (2014). For the remaining 121, Fern\u00e1ndez-Delgado et al.", "startOffset": 116, "endOffset": 321}, {"referenceID": 6, "context": "2 Datasets We started with the 121 datasets collected from the UCI site, processed, and converted by the authors of Fern\u00e1ndez-Delgado et al. (2014) into a unified format. The datasets is derived from the 165 available at UCI website in March 2013, where 56 were excluded by the authors of Fern\u00e1ndez-Delgado et al. (2014). For the remaining 121, Fern\u00e1ndez-Delgado et al. (2014) converted all categorical variables to numerical data, and each feature was standardized to zero mean and standard deviation equal to 1.", "startOffset": 116, "endOffset": 377}, {"referenceID": 6, "context": "2 Datasets We started with the 121 datasets collected from the UCI site, processed, and converted by the authors of Fern\u00e1ndez-Delgado et al. (2014) into a unified format. The datasets is derived from the 165 available at UCI website in March 2013, where 56 were excluded by the authors of Fern\u00e1ndez-Delgado et al. (2014). For the remaining 121, Fern\u00e1ndez-Delgado et al. (2014) converted all categorical variables to numerical data, and each feature was standardized to zero mean and standard deviation equal to 1. We downloaded the datasets preprocessed by the authors of Fern\u00e1ndezDelgado et al. (2014) in November 2014.", "startOffset": 116, "endOffset": 603}, {"referenceID": 33, "context": "Thus, glmnet (L1 and L2 regularized logistic regression (Zou and Hastie, 2005)) and sda a L1-regularized LDA are two mainly linear algorithms.", "startOffset": 56, "endOffset": 78}, {"referenceID": 17, "context": "lvq, or learning vector quantization (Kohonen, 1995) is a cluster plus distance, or dictionary based classification: a set of \u201cprototypes,\u201d or clusters, or \u201ccodebook\u201d is discovered in the data, many for each class, and new data is classified based on the distance to these prototypes.", "startOffset": 37, "endOffset": 52}, {"referenceID": 16, "context": "Neural network based classifiers include nnet a common 1-hidden layer logistic network, and elm or extreme learning machines (Huang et al., 2006).", "startOffset": 125, "endOffset": 145}, {"referenceID": 9, "context": "Finally we included one implementation of random forests (rf) and one implementation of gradient boosting machine classifiers (gbm) (Friedman, 2001).", "startOffset": 132, "endOffset": 148}, {"referenceID": 28, "context": "We used the R implementation in the package bst (Wang, 2014)", "startOffset": 48, "endOffset": 60}, {"referenceID": 6, "context": "As discussed in the Introduction, we argued that one of the possible criticisms to the Fern\u00e1ndez-Delgado et al. (2014) paper is that the authors do not distinguish different algorithms from different implementations of that algorithm.", "startOffset": 87, "endOffset": 119}, {"referenceID": 12, "context": "elm Extreme learning machines Implementation: package elmNN (Gosso, 2012)) gbm Gradient boosting machines.", "startOffset": 60, "endOffset": 73}, {"referenceID": 22, "context": "Implementation: package gbm (Ridgeway, 2013) glmnet Elastic net logistic regression classifier.", "startOffset": 28, "endOffset": 44}, {"referenceID": 8, "context": "Implementation : package glmnet (Friedman et al., 2010)) knn k-nearest neighbors classifier.", "startOffset": 32, "endOffset": 55}, {"referenceID": 27, "context": "Implementation: package class (Venables and Ripley, 2002).", "startOffset": 30, "endOffset": 57}, {"referenceID": 27, "context": "Implementation: package class (Venables and Ripley, 2002)) nb Naive Bayes classifier: package klaR (Weihs et al.", "startOffset": 30, "endOffset": 57}, {"referenceID": 29, "context": "Implementation: package class (Venables and Ripley, 2002)) nb Naive Bayes classifier: package klaR (Weihs et al., 2005) nnet A 1-hidden layer neural network with sigmoid transfer function.", "startOffset": 99, "endOffset": 119}, {"referenceID": 27, "context": "Implementation: package nnet (Venables and Ripley, 2002) rf Random forest.", "startOffset": 29, "endOffset": 56}, {"referenceID": 20, "context": "Implementation: package randomForest (Liaw and Wiener, 2002) rknn A bagging of knn classifiers on a random subset of the original features.", "startOffset": 37, "endOffset": 60}, {"referenceID": 19, "context": "Implementation: package rknn (Li, 2015) sda A L1 regularized linear discriminant classifier.", "startOffset": 29, "endOffset": 39}, {"referenceID": 2, "context": "Implementation: package sparseLDA (Clemmensen, 2012) svmLinear A SVM with linear kernel.", "startOffset": 34, "endOffset": 52}, {"referenceID": 21, "context": "package e1071 (Meyer et al., 2014)) svmPoly A SVM with polynomial kernel.", "startOffset": 14, "endOffset": 34}, {"referenceID": 21, "context": "package e1071 (Meyer et al., 2014)) svmRadial A SVM with RBF kernel.", "startOffset": 14, "endOffset": 34}, {"referenceID": 21, "context": "package e1071 (Meyer et al., 2014))", "startOffset": 14, "endOffset": 34}, {"referenceID": 8, "context": "The only relevant algorithm for this research is the elastic-net regularized logistic regression implemented by the package glmnet (Friedman et al., 2010)), which computes all the values (or the path as it is called) of the regularization parameter \u03bb.", "startOffset": 131, "endOffset": 154}, {"referenceID": 8, "context": "The only relevant algorithm for this research is the elastic-net regularized logistic regression implemented by the package glmnet (Friedman et al., 2010)), which computes all the values (or the path as it is called) of the regularization parameter \u03bb. Hastie et al. (2004) discuss a complete path algorithm for SVM (for the C hyperparameter) but we did not use this implementation in this paper.", "startOffset": 132, "endOffset": 273}, {"referenceID": 3, "context": "1 Demsar procedure We will follow the procedure proposed by Demsar (2006). The procedure suggests one should first apply a Friedman test (which can be seen as a", "startOffset": 2, "endOffset": 74}, {"referenceID": 18, "context": "Let us denote yad as the error rate for the algorithm a on dataset d, then the usual hierarchical model is (Kruschke, 2014): yad \u223cN(\u03bdad, \u03c30) (2a) \u03bdad =\u03b2 + \u03b1a + \u03b4d (2b) \u03c30 \u223cU(ySD/100, ySD \u2217 10) (2c) \u03b2 \u223cN(yMean, ySD \u2217 5) (2d) \u03b1a \u223cN(0, \u03c3a) (2e) \u03b4d \u223cN(0, \u03c3d) (2f) \u03c3a \u223cGamma(ySD/2, ySD \u2217 2) (2g) \u03c3d \u223cGamma(ySD/2, ySD \u2217 2) (2h) where U(L,H) is the uniform distribution with L and H as the low and high limits; N(\u03bc, \u03c3) is the normal distribution with mean \u03bc and standard deviation \u03c3; and Gamma(m,\u03c3) is the Gamma distribution with mode m and standard deviation \u03c3 - note that this is not the usual parametrization of the Gamma distribution.", "startOffset": 107, "endOffset": 123}, {"referenceID": 18, "context": "The standard Bayesian ANOVA, as described in Kruschke (2014) is based on normal distributions.", "startOffset": 45, "endOffset": 61}, {"referenceID": 18, "context": "The stronger results of the robust model can be explained by shrinkage (Kruschke, 2014); since it allows more outliers, the mean of each distribution of the algorithms\u2019 coefficient would", "startOffset": 71, "endOffset": 87}, {"referenceID": 6, "context": "Our results are in general compatible with those in Fern\u00e1ndez-Delgado et al. (2014). Random forest is the best ranking algorithm in both experiments; gradient boosting machines which was not included in Fern\u00e1ndezDelgado et al.", "startOffset": 52, "endOffset": 84}, {"referenceID": 6, "context": "Our results are in general compatible with those in Fern\u00e1ndez-Delgado et al. (2014). Random forest is the best ranking algorithm in both experiments; gradient boosting machines which was not included in Fern\u00e1ndezDelgado et al. (2014) performs well, as reported in various blogs, RBF SVM also performs well.", "startOffset": 52, "endOffset": 234}, {"referenceID": 6, "context": "Our results are in general compatible with those in Fern\u00e1ndez-Delgado et al. (2014). Random forest is the best ranking algorithm in both experiments; gradient boosting machines which was not included in Fern\u00e1ndezDelgado et al. (2014) performs well, as reported in various blogs, RBF SVM also performs well. The divergence starts with the next best classifications algorithms: Fern\u00e1ndez-Delgado et al. (2014) lists polynomial kernel SVM svmPoly and extreme learning machines elm as the next best families, but in our case svmPoly was equivalent to 1-hidden layer neural nets nnet, and a bagging of knn rknn.", "startOffset": 52, "endOffset": 408}, {"referenceID": 31, "context": "There has been more recent implementations of random forest (Wright and Ziegler, 2015) and gradient boosting machines (Distributed (Deep) Machine Learning Community, 2016) which claim both a faster execution time and shorter memory footprint.", "startOffset": 60, "endOffset": 86}, {"referenceID": 0, "context": "On the other hand, incremental and online solvers for SVM (Bordes et al., 2005; Shalev-Shwartz et al., 2011) may further tip the scale in favor of SVM.", "startOffset": 58, "endOffset": 108}, {"referenceID": 24, "context": "On the other hand, incremental and online solvers for SVM (Bordes et al., 2005; Shalev-Shwartz et al., 2011) may further tip the scale in favor of SVM.", "startOffset": 58, "endOffset": 108}, {"referenceID": 23, "context": "Variations of Random Forest, such as Rotation Forest (Rodriguez et al., 2006), Extremely Randomized Forest (Geurts et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": ", 2006), Extremely Randomized Forest (Geurts et al., 2006), random forest of conditional inference trees (Hothorn et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 15, "context": ", 2006), random forest of conditional inference trees (Hothorn et al., 2006; Strobl et al., 2007), should be compared with the standard algorithm.", "startOffset": 54, "endOffset": 97}, {"referenceID": 25, "context": ", 2006), random forest of conditional inference trees (Hothorn et al., 2006; Strobl et al., 2007), should be compared with the standard algorithm.", "startOffset": 54, "endOffset": 97}, {"referenceID": 26, "context": "Least square SVM (Suykens and Vandewalle, 1999) should be compared with the standard SVM, and different models of boosting such as AdaBoost, LogiBoost, BrownBoost, should be compared with gbm.", "startOffset": 17, "endOffset": 47}], "year": 2016, "abstractText": "We tested 14 very different classification algorithms (random forest, gradient boosting machines, SVM linear, polynomial, and RBF 1-hidden-layer neural nets, extreme learning machines, k-nearest neighbors and a bagging of knn, naive Bayes, learning vector quantization, elastic net logistic regression, sparse linear discriminant analysis, and a boosting of linear classifiers) on 115 real life binary datasets. We followed the Demsar analysis and found that the three best classifiers (random forest, gbm and RBF SVM) are not significantly different from each other. We also discuss that a change of less then 0.0112 in the error rate should be considered as an irrelevant change, and used a Bayesian ANOVA analysis to conclude that with high probability the differences between these three classifiers is not of practical consequence. We also verified the execution time of \u201cstandard implementations\u201d of these algorithms and concluded that RBF SVM is the fastest (significantly so) both in training time and in training plus testing time. keywords: Classification algorithms; comparison; binary problems; Demsar procedure; Bayesian analysis", "creator": "LaTeX with hyperref package"}}}