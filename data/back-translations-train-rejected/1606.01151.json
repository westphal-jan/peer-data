{"id": "1606.01151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Privacy Protection for Natural Language Records: Neural Generative Models for Releasing Synthetic Twitter Data", "abstract": "Redaction has been the most common approach to protecting text data, but synthetic data presents a potentially more reliable alternative for disclosure control. By producing new sample values which closely follow the original sample distribution but do not contain real values, privacy protection can be improved while utility from the data for specific purposes is maintained. We extend the synthetic data approach to natural language by developing a neural generative model for such data. We find that the synthetic models outperform simple redaction on both comparative risk and utility.", "histories": [["v1", "Fri, 3 Jun 2016 15:43:15 GMT  (385kb,D)", "https://arxiv.org/abs/1606.01151v1", null], ["v2", "Fri, 14 Oct 2016 03:34:37 GMT  (558kb,D)", "http://arxiv.org/abs/1606.01151v2", null], ["v3", "Fri, 13 Oct 2017 22:14:38 GMT  (1269kb,D)", "http://arxiv.org/abs/1606.01151v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander g ororbia ii", "fridolin linder", "joshua snoke"], "accepted": false, "id": "1606.01151"}, "pdf": {"name": "1606.01151.pdf", "metadata": {"source": "CRF", "title": "Using Neural Generative Models to Release Synthetic Twitter Corpora with Reduced Stylometric Identifiability of Users", "authors": ["Alexander G. Ororbia II", "Fridolin Linder", "Joshua Snoke"], "emails": [], "sections": [{"heading": null, "text": "We present a method for generating synthetic versions of Twitter data using neural generative models. The aim of our proposed approach is to protect individuals in the source data from stylometric re-identification attacks while releasing data with research value. In order to generate tweet corpora that maintain word distributions at the user level, our proposed approach expands powerful neural language models with local parameters that weigh user-specific inputs. We compare our work with two standard methods for protecting text data: editing and iterative translation. We evaluate the three methods of risk and benefit. We define the risk according to the stylometric models of re-identification and define the benefit based on two general linguistic scales and two common text analysis tasks. We find that neural models are able to significantly reduce the risk compared to previous methods at the expense of benefit."}, {"heading": "1 Introduction", "text": "This paper provides a method to produce synthetic tweet corporations that can be published instead of the original tweets, with the aim of minimizing user identifiability and maintaining user-specific word distributions, as well as the overall, analytical usefulness of the original tweet. In the AISTATS 2018 review, these two goals (colloquially known as minimizing risk and maximizing benefit, see [10]) are the basis for measuring the effectiveness of privacy practices. Risk and benefit are always in conflict, which means that protection methods provide protection methods with a certain level of benefit for a certain level of risk. Ideally, we want to produce a variety of protection methods to produce different levels of risk and use, so that data providers publish data as they see it."}, {"heading": "2 Problem Formulation", "text": "Our proposed methodology addresses two fundamental needs that are at odds when publishing research data subject to privacy restrictions. First, the published data should minimize the risk, or in this case the identification, for the individuals contained in the data as much as possible. Second, the published data should retain as much research value (i.e. benefit) of the original data as possible. In the following sections, we define how we measure these two results."}, {"heading": "2.1 Re-identification attack model", "text": "In fact, most people who have lived in the US for the past ten years are unable to understand the world and why they are able to change the world and change it, \"he said.\" But it's not that they are able to change the world. \"He added,\" It's not that they are able to change the world. \"He added,\" It's not that they are able to change the world. \"He added,\" It's not that they have been able to change the world. \"He added,\" But it's not that they have been able to save the world. \""}, {"heading": "2.2 Evaluating research utility", "text": "We measure the usefulness or usefulness of the published tweet Corpus in two ways: the general word distribution and similarity of the results from commonly used text analysis. Usefulness is always compared with the baseline, i.e. the original, unchanged data, since the maximum benefit would come from a researcher having access to the original tweets. For distributional similarity, we use the cosmic similarity of the unigrams and bigrams between the original and any modified corpus, defined as: CosSim (x, y) = ixiyi \u221a ix2iwhere x and y are term frequency vectors for a user in the original and the modified corpus, defined as: CosSim (x, y). We calculate the similarity for each user and take the average of all users as our measure of usefulness. We do this because each user in the corpus may have specific interests or possibilities of communication that a researcher wants to use."}, {"heading": "3 Protection method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A Multi-User Conditional Synthesis Model", "text": "In this paper, we aim to model the historical context underlying an observed tweet corpus, R, depending on the number of users, U. If we are able to accurately approximate this generative distribution, we can propose an architecture for the task of text synthesis and publish new synthetic tweets instead of the originals. In particular, to better understand the temporal and structural information inherent in the data, we assume that a finite dictionary of symbols D for input sequences c = (ct, ct \u2212 1, ct \u2212 m) is the distribution of interest p (ct, c < t), which is a 1-of-| D | encoding of a symbol (like a character)."}, {"heading": "3.2 Previous protection methods", "text": "Two other protection methods were applied to the data set of the publication: editing and iterative translation. For editing, we simply removed all the hashtags and handles that were present in the corpus, assuming that many users frequently repeat hashtags and handles, which makes these characteristics best used to identify a user. [19] The iterative translation was based on [19] and worked by first translating the original (edited) corpus into Arabic and then back into English using the Google Translate API1. Arabic was chosen because it turned out to offer the highest level of protection in [19]. Translation was done on the basis of the edited data, as the hashtags and handles would probably not change during iterative translation."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "The data set we are experimenting with consists of tweets collected by [3] for a study of German Twitter users. Twitter users were randomly selected (by generating uniformly random Twitter IDs and selecting them based on selection criteria at the professional level).For evaluation here, we only select tweets that were classified as English by Twitter's voice recognition algorithm. Subsequently, the data set was split into two parts as described in Section 2.1. Attack data set included 386,684 tweets from 627 users and publication data set 62,073 tweets from 627 users. 1cloud.google.com / translate"}, {"heading": "4.2 Experimental Design", "text": "The three variations of our neural synthesis model were the Elman RNN, the DeltaRNN and the GRU. Optimization of the parameters was done by stochastic gradient descent using the ADAM [15] adaptive learning rate scheme (step size \u03bb = 0.002 was considered sufficient in preliminary experiments).All models were trained for 150 epochs, with updates calculated using minibatches of 32 samples. Since tweets are limited, we were able to take full back-propagation over time (without any truncation).Regularization consisted of layer normalization [2], where pre-activations within each architecture were applied to the normalization provided in the supplied material. For each of the three synthetic corpora, five synthetic corpora were created for each of the temperatures {0.25, 0.5, 1, 1.25, 1.75}. The general risk description was applied to two gradual dimensions and two grades, each with 2.1."}, {"heading": "4.3 Experimental Results", "text": "In fact, most of them are able to survive themselves without being able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "5 Discussion", "text": "This paper presents a novel method of reducing risk by stylometric attacks on text data while preserving the original language. We show that a better informed risk-benefit balance is possible compared to earlier methods such as redaction and translation. In particular, we note that neural methods can reduce risk far beyond other methods, at the expense of benefit. In some cases, such as in unigram distributions or the classification task, neural methods offered better trade, while they did not in the case of bigrams or the sentiment task. Either way, when the temperature (output distribution) is used as a \"stick\" to control risk and benefit, neural models allow us to set the appropriate levels while maintaining a high benefit, still allowing over 50% re-identification of users who are unacceptably high."}], "references": [{"title": "Stylometric linkability of tweets", "author": ["M. ALMISHARI", "D. KAAFAR", "E. OGUZ", "G. TSUDIK"], "venue": "In Proceedings of the 13th Workshop on Privacy in the Electronic Society (2014),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Are governments more responsive to voters in issues they own? a comparative study of the quality of political representation using social media data", "author": ["P. BARBERA", "J. B\u00d8LSTAD"], "venue": "Presented at Polmeth", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "AND JAU- VIN, C. A neural probabilistic language model", "author": ["Y. BENGIO", "R. DUCHARME", "P. VINCENT"], "venue": "Journal of machine learning research 3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making", "author": ["P. BURNAP", "M.L. WILLIAMS"], "venue": "Policy & Internet 7,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Translate once, translate twice, translate thrice and attribute: Identifying authors and machine translation tools in translated text", "author": ["A. CALISKAN", "R. GREENSTADT"], "venue": "In Semantic Computing (ICSC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "When coding style survives compilation: De-anonymizing programmers from executable binaries", "author": ["A. CALISKAN-ISLAM", "F. YAMAGUCHI", "E. DAUBER", "R. HARANG", "K. RIECK", "R. GREENSTADT", "A. NARAYANAN"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A recurrent latent variable model for sequential data. In Advances in neural information processing systems", "author": ["J. CHUNG", "K. KASTNER", "L. DINH", "K. GOEL", "A.C. COURVILLE", "Y. BENGIO"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Disclosure risk vs. data utility: The ru confidentiality map", "author": ["G.T. DUNCAN", "S.A. KELLER-MCNULTY", "S.L. STOKES"], "venue": "Citeseer", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Generative adversarial nets", "author": ["I. GOODFELLOW", "J. POUGET-ABADIE", "M. MIRZA", "B. XU", "D. WARDE-FARLEY", "S. OZAIR", "A. COURVILLE", "Y. BENGIO"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. GRAVES"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A parsimonious rule-based model for sentiment analysis of social media text", "author": ["C.J. HUTTO", "GILBERT", "E. Vader"], "venue": "In Eighth international AAAI conference on weblogs and social media", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Obfuscating document stylometry to preserve author anonymity", "author": ["G. KACMARCIK", "M. GAMON"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["KINGMA D", "BA"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. KINGMA", "M. WELLING"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. LE", "T. MIKOLOV"], "venue": "arXiv preprint arXiv:1405.4053", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A persona-based neural conversation model", "author": ["J. LI", "M. GALLEY", "C. BROCKETT", "J. GAO", "B. DOLAN"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "The best way to a strong defense is a strong offense: Mitigating deanonymization attacks via iterative language translation", "author": ["N. MACK", "J. BOWERS", "H. WILLIAMS", "G. DOZIER", "J. SHELTON"], "venue": "International Journal of Machine Learning and Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. MIKOLOV", "M. KARAFI\u00c1T", "L. BURGET", "J. CERNOCK\u1ef2", "S. KHUDANPUR"], "venue": "In Interspeech (2010),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "On the feasibility of internet-scale author identification", "author": ["A. NARAYANAN", "H. PASKOV", "N.Z. GONG", "J. BETHEN- COURT", "E. STEFANOV", "E.C.R. SHIN", "D. SONG"], "venue": "In Security and Privacy (SP),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning a deep hybrid model for semi-supervised text classification", "author": ["A.G. ORORBIA II", "C.L. GILES", "D. REITTER"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["A.G. ORORBIA II", "C.L. GILES", "D. REITTER"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning simpler language models with the differential state framework", "author": ["A.G. ORORBIA II", "T. MIKOLOV", "D. REITTER"], "venue": "Neural Computation", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Online learning of deep hybrid architectures for semisupervised categorization. In Machine Learning and Knowledge Discovery in Databases (Proceedings", "author": ["A.G. ORORBIA II", "D. REITTER", "J. WU", "C.L. GILES"], "venue": "ECML PKDD 2015),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Can pseudonymity really guarantee privacy", "author": ["J.R. RAO", "P ROHATGI"], "venue": "In USENIX Security Symposium", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Ethical research standards in a world of big data", "author": ["C.M. RIVERS", "B.L. LEWIS"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Piecewise latent variables for neural variational text processing", "author": ["I.V. SERBAN", "A. ORORBIA II", "J. PINEAU", "A. COURVILLE"], "venue": "In Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Ladder variational autoencoders", "author": ["C.K. S\u00d8NDERBY", "T. RAIKO", "L. MAAL\u00d8E", "S.K. S\u00d8NDERBY", "O. WINTHER"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. SORDONI", "M. GALLEY", "M. AULI", "C. BROCKETT", "Y. JI", "M. MITCHELL", "NIE", "GAO J.-Y", "B. DOLAN"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems", "author": ["I. SUTSKEVER", "O. VINYALS", "LE", "Q. V"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "A neural conversational model", "author": ["VINYALS O", "LE"], "venue": "arXiv preprint arXiv:1506.05869", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Towards an ethical framework for publishing twitter data in social research: Taking into account users\u2019 views, online context and algorithmic estimation", "author": ["M.L. WILLIAMS", "P. BURNAP", "L. SLOAN"], "venue": "Sociology", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "but the data is already public\u201d: on the ethics of research in facebook", "author": ["M. ZIMMER"], "venue": "Ethics and information technology 12,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "the field of data privacy, these two goals (colloquially known as minimizing risk and maximizing utility, see [10]) form the basis for measuring the effectiveness of any protection method.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "These models have been shown to be successful in the task of author attribution in document collections such as the Federalists papers [21], online blogs [22], and even executable binaries [7].", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "These models have been shown to be successful in the task of author attribution in document collections such as the Federalists papers [21], online blogs [22], and even executable binaries [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "Recent work has also shown similar promise in short texts such as tweets (which exhibit high levels of linkability), despite the small character limit [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 30, "context": "As some others have pointed out [35], Twitter states in their developer guidelines that users have the right to be forgotten [33], such that if tweets are deleted, researchers will not use or release those tweets further.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 31, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 24, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 3, "context": "g identifying hate speech [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "These include semi-automatic methods that can obscure individual authors [14] or apar X iv :1 60 6.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 39, "endOffset": 47}, {"referenceID": 16, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 39, "endOffset": 47}, {"referenceID": 4, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 17, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 28, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 21, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 25, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 18, "context": "We emulate the stylometric attack by using an approach akin to that of [22] and [1].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "We emulate the stylometric attack by using an approach akin to that of [22] and [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "We use the uni-grams and bi-grams as simple common sense feature sets, and we used the feature sets from [1] and [22] because they were both shown to perform quite well for re-identification.", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "We use the uni-grams and bi-grams as simple common sense feature sets, and we used the feature sets from [1] and [22] because they were both shown to perform quite well for re-identification.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "These models were used in [22, 1].", "startOffset": 26, "endOffset": 33}, {"referenceID": 0, "context": "These models were used in [22, 1].", "startOffset": 26, "endOffset": 33}, {"referenceID": 18, "context": "KNN, we use row and feature normalization following in line with [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "As suggested by [22] we collapse the feature vectors for each user in R to the mean for testing.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "For the second, model-specific measure, we analyze how well the different protection methods preserve sentiment by user, using the \u201cVader\u201d sentiment model, specifically developed to measure sentiment in social media data [13].", "startOffset": 221, "endOffset": 225}, {"referenceID": 8, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 26, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 22, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 19, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 20, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 14, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 250, "endOffset": 254}, {"referenceID": 29, "context": "Interestingly enough, our specialized neural architecture also addresses a recent problem found in neural conversation agents [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "We note that other work has attempted to address the coherence/consistency problem in neural models [31, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 15, "context": "We note that other work has attempted to address the coherence/consistency problem in neural models [31, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 21, "context": "While we present the simplest formulation of our model above (the simple Elman-RNN), further improvement in bits-per-character (BPC) was found when experimenting with more complex gated models, unified under the Differential State Framework [25].", "startOffset": 241, "endOffset": 245}, {"referenceID": 21, "context": "Variations we experimented with included the concrete model proposed in [25], the Delta-RNN, and a Gated Recurrent Unit (GRU) [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "To generate samples from the neural model, we simply make use of the model\u2019s efficient inference procedure, similar to that in [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "The iterative translation was based on [19] and worked by first translating the original (redacted) corpus into Arabic and then back into English using the Google Translate API1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Arabic was chosen because it was found to offer the highest level of protection in [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "The dataset we experiment with consists of tweets collected by [3] for a study on German twitter users.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Optimization of parameters was carried out through stochastic gradient descent using the ADAM [15] adaptive learning rate scheme (step size of \u03bb = 0.", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "comparable re-identification levels compared to previous work [22, 1], which supports our risk models.", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "comparable re-identification levels compared to previous work [22, 1], which supports our risk models.", "startOffset": 62, "endOffset": 69}, {"referenceID": 13, "context": "should include reformulating our models such that neural variational inference [16] may be employed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "Furthermore, operating under such a Bayesian framework would allow us to easily integrate better text-specific prior distributions, such as the piecewise-constant distribution [29], easing the learning of difficult, multi-modal distributions.", "startOffset": 176, "endOffset": 180}], "year": 2017, "abstractText": "We present a method for generating synthetic versions of Twitter data using neural generative models. The goal is to protect individuals in the source data from stylometric re-identification attacks while still releasing data that carries research value. To generate tweet corpora that maintain user-level word distributions, our proposed approach augments powerful neural language models with local parameters that weight user-specific inputs. We compare our work to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general language measures and two common text analysis tasks. We find that neural models are able to significantly lower risk over previous methods at the cost of some utility. More importantly, we show that the risk utility trade-off depends on how the neural model\u2019s logits (or the unscaled preactivation values of the output layer) are scaled. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible.", "creator": "LaTeX with hyperref package"}}}