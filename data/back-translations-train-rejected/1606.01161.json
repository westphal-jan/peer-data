{"id": "1606.01161", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning", "abstract": "Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.", "histories": [["v1", "Fri, 3 Jun 2016 16:09:52 GMT  (757kb,D)", "http://arxiv.org/abs/1606.01161v1", "11 pages, 4 figures"]], "COMMENTS": "11 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiang guo", "wanxiang che", "haifeng wang", "ting liu"], "accepted": false, "id": "1606.01161"}, "pdf": {"name": "1606.01161.pdf", "metadata": {"source": "CRF", "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning", "authors": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn", "wanghaifeng@baidu.com"], "sections": [{"heading": null, "text": "This paper provides a universal framework for the use of these multi-layered tree banks to improve parsing through in-depth multi-task learning. We consider two types of tree banks as sources: the multilingual universal tree banks and the monolingual heterogeneous tree banks. Multiple tree banks are trained together and interact with the sharing of parameters at multiple levels. Experiments on multiple benchmark datasets in different languages show that our approach can effectively use arbitrary source tree banks to improve target parameter models."}, {"heading": "1 Introduction", "text": "This year, we have reached the point where we are able to live in a country where most people are able to develop, and where most of them are able to develop, to develop, to develop."}, {"heading": "2 Related Work", "text": "The current work refers to several strands of previous studies. Monolingually, we are able to generate a complete representation. Exploitation of heterogeneous treebanks for parsing has been investigated in various ways. (2009) It is possible to transform the dependency structure CDT into the phrase structure of CTB5 by using a trained constituency parser on CTB5, and then combine the converted treebanks for parsing. (2012) It is also possible to grasp the inconsistencies between different types of transformation patterns based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to complement the baseline parsing models. Johansson (2013) also takes up the idea of parameter sharing to integrate multiple treebanks. They focused on parameter sharing at the level with discrete representations limiting their multiplicity to the surface."}, {"heading": "3 Approach", "text": "This section describes the deep multi-task learning architecture using a formalism that extends to the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015), which is further enhanced by character modeling (Ballesteros et al., 2015)."}, {"heading": "3.1 Transition-based Neural Parsing", "text": "At the heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system on which the most likely transition is determined. Typically, a state comprises three primary components, a stack, a buffer, and a set of arcs of dependencies. Traditional parsing models treat properties extracted from manually defined characteristic patterns in a discrete attribute space that suffers from the problems of thriftiness, incompleteness, and extensive attribute compression. Instead, the neural network model proposed by Chen and Manning represents features extracted from continuous, low-dimensional vectors using a cube activation function for implicit attribute composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016)."}, {"heading": "3.2 Deep Multi-task Learning", "text": "In fact, the two tasks are multilingual or monolingual heterogeneities that are mutually beneficial to each other. We view target processing as the primary task, and source tracking as a related task. Multilingual or monolingual heterogeneity interacts between the two tasks."}, {"heading": "3.2.1 Parameter Sharing", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2.2 Learning", "text": "The training takes place stochastically by circling the tasks in a loop: 1. Select a task at random. 2. Select a sentence from the task and create instances for classification. 3. Update the corresponding parameters by backpropagating the instances. 4. Go to 1. We take over the development data of the target tree bank (primary task) for an early stop."}, {"heading": "4 Experiments", "text": "We first describe the data and settings in our experiments, then the results and analyses."}, {"heading": "4.1 Data and Settings", "text": "We are conducting experiments with UDT v2.03 and the CoNLL-X Shared Task Data. For monolingual heterogeneous sources, we are also experimenting with CTB5 using CDT as source tree base to compare with the previous work by Li et al. (2012). Statistics of the data sets are in Table 2. We are examining the following experiment settings: \u2022 MULTILINGUAL (UNIV \u2192 UNIV). In this setting, we are examining the integration of multilingual universal tree banks. Experiments are being carried out using the UDT data set. Specifically, we are considering DE, ES, FR, PT, IT and SV tree banks as target tree banks and the EN tree bank as common source tree bank. \u2022 MONOLINGUAL (CONLL) and the languages UNIV. Here, we are examining the integration of monolingual heterogeneous tree banks."}, {"heading": "4.2 Baseline Systems", "text": "We compare our approach with the following base systems. \u2022 Monolingual Supervised Training (SUP). Models are trained only on the target tree bank, with the LSTM-based parser. \u2022 Cascaded Training (CAS). This system has two stages. First, models are trained on the source tree bank. Subsequently, the parameters are used to initialize the neural network for training target parsers. A similar approach was used in Duong et al. (2015a) and Guo et al. (2016) for Parsing. For MULTILINGUAL (UNIV \u2192 UNIV), we also compare with the flat multi-task learning system (SMTL) as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL, all parameters except character embedding (echar) are shared, and tasks are embedded (not Duet al)."}, {"heading": "4.3 Results", "text": "In this section we present empirical evaluations under different conditions."}, {"heading": "4.3.1 Multilingual Universal Source Treebanks", "text": "Table 3 shows the results of the MULTILINGUAL (UNIV \u2192 UNIV) setting. CAS delivers slightly better performance than SUP, especially for SV (+ 1.52% UAS and + 2.04% LAS), which means that the training helps two treebanks along with a unique model. Furthermore, our deep Multi-Task Approach (MTL) exceeds the overall performance of SUP and achieves the best performance in five out of six languages, with the exception of Sweden."}, {"heading": "4.3.2 Monolingual Hetero. Source Treebanks", "text": "Table 6 shows the results of MONOLINGUAL (CONLL \u2194 UNIV). Overall, MTL systems exceed the monitored baselines by significant margins in both conditions, demonstrating the mutual benefit of UDT and CONLL-X tree banks. 4In addition, among the four languages here, we find a large proportion of SV test data in UDT / CONLL-X in the CONLL-X / UDT-SV training data (Nivre and Megyesi, 2007), i.e. a large overlap with the CONLL-X tree bank. In fact, we find a large proportion of SV test data in UDT / CONLL-X appears in the CONLL-X / UDT SV training data. Typically, we expect completely invisible data for testing, so we split the SV test data into two parts: IN-SRC and OUT-SRC including the sets that appear in the source tree base or not."}, {"heading": "4.4 Remarks", "text": "Overall, our approach yields significant gains over monitored baselines with either multilingual universal tree banks or monolingual heterogeneous tree banks as sources. With multilingual source tree banks, our model has the potential to improve even further by language-specific matching. Although this is not the primary focus of this study, we show that with limited resources, more emphasis can be placed on source tree banks through weighted task samples."}, {"heading": "5 Conclusion", "text": "We examine two scenarios, each using multilingual universal source tree banks and monolingual heterogeneous source tree banks, and design effective parameter exchange strategies for each scenario. We conduct extensive experiments on multiple benchmark tree banks in different languages, and the results show that our approach to base systems significantly improves under different experiments. In addition, our framework can flexibly integrate richer tree populations and associated tasks that we leave to future explorations."}, {"heading": "Acknowledgments", "text": "We thank Ryan McDonald for fruitful discussions and Dr. Zhenghua Li for sharing the processed CTB and CDT dataset. This work was supported by the National Key Basic Research Program of China on funding 2014CB340503 and the National Natural Science Foundation of China (NSFC) on funding 61133012 and 61370164."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transition-based system for joint part-ofspeech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. of the 2012 Joint Conference on EMNLP and CoNLL,", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In Proc. of the Tenth Conference on Computational Natural Language Learning (CoNLLX),", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "Two languages are better than one (for syntactic parsing)", "author": ["Burkett", "Klein2008] David Burkett", "Dan Klein"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Burkett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Burkett et al\\.", "year": 2008}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Bitext dependency parsing with bilingual subtree constraints", "author": ["Chen et al.2010] Wenliang Chen", "Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "In Proc. of the 48th ACL,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of the 25th ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong et al.2015] Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "2015a. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015a] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "2015b. A neural network model for low-resource universal dependency parsing", "author": ["Duong et al.2015b] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Crosslingual dependency parsing based on distributed representations", "author": ["Guo et al.2015] Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Guo et al.2016] Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), February", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese", "author": ["Hatori et al.2012] Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In Proc. of the 50th ACL (Volume 1: Long Papers),", "citeRegEx": "Hatori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2012}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "Titov2010] James Henderson", "Ivan Titov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["Paola Merlo", "Ivan Titov", "Gabriele Musillo"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Bilingually-constrained (monolingual) shift-reduce parsing", "author": ["Huang et al.2009] Liang Huang", "Wenbin Jiang", "Qun Liu"], "venue": "In Proc. of the 2009 Conference on EMNLP,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Hwa et al.2005] Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural language engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Training parsers on incompatible treebanks", "author": ["Richard Johansson"], "venue": "In Proc. of NAACL: HLT,", "citeRegEx": "Johansson.,? \\Q2013\\E", "shortCiteRegEx": "Johansson.", "year": 2013}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Li et al.2011] Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li"], "venue": "In Proc. of the 2011 Conference on EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Exploiting multiple treebanks for parsing with quasi-synchronous grammars", "author": ["Li et al.2012] Zhenghua Li", "Ting Liu", "Wanxiang Che"], "venue": "In Proc. of the 50th ACL (Volume 1: Long Papers),", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Multi-task sequence to sequence learning. CoRR, abs/1511.06114", "author": ["Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proc. of the 2011 Conference on EMNLP,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Exploiting heterogeneous treebanks", "author": ["Niu et al.2009] Zheng-Yu Niu", "Haifeng Wang", "Hua Wu"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2009}, {"title": "Integrating graph-based and transitionbased dependency parsers", "author": ["Nivre", "McDonald2008] Joakim Nivre", "Ryan McDonald"], "venue": "In Proc. of ACL-08: HLT,", "citeRegEx": "Nivre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2008}, {"title": "Bootstrapping a swedish treebank using cross-corpus harmonization and annotation projection", "author": ["Nivre", "Megyesi2007] Joakim Nivre", "Beata Megyesi"], "venue": "In Proc. of the 6th International Workshop on Treebanks and Linguistic Theories,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proc. of the Eighth International Conference on Language Resources and Evaluation", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Density-driven crosslingual transfer of dependency parsers", "author": ["Rasooli", "Michael Collins"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Rasooli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasooli et al\\.", "year": 2015}, {"title": "Parser adaptation and projection with quasisynchronous grammar features", "author": ["Smith", "Eisner2009] David A. Smith", "Jason Eisner"], "venue": "In Proc. of the 2009 Conference on EMNLP,", "citeRegEx": "Smith et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2009}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL: HLT,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proc. of COLING", "citeRegEx": "Tiedemann.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Fast and robust multilingual dependency parsing with a generative latent variable model", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proc. of the CoNLL Shared Task Session of EMNLPCoNLL", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Stacking dependency parsers", "author": ["Dipanjan Das", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Martins et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Zhang", "Barzilay2015] Yuan Zhang", "Regina Barzilay"], "venue": "In Proc. of the 2015 Conference on EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proc. of the 2008 Conference on EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Jointly or separately: Which is better for parsing heterogeneous dependencies", "author": ["Zhang et al.2014] Meishan Zhang", "Wanxiang Che", "Yanqiu Shao", "Ting Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "A neural probabilistic structured-prediction model for transition-based dependency parsing", "author": ["Zhou et al.2015] Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen"], "venue": "In Proc. of the 53rd ACL and the 7th IJCNLP (Volume 1: Long Papers),", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widelyused Penn Treebank (Marcus et al., 1993).", "startOffset": 171, "endOffset": 192}, {"referenceID": 25, "context": "To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasisynchronous grammar features (Li et al.", "startOffset": 160, "endOffset": 178}, {"referenceID": 21, "context": ", 2009), quasisynchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models.", "startOffset": 43, "endOffset": 60}, {"referenceID": 19, "context": ", 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models.", "startOffset": 42, "endOffset": 59}, {"referenceID": 18, "context": "Cross-lingual supervision has proven highly beneficial for low-resource language parsing (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars.", "startOffset": 89, "endOffset": 130}, {"referenceID": 24, "context": "Cross-lingual supervision has proven highly beneficial for low-resource language parsing (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars.", "startOffset": 89, "endOffset": 130}, {"referenceID": 22, "context": "Niu et al. (2009) automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a trained constituency parser on CTB5, and then combined the converted treebanks for constituency parsing.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models.", "startOffset": 0, "endOffset": 17}, {"referenceID": 19, "context": "Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks.", "startOffset": 0, "endOffset": 17}, {"referenceID": 39, "context": ", 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 18, "context": "inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al.", "startOffset": 74, "endOffset": 136}, {"referenceID": 33, "context": "inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al.", "startOffset": 74, "endOffset": 136}, {"referenceID": 24, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 32, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 12, "context": ", 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015).", "startOffset": 70, "endOffset": 161}, {"referenceID": 6, "context": "Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning.", "startOffset": 67, "endOffset": 131}, {"referenceID": 17, "context": "Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning.", "startOffset": 67, "endOffset": 131}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012).", "startOffset": 124, "endOffset": 186}, {"referenceID": 20, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012).", "startOffset": 124, "endOffset": 186}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings.", "startOffset": 125, "endOffset": 326}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings.", "startOffset": 125, "endOffset": 434}, {"referenceID": 14, "context": "There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010).", "startOffset": 125, "endOffset": 556}, {"referenceID": 8, "context": "Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.", "startOffset": 0, "endOffset": 169}, {"referenceID": 11, "context": "This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 1, "context": ", 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015).", "startOffset": 57, "endOffset": 83}, {"referenceID": 1, "context": ", 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks.", "startOffset": 58, "endOffset": 152}, {"referenceID": 11, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 36, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 40, "context": "More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016).", "startOffset": 77, "endOffset": 155}, {"referenceID": 11, "context": "Note that the subtrees within the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st,bt and at, and passed through a component-wise ReLU:", "startOffset": 119, "endOffset": 138}, {"referenceID": 28, "context": "We apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees.", "startOffset": 71, "endOffset": 84}, {"referenceID": 8, "context": "For example, Collobert and Weston (2008) only shares word embeddings, and Dong et al. (2015) shares the encoder of sequence-to-sequence models.", "startOffset": 74, "endOffset": 93}, {"referenceID": 29, "context": "Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and thus share the same set of transition actions.", "startOffset": 77, "endOffset": 98}, {"referenceID": 29, "context": "For simplicity reasons, we convert the language-specific POS tags in the heterogeneous treebanks into universal POS tags (Petrov et al., 2012).", "startOffset": 121, "endOffset": 142}, {"referenceID": 20, "context": "For monolingual heterogeneous source, we also experiment on CTB5 using CDT as the source treebank, to compare with the previous work of Li et al. (2012). Statistics of the datasets are summarized in Table 2.", "startOffset": 136, "endOffset": 153}, {"referenceID": 20, "context": "We follow the same settings of Li et al. (2012), and consider two scenarios using automatic POS tags and gold-standard POS tags respectively.", "startOffset": 31, "endOffset": 48}, {"referenceID": 9, "context": "Similar approach was studied in Duong et al. (2015a) and Guo et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 9, "context": "Similar approach was studied in Duong et al. (2015a) and Guo et al. (2016) for low-resource parsing.", "startOffset": 32, "endOffset": 75}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used.", "startOffset": 168, "endOffset": 213}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used. Unlike Duong et al. (2015b) and Ammar et al.", "startOffset": 168, "endOffset": 359}, {"referenceID": 9, "context": "For MULTILINGUAL (UNIV\u2192UNIV), we also compare with the shallow multi-task learning (SMTL) system, as described in Section 2, which is representative of the approach of Duong et al. (2015b) and Ammar et al. (2016). In SMTL all the parameters are shared except the character embeddings (Echar), and task embeddings (e) are not used. Unlike Duong et al. (2015b) and Ammar et al. (2016), we don\u2019t use external resources such as cross-lingual word clusters, embeddings and dictionaries which is beyond the scope of this work.", "startOffset": 168, "endOffset": 383}, {"referenceID": 9, "context": "To verify the second issue, we consider a low resource setup following Duong et al. (2015b), where the target language has a small treebank (3K tokens).", "startOffset": 71, "endOffset": 92}, {"referenceID": 9, "context": "shared by Duong et al. (2015b) on DE, ES and FR.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "sibling and grandparent structures, while LI12-O2SIB only use the sibling parts (Li et al., 2012).", "startOffset": 80, "endOffset": 97}, {"referenceID": 21, "context": "For CTB5, we follow (Li et al., 2012) and consider two scenarios which use automatic POS tags and gold-standard POS tags respectively.", "startOffset": 20, "endOffset": 37}], "year": 2016, "abstractText": "Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multitask learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.", "creator": "LaTeX with hyperref package"}}}