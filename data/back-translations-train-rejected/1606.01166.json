{"id": "1606.01166", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Generalizing CNNs for data structured on locations irregularly spaced out", "abstract": "Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones.", "histories": [["v1", "Fri, 3 Jun 2016 16:18:22 GMT  (42kb,D)", "http://arxiv.org/abs/1606.01166v1", null], ["v2", "Mon, 27 Jun 2016 09:41:55 GMT  (80kb,D)", "http://arxiv.org/abs/1606.01166v2", null], ["v3", "Tue, 4 Jul 2017 11:46:46 GMT  (79kb,D)", "http://arxiv.org/abs/1606.01166v3", null], ["v4", "Wed, 25 Oct 2017 12:28:26 GMT  (80kb,D)", "http://arxiv.org/abs/1606.01166v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["jean-charles vialatte", "vincent gripon", "gr\\'egoire mercier"], "accepted": false, "id": "1606.01166"}, "pdf": {"name": "1606.01166.pdf", "metadata": {"source": "CRF", "title": "Generalizing the Convolution Operator to Extend CNNs to Irregular Domains", "authors": ["Jean-Charles Vialatte", "Vincent Gripon", "Gr\u00e9goire Mercier"], "emails": ["1jean-charles.vialatte@cityzendata.com", "gregoire.mercier}@telecom-bretagne.eu"], "sections": [{"heading": "1 Introduction", "text": "Unlike classical multi-layer perceptrons (MLPs) [2], CNNs use the underlying structure of the input, but they cannot always be applied directly, even if the data is an exploitable underlying structure. An example of such data is spatio-temporal time series generated by a series of devices in the Internet of Things. They usually consist of data points distributed over a Euclidean space. As a result, a diagram G can be defined in which the devices and edges connect adjacent units. Other examples include signals on graphs distributed irregularly over a space."}, {"heading": "2 Related Works", "text": "For graph-structured data, Bruna et al. [4] have proposed an extension of CNN by applying graph signal processing theory [3], whose folding is defined in the spectral domain related to the laplac matrix of the graph. However, in the case where the graph is a lattice, the construction is analogous to the regular folding defined in the Fourier domain. The operation is defined as spectral multipliers obtained by smooth interpolation of a weight nucleus, and they explain how it ensures localization characteristics. In this paper, they also define a construction that creates a multi-resolution structure of a graph so that it can support a deep learning architecture. Henaff et al. [5] have expanded Bruna's spectral network to large-scale classification tasks, and have proposed both supervised and unsupervised methods to find an underlying graph structure when it is already partially unstructured."}, {"heading": "3 Proposed convolution operator", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Definitions", "text": "Let S be a set of points so that each defines a set of neighborhoods. An entry e of a dataset D is a column vector that can be of any size, for which each dimension represents a value taken by e at a certain point.D is said to be two entries e and e \u00b2 homogeneous if they are of the same size, and if their dimensions are always associated with the same points.3.2 Formal description of the generalized convolutionLet's will be embedded in S. We say that two entries e and e \u00b2 s are homogeneous if they are of the same size, and if their dimensions are always associated with the same points.D will be a generalized convolution operator.We want C to observe the following conditions: \u2022 The Locality \u2022 The Location \u2022 The Kernel Weighting \u2022 The kernel weight increases that are capable of."}, {"heading": "3.3 Generalized convolution supported by an underlying graph", "text": "If all entries are homogeneous, then Ge is called static. In this case, we find that Ge contains the edge (u \u2032, u) if and only if u. \"Let's leave aV: e 7 \u2192 Ae to be a map, so that the jth column of Aei will have a non-zero coordinate if and only if (ei, ej) is an edge of GeV. Then the generalized convolution of e by the pair (w, aV) is supported by the underlying graph GeV in the sense that W e = w\" aV (e) is its adaptation matrix."}, {"heading": "3.4 Example of a generalized convolution shaped by a rectangular window", "text": "We assume that its weight core w is of the quantity Nw = (2p + 1) (2q + 1) and that R is wide (2p + 1) and high (2q + 1), where \u00b5 is a given unit of measurement. LetGep, q is the subgraph of G e, so that it contains the edge (u \u2032, u) if and only if | u \u2032 x | \u2264 (p + 12) \u00b5 and | u \u2032 y \u2212 uy (q + 12) \u00b5. In other words, Gep, q connects a vertex u with each vertex centered in R. Then we define CR as being supported by Gep, q."}, {"heading": "3.5 Link with the standard convolution on image datasets", "text": "Let D be an image dataset. Entries of D are homogeneous and their dimensions represent the value at each pixel. In this case, we can set S = E, dimension 2, so that each pixel is at whole coordinates. Specifically, if the images are of width n and height m, then the pixels are at coordinates (i, j): 0, 1, 2,..., n} x {0, 1, 2,..., m}. Therefore, the pixels lie on a regular grid and are therefore at a constant distance \u00b5 = 1. Consider the static underlying graph Gp, q and the generalized folding by a rectangular window CR, as defined in the first section. Then, applying the same weight allocation strategy causes each weight of the core to be affected in the moving window R. Except at the boundary, one pixel falls and only one pixel in each square of the moving grid at each position, as shown in Figure 2, where a window behaves exactly like a moving one R."}, {"heading": "4 Application to CNNs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Neural network interpretation", "text": "Ld and Ld + 1 are two layers of neurons, so the forward propagation from Ld to Ld + 1 is defined. Let's define these layers as a series of neurons located in S. These layers must contain as many neurons as can be activated. In other words: S \u0445 = Ld \u0445 = Ld + 1. As such, we misuse the term neuron instead of point. The generalized linkage between these two layers can be interpreted as follows. An entry e activates the same N neurons in each layer. Then, a folding shape takes N positions on Ld, with each position assigned to one of the activated neurons of Ld + 1. At each position, connections from the activated neurons within the folding shape in the destination are drawn to the associated neuron. And a subset of weights from w is divided among these connections, according to a weight distribution strategy defined by a mapping structure in a diagram 3."}, {"heading": "4.2 Implementation", "text": "There are two main strategies for implementing the dispersions: the first is to start from (1), derive and vectorize them, implying the use of semi-sparse representations to minimize memory consumption and to use adequate semi-sparse tensor products.Instead, we opt for the interpretation of neural networks and the underlying graph structure, whose edges are sufficient for neuron connections."}, {"heading": "4.3 Forward and back propagation formulae", "text": "First of all, let us remember the propagation formulas from a neural network point of view. With eu, let us denote the value of a neuron of Ld located at the last layer of Ld + 1, with fv for a neuron of Ld + 1, and with gv, if this neuron is activated by the activation function of Ld + 1. We denote with prev (v) the placed neurons of the previous layer connected to v, and with next (u) those of the next layer connected to u.wuv, the weight that affects the connection between neurons u and v. b. After the forward propagation, the values of the neurons of Ld + 1 will be expressed by those of Ld: fv = previous u (v) euwuv (4) gv = forward output (fv + b) (5) Thanks to the chain rule, we can express the derivatives of a layer with those of the next layer of Ld + 1."}, {"heading": "4.4 Vectorization", "text": "The calculations are performed per group of inputs. B. Therefore, the graph structure used for the calculations must contain the weighted edges of all inputs. B. If necessary, inputs from B are made homogeneous: If a neuron u is not activated by an entry e, but by another entry of B. Then eu is defined and set to zero. Thus, the 3D tensor counterparts of e, f, and g are indexed by E, F, and G. Their third dimension indexes the channels (input channels or feature maps). Their submatrix along the neuron at x-S is called Ex, Fx, and Gx, the rows index entries, and the columns index channels. The counterparts of w and b are W and \u03b2. The first is a 3D tensor, and the second is a vector with one value per attribute."}, {"heading": "5 Experiments", "text": "To measure the performance gain that generalized CNN allows over MLP in irregular ranges, we created a set of benchmarks for distorted versions of the MNIST dataset [8], consisting of images of 28x28 pixels. To distort the input domain, we submerged the images in a 2-D euclidean space by converting whole coordinates into pixels. We then applied a Gaussian shift to each pixel, making the data irregular and unsuitable for regular rotations. For several values of the standard deviation of the shift, we trained a generalized CNN and compared it to an MLP that has the same number of parameters. We chose a simple but standard architecture to better see the effects of generalized layers. [1] The architecture used is the following: a generalized convolutional layer with relief [9] and a maximum summary of 20 characteristic maps followed by a soft density and a layer."}, {"heading": "6 Conclusion and future work", "text": "This operator enables the CNN paradigm to be transported into irregular areas. It retains the characteristics of a normal convolution operator, because it is linear, locally supported, and uses the same weight core for each local operation. The general convolution operator can then, of course, be used instead of conventional layers in a deep learning frame. Typically, the model created is good for input data with an underlying graph structure. The definition of this operator is flexible enough to adapt it to each input domain, so that the distribution of the core weight can be done in a natural way for that domain, depending on the case. However, in some cases there is no natural way, but several acceptable methods for defining the weight distribution. In further work, we plan to study these methods. We also plan to apply the generalized operator to unattended learning tasks."}, {"heading": "Acknowledgements", "text": "I would like to thank my academic mentors Vincent Gripon and Gr\u00e9goire Mercier, who helped me with this work, as well as my industrial mentor Mathias Herberts, who gave me insights into the application of the proposed model to industrial datasets, which was partly funded by Cityzen Data, the company behind the Warp10 platform, and ANRT (Agence Nationale de la Recherche et de la Technologie) through a CIFRE (Convention Industrielle de Formation par la REcherche) and the European Research Council under the Seventh Framework Programme of the European Union (FP7 / 2007-2013) / ERC Funding Agreement No. 290901."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, vol. 30, no. EPFL-ARTICLE-189192, pp. 83\u201398, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1891}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6203, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Shapenet: Convolutional neural networks on non-euclidean manifolds", "author": ["J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst"], "venue": "tech. rep., 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatially-sparse convolutional neural networks", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1409.6070, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 315\u2013323, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pp. 177\u2013186, Springer, 2010. 8", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1139\u20131147, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature selection, l 1 vs. l 2 regularization, and rotational invariance", "author": ["A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, p. 78, ACM, 2004. 9", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "CNNs[1] are state-of-the-art for performing supervised learning on data defined on lattices.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "Contrary to classical multilayer perceptrons (MLPs) [2], CNNs take advantage of the underlying structure of the inputs.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "In graph signal processing, extended convolutional operators exploiting G have been proposed [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "These operators have been applied to deep learning [4] and obtain performance similar to CNNs on regular domains, despite the fact they differ from classical convolutions.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "[4] have proposed an extension of the CNN using the graph signal processing theory [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] have proposed an extension of the CNN using the graph signal processing theory [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "[5] have extended Bruna\u2019s spectral network to large scale classification tasks and have proposed both supervised and unsupervided methods to find an underlying graph structure when it isn\u2019t already given.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "These properties are also retained by the convolution defined in the ShapeNet paper[6], which defines a convolution for data living on non-euclidean manifolds.", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "In the case where the data is sparse, Graham [7] has proposed a framework to implement a spatially sparse CNN efficiently.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "In order to measure the gain of performance allowed by the generalized CNN over MLP on irregular domains, we made a series of benchmarks on distorded versions of the MNIST dataset [8], consisting of images of 28x28 pixels.", "startOffset": 180, "endOffset": 183}, {"referenceID": 0, "context": "The architecture [1] used is the following: a generalized convolution layer with relu [9] and max pooling, made of 20 feature maps, followed by a dense layer and a softmax output layer.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "The architecture [1] used is the following: a generalized convolution layer with relu [9] and max pooling, made of 20 feature maps, followed by a dense layer and a softmax output layer.", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "For training, we used stochastic gradient descent [10] with Nesterov momentum [11] and a bit of L2 regularization [12].", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "Convolutional Neural Networks (CNNs) have become the state-of-the-art in supervised learning vision tasks. Their convolutional filters are of paramount importance for they allow to learn patterns while disregarding their locations in input images. When facing highly irregular domains, generalized convolutional operators based on an underlying graph structure have been proposed. However, these operators do not exactly match standard ones on grid graphs, and introduce unwanted additional invariance (e.g. with regards to rotations). We propose a novel approach to generalize CNNs to irregular domains using weight sharing and graph-based operators. Using experiments, we show that these models resemble CNNs on regular domains and offer better performance than multilayer perceptrons on distorded ones.", "creator": "LaTeX with hyperref package"}}}