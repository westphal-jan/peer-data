{"id": "1606.01269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning", "abstract": "This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.", "histories": [["v1", "Fri, 3 Jun 2016 20:32:52 GMT  (1358kb,D)", "http://arxiv.org/abs/1606.01269v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["jason d williams", "geoffrey zweig"], "accepted": false, "id": "1606.01269"}, "pdf": {"name": "1606.01269.pdf", "metadata": {"source": "CRF", "title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning", "authors": ["Jason D. Williams", "Geoffrey Zweig"], "emails": ["jason.williams@microsoft.com", "gzweig@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Consider the period after the war, when the war began, and the period after the Second World War."}, {"heading": "2 Model description", "text": "The three components of our model are a recurrent neural network, in which the user implements specific and well-encapsulated software that implements domain-specific functions, and a language understanding module. The software allows the developer to express his business logic by outputting actions that the developer finds useful for selecting actions, and the recurrent neural network is responsible for selecting the actions he takes. The neural network chooses from action patterns that go beyond the entities, such as the text action \"Do you want to call < name > API action is useful for selecting actions.\" Because a recurrent neural chooses a recurrent neural neural network, it chooses from the action patterns that extend beyond entities, such as the text action. \""}, {"heading": "3 Related work", "text": "Compared to previous work, it is helpful to look at the two main problems that dialog systems solve: state tracking, which relates to how information from the past is presented (whether human interpretable or not), and action selection, which relates to how the mapping from state to action is structured."}, {"heading": "3.1 State tracking", "text": "In a task-oriented dialog system, government tracking typically consists of tracking the user's goal, such as the type of cuisine and the price range used as search criteria for a restaurant, and the dialog history, such as whether a slot has already been requested or confirmed, whether a restaurant has already been offered, or whether a user has a favorite kitchen listed in his profile (Williams and Young, 2007). Most previous work on task-oriented dialog systems has used a handmade state representation framework for both of these quantities - i.e., the set of possible values for the user's goal and the dialog history have been designed manually. For example, in the State Tracking Challenge (DSTC), the state consists of a pre-defined framework of name / value pairs that form the user's goal (Williams et al., 2016). Many DSTC entries we learned from data on how to update the state by using rent methods such as neural neural networks."}, {"heading": "3.2 Action selection", "text": "All, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, everyone, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all, all"}, {"heading": "4 Example dialog task", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have played by."}, {"heading": "5 Optimizing with supervised learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Prediction accuracy", "text": "We first tried to measure whether the LSTM, which was trained with a small number of dialogs, would succeed in generalizing by performing a 21-fold leave-one-out cross-validation experiment. In each fold, a dialog was used as a test set, and four different training sets consisting of 1, 2, 5, 10, and 20 dialogs were formed. Within each fold, a model was trained on each training set, which was then evaluated on the basis of the test dialog performed. Training was performed using categorical cross-entropy as loss and with AdaDelta for smooth updates (pointer, 2012). Training was performed until the reconstruction of the training set. Figure 3 shows accuracy per turn and complete accuracy averaged over all 21 folds. After a single dialog, 70% of the dialog turns are correctly predicted. After 20 dialogs, this value rises to over 90%, with nearly 50% of the dialogs fully predicted."}, {"heading": "5.2 Benefit of recurrency", "text": "Next, we investigated whether recurrence in the LSTM was beneficial or whether a non-state deep neural network (DNN) would also work. We replaced the (stateful) LSTM with a non-state DNN with the same number of hidden units as the LSTM, loss function and gradient accumulator. We performed the same experiment with a standard recursive neural network (RNN). During the investigation, we found that some trains with different actions had identical local features but different histories. As the DNN is unable to store the history, these differences are indistinguishable from DNN.5."}, {"heading": "5.3 Active learning", "text": "Next, we investigated whether the model would be suitable for active learning (Cohn et al., 1994). The goal of active learning is to reduce the number of labels required to achieve a certain level of performance. In active learning, the current model is run on (yet) blank instances, and the blank instances for which the model is most insecure are next labeled. The model is then rebuilt and the cycle repeated. In order for active learning to be effective, the results of the model must be a good indicator of correctness. To evaluate this, we have plotted a Receiver Characteristic Curve (ROC) in Figure 4. In this figure, 20 dialogs were randomly assigned to a training set of 11 dialogues and a test set of 10 dialogues. LSTM was then estimated on the training set and then applied to the test set that the test results are correct, with the creation of the highest point activity and the correctness of that action being presented as incorrect."}, {"heading": "6 Optimizing with reinforcement learning", "text": "In the previous sections, supervised learning (SL) is applied to the LSTM to train the mimic dialogues provided by the system developers. (SL) In the previous sections, the LSTM is adapted to the mimic abilities of the system developers. (SL) We have defined the reward as 1 for successfully completing the task, and 0 otherwise, we will explore different action sequences in different situations and make adjustments to maximize the expected sum of rewards, which is called return. (We define the reward as 1 for successfully completing the task and 0 otherwise.) A discount of 0.95 was used to complete the dialogues. For optimization, we have chosen a political gradient approach (Williams, 1992)."}, {"heading": "7 Conclusion", "text": "Our approach is based on a recurring neural network that maps everything from raw dialog history to distributions of actions. LSTM automatically derives a representation of the dialog state, making it easier to create a representation of the dialog state. The code provided by the developer tracks entities, packages API calls to external actuators, and can enforce business rules in the policy. Experimental results have shown that supervised learning training results in a reasonable policy from a small number of training dialogs, and that this initial policy significantly accelerates optimization with enhanced learning. To our knowledge, this is the first demonstration of end-to-end learning of dialog control for task-oriented areas."}, {"heading": "A Example dialogs", "text": "A subset of the 21 sample dialogs that were created for this work are listed below: To make room, the entity tags that appear on the user and system side of the dialogs have been removed - for example, Call < Name > Jason < / Name > will be called Jason.How can I help you? Call Jason Williams Cell Phone Call Jason Williams, Cell Phone PlaceCallHow can I help you? Call Jason Williams What kind of phone: Cell Phone or Work? Office Call Jason Williams, Work PlaceCallHow can I help you? Call Frank on his cell Sorry, I have no cell phone number for Frank Seide. I have only one work phone. Do you want to call this number? Yes SavePhonetypeavail Frank Seide, Work PlaceCallHow can I help you? Call Frank on his cell Sorry, I have no cell phone number for Frank Seide. I only have one work phone."}], "references": [{"title": "Improving generalization with active learning", "author": ["Cohn et al.1994] David Cohn", "Les Atlas", "Richard Ladner"], "venue": "Machine learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Hybrid reinforcement/supervised learning for dialogue policies from Communicator data", "author": ["Oliver Lemon", "Kalliroi Georgila"], "venue": "In Proc Workshop on Knowledge and Reasoning in Practical Dialogue", "citeRegEx": "Henderson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2005}, {"title": "Word-based Dialog State Tracking with Recurrent Neural Networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proc SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Active learning for example-based dialog systems", "author": ["Graham Neubig", "Koichiro Yoshino", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In Proc Intl Workshop on Spoken Dialog Systems,", "citeRegEx": "Hiraoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hiraoka et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Statistical dialog management applied to WFST-based dialog systems", "author": ["Hori et al.2009] Chiori Hori", "Kiyonori Ohtake", "Teruhisa Misu", "Hideki Kashioka", "Satoshi Nakamura"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "Hori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hori et al\\.", "year": 2009}, {"title": "A stochastic approach to dialog management", "author": ["David Griol", "Emilio Sanchis", "Encarna Segarra"], "venue": "In Proc IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Hurtado et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hurtado et al\\.", "year": 2005}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as pomdps", "author": ["Blaise Thomson", "Steve Young"], "venue": "ACM Transactions on Speech and Language Pro-", "citeRegEx": "Jur\u010d\u00ed\u010dek et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jur\u010d\u00ed\u010dek et al\\.", "year": 2011}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["Kohl", "Stone2004] Nate Kohl", "Peter Stone"], "venue": "In Robotics and Automation,", "citeRegEx": "Kohl et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kohl et al\\.", "year": 2004}, {"title": "Information state and dialogue management in the TRINDI dialogue move engine toolkit", "author": ["Larsson", "Traum2000] Staffan Larsson", "David Traum"], "venue": "Natural Language Engineering,", "citeRegEx": "Larsson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2000}, {"title": "Example-based dialog modeling for practical multidomain dialog system", "author": ["Lee et al.2009] Cheongjae Lee", "Sangkeun Jung", "Seokhwan Kim", "Gary Geunbae Lee"], "venue": "Speech Communication,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "A stochastic model of human-machine interaction for learning dialogue strategies", "author": ["Levin et al.2000] Esther Levin", "Roberto Pieraccini", "Wieland Eckert"], "venue": "IEEE Trans on Speech and Audio Processing,", "citeRegEx": "Levin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "In Proc SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Dialogue management in the mercury flight reservation system", "author": ["Seneff", "Polifroni2000] Stephanie Seneff", "Joseph Polifroni"], "venue": "In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational Systems - Volume", "citeRegEx": "Seneff et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Seneff et al\\.", "year": 2000}, {"title": "Mastering the game of Go with", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "Fast and easy language understanding for dialog systems with microsoft language understanding intelligent service (luis)", "author": ["Eslam Kamal", "Mokhtar Ashour", "Hani Amr", "Jessica Miller", "Geoff Zweig"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "The dialog state tracking challenge series: A review", "author": ["Antoine Raux", "Matthew Henderson"], "venue": "Dialogue and Discourse,", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Applying POMDPs to dialog systems in the troubleshooting domain", "author": ["Jason D. Williams"], "venue": "In NAACL-HLT Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,", "citeRegEx": "Williams.,? \\Q2007\\E", "shortCiteRegEx": "Williams.", "year": 2007}, {"title": "The best of both worlds: Unifying conventional dialog systems and POMDPs", "author": ["Jason D. Williams"], "venue": "In Proc Intl Conf on Spoken Language Processing (ICSLP),", "citeRegEx": "Williams.,? \\Q2008\\E", "shortCiteRegEx": "Williams.", "year": 2008}, {"title": "POMDPbased Statistical Spoken Dialogue Systems: a Review", "author": ["Young et al.2013] Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "For example, in the Dialog State Tracking Challenge (DSTC), the state consisted of a pre-specified frame of name/value pairs that form the user\u2019s goal (Williams et al., 2016).", "startOffset": 151, "endOffset": 174}, {"referenceID": 2, "context": "Many DSTC entries learned from data how to update the state, using methods such as recurrent neural networks (Henderson et al., 2014), but the schema of the state be-", "startOffset": 109, "endOffset": 133}, {"referenceID": 22, "context": "Manually designed frames are also used for tracking the user\u2019s goal and dialog history in methods based on partially observable Markov decision processes (POMDPs) (Young et al., 2013), methods which learn from", "startOffset": 163, "endOffset": 183}, {"referenceID": 6, "context": "example dialogs (Hurtado et al., 2005; Lee et al., 2009), supervised learning/reinforcement learning hybrid methods (Henderson et al.", "startOffset": 16, "endOffset": 56}, {"referenceID": 11, "context": "example dialogs (Hurtado et al., 2005; Lee et al., 2009), supervised learning/reinforcement learning hybrid methods (Henderson et al.", "startOffset": 16, "endOffset": 56}, {"referenceID": 1, "context": ", 2009), supervised learning/reinforcement learning hybrid methods (Henderson et al., 2005), and also in commercial and open source frameworks such as VoiceXML2 and AIML.", "startOffset": 67, "endOffset": 91}, {"referenceID": 13, "context": "the words of the next utterance directly from the history of the dialog, using a recurrent neural network trained on a large corpus of dialogs (Lowe et al., 2015).", "startOffset": 143, "endOffset": 162}, {"referenceID": 6, "context": "For example, when a user input is received, a corpus of example dialogs can be searched for the most similar user input and dialog state, and the following system action can be output to the user (Hurtado et al., 2005; Lee et al., 2009; Hori et al., 2009; Lowe et al., 2015; Hiraoka et al., 2016).", "startOffset": 196, "endOffset": 296}, {"referenceID": 11, "context": "For example, when a user input is received, a corpus of example dialogs can be searched for the most similar user input and dialog state, and the following system action can be output to the user (Hurtado et al., 2005; Lee et al., 2009; Hori et al., 2009; Lowe et al., 2015; Hiraoka et al., 2016).", "startOffset": 196, "endOffset": 296}, {"referenceID": 5, "context": "For example, when a user input is received, a corpus of example dialogs can be searched for the most similar user input and dialog state, and the following system action can be output to the user (Hurtado et al., 2005; Lee et al., 2009; Hori et al., 2009; Lowe et al., 2015; Hiraoka et al., 2016).", "startOffset": 196, "endOffset": 296}, {"referenceID": 13, "context": "For example, when a user input is received, a corpus of example dialogs can be searched for the most similar user input and dialog state, and the following system action can be output to the user (Hurtado et al., 2005; Lee et al., 2009; Hori et al., 2009; Lowe et al., 2015; Hiraoka et al., 2016).", "startOffset": 196, "endOffset": 296}, {"referenceID": 3, "context": "For example, when a user input is received, a corpus of example dialogs can be searched for the most similar user input and dialog state, and the following system action can be output to the user (Hurtado et al., 2005; Lee et al., 2009; Hori et al., 2009; Lowe et al., 2015; Hiraoka et al., 2016).", "startOffset": 196, "endOffset": 296}, {"referenceID": 18, "context": "entity extraction errors are more prevalent, methods from the dialog state tracking literature for tracking user goals could be applied (Williams et al., 2016).", "startOffset": 136, "endOffset": 159}, {"referenceID": 12, "context": "was originally framed as a Markov decision process (Levin et al., 2000), and later as a partially observable Markov decision process (Young et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 22, "context": ", 2000), and later as a partially observable Markov decision process (Young et al., 2013).", "startOffset": 69, "endOffset": 89}, {"referenceID": 21, "context": "Business rules can be incorporated, in a similar manner to our approach (Williams, 2008).", "startOffset": 72, "endOffset": 88}, {"referenceID": 1, "context": "Past work has explored an alternate way of combining supervised learning and reinforcement learning for learning dialog control (Henderson et al., 2005).", "startOffset": 128, "endOffset": 152}, {"referenceID": 17, "context": "Understanding Intelligent Service (Williams et al., 2015).", "startOffset": 34, "endOffset": 57}, {"referenceID": 7, "context": "For the LSTM, we selected 32 hidden units, and initialized forget gates to zero, as suggested in (Jozefowicz et al., 2015).", "startOffset": 97, "endOffset": 122}, {"referenceID": 23, "context": "Training was performed using categorical cross entropy as the loss, and with AdaDelta to smooth updates (Zeiler, 2012).", "startOffset": 104, "endOffset": 118}, {"referenceID": 0, "context": "We next examined whether the model would be suitable for active learning (Cohn et al., 1994).", "startOffset": 73, "endOffset": 92}, {"referenceID": 19, "context": "For optimization, we selected a policy gradient approach (Williams, 1992).", "startOffset": 57, "endOffset": 73}, {"referenceID": 8, "context": "Policy gradient methods have been successfully applied to dialog systems (Jur\u010d\u00ed\u010dek et al., 2011), robotics (Kohl and Stone, 2004), and the board game Go (Silver et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 15, "context": ", 2011), robotics (Kohl and Stone, 2004), and the board game Go (Silver et al., 2016).", "startOffset": 64, "endOffset": 85}, {"referenceID": 8, "context": "6 Past work has applied the so-called natural gradient estimate (Peters and Schaal, 2008) to dialog systems (Jur\u010d\u00ed\u010dek et al., 2011).", "startOffset": 108, "endOffset": 131}, {"referenceID": 19, "context": ", the variance) (Williams, 1992).", "startOffset": 16, "endOffset": 32}, {"referenceID": 23, "context": "vergence substantially (Zeiler, 2012).", "startOffset": 23, "endOffset": 37}, {"referenceID": 20, "context": "The difficulty of discovering long action sequences with delayed rewards has been observed in other applications of RL to dialog systems (Williams, 2007).", "startOffset": 137, "endOffset": 153}], "year": 2016, "abstractText": "This paper presents a model for end-toend learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.", "creator": "LaTeX with hyperref package"}}}