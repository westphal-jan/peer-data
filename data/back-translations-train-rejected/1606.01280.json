{"id": "1606.01280", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Dependency Parsing as Head Selection", "abstract": "Conventional dependency parsers rely on a statistical model and a transition system or graph algorithm to enforce tree-structured outputs during training and inference. In this work we formalize dependency parsing as the problem of selecting the head (a.k.a. parent) of each word in a sentence. Our model which we call DeNSe (as shorthand for Dependency Neural Selection) employs bidirectional recurrent neural networks for the head selection task. Without enforcing any structural constraints during training, DeNSe generates (at inference time) trees for the overwhelming majority of sentences (95% on an English dataset), while remaining non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DeNSe on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with or outperform the state of the art.", "histories": [["v1", "Fri, 3 Jun 2016 21:27:03 GMT  (69kb,D)", "http://arxiv.org/abs/1606.01280v1", null], ["v2", "Mon, 20 Jun 2016 20:25:02 GMT  (70kb,D)", "http://arxiv.org/abs/1606.01280v2", null], ["v3", "Fri, 2 Dec 2016 22:22:10 GMT  (60kb,D)", "http://arxiv.org/abs/1606.01280v3", "to appear in EACL 2017"], ["v4", "Thu, 22 Dec 2016 15:28:34 GMT  (61kb,D)", "http://arxiv.org/abs/1606.01280v4", "to appear in EACL 2017; Our code is available atthis http URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "jianpeng cheng", "mirella lapata"], "accepted": false, "id": "1606.01280"}, "pdf": {"name": "1606.01280.pdf", "metadata": {"source": "CRF", "title": "Dependency Parsing as Head Selection", "authors": ["Xingxing Zhang", "Jianpeng Cheng"], "emails": ["x.zhang@ed.ac.uk,", "jianpeng.cheng@ed.ac.uk,", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own by blaming themselves and others. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "2 Related Work", "text": "In this section, we briefly review the previous work on dependency analysis, which focused on graph-based and transition-based models (2001). We also discuss how our model relates to previously proposed neural network-based parsers.Graph-based parsing-graph-based dependency parser refers to a model for scoring possible dependency diagrams for a given set. Graphs are typically incorporated into their components and the value of a tree is defined as the sum of all arcs. This factorization allows a traceable search for the highest scoring structure of dependency diagrams for a given set. Searching for the maximum stress tree (MST) in a graph that produces efficient algorithms for both non-projective and projective dependence trees. For example, the Chu-Liu-Edmonds algorithms (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., the MST extractor, 200b, is commonly used in the case of ice-mounted trees) and the MST is not."}, {"heading": "3 Dependency Parsing as Head Selection", "text": "In this section, we present our parsing model DENSE, which attempts to predict the head of each word within a sentence. Specifically, the model uses a sentence of length N and prints out N < head, dependent > arcs. We describe the model, which focuses on blank dependencies, and then discuss how to easily extend it to the labeled setting. We start by explaining how words are represented in our model, and then give details of how DENSE makes predictions based on these learned representations. As there is no guarantee that the results of DENSE are trees (although they usually are), we also discuss how to expand DENSE to enforce projective and non-projective tree outputs. In this essay, vectors (e.g. v or vi) are denoted by lowercase bold letters, while uppercase matrices (e.g. M or Mb) and lowercase scales (e.g. w or wi) are denoted."}, {"heading": "3.1 Word Representation", "text": "In this connection, it must also be noted that the two are very complex matters, which relate to a case involving a case where the presumption of the presumption is a case where the presumption of the presumption is a case where the presumption of the presumption of the presumption is a case where the presumption of the presumption of the presumption is a case where the presumption of the presumption of the presumption of the presumption of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the presupposition of the"}, {"heading": "3.2 Head Selection", "text": "It is not the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in which we behave in the way in which we behave in which we behave in the way in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave as we behave in which we behave in which we behave as we behave as we behave as we behave as we behave as we are worthy and which we behave as we behave as we behave as we behave as we behave as we behave as we, or which we behave as we behave as we behave as we behave as we behave as we"}, {"heading": "3.3 Maximum Spanning Tree Algorithms", "text": "In this case, the output of DENSE can be customized with a maximum spanning tree algorithm. We use the Chu-Liu Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to build non-projective trees and the Eisner algorithm (Eisner, 1996) for projective ones.According to McDonald et al. (2005b), we consider a sentence S = (w0 = ROOT, w1,., wN) as a graph GS = < VS, ES > with the sentence words and the dummy root symbol as vertices and a directed edge between each pair of different words and from the root symbol to each word. The directed graph GS is defined as: VS = {w0 = ROOT, w1,., wN} ES = {< i, j > algorithm root symbol as vertices and a directed edge between each word."}, {"heading": "4 Experiments", "text": "In the following, we describe the data sets we use and provide training details for our models. We also present comparisons with several previous systems and analyze the results of the parser."}, {"heading": "4.1 Datasets", "text": "In the projective environment, we evaluated the performance of our parser on the English Penn Treebank (PTB) and the Chinese Treebank 5.1 (CTB).3 Our experimental setup closely follows the presentation of Chen and Manning (2014) and Dyer et al. (2015).For English, we adopted the presentation of Pennford basic dependencies (SD) (De Marneffe et al., 2006).3 We follow the standard splits of PTB, sections 2-21 were used for training, section 22 for development and section 23 for testing. POS tags were assigned using the Stanford tagger (Toutanova et al., 2003) with an accuracy of 97.3%. For Chinese, we follow the same division of CTB5 introduced in Zhang and Clark (2008). Specifically, we used sections 001-815, 1001-1136 for training, sections 886-931, 1148-1151 for development and sections Stanser."}, {"heading": "4.2 Training Details", "text": "We trained our models on an Nvidia GPU card; the training lasts one to two hours and the parameters of the model were uniformly initialized in [\u2212 0,1,0,1]. We used Adam (Kingma and Ba, 2014) to optimize our models with hyperparameters recommended by the authors (i.e. learning rate 0.001, first pulse coefficient 0.9 and second pulse coefficient 0.999). To alleviate the problem of exploding gradients, we changed the gradient again when its norm was above 5 (Pascanu et al., 2013). Dropout (Srivastava et al., 2014) was applied to our model with the strategy recommended in Zaremba et al. (2014). We used two-layer LSTMs on all datasets and set d = s = 300, with d being the hidden unit size and s being the word embed size."}, {"heading": "4.3 Results", "text": "In fact, it is so that we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in"}, {"heading": "5 Conclusions", "text": "In this paper, we introduced DENSE, a neural dependency parser that we train without a transition system or graph-based algorithm. Experimental results show that DENSE achieves competitive performance in four different languages and can seamlessly transition from a projective to a non-projective parser by simply changing the MST algorithm after processing during inference. In the future, we want to increase the reach of our parser through tri-training (Li et al., 2014) and multitasking learning (Luong et al., 2015)."}], "references": [{"title": "Experiments with a multilanguage non-projective dependency parser", "author": ["Giuseppe Attardi"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Attardi.,? \\Q2006\\E", "shortCiteRegEx": "Attardi.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of COLING,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Non-projective parsing for statistical machine translation", "author": ["Carreras", "Collins2009] Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Carreras et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2009}, {"title": "Experiments with a higher-order projective dependency parser", "author": ["Xavier Carreras"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Carreras.,? \\Q2007\\E", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "On shortest arborescence of a directed graph", "author": ["Chu", "Liu1965] Yoeng-Jin Chu", "Tseng-Hong Liu"], "venue": "Scientia Sinica,", "citeRegEx": "Chu et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Chu et al\\.", "year": 1965}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the the EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Crammer", "Singer2001] Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2001}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Crammer", "Singer2003] Koby Crammer", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Optimum branchings", "author": ["Jack Edmonds"], "venue": "Journal of Research of the National Bureau of Standards B,", "citeRegEx": "Edmonds.,? \\Q1967\\E", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M Eisner"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Bilexical grammars and their cubic-time parsing algorithms", "author": ["Jason Eisner"], "venue": "In Advances in probabilistic and other parsing technologies,", "citeRegEx": "Eisner.,? \\Q2000\\E", "shortCiteRegEx": "Eisner.", "year": 2000}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Schapire1999] Yoav Freund", "Robert E Schapire"], "venue": "Machine learning,", "citeRegEx": "Freund et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1999}, {"title": "Relation extraction using dependency parse trees", "author": ["Fundel et al.2007] Katrin Fundel", "Robert K\u00fcffner", "Ralf Zimmer"], "venue": null, "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Goldberg", "Nivre2012] Yoav Goldberg", "Joakim Nivre"], "venue": "In COLING,", "citeRegEx": "Goldberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2012}, {"title": "A primer on neural network models for natural language processing. CoRR, abs/1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "Studies in Computational Intelligence. Springer", "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations. CoRR, abs/1603.04351", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Koo et al.2010] Terry Koo", "Alexander M Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Dependency Parsing", "author": ["Kubler et al.2009] Sandra Kubler", "Ryan McDonald", "Joakim Nivre", "Graeme Hirst"], "venue": null, "citeRegEx": "Kubler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kubler et al\\.", "year": 2009}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Lei et al.2014] Tao Lei", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the ACL", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Ambiguity-aware ensemble training for semi-supervised dependency parsing", "author": ["Li et al.2014] Zhenghua Li", "Min Zhang", "Wenliang Chen"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel Almeida", "Noah A. Smith"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "SardSrn: A neural network shift-reduce parser", "author": ["Mayberry", "Risto Miikkulainen"], "venue": "Proceedings of the IJCAI,", "citeRegEx": "Mayberry et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mayberry et al\\.", "year": 1999}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of the ACL,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Maltparser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006a] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In Proceedings of LREC,", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Labeled pseudo-projective dependency parsing with support vector machines", "author": ["Nivre et al.2006b] Joakim Nivre", "Johan Hall", "Jens Nilsson", "G\u00fcl\u015fen Eryi?it", "Svetoslav Marinov"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the IWPT. Citeseer", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "An effective neural network model for graphbased dependency parsing", "author": ["Pei et al.2015] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In Proceedings of COLING and ACL,", "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the NAACLHLT,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "A conditional random field word segmenter for sighan bakeoff", "author": ["Tseng et al.2005] Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning"], "venue": "In Proceedings of the fourth SIGHAN workshop on Chinese language Processing,", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Generalized higher-order dependency parsing with cube pruning", "author": ["Zhang", "McDonald2012] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of the EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Enforcing structural diversity in cubepruned dependency parsing", "author": ["Zhang", "McDonald2014] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 16, "context": "Dependency parsing plays an important role in many natural language applications, such as relation extraction (Fundel et al., 2007), machine translation (Carreras and Collins, 2009), and ontology construction (Snow et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 43, "context": ", 2007), machine translation (Carreras and Collins, 2009), and ontology construction (Snow et al., 2004).", "startOffset": 85, "endOffset": 104}, {"referenceID": 7, "context": "An arc is scored with a set of local features and a linear model, the parameters of which can be effectively learned with online algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003; Freund and Schapire, 1999; Collins, 2002).", "startOffset": 140, "endOffset": 234}, {"referenceID": 13, "context": "In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b).", "startOffset": 133, "endOffset": 185}, {"referenceID": 14, "context": "In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b).", "startOffset": 133, "endOffset": 185}, {"referenceID": 41, "context": "More recently, a few approaches (Chen and Manning, 2014; Pei et al., 2015; Kiperwasser and Goldberg, 2016) apply neural networks for learning dense feature representations.", "startOffset": 32, "endOffset": 106}, {"referenceID": 11, "context": "(Dyer et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "For instance, the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b) is often used to extract the MST in the case of non-projective trees, and the Eisner algorithm (Eisner, 1996; Eisner, 2000) in the case of projective trees.", "startOffset": 44, "endOffset": 102}, {"referenceID": 13, "context": ", 2005b) is often used to extract the MST in the case of non-projective trees, and the Eisner algorithm (Eisner, 1996; Eisner, 2000) in the case of projective trees.", "startOffset": 104, "endOffset": 132}, {"referenceID": 14, "context": ", 2005b) is often used to extract the MST in the case of non-projective trees, and the Eisner algorithm (Eisner, 1996; Eisner, 2000) in the case of projective trees.", "startOffset": 104, "endOffset": 132}, {"referenceID": 7, "context": "During training, weight parameters of the scoring function can be learned with margin-based algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003) or the structured perceptron (Freund and Schapire, 1999; Collins, 2002).", "startOffset": 185, "endOffset": 227}, {"referenceID": 4, "context": "Beyond basic first-order models, the literature offers a few examples of higherorder models involving sibling and grand parent relations (Carreras, 2007; Koo et al., 2010; Zhang and McDonald, 2012).", "startOffset": 137, "endOffset": 197}, {"referenceID": 26, "context": "Beyond basic first-order models, the literature offers a few examples of higherorder models involving sibling and grand parent relations (Carreras, 2007; Koo et al., 2010; Zhang and McDonald, 2012).", "startOffset": 137, "endOffset": 197}, {"referenceID": 37, "context": "A transition system typically includes a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b).", "startOffset": 217, "endOffset": 251}, {"referenceID": 38, "context": "In an arc-standard system (Yamada and Matsumoto, 2003; Nivre, 2004), the transitions include a SHIFT operation which removes the first word in the buffer and pushes it onto the stack; a LEFT-ARC operation adds an arc from the word in the beginning of the buffer to the word on top of the stack; and a RIGHT-ARC operation adds an arc from the word on top of the stack to the word in the beginning of the buffer.", "startOffset": 26, "endOffset": 67}, {"referenceID": 39, "context": "Extensions include the arc-eager system (Nivre, 2008) which always adds an arc at the earliest possible stage, a more elaborate (reduce) action space to handle non-projective parsing (Attardi, 2006), and the use of non-local training methods to", "startOffset": 40, "endOffset": 53}, {"referenceID": 0, "context": "Extensions include the arc-eager system (Nivre, 2008) which always adds an arc at the earliest possible stage, a more elaborate (reduce) action space to handle non-projective parsing (Attardi, 2006), and the use of non-local training methods to", "startOffset": 183, "endOffset": 198}, {"referenceID": 21, "context": "Neural Network-based Parsing Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen., 1999; Henderson, 2004; Titov and Henderson, 2007).", "startOffset": 101, "endOffset": 179}, {"referenceID": 19, "context": ", 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transitionor graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Pei et al.", "startOffset": 8, "endOffset": 237}, {"referenceID": 19, "context": ", 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transitionor graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Pei et al. (2015) do the same for a graph-based parser.", "startOffset": 8, "endOffset": 346}, {"referenceID": 19, "context": ", 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transitionor graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Pei et al. (2015) do the same for a graph-based parser. Lei et al. (2014) apply tensor decomposition to obtain word embeddings in their syntactic roles, which they subsequently use in a graph-based parser.", "startOffset": 8, "endOffset": 402}, {"referenceID": 11, "context": "Dyer et al. (2015) redesign components of a transition-based system where the the buffer, stack, and action sequences are modeled separately with stack long short-term memory networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Dyer et al. (2015) redesign components of a transition-based system where the the buffer, stack, and action sequences are modeled separately with stack long short-term memory networks. The hidden states of these LSTMs are concatenated and used as input features to a final transition classifier. Kiperwasser and Goldberg (2016) use a bi-directional LSTM to extract features for both a transition- and graph-based parser.", "startOffset": 0, "endOffset": 328}, {"referenceID": 27, "context": ",wN) denote a sentence of length N; following common practice in the dependency parsing literature (Kubler et al., 2009), we add an artificial ROOT token represented by w0.", "startOffset": 99, "endOffset": 120}, {"referenceID": 19, "context": ", Graves (2012) or Goldberg (2015).", "startOffset": 2, "endOffset": 16}, {"referenceID": 19, "context": ", Graves (2012) or Goldberg (2015). 3.", "startOffset": 19, "endOffset": 35}, {"referenceID": 1, "context": "Equations (5) and (6) compute the probability of adding an arc between two words, in a fashion similar to the neural attention mechanism in sequenceto-sequence models (Bahdanau et al., 2014).", "startOffset": 167, "endOffset": 190}, {"referenceID": 17, "context": "We employ a two-layer rectifier network (Glorot et al., 2011) for the classification task.", "startOffset": 40, "endOffset": 61}, {"referenceID": 12, "context": "We use the Chu-LiuEdmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) for building non-projective trees and the Eisner algorithm (Eisner, 1996) for projective ones.", "startOffset": 36, "endOffset": 70}, {"referenceID": 13, "context": "We use the Chu-LiuEdmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) for building non-projective trees and the Eisner algorithm (Eisner, 1996) for projective ones.", "startOffset": 130, "endOffset": 144}, {"referenceID": 33, "context": "Following McDonald et al. (2005b), we view a sentence S = (w0 = ROOT,w1, .", "startOffset": 10, "endOffset": 34}, {"referenceID": 12, "context": "Non-projective Parsing To build a non-projective parser, we solve the MST problem with the Chu-LiuEdmonds algorithm (Chu and Liu, 1965; Edmonds, 1967).", "startOffset": 116, "endOffset": 150}, {"referenceID": 13, "context": "Projective Parsing For projective parsing, we solve the MST problem with the Eisner algorithm (Eisner, 1996).", "startOffset": 94, "endOffset": 108}, {"referenceID": 11, "context": "Our experimental setup closely follows Chen and Manning (2014) and Dyer et al. (2015).", "startOffset": 67, "endOffset": 86}, {"referenceID": 46, "context": "POS tags were assigned using the Stanford tagger (Toutanova et al., 2003) with an accuracy of 97.", "startOffset": 49, "endOffset": 73}, {"referenceID": 10, "context": "For English, we adopted the Stanford basic dependencies (SD) representation (De Marneffe et al., 2006).3 We follow the standard splits of PTB, sections 2\u201321 were used for training, section 22 for development and section 23 for testing. POS tags were assigned using the Stanford tagger (Toutanova et al., 2003) with an accuracy of 97.3%. For Chinese, we follow the same split of CTB5 introduced in Zhang and Clark (2008). In particular, we used sections 001\u2013815, 1001\u20131136 for training, sections 886\u2013931, 1148\u20131151 for development and sections 816\u2013885, 1137\u20131147 for testing.", "startOffset": 80, "endOffset": 420}, {"referenceID": 10, "context": "For English, we adopted the Stanford basic dependencies (SD) representation (De Marneffe et al., 2006).3 We follow the standard splits of PTB, sections 2\u201321 were used for training, section 22 for development and section 23 for testing. POS tags were assigned using the Stanford tagger (Toutanova et al., 2003) with an accuracy of 97.3%. For Chinese, we follow the same split of CTB5 introduced in Zhang and Clark (2008). In particular, we used sections 001\u2013815, 1001\u20131136 for training, sections 886\u2013931, 1148\u20131151 for development and sections 816\u2013885, 1137\u20131147 for testing. The original constituency trees in CTB were converted to dependency trees with the Penn2Malt tool.4 We used gold segmentation and gold POS tags as in Chen and Manning (2014) and Dyer et al.", "startOffset": 80, "endOffset": 749}, {"referenceID": 10, "context": "For English, we adopted the Stanford basic dependencies (SD) representation (De Marneffe et al., 2006).3 We follow the standard splits of PTB, sections 2\u201321 were used for training, section 22 for development and section 23 for testing. POS tags were assigned using the Stanford tagger (Toutanova et al., 2003) with an accuracy of 97.3%. For Chinese, we follow the same split of CTB5 introduced in Zhang and Clark (2008). In particular, we used sections 001\u2013815, 1001\u20131136 for training, sections 886\u2013931, 1148\u20131151 for development and sections 816\u2013885, 1137\u20131147 for testing. The original constituency trees in CTB were converted to dependency trees with the Penn2Malt tool.4 We used gold segmentation and gold POS tags as in Chen and Manning (2014) and Dyer et al. (2015).", "startOffset": 80, "endOffset": 772}, {"referenceID": 40, "context": "To alleviate the gradient exploding problem, we rescaled the gradient when its norm exceeded 5 (Pascanu et al., 2013).", "startOffset": 95, "endOffset": 117}, {"referenceID": 44, "context": "Dropout (Srivastava et al., 2014) was applied to our model with the strategy recommended in Zaremba et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 40, "context": "To alleviate the gradient exploding problem, we rescaled the gradient when its norm exceeded 5 (Pascanu et al., 2013). Dropout (Srivastava et al., 2014) was applied to our model with the strategy recommended in Zaremba et al. (2014). On all datasets, we used two-layer LSTMs and set d = s = 300, where d is the hidden unit size and s is the word embedding size.", "startOffset": 96, "endOffset": 233}, {"referenceID": 11, "context": "As in previous neural dependency parsing work (Chen and Manning, 2014; Dyer et al., 2015), we used pre-trained word vectors to initialize our word embedding matrix We.", "startOffset": 46, "endOffset": 89}, {"referenceID": 42, "context": "For the PTB experiments, we used 300 dimensional pre-trained GloVe6 vectors (Pennington et al., 2014).", "startOffset": 76, "endOffset": 101}, {"referenceID": 47, "context": "For the CTB experiments, we trained 300 dimensional GloVe vectors on the Chinese Gigaword corpus which we segmented with the Stanford Chinese Segmenter (Tseng et al., 2005).", "startOffset": 152, "endOffset": 172}, {"referenceID": 2, "context": "Bohnet10 (Bohnet, 2010), Martins13 (Martins et al.", "startOffset": 9, "endOffset": 23}, {"referenceID": 31, "context": "Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2013), and Z&M14 (Zhang and McDonald, 2014) are graph based-parsers with different models or pruning strategies and their results are reported in Weiss et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 2, "context": "Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2013), and Z&M14 (Zhang and McDonald, 2014) are graph based-parsers with different models or pruning strategies and their results are reported in Weiss et al. (2015). Z&N11 (Zhang and", "startOffset": 0, "endOffset": 218}, {"referenceID": 11, "context": "Dyer15 (Dyer et al., 2015) uses (stack) LSTMs to model the states of the buffer, the stack, and the action sequence of a transition system.", "startOffset": 7, "endOffset": 26}, {"referenceID": 48, "context": "Weiss15 (Weiss et al., 2015) is another transition-based parser, however with a more elaborate training procedure.", "startOffset": 8, "endOffset": 28}, {"referenceID": 13, "context": "When we post-process the remaining 13% of non-projective outputs with the Eisner algorithm (DENSE+E), we obtain a slight improvement on UAS (94.10%). Kiperwasser and Goldberg (2016) use bidirectional LSTMs as features and feed them to a graph-based parser (K&G16 graph) and a transition-based parser (K&G16 trans), but obtain worse performance than our model.", "startOffset": 74, "endOffset": 182}, {"referenceID": 31, "context": ", 2005b), the Turbo parser (Martins et al., 2013), and the RBG parser (Lei et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 28, "context": ", 2013), and the RBG parser (Lei et al., 2014).", "startOffset": 28, "endOffset": 46}, {"referenceID": 27, "context": ", 2013), and the RBG parser (Lei et al., 2014). We show the performance of these parsers in the first order setting (e.g., MST-1st) and in higher order settings (e.g., Turbo-3rd). The results of MST-1st, MST-2nd, RBG-1st and RBG3rd are reported in Lei et al. (2014) and the results of Turbo-1st and Turbo-3rd are reported in Martins et al.", "startOffset": 29, "endOffset": 266}, {"referenceID": 27, "context": ", 2013), and the RBG parser (Lei et al., 2014). We show the performance of these parsers in the first order setting (e.g., MST-1st) and in higher order settings (e.g., Turbo-3rd). The results of MST-1st, MST-2nd, RBG-1st and RBG3rd are reported in Lei et al. (2014) and the results of Turbo-1st and Turbo-3rd are reported in Martins et al. (2013). We show results for our parser with greedy inference (see DENSE in the table) and when we use the Chu-Liu-Edmonds algorithm to postprocess non-tree outputs (DENSE+CLE).", "startOffset": 29, "endOffset": 347}, {"referenceID": 29, "context": "In the future, we would like to increase the coverage of our parser by using tri-training techniques (Li et al., 2014) and multitasking learning (Luong et al.", "startOffset": 101, "endOffset": 118}, {"referenceID": 30, "context": ", 2014) and multitasking learning (Luong et al., 2015).", "startOffset": 34, "endOffset": 54}], "year": 2017, "abstractText": "Conventional dependency parsers rely on a statistical model and a transition system or graph algorithm to enforce tree-structured outputs during training and inference. In this work we formalize dependency parsing as the problem of selecting the head (a.k.a. parent) of each word in a sentence. Our model which we call DENSE (as shorthand for Dependency Neural Selection) employs bidirectional recurrent neural networks for the head selection task. Without enforcing any structural constraints during training, DENSE generates (at inference time) trees for the overwhelming majority of sentences (95% on an English dataset), while remaining non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DENSE on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with or outperform the state of the art.", "creator": "LaTeX with hyperref package"}}}