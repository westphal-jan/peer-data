{"id": "1606.01283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory", "abstract": "In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.", "histories": [["v1", "Fri, 3 Jun 2016 21:39:42 GMT  (21kb)", "http://arxiv.org/abs/1606.01283v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "marco idiart", "aline villavicencio"], "accepted": false, "id": "1606.01283"}, "pdf": {"name": "1606.01283.pdf", "metadata": {"source": "CRF", "title": "Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory", "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br,", "avillavicencio@inf.ufrgs.br,", "idiart@if.ufrgs.br"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.01 283v 1 [cs.C L] 3J un"}, {"heading": "1 Introduction", "text": "Although Baroni et al. (2014) pointed out that predictive models that use neural networks to generate distributed word representations (also known as embeddings in this context) surpass counting models that function on coexistence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016). In this paper, we focus on improving a state-of-the-art counting model, LexVec et al. (2016), that performs factoring of positive mutual information (PPMI) using window sampling and negative sampling (WSNS). Salle et al. (2016) suggest that the model LexVec et al."}, {"heading": "2 LexVec", "text": "LexVec uses WSNS to factorize the PPMI matrix into two lower rank q matrices (2016).The coexistence matrix M is then calculated from word-context pairs (w, c) obtained by pushing a symmetrical size window.The PPMI matrix is then calculated as: PMIwc = max (0, log Mwc M), are obtained by minimizing the M profile c) (1), where there is an index total.The word and context embedding W and W, with the dimensions W | d (where V is the vocabulary and d is the embedding dimension #. # Minimization via stochastic gradient derivation (SGD) is a combination of the loss functions Lwc = 12 (WwW \u04323)."}, {"heading": "3 Enhancing LexVec", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Positional Contexts", "text": "As suggested by Levy et al. (2015) and Salle et al. (2016), position contexts (introduced in Levy et al. (2014)) are a potential solution for poor performance in syntactic analogy tasks. Ratherthan takes into account not only which contextual words occur around a target word, but also their position in relation to the target word. In the sentence \"the big dog barked loudly,\" target word dog has contexts (the \u2212 2, large \u2212 1, barking 1, loud 2) and the coincidence matrix takes on dimensions before it has dimensions."}, {"heading": "3.2 External Memory", "text": "Since window scanning through the training corpus and negative scanning selects random contexts, (w, ci) pairs are generated and the corresponding PPMIwci cell must be accessed so that eqs. (2) and (3) can be minimized. Unfortunately, this results in random access to the PPMI matrix that requires it to be stored in main memory. Pennington et al. (2014) shows that the sparse matrix generated under certain assumptions grows as O (| C | 0.8), limiting the maximum corpus size that LexVec.We propose an approximation to WSNS that works as follows: All word context pairs (w, ci) generated by window scanning of the corpus and by negative scanning of each target are first written into a file F. The file F is then sorted with identical lines, and the lines in the format (w, w, ci, where the pair is not dead)."}, {"heading": "4 Materials", "text": "We report on the results of Salle et al. (2016) and use the same training corpus and the same parameters to train LexVec with position contexts and external memory. The corpus is a Wikipedia dump from June 2015, tokenized, lowereled, and split in sets, removal punctuation and converting numbers to words, for a final vocabulary of 302,203 words.All generated embeddings have dimensionality equal to 300. As recommended in Levy et al. (2015) and used in Salle et al. (2016), the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiing frequencies to the power 0.75. PPMI-SVD is the singular value decomposition of the PPMI matrix."}, {"heading": "5 Results", "text": "Position contexts improved performance in terms of both similarity (Table 1) and analogy tasks (Table 2). As assumed, their use significantly improved LexVec's performance in terms of syntactic analogies, resulting in the highest score in GSyn and outperforming GloVe and SGNS. This confirms the relevance of the use of position contexts to capture syntactic dependencies. Salle et al. (2016) reported that the combination of word and context embedding in word similarity tasks performed slightly better than W + W. 1http: / / www.cs.cmu.edu / mfaruqui / suite.htmlThe use of word embedding alone results in much better syntactic performance, suggesting that the embedding of W achieves a better balance between syntax and semantics than W + W. 1http: / www.cs.cmu.cmu.cedu / mufarqui / mufarqui / msuite.htmlThe use alone of the word embedding results in much better syntactic performance, suggesting that the embedding of W achieves a better balance between syntax and semantics than W + W. 1http: / www.cs.cmu.u / mufaruqui / mufarqui / mlThe use of external memory contexts exactly corresponds to what was expected by the external memory memory memory memory reader (without the use of the external application)."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, two improvements to the LexVec word embedding model were presented: the first leads to state-of-the-art performance with syntactic analogies through the use of positioning contexts; the second solves the need to store the PPMI matrix in main memory by using external memory; and the SI variant of implementing external memory was a good approximation of the LexVec standard's WSNS, which enables future training through web-scale enterprises. In future work, we plan to explore the model's hyperparameter space, which could potentially enhance the model's performance after previously limiting ourselves to the parameters recommended in Levy et al. (2015); and finally, following Tsvetkov et al. (2015), in addition to the intrinsic assessments used in this paper, we will advance the model's evaluation to downstream tasks."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Associ-", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM,", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computa-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics pages 211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "CoNLL-2014 page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL-2013 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A. Miller", "Walter G. Charles."], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of the 20th international conference on World wide", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Communications of the ACM 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Matrix factorization using window sampling and negative sampling for improved word representations", "author": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio."], "venue": "arXiv preprint arXiv:1606.00819 .", "citeRegEx": "Salle et al\\.,? 2016", "shortCiteRegEx": "Salle et al\\.", "year": 2016}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani."], "venue": "ACM computing surveys (CSUR) 34(1):1\u201347.", "citeRegEx": "Sebastiani.,? 2002", "shortCiteRegEx": "Sebastiani.", "year": 2002}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL (1). pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 18, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 17, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 6, "context": "(2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016).", "startOffset": 266, "endOffset": 305}, {"referenceID": 15, "context": "(2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016).", "startOffset": 266, "endOffset": 305}, {"referenceID": 0, "context": "Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 15, "context": "In this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016), which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS).", "startOffset": 78, "endOffset": 98}, {"referenceID": 15, "context": "In this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016), which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS). Salle et al. (2016) suggest that LexVec matches and often outperforms competing models in word similarity and semantic analogy tasks.", "startOffset": 79, "endOffset": 260}, {"referenceID": 9, "context": "with \u03b1 = 3/4 (Mikolov et al., 2013b; Salle et al., 2016), and #(w) the unigram frequency of w.", "startOffset": 13, "endOffset": 56}, {"referenceID": 15, "context": "with \u03b1 = 3/4 (Mikolov et al., 2013b; Salle et al., 2016), and #(w) the unigram frequency of w.", "startOffset": 13, "endOffset": 56}, {"referenceID": 15, "context": "(2) and (3): Mini-batch and Stochastic (Salle et al., 2016).", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al. (2016), positional contexts (introduced in Levy et al.", "startOffset": 16, "endOffset": 59}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al. (2016), positional contexts (introduced in Levy et al. (2014)) are a potential solution to poor performance on syntactic analogy tasks.", "startOffset": 16, "endOffset": 114}, {"referenceID": 12, "context": "Pennington et al. (2014) show that the under certain assumptions, this sparse matrix grows as O(|C|), which bounds the maximum corpus size that can be processed by LexVec.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "We report results from Salle et al. (2016) and use the same training corpus and parameters to train LexVec with positional contexts and external memory.", "startOffset": 23, "endOffset": 43}, {"referenceID": 12, "context": "Both GloVe (Pennington et al., 2014) and Skip-gram with negative sampling (SGNS) (Mikolov et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 9, "context": ", 2014) and Skip-gram with negative sampling (SGNS) (Mikolov et al., 2013b) were trained using a symmetric window of size 10.", "startOffset": 52, "endOffset": 75}, {"referenceID": 9, "context": "LexVec, PPMI-SVD, and SGNS use dirty subsampling (Mikolov et al., 2013b; Levy et al., 2015) with threshold t = 10.", "startOffset": 49, "endOffset": 91}, {"referenceID": 5, "context": "LexVec, PPMI-SVD, and SGNS use dirty subsampling (Mikolov et al., 2013b; Levy et al., 2015) with threshold t = 10.", "startOffset": 49, "endOffset": 91}, {"referenceID": 5, "context": "As recommended in Levy et al. (2015) and used in Salle et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 5, "context": "As recommended in Levy et al. (2015) and used in Salle et al. (2016), the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiating context frequencies to the power 0.", "startOffset": 18, "endOffset": 69}, {"referenceID": 2, "context": "(2016), namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 1, "context": ", 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": ", 2012), MTurk (Radinsky et al., 2011), RW (Luong et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 7, "context": ", 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 3, "context": ", 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 11, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 14, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 4, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) word similarity tasks1, and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 8, "context": ", 2012) word similarity tasks1, and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 10, "context": ", 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) tasks.", "startOffset": 43, "endOffset": 66}, {"referenceID": 6, "context": "Word analogy tasks are solved using both 3CosAdd and 3CosMul (Levy et al., 2014).", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": "Therefore, we perform the exact same evaluation as Salle et al. (2016), namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "In future work, we plan to explore the model\u2019s hyperparameter space, which could potentially boost model performance, having so far restricted ourselves to parameters recommended in Levy et al. (2015). Finally, following Tsvetkov et al.", "startOffset": 182, "endOffset": 201}, {"referenceID": 5, "context": "In future work, we plan to explore the model\u2019s hyperparameter space, which could potentially boost model performance, having so far restricted ourselves to parameters recommended in Levy et al. (2015). Finally, following Tsvetkov et al. (2015), we will pursue evaluation of the model on downstream tasks in addition to the intrinsic evaluations used in this paper.", "startOffset": 182, "endOffset": 244}], "year": 2016, "abstractText": "In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}