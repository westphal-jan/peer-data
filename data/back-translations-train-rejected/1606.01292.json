{"id": "1606.01292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "An Attentional Neural Conversation Model with Improved Specificity", "abstract": "In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention. Secondly, it avoids generating non-specific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval.", "histories": [["v1", "Fri, 3 Jun 2016 22:26:01 GMT  (66kb,D)", "http://arxiv.org/abs/1606.01292v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["kaisheng yao", "baolin peng", "geoffrey zweig", "kam-fai wong"], "accepted": false, "id": "1606.01292"}, "pdf": {"name": "1606.01292.pdf", "metadata": {"source": "CRF", "title": "An Attentional Neural Conversation Model with Improved Specificity", "authors": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Kam-Fai Wong"], "emails": ["kaisheny@microsoft.com", "blpeng@se.cuhk.edu.hk", "gzweig@microsoft.com", "kfwong@se.cuhk.edu.hk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, neural conversation models (Serban et al., 2015b; Li et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015) have emerged as a promising complement to traditional partially observable Markov decision-making processes (POMDP) models (Young et al., 2013). Neural network-based techniques require little explicit linguistic knowledge, such as the creation of a semantic parser, and therefore have the promise of scalability, flexibility, and language independence. Broadly speaking, there are two approaches to building a neural conversation model, the first being to train what is essentially a conversation-based language model used in generative mode to produce a likely response to a given conversational context."}, {"heading": "2 The model", "text": "The proposed model is part of the encoder decoder model (Sutskever et al., 2014), but contains a hierarchical structure for modelling dependence between the phrases of the conversation process. The encoder network processes the user's input and presents it as a vector. This vector is the input to a recurring network that models the context or intention of generating a response in the decoder network. The decoder generates a response sequence word by word. For each word, the decoder uses an attention model to the words in the user's input. Subsequently (Grosz and Sidner, 1986), we designate the context of the conversation as intention. Since the decoder uses an attention model, we refer to this model as a neural conversation model with intention (AWI). A detailed image for a specific turn is shown in Figure 1. We work out each component of this model in the following sections."}, {"heading": "2.1 Encoder network", "text": "Faced with a user input sequence with the length M at curve k, the encoder network converts it into a sequence of vectors x (k) = [x (k) m: m = 1, \u00b7 \u00b7 \u00b7 \u00b7, M], with the vector x (k) m-Rde denoting a word embedding representation of the word at position m. the model uses a feed network to process this sequence. It has two inputs. The first is a simple word unigram function, which is extracted as an average of the word embedding representation in x (k). The second input is a representation of the previous answer. This representation is also a word unigram function, but is applied to the previous answer. The output, u (k) and Rdx, from the top layer of the feed network is a vector representation of the user input. In addition to this vector representation, the encoder network returns the vector sequence k (k)."}, {"heading": "2.2 Intention network", "text": "The middle layer represents a conversation context that we call intention, represented in the Grosz and Sidner hypothesis, 1986. At curve k, it is a vector z (k), and at the current curve, the activity in z (k) is dependent on the activity in the previous move z (k-1) and user input. Therefore, it is natural to represent z (k) as: z (k) = tanh (Wiz (k-1) + Uiu (k)), (1) where tanh () is a tanh operation. Wi-Rdz \u00b7 dz and Ui-Rdz \u00b7 dx are matrices to transform inputs into a space with the dimension dz. Normally, we apply several layers of the above nonlinear processing, and the higher layer applies Wi only to the output of the underlying layer. Note that Wis resolved across layers."}, {"heading": "2.3 Decoder network", "text": "Indeed, it is indeed so that it is a way in which it is about a way in which the people in the real real world in the real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real"}, {"heading": "3 Training and decoding algorithms", "text": "This section introduces training and decoding algorithms for generating and retrieving answers. Section 3.1 is the standard cross-entropy training, used for training both generation and retrieval models. Section 3.2 introduces training and decoding algorithms to improve specificity for generation. Algorithms for training and decoding for retrieval are described in Section 3.3."}, {"heading": "3.1 Maximum-likelihood training", "text": "The standard training method maximizes the probability of predicting the correct word y (k) n given user input x (k), context z (k) and past prediction y (k) n-1; i.e., the goal is to maximize the following log liquidity probability over the model parameter \u03b8, L (y (k)) = log N \u00b2 n = 1p (y (k) n | y (k) n-1, z (k), x (k); \u03b8))). (8) One problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016). Another problem is that decoding to maximize the saturation probability typically results in non-specific high frequency words (Li et al., 2016)."}, {"heading": "3.2 Improving specificity for generation", "text": "We propose to use the Inverse Document Frequency (IDF) (Salton and Buckley, 1988) to measure the specificity of a response; the IDF is used for decoding in paragraph 3.2.2. We describe a novel algorithm in paragraph 3.2.3 that involves the IDF in the training goal."}, {"heading": "3.2.1 Specificity", "text": "The denominator represents the number of sentences in which the word w occurs. A characteristic of the IDF is that words very often have a low IDF value. We define an IDF at the sentence level more broadly than the average of the IDF values of words in a sentence. I.e., idf (c) = 1 | c | c | c idf (w), (10) where the denominator | c | is the number of words in the sentence. A corpus-level IDF is similarly calculated on the basis of a corpus with an average operation like idf (C) = 1 | C | c \u00b2 c \u00b2 C, w = c idf (w) and the denominator in the equation is the number of words in the corpus."}, {"heading": "3.2.2 Reranking with IDF", "text": "One way to improve specificity is to use IDF to recalculate hypotheses from beam detection decoding. Length-normalized log probability values of these hypotheses are interpolated with IDF values at the set level. Coordination of the interpolation weight is based on a development theorem with minimal error rate training (MERT) (Och, 2003). The interpolation weight, which reaches the highest BLEU value (Papineni et al., 2002) on the development theorem, is used for testing purposes."}, {"heading": "3.2.3 Incorporating IDF in training objective", "text": "Alternatively, we raise our problem of the optimization of a model directly on specificity in the context of reinforcement learning (Sutton and Barto, 1988).The decoder is an agent with its policy of equation (2).Its action consists in generating a word using the policy, and therefore it has to take V-measures at all times. At the end of generating a whole sequence of words for the answer, the agent receives a reward, which is calculated as the set value of the generated response. Therefore, the training should find a policy that maximizes the expected reward. This problem can be solved by REINFORCE (Williams, 1992), in which the gradient for updating the model parameter is calculated as follows: Inequality (r (w (k) \u2212 b (x (k))) (11).This protocol logs p (y (k) n | y (k) n | y (k) n-1, z (k) n-1, x (k))."}, {"heading": "3.3 Training and decoding for retrieval", "text": "We briefly describe TF-IDF in Section 3.3.1. Section 3.3.2 introduces the algorithm for forming the AWI model for retrieval. Section 3.3.3 combines the model with TFIDF. Note that TF-IDF uses IDF to punish non-specific words; combining the AWI model with TFIDF should have improved specificity, which could lead to improved retrieval performance."}, {"heading": "3.3.1 TF-IDF", "text": "The Term-Frequency Inverse Document Frequency (TFIDF) (Salton and Buckley, 1988) is an established baseline for the call model used for retrieval (Lowe et al., 2015).The Term-Frequency (TF) is a count of the number of times a word appears in a given context, and the IDF sets the penalty for how often the word appears in the corpus.The TF-IDF is a vector for a context that is calculated as follows for a word w in a context c, tfidf (w, c, C) = o (w, c) \u00b7 log N | c: w, c |, where o (w, c) is the number of times the word w occurs in context c."}, {"heading": "3.3.2 Training models with ranking criterion", "text": "To train the AWI model for retrieval, the model requires two types of responses: the positive answer is the correct one, and the negative responses are those randomly collected from the training set. For an answer w (k), its length-normalized log probability is calculated as follows: llk (w (k) = L (w (k)) | w (k) |, (12) where L (w (k)) is calculated by equation. (8) with y (k) replaced by w (k). | w (k) | is the number of words in w (k). The goal is to have a high retrieval rate, so that the correct answers are rated higher than negative answers. To this end, the algorithm maximizes the difference between the length-normalized log probability of the correct answers to the longest-normalized log probability of the negative answers; i.e. R = max {llk (k) \u2212 llk (k) \u2212 w (k) (k) (k) is the probability of the longest-normalized response to the negative response."}, {"heading": "3.3.3 Ranking with AWI together with TF-IDF", "text": "Of course, the length-normalized log likelihood score of the model trained in paragraph 3.3.2 can be interpolated with the similarity score of TF-IDF in paragraph 3.3.1. The optimal interpolation weight is selected on the basis of a development set when it achieves the best retrieval rates."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We use a commercial data set from the real world to evaluate the models. The data includes human-human dialogs from a helpdesk chat service for Office and Windows services. In this service, customers seek help on computer and software-related topics from human agents. The training set consists of 141,204 dialogs with 2 million spins. The average number of spins in a dialog is 12, with the largest number of 140 spins and the minimum number of 1 spins. More than 90% of the dialogs have 25 or less spins. The number of tokens is 25,410,683 on the source side and 37,796,000 on the landing side. The vocabulary includes 8098 words from both the source side and the target side. Development and test sets each have 10,000 spins. The test set has 125,451 tokens on the source side and 187,118 on the landing side."}, {"heading": "4.2 Training Details", "text": "Unless otherwise stated, all recurring networks have two levels. The encoder network uses word embedding initialized by 300-dimensional GLOVE vector (Pennington et al., 2014) formed by 840 billion words. Therefore, the embedding dimension is de 300. The hidden layer dimension for the encoder, dx, is 1000. The decoder dimension, dr, is 1000. The intention network has a 300-dimensional vector; i.e., dz = 300. The alignment dimension there is 100. All parameters, with the exception of distortions, are initialized by means of a uniform distribution in [\u2212 1.0, 1.0] but they are scaled inversely proportional to the number of parameters. All bias vectors are initialized to zero. The maximum number of training periods is 10. We use RMSProp with Momentum (Tieleman and Hinton, 2012) to update the models."}, {"heading": "4.3 Evaluation metrics", "text": "The first measurement is BLEU (Papineni et al., 2002), which uses the Ngram (Dunning, 1994) to calculate similarities between references and answers, along with a penalty for the brevity of the sentence. The second measurement is BLEU at 4-grams. While BLEU can unfairly punish paraphrases with different formulations, we have found that it correlates well with human judgment of answer-creation tasks (Galley et al., 2015). The second measurement is perplexity (Brown et al., 1992), which measures the probability of generating a word-given observations. We use it in Section 4.4.1 to compare the proposed model with other neural network models that also report perplexity. However, since our training algorithm in Sec. 3.2 is designed to improve specificity that does not directly correlate with the standard probability."}, {"heading": "4.4 Performance as a generation model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Comparison with other methods", "text": "We compared the AWI model with the Sequenceto Sequence (Seq2Seq) (Vinyals and Le, 2015) and the hierarchically recurring encoder decoder (HRED) (Serban et al., 2015a) models, all of which had two layers of encoder and decoder. Hidden dimensions for the encoder and decoder were set to 200 in all models, and the hidden dimensions for the intention network were set to 50. All models had their optimal configuration based on the development approach. Both Seq2Seq and HRED used typical short-time memory networks (Hochreiter and Schmidhuber, 1997), the number of parameters being about 4.48 x 106 for Seq2Seq and 4.50 x 106 for HRED. AWI did not have the input similarity feature and it had 5.71 x 106 parameters. Greedy search was used in this experiment. Table 1 shows that AWI's hierarchical structure is worse than other AWI models."}, {"heading": "4.4.2 Results with specificity improved models", "text": "We report on BLEU and IDF values in Table 2. The baseline is trained with standard cross entropy in Sec. 3.1. For comparison, we used a sampling method (Mikolov et al., 2011) to generate responses called \"AWI + Sampling.\" Using sampling would result in a reasonable number of rare words and therefore an IDF score similar to that of the reference responses. In fact, this is observed in the table in \"AWI + Sampling.\" It has an IDF score of 2.76, close to the IDF score of 2.74 from the training set. However, sampling results in inferior BLEU scores, although it has a higher IDF score than AWIDF decoding score (Li et al., 2016), which is called \"AWI + MMDF Score.\""}, {"heading": "4.4.3 Analysis", "text": "Figure 2 uses t-SNE (van der Maaten and Hinton, 2008) to visualize the intention vectors. It shows clear clusters, although training intention vectors do not use explicit labels. To relate these clusters with explicit meaning, we look at the answers generated from these intention vectors and mark similar responses with the same color. Some answer types are clearly clustered, such as \"Greetings\" and \"Close this chat.\" However, other types are more distributed and do not find a unique day for these answers. Therefore, we leave them untagged. We also show two examples of responses in Tables 3 and 4 of AWI and IR-AWI. Responses from AWI + MMI and AWI + IDF are the same as from AWI, so we list only answers from AWI and IRAWI. These answers are reasonable. However, the IR AWI responses in these tables are more specific than the generic responses from AWI."}, {"heading": "4.5 Performance for retrieval", "text": "We report recall rates, R @ 1 and R @ 5, in Table 5. The AWI model, which was trained using the ranking criterion in Section 3.3.2, performs significantly better than TF-IDF. Importantly, the AWI model was able to combine TF-IDF with the method described in Section 3.3.3, resulting in significant performance improvements."}, {"heading": "5 Related work", "text": "Our work refers to both goal-oriented and non-goal-oriented dialog systems, since the proposed model can be used as a language generation component in a goal-oriented dialog (Young et al., 2013). While traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) relies on explicit state in the POMDP framework for a goal-oriented dialog system (Williams, 2009), the proposed model may loosen such requirements. However, the grounding of the generated conversation with actions and knowledge is not examined in this paper. It will be a future work. The proposed model refers to recent work in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015a) that uses to use a coding model."}, {"heading": "6 Conclusions", "text": "We presented a novel neural conversation model with improved specificity, which was evaluated for both generation and retrieval of responses. We observed significant improvements in performance compared to alternative methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "IRIS: a chat-oriented dialogue system based on the vector space model", "author": ["Banchs", "Li2012] Rafael E. Banchs", "Haizhou Li"], "venue": null, "citeRegEx": "Banchs et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2012}, {"title": "An estimate of an upper bound for the entropy of English", "author": ["Brown et al.1992] Peter Brown", "Vincent Pietra", "Robert Mercer", "Stephen Pietra", "Jennifer Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Carbonell", "Goldstein1998] Jaime Carbonell", "Jade Goldstein"], "venue": "Research and development in information retrieval,", "citeRegEx": "Carbonell et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1998}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Statistical identification of language", "author": ["Ted Dunning"], "venue": "Technical Report Technical Report MCCS 94-273,", "citeRegEx": "Dunning.,? \\Q1994\\E", "shortCiteRegEx": "Dunning.", "year": 1994}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Galley et al.2015] Michel Galley", "Chris Quirk", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Online policy optimisation of Bayesian spoken dialogue systems via human interaction", "author": ["Gasic et al.2013] M. Gasic", "C. Breslin", "M. Henderson", "D. Kim", "M. Szummer", "B. Thomson", "P. Tsiakoulis", "S. Young"], "venue": "In ICASSP", "citeRegEx": "Gasic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gasic et al\\.", "year": 2013}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Gimpel et al.2013] Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Attention, intentions, and the structure of discourse", "author": ["Grosz", "Sidner1986] Barbara J. Grosz", "Candace L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "Grosz et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1986}, {"title": "Word-based dialog state tracking with recurrent neural networks. In SIGDIAL", "author": ["Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A diversitypromopting objective function for neural conversation model", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: an empirical study of unsupervised evaluation metics for dialogue response generation", "author": ["Liu et al.2016] Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "In SIGDIAL", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "On the evluation of dialogue systems with next utterance classification", "author": ["Lowe et al.2016] Ryan Lowe", "Iulian V. Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Lowe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "Jan Honza Cernock", "Sanjeev Khudanpur"], "venue": "In ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Minimum error rate training for statistical machine translation", "author": ["Franz Josef Och"], "venue": null, "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Using TF-IDF to determine word relevance in document queires", "author": ["Juan Ramos"], "venue": "In ICML", "citeRegEx": "Ramos.,? \\Q2003\\E", "shortCiteRegEx": "Ramos.", "year": 2003}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Unsupervised modeling of Twitter conversation", "author": ["Ritter et al.2010] Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Buckley1988] Gerard Salton", "Christopher Buckley"], "venue": "Information Processing and Management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Building end-to-end dialogue systems using generative hierachical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for shorttext conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "2015a. A neural network approach to context-sensitive generation of conversation responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "2015b. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jacob G. Simonsen", "Jian-Yun Nie"], "venue": "In CIKM", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Barto1988] Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1988}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. In COURSERA: Neural Networks for Machine Learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Visualizing data usin t-SNE", "author": ["van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A nerual converstion model", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc V. Le"], "venue": "In ICML Deep Learning Workshop", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Simple statistical gradient-flow algorithms for connectionist reinforcement learning", "author": ["Ronald Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Spoken dialogue systems: Challenges, and opportunities for research", "author": ["Jason D. Williams"], "venue": "In ASRU", "citeRegEx": "Williams.,? \\Q2009\\E", "shortCiteRegEx": "Williams.", "year": 2009}, {"title": "POMDPbased statistical spoken dialog systems: A review", "author": ["Young et al.2013] Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Sutskever2015] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, neural network based conversation models (Serban et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015) have emerged as a promising complement to traditional partially observable Markov decision process (POMDP) models (Young et al.", "startOffset": 58, "endOffset": 145}, {"referenceID": 38, "context": ", 2015) have emerged as a promising complement to traditional partially observable Markov decision process (POMDP) models (Young et al., 2013).", "startOffset": 122, "endOffset": 142}, {"referenceID": 30, "context": "Neural conversation models are usually trained similarly to neural machine translation models (Sutskever et al., 2014; Cho et al., 2014), which treat response generation as a surface-to-surface transformation.", "startOffset": 94, "endOffset": 136}, {"referenceID": 4, "context": "Neural conversation models are usually trained similarly to neural machine translation models (Sutskever et al., 2014; Cho et al., 2014), which treat response generation as a surface-to-surface transformation.", "startOffset": 94, "endOffset": 136}, {"referenceID": 12, "context": "We further tackle a second key problem of neural conversation models, their tendency to generate generic, non-specific responses (Vinyals and Le, 2015; Li et al., 2016).", "startOffset": 129, "endOffset": 168}, {"referenceID": 12, "context": "To address the problem of specificity, a maximum mutual information (MMI) method for generation was proposed in (Li et al., 2016), which models both sides of the conversation, each conditioned on the other.", "startOffset": 112, "endOffset": 129}, {"referenceID": 21, "context": "Second, we address the specificity problem by incorporating inverse document frequency (IDF) (Salton and Buckley, 1988; Ramos, 2003) into the training process.", "startOffset": 93, "endOffset": 132}, {"referenceID": 12, "context": "Empirically, we find that it performs better than the dual-model method of (Li et al., 2016).", "startOffset": 75, "endOffset": 92}, {"referenceID": 14, "context": "Using a recently proposed evaluation metric (Lowe et al., 2015; Lowe et al., 2016), we observed that this model was able to incorporate term-frequency inverse document frequency (TF-IDF) (Salton and Buckley, 1988) and significantly outperformed a TFIDF retrieval baseline and the model without using TF-IDF.", "startOffset": 44, "endOffset": 82}, {"referenceID": 15, "context": "Using a recently proposed evaluation metric (Lowe et al., 2015; Lowe et al., 2016), we observed that this model was able to incorporate term-frequency inverse document frequency (TF-IDF) (Salton and Buckley, 1988) and significantly outperformed a TFIDF retrieval baseline and the model without using TF-IDF.", "startOffset": 44, "endOffset": 82}, {"referenceID": 30, "context": "The proposed model is in the encoder-decoder framework (Sutskever et al., 2014) but incorporates a hierarchical structure to model dependence between turns of conversation process.", "startOffset": 55, "endOffset": 79}, {"referenceID": 0, "context": "Attention layer We use the content-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 61, "endOffset": 104}, {"referenceID": 16, "context": "Attention layer We use the content-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 61, "endOffset": 104}, {"referenceID": 18, "context": "A problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016).", "startOffset": 95, "endOffset": 128}, {"referenceID": 22, "context": "A problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016).", "startOffset": 95, "endOffset": 128}, {"referenceID": 12, "context": "Another problem is that decoding to maximize sentence likelihood typically results in non-specific high-frequency words (Li et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 18, "context": "Tuning the interpolation weight is on a development set using minimum error rate training (MERT) (Och, 2003).", "startOffset": 97, "endOffset": 108}, {"referenceID": 19, "context": "The interpolation weight that achieves the highest BLEU (Papineni et al., 2002) score on the development set is used for testing.", "startOffset": 56, "endOffset": 79}, {"referenceID": 36, "context": "This problem can be solved using REINFORCE (Williams, 1992), in which the gradient to update model parameter is calculated as follows", "startOffset": 43, "endOffset": 59}, {"referenceID": 14, "context": "Term-frequency inverse document frequency (TFIDF) (Salton and Buckley, 1988) is an established baseline for conversation model used for retrieval (Lowe et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 20, "context": "The encoder network uses word embedding initialized from 300-dimension GLOVE vector (Pennington et al., 2014) trained", "startOffset": 84, "endOffset": 109}, {"referenceID": 19, "context": "The first measure is BLEU (Papineni et al., 2002), which uses the Ngram (Dunning, 1994) to compute similarities between references and responses, together with a penalty for sentence brevity.", "startOffset": 26, "endOffset": 49}, {"referenceID": 5, "context": ", 2002), which uses the Ngram (Dunning, 1994) to compute similarities between references and responses, together with a penalty for sentence brevity.", "startOffset": 30, "endOffset": 45}, {"referenceID": 6, "context": "While BLEU may unfairly penalize paraphrases with different wording, it has found correlated well with human judgement on responses generation tasks (Galley et al., 2015).", "startOffset": 149, "endOffset": 170}, {"referenceID": 2, "context": "The second measure is perplexity (Brown et al., 1992), which measures the likelihood of generating a word given observations.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "adopt a response selection measure proposed in (Lowe et al., 2015), in which the performance of a conversation model is measured by the recall rate of those correct responses in the top ranks.", "startOffset": 47, "endOffset": 66}, {"referenceID": 14, "context": "The number of candidates for retrieval is 10, following (Lowe et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 15, "context": "This measure is observed to correlate well with human judgment for retrieval based conversation model (Lowe et al., 2016; Liu et al., 2016).", "startOffset": 102, "endOffset": 139}, {"referenceID": 13, "context": "This measure is observed to correlate well with human judgment for retrieval based conversation model (Lowe et al., 2016; Liu et al., 2016).", "startOffset": 102, "endOffset": 139}, {"referenceID": 12, "context": "AWI + MMI (Li et al., 2016) 11.", "startOffset": 10, "endOffset": 27}, {"referenceID": 17, "context": "For comparison, we used a sampling method (Mikolov et al., 2011) to generate responses, denoted as \u201dAWI + sampling\u201d.", "startOffset": 42, "endOffset": 64}, {"referenceID": 12, "context": "We also report result using MMI method for decoding (Li et al., 2016), denoted as \u201dAWI + MMI\u201d.", "startOffset": 52, "endOffset": 69}, {"referenceID": 38, "context": "Our work is related both to goal and non-goal oriented dialogue systems as the proposed model can be used as a language generation component in a goal-oriented dialogue (Young et al., 2013) or simply to produce chit-chat style dialogue without a Models R@1 R@5", "startOffset": 169, "endOffset": 189}, {"referenceID": 23, "context": "specific goal (Ritter et al., 2010; Banchs and Li, 2012; Ameixa et al., 2014).", "startOffset": 14, "endOffset": 77}, {"referenceID": 10, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 7, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 35, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 37, "context": ", 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 31, "endOffset": 47}, {"referenceID": 38, "context": ", 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al., 2013), the proposed model may relax such requirement.", "startOffset": 101, "endOffset": 121}, {"referenceID": 27, "context": "The proposed model is related to the recent works in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015a), which use an encoder-decoder framework to model conversation.", "startOffset": 53, "endOffset": 118}, {"referenceID": 8, "context": "Prior work to potentially increase specificity or diversity aims at producing multiple outputs (Carbonell and Goldstein, 1998; Gimpel et al., 2013) and our work is the same as in (Li et al.", "startOffset": 95, "endOffset": 147}, {"referenceID": 12, "context": ", 2013) and our work is the same as in (Li et al., 2016) to produce a single nontrivial output.", "startOffset": 39, "endOffset": 56}, {"referenceID": 12, "context": "Instead of using an objective function in (Li et al., 2016) that has an indirect relation to specificity, our model uses a specificity measure directly for training and decoding.", "startOffset": 42, "endOffset": 59}], "year": 2016, "abstractText": "In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention. Secondly, it avoids generating nonspecific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval.", "creator": "LaTeX with hyperref package"}}}