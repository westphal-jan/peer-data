{"id": "1606.01341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Neural Architectures for Fine-grained Entity Type Classification", "abstract": "In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset.", "histories": [["v1", "Sat, 4 Jun 2016 07:52:22 GMT  (273kb,D)", "http://arxiv.org/abs/1606.01341v1", "10 pages, 3 figures"], ["v2", "Tue, 21 Feb 2017 06:49:42 GMT  (532kb,D)", "http://arxiv.org/abs/1606.01341v2", "10 pages, 3 figures, accepted at EACL2017 conference"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sonse shimaoka", "pontus stenetorp", "kentaro inui", "sebastian riedel"], "accepted": false, "id": "1606.01341"}, "pdf": {"name": "1606.01341.pdf", "metadata": {"source": "CRF", "title": "Neural Architectures for Fine-grained Entity Type Classification", "authors": ["Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "emails": ["simaokasonse@ecei.tohoku.ac.jp", "inui@ecei.tohoku.ac.jp", "p.stenetorp@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has been shown that most of them are people who are not able to follow the rules. (...) In recent years, it has been shown that people are able to follow the rules. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...). (...) The world has changed. (...) In the last ten years, the situation has changed. (...). (...) In the last ten years, the situation has changed. (...). (...) In the last ten years, the situation has changed. (...). (...) The world has changed. (...) In the last ten years, the world has changed. (...) The world has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...). (... \"In the last ten years, the situation has changed. (...) The world has changed. (...\" In the last ten years, the world has changed. (... \"In the last ten years, the world has changed. (...). (...) The world has changed. (...\" In the last ten years, the last ten years, the world has changed. (... The world has changed. (...)."}, {"heading": "2 Related Work", "text": "This year it is more than ever before."}, {"heading": "3 Models", "text": "In this section, we describe the models by Shimaoka et al. (2016), our extensions, and a strong property-based baseline from literature. Wepose fine-grained entity classification as a multi-level, multi-level classification problem. When mentioned in a sentence, the classifier predicts the types t, where K is the size of the type set. In all models, we calculate a probability yk-R for each of the K types using logistic regression. Variations in the models result from the way in which the input into logistic regression is calculated. At the time of the conclusion, we assume that at least one type is assigned to each mention, by first assigning the type with the highest probability. Then, we assign other types based on the condition that their corresponding probabilities must be greater than a threshold of 0.5."}, {"heading": "3.1 Sparse Feature Model", "text": "The characteristics used are described in Table 1, which is similar to those of Gillick et al. (2014) and Yogatama et al. (2015), so that they constitute a meaningful, well-established baseline; however, there are two notable differences. First, we use the more widely used cluster method of Brown et al. (1992), in contrast to Uszkoreit and Brants (2008), since Gillick et al. (2014) have not made the data used for their clusters publicly available. Second, we have implemented a number of 15 topics from the OntoNotes dataset using the LDA (lead et al.), as we see the differences between the popular Gillick et al. (2014) and the untrained Gillick dataset (2014) as opposed to the unspecific characteristics we use in comparison."}, {"heading": "3.2 Neural Models", "text": "The neural models proposed by Shimaoka et al. (2016) process the embedding of the words of mention and their context; and we apply a similar formalism when presenting their models and our variants. First, the mention representation vm-RDm-1 and the context representation vc-RDc-1 are calculated separately, and then the concatenation of these representations is used to calculate the prediction: y = 11 + exp (\u2212 Wy [vm vc]) (1) Where Wy-RK-1 (Dm + Dc) is the weight matrix. Let's leave the words in the mention m1, m2,..., m | m |. Then the representation of the mention is calculated as follows: vm = 1 | m | m | view i = 1 u (mi) (2) Where u is an assignment of a word to an embedding. As Shimaoka et al. (2016), the reason given for this is relative to the application of the three models."}, {"heading": "3.2.1 Averaging Encoder", "text": "Similar to the method for calculating the mention representation, the averaging encoder calculates the mean values of the words in the left and right context. Formally, l1,..., lC and r1,... rC should be the words in 1http: / / radimrehurek.com / gensim / in the left and right context, respectively, where C is the window size. We then calculate the average of the corresponding word embeddings for each word sequence. These two vectors are then linked to form the representation of the context vc."}, {"heading": "3.2.2 LSTM Encoder", "text": "For the LSTM encoder, the left and the right context are encoded by an LSTM (Hochreiter and Schmidhuber, 1997). The higher-level formulation of an LSTM is as follows: hi, si = lstm (ui, hi \u2212 1, si \u2212 1) (3) Where ui-RDm \u00b7 1 is an input embed, hi \u2212 1-RDh \u00b7 1 is the previous output and si \u2212 1-RDh \u00b7 1 is the previous cell state. For the left context, the LSTM is applied to the sequence l1,..., lC from left to right and generates the outputs \u2212 \u2192 hl1,..., \u2212 hlC. For the right context, the sequence rC,..., r1 is processed from right to left to produce the outputs. The concatenation of \u2212 \u2192 hlC and \u2190 \u2212 hr1 then serves as context representation vc."}, {"heading": "3.2.3 Attentive Encoder", "text": "An attention mechanism aims to encourage the model to focus on outstanding local information relevant to the classification decision. In this paper, the variant of the attention mechanism used is defined as: First, bidirectional LSTMs (Graves, 2012) are applied for both the right and left context. We define the output levels of the bidirectional LSTMs as \u2212 hl1, \u00b7 hl1, \u2212 hlC and \u2212 hr1, \u2212 hr1, \u2212 hrC. For each output layer, a scalar value a-i-hli is calculated, using a feeding neural network with the hidden layer ei-RDa-1 and the weight matrices We-RDa-2Dh and Wa-R1-hrC."}, {"heading": "3.3 Hybrid Models", "text": "To enable model variants to leverage both human background knowledge through handmade features and data-derived features, we have expanded the neural models to create new hybrid model variants. Let vf-RDl-1 be a low-dimensional projection of the sparse feature f (m): vf = Wff-RDl-Df is a projection matrix. The hybrid model variants are then defined as: y = 11 + exp-Wy vmvcvf (9) These models can therefore draw on learned features from vm and vc in classification decisions, as well as handmade features using vf. While existing work on the fine-grained classification of entities has used either sparse, manually designed features or dense, automatically learned embedding of vectors, our work is the first to suggest and evaluate a model using the combination of both features."}, {"heading": "3.4 Hierarchical Label Encoding", "text": "Since the fine-grained types tend to form a forest of type hierarchies (e.g. musician is a subtype of the artist, who in turn is a subtype of the person), we investigated whether the coding of each label could use this structure to allow sharing of parameters. Specifically, we put together the weight matrix Wy for the logistic regression layer as the product of a learned weight matrix Vy and a constant sparse binary matrix S: W Ty = VyS (10). We coded the type hierarchy, which consists of the set of types in the binary matrix S, as follows: Each type is assigned to a unique column in S, in which the affiliation at each level of its type hierarchy is characterized by a 1. For example, if we use the set of types defined by Gillick et al. (2014), the column for the person in types 1, 0 could be."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "Although the research community has broadly agreed to use the manually annotated data sets FIGER (GOLD) (Ling and Weld, 2012) and OntoNotes (Gillick et al., 2014) for evaluation, there is still a notable difference in the data used to train models (Table 2), which are then evaluated on the same manually annotated data sets. It is also noteworthy that some data is not even publicly available, which makes a fair comparison between the methods even more difficult. For evaluation, we use the two well-established manually annotated data sets FIGER (GOLD) and OntoNotes in our experiments, where, like Gillick et al. (2014), we have discarded pronominal mentions, resulting in a total of 8,963 mentions. For training purposes, we use the automatically induced publicly available data sets FIGER (GOLD) and OntoNotes, which were provided by Ren et al (2016)."}, {"heading": "4.2 Pre-trained Word Embeddings", "text": "We use pre-trained word embeddings that have not been updated during the training to help the model generalize words that do not appear in the training set (Rockta \u00bc schel et al., 2015). To this end, we use the freely available 300-dimensional enclosed word embeddings trained on 840 billion tokens from the Common Crawl by Pennington et al. (2014). For words that do not appear in the pre-trained word embeddings, we use the embedding of the \"unk\" token."}, {"heading": "4.3 Evaluation Criteria", "text": "We use the same criteria as Ling and Weld (2012), i.e. we evaluate the performance of the model using strict accuracy, loose macro and loose micro values."}, {"heading": "4.4 Hyperparameter Settings", "text": "Specifically, all neural and hybrid models used the same Dm = 300-dimensional word embedding, the hidden size of the LSTM was set to Dh = 100, the hidden layer size of the attention module to Da = 100.2. Although Ren et al. (2016) provided both \"raw\" data and a pipeline to \"denocialize\" the data, we were unable to replicate the performance benefits reported in their work after operating their pipeline. We contacted them in this regard, as we would be interested in comparing the benefits of their denocidating algorithm for each model, but at the time of writing we have not yet received an answer, and the size of the low-dimensional projection of the sparse features was set to Dl = 50. We used Adam (Kingma and Ba, 2014) as our optimization method with a learning rate of 0.001, a mini-batch of word size that we applied beyond the scope of the training to represent the set and the characteristics in the sentence."}, {"heading": "4.5 Results", "text": "When presenting our results, it should be noted that we strive for a clear separation between results and models that have been trained on the basis of different data sets."}, {"heading": "4.5.1 FIGER (GOLD)", "text": "First, we analyze the results on FIGER (GOLD) (Tables 3 and 4).The performance of the base model, which uses the sparse handmade features, is relatively close to that of the Ling and Weld FIGER system (2012).This is consistent with the fact that both systems use linear classifiers, similar sets of features, and training data of the same size and domain. Looking at the results of neural models, we observe a consistent pattern that adding handmade features significantly increases performance, suggesting that the learned and handmade features complement each other.The effects of adding hierarchical label coding were inconsistent, sometimes increasing, sometimes decreasing. We therefore decided not to include them in the results table, due to space constraints and hypothesis that given the size of the training data, the parameter distribution does not produce major performance benefits in 2016. We also note that among the encoding models we have significantly worse than the encoding models previously."}, {"heading": "4.5.2 OntoNotes", "text": "Second, we discuss the results on OntoNotes (Tables 5 and 6). Once again, we see consistent performance improvements when the sparse craft characteristics are added to the neural models. In the absence of craft characteristics, the average encoder suffers from relatively poor performance and the attentive encoder performs best. However, when the craft characteristics are added, a significant improvement occurs for the average encoder, making the performance of the three neural models much more equal. As a reason for these results, we speculate that some of the craft characteristics, such as the dependency role and parent word of the head, provide crucial information for the task that cannot be captured by the simple average model, but can be learned if an attention mechanism is present. Another speculative reason is that the training data set is noisy compared to the FIGER (GOLD), the anchor used to detect the animals."}, {"heading": "4.6 PCA visualization of label embeddings", "text": "By visualizing the learned label embeddings (Figure 3) and comparing the non-hierarchical and hierarchical label encodings, we can observe that the hierarchical encoding forms clear, unambiguous clusters."}, {"heading": "4.7 Attention Analysis", "text": "To better understand this, we first analyzed a large number of attention visualizations and found that the word that was most frequently used and the words that were included in the formulation that were mentioned tended to receive the highest level of attention. To quantify this perception, we calculated how often the word that was most frequently used in all of the mentions of a particular type was the syntactic head or words before and after the mention in its formulation. What we found from our analysis (Table 7) was that our attentive model, without any craft features, actually learns that the word and the formulation that surrounds the mention are most indicative of the mention type, without explicit monitoring. Furthermore, we believe that this may partially explain why the benefit of adding handcrafted features was lower for the attention-oriented model than for our other two neural variants."}, {"heading": "5 Conclusions and Future Work", "text": "In this work, we have proposed a novel state-of-the-art neural network architecture with an attention mechanism for the task of fine-grained entity classification and demonstrated that the model can successfully learn to pay attention to expressions that are important for the classification of fine-grained types. As a future work, we consider the re-implementation of other methods from the literature as a desirable goal so that they can be evaluated after using the same training data. Once adopted, we will publish our code and our experimental pipeline to enable the reproducibility of our work."}, {"heading": "Acknowledgments", "text": "This work was supported by CREST-JST, JSPS KAKENHI Grant Number 15H01702, a Marie CurieCareer Integration Award and an Allen Distinguished Investigator Award. We would like to thank Dan Gillick for answering several questions related to his 2014 work."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2003", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u2013", "citeRegEx": "Blei et al.2003", "shortCiteRegEx": null, "year": 1022}, {"title": "Della Pietra", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J"], "venue": "and J.C. Lai.", "citeRegEx": "Brown et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Estevam R Hruschka Jr", "author": ["Andrew Carlson", "Justin Betteridge", "Richard C Wang"], "venue": "and Tom M Mitchell.", "citeRegEx": "Carlson et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Rainer Gemulla", "author": ["Luciano Del Corro", "Abdalghani Abujabal"], "venue": "and Gerhard Weikum.", "citeRegEx": "Del Corro et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ming Zhou", "author": ["Li Dong", "Furu Wei", "Hong Sun"], "venue": "and Ke Xu.", "citeRegEx": "Dong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Jesse Kirchner", "author": ["Dan Gillick", "Nevena Lazic", "Kuzman Ganchev"], "venue": "and David Huynh.", "citeRegEx": "Gillick et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised sequence labelling", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ilya Sutskever", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky"], "venue": "and Ruslan R Salakhutdinov.", "citeRegEx": "Hinton et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ji-Hyun Wang", "author": ["Changki Lee", "Yi-Gyu Hwang", "Hyo-Jung Oh", "Soojong Lim", "Jeong Heo", "Chung-Hee Lee", "Hyeon-Jin Kim"], "venue": "and Myung-Gil Jang.", "citeRegEx": "Lee et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S Weld"], "venue": "In In Proc. of the 26th AAAI Conference on Artificial Intelligence. Citeseer", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Rion Snow", "author": ["Mike Mintz", "Steven Bills"], "venue": "and Dan Jurafsky.", "citeRegEx": "Mintz et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Marie-Catherine de Marneffe", "author": ["Marta Recasens"], "venue": "and Christopher Potts.", "citeRegEx": "Recasens et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Heng Ji", "author": ["Xiang Ren", "Wenqi He", "Meng Qu", "Clare R Voss"], "venue": "and Jiawei Han.", "citeRegEx": "Ren et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "and Phil Blunsom.", "citeRegEx": "Rockt\u00e4schel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Extended named entity ontology with attribute information", "author": ["Satoshi Sekine"], "venue": "In LREC,", "citeRegEx": "Sekine.,? \\Q2008\\E", "shortCiteRegEx": "Sekine.", "year": 2008}, {"title": "Kentaro Inui", "author": ["Sonse Shimaoka", "Pontus Stenetorp"], "venue": "and Sebastian Riedel.", "citeRegEx": "Shimaoka et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed word clustering for large scale class-based language modeling in machine translation", "author": ["Uszkoreit", "Brants2008] Jakob Uszkoreit", "Thorsten Brants"], "venue": "In Proceedings of ACL08: HLT,", "citeRegEx": "Uszkoreit et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit et al\\.", "year": 2008}, {"title": "Dan Gillick", "author": ["Dani Yogatama"], "venue": "and Nevena Lazic.", "citeRegEx": "Yogatama et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Marc Spaniol", "author": ["Mohamed Amir Yosef", "Sandro Bauer", "Johannes Hoffart"], "venue": "and Gerhard Weikum.", "citeRegEx": "Yosef et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the wellestablished FIGER (GOLD) dataset.", "creator": "LaTeX with hyperref package"}}}