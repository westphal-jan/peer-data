{"id": "1606.01380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Effective Multi-Robot Spatial Task Allocation using Model Approximations", "abstract": "Real-world multi-agent planning problems cannot be solved using decision-theoretic planning methods due to the exponential complexity. We approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent Markov decision process. We use recent approximation methods for spatial task problems to reduce the model complexity. Our approximations are single-agent, static task, shortest path pruning, dynamic planning horizon, and task clustering. We create scenarios from RoboCup Rescue Simulation maps and evaluate our methods on these graph worlds. The results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy. We also show that our method has a similar performance as DCOP methods on example RCRS scenarios.", "histories": [["v1", "Sat, 4 Jun 2016 14:12:32 GMT  (1033kb,D)", "http://arxiv.org/abs/1606.01380v1", "RoboCup 2016 Symposium"]], "COMMENTS": "RoboCup 2016 Symposium", "reviews": [], "SUBJECTS": "cs.AI cs.MA cs.RO", "authors": ["okan a\\c{s}{\\i}k", "h levent ak{\\i}n"], "accepted": false, "id": "1606.01380"}, "pdf": {"name": "1606.01380.pdf", "metadata": {"source": "CRF", "title": "Effective Multi-Robot Spatial Task Allocation using Model Approximations", "authors": ["Okan A\u015f\u0131k", "Levent Ak\u0131n"], "emails": ["akin}@boun.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-agent Markov Decision Process", "text": "A Multi-Agent Markov Decision Process (MMDP) is a mathematical formalization for multi-agent planning in observable but uncertain action environments. Multiagent MDP is a 5x < D, S, A, T, R > references - D is the set of actors, - S is the finite set of states, - A is the finite set of joint actions (A1 \u00b7 A2 \u00b7.. \u00d7 An), - T is the transition function that assigns probabilities for the transition from one state to another in a joint action, - R is the immediate reward function. We can solve an MMDP using the standard offline MDP scheduling algorithms, e.g. with the value teration [10]. The value teration algorithm iteratively improves the estimation of the expected value of a state with the following Bellmann equation: Q (s, a) = R (s, a) + 1 (s) (s) (s) (s) (a) (a) stands for a value (a)."}, {"heading": "2.2 Rescue Spatial Task Allocation Problem", "text": "In fact, it is so that most people who are able to survive themselves are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, they are able to survive themselves and survive themselves."}, {"heading": "3 Related Work", "text": "The teams in RoboCup Rescue Agent Simulation (RCRS) generally use government strategies in behavioral agent frameworks 1. Teams prefer agent frameworks that allow them to take advantage of typical scenarios. These agent frameworks fine-tune the teams \"behavior according to the cases that arise over the time periods of trial failures. In a recent study, Parker et al. report on the performance of decentralized coalitions for RCRS [8]. Agents use a greedy algorithm with a service function designed for different tasks. They compare static and dynamic coalitions with heterogeneous agents. Due to the different characteristics of each RCRS scenario, they find that different approaches work well for different scenarios. In the literature, the RCRS problem is also modeled as a task assignment problem. The tasks represent the rescue of a civilian population, firefighting and clearing up of the blocks of time and the tasks are distributed over the time."}, {"heading": "4 Methods", "text": "We model the firefighting task of the RoboCup Rescue Simulation (RCRS) as a Multi-Agent Markov Decision Process (MMDP).We create an approximate MMDP model with single agents, static task, shortest section, task clusters and online planning horizon approximations in our online planning framework. The online planning framework draws the current state from the simulator (either Rescue SPATAP Simulator or RoboCup Rescue Simulator) and creates a new problem by merging the obvious tasks. Subsequently, the approximations are applied to the cluster model in order to have a less complex model.The policy for the approximate model is calculated using the Value Iteration algorithm [10]. We calculate the goal of each agent by following the guidelines to greedily assign each agent a cluster. Since we have assigned each agent a cluster, the approximate model is aligned with the tasks and the guidelines of the cluster only."}, {"heading": "4.1 Hierarchical Planning by Task Clustering", "text": "Before applying any approximations to the actual model of the problem, we create clusters to further reduce complexity. As fires spread from the original ignition points, the tasks appear as clusters. Therefore, we introduce a distance-based cluster algorithm for tasks. Tasks that are closer to each other than the meter belong to the same cluster. We assign each agent a cluster according to model approximations and value repetition algorithms. After each agent has been assigned a cluster, we plan only for tasks that belong to the cluster of the agent. To create a cluster, we iterate over all burning buildings and compare the distance between the building and the buildings in a cluster. If the building is closer than dMeter to one of the buildings that are located in a cluster, the building is added to the cluster. The cluster buildings and their neighbors are removed. A new building for each cluster is created with the area that corresponds to the sum of the area of the buildings in the cluster in the cluster, the value of the neighboring ATT is assigned as the first cluster agent."}, {"heading": "4.2 Single-Agent Approximation", "text": "To reduce this complexity, we plan as a single agent, using the positions of other agents as an indicator of their policies. We calculate a policy for all agents as if they were the only agent in the environment. Then, for each agent, we calculate the aggregate effect of the other agents, which is called the presence mass. Presence mass is the probability distribution of the positions of other agents on the world map. We use this policy to have an idea of the most desirable action from the perspective of this agent. To reflect the uncertainty, we use this policy to calculate a Boltzmann distribution of actions for each state. We define a Boltzmann distribution of state action values (the expected cumulative reward if an action is taken in a state)."}, {"heading": "4.3 Static Task Approximation", "text": "We also aim to reduce the exponential complexity due to the fire levels of buildings. Therefore, we use the approach proposed by [1] and redefine the state space to include only buildings in a burning state. Claes and et al. [1] propose this approach for spatially distributed tasks where the occurrence of new tasks is independent. In the fire-fighting problem, there is the impact of neighboring buildings on the occurrence of new tasks. However, the spread of fire on RCRS is so slow that we can only plan buildings that are in flames without calculating their impact on their neighbors. The deterministic measures and static task approximations on a graph world for a single agent MDP result in the following Bellman equation: V (s) = max s \u00b2 N (s) R (s) + \u03b3V (s) R (s) (s) (4) s denotes the current state, s \u00b2 the neighboring function, n \u00b2 the discontinuous factor, and the value for V."}, {"heading": "4.4 The Shortest Path Approximation", "text": "The traceability of the Bellman equation depends on the state space and the transition function (i.e. the neighbor). If the agent does not move to the vertex that is in the burning state, the state can only be identified by the position of the agent. If we remove the neighbors that are not visited by the agent, we achieve that the state space and also the branching factor of the transition function is reduced. The value iteration algorithm propagates rewards from the target state, in our case this is one of the vertices that are in the burning state. We calculate the shortest path between the agent and the vertices that are in the burning state, as well as all possible burning vertices pairs. Since the actions are deterministic and the tasks static, the optimal policy leads to a movement of the agent on the shortest path from his own position to that of the tasks."}, {"heading": "4.5 Online Dynamic Planning Horizon", "text": "The runtime of the value estimation algorithm for the problems of the finite horizon also depends on the planning horizon, which determines the number of iterations of the algorithm. Considering Rescue-SPATAP, we should plan for optimal performance according to the current time step of the RCRS. However, the RCRS simulation runs over 300 time steps. Therefore, we propose to construct the planning horizon based on the reachability of the value estimation agent for 300 \u2212 30 = 270 time steps. However, as we have already approximated the problem, it could not increase performance after a certain horizon. Therefore, we propose to determine the planning horizon based on the reachability of the in the burning state. As shown by Claes et al. [1], if the agents plan only for the k next tasks, the algorithm still has adequate performance."}, {"heading": "5 Experiments and Results", "text": "We evaluate the effectiveness of our approach on the basis of the sampled graphs from RCRS cards. All experiments are carried out with the library BURLAP [6]."}, {"heading": "5.1 Comparison with The Optimal Policy", "text": "To measure the feasibility of our approach, we developed a Rescue SPATAP Simulator. We create 10 random scenarios with 8 buildings from five city diagrams of the RoboCup Rescue Simulation (RCRS), namely Istanbul, Berlin, Eindhoven, Joao Pessoa and Kobe. The graph sample can be considered as the random extraction of districts from a city map (see Figure 3). We define three random ignition points and two agents positioned in random locations. In the initial state, the ignition buildings are in the burning state. The distance of the building to the spread of the fire is d = 50 meters and the burning buildings add up p = 0.05 probability that their neighbors will ignite a fire. For example, if a building has 2 neighbors, the probability is that the state will not change the state of any fire until it burns."}, {"heading": "5.2 Scalability", "text": "In Figure 4a we show the average reward per time step for scenarios with different number of buildings. These scenarios have five agents and three random ignition points. Values are averaged over 50 runs and each run is set for 50 horizons. We can see that SPATAP-Ext performs better or equally well compared to the Greedy Algorithm. Depending on the ignition point, position of the agents and diagram, the difference between two algorithms could become greater or lesser. 1015202530354045 50 60 70 70 80 90 100 The average size of the state versus the timeesDue to the approximations, we are able to have linear runtimes when the number of agents or the number of buildings increases. The shortest path per time step versus the number of vertices (b) versus the average size of the state of the timees.Due to the approximations, we are able to have linear runtimes when the number of agents or the number of buildings increases."}, {"heading": "5.3 RoboCup Rescue RMASBench", "text": "All scenarios 2 have five to ten ignition points, 8 agents 2 test scenarios: https: / / github.com / okanasik / spatial _ task _ allocation in random positions and 100 horizon 3. Agents do not act before the 20th time step of the simulation to ensure the spread of the fire. RCRS score at the end of the simulation is shown in Table 2. Since the fire propagation behavior is not randomized, we report the results over a single run. This score represents the percentage of damage in the city. We compare the performance of the SPATAP Ext with the DCOP algorithms (greedy, DSA, BinaryMaxSum) of RMASBench. Although the DCOP methods generally work better than SPATAP Ext, they have the advantage that this CRAP factor affects the performance of the single extinguisher."}, {"heading": "6 Conclusion", "text": "We will demonstrate the application of online approximations to one of the challenging multi-agent planning problems. Our approach expands the SPATAP framework by introducing the shortest path, the most dynamic planning horizon, and the aggregation of approaches to a more difficult problem Rescue-SPATAP. We will show that our approach is better than the greedy approach and has similar performance to SPATAP, but requires less computation.3 An example run can be seen here: https: / / youtu.be / nuj8s9aFAlgIn the future, we plan to expand this framework for heterogeneous actors to model the entire RCRS problem. By introducing partial observability, communication, and decentralized planning, we plan to fully implement the online planning framework for RCRS. We will also reduce complexity by introducing macroeconomic measures."}], "references": [{"title": "Effective Approximations for Multi-Robot Coordination in Spatially Distributed Tasks", "author": ["D. Claes", "P. Robbel", "F.A. Oliehoek", "K. Tuyls", "D. Hennes", "W. van der Hoek"], "venue": "Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems. pp. 881\u2013890", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards efficient multiagent task allocation in the robocup rescue: a biologically-inspired approach", "author": ["F. Dos Santos", "A.L. Bazzan"], "venue": "Autonomous Agents and Multi-Agent Systems 22(3), 465\u2013486", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Decentralised Coordination of LowPower Embedded Devices Using the Max-Sum Algorithm", "author": ["A. Farinelli", "A. Rogers", "A. Petcu", "N.R. Jennings"], "venue": "Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS). pp. 639\u2013646", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed Coordination through Anarchic Optimization", "author": ["S. Fitzpatrick", "L. Meertens"], "venue": "Distributed Sensor Networks: A Multiagent Perspective, pp. 257\u2013295", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Rmasbench: Benchmarking dynamic multi-agent coordination in urban search and rescue", "author": ["A. Kleiner", "A. Farinelli", "S. Ramchurn", "B. Shi", "F. Maffioletti", "R. Reffato"], "venue": "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems. pp. 1195\u20131196. AAMAS \u201913, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Burlap library (2016), http://burlap.cs.brown.edu", "author": ["J. MacGlashan"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Approaching Urban Disaster Reality : The ResQ Firesimulator", "author": ["T.A. N\u00fcssle", "A. Kleiner", "M. Brenner"], "venue": "RoboCup 2004: Robot Soccer World Cup VIII. pp. 474\u2013482. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploiting spatial locality and heterogeneity of agents for search and rescue teamwork", "author": ["J. Parker", "E. Nunes", "J. Godoy", "M. Gini"], "venue": "Journal of Field Robotics 7(PART 1), n/a\u2013n/a", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient Inter-Team Task Allocation in RoboCup Rescue", "author": ["M. Pujol-Gonzalez", "J. Cerquides", "A. Farinelli", "P. Meseguer", "J.A. Rodriguez-Aguilar"], "venue": "Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems. pp. 413\u2013421", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Allocating tasks in extreme teams", "author": ["P. Scerri", "A. Farinelli", "S. Okamoto", "M. Tambe"], "venue": "Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems. pp. 727\u2013734. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "al [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "al [1] propose a series of approximations for SPATAP planning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "al [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "We use Value Iteration [10] algorithm to calculate the best action, but we choose the planning horizon according to the time step required to reach k tasks.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "We show that the comparison with the optimal value not as good as pure SPATAP problems, but our method performs better than other algorithms including the SPATAP algorithm [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Finally, we apply our SPATAP approximations to RoboCup Rescue Simuation (RCRS) scenarios and have similar performance to Distributed Constraint Optimization (DCOP) methods of RMASBench [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 9, "context": "We can solve an MMDP using the standard offline MDP planning algorithms such as value iteration [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Since the fire simulator of the RCRS is quite complex to model [7], we model fire spreading as independent events where the building on fire affects the neighbor buildings\u2019 fire state.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "report the performance of decentralized coalition formation approach for RCRS [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "This distributed dynamic task allocation problem is modeled as the distributed constraint optimization problem (DCOP) [11] and solved using state of the art DCOP algorithms such as MaxSum [3], and DSA [4].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "This distributed dynamic task allocation problem is modeled as the distributed constraint optimization problem (DCOP) [11] and solved using state of the art DCOP algorithms such as MaxSum [3], and DSA [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "This distributed dynamic task allocation problem is modeled as the distributed constraint optimization problem (DCOP) [11] and solved using state of the art DCOP algorithms such as MaxSum [3], and DSA [4].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "improve the computational efficiency of MaxSum by introducing Binary MaxSum for RCRS [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "There are also attempts to solve fire task allocation problem with biologicallyinspired methods [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "RMASBench is an effort to provide a software repository to easily model RCRS as a DCOP and benchmark the different algorithms [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "The policy for the approximated model is calculated using the Value Iteration [10] algorithm.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Then, for every agent we calculate the other agents\u2019 total effect which is called presence mass [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Therefore, we discount the future expected reward according to presence mass as proposed by [1]:", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The parameter fi which is used to scale the future value is calculated as the ratio of maximum reward to the maximum value as suggested by [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "Therefore, we use the approximation proposed by [1] and redefine the state space to include only the buildings that are in burning state.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "[1] propose this approximation for spatially distributed tasks where the occurrence of new tasks are independent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], if the agents plan only for the k closest tasks, the algorithm still has reasonable performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "All the experiments are implemented using the BURLAP [6] library.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We compare the performance of the SPATAP-Ext with DCOP algorithms (Greedy, DSA, BinaryMaxSum) of the RMASBench [5].", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "Real-world multi-agent planning problems cannot be solved using decision-theoretic planning methods due to the exponential complexity. We approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent Markov decision process. We use recent approximation methods for spatial task problems to reduce the model complexity. Our approximations are single-agent, static task, shortest path pruning, dynamic planning horizon, and task clustering. We create scenarios from RoboCup Rescue Simulation maps and evaluate our methods on these graph worlds. The results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy. We also show that our method has a similar performance as DCOP methods on example RCRS scenarios.", "creator": "LaTeX with hyperref package"}}}