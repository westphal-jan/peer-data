{"id": "1606.01404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Generating Natural Language Inference Chains", "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.", "histories": [["v1", "Sat, 4 Jun 2016 18:34:51 GMT  (310kb,D)", "http://arxiv.org/abs/1606.01404v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["vladyslav kolesnyk", "tim rockt\\\"aschel", "sebastian riedel"], "accepted": false, "id": "1606.01404"}, "pdf": {"name": "1606.01404.pdf", "metadata": {"source": "CRF", "title": "Generating Natural Language Inference Chains", "authors": ["Vladyslav Kolesnyk"], "emails": ["vladyslav.kolesnyk.12@ucl.ac.uk,", "t.rocktaschel@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The ability to determine the origin or contradiction between the text of natural language is essential for improving performance in a wide range of natural language processing tasks. Recognition of Textual Entailment (RTE) is a task primarily designed to determine whether two sentences of natural language are independent, contradictory, or in a convoluted relationship in which the second sentence (the hypothesis) can be derived exclusively from the first (the premise). Although systems that work well in RTE can potentially be used to improve the answering of questions, information extraction, text aggregation, and machine translation [6] are actually available in only a few of these downstream NLP tasks."}, {"heading": "2 Method", "text": "In this section, we briefly present the task of generating sequential damage and our sequence-to-sequence model."}, {"heading": "2.1 Entailment Generation", "text": "To create the data set for generating conclusions, we simply filter the Stanford Natural Language Inference Corpus by sequence class pairs. This results in a training set of 183, 416 sentence pairs, a development set of 3, 329 pairs, and a test of 3, 368 pairs. Instead of a classification task, we can now use this data set for a sequence transduction task."}, {"heading": "2.2 Sequence-to-Sequence", "text": "Recurring neural sequence-to-sequence networks [14] are successfully used for many sequence transduction tasks in NLP, such as machine translation [1, 5], constituency parsing [16], sentence summary [13], and answering questions [7]. They consist of two recurring neural networks (RNNs): an encoder that maps an input sequence of words into a dense vector representation, and a decoder that conditions this vector representation generates an output sequence. Specifically, we use long-term memory (LSTM) RNNs [8] for encoding and decoding. In addition, we experiment with word-for-word attention [1], which allows the decoder to search for these in the encoder output to bypass the LSTM memory bottleneck."}, {"heading": "2.3 Optimization and Hyperparameters", "text": "We use the stochastic gradient descend with a minibatch size of 64 and the ADAM optimizer [9] with a first impulse coefficient of 0.9 and a second impulse coefficient of 0.999. Word embedding is initialized with pre-trained word2vec vectors [10]. Words outside the vocabulary (10.5%) are randomly initialized by sampling values that are uniformly selected from [\u2212 \u221a 3, \u221a 3] and optimized during the training. In addition, we cut off gradients with a standard of 5.0. After 25 epochs we stop the training."}, {"heading": "3 Experiments and Results", "text": "We present results for various tasks: (i) presume a premise, generate a proposition that can be derived from the premise, (ii) construct chains of consequence by creating propositions recursively, and (iii) presume a proposition, create a premise that would entail this proposition, i.e. form a more descriptive proposition by adding specific information."}, {"heading": "3.1 Quantitative Evaluation", "text": "To our surprise, we found that the use of attention yields only a slightly higher BLEU score (43.1 vs. 42.8). We suspect that this is due to the fact that the generation of connected sentences has a larger space of valid target sequences, which makes the use of BLEU problematic and penalizes correct solutions. Therefore, we commented 100 random test sentences manually and decided whether the generated sentence can actually be derived from the source sentence. We found that sentences generated by an LSTM with attention are much more accurate (82% accuracy) than those generated from an LSTM baseline (71.7%). In order to gain more insight into the capabilities of the model, we turn to a thorough qualitative analysis of the attention LSTM model in the rest of this work."}, {"heading": "3.2 Example Generations", "text": "Figure 2 shows examples of sentences generated from the development group. Syntactical simplification of the input sentence seems to be the most common approach. The model removes certain parts of the premise such as adjectives, resulting in a more abstract sentence (see Figure 2.1). Figure 2.2 shows that the system can recognize the number of subjects in the sentence and incorporates this information into the generated sentence. However, we did not observe such \"counting behavior\" in more than four subjects, suggesting that the system memorizes the frequency patterns from the training group. Furthermore, we found predictions that suggest reasonable assumptions: If a sentence is about a father with a newborn baby, it is most likely that the newborn baby is his own child (Example 2.3). Restrictions Two recurring constraints of the proposed model relate to the use of words that have a very different meaning but contain similar word2vec embeddings (e.g.), as well as ambiguous words."}, {"heading": "3.3 Inference Chain Generation", "text": "Figure 5 shows that this works well, even though the model has only been trained on pairs of sentences. In addition, by creating inference chains for all sentences in the development sentence, we construct an entropy graph. In this graph, we found that sentences with common semantics are ultimately mapped to the same sentence that captures the common meaning.A visualization of the topology of the entropy graph is shown in Figure 6. Note that there are several long inference chains as well as large aggregations of sentences (nodes) that are mapped to the same common meaning (linkages)."}, {"heading": "3.4 Inverse Inference", "text": "By exchanging source and target sequences for the training, we can train a model that invents additional information for a sentence to generate a new sentence (Figure 7). We believe that this could prove useful to increase the linguistic diversity and complexity of AI unit tests such as the Facebook bAbI task [18], but leave this to future work."}, {"heading": "4 Conclusion and Future Work", "text": "We investigated the ability of sequence sequence models to generate sentences from a source set. To this end, we trained an attentive LSTM on pairs of sequence sequences in the SNLI corpus. We found that this works well and generalizes beyond indomain sentences. Therefore, it could become a useful component to improve the performance of other NLP systems. We were able to generate natural language sequences by recursively generating sentences from previously derived sentences, which enabled us to construct an outcome graph for sentences in the SNLI development corpus. In this graph, the common meaning of two related sentences is represented by the first natural language set connecting the two sentences. Each consequence step is interpretable as it maps one sentence from the natural language to another. In the direction of high-quality data augmentation, we experimented with reversing the generational task. We found that the model was able to find this in specific information."}, {"heading": "Acknowledgments", "text": "We would like to thank Guillaume Bouchard, who proposed the reverse generational task, and Dirk Weissenborn, Isabelle Augenstein and Matko Bosnjak for comments on the drafts of this work. This work was supported by Microsoft Research through its doctoral fellowship program, an Allen Distinguished Investigator Award and a Marie Curie Career Integration Award."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["S.R. Bowman", "J. Gauthier", "A. Rastogi", "R. Gupta", "C.D. Manning", "C. Potts"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long  short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": "In ICLR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Learning Natural Language Inference with LSTM", "author": ["S. Wang", "J. Jiang"], "venue": "In NAACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "In ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Although systems that perform well in RTE could potentially be used to improve question answering, information extraction, text summarization and machine translation [6], only in few of such downstream NLP tasks sentence-pairs are actually available.", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 185, "endOffset": 203}, {"referenceID": 3, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 185, "endOffset": 203}, {"referenceID": 11, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 185, "endOffset": 203}, {"referenceID": 14, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 185, "endOffset": 203}, {"referenceID": 16, "context": "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].", "startOffset": 185, "endOffset": 203}, {"referenceID": 13, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 156, "endOffset": 162}, {"referenceID": 4, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 156, "endOffset": 162}, {"referenceID": 15, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 185, "endOffset": 189}, {"referenceID": 12, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 214, "endOffset": 218}, {"referenceID": 6, "context": "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].", "startOffset": 242, "endOffset": 245}, {"referenceID": 7, "context": "Specifically, we use long short-term memory (LSTM) RNNs [8] for encoding and decoding.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "Furthermore, we experiment with wordby-word attention [1], which allows the decoder to search in the encoder outputs to circumvent the LSTM\u2019s memory bottleneck.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "We use stochastic gradient descent with a minibatch size of 64 and the ADAM optimizer [9] with a first momentum coefficient of 0.", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Word embeddings are initialized with pre-trained word2vec vectors [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "After training, we take the best model in terms of BLEU score [11] on the development set and calculate the BLEU score on the test set.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "complexity of AI unit tests such as the Facebook bAbI task [18], but we leave this for future work.", "startOffset": 59, "endOffset": 63}], "year": 2016, "abstractText": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.", "creator": "LaTeX with hyperref package"}}}