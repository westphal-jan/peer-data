{"id": "1606.01433", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "abstract": "We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time.", "histories": [["v1", "Sat, 4 Jun 2016 23:22:41 GMT  (97kb,D)", "http://arxiv.org/abs/1606.01433v1", "NAACL HLT 2016, SemEval-2016 Task 12 submission"]], "COMMENTS": "NAACL HLT 2016, SemEval-2016 Task 12 submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason alan fries"], "accepted": false, "id": "1606.01433"}, "pdf": {"name": "1606.01433.pdf", "metadata": {"source": "CRF", "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "authors": ["Jason Alan Fries"], "emails": ["jason-fries@stanford.edu"], "sections": [{"heading": null, "text": "We submitted two systems to SemEval2016 task 12: Clinical TempEval challenge, which participated in phase 1, where we identified spans of time and event expression in clinical notes, and phase 2, where we predicted a relationship between an event and the time a parent document was created. In extracting temporal units, we found that a common inference-based approach using structured prediction outperforms a recursive neural network that includes word embeddings trained on a variety of large clinical document sets. In the time relationship to document creation, we found that a combination of data canonization and remote monitoring rules to predict both events and time expressions improves classification, although the benefits are limited, probably due to the small scale of training data."}, {"heading": "1 Introduction", "text": "This paper discusses two information extraction systems for identifying temporal information in the clinical text that were submitted to SemEval-2016 Task 12: Clinical TempEval (Bethard et al., 2016). We participated in tasks from both phases: (1) Identification of spans of time and event mentions; and (2) Predicting relationships between clinical events and document creation time. Timestamps and the structured nature of electronic medical records (EMR) directly capture some aspects of time. In the clinical field, this is a key requirement for medical argumentation systems as well as longitudinal research on disease progression. While timestamps and the structured nature of electronic medical records (EMR) capture some aspects of time, a large amount of information about disease progression is found in the unstructured text component of EMR, where the temporal structure is less obvious. We examine a deep learning approach to sequencing a network using a vanilla recurrence system (the one best used in 2015)."}, {"heading": "2 Methods and Materials", "text": "It is a question of whether or not the solution he has negotiated is actually a solution. (...) It is a question of whether there is a solution or not. (...) It is a question of whether there is a solution or not. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a question of whether there is a solution. (...) It is a solution. (...) It is a solution. (...) It is a solution. (...) It is a solution. (...) It is a solution. (...) It is a solution. (... It is a solution."}, {"heading": "2.1.2 Word Embeddings", "text": "For the base models of RNN, all embedding parameters are randomly initialized in the range [-1.0, 1.0]. For all other word-based models, embedding vectors are initialized or pre-trained with parameters trained on different clinical corpora. Pre-training generally improves classification performance over random initialization and provides a mechanism to utilize large collections of unlabeled data for use in semi-supervised learning (Erhan et al., 2010). We create word embedding using two collections of clinical documents: the MIMIC III database, which contains 2.4M notes from intensive care patients at the Beth Israel Deaconess Medical Center (Saeed et al., 2011); and the corpus of the University of Iowa Hospitals and Clinics (UIHC), which contains 15 M of predominantly stationary notes (see Table 1). All word embedding in this document is created using word2vec (the model of 300, see the skipgram previously)."}, {"heading": "2.1.3 RNN Taggers", "text": "We train for three tasks in Phase 1: an age in which most people are able to acquire their identity, and all words will be able to acquire their identity in the form in which they are able to find themselves."}, {"heading": "2.2 DeepDive", "text": "DeepDive developers incorporate domain knowledge into applications by using a combination of remote supervision rules that use heuristics to generate noisy training examples, and inference rules that use factors to define relationships between random variables. This design pattern allows us to quickly encode domain knowledge into a probabilistic graphical model and draw common conclusions over a large space of random variables. For example, we want to capture the relationship between EVENT units and their next TIMEX3 mentions in the text, as this provides some information about when the EVENT occurred relative to the document's creation time. TIMEX3s lacks a DocRelTime class, but we can use a remote supervision rule to create a noisy label that we then use to predict adjacent EVENT labels directly. We also know that the set of all EVENT / IMEX3 variables should be shared with our patient discs, such as within this specific patient discrepancy section."}, {"heading": "2.2.1 TIMEX3 and EVENT Spans", "text": "Phase 1: Our baseline tagger consists of three sequential rules: logistic regression, conditional random fields (CRF), and skip chain CRF (Sutton and McCallum, 2006). In CRFs, factor edges link adjacent words in a linear chain structure and capture label dependencies between adjacent words. CRFs generalize this idea by including skip edges that can connect non-adjacent words, for example, we can link labels for all identical words in a particular sentence window. We use DeepDive's feature library ddlib to generate common textual features such as context windows and dictionary affiliation. We examined combinations of left-right windows of two adjacent and POS tags, lettercase, and entity dictionaries for all vocabularies identified by the baseline of the challenge, i.e. all phrases designated as true entities."}, {"heading": "2.2.2 Document Creation Time Relations", "text": "Phase 2: To predict the relationship between an event and the creation time of its parent document, we assign a DocRelTime random variable to each mention of TIMEX3 and EVENT. For EVENTs, these values are provided by the training data, for TIMEX3s, we have to calculate class names. Approximately 42% of the mentions of TIMEX3 are simple data (\"12 / 29 / 08,\" \"October 16,\" etc.) and can be naively canonized to a universal timestamp. This is done using regular expressions to identify common date patterns and heuristics in order to deal with incomplete datasets. \"The missing year in\" October 16, \"for example, can be filled in by using the closest date; if this is not available, we use the year of document creation. These mentions are then assigned to a class that uses the DocTime value of the parent document and any revision times of DIMEXT as current time recipients of WINDIMEXT while other DIMEXT references are occurring."}, {"heading": "3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Phase 1", "text": "Word tokenization performance was high, F1 = 0.993, while record limit recognition was lower at F1 = 0.938 (document micro-average F1 = 0.985). Tokenization errors were largely limited to splitting numbers and separated words (\"ex-smoker\" vs. \"ex-smoker\"), which has minimal impact on upstream sequence labeling. Sentence margin errors were largely missing terminal words, resulting in longer sentences, which is preferable to short, less informative sequences in terms of impact on RNN minibatches. Tables 2 and 3 contain results for all sequence labeling models. In TIMEX3 spans, the best RNN ensemble model performed poorly compared to the winning system (0.706 vs. 0.795). DeepDive performed 2-3 as well as the best system in 2015, but also lagged behind the top system (0.730 vs. 0.795)."}, {"heading": "3.2 Phase 2", "text": "Finally, Table 4 contains our DocRelTime relation extraction. Our simple remote monitoring rule leads to better performance than the median submission of the system, but also falls well short of the current state of the art."}, {"heading": "4 Discussion", "text": "The fact is that we are able to solve the problems mentioned, and we cannot solve them if we do not want to solve them, \"he said."}, {"heading": "Acknowledgments", "text": "This work was supported by the Mobilize Center, a center of excellence for Big Data to Knowledge (BD2K) supported by the U54EB020405 grant."}], "references": [{"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "James Pustejovsky", "author": ["Steven Bethard", "Leon Derczynski", "Guergana Savova", "Guergana Savova"], "venue": "and Marc Verhagen.", "citeRegEx": "Bethard et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "James Pustejovsky", "author": ["Steven Bethard", "Guergana Savova", "Wei-Te Chen", "Leon Derczynski"], "venue": "and Marc Verhagen.", "citeRegEx": "Bethard et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Association for Computational Linguistics", "author": ["San Diego", "California", "June"], "venue": null, "citeRegEx": "2016. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "2016. et al\\.", "year": 2016}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Pascal Vincent", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol"], "venue": "and Samy Bengio.", "citeRegEx": "Erhan et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling Words for Online Sexual Behavior Surveillance and Clinical Text Information Extraction", "author": ["Jason Fries"], "venue": "Ph.D. thesis,", "citeRegEx": "Fries.,? \\Q2015\\E", "shortCiteRegEx": "Fries.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Li Deng", "author": ["Gr\u00e9goire Mesnil", "Xiaodong He"], "venue": "and Yoshua Bengio.", "citeRegEx": "Mesnil et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Benjamin Moody", "author": ["Mohammed Saeed", "Mauricio Villarroel", "Andrew T Reisner", "Gari Clifford", "Li-Wei Lehman", "George Moody", "Thomas Heldt", "Tin H Kyaw"], "venue": "and Roger G Mark.", "citeRegEx": "Saeed et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Guergana Savova", "author": ["William F Styler IV", "Steven Bethard", "Sean Finan", "Martha Palmer", "Sameer Pradhan", "Piet C de Groen", "Brad Erickson", "Timothy Miller", "Chen Lin"], "venue": "et al.", "citeRegEx": "Styler IV et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to conditional random fields for relational learning. Introduction to statistical relational learning, pages 93\u2013128", "author": ["Sutton", "McCallum2006] Charles Sutton", "Andrew McCallum"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2006}, {"title": "Richard Socher", "author": ["Kai Sheng Tai"], "venue": "and Christopher D Manning.", "citeRegEx": "Tai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "DeepDive: A Data Management System for Automatic Knowledge Base Construction", "author": ["Ce Zhang"], "venue": "Ph.D. thesis,", "citeRegEx": "Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Zhang.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We submitted two systems to the SemEval2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time. For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data.", "creator": "LaTeX with hyperref package"}}}