{"id": "1606.01467", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks", "abstract": "In this paper, we show that by feeding the weights of a deep neural network (DNN) during training into a deep Q-network (DQN) as its states, this DQN can learn policies to accelerate the training of that DNN. The actions of the DQN modify different hyperparameters during training. Empirically, this acceleration leads to better generalization performance of the DNN.", "histories": [["v1", "Sun, 5 Jun 2016 06:42:56 GMT  (352kb,D)", "http://arxiv.org/abs/1606.01467v1", null], ["v2", "Wed, 8 Jun 2016 17:02:49 GMT  (380kb,D)", "http://arxiv.org/abs/1606.01467v2", null], ["v3", "Mon, 1 Aug 2016 14:17:18 GMT  (254kb,D)", "http://arxiv.org/abs/1606.01467v3", null], ["v4", "Sun, 16 Oct 2016 04:20:25 GMT  (42kb,D)", "http://arxiv.org/abs/1606.01467v4", null], ["v5", "Mon, 7 Nov 2016 05:27:02 GMT  (318kb,D)", "http://arxiv.org/abs/1606.01467v5", null], ["v6", "Fri, 11 Nov 2016 06:22:25 GMT  (313kb,D)", "http://arxiv.org/abs/1606.01467v6", null], ["v7", "Thu, 17 Nov 2016 06:16:24 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v7", "This paper has been withdrawn by the author due to a crucial error in the source-code (the epsilon configuration), which makes the results invalid"], ["v8", "Tue, 20 Jun 2017 10:28:34 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v8", "The DQN itself has too many hyperparameters"], ["v9", "Wed, 12 Jul 2017 12:29:29 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v9", "We choose to withdraw this paper. The DQN itself has too many hyperparameters, which makes it almost impossible to be applied to reasonably large datasets. In the later versions with SGDR experiments, the policies seem to be random actions only"], ["v10", "Thu, 13 Jul 2017 08:49:36 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v10", "We choose to withdraw this paper. The DQN itself has too many hyperparameters, which makes it almost impossible to be applied to reasonably large datasets. In the later versions (from v4) with SGDR experiments, it seems that the agent only performs random actions"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jie fu"], "accepted": false, "id": "1606.01467"}, "pdf": {"name": "1606.01467.pdf", "metadata": {"source": "META", "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks", "authors": ["Jie Fu", "Zichuan Lin", "Miao Liu", "Nicholas Leonard", "Jiashi Feng", "Tat-Seng Chua"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The motivation for this work is based on the evidence [4] that deep networks trained by a stochastic gradient method with fewer iterations have better generalization capability. A natural question arises: How can we speed up the formation of deep networks in practice? In this paper, we achieve this by setting two unusual hyperparameters for each iteration: mini-batch selection and learning rate planning. The contributions of this paper are as follows: \u2022 We propose a practical framework based on DQNs to improve the generalization performance of modern deep neural networks by accelerating their formation. \u2022 We demonstrate how to use a DQN to optimize the learning rates of a DNN during training."}, {"heading": "2 Related Work", "text": "The authors add a Bayesian linear regressor to the last hidden layer of a deep mesh, marginalizing only the output weights of the mesh, while a point estimate is used for the remaining parameters. Hyperparameters of this Pesudo-Bayesian mesh are optimized by a different Gaussian process based on Bayesian optimizers. Similar work is [3] where a DQN was proposed to control a hyperparameter. However, the states require a careful hand construction of characteristics and can only be used to adjust learning rates. Moreover, their method cannot handle stochastic environments and is therefore impracticable. As with mini-batch selection, the authors know in [10] how to select samples for a mini-batch. Their rank-based approach is based on the assumption that ranks (defined by entropy of training problems during training) show large wasted time based on the rule not on training."}, {"heading": "3 Deep Q-Networks (DQNs)", "text": "The goal of a reinforcement learning tool is to maximize its expected total reward by learning an optimal policy (mapping states to actions). At each step t, the agent observes a state st \u00b2 S (in fact, we set st = wt), selects an action at A and receives a reward rt + 1, according to the decision of the agent, he observes the next state st + 1. The temporal return t is defined by Rt + 1 = rt + 1 + \u2211 T \u2212 t (s, a): (S, A) \u2192 R measures the expected return after observing the state st + 1, where T is the end-time step. In this essay, an episode is defined as a whole training of the inner DNN loop from random weights to convergence. The action value function Q\u03c0 (s, a) \u2192 R measures the expected return after observing the state st and execution of an action at the border and the limit st: S \u2192 A (s, a) = [s, at the border |)."}, {"heading": "4 Generic DQN Design for Tuning Hyperparameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Weights as States", "text": "In this thesis, the states that are fed into a DQN are defined by the learned weights of a DNN during the training, which differs from [3], where state characteristics are created manually by the author. This only avoids the annoying feature engineering procedure, but also allows us to tune other types of hyperparameters in addition to the learning rate. As we will show later, the states are also useful, for example, for the selection of mini-batches. As the currently most powerful DNN type for supervised tasks are Convolutionary Neural Networks (CNNs), we focus on tuning the hyperparameters of CNNs. More precisely, the states are defined by the weights of these revolutionary layers during the training."}, {"heading": "4.2 Fixing Weights by Adding Regressors", "text": "Unfortunately, due to the global updating mechanism of retransmission, the positions of the filters are arbitrarily changed after each episode. To force the filters to appear at the same positions in each episode, we add a regressor to each level: Lregressor (Wcurrent, Wlast) = 12 \"Wcurrent - Wlast\" 2, with Wcurrent being the parameters of the current iteration and Wpast being the parameters copied from each episode after the convergence. By setting the weights of a CNN, we have two additional advantages: The first advantage is that CNN's repeated training processes can be accelerated. Obviously, the training paths of CNN training should not be significantly different from each episode if we only gradually change some hyperparameters. Therefore, this regressor will provide more information on CNN training from the previous episodes. The second by-product is that the number of states can be drastically reduced, which is crucial for its representation."}, {"heading": "4.3 Reward Function", "text": "The reward functions are defined to ensure that the DQN learns a strategy to find the optimal objective value in as few time steps as possible. We define the reward function similarly [3] as follows: r (f, s \u2032 t) = 1 f (s \u2032 t) \u2212 flower, f (s \u2032) > flower does \"(3), where flower is a predefined subliminal constant of training loss, s\" t the total weights of CNN during training and f the training loss function."}, {"heading": "5 DQN Actions for Specific Tasks", "text": "We need to define the appropriate measures to optimise learning rates and the selection of minibatches."}, {"heading": "5.1 Tuning Learning Rates", "text": "The formation of a DNN with n free parameters can be formulated as a problem of minimizing a function f: Rn \u2192 R. Following the tradition of [10, 5], we define a loss function for each training sample: Rn \u2192 R; the distribution of the training samples then leads to a distribution via function D, and the total function f that we want to optimize is the expectation of this distribution: f (w): = Emanuel D (w)]. (4) The usual procedure for optimizing f is the iterative adjustment of wt (the parameter vector in time step t) based on gradient information obtained on a minibatch of size b. More precisely, in each time step t and for a given weight step Rn, a minibatch {2 = 1} is selected to calculate the optimization rate of ft (wt), selecting the optimization rate of ft (wt) (wt), assuming the learning rate of minimum content (this work) = 1 (wt)."}, {"heading": "5.2 Tuning Mini-Batch Selection", "text": "The next natural question is how to efficiently and effectively compress the training data. Self-directed learning [8] has a similar taste to active learning, which selects a sample from which to learn each iteration, based on entropy during training. Active learning approaches differ in their sample selection criteria. Specifically, entropy is a type of low-threshold information in the sense that it changes rapidly during training. In this paper, we consider the information associated with labels as selection criteria that come closer to the curriculum [2, 7]. Specifically, DQN will decide which categories of training samples should be included in a mini-batch. Unlike [7], where they promote different examples within a mini-batch, we do not encode this foreknowledge hard."}, {"heading": "6 Experiments", "text": "In this section we show empirically how a DQN can accelerate a CNN and thus improve the testing accuracy of CNN. All experiments are performed on the MNIST dataset with 20,000 training samples and 10,000 test samples. This CNN consists of 2 sinuous layers, the first having 16 and the second having 256 filters, plus 2 fully connected layers with 256 and 128 neurons, respectively. All nonlinear functions are Tanh. The initial learning rate is set to 0.05 in all experiments."}, {"heading": "6.1 Tuning Learning Rates", "text": "We are only testing whether the DQN can plan useful learning rates at all, but we do not intend to compare it with other adaptive learning rates. Minibatch size is set to 10. After 20 episodes of training the DQN, the final test accuracy achieved by a CNN tuned to this DQN is 98.02%, whereas the test accuracy of CNN is 97%. In Figure 1, we can also find that the convergence rate of CNN tuned by a DQN is much faster than that of CNN."}, {"heading": "6.2 Tuning Mini-Batch Selection", "text": "Here, we test whether the selection of training samples makes the training algorithms converge faster. Minibatch size is set to 128, buffer size is 10, and number of epochs is 40. In fact, we tried 10 and 32 as minibatch size, but the improvements were not significant. However, increasing the minibatch size is more than a pedantic trick, as it would allow the parallel computing resources to be used more effectively [6]. As shown in Figure 2, the final test accuracy of a CNN with adaptive mini-batch selected by a DQN is 90.6 after training the DQN for 64 episodes, while the test accuracy of the CNN base model is 89.98. Figure 2 also shows that CNN with adaptive minibatch selection consistently performs better than the CNN base batch in each epoch."}, {"heading": "7 Conclusion and Discussion", "text": "We have shown how to use a DQN to optimize the learning rates of a DNN during training. We have also shown how to use a DQN to select mini-batches for a DNN during training. [14] It has been shown that lower-layer features are relatively easy to transfer, so we are interested in investigating the transferability of the strategies learned from DQNs. We would also jointly investigate the combined effects of tuning learning rates and mini-batch selection. Publishing an easy-to-use toolbox based on this paper is considered one of our future work."}, {"heading": "Acknowledgement", "text": "Jie Fu thanks Amazon Web Services for providing free GPUs. This work is also supported by the NUS-Tsinghua Extreme Search (NExT) project through the National Research Foundation, Singapore."}], "references": [{"title": "Curriculum learning with deep convolutional neural networks", "author": ["Vanya Avramova"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Using deep q-learning to control optimization hyperparameters", "author": ["Samantha Hansen"], "venue": "arXiv preprint arXiv:1602.04062,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Beyond convexity: Stochastic quasi-convex optimization", "author": ["Elad Hazan", "Kfir Levy", "Shai Shalev-Shwartz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Self-paced curriculum learning", "author": ["Lu Jiang", "Deyu Meng", "Qian Zhao", "Shiguang Shan", "Alexander G Hauptmann"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M Pawan Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Are all training examples equally valuable", "author": ["Agata Lapedriza", "Hamed Pirsiavash", "Zoya Bylinskii", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1311.6510,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online batch selection for faster training of neural networks", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": "arXiv preprint arXiv:1511.06343,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "No more pesky learning rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1206.1106,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "The motivation for this work is founded on the proof [4] that deep networks trained by a stochastic gradient method with fewer iterations have a better generalization ability.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "A pseudo-Bayesian neural network has been used to tune a few hyperparameters of another deep neural network in [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "For tuning learning rates, the most similar work is [3], where a DQN has been proposed to control one hyperparameter.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "As for mini-batch selection, the authors in [10] study how to choose samples for a mini-batch.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "Also based on the entropy of training samples, self-paced learning methods[7] are, however, usually coupled with the original optimization problems and, as far as we know, there is no work successfully applying self-paced learning to deep nets training [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Also based on the entropy of training samples, self-paced learning methods[7] are, however, usually coupled with the original optimization problems and, as far as we know, there is no work successfully applying self-paced learning to deep nets training [1].", "startOffset": 253, "endOffset": 256}, {"referenceID": 10, "context": "The DQNmethod [11] approximates the optimal Q-function with a DNN.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "This is different from [3], where state features are manually crafted by the author.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "Also because of this change1, different from [11], we only employ a standard MLP in the DQN.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "This is quite different from the states used by Atari games [11], where the global state representations are needed.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "We define the reward function, similar to [3], as:", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Following the tradition of [10, 5], we define a loss function \u03c8 : R \u2192 R for each training sample; the distribution of training samples then induces a distribution over function D, and the overall function f we aim to optimize is the expectation of this distribution:", "startOffset": 27, "endOffset": 34}, {"referenceID": 4, "context": "Following the tradition of [10, 5], we define a loss function \u03c8 : R \u2192 R for each training sample; the distribution of training samples then induces a distribution over function D, and the overall function f we aim to optimize is the expectation of this distribution:", "startOffset": 27, "endOffset": 34}, {"referenceID": 11, "context": "The SGD is not robust in that its performance is heavily dependent on how learning rates are tuned over time [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "It has been shown in [10] that not only the selection of the update of wt given \u2207ft(wt) is crucial, but the selection of the mini-batch {\u03c8b i=1} \u223c Db used to compute \u2207ft(wt) also greatly contributes to the overall performance.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Training samples are not equally valuable [9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "The next natural question is how can we squeeze the training data efficiently and effectively? Self-paced learning [8] has a similar flavor to active learning, which chooses a sample to learn from at each iteration, based on the entropy during training.", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "In this paper, we consider the information associate with labels as the selecting criteria, which is more close to curriculum learning [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "In this paper, we consider the information associate with labels as the selecting criteria, which is more close to curriculum learning [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "Different from [7], where they encourage diverse examples within a mini-batch, we do not hard-encode such prior knowledge.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "However, increasing the mini-batch size is more than a pedantic trick, as doing so could utilize the parallel computing resources more effectively [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "It has been shown that [14] lower layers features are relatively easy to transfer.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "In this paper, we show that by feeding the weights of a deep neural network (DNN) during training into a deep Q-network (DQN) as its states, this DQN can learn policies to accelerate the training of that DNN. The actions of the DQN modify different hyperparameters during training. Empirically, this acceleration leads to better generalization performance of the DNN.", "creator": "LaTeX with hyperref package"}}}