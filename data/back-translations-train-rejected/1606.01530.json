{"id": "1606.01530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Adaptive Submodular Ranking", "abstract": "We study a general adaptive ranking problem where an algorithm needs to perform a sequence of actions on a random user, drawn from a known distribution, so as to \"satisfy\" the user as early as possible. The satisfaction of each user is captured by an individual submodular function, where the user is said to be satisfied when the function value goes above some threshold. We obtain a logarithmic factor approximation algorithm for this adaptive ranking problem, which is the best possible. The adaptive ranking problem has many applications in active learning and ranking: it significantly generalizes previously-studied problems such as optimal decision trees, equivalence class determination, decision region determination and submodular cover. We also present some preliminary experimental results based on our algorithm.", "histories": [["v1", "Sun, 5 Jun 2016 16:19:58 GMT  (166kb,D)", "http://arxiv.org/abs/1606.01530v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["fatemeh navidi", "prabhanjan kambadur", "viswanath nagarajan"], "accepted": false, "id": "1606.01530"}, "pdf": {"name": "1606.01530.pdf", "metadata": {"source": "CRF", "title": "Adaptive Submodular Ranking", "authors": ["Fatemeh Navidi", "Prabhanjan Kambadur", "Viswanath Nagarajan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many tasks in machine learning can be presented as sequential decision-making processes that simultaneously represent many previously investigated problems as optimal decision-making structures. An algorithm is presented as a priority distribution D over input factors, and its goal is to satisfy the realized input factors. In each step, the algorithm selects an action that partially satisfies the problem and also provides feedback, depending on whether this feedback is then used to refine the distribution of i-capabilities for subsequent steps. An algorithm in this setting (also referred to as politics) is an adaptive sequence of activities. In addition, many different criteria for satisfying the realized input types can be modeled to cover a submodular function. Submodular functions are a very general class of set functions that have a certain convexity as concavity properties. [24] These functions are used to study modular utilities of the previously sought-for decision-making structures in this paper, submodular sutilities of the social networks, in the optimization of decision-making trees."}, {"heading": "2 The Algorithm", "text": "We remember that an instance of the ASR consists of a floor group of elements with costs which we cannot cover. (D) We remember that an instance of the ASR consists of a floor group of elements with costs which we cannot cover. (D) We assume that the desired size of elements in the desired size of 1 to 2 percent of the desired size of 1 percent of the desired size of 1 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum of 1 percent of the desired sum of 2 percent of the desired sum."}, {"heading": "3 Extensions", "text": "Our algorithm can be easily generalized to handle arbitrary (non-binary) feedbacks as well. As before, we have elements U with cost {ce} e-U and m scenarios with probabilities {pi} mi = 1 (the realized scenario i * = i with probability pi). Each element e * U has an initially unknown state. If an element e is represented under scenario i, it provides its own state (e) as feedback. Each scenario specifies a state for each element in U, i.e. we get values {di, e: e \u00b2 U, i [m]} where di, e is the state of the element e under scenario i. In the binary case considered in section 2, | = 2 (corresponding to yes / no) and each scenario i specifies a \"yes\" state for the elements e \u00b2 -Si and a \"no\" state for the elements e \u00b2 -Si."}, {"heading": "4 Applications", "text": "This year, it's gotten to the point where it's going to be able to put itself at the top, in the way it has been in the past, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said."}, {"heading": "5 Experiments", "text": "In this section we present experimental results for the Adaptive Multiple Intent Ranking Problem (MIR) and the (generalized) Optimal Decision Tree Problem (ODT). We use expected cost (number of elements) as a performance measure to compare different algorithms for the MIR and ODT problems. For example, in the MIR case, if I am satisfied after looking at the k elements, we say that costi * = k; the performance indicator is then [i \u2212 H pi \u00b7 costi.Environment: We developed high-quality Python modules to evaluate the performance of our algorithms against their known counterparts for both ODT and MIR. [Our experimental machine has 40 Intel R \u2212 Xeon R \u2212 E5 \u2212 2660 cores running at 2.6 Ghz, with 396 GB of RAM running Linux 2.6.32 Kernel. We used the Python 2.7.10 Interperter to conduct our experiments."}, {"heading": "5.1 Optimal Decision Trees", "text": "In our experiments, we compare and compare the results of 3 algorithms that use different objectives to select elements: (a) ODT-adsub that uses the goal described in (3), (b) ODT-greedy, (b) ODT-adsub that uses the goal described in (3), (3) ODT-greedy that uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \"machine learning\" algorithm that works as follows. ODT-ml-ml parameterized by K-Date-Means [2] to represent a primary partition U in K-Cluster. Each cluster cj, j, j [1, K] is initially weighted wcj = 1. To select the next element e-U-E, a cluster j that is not uniformly wj-element is selected."}, {"heading": "5.2 Adaptive Multiple Intent Ranking", "text": "In our experiments, we compare and compare the results of 4 algorithms that use different methods to select elements: (a) MIR-adsub, described in (3), (b) MIR-static, which classifies elements statically using [4] and selects elements in order, (c) MIR-adstatic, which improves MIR-static by removing elements from the static list if they belong to invalid scenarios, and (d) MIR-ml, an \"machine learning\" algorithm that uses the multiplicative scheme described in Section 5.1.Results of executing MIR-adsub, MIR-static, MIR-adstatic, and MIR-ml based on invalid scenarios, an algorithm of the opposite ML-100 that uses Kos. The table in the left panel shows the expected costs when the MIR-static, MIR-static, MIR-static, MIR-static, MIR-adatic, and MIR-adml values are set to three."}], "references": [{"title": "Approximating optimal binary decision trees", "author": ["M. Adler", "B. Heeringa"], "venue": "Algorithmica, 62(3-4):1112\u20131121", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Ranking with submodular valuations", "author": ["Y. Azar", "I. Gamzu"], "venue": "SODA, pages 1070\u20131079", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple intents re-ranking", "author": ["Y. Azar", "I. Gamzu", "X. Yin"], "venue": "STOC, pages 669\u2013678", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A constant factor approximation algorithm for generalized min-sum set cover", "author": ["N. Bansal", "A. Gupta", "R. Krishnaswamy"], "venue": "SODA, pages 1539\u20131545", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "When LP is the cure for your matching woes: Improved bounds for stochastic matchings", "author": ["N. Bansal", "A. Gupta", "J. Li", "J. Mestre", "V. Nagarajan", "A. Rudra"], "venue": "Algorithmica, 63(4):733\u2013762", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Group-based active query selection for rapid diagnosis in time-critical situations", "author": ["G. Bellala", "S.K. Bhavnani", "C. Scott"], "venue": "IEEE Trans. Information Theory, 58(1):459\u2013478", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Decision trees for entity identification: Approximation algorithms and hardness results", "author": ["V.T. Chakaravarthy", "V. Pandit", "S. Roy", "P. Awasthi", "M.K. Mohania"], "venue": "ACM Transactions on Algorithms, 7(2):15", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost", "author": ["F. Cicalese", "E.S. Laber", "A.M. Saettler"], "venue": "ICML, pages 414\u2013422", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximating the stochastic knapsack problem: The benefit of adaptivity", "author": ["B.C. Dean", "M.X. Goemans", "J. Vondr\u00e1k"], "venue": "Math. Oper. Res., 33(4):945\u2013964", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A threshold of ln n for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM, 45(4):634\u2013652", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "J. Artif. Intell. Res. (JAIR), 42:427\u2013486", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "NIPS, pages 766\u2013774", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Scenario submodular cover", "author": ["N. Grammel", "L. Hellerstein", "D. Kletenik", "P. Lin"], "venue": "CoRR, abs/1603.03158", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Average-Case Active Learning with Costs", "author": ["A. Guillory", "J. Bilmes"], "venue": "Algorithmic Learning Theory, pages 141\u2013155. Springer Berlin / Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous learning and covering with adversarial noise", "author": ["A. Guillory", "J. Bilmes"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 369\u2013376", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation algorithms for optimal decision trees and adaptive tsp problems", "author": ["A. Gupta", "V. Nagarajan", "R. Ravi"], "venue": "ICALP (1), pages 690\u2013701", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Constructing optimal binary decision trees is NP -complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Lett.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1976}, {"title": "and R", "author": ["S. Im", "V. Nagarajan"], "venue": "van der Zwaan. Minimum latency submodular cover. ICALP, pages 485\u2013497", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Near optimal bayesian active learning for decision making", "author": ["Sh. Javdani", "Y. Chen", "A. Karbasi", "A. Krause", "D. Bagnell", "S.S. Srinivasa"], "venue": "In AISTATS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "On an Optimal Split Tree Problem", "author": ["S.R. Kosaraju", "T.M. Przytycka", "R.S. Borgstrom"], "venue": "Proceedings of the 6th International Workshop on Algorithms and Data Structures, pages 157\u2013168", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Combinatorial optimization: polyhedra and efficiency", "author": ["A. Schrijver"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "A note on the generalized min-sum set cover problem", "author": ["M. Skutella", "D.P. Williamson"], "venue": "Oper. Res. Lett., 39(6):433\u2013436", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "author": ["L.A. Wolsey"], "venue": "Combinatorica, 2(4):385\u2013393", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1982}], "referenceMentions": [{"referenceID": 23, "context": "Submodular functions are a very general class of set-functions that have certain convexity as well as concavity properties [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 22, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 9, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 7, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 17, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 8, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 185, "endOffset": 207}, {"referenceID": 13, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 241, "endOffset": 248}, {"referenceID": 6, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 241, "endOffset": 248}, {"referenceID": 21, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 280, "endOffset": 284}, {"referenceID": 2, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 308, "endOffset": 315}, {"referenceID": 20, "context": "In this paper, we study an adaptive optimization problem in the setting described above which simultaneously generalizes many previously-studied problems such as optimal decision trees [20, 23, 10, 8, 18, 9], equivalence class determination [14, 7], decision region determination [22] and submodular ranking [3, 21].", "startOffset": 308, "endOffset": 315}, {"referenceID": 3, "context": "This is an adaptive version of the search ranking problem studied in [4, 5, 25].", "startOffset": 69, "endOffset": 79}, {"referenceID": 4, "context": "This is an adaptive version of the search ranking problem studied in [4, 5, 25].", "startOffset": 69, "endOffset": 79}, {"referenceID": 24, "context": "This is an adaptive version of the search ranking problem studied in [4, 5, 25].", "startOffset": 69, "endOffset": 79}, {"referenceID": 10, "context": "[11, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "[11, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "See [24] for background.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Each scenario i \u2208 [m] := {1, \u00b7 \u00b7 \u00b7 ,m} is specified by an interest-set Si \u2286 U and a normalized monotone submodular function fi : 2Si \u2192 [0, 1] where fi(\u2205) = 0 and fi(Si) = 1 (every monotone submodular function on Si can be expressed in this form by scaling and truncation).", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "For notational simplicity, we extend each function fi : 2 Si \u2192 [0, 1] to arbitrary subsets S \u2286 U by setting fi(S) = fi(S \u2229 Si).", "startOffset": 63, "endOffset": 69}, {"referenceID": 25, "context": "[26, 3, 21, 15].", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "[26, 3, 21, 15].", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "[26, 3, 21, 15].", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "[26, 3, 21, 15].", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "Assuming P 6= NP , this result is the best possible (up to constant factors) as the set cover problem [12] is a special case of ASR even when m = 1.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 22, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 9, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 0, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 7, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 15, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 17, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 12, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 8, "context": "Such a simple algorithm was previously unknown even in the special case of optimal decision tree (under arbitrary costs/probabilities), despite a large number of papers [20, 23, 10, 1, 8, 16, 18, 13, 9] on this topic.", "startOffset": 169, "endOffset": 202}, {"referenceID": 17, "context": "The first O(logm)-approximation algorithm for ODT was obtained in [18], and this result was extended to the equivalence class determination problem in [9].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "The first O(logm)-approximation algorithm for ODT was obtained in [18], and this result was extended to the equivalence class determination problem in [9].", "startOffset": 151, "endOffset": 154}, {"referenceID": 22, "context": "[23, 10, 1, 8, 16], based on a simple greedy \u201csplitting\u201d algorithm, had a logarithmic dependence on either costs or probabilities which (in the worst case) can be exponential in m.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "[23, 10, 1, 8, 16], based on a simple greedy \u201csplitting\u201d algorithm, had a logarithmic dependence on either costs or probabilities which (in the worst case) can be exponential in m.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "[23, 10, 1, 8, 16], based on a simple greedy \u201csplitting\u201d algorithm, had a logarithmic dependence on either costs or probabilities which (in the worst case) can be exponential in m.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "[23, 10, 1, 8, 16], based on a simple greedy \u201csplitting\u201d algorithm, had a logarithmic dependence on either costs or probabilities which (in the worst case) can be exponential in m.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "[23, 10, 1, 8, 16], based on a simple greedy \u201csplitting\u201d algorithm, had a logarithmic dependence on either costs or probabilities which (in the worst case) can be exponential in m.", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "One line of work is on the submodular cover problem and its variants [26, 4, 3, 21].", "startOffset": 69, "endOffset": 83}, {"referenceID": 3, "context": "One line of work is on the submodular cover problem and its variants [26, 4, 3, 21].", "startOffset": 69, "endOffset": 83}, {"referenceID": 2, "context": "One line of work is on the submodular cover problem and its variants [26, 4, 3, 21].", "startOffset": 69, "endOffset": 83}, {"referenceID": 20, "context": "One line of work is on the submodular cover problem and its variants [26, 4, 3, 21].", "startOffset": 69, "endOffset": 83}, {"referenceID": 22, "context": "[23, 10, 18, 13, 9].", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "[23, 10, 18, 13, 9].", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "[23, 10, 18, 13, 9].", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "[23, 10, 18, 13, 9].", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "[23, 10, 18, 13, 9].", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "In particular, we combine the time-based analysis for the deterministic submodular ranking problem in [21] with the phase-based analysis for optimal decision tree in [23, 18, 9].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "In particular, we combine the time-based analysis for the deterministic submodular ranking problem in [21] with the phase-based analysis for optimal decision tree in [23, 18, 9].", "startOffset": 166, "endOffset": 177}, {"referenceID": 17, "context": "In particular, we combine the time-based analysis for the deterministic submodular ranking problem in [21] with the phase-based analysis for optimal decision tree in [23, 18, 9].", "startOffset": 166, "endOffset": 177}, {"referenceID": 8, "context": "In particular, we combine the time-based analysis for the deterministic submodular ranking problem in [21] with the phase-based analysis for optimal decision tree in [23, 18, 9].", "startOffset": 166, "endOffset": 177}, {"referenceID": 20, "context": "We note that [21] also considers a stochastic variant of submodular ranking, but it is different from ASR because [21] assumes an independent distribution whereas we assume a correlated scenariobased distribution.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "We note that [21] also considers a stochastic variant of submodular ranking, but it is different from ASR because [21] assumes an independent distribution whereas we assume a correlated scenariobased distribution.", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "In particular, unlike ASR, the stochastic submodular ranking problem in [21] does not capture the optimal decision tree problem and its variants.", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Recently, [15] also considered a scenario-based submodular cover problem.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "In this respect our setting is a generalization of [15], eg.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "ASR captures the submodular ranking problem [3] whereas [15] does not.", "startOffset": 44, "endOffset": 47}, {"referenceID": 14, "context": "ASR captures the submodular ranking problem [3] whereas [15] does not.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "On the other hand, [15] allows arbitrary feedback whereas ASR as defined only considers binary (yes/no) feedback.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "The notion of \u201cadaptive submodularity\u201d [13] has been very useful in obtaining algorithms for some previously-studied special cases [14, 7, 22] of ASR.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The notion of \u201cadaptive submodularity\u201d [13] has been very useful in obtaining algorithms for some previously-studied special cases [14, 7, 22] of ASR.", "startOffset": 131, "endOffset": 142}, {"referenceID": 6, "context": "The notion of \u201cadaptive submodularity\u201d [13] has been very useful in obtaining algorithms for some previously-studied special cases [14, 7, 22] of ASR.", "startOffset": 131, "endOffset": 142}, {"referenceID": 21, "context": "The notion of \u201cadaptive submodularity\u201d [13] has been very useful in obtaining algorithms for some previously-studied special cases [14, 7, 22] of ASR.", "startOffset": 131, "endOffset": 142}, {"referenceID": 12, "context": "In particular, among other results [13] obtained an O(log 1/ + log 1/pmin)-approximation algorithm for ASR when fi = f for all scenarios i, the function f is adaptive-submodular (a stronger condition than submodular) and pmin = min m i=1 pi is the minimum probability.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "We note that [15] is also an improvement over [13] in points (ii) and (iii) above.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "We note that [15] is also an improvement over [13] in points (ii) and (iii) above.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "Recall that an instance of ASR consists of a ground set U of elements with costs {ce}e\u2208U , and m scenarios with an interest-set Si \u2286 U , submodular function fi : 2Si \u2192 [0, 1] and probability pi associated with each scenario i \u2208 [m].", "startOffset": 168, "endOffset": 174}, {"referenceID": 2, "context": "1 in [3] which relies on the definition of in (1).", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Each scenario i \u2208 [m] is also associated with a function fi defined over subsets of U \u00d7 \u0393; we again assume that fi is monotone submodular and takes values in [0, 1].", "startOffset": 158, "endOffset": 164}, {"referenceID": 2, "context": "Moreover, by observing that in (3) Le = \u2205 always, we obtain an O(log 1 )-approximation, which matches the best result known [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 17, "context": "This matches the best result known in [18], but their algorithm is more complicated than ours.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "This is equal to the best previous result [9], but again our algorithm is much simpler.", "startOffset": 42, "endOffset": 45}, {"referenceID": 16, "context": "In [17] it is shown that the \u201cOR of submodular functions\u201d is submodular.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] obtained an O(min{r, d} \u00b7 ln 1 mini pi )-approximation algorithm for this problem, which was improved by [15] to O(min{r, d} \u00b7 logm); here d is the maximum size of a decision region.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[22] obtained an O(min{r, d} \u00b7 ln 1 mini pi )-approximation algorithm for this problem, which was improved by [15] to O(min{r, d} \u00b7 logm); here d is the maximum size of a decision region.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Our algorithm runs in time polynomial in m and r, unlike [22, 15] which required time exponential in r.", "startOffset": 57, "endOffset": 65}, {"referenceID": 14, "context": "Our algorithm runs in time polynomial in m and r, unlike [22, 15] which required time exponential in r.", "startOffset": 57, "endOffset": 65}, {"referenceID": 21, "context": "As in [22, 15], we can also obtain an O(d logm)-approximation with running time exponential in d.", "startOffset": 6, "endOffset": 14}, {"referenceID": 14, "context": "As in [22, 15], we can also obtain an O(d logm)-approximation with running time exponential in d.", "startOffset": 6, "endOffset": 14}, {"referenceID": 18, "context": "Datasets: The real-world dataset used in our experiments \u2014 called ML-100 \u2014 is the 100K example from the MovieLens [19] repository, which contains 100,000 ratings on a scale of [1,5] from 943 users", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "Datasets: The real-world dataset used in our experiments \u2014 called ML-100 \u2014 is the 100K example from the MovieLens [19] repository, which contains 100,000 ratings on a scale of [1,5] from 943 users", "startOffset": 176, "endOffset": 181}, {"referenceID": 4, "context": "Datasets: The real-world dataset used in our experiments \u2014 called ML-100 \u2014 is the 100K example from the MovieLens [19] repository, which contains 100,000 ratings on a scale of [1,5] from 943 users", "startOffset": 176, "endOffset": 181}, {"referenceID": 22, "context": "For the ODT problem, we also use a synthetic dataset \u2014 SYN-K \u2014 that is parameterized by k; this is based on a hard instance for the greedy algorithm [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 22, "context": "In our experiments, we compare and contrast the results of 3 algorithms that use different objectives to choose elements: (a) ODT-adsub, which uses the objective described in (3), (b) ODT-greedy, which uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \u201cmachine learning\u201d algorithm that operates as follows.", "startOffset": 236, "endOffset": 254}, {"referenceID": 9, "context": "In our experiments, we compare and contrast the results of 3 algorithms that use different objectives to choose elements: (a) ODT-adsub, which uses the objective described in (3), (b) ODT-greedy, which uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \u201cmachine learning\u201d algorithm that operates as follows.", "startOffset": 236, "endOffset": 254}, {"referenceID": 0, "context": "In our experiments, we compare and contrast the results of 3 algorithms that use different objectives to choose elements: (a) ODT-adsub, which uses the objective described in (3), (b) ODT-greedy, which uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \u201cmachine learning\u201d algorithm that operates as follows.", "startOffset": 236, "endOffset": 254}, {"referenceID": 7, "context": "In our experiments, we compare and contrast the results of 3 algorithms that use different objectives to choose elements: (a) ODT-adsub, which uses the objective described in (3), (b) ODT-greedy, which uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \u201cmachine learning\u201d algorithm that operates as follows.", "startOffset": 236, "endOffset": 254}, {"referenceID": 15, "context": "In our experiments, we compare and contrast the results of 3 algorithms that use different objectives to choose elements: (a) ODT-adsub, which uses the objective described in (3), (b) ODT-greedy, which uses the classic greedy objective [23, 10, 1, 8, 16], and (c) ODT-ml, a \u201cmachine learning\u201d algorithm that operates as follows.", "startOffset": 236, "endOffset": 254}, {"referenceID": 1, "context": "ODT-ml, which is parameterized by K uses k-Means [2] to a priori partition U into K clusters.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "In our experiments, we compare and contrast the results of 4 algorithms that use different methods to chose elements: (a) MIR-adsub, which is described in (3), (b) MIR-static, which statically ranks the elements using [4] and choses elements in rank order, (c) MIR-adstatic, which improves on MIR-static by using feedback to eliminate elements from the static list if they belong to invalid scenarios, and (d) MIR-ml, a \u201cmachine learning\u201d algorithm that uses the multiplicative scheme described in Section 5.", "startOffset": 218, "endOffset": 221}, {"referenceID": 14, "context": "Acknowledgement: We thank Lisa Hellerstein for a clarification on [15] regarding the OR construction of submodular functions.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "We study a general adaptive ranking problem where an algorithm needs to perform a sequence of actions on a random user, drawn from a known distribution, so as to \u201csatisfy\u201d the user as early as possible. The satisfaction of each user is captured by an individual submodular function, where the user is said to be satisfied when the function value goes above some threshold. We obtain a logarithmic factor approximation algorithm for this adaptive ranking problem, which is the best possible. The adaptive ranking problem has many applications in active learning and ranking: it significantly generalizes previously-studied problems such as optimal decision trees, equivalence class determination, decision region determination and submodular cover. We also present some preliminary experimental results based on our algorithm.", "creator": "LaTeX with hyperref package"}}}