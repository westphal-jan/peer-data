{"id": "1606.01540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "OpenAI Gym", "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.", "histories": [["v1", "Sun, 5 Jun 2016 17:54:48 GMT  (546kb,D)", "http://arxiv.org/abs/1606.01540v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["greg brockman", "vicki cheung", "ludwig pettersson", "jonas schneider", "john schulman", "jie tang", "wojciech zaremba"], "accepted": false, "id": "1606.01540"}, "pdf": {"name": "1606.01540.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["OpenAI Gym", "Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement Learning (RL) is the branch of machine learning that deals with decision-making. RL has a rich mathematical theory and has found a variety of practical applications [1]. Recent advances that combine in-depth learning with Reinforcement Learning have led to great enthusiasm in the field, as it has been shown that general algorithms such as Policy Gradients and Q-Learning can perform well on difficult problems without problem-specific engineering [2, 3, 4]. To build on recent advances in Reinforcement Learning, the research community needs good benchmarks to compare algorithms. [6] A variety of benchmarks have been published, such as the Arcade Learning Environment (ALE) [5], which has exposed a collection of Atari 2600 games to amplify learning problems, and more recently the RLLab Benchmark for Continuous Control [5], to which we refer the reader for a survey of other RL benchmarks of these benchmarks, which is a common benchmark [7, including 7, 11, which is an AGB for OpenAGM]."}, {"heading": "2 Background", "text": "Reinforcement learning assumes that there is an agent who is in an environment. Each step leads the agent to an action, and he receives an observation and reward from the environment. An RL algorithm seeks to maximize a certain level of the agent's overall reward as the agent interacts with the environment. In the RL literature, the environment is formalized as a partially observable Markov decision-making process (POMDP) [12].OpenAI Gym focuses on the episodic setting of Reinforcement Learning, in which the agent's experience is divided into a series of episodes. In each episode, the initial state of the agent is randomly sampled from a distribution and the interaction continues until the environment reaches a final state. The goal of episodic Reinforcement Learning is to maximize the expectation of the total reward per episode and achieve a high level of performance in as few episodes per episode as possible 100 points."}, {"heading": "3 Design Decisions", "text": "In fact, it is the case that you will be able to follow the rules that you have set yourself in order to put them into practice."}, {"heading": "4 Environments", "text": "OpenAI Gym contains a collection of environments (POMDPs) that will grow over time. See Figure 1 for examples. At the time of the first beta release of Gym, the following environments were included: \u2022 classic control and toy text: small tasks from the RL literature. \u2022 Algorithm: Calculation such as adding multi-digit numbers and inverting sequences. Most of these tasks require memory and their difficulty can be selected by varying the sequence length. \u2022 Atari: classic Atari games with screen images or RAM as input using the Arcade Learning Environment [5]. \u2022 Board games: Currently, we have included the game Go on 9x9 and 19x19 boards, where the Pachi engine [13] serves as the opponent. \u2022 2D and 3D robots: controlling a robot in the simulation. These tasks use the MuJoCo physics engine, which was designed for a quick and precise simulation based on the original R2-D [starting from game 14]."}, {"heading": "5 Future Directions", "text": "In the future, we hope to expand OpenAI Gym in several ways. \u2022 Multi-Agent Setting. It will be interesting to eventually take up tasks where agents must collaborate or compete with other agents. \u2022 Curriculum and transfer learning. For now, the tasks are to be solved from the ground up. Later, it will be more interesting to look at task sequences so that the algorithm is trained one task at a time. Here, we will create sequences of increasingly difficult tasks to solve one after the other. \u2022 Operation in the real world. At some point, we want to integrate the Gym API with robotic hardware to validate amplification learning algorithms in the real world."}], "references": [{"title": "Dynamic programming and optimal control", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Sadik Beattie", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen"], "venue": "C., Antonoglou A., H. I., King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res., 47:253\u2013279", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "RLPy: A value-function-based reinforcement learning framework for education and research", "author": ["A. Geramifard", "C. Dann", "R.H. Klein", "W. Dabney", "J.P. How"], "venue": "J. Mach. Learn. Res., 16:1573\u20131578", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "RL-Glue: Language-independent software for reinforcement-learning experiments", "author": ["B. Tanner", "A. White"], "venue": "J. Mach. Learn. Res., 10:2133\u20132136", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "PyBrain", "author": ["T. Schaul", "J. Bayer", "D. Wierstra", "Y. Sun", "M. Felder", "F. Sehnke", "T. R\u00fcckstie\u00df", "J. Schmidhuber"], "venue": "J. Mach. Learn. Res., 11:743\u2013746", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "RLLib: Lightweight standard and on/off policy reinforcement learning library (C++)", "author": ["S. Abeyruwan"], "venue": "http://web.cs.miami.edu/home/saminda/rilib.html", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The reinforcement learning competition", "author": ["Christos Dimitrakakis", "Guangliang Li", "Nikoalos Tziortziotis"], "venue": "AI Magazine,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Pachi: State of the art open source go program", "author": ["Petr Baudi\u0161", "Jean-loup Gailly"], "venue": "In Advances in Computer Games,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": "arXiv preprint arXiv:1605.02097,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "RL has a rich mathematical theory and has found a variety of practical applications [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 2, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 3, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 4, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 234, "endOffset": 237}, {"referenceID": 6, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 7, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 8, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 9, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 10, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 11, "context": "In the RL literature, the environment is formalized as a partially observable Markov decision process (POMDP) [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "\u2022 Atari: classic Atari games, with screen images or RAM as input, using the Arcade Learning Environment [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "\u2022 Board games: currently, we have included the game of Go on 9x9 and 19x19 boards, where the Pachi engine [13] serves as an opponent.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "A few of the tasks are adapted from RLLab [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "Since the initial release, more environments have been created, including ones based on the open source physics engine Box2D or the Doom game engine via VizDoom [15].", "startOffset": 161, "endOffset": 165}], "year": 2016, "abstractText": "OpenAI Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.", "creator": "LaTeX with hyperref package"}}}