{"id": "1606.01568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Active Regression with Adaptive Huber Loss", "abstract": "This paper addresses the scalar regression problem presenting a solution for optimizing the Huber loss in a general semi-supervised setting, which combines multi-view learning and manifold regularization. To this aim, we propose a principled algorithm to 1) avoid computationally expensive iterative solutions while 2) adapting the Huber loss threshold in a data-driven fashion and 3) actively balancing the use of labelled data to remove noisy or inconsistent annotations from the training stage. In a wide experimental evaluation, dealing with diverse applications, we assess the superiority of our paradigm which is able to combine strong performance and robustness to noise at a low computational cost.", "histories": [["v1", "Sun, 5 Jun 2016 21:59:34 GMT  (4477kb,D)", "http://arxiv.org/abs/1606.01568v1", null], ["v2", "Sun, 26 Jun 2016 07:24:43 GMT  (4476kb,D)", "http://arxiv.org/abs/1606.01568v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jacopo cavazza", "vittorio murino"], "accepted": false, "id": "1606.01568"}, "pdf": {"name": "1606.01568.pdf", "metadata": {"source": "CRF", "title": "Active-Labelling by Adaptive Huber Loss Regression", "authors": ["Jacopo Cavazza", "Vittorio Murino"], "emails": ["jacopo.cavazza@iit.it."], "sections": [{"heading": null, "text": "In fact, it is so. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. (... It is. It is. (...) It is. It is. It is. (...) It is. It is. (... It is. It is. It is. It is. (...) It is. It is. It is. (... It is. It is. It is. It is."}, {"heading": "II. MULTI-VIEW SCALAR REGRESSION", "text": "s specify the function x = x. Strictly speaking, we assume that for each x-X model x = [x1,... xm and x\u03b1 we belong to the subspace X\u03b1, for each x = 1,.., m. This is a very natural way to model high-dimensional data: x is the concatenation of x1,., xm, each representing a specific class of characteristics, i.e. one of several views [1], [2], [10] in which data can be detected x-dimensional data. To find the regression map, we assume that it belongs to a hypotheses space H of functions h: X \u2192 R, the construction of which is examined below. For each x,."}, {"heading": "III. HUBER LOSS-BASED REGRESSION", "text": "Once the Space H hypothesis is defined in Section II, we can perform an optimization based on empirical knowledge for the regression map. For this purpose, we consider a training set D consisting of \"designated instances (x1, y1),.. (x,\" y \"),\" X \u00b7 R \"and\" u additional blank inputs x \"+ 1,.., x\" + u \u00b2 X. \"Under the class of functions (2), similar to [3], [10], the regression map is found by looking at the following objective function inputs x\" and u additional blank inputs x \"+ 1.\" (f), \""}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 3", "text": "In this sense, we will develop an adaptive pipeline to learn from it (see Section IV). For all these reasons, we will focus on the positive aspect of both losses while remarkably mitigating their weaknesses. Of course, Horizon suffers from the issue of setting the threshold and, to this end, we will develop an adaptive pipeline to learn from it (see Section IV). To optimize our goal (3), we can use the theory of RKHS to optimize SK by using the Reposition Theorem [4]."}, {"heading": "IV. THE HLR ALGORITHM", "text": "D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 5", "text": "In fact, in which he is able to be in a position to be in the position he is in and in which he sees himself in a position to be in, in a position in which he is able to be in, in which he is able to be in a position to be in, in which he is able to be in a position to be in, in which he is in a position to be in, in which he is able to be in a position to be in a position to be in."}, {"heading": "V. EMPIRICAL VALIDATION OF HLR", "text": "In Section V-A, we review the effectiveness of the proposed solution for Huber loss and the adaptation criterion to learn automatically from the data. In Section V-B, the active labeling components of HLR are benchmarked in terms of curve fit and learning from loud labeling problems. Section V-C compares HLR with state-of-the-art regression methods for classic machine learning data sets. Finally, in Section V-D, we look at the application of crowd count and compare our method with the most effective in the literature through multiple experiments."}, {"heading": "A. Comparison with state-of-the-art convex solver", "text": "As emphasized in the introduction and demonstrated in Section IV, HLR uses an exact solution for optimizing the Huber loss as a diametric counterpoint to the iterative solution. In order to test experimentally the performance of such an aspect, we compare against CVX [14], which is a state-of-the-art optimization tool for convex problems. Precisely by using either HLR or CVX, we are able to functionally optimize the same goal (3) to investigate which of the two methods is more efficient in terms of both performance and runtime. At the same time, we are able to inspect the automatic adaptation pipeline in a data-driven manner while comparing it to a classical cross-validation. To achieve these goals, we face a classic linear regression problem, caused by the underlying model y > x, where x, y R and \u03b2. \""}, {"heading": "B. Evaluation of the active-labelling component", "text": "In this section, we evaluate the robustness that the HLR active labeling component effectively provides to us, based on data we have removed due to the use of Huber loss. To this end, we either consider a curve fit as an example, or we also address the problem of binary classification in a corrupt data regime. It is a pretty meaningful change, since each entry of x is uniformly distributed in [0, 1], so it is not negative. Consequently, our algorithm should be able to detect the negative data as clear outliers and automatically remove them from the training set. Such evaluation is done by Table II, where we report different noise ratios while measuring the reconstruction error of whether the HLR labels actually refer to corrupted inputs."}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C. HLR for classical regression applications", "text": "To compare the effectiveness of HLR in the learning environment, we need to focus on four sets of data from the UCI Machine Learning Repository5, which we are able to analyze the actual results. (Indeed), physical simulations (AirFoil Self-Noise - Air and Yatch Hydrodynamics - Idro) and agronomic quality control (Wine). The datasets consist of 506 examples and 13 feature components that are either discrete (average number of rooms) or not. (Whether we track Charles River or not) or continuous (pupil-teacher ratio by city). It is provided by NASA and shows 1503 aerodynamic and three-dimensional acquisitions of two and three-dimensional air blade sections."}, {"heading": "D. HLR for crowd counting application", "text": "As the final test bed of our proposed framework, we will look at the Crowd Countdown application, which is to estimate the number of people in a real environment using video data. Crowd Countdown can be reformulated by learning a regression map from frame-specific features to the number of people using [20]. Three benchmark data sets were used to test the performance of our Huber loss reduction method: MALL [21], UCSD [22] and PETS 2009 [23].MALL - 2000 RGB images were extracted from a surveillance camera in a shopping mall. In each image, the crowd density varies between 13 and 53. The main challenges relate to shadows and reflections. Following the literature [21], our system is trained with the first 800 images, and the remaining are left for testing. UCSD - A hand-held camera recorded a campus outdoor scene composed of 2000-246% of the UK frames used during the 2009-2015 period."}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 9", "text": "Edges refers to the relative changes in the gray-level values and binary detectors (canny algorithms [26]) are used for extraction; the texture class includes several statistics, such as energy or entropy, which are compressed from Gray-level co-occurrence [27] or local patterns."}, {"heading": "VI. CONCLUSION, LIMITATIONS & FUTURE WORK", "text": "Unlike previous approaches [8], [6], [7], [9], which use our solutions, the proposed HLR algorithm avoids time-consuming iterative solving while automatically learning the threshold between annotations and 3) actively selects the most advantageous annotations for the learning phases. Such unique aspects have resulted in remarkable performance for various tasks where HLR always performs equally and often better than modern algorithms for learning from loud annotations, classic regression problems and the use of crowd counting applications. In addition, HLR has registered low errors at low compression costs, guaranteeing fast compression without expensive parameter setup. Future work will focus mainly on specifying the framework for classification tasks, although the main load theory could also face the disadvantages that do not consist of funnel Estimulation."}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 11", "text": "[23] A. Chan, M. Morrow, and N. Vasconcelos, \"Analysis of crowded scenes using holistic properties,\" in Workshop on Performance Evaluation of Tracking and Surveillance, CVPR, 2009. [24] D. Ryan, S. Denman, S. Sridharan, and C. Fookes, \"An Evaluation of Crowd counting methods, features and regression models,\" Computer Vision and Image Understanding, Vol. 130, pp. 1-17, 2015. [25] A. C. Davies, J. H. Yin, and S. A. Velastin, \"Crowd monitoring using image processing,\" Electron Commun Eng, vol. 7, pp."}, {"heading": "APPENDIX", "text": "(3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3).). (3). (3). (3). (3). (3). (3). (3). (3).). (3). (3). (3"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper addresses the scalar regression problem<lb>presenting a solution for optimizing the Huber loss in a general<lb>semi-supervised setting, which combines multi-view learning and<lb>manifold regularization. To this aim, we propose a principled<lb>algorithm to 1) avoid computationally expensive iterative solu-<lb>tions while 2) adapting the Huber loss threshold in a data-driven<lb>fashion and 3) actively balancing the use of labelled data to<lb>remove noisy or inconsistent annotations from the training stage.<lb>In a wide experimental evaluation, dealing with diverse applica-<lb>tions, we assess the superiority of our paradigm which is able<lb>to combine strong performance and robustness to noise at a low<lb>computational cost.", "creator": "LaTeX with hyperref package"}}}