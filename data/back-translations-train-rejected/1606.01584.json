{"id": "1606.01584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Curie: A method for protecting SVM Classifier from Poisoning Attack", "abstract": "Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine learning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce - Curie - a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results shows that it works very well in filtering out the poisoned data.", "histories": [["v1", "Sun, 5 Jun 2016 23:42:56 GMT  (416kb,D)", "https://arxiv.org/abs/1606.01584v1", "19 pages, 10 figures"], ["v2", "Tue, 7 Jun 2016 00:41:08 GMT  (416kb,D)", "http://arxiv.org/abs/1606.01584v2", "19 pages, 10 figures"]], "COMMENTS": "19 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["ricky laishram", "vir virander phoha"], "accepted": false, "id": "1606.01584"}, "pdf": {"name": "1606.01584.pdf", "metadata": {"source": "CRF", "title": "Curie: A method for protecting SVM Classifier from Poisoning Attack", "authors": ["Ricky Laishram", "Vir Virander Phoha", "R. Laishram", "V. V. Phoha"], "emails": ["rlaishra@syr.edu", "vvphoha@syr.edu"], "sections": [{"heading": null, "text": "Keywords: machine learning, security, poison attack, SVM"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 Background and Related Works", "text": "In this section we will discuss some background information related to intoxication attacks against machine learning algorithms. We will also discuss the intoxication attack against SVM proposed by Baggio et al. [6] and Xiao [18], as well as some solutions to protect against such attacks."}, {"heading": "2.1 Poisoning Attack", "text": "In many security machine learning applications, data is non-stationary, i.e. the distribution of data shifts over time. For example, in a biometric authentication system, characteristics such as walking pattern, etc. can change for a valid user over time for a variety of reasons. There are two ways to deal with this non-stationary distribution - incremental algorithm or periodic retraining (Figure 1). In the first case, the model is gradually updated when new data arrives, and in the second case, the data is buffered and the model is periodically retrained based on the buffered data. In the periodic retraining model (Figure 1b), the input is stored in a buffer. The model is retrained after a specified time interval has passed, or when the classified performance falls below a predetermined threshold. In our paper, this is the type of model we are considering."}, {"heading": "2.2 Poisoning Attack against SVM", "text": "In SVM, the basic idea is to find a decision interface in the Feature Room that separates the training data based on the class labels [9]. In order to carry out an intoxication attack against SVM, the attacker inserts some data into the training data with the inverted labels [6,18]. In all the attack scenarios described, the attacker's ability is overestimated. It is assumed that the attacker has complete knowledge of the classification system and has the ability to insert some data into the training data. [18] The naive way to carry out the attack is to randomly flip the labels of the data to be used for the training, which corresponds to the addition of sounds to the training data [18]. Another option is to select the points located near the support vectors and flip their labels. A third option is to select points that are furthest away from the support vectors and flip the labels."}, {"heading": "2.3 Previous Methods to protect against Poisoning Attack", "text": "Most of the work [8,7] on safe classification algorithms uses a game theory model - the problem is modeled as a game between the opponent and the learner. The problem with this approach is that it is difficult to integrate into existing systems. As our goal is to protect a system that uses SVM, this method is not of interest. Another method is the use of multiple classifiers [4]. In the proposed approach, the poisoned data are treated as outliers and an overall set of classifiers is used. However, a lack of this method, which they use from 3 to 50 base classifiers, is very resource intensive for the learner. Furthermore, they are not treated as more intelligent methods in the experiments carried out in this work."}, {"heading": "3 Methodology", "text": "In Section 2.2, we have described different strategies for intoxication attacks. In this paper, we will deal with the type of attack [18,6], where the attack points are selected to maximize the loss function. We propose a method to protect an SVM classifier from this type of poison attack. Our goal is to develop a method to identify and filter out the attack points added by the adversary before using the data for retraining. In this section, we will first discuss the threat model and then describe in detail our proposed method."}, {"heading": "3.1 Threat Model", "text": "In this essay, the type of attack we are considering is a causal integrity attack - the opponent has complete knowledge of the system and is trying to increase the false positive rate of the system. In the real world, it is unlikely that the opponent has complete knowledge of the classifier. For the sake of simplicity, we assume that we are dealing with a two-class classification - positive and negative classes. In the attack scenario considered in this essay, the attacker has the ability to inject some data into the training data. In the experiments presented, we look at different cases according to the amount of data the attacker has added. The attacker injects as few specially designed data points as possible, so that the false positive rate is increased. Thus, the aim of the attack is to increase the number of negative classes that are significantly changed by the application of the proposed method."}, {"heading": "3.2 Curie - Poison Points Filter", "text": "The basic idea of our proposed method, Curie, is to identify the data points added by the attacker. However, we refer to these data as poison points. Once these poison points are identified, they can be filtered out before the data is used to retrain the classifier. Figure 2 shows how our method fits into the existing periodic retraining model. In the model mentioned in Figure 2.2, the attack works by selecting a series of points in the attribute space that maximizes the loss function and then flipping their labels. An additional limitation for the attacker is that these points are well hidden within the rest. Figure 3a shows the clusters in the attribute space after the poison points have been injected."}, {"heading": "3.3 Curie in a Multi-Class Classification System", "text": "In the previous sections we considered only two-class classification. This applies to authentication systems, as they only need to classify users as valid or invalid. However, there are security systems in which the classifier must handle multiple classes. In this section we present an extension of Curie for multi-class classification. Consider a multi-class classifier with NC number of classes. From the attacker's perspective, this is no different from the two-class classification. Algorithm 1: Curie: Algorithm for filtering Poison pointsData: Data = (F, C) so that F is a set of feature vectors and C is a set of class names result: M vectors after removal of attack points / * Reduce the number of dimensions * / PcaData \u2190 PCA (Data.F); / * Cluster the data thoroughly DBSCAN * / Cluster class DBSCAN (PcaData)."}, {"heading": "4 Datasets and Experiments", "text": "In this section, we will first describe the data sets we use for our experiments, and then describe the experimental setup for the three different experiments."}, {"heading": "4.1 Datasets", "text": "To verify our method, we conduct experiments with the MNIST dataset [15]. The MNIST dataset consists of 10 classes and approximately 60000 images of the dimensions 28 x 28. We convert each of the images into a vector of the length 784, with each value representing a pixel in the image. Each element of the vector has a value in the range [0, 255]. As described in Section 4.2, we need two datasets - one with 2 classes and one with 3 classes. To create the 2-class dataset, we randomly create 1250 points without substitution from classes 0 and 1 in the original dataset. For the 3-class dataset, we randomly sample 1500 data points without substitution from classes 0, 1 and 2. Thus, the first dataset is of the size 2500 and the second 4500. Then we generate poison points for each of these two datasets. We use the method proposed by Biggio et al. [6] using the Adversarial data library, we generate an additional 50 for each of the 75 and 50 assets."}, {"heading": "4.2 Experimental Setup", "text": "For our experiments, we use a linear SVM classifier, with penalty (C) of value 1. We do not implement the entire system shown in Figure 2, as the purpose of Curie is only to locate the poison points.We have three separate experimental arrangements; the first experiment is to investigate the effectiveness of Curie in eliminating the poison points compared to an outlier detection algorithm; the second experiment is to investigate the effects of changes in the performance of Curie due to the change in hyperparameters; the third experiment is to determine the effectiveness of MultiClass Curie.As mentioned in Section 2.3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18]. So, we evaluate the performance of Curie by comparing it to the case without Curie.Experiment 1, as mentioned in Section 4.1, we have five instances of Class 2 NMIST data aset."}, {"heading": "5 Results and Analysis", "text": "In the previous section we described the MNIST dataset and experimental setup. In this section we present the results of the described experiments in Section 4.2."}, {"heading": "5.1 Results of Experiment 1", "text": "The performance comparison of the SVM classifier with and without Curie is shown in Table 1. Figures 4a and 4b show the accuracy and false positive rate for various injected poison points. In Figure 4, the green line represents our method - Curie. In both Figure 4a and 4b, it can be observed that in Curie, the accuracy and false positive rate of the classifier remains almost constant even when the poison points are increased, indicating that the poison points are filtered out before they are used to form the SVM classifier. These results show that Curie is very effective at identifying and removing poison points, regardless of the amount of poison points injected."}, {"heading": "5.2 Results of Experiment 2", "text": "In this section we present the results of experiment 2. For this experiment we used only the instance of the 2-class dataset with 50 poison points. Figure 5a shows the accuracy of the classifier for different values of \u03c9 and Figure 5b shows the false positive rate of the classifier. The vertical green dashed line represents the theoretical value of \u043c given by 13 with \u03c1 = 0.02.These results show that Curie is not very sensitive to \u03c9, and vice versa \u03c1 (equation 13). In a real dataset the exact value of \u043c is unknown. Thus, the results show that Curie can function in such cases."}, {"heading": "5.3 Results of Experiment 3", "text": "In this subsection we present the results of the third experiment. The accuracy comparison of the classifier with and without MultiClass-Curie for different amounts of poison points is shown in Table 3.Figure 6, which shows the comparison of the classification accuracy for different amounts of poison points with and without MultiClass-Curie. It can be noted that the accuracy of the classifier decreases even with the increasing number of poison points. However, with MultiClass-Curie the accuracy decreases very slowly with the number of poison points. These results show that MultiClass-Curie can still be used in the case of multiclass classifiers to protect the SVM classifier from poisoning attacks."}, {"heading": "6 Conclusion", "text": "The use of machine learning for security applications is becoming more widespread. Thus, an investigation of the behavior of machine learning algorithms in an adversarial environment being removed less than 95% is important. Poisoning attacks pose a very real threat to security systems (Section 2) that use machine learning techniques. Research [6,18] has shown that SVM, a very common machine learning algorithm, is highly susceptible to poisoning attacks. In this paper, we examined the poison attack against SVM, and propose a method we call curie to protect a system that uses SVM classifiers from such attacks. curie functions as a filter before the buffered data is used to retrain the SVM classification. Curie works by exploiting the fact that the poison data looks like a normal point in the feature space, but its labels are rotated. The data is bundled in the feature space, the points are calculated from each other cluster's average distance."}, {"heading": "A Curie Hyperparameters", "text": "In Section 3.2, we described the algorithm for Curie. In the described algorithm, we will use two hyperparameters - \u03b8 and \u03c9. In this section, we will describe these hyperparameters and the relationship between them. In Curie, the hyperparameter \u03c9 is the weight used for mapping the class name to the additional dimension in (feature + caption) space. In this section, we define the threshold above which a data point in the training data is considered a poisonous point. Assume that the positive class to x + and the negative class to x \u2212 is mapped in the additional dimension introduced in the (feature + caption) space. We define the threshold in our algorithm (Section 3.2) so that it is x + x \u2212 x \u2212 x \u2212 | (1) Consider a cluster C of size n in the attribute space. Let Sc set the data points in cluster C. Suppose that there are l dimensions in the data."}, {"heading": "B MultiClass-Curie Hyperparameters", "text": "As mentioned in Section 3.3, in MultiClass-Curie (NC \u2212 1) there are additional dimensions in (feature + label) space. Thus, Equation 5 is rewritten as, distf + l (p) = 1 n \u2212 1 \u2211 x Sc \u2212 p (l + NC \u2212 1) as mentioned in Section 3.3, when the attacker constructs the attack, the classifier is considered as two classifiers - one class represents the interest class CA \u2212 p = 1 (xi \u2212 pi) 2 + NC \u2212 1 \u2211 j = 0 (xj \u2212 pj) 2 (14). When the attacker constructs the attack, the classifier is considered as two classifiers - one class represents the interest class CA \u2212 p and the other class represents the rest of the classes. Thus, the difference xj \u2212 pj will be the same in all cases, with the exception of the one that corresponds to the CA \u2212 pi, and the rest of the class is not equal (Sc)."}], "references": [{"title": "New outlier detection method based on fuzzy clustering", "author": ["M. Al-Zoubi", "A. Al-Dahoud", "A.A. Yahya"], "venue": "WSEAS transactions on information science and applications 7(5), 681\u2013690", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar"], "venue": "Machine Learning 81(2), 121\u2013148", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Can machine learning be secure? In: Proceedings of the 2006 ACM Symposium on Information, computer and communications security", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "pp. 16\u201325. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial classification tasks", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "Multiple Classifier Systems, pp. 350\u2013359. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML 20, 97\u2013112", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "arXiv preprint arXiv:1206.6389", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "The Journal of Machine Learning Research 13(1), 2617\u20132654", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 547\u2013555. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20(3), 273\u2013297", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "S. Sanghai", "D Verma"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 99\u2013108. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "venue": "Kdd. vol. 96, pp. 226\u2013231", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "The nist 2014 speaker recognition i-vector machine learning challenge", "author": ["C.S. Greenberg", "D. Bans\u00e9", "G.R. Doddington", "D. Garcia-Romero", "J.J. Godfrey", "T. Kinnunen", "A.F. Martin", "A. McCree", "M. Przybocki", "D.A. Reynolds"], "venue": "Odyssey: The Speaker and Language Recognition Workshop", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering cluster-based local outliers", "author": ["Z. He", "X. Xu", "S. Deng"], "venue": "Pattern Recognition Letters 24(9), 1641\u20131650", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Authenticating users through their arm movement patterns", "author": ["R. Kumar", "V.V. Phoha", "R. Raina"], "venue": "arXiv preprint arXiv:1603.02211", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "Moore", "A.W"], "venue": "ICML. vol. 1", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Experimental study with real-world data for android app security analysis using machine learning", "author": ["S. Roy", "J. DeLoach", "Y. Li", "N. Herndon", "D. Caragea", "X. Ou", "V.P. Ranganath", "H. Li", "N. Guevara"], "venue": "Proceedings of the 31st Annual Computer Security Applications Conference. pp. 81\u201390. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial label flips attack on support vector machines", "author": ["H. Xiao", "H. Xiao", "C. Eckert"], "venue": "ECAI. pp. 870\u2013875", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "tries to manipulate the classifier [10].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Most machine learning algorithms assume that the training and test data have the same distribution [10,7].", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "Most machine learning algorithms assume that the training and test data have the same distribution [10,7].", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "[3] explored the poisoning attacks and broadly categorized them through two aspects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] and Xiao et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] and Xiao et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] and Xiao [18], and some proposed solutions to protect against such attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[6] and Xiao [18], and some proposed solutions to protect against such attacks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "Retraining the system opens up an attack surface [3,2].", "startOffset": 49, "endOffset": 54}, {"referenceID": 1, "context": "Retraining the system opens up an attack surface [3,2].", "startOffset": 49, "endOffset": 54}, {"referenceID": 8, "context": "2 Poisoning Attack against SVM In SVM, the fundamental idea is to find a decision surface in the feature space that separates the training data based on the class labels [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "To perform a poisoning attack against SVM, the attacker inserts some data in the training data with the labels flipped [6,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 17, "context": "To perform a poisoning attack against SVM, the attacker inserts some data in the training data with the labels flipped [6,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 17, "context": "This is equivalent to adding noise to the training data [18].", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Another method [5] is to select a combination of points that maximizes the classification error (or false positive rate).", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "These methods [18,6] works by selecting points that maximizes the loss function of the SVM classifier.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "These methods [18,6] works by selecting points that maximizes the loss function of the SVM classifier.", "startOffset": 14, "endOffset": 20}, {"referenceID": 17, "context": "[18], a set of points is selected based on the loss maximization framework and their labels are flipped.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], some data points are selected and they are moved to other points in the feature space such that the loss is maximized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Most of the works [8,7] on secure classifier algorithms uses a game theory model - the problem is modeled as a game between the adversary and the learner.", "startOffset": 18, "endOffset": 23}, {"referenceID": 6, "context": "Most of the works [8,7] on secure classifier algorithms uses a game theory model - the problem is modeled as a game between the adversary and the learner.", "startOffset": 18, "endOffset": 23}, {"referenceID": 3, "context": "Another method is to use multiple classifiers [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "In addition, in the experiments performed in that work, they did not use the more intelligent methods [6,18] of generating the poison data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 17, "context": "In addition, in the experiments performed in that work, they did not use the more intelligent methods [6,18] of generating the poison data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 5, "context": "In this paper, we consider the poisoning attack with gradient ascent proposed by Biggio and Xiao [6,18].", "startOffset": 97, "endOffset": 103}, {"referenceID": 17, "context": "In this paper, we consider the poisoning attack with gradient ascent proposed by Biggio and Xiao [6,18].", "startOffset": 97, "endOffset": 103}, {"referenceID": 17, "context": "In this paper, we are addressing the type of attack [18,6] in which the attack points are selected to maximize the loss function.", "startOffset": 52, "endOffset": 58}, {"referenceID": 5, "context": "In this paper, we are addressing the type of attack [18,6] in which the attack points are selected to maximize the loss function.", "startOffset": 52, "endOffset": 58}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] is very similar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In our algorithm, the clustering algorithm used is DBSCAN [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "To verify our method, we perform experiments using the MNIST dataset [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "[6] using the AdversariaLib library.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18].", "startOffset": 93, "endOffset": 99}, {"referenceID": 17, "context": "3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18].", "startOffset": 93, "endOffset": 99}, {"referenceID": 5, "context": "Researches [6,18] have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.", "startOffset": 11, "endOffset": 17}, {"referenceID": 17, "context": "Researches [6,18] have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.", "startOffset": 11, "endOffset": 17}], "year": 2016, "abstractText": "Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine le arning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce Curie a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results show that it works very well in filtering out the poisoned data.", "creator": "LaTeX with hyperref package"}}}