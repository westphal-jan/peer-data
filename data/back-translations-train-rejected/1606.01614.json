{"id": "1606.01614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification", "abstract": "In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to low-resource languages where only unlabeled data exists. ADAN is a \"Y-shaped\" network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system.", "histories": [["v1", "Mon, 6 Jun 2016 05:04:23 GMT  (4327kb,D)", "http://arxiv.org/abs/1606.01614v1", null], ["v2", "Thu, 27 Oct 2016 15:28:02 GMT  (6231kb,D)", "http://arxiv.org/abs/1606.01614v2", null], ["v3", "Thu, 16 Feb 2017 01:30:30 GMT  (1655kb,D)", "http://arxiv.org/abs/1606.01614v3", null], ["v4", "Mon, 17 Apr 2017 18:48:19 GMT  (2431kb,D)", "http://arxiv.org/abs/1606.01614v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xilun chen", "yu sun", "ben athiwaratkun", "claire cardie", "kilian weinberger"], "accepted": false, "id": "1606.01614"}, "pdf": {"name": "1606.01614.pdf", "metadata": {"source": "CRF", "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification", "authors": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie"], "emails": ["xlchen@cs.cornell.edu", "pa338@cornell.edu", "ys646@cornell.edu", "kqw4@cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2 Related Work", "text": "In fact, most people who are able to move to another world, to move to another world in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves."}, {"heading": "3.1 Network Architecture", "text": "As illustrated in Figure 1, the ADAN model is a two-branch feedback network. Therefore, there are three main components in the network, a common feature extractor F that maps an input sequence x to the feature space, a sentiment classifier P that predicts the sensation tag for x, but given the feature representation F (x), and a speech predictor Q that also takes the feature F (x) but predicts whether x comes from English or Chinese. An input document is trained and tested as a word sequence x = {w1,.., wn} where n is the number of tokens in x. Each word w \u00b2 x is represented by its word embedding vw (Mikolov et al., 2013). Since the same feature extractor F is trained and tested in both English and Chinese sentences, it is advantageous if the word representations for both languages are roughly aligned in a shared space."}, {"heading": "3.2 Training", "text": "The ADAN model can be trained end-to-end with standard back propagation, which we will analyze in detail in this section. For the two classifiers P and Q, we can each use the traditional cross-entropy method, which is in fact called Lp (y, y) andLq (y, y). Lp is the negative log probability of the model P, which predicts the correct sentiment labeling, and Lq is that Q predicts the correct language. Therefore, we are looking for the minimum of the following loss functions for P and Q: min \u03b8p Jp (p, f)."}, {"heading": "4 Experiments and Discussions", "text": "To demonstrate the effectiveness of our model, we are experimenting with a mood classification at sentence level with 5 labels (strongly negative, slightly negative, neutral, slightly positive and strongly positive)."}, {"heading": "4.1 Data", "text": "We use a balanced data set of 700k Yelp reviews from Zhang et al. (2015) with their ratings as labels (scale 1-5. We also adopt their train validation split: 650k reviews form the training set and the remaining 50k form a validation set used only in English benchmarks. Described Chinese data. As ADAN does not require labeled Chinese data for training, this annotated data is used exclusively to validate the performance of our model. 10k balanced Chinese hotel reviews from Lin et al. (2015) serve as a validation set for model selection and parameter tuning. The results are reported on a separate test set of another 10k hotel reviews. Unlabeled Chinese data. We use another 150k unlabeled Chinese hotel reviews for training."}, {"heading": "4.2 Chinese Sentiment Classification", "text": "The DAN + BWE base model uses bilingual word embedding to map both English and Chinese ratings to the same space, and is only trained using English Yelp ratings. We can see from Table 1 that bilingual embedding alone is not enough to transfer knowledge of the English sentiment classification to Chinese, and performance is poor (29.11%).We then compare ADAN to domain customization baselines that have not produced satisfactory results for our task. TCA (Pan et al., 2011) has not worked because it requires square space in terms of the number of samples (650k in our case).SDA (Glorot et al., 2011) and subsequent mSDA systems (Chen et al., 2012) are very effective for cross-domain sensing."}, {"heading": "4.3 Qualitative Analysis and Visualizations", "text": "To qualitatively show how ADAN bridges the distributional discrepancies between the English and Chinese instances, t-SNE (Van der Maaten and Hinton, 2008) visualizations of activations at different levels are shown in Figure 2. We randomly select 1000 sentences from the Chinese and English validation sentences, and record the t-SNE of hidden node activations at three locations in our model: the middle layer, the end of the common feature extractor, and the last hidden layer in the sentiment classifier before softmax. The turn-to-English model is the DAN + BWE baseline in Table 1. Note that there is actually only one \"branch\" in this baseline model, but to compare the first three layers in the sentiment extractor than the feature extractor is. Moreover, t-SNE plot is the bilingual plot of embedding the most common words in both languages."}, {"heading": "4.4 Side: English Sentiment Classification", "text": "The result suggests that the DAN baselines are competitive in our experiments for the English sentiment classification. While Iyyer et al. (2015) tested DAN for sentiment analysis on the smaller Stanford Sentiment Treebank dataset, we also show its effectiveness on the large Yelp reviews dataset. Ta-ble 2 compares our DAN with the results in (Zhang et al., 2015). DAN surpasses most of the baselines including LSTM and is close to the best model in (Zhang et al., 2015), a very large Convolutionary Neural Network. For more discussion of the various embedding options, see Section 4.5."}, {"heading": "4.5 Impact of Bilingual Word Embeddings", "text": "In this section we discuss the impact of bilingual word embedding, which we see as a key factor for future improvements. Table 2 shows that the low dimensionality of the BWE is a limitation of the performance of the DAN on the English sentiment classification, especially since there is a large gap between the dimensionality of the embedding and the hidden layers (50 vs. 900). Even the use of 300-dimensional random embedding exceeds the 50d BWE and is only slightly worse than BWE (50d) + Random 250d (increasing the BWE to 300 dimensions). Furthermore, with better word embedding of word2vec (Mikolov et al., 2013), the performance is only slightly worse and is close to the state of the art. Nevertheless, the random WE no longer works when performing multilingual training for ADAN, as shown in Table 1."}, {"heading": "4.6 Implementation Details", "text": "All hidden layers contain 900 hidden units. This choice is more or less ad hoc and performance could potentially be improved by a more careful model selection. Batch normalization (Ioffe and Szegedy, 2015) is used in each hidden layer. We confirm the observations of Ioffe and Szegedy (2015) that batch normalization can sometimes eliminate the need for dropout (Srivastava et al., 2014) or word dropout (Iyyer et al., 2015), which make little difference in our experiments. We use Adam (Kingma and Ba, 2014) with a learning rate of 0.05 for optimization. ADAN is implemented in Torch7 (Collobert et al., 2011) and the code is made available after the verification process."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we introduced ADAN, a contradictory, deep average network for linguistic sentiment classification. ADAN uses the affluent resources of English to support sensory categorization in other languages where little or no annotated data exists. We confirmed our hypothesis through empirical experiments on Chinese sentiment categorization, in which we labeled English training data and only unlabeled Chinese data. Experiments show that ADAN outperforms several baselines, including domain adaptation models and a highly competitive MT baseline using Google Translate. Furthermore, we showed that ADAN, in the presence of labeled data in the target language, can naturally incorporate this additional oversight and yields even more competitive outcomes. For future work, one direction is to investigate better bilingual word embedding, which we identify as a key factor limiting ADAN's performance. In another direction, our contradictory textual framework for STAN can only be applied to other levels, as well."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["Ajakan et al.2014] Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In Second Workshop on transfer and Multi-Task Learning (NIPS", "citeRegEx": "Ajakan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "Multilingual subjectivity analysis using machine translation", "author": ["Banea et al.2008] Carmen Banea", "Rada Mihalcea", "Janyce Wiebe", "Samer Hassan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Banea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2008}, {"title": "Multilingual subjectivity: Are more languages better", "author": ["Banea et al.2010] Carmen Banea", "Rada Mihalcea", "Janyce Wiebe"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "Banea et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2010}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguis-", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha"], "venue": "In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learn-", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF192376", "author": ["Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Soumith Chintala", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Ganin", "Lempitsky2015] Yaroslav Ganin", "Victor Lempitsky"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Ganin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2015}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Con-", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Generative adversarial nets", "author": ["Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Exploring english lexicon knowledge for chinese sentiment analysis", "author": ["He et al.2010] Yulan He", "Harith Alani", "Deyu Zhou"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["\u0130rsoy", "Cardie2014a] Ozan \u0130rsoy", "Claire Cardie"], "venue": null, "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["\u0130rsoy", "Cardie2014b] Ozan \u0130rsoy", "Claire Cardie"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "An empirical study on sentiment classification of chinese review using word embedding", "author": ["Lin et al.2015] Yiou Lin", "Hang Lei", "Jia Wu", "Xiaoyu Li"], "venue": "In Proceedings of the 29th Pacific Asia Conference on Language,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Joint bilingual sentiment classification with unlabeled parallel corpora", "author": ["Lu et al.2011] Bin Lu", "Chenhao Tan", "Claire Cardie", "Benjamin K Tsou"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Lu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["Carmen Banea", "Janyce M Wiebe"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Domain adaptation via transfer component analysis", "author": ["Pan et al.2011] Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sentiment after translation: A case-study on arabic social media posts", "author": ["Saif Mohammad", "Svetlana Kiritchenko"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Salameh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salameh et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "An empirical study of sentiment analysis for chinese documents", "author": ["Tan", "Zhang2008] Songbo Tan", "Jin Zhang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Tan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2008}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Bilingual word embeddings from nonparallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis", "author": ["Xiaojun Wan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Wan.,? \\Q2008\\E", "shortCiteRegEx": "Wan.", "year": 2008}, {"title": "Co-training for crosslingual sentiment classification", "author": ["Xiaojun Wan"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "Wan.,? \\Q2009\\E", "shortCiteRegEx": "Wan.", "year": 2009}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 27, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 15, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 25, "context": "Most of these, however, rely on a massive amount of labeled training data or fine-grained annotations such as the Stanford Sentiment Treebank (Socher et al., 2013), which provides sentiment annotations for each phrase in the parse tree of every sentence.", "startOffset": 142, "endOffset": 163}, {"referenceID": 31, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 32, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 19, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 0, "context": ", 2014) and visual domain adaptation (Ganin and Lempitsky, 2015; Ajakan et al., 2014).", "startOffset": 37, "endOffset": 85}, {"referenceID": 15, "context": "neural networks (Iyyer et al., 2015).", "startOffset": 16, "endOffset": 36}, {"referenceID": 21, "context": "For each document, DAN takes the arithmetic mean of the document word vectors (Mikolov et al., 2013; Pennington et al., 2014) as input, and", "startOffset": 78, "endOffset": 125}, {"referenceID": 23, "context": "For each document, DAN takes the arithmetic mean of the document word vectors (Mikolov et al., 2013; Pennington et al., 2014) as input, and", "startOffset": 78, "endOffset": 125}, {"referenceID": 20, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 1, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 2, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 31, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 32, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 19, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 11, "context": ", 2011) and the more traditional lexical direction (He et al., 2010).", "startOffset": 51, "endOffset": 68}, {"referenceID": 19, "context": "Lu et al. (2011) instead uses labeled data from both languages to im-", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al. (2011) and Chen et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al. (2011) and Chen et al. (2012) try to learn effective classifiers for which the training and test samples are from different underlying distributions.", "startOffset": 18, "endOffset": 85}, {"referenceID": 9, "context": "Adversarial Networks (Goodfellow et al., 2014; Ganin and Lempitsky, 2015) have enjoyed much success in computer vision, but to the best of our knowledge, have not yet been applied in NLP with comparable success.", "startOffset": 21, "endOffset": 73}, {"referenceID": 9, "context": "chitectures similar to ours, by pitching a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015).", "startOffset": 143, "endOffset": 189}, {"referenceID": 6, "context": "chitectures similar to ours, by pitching a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015).", "startOffset": 143, "endOffset": 189}, {"referenceID": 21, "context": "Each word w \u2208 x is represented by its word embedding vw (Mikolov et al., 2013).", "startOffset": 56, "endOffset": 78}, {"referenceID": 34, "context": "Prior work on bilingual word embeddings (Zou et al., 2013; Vuli\u0107 and Moens, 2015; Gouws et al., 2015) attempts to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language.", "startOffset": 40, "endOffset": 101}, {"referenceID": 10, "context": "Prior work on bilingual word embeddings (Zou et al., 2013; Vuli\u0107 and Moens, 2015; Gouws et al., 2015) attempts to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language.", "startOffset": 40, "endOffset": 101}, {"referenceID": 34, "context": "word embeddings (BWE) in (Zou et al., 2013).", "startOffset": 25, "endOffset": 43}, {"referenceID": 34, "context": "Note that using the word embeddings in (Zou et al., 2013) makes our work implic-", "startOffset": 39, "endOffset": 57}, {"referenceID": 15, "context": "The feature extractor F is a Deep Averaging Network (DAN) (Iyyer et al., 2015).", "startOffset": 58, "endOffset": 78}, {"referenceID": 9, "context": "The first one, inspired by (Goodfellow et al., 2014), performs alternate training: first the sentiment classifier and the feature extractor are trained together, then the", "startOffset": 27, "endOffset": 52}, {"referenceID": 9, "context": "For instance, in (Goodfellow et al., 2014), a hyper-parameter k", "startOffset": 17, "endOffset": 42}, {"referenceID": 4, "context": "11% mSDA (Chen et al., 2012) 31.", "startOffset": 9, "endOffset": 28}, {"referenceID": 33, "context": "We use a balanced dataset of 700k Yelp reviews from Zhang et al. (2015) with their ratings as labels (scale 1-5).", "startOffset": 52, "endOffset": 72}, {"referenceID": 18, "context": "10k balanced Chinese hotel reviews from Lin et al. (2015) are used as validation set for model selection and parameter tuning.", "startOffset": 40, "endOffset": 58}, {"referenceID": 22, "context": "TCA (Pan et al., 2011) did not work since it requires quadratic space in terms of the number", "startOffset": 4, "endOffset": 22}, {"referenceID": 8, "context": "SDA (Glorot et al., 2011) and the subsequent mSDA (Chen et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": ", 2011) and the subsequent mSDA (Chen et al., 2012) are proven very effective for cross-domain sentiment classification on Amazon reviews.", "startOffset": 32, "endOffset": 51}, {"referenceID": 1, "context": "Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language.", "startOffset": 17, "endOffset": 59}, {"referenceID": 24, "context": "Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language.", "startOffset": 17, "endOffset": 59}, {"referenceID": 15, "context": "While Iyyer et al. (2015) tested DAN for sentiment analysis on the smaller Stanford Sen-", "startOffset": 6, "endOffset": 26}, {"referenceID": 33, "context": "\u2020 Results from (Zhang et al., 2015).", "startOffset": 15, "endOffset": 35}, {"referenceID": 33, "context": "ble 2 compares our DAN with results in (Zhang et al., 2015).", "startOffset": 39, "endOffset": 59}, {"referenceID": 33, "context": "DAN beats most of the baselines including LSTM, and is close to the best model in (Zhang et al., 2015), a very large convolutional neural network.", "startOffset": 82, "endOffset": 102}, {"referenceID": 21, "context": "word2vec (Mikolov et al., 2013), the performance is 2% higher and close to the state of the art.", "startOffset": 9, "endOffset": 31}, {"referenceID": 34, "context": "embeddings from (Zou et al., 2013), and (b) is the updated embeddings in the trained ADAN model.", "startOffset": 16, "endOffset": 34}, {"referenceID": 26, "context": "We corroborate the observations by Ioffe and Szegedy (2015) that Batch Normalization can sometimes eliminate the need for Dropout (Srivastava et al., 2014) or Word Dropout (Iyyer et al.", "startOffset": 130, "endOffset": 155}, {"referenceID": 15, "context": ", 2014) or Word Dropout (Iyyer et al., 2015), which make little difference in our experiments.", "startOffset": 24, "endOffset": 44}, {"referenceID": 5, "context": "ADAN is implemented in Torch7 (Collobert et al., 2011), and the code will be made avail-", "startOffset": 30, "endOffset": 54}], "year": 2017, "abstractText": "In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to lowresource languages where only unlabeled data exists. ADAN is a \u201cY-shaped\u201d network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system.", "creator": "LaTeX with hyperref package"}}}