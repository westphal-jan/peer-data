{"id": "1606.01781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Very Deep Convolutional Networks for Text Classification", "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.", "histories": [["v1", "Mon, 6 Jun 2016 15:14:50 GMT  (34kb)", "http://arxiv.org/abs/1606.01781v1", null], ["v2", "Fri, 27 Jan 2017 12:49:11 GMT  (41kb)", "http://arxiv.org/abs/1606.01781v2", "10 pages, EACL 2017, camera-ready"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["alexis conneau", "holger schwenk", "lo\\\"ic barrault", "yann lecun"], "accepted": false, "id": "1606.01781"}, "pdf": {"name": "1606.01781.pdf", "metadata": {"source": "CRF", "title": "Very Deep Convolutional Networks for Natural Language Processing", "authors": ["Alexis Conneau", "Holger Schwenk"], "emails": ["aconneau@fb.com", "schwenk@fb.com", "yann@fb.com", "loic.barrault@univ-lemans.fr"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.01 781v 1 [cs.C L] 6"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related work", "text": "This year it is more than ever before."}, {"heading": "3 Architecture", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "4 Experimental evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Tasks and data", "text": "In the Computer Vision community, the availability of large sets of data for object recognition and image classification has fueled the development of new architectures, allowing in particular the comparison of many different architectures and the presentation of the benefits of very deep Convolutionary Networks. We present our results on eight freely available large sets of data introduced by Zhang et al. [20] and covering several classification tasks such as mood analysis, theme classification or news categorization (see Table 2). The number of training examples varies from 120k to 3.6M, and the number of classes ranges from 2 to 14. This is significantly lower than in computer vision (e.g. 1000 classes for ImageNet), which means that each example generates less grading information, which can make it difficult to train large architectures. It should also be noted that some of the tasks are very ambiguous, especially mood analyses, for which it is difficult to clearly assign fine-grained labels."}, {"heading": "4.2 Common model settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3 Experimental results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Conclusion", "text": "We have introduced a new architecture for NLP that follows two design principles: 1) work on the lowest atomic representation of text, i.e. characters, and 2) use a deep stack of local operations, i.e. coils and max pooling of size 3, to learn a high hierarchical representation of a set. This architecture has been evaluated on eight freely available large datasets, and we have been able to show that increasing the depth up to 29 revolutionary layers continuously improves performance. Our models are much deeper than previously published Constitutional Neural Networks, and they exceed these approaches on all datasets. To get the most out of our knowledge, this is the first time that the \"benefit of depth\" for Convolutionary Neural Networks has been demonstrated in NLP. What makes Constitutional Networks suitable for image processing is that the visual world is compositional. Combining pixels to form shapes, contours and profiles."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston L\u00e9on Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u00edcero Nogueira dos Santos", "Ma\u00edra Gatti"], "venue": "In Coling,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In http:// arxiv.org/abs/", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A convolutional neural network for modelling sentences. In http:// arxiv.org/ abs", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Convolutional encoders for neural machine translation", "author": ["Andrew Lamb", "Michel Xie"], "venue": "In WEB download,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["Pedro O. Pinhero", "Ronan Collobert"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": "In http:// arxiv.org/abs/", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 2, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 3, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 6, "context": "The best networks are using more than 150 layers [7, 8].", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": "The best networks are using more than 150 layers [7, 8].", "startOffset": 49, "endOffset": 55}, {"referenceID": 16, "context": "[17, 18] to name just a few.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[17, 18] to name just a few.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[18], or in both directions, named bidirectional LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The design of our architecture is inspired by recent progress in computer vision, in particular [15, 7].", "startOffset": 96, "endOffset": 103}, {"referenceID": 6, "context": "The design of our architecture is inspired by recent progress in computer vision, in particular [15, 7].", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "At each node, the left and right context are combined using weights which are shared for all nodes [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "First works using convolutional neural networks for NLP are probably [3, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "First works using convolutional neural networks for NLP are probably [3, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 11, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 10, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 19, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 11, "context": "A rather shallow neural net was proposed in Kim [12]: one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "[11], but using five convolutional layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] were the first to perform sentiment analysis entirely at the character level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The use of character level information was also proposed by dos Santos and Gatti [5]: all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The same idea was recently applied to sentence classification [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "[20] with networks which have significantly fewer parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] or even outperform ConvNets for some data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks, namely 19 layers [15], or even up to 152 layers [7].", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks, namely 19 layers [15], or even up to 152 layers [7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Our architecture can be in fact seen as a temporal adaptation of the VGG network [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "[7], namely identity and 1\u00d7 1 convolutions (see Figure 1).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Each convolutional block (see Figure 2) is a sequence of two convolutional layers, each one followed by a temporal BatchNorm [9] layer and an ReLU activation.", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "(ii) Ki is followed by a k-max pooling layer [11] where k is such that the resolution is halved.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "[20] which cover several classification tasks such as sentiment analysis, topic classification or news categorization (see Table 2).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for more details on the construction of the data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for all data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for a detailed description.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] best results use a Thesaurus data augmentation technique (marked with an ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Xiao and Cho [19] propose a combination of convolution and recurrent layers.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "[20], all processing is done at the character level which is the atomic representation of a sentence, same as pixels for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "However, with our very deep architecture, we get closer to the state-of-the-art which are ngrams TF-IDF for these data sets and significantly surpass convolutional models presented in [20].", "startOffset": 184, "endOffset": 188}, {"referenceID": 6, "context": "[7], the gain in accuracy due to the the increase of the depth is limited when using standard ConvNets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "To overcome this degradation of the model, the ResNet model introduced shortcut connections between convolutional blocks that allow the gradients to flow more easily in the network [7].", "startOffset": 181, "endOffset": 184}], "year": 2016, "abstractText": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.", "creator": "LaTeX with hyperref package"}}}