{"id": "1606.01981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "abstract": "Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic--namely, projecting quantized weights during backpropagation--can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.", "histories": [["v1", "Tue, 7 Jun 2016 00:28:42 GMT  (645kb,D)", "http://arxiv.org/abs/1606.01981v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["paul merolla", "rathinakumar appuswamy", "john arthur", "steve k esser", "dharmendra modha"], "accepted": false, "id": "1606.01981"}, "pdf": {"name": "1606.01981.pdf", "metadata": {"source": "CRF", "title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "authors": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K. Esser", "Dharmendra Modha"], "emails": ["pameroll@us.ibm.com", "rappusw@us.ibm.com", "arthurjo@us.ibm.com", "sesser@us.ibm.com", "dmodha@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related work", "text": "Our current work relates to a flurry of recent research on low-precision DNNs. The great advance has been the discovery of a backprop-based learning method for quantifying weights during propagation. This training method was introduced in [12, 10] with impressive results on CIFAR-10 and developed in the context of neuromorphic hardware in [9]. [13] proposed a similar rule for contrastive divergence. At this point, we build on this work by examining a more general class of weight predictions that are not limited to quantification. Another more recent approach approaches weights with binary values during training, formulated as a limited optimization problem, with promising results on ImageNet [7]. We expect their results to be consistent with our results, but this is left to future work. Other approaches have developed a probable interpretation of discrete weights [14], but they have not yet been expanded to include data contexts or IST."}, {"heading": "3 DNNs with projected weights", "text": "We consider a DNN as two sets of tensors: input activations A, and weights W, with each set indexed by its layer k. Output of one layer (which is also the input to the next layer) is calculated using convolution operation *, for example Ak + 1 = r (Ak \u0445 Proj (Wk)), where r is the typical ReLU activation, and Proj is a projection operator described below. For notational simplification, we specify fully connected layers as convolutions and do not consider neurons biases.In this work, we examine DNN among various projections on the weights used for training and testing. These projections are defined and extended in Table 1 as scaled functions to operate on tensors. We refer to the i-th element of tensor Wk of wki. We insert one per layer factor (Wki = maxi) to normalize weights."}, {"heading": "4 Results", "text": "In this section, we examine the performance and robustness of networks trained on CIFAR-10 (Section 4.1) and ImageNet (Section 4.2) using different weight calculations and clip settings."}, {"heading": "4.1 CIFAR-10 experiments", "text": "CIFAR-10 is an image classification dataset with small (32 \u00d7 32) color images in 10 categories [16]. For our experiments, we do not use data augmentation. To train, we use the ADAM learning rule [17] with a learning rate of 0.003 and a batch size of 50; a loss of square hinges and a normalization of batches [18]. These results were obtained in TensorFlow [19], except in a control network in which Caffe [20] was used. Our experimental flow is as follows: With the training set processed with global contrast normalization and ZCA brightening from Pylearn2 [21], we trained six networks for 500 epochs with a DNN with 6 Conv and 2 FC layers (Figure 1A). These networks are named after their training parameters."}, {"heading": "4.1.1 High precision or -1,+1? It doesn\u2019t (much) matter", "text": "An interesting finding is that all of our networks have comparable test errors for Te-None and Te-Sign. To see why this is surprising, consider the DNN from Figure 1A, which was trained using the sign projection (TrSign-C). During training, the loss is calculated in relation to the sign of the weights and not directly on the high-precision weights. Nevertheless, this network performs well when evaluated using either binarized weights or the high-precision weights (Figure 1B). This result would be expected if the two weight distributions converge to the same values, but this is not the case: For example, the weights of two corresponding filters are noticeably different (Figure 1B, inserts) and these quantization errors are present in all layers (Figure 1C). However, despite these differences, activities in the network continue to converge to similar patterns as it expands through the layers, which we are surprised by observing (Figure 1D)."}, {"heading": "4.1.2 Robustness to weight distortions beyond -1,+1", "text": "We examine the premise that networks that perform well on binary weights also perform well on many types of weight distortions. Here, we focus on networks that are built with \u03b2-based weight predictions (based on BinaryConnect networks), where allowed weight states are discrete during training. Specifically, we look at Tr-Sign-C and Tr-Stoch-C among three distortions: Te-AddNorm, Te-MultUnif and Te-Power. NiN-ctrl is also shown for comparison. In the case of adding Gaussian noise to each weight (Te-AddNorm), we find that test errors in the precursor signs used (Figure 2A), however, Tr-Sign-C and Tr-Stoch-C are significantly more resistant than these NiN-ctrl-ctrl-ctrl-ctrl. In the case of adding Gausal noise to each weight, however, Tr-Sign-C and Tr-Significant-Significant-Significant-Significant-Significant-Signs are found to be used."}, {"heading": "4.1.3 Learning robust networks with and without weight projections", "text": "In this section, we try to uncover the aspects of training that lead to robust networks. Our first experiment is to see if projections other than the quantification of weight also work. Accordingly, we have trained a network with power projection (Tr-Power-C) in which a new \u03b2 value is drawn for each minibatch [0, 2]. The network converges to a solution that is similarly robust to Tr-Sign-C (Figure 2A-C), which is remarkable considering that the projected weights are subject to stochastic nonlinear distortion in each training step. This confirms that training also works with weight predictions other than quantification, and opens the door to more exotic projections (see Section 4.1.4). Next, we have tried to remove weight preconceptions altogether. We have trained networks Tr-None-C (no projection with circumcision) and Tr-None-NC (no projection)."}, {"heading": "4.1.4 Stochastic multiplicative projection", "text": "Inspired by the Stoch projection first presented in [10], we constructed a new stochastic projection rule, StochM. In Stoch, each weight is randomly projected to + 1 with the probability p = 12 (wki \u03b1k + 1) and \u2212 1 with the probability 1 \u2212 p.StochM derives from a similar idea, but now projects each weight onto the interval [\u03b3wki, wki\u03b3] with the probability p and [\u2212 \u03b3wki, \u2212 wki \u03b3] with 1 \u2212 p.The consideration is that it is nothing special to project onto two values, so why not scan the weight space more densely? In terms of performance, Tr-StochM-C achieves an error of 7.64% for Te-None and 8.25% for Te-Sign (Table 2), which are state of the art for this data set without data augmentation, to the best of knowledge."}, {"heading": "4.2 Towards a robust AlexNet", "text": "To see if our results go beyond CIFAR-10, we moved to ImageNet (ILSVRC2012), a dataset with 2M training images and 1K classes [24]. We used an AlexNet [1] with 5 folds and 2 fully connected layers, modified with batch-standard layers. These experiments were conducted in MatConvnet [25] using SGD without dynamics and a batch size of 512. Prior to benchmarking, we tested whether weight sections were required to obtain robust networks. Accordingly, two AlexNets were trained without predictions for 20K iterations (8.5 epochs): one without weight section (Tr-None-NC) and one with weight section (Tr-None-C). In both cases, the top-5 error for Te-None, Te-Round-None and Te-Sign-Sign was reported for observation of Tr-C."}, {"heading": "4.2.1 ImageNet benchmarks", "text": "We benchmarked Tr-StochM3-C2 in ImageNet. This network was trained for 150K iterations (64 eras), the learning rate decreased from 0.1 x 0.01 x at 100K and 125K iterations, and we also increased clip values by scaling the global clip factor f from 4.5 by 1.4 x at 5K, 10K and 15K iterations, resulting in the values of Te-None and Te-Round remaining in sync while the network reached lower errors at the same time. 3 We investigated the robustness of the network using Te-Power and compared test errors with two current models using binary weights (Figure 4)."}, {"heading": "5 Hypotheses on learning robust networks", "text": "This method reaches 22.5% after 100K iterations. 2StochM3 is the trivalent counterpart to StochM. Specifically, the mean 50% of weights can act as regulators, similar to DropConnect [26]. This idea was first proposed in [10] for the case of weight binarization. Here, we examine how impressive weight clipping (without weight predictions) can also act as a regulator in the context of proximal methods; see [27] for an excellent review of proximal methods."}, {"heading": "6 Conclusion and future work", "text": "Here we show that the introduction of certain weight distortions during training leads to a regime in which the network becomes robust not only against this distortion, but also against an entire family of distortions. On the basis of this observation, we have proposed a new rule that stochastically projects each weight into a random interval based on its current value. We assume that this rule, similar to the stochastic projection rule in BinaryConnect, does not directly optimize the weight values, but instead optimizes the neighborhood of the weight vector. In practice, our rule results in the performance of CIFAR-10 being state-of-the-art for both binary and non-binary weighted networks."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K Esser", "Paul A Merolla", "John V Arthur", "Andrew S Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J Berg", "Jeffrey L McKinstry", "Timothy Melano", "Davis R Barch"], "venue": "arXiv preprint arXiv:1603.08270,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Steve K Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V Arthur", "Dharmendra S Modha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms", "author": ["Evangelos Stromatias", "Daniel Neil", "Michael Pfeiffer", "Francesco Galluppi", "Steve B Furber", "Shih-Chii Liu"], "venue": "Frontiers in neuroscience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Neural network adaptations to hardware implementations", "author": ["Perry Moerland", "Emile Fiesler"], "venue": "Technical report, IDIAP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 1, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 2, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 3, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 4, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 136, "endOffset": 142}, {"referenceID": 6, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 249, "endOffset": 255}, {"referenceID": 7, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 249, "endOffset": 255}, {"referenceID": 8, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 7, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 5, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 9, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 8, "context": "It has been argued, rather reasonably, that this training procedure is essential to learning low precision representations [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Based on these observations, we propose a new stochastic projection rule inspired by BinaryConnect [10], except our rule projects weights to random intervals (as opposed to quantized values).", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 39, "endOffset": 47}, {"referenceID": 8, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 39, "endOffset": 47}, {"referenceID": 7, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 11, "context": "[13] proposed a similar rule for contrastive divergence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Another recent approach approximates weights using binary values during training, formulated as a constrained optimization problem, with promising results on ImageNet [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "Other approaches have developed a probabilistic interpretation of discrete weights [14, 11], however, they have not yet been extended to convnets or datasets larger than MNIST.", "startOffset": 83, "endOffset": 91}, {"referenceID": 9, "context": "Other approaches have developed a probabilistic interpretation of discrete weights [14, 11], however, they have not yet been extended to convnets or datasets larger than MNIST.", "startOffset": 83, "endOffset": 91}, {"referenceID": 5, "context": "While normalizing across the entire layer seems crude compared to a filter-wise normalization (as in [7]), we have found that the two cases lead to similar results.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "The procedure that we use to train is described in Algorithm 1, which is similar to BinaryConnect [10] except we allow for arbitrary weight projections.", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "ck is defined as the standard deviation of the initialized weights (from [15]) scaled by a global factor f , where f = 0.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "CIFAR-10 is an image classification dataset of small (32\u00d7 32) color images with 10 categories [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "To train, we use the ADAM learning rule [17] with a learning rate of 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "003, and a batch size of 50; a square hinge loss; and batch normalization [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "These results were obtained in TensorFlow [19], except for a control network that used Caffe [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "These results were obtained in TensorFlow [19], except for a control network that used Caffe [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "Our experimental flow is as follows: Using the training set pre-processed with global contrast normalization and ZCA whitening from Pylearn2 [21], we trained six networks for 500 epochs using a DNN with 6 Conv and 2 FC layers (Figure 1A).", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "2% \u03b2 \u2208 U[0, 2] Tr-StochM-C 7.", "startOffset": 8, "endOffset": 14}, {"referenceID": 8, "context": "4% BC-Sign [10] 9.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "BC-Stoch [10] 8.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Finally for Te-Power, each weight is normalized to [0, 1], raised to the power \u03b2 \u2208 [0, 2], and multiplied by its sign.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "Finally for Te-Power, each weight is normalized to [0, 1], raised to the power \u03b2 \u2208 [0, 2], and multiplied by its sign.", "startOffset": 83, "endOffset": 89}, {"referenceID": 1, "context": "Accordingly, we trained a network using the Power projection (Tr-Power-C) where a new \u03b2 \u2208 [0, 2] is drawn for each minibatch.", "startOffset": 90, "endOffset": 96}, {"referenceID": 20, "context": "This conflicts with previous findings where crude quantization does not lead to good performance [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "Inspired by the Stoch projection first introduced in [10], we constructed a new stochastic projection rule, StochM.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "same as wki, which was originally thought to be important for these stochastic projections to work properly [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "2M training images and 1K classes [24].", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "We use an AlexNet [1] with 5 convolution and 2 fully connected layers, modified with batch norm layers.", "startOffset": 18, "endOffset": 21}, {"referenceID": 22, "context": "These experiments were run in MatConvnet [25] using SGD with no momentum and a batch size of 512.", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "6% BC-Sign [7] 39%", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "BWN [7] 23%", "startOffset": 4, "endOffset": 7}, {"referenceID": 23, "context": "First, we explore the idea that imposing constraints on weights can act as a regularizer, similar to DropConnect [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "This idea was first suggested in [10] for the case of weight binarization.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "For future work, one could also try other operations on weights commonly used in proximal methods; see [28] for preliminary work in this vein.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "For instance, recent work from [7] has already used a projection that depends on the `1 norm of the weights for each filter, although their work is in a slightly different context from ours.", "startOffset": 31, "endOffset": 34}, {"referenceID": 25, "context": "Viewing these distortions as a type of noise may bridge our findings with recent work that suggests adding explicit gradient noise results in better overall performance [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 26, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}, {"referenceID": 5, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}, {"referenceID": 7, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}], "year": 2016, "abstractText": "Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic\u2014 namely, projecting quantized weights during backpropagation\u2014can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.", "creator": "LaTeX with hyperref package"}}}