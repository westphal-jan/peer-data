{"id": "1606.01990", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features", "abstract": "Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.", "histories": [["v1", "Tue, 7 Jun 2016 01:17:00 GMT  (118kb,D)", "http://arxiv.org/abs/1606.01990v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["attapol t rutherford", "vera demberg", "nianwen xue"], "accepted": false, "id": "1606.01990"}, "pdf": {"name": "1606.01990.pdf", "metadata": {"source": "CRF", "title": "Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features", "authors": ["Attapol T. Rutherford", "Vera Demberg", "Nianwen Xue"], "emails": ["teruth@yelp.com", "vera@coli.uni-saarland.de", "xuen@brandeis.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "The prevailing approach to this task is the use of surface features derived from various semantic lexicon (Pitler et al., 2009), which reduces the number of parameters by using blank markers in the arguments of discourse relationships to a limited number of entries in a semantic lexicon such as polarity and verb class. Along the same vein, brown cluster assignments are also used as a general purpose lexicon that does not require human manual annotation (Rutherford and Xue, 2014). However, these solutions still suffer from the problem of data economy and require a comprehensive set of functions to function well (Park and Cardie, 2012; Lin et al., 2009; Ji and Eisenstein, 2015). The work we are researching here, the use of the expressive representations of distributed representations to overcome data saving almost always requires a comprehensive set of functions to function well (the Park and Let, 2012)."}, {"heading": "3 Model Architectures", "text": "Following on from previous work, we assume that the two arguments for an implicit discourse relationship are given so that we can focus on predicting the senses of the implicit discourse relationships. Input to our model is a pair of text segments called Arg1 and Arg2, and the label is one of the senses defined in the Penn Discourse Treebank, as in the following example: Input: Arg1 Senator Pete Domenici calls this effort \"the first gift of democracy\" Arg2 Poles would do better to consider it a Trojan horse. Output: Sense Comparison.ContrastIn all architectures, every word of the argument is presented as a k-dimensional word vector trained on an uncommented dataset. We use various model architectures to transform the semantics represented by the word vectors into distributed continuously evaluated characteristics."}, {"heading": "3.1 Bag-of-words Feedforward Model", "text": "This model does not model the structure or word order of a sentence; the features are simply weighted by elementary pooling functions = i =. Pooling is one of the key techniques in neural network modeling of computer vision (Krizhevsky et al., 2012; LeCun et al., 2010). Max pooling is known to be very effective in vision, but it is unclear which pooling function works well when it comes to pooling word vectors. Summation pooling and Mean pooling claim to compose the meaning of a short phrase from individual word vectors well (Le and Mikolov, 2014; Blacoe and Lapata, 2012; Mikolov et al., 2013b; Braud and Denis, 2015). The Arg1 vectors a1 and Arg2 vector a2 are calculated by applying element pooling function f to all N1 word vectors Arg1 = 1 and Ng2 word vectors = 3."}, {"heading": "3.2 Sequential Long Short-Term Memory (LSTM)", "text": "A sequential Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) models the semantics of a word sequence by using hidden state vectors. Therefore, the word sequence influences the resulting hidden state vectors, in contrast to the sack-of-word model. For each word vector at word position t, we calculate the corresponding hidden state vector st and the memory cell vector ct from the previous step. It = sigmoid (Wi \u00b7 wt + Ui \u00b7 st \u2212 1 + bi) ft = sigmoid (Wf \u00b7 wt + Uf \u00b7 st \u2212 1 + bf) ot = sigmoid (Wo \u00b7 wt + Uo \u00b7 st \u2212 1 + bo) c \u2032 t = tanh (Wc \u00b7 wt + Ui \u00b7 st \u2212 1 + bc) ft = sigmoid (Wf \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t = ct \u00b2 t \u00b2 t = t \u00b2 t \u00b2 t = multiplication = moid."}, {"heading": "3.3 Tree LSTM", "text": "The principle of compositivity leads us to believe that the semantics of the argument vector should be determined by the syntactic structures and the meanings of the components. To allow a fair comparison with the sequential model, we apply the same formulation of LSTM to the binary parse tree of components. The hidden state vector now corresponds to an ingredient in the tree. These hidden state vectors are then used in the same way as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015). This model is similar to the recursive neural networks proposed by Ji and Eisenstein (2015). Our model differs from their model in several respects. We use the LSTM networks instead of the \"vanilla\" RNN formula and expect better results due to fewer complications with vanished and exploding gradients during training. Furthermore, our purpose is to compare the influence of the model structures."}, {"heading": "4 Corpora and Implementation", "text": "We use the PDTB because of its theoretical simplicity in discourse analysis and its relatively large size; the note is made as another level on the Penn Treebank in the sections of the Wall Street Journal; each relationship consists of two ranges of text that are minimally required to derive the relationship, and the sense is hierarchically organized; the classification problem can be formulated in different ways based on the hierarchy; previous work in this task has been done via three assessment schemes: top-level 4-way classification (Pitler et al., 2009), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification introduced in the CoNLL 2015 Shared Task (Xue et al., 2015). We focus on second-level 11-way classification because the labels are fine grained enough to be useful for downstream tasks."}, {"heading": "5 Experiment on the Second-level Sense in the PDTB", "text": "We want to test the effectiveness of the interaction between the arguments and the three models described above using the fine-grained discourse relationships in English. Data sharing and label set are exactly the same as previous work using this label set (Lin et al., 2009; Ji et al., 2015). We use the Berkeley parser to analyze all data (Petrov et al., 2006). We test the effects of word vector sizes. 50-dimensional and 100-dimensional word vectors are trained on the training parts of the WSJ data, which is the same text as the PDTB note. Although this seems to be little data, 50-dimensional WSJ-trained word vectors have previously proven to be the most effective (Ji and Eisenstein, 2015)."}, {"heading": "5.1 Results and discussion", "text": "It's the first time that we're able to assert ourselves, that we're able to hide ourselves, and that we're able to hide ourselves, \"he said."}, {"heading": "5.2 Discussion", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "6 Extending the results across label sets and languages", "text": "We would like to extend our findings to other labels and languages by evaluating our models using non-explicit discourse relationship data used in the English and Chinese CoNLL 2016 Shared Task. We will have more confidence in our model if it works well across labels. It is also important that our model works across languages, as other languages may not have the resources such as semantic lexicographs or parsers required by some previously used functions."}, {"heading": "6.1 English discourse relations", "text": "We are following the experimental framework used in CoNLL 2015-2016 Shared Task to compare our results with previous systems. This setting differs from the previous experiment in several respects. Entity relations (EntRel) and alternative lexicalizing relationships (AltLex) are included in this setting. The label set is modified by the organizers of the joint task in 15 different senses, including EntRel as a broader sense (Xue et al., 2015). We use the 300-dimensional word vector used in the previous experiment and match the number of hidden layers and hidden units on the development set. The best results from the joint task from last year are used as a strong baseline. Only surface features are used and also achieve the current performance under this label set (Wang and Lan, 2015). These features are similar to those of Lin et al. (2009)."}, {"heading": "6.2 Chinese discourse relations", "text": "We evaluate our model on the basis of the Chinese Discourse Bank (CDTB), because its annotations are most closely comparable to the PDTB (Zhou and Xue, 2015).The Sense Theorem consists of 10 different senses, which, unlike the PDTB, are not organized in a hierarchy. We use the version of the data provided to participants in the CoNLL 2016 Shared Task. This version comprises a total of 16,946 cases of discourse relationships in the combined Training and Development Sets. The test set is not yet available at the time of submission, so the system is loaded with the previously proven effective feature sets for English, namely Dependency Control Pairs, Production Control Pairs (Lin al, 2009), and Gigu-Cluster Architecture (respectively).We evaluate our model on the basis of the Chinese Discourse Bank (CDTB), because their annotations are most similar to the PDTB (Zhou and Xue, 2015).This version includes a total of 16,946 cases of discourse relationships in the combined Training and Development Sets. The test set is not yet available at the time of submission, so the system is based on the average accuracy of over 7-fold cross-validation based on the combined Training and Development Sectors. There is no previously published baseline for Chinese."}, {"heading": "6.3 Results", "text": "The feedback variant of our model performs significantly better than the strong baseline in both English and Chinese (p < 0.05 bootstrap test), indicating that our approach is robust against different markup groups and that our results are valid across all languages. Our Chinese model outperforms all the features that work well in English, even though it only uses word vectors.The choice of neural architecture used to generate Chinese word vectors proves to be crucial. Chinese word vectors from the Skipgram model consistently perform better than those from the CBOW model (Figure 5). These two types of word vectors do not differ much in English tasks."}, {"heading": "7 Conclusions and future work", "text": "We report on a series of experiments that systematically examine the effectiveness of different neural network architectures in the task of classifying implicit discourse relationships. Given the small amount of commented data, we found that a feedback variant of our model, combined with hidden layers and high-dimensional word vectors, outperforms more complicated LSTM models. Our model performs better or more competitively than models that use manually generated surface features, and it is the first neural CDTB-style discourser in China. We will make our code and models publicly available."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Braud", "Denis2015] Chlo\u00e9 Braud", "Pascal Denis"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Braud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braud et al\\.", "year": 2015}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["\u0130rsoy", "Cardie2014] Ozan \u0130rsoy", "Claire Cardie"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "One vector is not enough: Entityaugmented distributed semantics for discourse relations. Transactions of the Association for Computational Linguistics, 3:329\u2013344", "author": ["Ji", "Eisenstein2015] Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "A latent variable recurrent neural network for discourse relation language models", "author": ["Ji et al.2016] Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Segmented discourse representation theory: Dynamic semantics with discourse structure", "author": ["Lascarides", "Asher2007] Alex Lascarides", "Nicholas Asher"], "venue": "In Computing meaning,", "citeRegEx": "Lascarides et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lascarides et al\\.", "year": 2007}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Convolutional networks and applications in vision", "author": ["LeCun et al.2010] Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on,", "citeRegEx": "LeCun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015] Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Lin et al.2009] Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Vol-", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["Mann", "Thompson1988] William C Mann", "Sandra A Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["Marcu", "Echihabi2002] Daniel Marcu", "Abdessamad Echihabi"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving implicit discourse relation recognition through feature set optimization", "author": ["Park", "Cardie2012] Joonsuk Park", "Claire Cardie"], "venue": "In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Park et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Park et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al.2006] Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Easily identifiable discourse relations", "author": ["Pitler et al.2008] Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind K Joshi"], "venue": "Technical Reports (CIS),", "citeRegEx": "Pitler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Pitler et al.2009] Emily Pitler", "Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Pitler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "The penn discourse treebank 2.0", "author": ["Prasad et al.2008] Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi", "Bonnie L Webber"], "venue": "In LREC. Citeseer", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Rutherford", "Xue2014] Attapol T. Rutherford", "Nianwen Xue"], "venue": "In Proceedings of the 14th Conference of the European Chapter", "citeRegEx": "Rutherford et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A refined end-to-end discourse parser", "author": ["Wang", "Lan2015] Jianxiang Wang", "Man Lan"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The conll-2015 shared task on shallow discourse parsing", "author": ["Xue et al.2015] Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol Rutherford"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natu-", "citeRegEx": "Xue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2015}, {"title": "Pdtb-style discourse annotation of chinese text", "author": ["Zhou", "Xue2012] Yuping Zhou", "Nianwen Xue"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "The chinese discourse treebank: A chinese corpus annotated with discourse relations. Language Resources and Evaluation, 49(2):397\u2013431", "author": ["Zhou", "Xue2015] Yuping Zhou", "Nianwen Xue"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "The discourse structure of a natural language text has been analyzed and conceptualized under various frameworks (Mann and Thompson, 1988; Lascarides and Asher, 2007; Prasad et al., 2008).", "startOffset": 113, "endOffset": 187}, {"referenceID": 28, "context": "The Penn Discourse TreeBank (PDTB) and the Chinese Discourse Treebank (CDTB), currently the largest corpora annotated with discourse structures in English and Chinese respectively, view the discourse structure of a text as a set of discourse relations (Prasad et al., 2008; Zhou and Xue, 2012).", "startOffset": 252, "endOffset": 293}, {"referenceID": 28, "context": "\u2217Work performed while being a student at Brandeis connective taking two text segments as arguments (Prasad et al., 2008).", "startOffset": 99, "endOffset": 120}, {"referenceID": 26, "context": "relatively easy, as the discourse connective itself provides a strong cue for the discourse relation (Pitler et al., 2008), the classification of implicit discourse relations has proved to be notoriously hard and it has remained one of the last missing", "startOffset": 101, "endOffset": 122}, {"referenceID": 33, "context": "pieces in an end-to-end discourse parser (Xue et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 27, "context": "These features include word pairs that are the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014).", "startOffset": 209, "endOffset": 256}, {"referenceID": 27, "context": "The prevailing approach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009), reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes.", "startOffset": 104, "endOffset": 125}, {"referenceID": 17, "context": "However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009; Ji and Eisenstein, 2015).", "startOffset": 136, "endOffset": 202}, {"referenceID": 6, "context": "LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al.", "startOffset": 115, "endOffset": 154}, {"referenceID": 7, "context": "LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al.", "startOffset": 115, "endOffset": 154}, {"referenceID": 6, "context": "LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al., 2015). Many neural architectures have been explored and evaluated, but there is no single technique that is decidedly better across all tasks. The LSTM-based models such as Kiros et al. (2015) perform well across tasks but do not outperform some other strong neural baselines.", "startOffset": 116, "endOffset": 448}, {"referenceID": 6, "context": "LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al., 2015). Many neural architectures have been explored and evaluated, but there is no single technique that is decidedly better across all tasks. The LSTM-based models such as Kiros et al. (2015) perform well across tasks but do not outperform some other strong neural baselines. Ji et al. (2016) uses a joint discourse language model to improve the performance on the coarse-grained label in the PDTB, but in our case, we would like", "startOffset": 116, "endOffset": 549}, {"referenceID": 12, "context": "Pooling is one of the key techniques in neural network modeling of computer vision (Krizhevsky et al., 2012; LeCun et al., 2010).", "startOffset": 83, "endOffset": 128}, {"referenceID": 15, "context": "Pooling is one of the key techniques in neural network modeling of computer vision (Krizhevsky et al., 2012; LeCun et al., 2010).", "startOffset": 83, "endOffset": 128}, {"referenceID": 30, "context": "The mathematical formulation is the same as Tai et al. (2015).", "startOffset": 44, "endOffset": 62}, {"referenceID": 27, "context": "Previous work in this task has been done over three schemes of evaluation: top-level 4-way classification (Pitler et al., 2009), second-level 11-way classification (Lin et al.", "startOffset": 106, "endOffset": 127}, {"referenceID": 17, "context": ", 2009), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification introduced in the CoNLL 2015 Shared Task (Xue et al.", "startOffset": 44, "endOffset": 87}, {"referenceID": 33, "context": ", 2009; Ji and Eisenstein, 2015), and modified second-level classification introduced in the CoNLL 2015 Shared Task (Xue et al., 2015).", "startOffset": 116, "endOffset": 134}, {"referenceID": 1, "context": "Training Weight initialization is uniform random, following the formula recommended by Bengio (2012). The cost function is the standard crossentropy loss function, as the hinge loss function (large-margin framework) yields consistently in-", "startOffset": 87, "endOffset": 101}, {"referenceID": 2, "context": "Implementation All of the models are implemented in Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 59, "endOffset": 104}, {"referenceID": 0, "context": "Implementation All of the models are implemented in Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 59, "endOffset": 104}, {"referenceID": 17, "context": "The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009; Ji and Eisenstein, 2015).", "startOffset": 96, "endOffset": 139}, {"referenceID": 20, "context": "Preprocessing All tokenization is taken from the gold standard tokenization in the PTB (Marcus et al., 1993).", "startOffset": 87, "endOffset": 108}, {"referenceID": 25, "context": "We use the Berkeley parser to parse all of the data (Petrov et al., 2006).", "startOffset": 52, "endOffset": 73}, {"referenceID": 17, "context": "56 Lin et al., (2009) 40.", "startOffset": 3, "endOffset": 22}, {"referenceID": 24, "context": "Other models such as GloVe and continuous bag-of-words seem to yield broadly similar results (Pennington et al., 2014).", "startOffset": 93, "endOffset": 118}, {"referenceID": 17, "context": "05; bootstrap test) and performs comparably with the surface feature baseline (Lin et al., 2009), which uses various lexical and syntactic features and extensive feature selection.", "startOffset": 78, "endOffset": 96}, {"referenceID": 31, "context": "have (Vinyals et al., 2015; Chiu and Nichols, 2015; \u0130rsoy and Cardie, 2014).", "startOffset": 5, "endOffset": 75}, {"referenceID": 30, "context": "Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015).", "startOffset": 102, "endOffset": 137}, {"referenceID": 16, "context": "Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015).", "startOffset": 102, "endOffset": 137}, {"referenceID": 29, "context": "For example, Vinyals et al. (2015) use sequential LSTM to encode the features for syntactic parse output.", "startOffset": 13, "endOffset": 35}, {"referenceID": 33, "context": "The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015).", "startOffset": 115, "endOffset": 133}, {"referenceID": 17, "context": "These features are similar to the ones used by Lin et al. (2009).", "startOffset": 47, "endOffset": 65}, {"referenceID": 17, "context": "To establish baseline comparison, we use MaxEnt models loaded with the feature sets previously shown to be effective for English, namely dependency rule pairs, production rule pairs (Lin et al., 2009), Brown cluster pairs (Rutherford and Xue, 2014), and word pairs (Marcu and Echihabi, 2002).", "startOffset": 182, "endOffset": 200}], "year": 2016, "abstractText": "Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.", "creator": "LaTeX with hyperref package"}}}