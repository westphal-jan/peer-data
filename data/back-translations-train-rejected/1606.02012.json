{"id": "1606.02012", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Can neural machine translation do simultaneous translation?", "abstract": "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.", "histories": [["v1", "Tue, 7 Jun 2016 03:38:46 GMT  (3608kb,D)", "http://arxiv.org/abs/1606.02012v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kyunghyun cho", "masha esipova"], "accepted": false, "id": "1606.02012"}, "pdf": {"name": "1606.02012.pdf", "metadata": {"source": "CRF", "title": "Can neural machine translation do simultaneous translation?", "authors": ["Kyunghyun Cho", "Masha Esipova"], "emails": ["kyunghyun.cho@nyu.edu", "masha.esipova@nyu.edu"], "sections": [{"heading": null, "text": "We are exploring the potential of attention-based neural machine translation in simultaneous translation. We are introducing a novel decoding algorithm, called simultaneous greedy decoding, which allows an existing neural machine translation model to start translating before a full source sentence is available. This approach is unique compared to previous work on simultaneous translation, as segmentation and translation are carried out jointly to maximize translation quality, and the translation of each segment is heavily dependent on all previous segments. This paper is a first step towards building a full simultaneous translation system based on neural machine translation."}, {"heading": "1 Introduction", "text": "In simultaneous translation, the goal of a translator or translation system is actually defined as a combination of quality and delay, as opposed to consecutive translation, where the quality of the translation is all that matters. To minimize delays while maximizing quality, a simultaneous translator must begin to generate symbols in a target language before receiving a full source sentence. Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al.). A segmentation algorithm or model first splits a source sentence into phrases. Each phrase is separated by an underlying, often black box, translation system, largely independent of the preceding phrases."}, {"heading": "2 Attention-based Neural Translation", "text": "Neural machine translation (Forcada and N'eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has recently become an important alternative to the existing statistical phrase system (Koehn et al., 2003). For example, in the translation task of WMT et al., 2015; attention-based machine translation is used as a composition of three module encoders, decoders, and attention mechanisms; the encoder is typically implemented as a recursive network that reads a source sentence X = (x1,., xTx) and returns a number of contexts."}, {"heading": "3 Simultaneous Greedy Decoding", "text": "We are exploring the potential of using a trained neural machine translator as a simultaneous translator by introducing a novel decoding algorithm. In this section, we propose and describe in detail such an algorithm as simultaneous greedy decoding."}, {"heading": "3.1 Algorithm", "text": "The question of the \"why\" - \"why\" - \"why\" - \"why\" - \"\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\""}, {"heading": "3.2 Waiting Criteria \u039b", "text": "At any given time, simultaneous greedy decoding makes a decision on whether to wait for a next set of source words or create a target symbol based on the current context (C-C-in line 10 of Alg. 1.). In this paper, we examine two different but related criterias.Criterion 1: Wait-if-worse The first alternative is wait-if-worse. It takes into account whether confidence in the prediction decreases based on the current source code (up to s source words) with more source context (up to s source words). This is most likely by comparing the log probabilities of the selected target word when only the first s source words were considered, i.e. when the prediction (C-C words) is most likely."}, {"heading": "3.3 Delay in Translation", "text": "For each decoded target symbol y-t, we can track how many source symbols were required, i.e., in line 10 of Alg. 1. We will use s (t) to specify this quantity. Then, we define the (normalized) total effort required to translate a source set or an equivalent delay in the translation as 0 < \u03c4 (X, Y) = 1 | X | | Y | | x = 1 s (t) \u2264 1. (6) In the case of full translation as opposed to simultaneous translation, this results in the fact that s (t) = | X | for all. In the case of word-for-word translation, in which case | X | Y | as opposed to simultaneous translation (X, Y) = 0.5, because s (t) = t.Alignment vs. Delay s (t) does not exactly reflect which prefix reflects the source between the target symbol, which is more useful (X, Y)."}, {"heading": "4 Related Work", "text": "Perhaps unsurprisingly, there is only a small number of previous work on simultaneous machine translation, many of which are done in the context of language translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, the incoming speech is transcribed by an automatic speech recognition system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic terms (e.g. silence or punctuation marks), each of which is then translated largely independently by a separate machine translation system. Simultaneous greedy decoding with neural machine translation proposed in this paper is clearly different from these approaches in (1) segmentation and translation (3)."}, {"heading": "5 Experimental Settings", "text": "In this context, it should also be mentioned that the measures we have mentioned are not measures taken by the EU, but measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU. (...) It is about measures taken by the EU."}, {"heading": "6 Quantitative Analysis", "text": "This trade-off is observed independently of the maintenance criterion, although it is more obvious with the Wait-If-Diff. Of two maintenance criteria, the Wait-If-Worse tends to achieve better translation quality, while it has a much higher delay in translation. Despite this general trend, we notice significant differences between the three languages we have considered. Firstly, in the case of German, we see the trade-off between delay and quality maintained in both translation directions (De and De \u2192 En), while the delay in translation into English slightly decreases. A similar trend is observed with Czech, except that the Wait-If-Diff criterion tends to improve translation quality while the delay is maintained. On the other hand, experiments with Russian show that this trend is not universal across languages."}, {"heading": "7 Qualitative Analysis", "text": "We define a measure of the quality-to-delay ratio as Q2D = BLEU\u03c4, where \u03c4 is an average delay over a test corpus. We use this ratio, which favors a translation system with high score and low delay, to select the best model for each language pair direction.In Fig. 3, we present the simultaneous translation examples of the selected models when translating from English into a target language. We switch between red and blue colors to indicate the correspondence between the target symbols and their source context.The starting sentence is divided into blocks based on s \u2032 (t), as defined in Fig. 3.3. We see that the Wait-If word is relatively conservative than the Wait-If diff previously observed in Fig. 1."}, {"heading": "7.1 Ru-En Simultaneous Translation", "text": "In Fig. 4, we show the Russian translations of \"All effects of vitamin D on cancer are not clear.\" The most obvious observation we can make is again that some words or phrases are rendered as translated. For example, in Fig. 4 (c), we see that \"a) vs. (b-c)\" another remarkable phenomenon occurs with the \"Wait-If-Diff,\" which consists in some words or phrases being rendered as translated. Unlike in Fig. 4 (c), we see that \"a) vs. (b-c)\" another striking example was repeated twice with the \"Wait.\" This has been observed with other sentences."}, {"heading": "8 Discussion and Future Research", "text": "The quantitative analyses revealed two important results: First, we found that it is indeed possible to use the neural machine translation model described in Alg. 1 without simultaneous translation as a target for the purpose of simultaneous machine translation; the algorithm, which has two adjustable parameters - s0 and \u03b4 - enables the user to target translation delays and quality, as in Fig. 1. The second finding is that this trade-off algorithm strongly depends on the choice of the wait-off criterion; two criteria introduced in this paper showed clearly conflicting behaviours, with the Wait-IfWorse being sensitive to s0, while the other, WaitIf-Diff, was responsible for the translation; we suspect that this difference in sensitivity led to strong qualitative differences, such as those discussed in Sec. 7.1."}, {"heading": "Acknowledgments", "text": "KC thanks Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Real-time incremental speech-to-speech translation of dialogs", "author": ["Srinivas Bangalore", "Vivek Kumar Rangarajan Sridhar", "Prakash Kolan", "Ladan Golipour", "Aura Jimenez."], "venue": "NAACL, pages 437\u2013445.", "citeRegEx": "Bangalore et al\\.,? 2012", "shortCiteRegEx": "Bangalore et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 Iii", "John Langford", "Daniel Marcu."], "venue": "Machine learning, 75(3):297\u2013325.", "citeRegEx": "Iii et al\\.,? 2009", "shortCiteRegEx": "Iii et al\\.", "year": 2009}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "ACL.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "NAACL.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Recursive hetero-associative memories for translation", "author": ["Mikel L. Forcada", "Ram\u00f3n P. \u00d1eco."], "venue": "IWANN\u201997, pages 453\u2013462.", "citeRegEx": "Forcada and \u00d1eco.,? 1997", "shortCiteRegEx": "Forcada and \u00d1eco.", "year": 1997}, {"title": "Simple, lexicalized choice of translation timing for simultaneous speech translation", "author": ["Tomoki Fujita", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "INTERSPEECH, pages 3487\u2013 3491.", "citeRegEx": "Fujita et al\\.,? 2013", "shortCiteRegEx": "Fujita et al\\.", "year": 2013}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin Grissom II", "He He", "Jordan L Boyd-Graber", "John Morgan", "Hal Daum\u00e9 III."], "venue": "EMNLP, pages 1342\u2013 1352.", "citeRegEx": "II et al\\.,? 2014", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, 2:3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Optimizing segmentation strategies for simultaneous speech translation", "author": ["Yusuke Oda", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "ACL, pages 551\u2013556.", "citeRegEx": "Oda et al\\.,? 2014", "shortCiteRegEx": "Oda et al\\.", "year": 2014}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Segmentation strategies for streaming speech translation", "author": ["Vivek Kumar Rangarajan Sridhar", "John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan."], "venue": "HLT-NAACL, pages 230\u2013238.", "citeRegEx": "Sridhar et al\\.,? 2013", "shortCiteRegEx": "Sridhar et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Incremental segmentation and decoding strategies for simultaneous translation", "author": ["Mahsa Yarmohammadi", "Vivek Kumar Rangarajan Sridhar", "Srinivas Bangalore", "Baskaran Sankaran."], "venue": "IJCNLP, pages 1032\u20131036.", "citeRegEx": "Yarmohammadi et al\\.,? 2013", "shortCiteRegEx": "Yarmohammadi et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 99, "endOffset": 193}, {"referenceID": 8, "context": "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 99, "endOffset": 193}, {"referenceID": 18, "context": "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 99, "endOffset": 193}, {"referenceID": 20, "context": "Conventional approaches to simultaneous translation divide the translation process into two stages (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 99, "endOffset": 193}, {"referenceID": 7, "context": "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and \u00d1eco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 109, "endOffset": 180}, {"referenceID": 19, "context": "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and \u00d1eco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 109, "endOffset": 180}, {"referenceID": 0, "context": "In this paper, we study the problem of simultaneous translation in the context of neural machine translation (Forcada and \u00d1eco, 1997; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 109, "endOffset": 180}, {"referenceID": 7, "context": "Neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.", "startOffset": 27, "endOffset": 130}, {"referenceID": 10, "context": "Neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.", "startOffset": 27, "endOffset": 130}, {"referenceID": 19, "context": "Neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.", "startOffset": 27, "endOffset": 130}, {"referenceID": 0, "context": "Neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al.", "startOffset": 27, "endOffset": 130}, {"referenceID": 11, "context": ", 2015), has recently become a major alternative to the existing statistical phrase-based machine translation system (Koehn et al., 2003).", "startOffset": 117, "endOffset": 137}, {"referenceID": 0, "context": "For instance, in the translation task of WMT\u201916, the top rankers for En\u2194Cs, En\u2194De, En\u2192Fi and En\u2192Ru all used attention-based neural machine translation (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 151, "endOffset": 194}, {"referenceID": 13, "context": "For instance, in the translation task of WMT\u201916, the top rankers for En\u2194Cs, En\u2194De, En\u2192Fi and En\u2192Ru all used attention-based neural machine translation (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 151, "endOffset": 194}, {"referenceID": 13, "context": "Instead of a vanilla, unidirectional recurrent network (Luong et al., 2015), it is possible to use a more sophisticated network such as a bidirectional recurrent network (Bahdanau et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": ", 2015), it is possible to use a more sophisticated network such as a bidirectional recurrent network (Bahdanau et al., 2015) or a tree-based recursive network (Eriguchi et al.", "startOffset": 102, "endOffset": 125}, {"referenceID": 5, "context": ", 2015) or a tree-based recursive network (Eriguchi et al., 2016).", "startOffset": 42, "endOffset": 65}, {"referenceID": 14, "context": "The decoder is a conditional language model based on a recurrent network (Mikolov et al., 2010).", "startOffset": 73, "endOffset": 95}, {"referenceID": 13, "context": "This content-based attention mechanism can be extended to incorporate also the location of each context vector (Luong et al., 2015).", "startOffset": 111, "endOffset": 131}, {"referenceID": 1, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 66, "endOffset": 160}, {"referenceID": 8, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 66, "endOffset": 160}, {"referenceID": 18, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 66, "endOffset": 160}, {"referenceID": 20, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 66, "endOffset": 160}, {"referenceID": 1, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, incoming speech is transcribed by an automatic speech recognition (ASR) system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic cues (e.g., silence or punctuation marks.) Each of these segments is then translated largely independent from each other by a separate machine translation system. The simultaneous greedy decoding with neural machine translation, proposed in this paper, is clearly distinguished from these approaches in that (1) segmentation and translation happen jointly to maximize the translation quality and (2) each segmentation is strongly dependent on all the previous segment in both the source and target sentences. More recently, Grissom II et al. (2014) and Oda et al.", "startOffset": 67, "endOffset": 901}, {"referenceID": 1, "context": "Much of those works are done in the context of speech translation (Bangalore et al., 2012; Fujita et al., 2013; Sridhar et al., 2013; Yarmohammadi et al., 2013). Often, incoming speech is transcribed by an automatic speech recognition (ASR) system, and the transcribed text is segmented into a translation unit largely based on acoustic and linguistic cues (e.g., silence or punctuation marks.) Each of these segments is then translated largely independent from each other by a separate machine translation system. The simultaneous greedy decoding with neural machine translation, proposed in this paper, is clearly distinguished from these approaches in that (1) segmentation and translation happen jointly to maximize the translation quality and (2) each segmentation is strongly dependent on all the previous segment in both the source and target sentences. More recently, Grissom II et al. (2014) and Oda et al. (2014) proposed to extend those earlier approaches by introducing a trainable segmentation policy which is trained to maximizes the translation quality.", "startOffset": 67, "endOffset": 923}, {"referenceID": 17, "context": "2 All the sentences were first tokenized3 and segmented into subword units using byte pair encoding (BPE) following (Sennrich et al., 2015b).", "startOffset": 116, "endOffset": 140}, {"referenceID": 12, "context": "perl from Moses (Koehn et al., 2007).", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "We use a unidirectional recurrent network with 1028 gated recurrent units (GRU, (Cho et al., 2014)) as an encoder.", "startOffset": 80, "endOffset": 98}, {"referenceID": 21, "context": "Each model is trained with Adadelta (Zeiler, 2012) until the average log-probability on the validation set does not improve, which takes about a week per model.", "startOffset": 36, "endOffset": 50}, {"referenceID": 19, "context": "These trained models do not achieve the state-ofthe-art translation qualities, as they do not exploit the ensemble technique (Sutskever et al., 2014) nor monolingual corpus (Sennrich et al.", "startOffset": 125, "endOffset": 149}, {"referenceID": 16, "context": ", 2014) nor monolingual corpus (Sennrich et al., 2015a), both of which have been found to be crucial in improving neural machine translation.", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": "We use a unidirectional recurrent network with 1028 gated recurrent units (GRU, (Cho et al., 2014)) as an encoder. A decoder is similarly a recurrent neural net with 1028 GRUs. The soft-alignment function is a feedforward network with one hidden layer consisting of 1028 tanh units. Each model is trained with Adadelta (Zeiler, 2012) until the average log-probability on the validation set does not improve, which takes about a week per model. These trained models do not achieve the state-ofthe-art translation qualities, as they do not exploit the ensemble technique (Sutskever et al., 2014) nor monolingual corpus (Sennrich et al., 2015a), both of which have been found to be crucial in improving neural machine translation. Under these constraints, however, the trained models translates as well as those reported earlier by, for instance, Firat et al. (2016), as can be seen in Table 1.", "startOffset": 81, "endOffset": 864}, {"referenceID": 3, "context": "The beam width is set to 5, as used in (Chung et al., 2016).", "startOffset": 39, "endOffset": 59}], "year": 2016, "abstractText": "We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.", "creator": "TeX"}}}