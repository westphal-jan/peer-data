{"id": "1606.02228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Systematic evaluation of CNN advances on the ImageNet", "abstract": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc.", "histories": [["v1", "Tue, 7 Jun 2016 17:38:06 GMT  (2747kb,D)", "http://arxiv.org/abs/1606.02228v1", "Submitted to CVIU Special Issue on Deep Learning"], ["v2", "Mon, 13 Jun 2016 13:48:39 GMT  (2751kb,D)", "http://arxiv.org/abs/1606.02228v2", "Submitted to CVIU Special Issue on Deep Learning. Updated dataset quality experiment"]], "COMMENTS": "Submitted to CVIU Special Issue on Deep Learning", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["dmytro mishkin", "nikolay sergievskiy", "jiri matas"], "accepted": false, "id": "1606.02228"}, "pdf": {"name": "1606.02228.pdf", "metadata": {"source": "CRF", "title": "Systematic evaluation of CNN advances on the ImageNet", "authors": ["Dmytro Mishkin", "Nikolay Sergievskiy", "Jiri Matas"], "emails": [], "sections": [{"heading": null, "text": "The paper systematically examines the impact of a number of recent advances in CNN architectures and learning methods on the problem of object categorization (ILSVRC), assessing the impact of the following architectural decisions: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastical, max, average, mixed), network width, classification design (convolutional, fully networked, SPP), image pre-processing, and learning parameters: learning rate, batch size, cleanliness of data, etc. Performance gains of the proposed modifications are tested first individually and then in combination; the sum of individual gains is greater than the observed improvement when all modifications are introduced, but the \"deficit\" is small, suggesting the independence of their benefits; we show that the use of 128x128 pixel images is sufficient to draw qualitative conclusions about the optimal network structure, and the full size of the network structure."}, {"heading": "1. Introduction", "text": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many others.Besides two classic works on the formation of neural networks - [8] and [9], which are still highly relevant, there is very little guidance or theory about the plethora of design decisions and hyperparameter settings of CNNs with the consequence that researchers proceed by trial and error copying experiments and architecture by sticking to established network types. With good results in the ImageNet competition, Preprint, the Computer Vision and Image Understanding June 8, 2016ar Xiv: 160 6.02 228v 1 [cs.N E] 7J unthe AlexNet. \""}, {"heading": "2. Evaluation", "text": "The standard CaffeNet parameters and architecture are shown in Table 2, the full list of tested attributes in Table 1."}, {"heading": "2.1. Evaluation framework", "text": "All tested networks were trained on the ImageNet dataset [1] for the problem of 1000 object category classification. The set consists of a 1.2M image training, a 50K image validation set and a 100K image test set. The test set is not used in the experiments.Normal pre-processing involves image recalculation to 256xN using N \u2265 256, and then cutting out a random 224x224 square [10, 19].The setup achieves good classification results, but training a network of this size takes several days even on modern GPUs. We suggest that 1https: / / github.com / ducha-aiki / caffenet-benchmarkslimit the image size to 144xN, with N \u2265 128 (referred to as ImageNet128px), for example, the CaffeNet [20] within 24 hours with NVIA DIGT8X12x ImageNet128x."}, {"heading": "2.1.1. Architectures", "text": "The reduction in the input size is validated by training CaffeNet, GoogLeNet and VGGNet on both the reduced and the standard image sizes. Results are presented in Figure 1. Decreasing the input image size results in a consistent decrease in the top-1 accuracy of about 6% for all architectures popular there and does not change their relative sequence (VGGNet > GoogLeNet > CaffeNet) or accuracy difference. To reduce the likelihood of overmatch and make experiments less demanding in memory, a further change is made to CaffeNet. A number of filters in fully connected layers 6 and 7 have been reduced by a factor of two, from 4096 to 2048. Results validating the resolution reduction are presented in Figure 1. The parameters and architecture of the standard CaffeNet are shown in Table 2. For experiments, we used CaffeNet with 2 thin layers of fully connected NetaffeNet architecture, the Caffe48-128 can be called the Caffe48-128 architecture."}, {"heading": "2.1.2. Learning", "text": "SGD with a pulse of 0.9 is used for learning, the initial learning rate is set to 0.01 and decreased by a factor of ten after each 100K iteration until learning stops after 320K iterations. L2 weight drop in coil weights is set to 0.0005 and not applied to bias. Dropout [21] with a 0.5 probability is used before the last two layers. All networks are initialized with LSUV [22]. Biases are initialized to zero. Since the LSUV initialization works on the assumption that the variance of the input unit is maintained, the pixel intensities were scaled by 0.04 after subtracting the mean of the BGR pixel values (104 117 124)."}, {"heading": "3. Single experiments", "text": "This section is dedicated to experiments with a single hyperparameter or design selection per experiment."}, {"heading": "3.1. Activation functions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1. Previous work", "text": "The activation functions for neural networks are a current topic, many functions have been proposed since the discovery of ReLU [23]. The first group refers to ReLU, i.e. LeakyReLU [24] and Very Leaky ReLU [25], RReLU [26], PReLU [27] and its generalized version - APL [28], ELU [29]. Others are based on different ideas, e.g. maxout [30], MBA [31], etc. To the best of our knowledge, however, only a small fraction of these activation functions have been evaluated on ImageNetscale datasets. And when they have done so, e.g. ELU, the network architecture used in the evaluation was designed specifically for the experiment and is not widely used."}, {"heading": "3.1.2. Experiment", "text": "We have tested the most popular activation features and all with available or trivial implementations: ReLU, tanh, sigmoid, VLReLU, RReLU, PReLU, linear, maxout, APL, SoftPlus. Formulas and references are in Table 3. We have selected APL and maxout with two linear parts. Maxout is tested in two modifications: MaxW - with the same effective network width, which doubles the number of parameters and calculation costs due to the two linear parts, and MaxS - with the same computing complexity - with two thinner parts. In addition, we have tested \"optimally scaled\" Tanh, proposed by LeCun [8]. We have also tried to train sigmoid [32] networks, but the initial loss has never decreased. Finally, as suggested by Swietojanski et.al, we have tested the combination of ReLU for first layers and maxout for the last layer, which is the same computing power of EL.U."}, {"heading": "3.2. Pooling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1. Previous work", "text": "The most popular options are max pooling and average pooling. Recent advances include: Stochastic pooling [35], LP standard pooling [36] and tree-gated pooling [37]. Only the authors of the last paper tested their pooling on ImageNet. Pooling receptive field is another design choice. Krizhevskiy etal. [10] claimed the superiority of overlapping pooling with 3x3 window size and step width 2, while VGGNet [11] uses a non-overlapping 2x2 window."}, {"heading": "3.2.2. Experiment", "text": "We tested (see Table 4) the average, maximum, stochastic and incremental sum of average and maximum pooling proposed by Lee et al. [37] and skipped the pooling altogether by replacing it with incremental turns proposed by Springenberd et al. [38] We also tried tree and gated poolings [37], but we encountered convergence problems and the results were heavily dependent on the input image size. We do not know if it is a problem of implementation or the method itself and therefore omitted the results. The results are shown in Figure 3 on the left. Stochastic pooling had very poor results. To verify whether it is due to extreme randomization by stochastic pooling and dropout, we trained the network without the dropout. This reduced accuracy is even stronger. The best results were obtained by a combination of maximum and average pooling. Our assumption is that maximum selectivity and maximum selectivity is achieved."}, {"heading": "3.3. Learning rate policy", "text": "Learning rate is one of the most important hyperparameters influencing CNN's final performance. Surprisingly, the most commonly used learning rate decay policy is \"Reduce the learning rate 10x when the validation error stops declining,\" which is applied without a parameter search. Although this works well in practice, such a lazy policy can be suboptimal. We have tested four strategies for learning rate: step, square and square root decay (used for training GoogLeNet by BVLC [20]) and linear decay. The actual dynamics of the learning rate are shown in Figure 4, left. Validation accuracy is shown on the right. Linear decay provides the best results."}, {"heading": "3.4. Image pre-processing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1. Previous work", "text": "The most commonly used input to CNN is raw RGB pixels, and the commonly accepted recommendation is not to use pre-processing. There is not much research on the optimal color space or pre-processing techniques for CNN. Rachmadi and Purnama [39] examined different color spaces for vehicle color recognition, Dong et.al [40] compared YCrCb and RGB channels for high resolution images, Graham [41] extracted local average color from retinal images to emerge victorious from the Kaggle contest."}, {"heading": "3.4.2. Experiment", "text": "The pre-processing experiment is divided into two parts. First, we tested popular, handmade image preprocessing methods and color spaces. Since all transformations were performed on-the-fly, we first tested whether the calculation of the middle pixel and variance over the image section could be replaced by applying batch normalization to input images. It reduces the final accuracy by 0.3% and can be considered a starting point for all other methods. We tested HSV, YCrCb, Lab, RGB and single-channel grayscale color spaces. Results are shown in Figure 5. The experiment confirms that RGB is the most suitable color space for CNNs. Laboratory-based network has not improved the initial loss after 10K iterations. Removing color deformations from images costs between 5.8% and 5.2% of the accuracy, for OpenCV RGB2Gray and learned discoloration as Global [42] and CLA43 (HE] histogram."}, {"heading": "3.5. Batch normalization", "text": "Batch normalization [13] (BN) is a new method that solves the problem of exploding / disappearing gradients and ensures a near-optimal learning regime for the layer that follows batch normalization. Following [22], we first tested various options for where BN should be used - before or after non-linearity. The results presented in Table 7 are surprisingly contradictory: the CaffeNet architecture prefers ConvReLU-BN-Conv, while GoogLeNet - Conv-BN-ReLU-Conv-Placement. In addition, the results for GoogLeNet are worse than the pure network. The difference from [13] is that we have not changed any parameters other than the use of BN, while the authors in the original paper reduced regulation (both weight loss and dropout), changed the learning rate for GoogLeNet and reshuffled an additional training group."}, {"heading": "3.6. Classifier design", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.6.1. Previous work", "text": "The CNN architecture can be seen as an integration of the feature detector followed by a classifier. Ren et. al. [44] suggested that revolutionary layers of AlexNet should be considered as a feature extractor and fully connected layers as a 2-layer MLP as a classifier. They argued that 2 fully connected layers were not the optimal design and examined different architectures instead. However, they considered only pre-formed CNN or HOGs as a feature extractor, i.e. mainly the transfer learning scenario when most network weights are frozen. In addition, they examined architectures with additional evolution layers, which cannot be considered a better classifier, but an extension of the feature extractor. There are three of the most popular approaches to classifier design. First, the last layer of the feature extractor is the maximum pooling layer and the classifier is a one-layer or second layer of the net layer 12, followed by Alexfictitious [the net layer 45]."}, {"heading": "3.6.2. Experiment", "text": "We have investigated the following variants: standard 2-layer MLP, SPPNet with 2 and 3 pyramid levels, removal of the Pool5 layer, treatment of completely bonded layers as undulating, which allows the use of zero padding, thus increasing the effective number of training examples for this layer, characteristics averaged before the Softmax layer or spatial predictions of the Softmax layer averaged [47]. The best results are obtained when predictions of all spatial positions are averaged and MLP layers are treated as folding - with zero padding. The advantage of SPP over standard max pooling is less pronounced."}, {"heading": "3.7. Batch size and learning rate", "text": "Minibatch size is always a trade-off between computing efficiency - because the GPU architecture prefers it big enough - and accuracy; early work by Wilson and Martinez [48] shows the superiority of online training over batch training. At this point, we are examining the influence of minibatch size on final accuracy. Experiments show that maintaining a constant learning rate for different minibatch sizes has a negative impact on performance. We have also tested the heuristics proposed by Krizhevskiy [49], which suggests that the product of minibatch size and learning rate should be kept constant. Results are shown in Figure 8. Heuristics work, but large (512 and more) minibatch sizes lead to a fairly significant reduction in performance. On the other hand, online training (minibatch with individual examples) does not increase accuracy above 64 or 256, but significantly slows down training time."}, {"heading": "3.8. Network width", "text": "To our knowledge, there is no study on network width - final dependence on accuracy. Canziani et.al [50] carried out a comparative analysis of the ImageNet winner in terms of accuracy, number of parameters and computational complexity, but it is a comparison of the different architectures. In this subsection, we evaluate how far you can get by increasing the CaffeNet width without any other changes. The results are in Figure 9. The original architecture is almost optimal in terms of accuracy in the FLOPS sense: a reduction in the number of filters leads to a rapid and significant decrease in accuracy, while a densification of the network brings advantages, but it saturates quickly. A densification of the network results in a very limited gain in accuracy more than three times as fast."}, {"heading": "3.9. Input image size", "text": "Our initial experiment, shown in Figure 1, shows that CaffeNet, trained on 227x227 images, can compete with a much more complex GoogLeNet architecture trained on smaller images, so the obvious question is what the dependence is between image size and final accuracy. We conducted an experiment with different input image sizes: 96, 128, 180 and 224 pixels wide. The results are in Figure 10. The bad news is that while the accuracy depends linearly on the image size, the calculations required grow square, making it a very expensive method of performance enhancement. In the second part of the experiment, we kept the spatial output size of the Pool1 layer fixed while we changed the input image size. To prove this, we each change the step width and filter size of the ac1 layer, and the results show that the gain from a large image size (mostly after a minimum value) comes from the larger layers, rather than from the larger ones."}, {"heading": "3.10. Dataset size and noisy labels", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.10.1. Previous work", "text": "In DeepFace [51], the authors show that the reduction of the dataset from 4.4M to 1.5M leads to a decrease in accuracy of 1.74%. Schroff et.al [52] shows a similar dependence, but on an extra-large dataset: a decrease in the dataset size from 260M to 2.6M leads to a decrease in accuracy of 10%. However, these datasets are private and the experiments are not reproducible. Another important characteristic of a dataset is the cleanliness of the data. For example, an estimate of human accuracy in ImageNet is 5.5% for the top 5 [1]. To create the ImageNet, each image was chosen by ten different people [1]."}, {"heading": "3.10.2. Experiment", "text": "We investigate the relationship between the accuracy and the size / cleanliness of the dataset in ImageNet. For the experiment with the dataset size, 200, 400, 600, 800 thousand examples were randomly selected from a complete training set. For each reduced dataset, a CaffeNet is retrained from scratch. For the cleanliness test, we have replaced the labels for 5%, 10%, 15% and 32% of the examples with a randomly incorrect one. Contrary to recent work on troublesome labels, the labels are fixed as a regulatory method [53]. The results are shown in Figure 11, which clearly shows that a larger (and more diverse) dataset brings an improvement. There is a minimum size below which performance quickly declines. Less clean data outperforms more noisy ones: A clean dataset with 400K images is on par with 1.2M datasets with 800K-correct images."}, {"heading": "3.11. Bias in convolution layers", "text": "We conducted a simple experiment on the importance of bias in folding and fully connected layers. First, the network is trained as usual, second - bias is initialized with zeros and the bias is set to zero. Non-bias network has 2.6% less accuracy than the default value - see Table 8."}, {"heading": "4. Best-of-all experiments", "text": "Finally, we test how all improvements that do not increase computing costs work together. We combine: the learned color space transforms F, ELU as nonlinearity for folding layers and maxout for fully connected layers, linear decay policy of learning rate, average plus maximum pooling. The improvements are applied to CaffeNet128, CaffeNet224, VGGNet128, and GoogleNet128. The first three showed consistent performance growth (see Figure 12), while GoogLeNet deteriorated performance as noted during batch normalization, possibly due to the complex and optimized structure of the GoogLeNet network. Unfortunately, the cost of training VGGNet224 is prohibitive, one month of GPU time, so we have not yet tested it."}, {"heading": "5. Conclusions", "text": "We have shown that benchmarking can be done at an affordable cost of time and computation. A summary of the recommendations: \u2022 use ELU nonlinearity without batch norm or ReLU with it. \u2022 apply a learned color space transformation of RGB. \u2022 use the linear decay policy of the learning rate. \u2022 use a sum of average and maximum pooling layers. \u2022 use mini batch size of 128 or 256. If this is too large for your GPU, reduce the learning rate proportional to the batch size. \u2022 use fully connected layers as a convolutionary and calculate the predictions for the final decision. \u2022 if you invest in increasing the training set size, check if a plateau has not been achieved. \u2022 Cleanliness of the data is more important than the batch size. \u2022 if you cannot increase the input image size, reduce the step in the subsequent layers with roughly the same effect. \u2022 If your network becomes highly optimized as Google's network architecture."}, {"heading": "Acknowledgements", "text": "The authors were supported by The Czech Science Foundation Project GACR P103 / 12 / G084 and CTU student grant SGS15 / 155 / OHK3 / 2T / 13.Table 9: Results of all tests on ImageNet-128pxGroup Name acc =%] Baseline 47.1 Non-Linearity Linear 38.9tanh 40.1 VReLU 46.9 APL2 47.1 ReLU 47.1 RReLU 47.1 maxout (MaxS) 48.2 PReLU 48.5 ELU 48.8 maxout (MaxW) 51.7Batch standardization (BN) before non-linearity 40.4 after non-linearity 49.9 BN + non-linearity linearity linearity linearity 38.4 tanh 44.8 sigmoid 47.5 maxout (MaxS 48.7) 48.7 ELU 49.7 Netgrob-48.7 Netgrob-48.7 Linearity 48.7 Linearity 49.1 Netearity 49.1 Netearity 49.49.4449.4444444440.49 Netearity 40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.49.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.4"}, {"heading": "6. References", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV) 88 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "in: Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, Springer-Verlag, London, UK, UK", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Neural Networks: Tricks of the Trade: Second Edition", "author": ["Y. Bengio"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: F. Pereira, C. Burges, L. Bottou, K. Weinberger (Eds.), Advances in Neural Information Processing Systems 25, Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for largescale visual recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "in: Proceedings of ICLR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "in: CVPR 2015", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "in: D. Blei, F. Bach (Eds.), Proceedings of the 32nd International Conference on Machine Learning (ICML- 15), JMLR Workshop and Conference Proceedings", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "in: Proceedings of ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in: G. J. Gordon, D. B. Dunson (Eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), Vol. 15, Journal of Machine Learning Research - Workshop and Conference Proceedings", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Train you very own deep convolutional network", "author": ["B. Graham"], "venue": "train-you-very-own-deep-convolutional-network", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "in: Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "vol. 1, MIT Press, Cambridge, MA, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1986}, {"title": "Investigation of maxout networks for speech recognition", "author": ["P. Swietojanski", "J. Li", "J.-T. Huang"], "venue": "in: Proceedings of ICASSP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "in: Proceedings of ICLR Workshop", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Kaggle diabetic retinopathy detection competition report", "author": ["B. Graham"], "venue": "Tech. rep. ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Image enhancement by histogram transformation", "author": ["R. Hummel"], "venue": "Computer Graphics and Image Processing 6 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1016}, {"title": "Graphics gems iv", "author": ["K. Zuiderveld"], "venue": "Academic Press Professional, Inc., San Diego, CA, USA", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "in: Proceedings of the IEEE, Vol. 86", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1998}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Netw. 16 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1016}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 154, "endOffset": 160}, {"referenceID": 1, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Besides two classic works on training neural networks \u2013 [8] and [9], which are still highly relevant, there is very little guidance or theory on the plethora of design choices and hyper-parameter settings of CNNs with the consequent that researchers proceed by trial-and-error experimentation and architecture copying, sticking to established net types.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Besides two classic works on training neural networks \u2013 [8] and [9], which are still highly relevant, there is very little guidance or theory on the plethora of design choices and hyper-parameter settings of CNNs with the consequent that researchers proceed by trial-and-error experimentation and architecture copying, sticking to established net types.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "First applied in the ILSVRC [1] competition, they have been adopted in different research areas.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "We survey the recent developments and perform a large scale experimental study that considers the choice of nonlinearity, pooling, learning rate policy, classifier design, network width, batch normalization [13].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Evaluation framework All tested networks were trained on the 1000 object category classification problem on the ImageNet dataset [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "The commonly used pre-processing includes image rescaling to 256xN, where N \u2265 256, and then cropping a random 224x224 square [10, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 8, "context": "The dropout [21] with probability 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "All the networks were initialized with LSUV [22].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Previous work The activation functions for neural networks are a hot topic, many functions have been proposed since the ReLU discovery [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "LeakyReLU [24] and Very Leaky ReLU [25], RReLU [26],PReLU [27] and its generalized version \u2013 APL [28], ELU [29].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "maxout [30], MBA [31], etc.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Besides this, we have tested \u201doptimally scaled\u201d tanh, proposed by LeCun [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "We have also tried to train sigmoid [32] network, but the initial loss never decreased.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "al [33], we have tested combination of ReLU for first layers and maxout for the last layers of the network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "al [33] hypothesis about maxout power in the final layers is confirmed and combined ELU (after convolutional layers) + maxout (after fully connected layers) shows the best performance among non-linearities with speed close to ReLU.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "[10] claimed superiority of overlapping pooling with 3x3 window size and stride 2, while VGGNet [11] uses a non-overlapping 2x2 window.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[10] claimed superiority of overlapping pooling with 3x3 window size and stride 2, while VGGNet [11] uses a non-overlapping 2x2 window.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "al [40] compared YCrCb and RGB channels for image superresolution, Graham [41] extracted local average color from retina images in winning solution to the Kaggle competition.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Global [42] and local (CLAHE [43]) histogram equalizations hurt performance as well.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "Global [42] and local (CLAHE [43]) histogram equalizations hurt performance as well.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "Batch normalization Batch normalization [13] (BN) is a recent method tha t solves the gradient exploding/vanishing problem and guarantees near-optimal learning regime for the layer following the batch normalized one.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Following [22], we first tested different options where to put BN \u2013 before or after the non-linearity.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "The difference to [13] is that we have not changed any other parameters except using BN, while in the original paper, authors decreased regularization (both weight decay and dropout), changed the learning rate decay policy and applied an additional training set re-shuffling.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 6, "context": "This variant is used in GoogLeNet [12] and ResNet [14].", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Batch size and learning rate The mini-batch size is always a trade-off between computation efficiency \u2013 because GPU architecture prefers it large enough \u2013 and accuracy; early work by Wilson and Martinez [48] shows superiority of the online training to batchtraining.", "startOffset": 203, "endOffset": 207}, {"referenceID": 21, "context": "In DeepFace [51], the authors shows that dataset reduction from 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "al [52] but on an extra-large dataset: decreasing the dataset size from 260M", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "1% for top-5 [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "To create the ImageNet, each image was voted on by ten different people [1].", "startOffset": 72, "endOffset": 75}], "year": 2016, "abstractText": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the \u201ddeficit\u201d is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.", "creator": "LaTeX with hyperref package"}}}