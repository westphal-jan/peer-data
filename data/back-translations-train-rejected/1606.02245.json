{"id": "1606.02245", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Iterative Alternating Neural Attention for Machine Reading", "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.", "histories": [["v1", "Tue, 7 Jun 2016 18:25:48 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v1", "Under review for EMNLP 2016"], ["v2", "Wed, 8 Jun 2016 18:17:03 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v2", "Under review for EMNLP 2016"], ["v3", "Fri, 10 Jun 2016 21:16:56 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v3", "Under review for EMNLP 2016"], ["v4", "Wed, 9 Nov 2016 18:11:09 GMT  (280kb,D)", "http://arxiv.org/abs/1606.02245v4", null]], "COMMENTS": "Under review for EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["alessandro sordoni", "philip bachman", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1606.02245"}, "pdf": {"name": "1606.02245.pdf", "metadata": {"source": "CRF", "title": "Iterative Alternating Neural Attention for Machine Reading", "authors": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio"], "emails": ["phil.bachman}@maluuba.com,", "bengioy@iro.umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "The second factor is the formulation of standard machine concepts based on nature surveys (Hill et al., 2015), which allow a rapid integration between model concepts and experimental assessments; the second is the formulation of standard machine concepts based on nature surveys (Hill et al., 2015), which allow a rapid integration between model concepts and experimental assessments; and the third is the formulation of surveys based on nature surveys (Hill et al., 2015), which are conducted on the basis of surveys."}, {"heading": "2 Task Description", "text": "The CBT (Hill et al., 2015) and CNN (Hermann et al., 2015) corpora are two such data sets. The CBT1 corpus was generated from well-known children's books available as part of the Gutenberg project. Documents consist of 20-sentence excerpts from these books. The corresponding query is formed from an excerpt of the 21st sentence by replacing a single word with an anonymous placeholder token. The data set is divided into four subsets depending on the type of the replaced word. The subsets are called entity, common noun, verb and preposition. We will focus our evaluation exclusively on the first two subsets, i.e. CBTNE (called entity) and CBT-CN (common nouns), as the latter two are relatively simple, as with (Hill al., 2015).The summary of the Q itself will be replaced by the older articles."}, {"heading": "3 Alternating Iterative Attention", "text": "Our model is illustrated in Figure 1. Its workflow consists of three steps. First, the encoding phase is where we calculate a series of vector representations that act as a reminder of the content of the input document and the query. Second, the inference phase aims to unravel the complex semantic relationships between the document and the query to provide sufficiently strong evidence of the success of the response prediction. To achieve this, we use an iterative process that alternates attentive memory access to the query and the document at each iteration. Finally, the prediction phase uses the information gathered from the repeated attention by the query and the document to maximize the likelihood of the correct answer. We describe each of the phases in the following sections."}, {"heading": "3.1 Bidirectional Encoding", "text": "The input into the encoding phase is a sequence of words X = (x1,..,.), like a document or query drawn from a vocabulary V. Each word is represented by a continuous word embedding x-Rd in a word embedding matrix (Cho et al., 2014). For each position i in the input sequence, the GRU uses a recurring neural network encoder (Goodfellow et al., 2016) with gated recurrent units (GRU) (Cho et al., 2014). For each position i in the input sequence, the GRU takes the word embedding xi as input and develops a hidden state hi \u2212 1 to hi = f (xi, hi \u2212 1), where f is represented by: ri = circular shape (Ir xi + Hrhi \u2212 1) and the state c, ui \u2212 ui."}, {"heading": "3.2 Alternating Iterative Attention", "text": "The question is whether the question of the manner in which this question is posed is really the question of whether the question of the way in which it is posed is the question that it is about, the question about the way in which it is posed. (1) The question about the way in which the question is posed, the question about the way in which the question is posed, whether the question about the way in which the question is posed, whether the question about the way in which it is posed, has to be answered. (2) The question about the way in which the question is posed, the question about the way in which the question is posed, is the question about the way in which the question is posed. (3) The question about the way in which the question is asked, the way in which the question is asked, the question about the way in which it is asked, the way in which it is (and the way in which it is), and the way in which it is (and the way in which it is)."}, {"heading": "3.3 Answer Prediction", "text": "After a fixed number of time steps T, the attention weights obtained in the last search step di, T of the document are used to predict the probability of the answer in view of the document and the query P (a | Q, D). Formally, we follow (Kadlec et al., 2016) and apply the \"pointer sum\" loss: P (a | Q, D) = \u2211 i-I (a, D) di, T, (2), where I (a, D) is a series of positions where an error occurs in the document. The model is trained to maximize logP (a | Q, D) over the training corpus."}, {"heading": "4 Training Details", "text": "To train our model, we used stochastic gradient parentage with the ADAM Optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001. We set the batch size to 32 and decay the learning rate by 0.8 if the accuracy of the validation quantity does not increase after half an epoch, i.e. 2000 batches (for CBT) and 5000 batches for (CNN). We initialize all the weights of our model by scanning from the normal distribution N (0, 0.05). Following (Saxe et al., 2013), the recurring weights are initialized to be orthogonal and the distortions are initialized to zero. To stabilize learning, we cut the gradients if their standard is greater than 5 (Pascanu et al al al al al., 2013). We performed a hyperparameter search with the regulation embedding in {0.001, 0.001}, beta size {5, T, 25d}."}, {"heading": "5 Results", "text": "We report the results of our model on the CBT-CN, CBT-NE, and CNN datasets previously described in Section 2."}, {"heading": "5.1 CBT", "text": "Table 2 reports on our results on the CBT-CN and the CBTNE dataset. We found that 2.5% of the predicted 27.5% -Q numbers generally contain multiple BT values. Results from Humans, LSTMs and Memory Networks (MemNNs) come from (Hill et al., 2015) and the Attention-Sum Reader (AS Reader) is a recent result achieved by (Kadlec et al., 2016). Main result Our model (row 7) sets a new state of the art in the common noun category by gaining 3.6 and 5.5 points in validation and verification over the best baseline AS Reader (row 5). This performance gap is only partially reflected on the CBT-NE dataset. We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set, which is on par with the best baseline reader."}, {"heading": "5.2 CNN", "text": "We compare our model with a simple word distance model, the three neural approaches (Hermann et al., 2015) (Deep LSTM Reader, Attentive Reader and Impatient Reader), and with the AS reader (Kadlec et al., 2016).Main Results The results show that our model (line 8) improves state-of-the-art accuracy by 4 percent absolutely on validation and 3.4 on test in relation to the most recent published result (line 7).We also report on the very recent results of the Stanford AR system, which caught our attention during the preparation of this article (Chen et al., 2016) (line 9).Our model improves this strong baseline by 1.1 percent on validation and 1.5 percent on test. We note that the latter comparison can be influenced by various training and initiation strategies."}, {"heading": "5.3 Discussion", "text": "The title of the article is \"Dante turns in his grave when the Italian language decreases,\" and it discusses the decline of the Italian language in schools. The plot is illustrated in Figure 5.2, where places taken into account in the query and in the document are located in the left and right columns. Each line corresponds to an inference time of 1 \u2264 t \u2264 8. In the first step, the query attention focuses on the placeholder, since its local context is generally important to distinguish the answer. Initially, the model focuses on @ entity148, which corresponds to \"Greek\" in this article. At this point, the query attention focuses on the placeholder token, as its local context is generally important to distinguish the answer."}, {"heading": "6 Related Works", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "7 Conclusion", "text": "We presented an iterative neural attention model and applied it to machine sensing tasks. Our architecture uses a novel alternating attention mechanism and tightly integrates successful ideas from past work into the machine reading comprehension to obtain current results across three sets of data.The iterative alternating attention mechanism continuously refines its view of the query and document, while simultaneously aggregating the information required to answer a query. Several future research directions can be envisaged. We plan to dynamically select the optimal number of sequential steps required for each example. Furthermore, we suspect that the shift towards stochastic attention should enable us to learn more interesting search guidelines. Finally, we believe that our model is completely general and can easily be applied to other tasks such as retrieving information."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio."], "venue": "Deep", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A thorough examination of the cnn / daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proc. of ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. of EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville."], "venue": "Book in preparation for MIT Press.", "citeRegEx": "Goodfellow et al\\.,? 2016", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves."], "venue": "CoRR, abs/1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proc. of NIPS, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proc. of ICLR.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "arXiv preprint arXiv:1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Proc. of NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio."], "venue": "Proc. of ICML.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proc. EMNLP, 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov."], "venue": "JMLR, 15(1).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor."], "venue": "Journalism and Mass Communication Quarterly, 30.", "citeRegEx": "Taylor.,? 1953", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems, pages 2674\u20132682.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proc. of ICML, pages 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "The first is the advent of deep learning techniques (Goodfellow et al., 2016), which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data.", "startOffset": 52, "endOffset": 77}, {"referenceID": 8, "context": "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (Hill et al., 2015; Hermann et al., 2015), which permit fast integration loops between model conception and experimental evaluation.", "startOffset": 111, "endOffset": 152}, {"referenceID": 7, "context": "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (Hill et al., 2015; Hermann et al., 2015), which permit fast integration loops between model conception and experimental evaluation.", "startOffset": 111, "endOffset": 152}, {"referenceID": 19, "context": "Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement.", "startOffset": 20, "endOffset": 34}, {"referenceID": 8, "context": "In a pragmatic approach, recent work (Hill et al., 2015) formed such questions by extracting a sentence from a larger document.", "startOffset": 37, "endOffset": 56}, {"referenceID": 7, "context": "The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text (Hermann et al., 2015).", "startOffset": 111, "endOffset": 133}, {"referenceID": 0, "context": "Encouraged by the recent success of deep learning attention architectures (Bahdanau et al., 2015; Sukhbaatar et al., 2015), we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks.", "startOffset": 74, "endOffset": 122}, {"referenceID": 18, "context": "Encouraged by the recent success of deep learning attention architectures (Bahdanau et al., 2015; Sukhbaatar et al., 2015), we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks.", "startOffset": 74, "endOffset": 122}, {"referenceID": 5, "context": "The model first reads the document and the query using a recurrent neural network (Goodfellow et al., 2016).", "startOffset": 82, "endOffset": 107}, {"referenceID": 9, "context": "Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes (Hill et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 8, "context": ", 2016) and iterative attention processes (Hill et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 42, "endOffset": 86}, {"referenceID": 18, "context": ", 2016) and iterative attention processes (Hill et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 42, "endOffset": 86}, {"referenceID": 8, "context": "The CBT (Hill et al., 2015) and CNN (Hermann et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 7, "context": ", 2015) and CNN (Hermann et al., 2015) corpora are two such datasets.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "CBTNE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by (Hill et al., 2015).", "startOffset": 110, "endOffset": 129}, {"referenceID": 5, "context": "The sequence X is processed using a recurrent neural network encoder (Goodfellow et al., 2016) with gated recurrent units (GRU) (Cho et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 3, "context": ", 2016) with gated recurrent units (GRU) (Cho et al., 2014).", "startOffset": 41, "endOffset": 59}, {"referenceID": 9, "context": "In order to incorporate information from the future tokens x>i, we choose to process the sequence in reverse with an additional GRU (Kadlec et al., 2016).", "startOffset": 132, "endOffset": 153}, {"referenceID": 8, "context": "The attention we use here is similar to the formulation used in (Hill et al., 2015; Sukhbaatar et al., 2015), but with two differences.", "startOffset": 64, "endOffset": 108}, {"referenceID": 18, "context": "The attention we use here is similar to the formulation used in (Hill et al., 2015; Sukhbaatar et al., 2015), but with two differences.", "startOffset": 64, "endOffset": 108}, {"referenceID": 12, "context": "This simple bilinear attention has been successfully used in (Luong et al., 2015).", "startOffset": 61, "endOffset": 81}, {"referenceID": 0, "context": "Intuitively, aq may be considered as asking \u201cwhat are the important words in the question?\u201d and is similar to what is achieved by the original attention mechanism proposed in (Bahdanau et al., 2015) without the burden of the additional tanh layer.", "startOffset": 175, "endOffset": 198}, {"referenceID": 18, "context": "to use previously obtained document information to bias future attended locations, which is particularly important for natural language inference tasks (Sukhbaatar et al., 2015).", "startOffset": 152, "endOffset": 177}, {"referenceID": 9, "context": "Formally, we follow (Kadlec et al., 2016) and apply the \u201cpointer-sum\u201d loss:", "startOffset": 20, "endOffset": 41}, {"referenceID": 10, "context": "To train our model, we used stochastic gradient descent with the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "Following (Saxe et al., 2013), the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "In order to stabilize the learning, we clip the gradients if their norm is greater than 5 (Pascanu et al., 2013).", "startOffset": 90, "endOffset": 112}, {"referenceID": 17, "context": "We regularize our model by applying a dropout (Srivastava et al., 2014) rate of 0.", "startOffset": 46, "endOffset": 71}, {"referenceID": 8, "context": "Results marked with 1 are from (Hill et al., 2015) and those marked with 2 are from (Kadlec et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 9, "context": ", 2015) and those marked with 2 are from (Kadlec et al., 2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "Our model is implemented in Theano (Bastien et al., 2012), using the Keras (Chollet, 2015) library.", "startOffset": 35, "endOffset": 57}, {"referenceID": 4, "context": ", 2012), using the Keras (Chollet, 2015) library.", "startOffset": 25, "endOffset": 40}, {"referenceID": 9, "context": "Computational Complexity Similar to previous state-of-the-art models (Kadlec et al., 2016; Chen et al., 2016) which use a bidirectional encoder, the major bottleneck of our method is computing the document and query encodings.", "startOffset": 69, "endOffset": 109}, {"referenceID": 2, "context": "Computational Complexity Similar to previous state-of-the-art models (Kadlec et al., 2016; Chen et al., 2016) which use a bidirectional encoder, the major bottleneck of our method is computing the document and query encodings.", "startOffset": 69, "endOffset": 109}, {"referenceID": 9, "context": "our computation cost is comparable to (Kadlec et al., 2016; Chen et al., 2016), but we outperform the latter models on the datasets tested.", "startOffset": 38, "endOffset": 78}, {"referenceID": 2, "context": "our computation cost is comparable to (Kadlec et al., 2016; Chen et al., 2016), but we outperform the latter models on the datasets tested.", "startOffset": 38, "endOffset": 78}, {"referenceID": 8, "context": "The Humans, LSTMs and Memory Networks (MemNNs) results are taken from (Hill et al., 2015) and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al.", "startOffset": 70, "endOffset": 89}, {"referenceID": 9, "context": ", 2015) and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016).", "startOffset": 99, "endOffset": 120}, {"referenceID": 7, "context": "Results marked with 1 are from (Hermann et al., 2015), 2 from (Hill et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": ", 2015), 2 from (Hill et al., 2015), 3 from (Kadlec et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 9, "context": ", 2015), 3 from (Kadlec et al., 2016) and 4 from (Chen et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": ", 2016) and 4 from (Chen et al., 2016).", "startOffset": 19, "endOffset": 38}, {"referenceID": 9, "context": "This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work (Kadlec et al., 2016; Chen et al., 2016).", "startOffset": 156, "endOffset": 196}, {"referenceID": 2, "context": "This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work (Kadlec et al., 2016; Chen et al., 2016).", "startOffset": 156, "endOffset": 196}, {"referenceID": 7, "context": "We compare our model with a simple word distance model, the three neural approaches from (Hermann et al., 2015) (Deep LSTM Reader, Attentive Reader and Impatient Reader), and with the AS reader (Kadlec et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 9, "context": ", 2015) (Deep LSTM Reader, Attentive Reader and Impatient Reader), and with the AS reader (Kadlec et al., 2016).", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": "We also report the very recent results of the Stanford AR system that came to our attention during the write-up of this article (Chen et al., 2016) (line 9).", "startOffset": 128, "endOffset": 147}, {"referenceID": 15, "context": "First, Stanford AS uses GloVe embeddings (Pennington et al., 2014), pre-trained from a large external corpus.", "startOffset": 41, "endOffset": 66}, {"referenceID": 2, "context": "Category analysis (Chen et al., 2016) classified a sample of 100 CNN stories based on the type of inference required to guess the answer.", "startOffset": 18, "endOffset": 37}, {"referenceID": 2, "context": "The first three categories require local context matching, the next two global context matching and coreference errors are unanswerable questions (Chen et al., 2016).", "startOffset": 146, "endOffset": 165}, {"referenceID": 2, "context": "This is a barrier to achieving accuracies considerably above 75% (Chen et al., 2016).", "startOffset": 65, "endOffset": 84}, {"referenceID": 2, "context": "This aligns with the findings of (Chen et al., 2016) concerning CNN, which state that the required reasoning and inference levels for this dataset are quite simple.", "startOffset": 33, "endOffset": 52}, {"referenceID": 6, "context": "These include, but are not limited to, handwriting recognition (Graves, 2013), digit classification (Mnih et al.", "startOffset": 63, "endOffset": 77}, {"referenceID": 13, "context": "These include, but are not limited to, handwriting recognition (Graves, 2013), digit classification (Mnih et al., 2014), machine translation (Bahdanau et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 0, "context": ", 2014), machine translation (Bahdanau et al., 2015), question answering (Sukhbaatar et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 18, "context": ", 2015), question answering (Sukhbaatar et al., 2015; Hermann et al., 2015) and caption generation (Xu et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 7, "context": ", 2015), question answering (Sukhbaatar et al., 2015; Hermann et al., 2015) and caption generation (Xu et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 21, "context": ", 2015) and caption generation (Xu et al., 2015).", "startOffset": 31, "endOffset": 48}, {"referenceID": 18, "context": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015), which were also applied to question answering.", "startOffset": 32, "endOffset": 139}, {"referenceID": 11, "context": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015), which were also applied to question answering.", "startOffset": 32, "endOffset": 139}, {"referenceID": 7, "context": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015), which were also applied to question answering.", "startOffset": 32, "endOffset": 139}, {"referenceID": 9, "context": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015), which were also applied to question answering.", "startOffset": 32, "endOffset": 139}, {"referenceID": 8, "context": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015), which were also applied to question answering.", "startOffset": 32, "endOffset": 139}, {"referenceID": 9, "context": "The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by (Kadlec et al., 2016), which in turn was based on the earlier Pointer Networks of (Vinyals et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 20, "context": ", 2016), which in turn was based on the earlier Pointer Networks of (Vinyals et al., 2015).", "startOffset": 68, "endOffset": 90}, {"referenceID": 9, "context": "However, differently from our work, (Kadlec et al., 2016) perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks.", "startOffset": 36, "endOffset": 57}, {"referenceID": 7, "context": "A similar attempt in attending different components of the query may be found in (Hermann et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 18, "context": "Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015; Hill et al., 2015).", "startOffset": 102, "endOffset": 146}, {"referenceID": 8, "context": "Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015; Hill et al., 2015).", "startOffset": 102, "endOffset": 146}], "year": 2017, "abstractText": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children\u2019s Book Test (CBT) dataset.", "creator": "TeX"}}}