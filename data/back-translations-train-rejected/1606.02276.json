{"id": "1606.02276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Multilingual Visual Sentiment Concept Matching", "abstract": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we pro- vide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to repre- sent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural in- sights of portrait-related affective visual concepts.", "histories": [["v1", "Tue, 7 Jun 2016 19:40:00 GMT  (7864kb,D)", "http://arxiv.org/abs/1606.02276v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.IR cs.MM", "authors": ["nikolaos pappas", "miriam redi", "mercan topkara", "brendan jou", "hongyi liu", "tao chen", "shih-fu chang"], "accepted": false, "id": "1606.02276"}, "pdf": {"name": "1606.02276.pdf", "metadata": {"source": "CRF", "title": "Multilingual Visual Sentiment Concept Matching", "authors": ["Nikolaos Pappas", "Miriam Redi", "Mercan Topkara", "Brendan Jou", "Hongyi Liu", "Tao Chen", "Shih-Fu Chang"], "emails": ["npappas@idiap.ch", "redi@yahoo-inc.com", "mercan@jwplayer.com", "bjou@ee.columbia.edu", "hongyi.liu@columbia.edu", "taochen@ee.columbia.edu", "sfchang@ee.columbia.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Multilingual; Language; Cultures; Intercultural; Emotion; Emotions; Ontology; Conceptual recognition; Social Multimedia"}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. RELATED WORK", "text": "Research on distributed word representations [2, 3, 4, 5] has recently expanded to include multiple languages, using either bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages. [6] For example, suggested learning distributed word alignments across multiple languages by using a multilingual corpus from Wikipedia. [7, 8] also suggested learning bilingual embeddings in the context of neural language models, using multilingual word alignments. [9] suggested learning embeddings in common space across multiple languages without relying on word alignments. [10] also proposed autoencoder-based methods for learning multilingual word embeddings. One limitation in dealing with many languages is the scarcity of data for all pairs. In this study, we use a pivot language to align multilingual languages by means of machine translation. [10] Underlying our multilingual context is the multilingual concept used for multilingual assessment."}, {"heading": "3. VISUAL SENTIMENT OF CONCEPTS", "text": "In this paper, we use the MVSO dataset [1]. One disadvantage of MVSO is that the sentiment scores associated with affective visual concepts are automatically derived from sentiment analysis tools. Although such tools have performed impressively in recent years, they are usually based solely on text modalities. To counteract this, we designed a crowdsourcing experiment with CrowdFlower1 to comment on the mood of the multilingual adjective-noun pairs (ANPs) in MVSO. We looked at 11 out of 12 languages in MVSO, excluding Persian due to the limited number of ANPs. We constructed separate tasks for each language by using all ANPs in MVSO for that language. Task interface asked crowdsourcing workers to rate the sentiment value of ANPs on a scale of 1 to 5."}, {"heading": "4. EXACT CONCEPT MATCHING", "text": "In order to match visual sentiment concepts (ANPs) between languages, we translated them from each language using the Google Translate API2. We chose English as the linchpin 1http: / / www.crowdflower.com 2https: / / cloud.google.com / translatelanguage because, due to its popularity in relevant studies, it has the most complete translation resources (parallel corpora) for each of the other languages. Thus, for example, the concepts chien heureux (French), perro feliz (Spanish) and happy dog (German) are translated into the English concept happy dog. One would rightly expect that the visual sentiment concepts in the pivot language have shifted in terms of feeling and meaning as a result of the translation process. Thus, we study and analyze the impact of translation on the meaning and correspondence between the different languages as well as on the correspondence of concepts."}, {"heading": "4.1 Sentiment and Meaning Shift", "text": "To quantify the effect of the translation on the perception of terms, we used the crowdsourced sentiment values and counted the number of terms for which the sensation shifts after the translation into English. The higher this number is for a particular language, the higher the specificity of the visual sensation for that language. To avoid counting sentiment shifts due to small sentiment values, we define a sentiment threshold t below which we do not take character changes into account, as follows: | sent (ci) | >. Table 1 shows the percentage of concepts with shifted characters due to the translation. The percentages are on average 33% for t = 0. The highest percentage of character shifts is 60% from Arabic and the lowest percentage is 18.6% from Dutch. In addition, the percentage of concepts with shifted characters decreases in most languages, as we increase the absolute value range, which is particularly interesting because the result of this understanding is 0.3."}, {"heading": "4.2 Matching Coverage", "text": "Matching coverage is an essential characteristic of the multilingual concept of matching and clustering. We examine this characteristic by performing k-Means clustering of multilingual concepts and counting the number of concepts between two languages belonging to the same cluster. This shows the connectivity of language clusters based on exact matching, as shown in Figure 3 (a) for the eight most popular languages in the MVSO. Based on the connecting strips representing the number of concepts between two languages, we can find that concept clusters are dominated by single languages when matching exactly. For example, in all languages there is a connecting strip that traces back to the same language: this means that many clusters contain monolingual concepts. Another example that highlights the disadvantage of accurate matching is that of all German translations (781) the concepts that correspond to Dutch concepts (39) are more numerous than the concepts that correspond to Chinese concepts (23)."}, {"heading": "5. APPROXIMATE CONCEPT MATCHING", "text": "To overcome these limitations, we loosen the exact condition for concepts to match multilingual concepts and instead agree on concepts based on their semantic meaning. Intuitively, in order to match concepts from different languages, we need an approximation (or distance) measure that reflects how \"close\" or similar concepts are in the semantic distance space. This allows us to achieve our main goal: to compare visual concepts across languages and group them into multilingual groups. When used, the approximate agreement of clustering connectivity between languages is greatly enriched, as shown in Figure 3 (b): connecting strips are more evenly distributed across all languages. To learn such representations of meaning, we take advantage of recent advances in distributed lexical setics [4, 5, 20, 21] using the script model provided by a large word2word2corporon."}, {"heading": "5.1 Distributed Word Representations", "text": "Essentially, the Skip-gram model aims to learn vector representations for words by predicting the context of a word in a large corpus. Context is defined as a window of words before and after the current word. We look at the following corpora in English on which the Skip-gram model is trained: 1. Google News: A corpus of messages containing 100 billion tokens and 3 million unique words that have at least 5 occurrences from [22]. Messages are typically used to describe real-world events that contain precise word meanings but have indirect relevance to visual contents.Wikipedia: A corpus of Wikipedia articles that contain 1.74 billion tokens and 693,056 unique words that have at least 10 occurrences."}, {"heading": "5.2 Distributed Concept Representations", "text": "To represent concepts in a semantic space, we use the word embedding in the pivot language (English) and compose the representation of a concept based on its composite words. Each sentimental visual concept ci includes zero or more adjectives and one or more nouns (since the translation does not necessarily retain the adjective-noun pair structure of the original phrase). Given the word vector embedding ~ xadj and ~ xnoun, we calculate the concept embedding ~ ci using the sum operation for the composition: ~ ci = ~ xadj + ~ xnoun or the concept embedding ~ ci, which is learned directly from the Skip-gram model. This allows us to compare multilingual concepts using the pivot language (English). At this stage, we realize that there are several other ways to define the composition of short phrases, e.g. [24, 25, 22] but let's focus on the grain type, instead of the word being used here."}, {"heading": "6. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Visual Semantic Relatedness", "text": "In fact, it is the case that most of us are in a position to go into another world, in which they are able to change the world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they are able to"}, {"heading": "6.2 Multilingual Concept Clustering", "text": "There is a series of approaches which deal with the question of to what extent it is actually a matter of a way in which people in the world, who live in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the, in the world, in the, in the world, in the world, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the, in the world, in the, in the world, in the, in the world, in the"}, {"heading": "7. PORTRAIT CONCEPT CLUSTERING", "text": "The proposed multilingual concept cluster framework can be a useful tool to explore and analyze large multilingual collections of visual concepts. As an example application, we have used this concept matching framework to investigate how affective concepts attach human portraits, i.e. photos of faces, through the viewing lens of different languages."}, {"heading": "7.1 Portrait-based Sentiment Ontology", "text": "Portrait and face-centric photography has been a subject of research in multiple disciplines for years. Facial perception is one of the most advanced human abilities, where our brain even contains a dedicated sub-network of neurons for facial processing [27]. Lately, computational understanding portrait modeling has attracted a lot of attention from the multimedia community, e.g. in computational aesthetics [28], animated GIFs [29], and social dynamics [30]. Here, we are trying to unpack what sentimental visual concepts, especially ANPs, locate on faces. Face recognition and ANP filtering. To obtain a corpus of visual concepts related to faces, we operated a frontal face detector [31] that projects images onto a normalized pixel difference, characterizes space, and performs quadtree-based face detection. A total of 3,858,869 faces were captured in the 7,368,364 images in the MVSO image sets."}, {"heading": "7.2 Multilingual Portraits", "text": "In fact, it is as if most of them are able to abide by the rules that they have imposed on themselves and are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if they are able to abide by the rules. (...) It is not as if they abide by the rules, that they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they are abiding by the rules. (...) It is as if they are abiding by the rules. (...) It is as if they are abiding by the rules. (...)"}, {"heading": "8. CONCLUSIONS & FUTURE WORK", "text": "In this study, we demonstrated that visual sentiment concepts from multiple languages can be effectively represented in a common semantic space by using a pivot language (English) and existing advances in the distribution semantics of words. Best results in predicting visually based semantic distances were obtained through a skip-gram model trained on real image metadata, namely titles and descriptions from the Flickr 100M corpus, with a summary combination of concept word embedding or direct learning. This enabled multilingual clustering of visual sentiment concepts in 11 languages and enabled us to better organize an ontology in [1] and provide in-depth analysis of portrait images from a multilingual perspective."}, {"heading": "9. REFERENCES", "text": "[1] B. Jou, T. Chen, N. Pappas, M. Redi, M. Topkara, and S.-F. Chang, \"Visual influence around the world: A large-scale multilingual visual sentiments ontology,\" in ACM International Conference on Multimedia, (Brisbane, Australia), pp. 159-168, 2015. [2] J. Turian, L. Ratinov, and Y. Bengio, \"Word representations: A simple and general method for semi-supervised learning,\" in 48th Annual Meeting of the Association for Computational Linguistics, ACL '10, (Uppsala, Sweden), pp. 384-394, 2010. [R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \"Natural language processing (almost) scratch,\" Journal of Machine Learning Research, vol. 12, 2493-2537, 2011. [4]."}], "references": [{"title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology", "author": ["B. Jou", "T. Chen", "N. Pappas", "M. Redi", "M. Topkara", "S.-F. Chang"], "venue": "ACM International Conference on Multimedia, (Brisbane, Australia), pp. 159\u2013168, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, (Uppsala, Sweden), pp. 384\u2013394, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing, pp. 1532\u20131543, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "CoRR, vol. abs/1307.1662, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "Proceedings of COLING 2012, (Mumbai, India), pp. 1459\u20131474, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, (Seattle, WA, USA), pp. 1393\u20131398, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 58\u201368, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["A.P.S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V.C. Raykar", "A. Saha"], "venue": "CoRR, vol. abs/1402.1454, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "CoRR, vol. abs/1408.3456, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "Journal of Artificial Intelligence Research, vol. 49, pp. 1\u201347, Jan. 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "52nd Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 721\u2013732, June 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (Denver, Colorado), pp. 153\u2013163, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F. Li"], "venue": "Advances in Neural Information Processing Systems 27, pp. 1889\u20131897, Curran Associates, Inc., 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1889}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, vol. abs/1411.2539, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL, vol. 2, pp. 207\u2013218, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, vol. abs/1410.1090, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["S. Kottur", "R. Vedantam", "J.M.F. Moura", "D. Parikh"], "venue": "CoRR, vol. abs/1511.07067, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Conference on Empirical Methods in Natural Language Processing, (Lisbon, Portugal), pp. 298\u2013307, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of Association for Computational Linguistics, vol. 3, pp. 211\u2013225, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, pp. 3111\u20133119, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Word embeddings through hellinger pca", "author": ["R. Lebret", "R. Collobert"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics, (Gothenburg, Sweden), pp. 482\u2013490, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "Conference on Empirical Methods in Natural Language Processing, (Cambridge, MA, USA), pp. 1183\u20131193, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (Jeju Island, Korea), pp. 1201\u20131211, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "International Conference on New Methods in Language Processing, (Manchester, UK), 1994.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Neurons that keep a straight face", "author": ["W.A. Freiwald", "D.Y. Tsao"], "venue": "National Academy of Sciences, vol. 111, no. 22, pp. 7894\u20137895, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The beauty of capturing faces: Rating the quality of digital portraits", "author": ["M. Redi", "N. Rasiwasia", "G. Aggarwal", "A. Jaimes"], "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, (Ljubljana, Slovenia), pp. 1\u20138, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting viewer perceived emotions in animated GIFs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "ACM International Conference on Multimedia, (Orlando, Florida, USA), pp. 213\u2013216, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Faces engage us: Photos with faces attract more likes and comments on instagram", "author": ["S. Bakhshi", "D.A. Shamma", "E. Gilbert"], "venue": "ACM Conference on Human Factors in Computing Systems, (Toronto, ON, Canada), pp. 965\u2013974, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast and accurate unconstrained face detector", "author": ["S. Liao", "A.K. Jain", "S.Z. Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, pp. 211\u2013223, Feb 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For instance, [1] showed that Flickr users with different cultural backgrounds use different concepts to describe visual emotions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Our goal is to study the lexical differences of sentiment-biased visual concepts across multiple languages from the MVSO dataset [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 2, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 3, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 4, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 5, "context": "For instance, [6] proposed to learn distributed representations of words across languages by using a multilingual corpus from Wikipedia.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "[7, 8] proposed to learn bilingual embeddings in the context of neural language models utilizing multilingual word alignments.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8] proposed to learn bilingual embeddings in the context of neural language models utilizing multilingual word alignments.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[9] proposed to learn joint-space embeddings across multiple languages without relying on word alignments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Similarly, [10] proposed auto-encoder-based methods to learn multilingual word embeddings.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "However, studies on multimodal distributional semantics have combined visual and textual features to learn more informed word embeddings and have used the notion of semantics [11, 12] and", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "However, studies on multimodal distributional semantics have combined visual and textual features to learn more informed word embeddings and have used the notion of semantics [11, 12] and", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "visual similarity to evaluate word embeddings [13, 14].", "startOffset": 46, "endOffset": 54}, {"referenceID": 13, "context": "visual similarity to evaluate word embeddings [13, 14].", "startOffset": 46, "endOffset": 54}, {"referenceID": 14, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 15, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 16, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 17, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 18, "context": "Perhaps the most related study to ours is in [19] which aimed to learn visually grounded word embeddings to capture visual notions of semantic relatedness using abstract visual scenes.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "In this work, we make use of the MVSO dataset [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "54 between crowdsourced sentiment scores and the automatically assigned sentiment scores in [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Unlike the findings of previous work using automatic sentiment scores [1],", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 4, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 19, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 20, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 21, "context": "Google News: A corpus of news which contains 100 billion tokens and 3,000,000 unique words which have at least 5 occurrences from [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "The pre-processed text of this corpus was obtained from [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "The preprocessed text of this corpus was obtained from [23].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "For the Google News corpus, we used pre-trained embeddings of 300 dimensions with a context window of 5 words provided by [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "and analogy relations [4].", "startOffset": 22, "endOffset": 25}, {"referenceID": 25, "context": "We perform part-of-speech tagging on the translation to extract the representative noun or adjective with TreeTagger [26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "the most developed human capabilities, where our brains even contain a dedicated sub-network of neurons for face processing [27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "To obtain a corpus of visual concepts relating to faces, we ran a frontal face detector [31] which projects images onto a normalized pixel difference feature space and performs quadtree-based face detection.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "A total of 3,858,869 faces were detected across the 7,368,364 images in the MVSO image dataset [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "This enabled multilingual clustering of visual sentiment concepts in 11 languages, and allowed us to better hierarchically organize an ontology in [1] as well as provide deep analysis into portrait imagery from a multilingual perspective.", "startOffset": 147, "endOffset": 150}], "year": 2016, "abstractText": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we provide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourcing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to represent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural insights of portrait-related affective visual concepts.", "creator": "LaTeX with hyperref package"}}}