{"id": "1606.02407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "abstract": "We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.", "histories": [["v1", "Wed, 8 Jun 2016 05:31:43 GMT  (637kb,D)", "http://arxiv.org/abs/1606.02407v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV cs.LG", "authors": ["rathinakumar appuswamy", "tapan nayak", "john arthur", "steven esser", "paul merolla", "jeffrey mckinstry", "timothy melano", "myron flickner", "dharmendra modha"], "accepted": false, "id": "1606.02407"}, "pdf": {"name": "1606.02407.pdf", "metadata": {"source": "CRF", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "authors": ["Rathinakumar Appuswamy", "Tapan K. Nayak", "Steven Esser", "Paul A. Merolla"], "emails": ["rappusw@us.ibm.com", "tknayak@us.ibm.com", "arthurjo@us.ibm.com", "sesser@us.ibm.com", "pameroll@us.ibm.com", "jlmckins@us.ibm.com", "tmelano@us.ibm.com", "mdflickner@us.ibm.com", "dmodha@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2 Sparse Binary convolutional networks", "text": "It is a particularly elegant instance of low-precision targeting object-recognition tasks. Motivated by the possibility of using advanced image recognition networks on low-power mobile devices, there has been a growing interest in binary networks, although in this study we will confine ourselves to the TrueNorth architecture [9], most of our conclusions also apply to other platforms that offer similar functionality. [22, 25, 21] A TrueNorth chip consists of a network of neurosynaptic cores with programmable connectivity, synapses, and neuron parameters. It is a multicultural array in which each core consists of 256 input lines, 256, and 25 array of 25 arrangements that rely on programmable connections."}, {"heading": "3 Symmetric kernels", "text": "In fact it is so that it is a way in which one sees oneself in a position to be as it is able to establish oneself in the real world. (...) It is so that one is able to establish oneself in the real world. (...) It is as if one distinguishes oneself in the real world from the real world. (...) It is as if one is in the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world, from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the world from the real world from the world from the real world from the world from the world from the real world from the world from the real world from the world from the world from the world from the real world from the world from the world from the world from the world from the real world from the world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the real world from the"}, {"heading": "If K contains at least four distinct entries and no zeros, then the conditions on K are also necessary.", "text": "We refer to the convolution of nuclei identified in Theorem 3.1 as symmetrical nuclei."}, {"heading": "4 Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Weights", "text": "In a typical deep learning environment, the goal is to learn a series of cores for each continuous layer in order to minimize some loss functions through a gradient-based optimization method. In our case, we want the cores to come from the class of symmetrical cores efficiently mapped to the TrueNorth architecture. Our strategy is: First, we allow every Kernel K in the network to be learned without any limitations. Second, because our goal is to create a trained network assigned to TrueNorth, as soon as performance on the validation data is saturated, the unrestricted real Kernel K is replaced by a suitable member of the Symmetrical Core. If K P Rl2 l0 describes the unrestricted Convolution Kernel, then letpf \u02da, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, as described below, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers, Profile layers."}, {"heading": "4.2 Neurons", "text": "Our strategy is to start training with ReLU activations efficiently and make them louder and louder so that they can eventually be replaced by binary activations.The use of stochastic neuron activation functions has been researched in literature for various purposes [41, 42, 43, 44]. An early attempt to use noisy ReLU with the goal of obtaining binary activations has been reported in [45] for single-layer networks. It is well known in information theory that the best way to communicate with an amplitude-limited channel is to use a discrete set of input values for x, the presence of additive noise of the form shown in Figure 2."}, {"heading": "4.3 Systems and methods", "text": "The stochastic gradient descent was used with lunge layers [49], pulse (0.9), weight decay (10 '6), and batch normalization [50].The parameters learned during the training are mapped onto hardware using reusable, composable network description functions, known as corelets [51].The corelets created for this work automatically compile the learned network parameters, which are independent of any neuromorphic platform, into a platform-specific hardware configuration file that TrueNorth chips can program directly."}, {"heading": "5 Results", "text": "The proposed method was evaluated using the CIFAR-10 image recognition benchmark dataset (Figure 3), which consists of natural color images measuring 32 x 32 pixels in 10 classes of 50,000 training images and 10,000 test images."}, {"heading": "5.1 Networks", "text": "Recent results [5, 8, 3], as well as our own experiments, seem to suggest that bundling layers is unsuitable for binary neuron activation networks. Thus, our networks are built by stacking four groups of folding layers, and each set contains four folding layers. The smaller network is described in Table 1, while the large network we used was achieved simply by increasing the number of features in each layer. Final output characteristics are uniformly distributed among classes, and evaluation is done at 1 classification per hardware tick."}, {"heading": "5.2 Performance", "text": "To characterize performance, the trained 1-chip folding networks were used and executed on an IBM TrueNorth NS1e board with 4096 neuromorphic cores to measure classification accuracy and throughput. [53] The 4-chip networks were operated on a simulation basis, and classification accuracy was measured for the dataset. Results are presented in Table 2 compared to Energy-efficient deep networks (EEDN) on TrueNorth [3]. By using symmetrical cores, we were able to deploy a network with twice as many features on the same number of cores as was possible with the approach described in [3]. Thus, we were able to achieve an almost state-of-the-art accuracy for the same amount of hardware, although the amount of allowed convolutionary cores is a strict subset of the group of t '1, 0, 1u-rated cores."}, {"heading": "6 Conclusions", "text": "Our study shows that revolutionary networks built only with structured cores are surprisingly rich and retain their power of representation, and that back propagation can be adapted to learn in the face of such constraints. Furthermore, our research suggests that co-design of algorithms and architecture could be a successful strategy for an even more energy-efficient deployment platform for deep learning."}, {"heading": "7 Appendix", "text": "Suppose a TrueNorth core receives a vectorized 16\u0445 q \u00a8 q \u00a8 q \u00a8 q \u00a8 q \u00a8 q \u00a8 q \u00a8 1, and calculates the convolution with the Laplacian operator K in Example 1. Therefore, it is easy to determine that W pKq is a 256 \"pn\" l \"1q2 (i.e., 256\" 196 \") matrix. Since K meets the conditions of Theorem 3.1, we now use f, p\" 2 and p \"2 from the example to construct g, C and s1,\" s196 such that Mpg, C, tsiu196i \"1q\" W pKq. \"Define the 16\" matrix G by Gi, j \"11\" pelle j, \"j\" 1 \"and s1,\" s2, \"sq2.\""}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano", "D.R. Barch", "C. di Nolfo", "P. Datta", "A. Amir", "B. Taba", "M.D. Flickner", "D.S. Modha"], "venue": "CoRR, vol. abs/1603.08270, 2016. [Online]. Available: http://arxiv.org/abs/1603.08270", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, vol. abs/1602.02830, 2016. [Online]. Available: http://arxiv.org/abs/1602.02830", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarized neural networks", "author": ["I. Hubara", "D. Soudry", "R.E. Yaniv"], "venue": "CoRR, vol. abs/1602.02505, 2016. [Online]. Available: http://arxiv.org/abs/1602.02505 14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3105\u2013 3113.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "CoRR, vol. abs/1511.06530, 2015. [Online]. Available: http://arxiv.org/abs/1511.06530", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "CoRR, vol. abs/1603.05279, 2016. [Online]. Available: http://arxiv.org/abs/1603.05279", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science, vol. 345, no. 6197, pp. 668\u2013673, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": "Neural Information Processing Systems (NIPS), 2015. [Online]. Available: http://arxiv.org/pdf/1510.01722v1.pdf", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Acdc: A structured efficient linear layer", "author": ["M. Moczulski", "M. Denil", "J. Appleyard", "N. de Freitas"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix backpropagation for deep networks with structured layers", "author": ["C. Ionescu", "O. Vantzos", "C. Sminchisescu"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2965\u20132973.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep neural networks using a rankconstrained topology", "author": ["P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada"], "venue": "Proceedings of Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 1473\u20131477.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L.D. Bourdev"], "venue": "CoRR, vol. abs/1412.6115, 2014. [Online]. Available: http://arxiv.org/abs/1412.6115", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, vol. abs/1510.00149, 2015. [Online]. Available: http://arxiv.org/abs/1510.00149", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "CoRR, vol. abs/1504.04788, 2015. [Online]. Available: http://arxiv.org/abs/1504.04788", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "Advances in Neural Information Processing Systems. Morgan Kaufmann, 1990, pp. 598\u2013605.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012, 2012, pp. 4153\u20134156. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2012.6288833", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing bottle-neck features for lvcsr", "author": ["F. Gr\u00e9zl", "P. Fousek"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4729\u20134732.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "CoRR, vol. abs/1602.01528, 2016. [Online]. Available: http://arxiv.org/abs/1602.01528", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network simulation", "author": ["E. Painkras", "L.A. Plana", "J. Garside", "S. Temple", "F. Galluppi", "C. Patterson", "D.R. Lester", "A.D. Brown", "S.B. Furber"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 48, no. 8, pp. 1943\u20131953, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1943}, {"title": "Six networks on a universal neuromorphic computing substrate", "author": ["T. Pfeil", "A. Gr\u00fcbl", "S. Jeltsch", "E. M\u00fcller", "P. M\u00fcller", "M.A. Petrovici", "M. Schmuker", "D. Br\u00fcderle", "J. Schemmel", "K. Meier"], "venue": "arXiv preprint arXiv:1210.7083, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "A neuromorphic network for generic multivariate data classification", "author": ["M. Schmuker", "T. Pfeil", "M.P. Nawrot"], "venue": "Proceedings of the National Academy of Sciences, vol. 111, no. 6, pp. 2081\u20132086, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An event-based neural network architecture with an asynchronous programmable synaptic memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013107, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A 65k-neuron 73-mevents/s 22-pj/event asynchronous micro-pipelined integrate-and-fire array transceiver", "author": ["J. Park", "S. Ha", "T. Yu", "E. Neftci", "G. Cauwenberghs"], "venue": "Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 675\u2013678. 15", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "The design and implementation of fftw3", "author": ["M. Frigo", "S.G. Johnson"], "venue": "Proceedings of the IEEE, vol. 93, no. 2, pp. 216\u2013231, 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "On the computational power of circuits of spiking neurons", "author": ["W. Maass", "H. Markram"], "venue": "Journal of computer and system sciences, vol. 69, no. 4, pp. 593\u2013616, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Guest editors? introduction: The top 10 algorithms", "author": ["J. Dongarra", "F. Sullivan"], "venue": "Computing in Science & Engineering, vol. 2, no. 1, pp. 22\u201323, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Quicksort", "author": ["C.A. Hoare"], "venue": "The Computer Journal, vol. 5, no. 1, pp. 10\u201316, 1962.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1962}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMP- STAT\u20192010. Springer, 2010, pp. 177\u2013186.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Adjustable bounded rectifiers: Towards deep binary representations", "author": ["Z. Wu", "D. Lin", "X. Tang"], "venue": "http://arxiv.org/abs/1511.06201, vol. abs/1511.06201, 2015. [Online]. Available: http://arxiv.org/abs/1511. 06201", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores", "author": ["A.S. Cassidy", "P. Merolla", "J.V. Arthur", "S.K. Esser", "B. Jackson", "R. Alvarez-Icaza", "P. Datta", "J. Sawada", "T.M. Wong", "V. Feldman"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u201310.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Toeplitz and circulant matrices: A review", "author": ["R.M. Gray"], "venue": "now publishers inc,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Optimizing convolution operations on gpus using adaptive tiling", "author": ["B. Van Werkhoven", "J. Maassen", "H.E. Bal", "F.J. Seinstra"], "venue": "Future Generation Computer Systems, vol. 30, pp. 14\u201326, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, vol. abs/1410.0759, 2014. [Online]. Available: http://arxiv.org/abs/1410.0759", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolution engine: balancing efficiency & flexibility in specialized computing", "author": ["W. Qadeer", "R. Hameed", "O. Shacham", "P. Venkatesan", "C. Kozyrakis", "M.A. Horowitz"], "venue": "ACM SIGARCH Computer Architecture News, vol. 41, no. 3. ACM, 2013, pp. 24\u201335.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological), pp. 1\u201338, 1977.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1977}, {"title": "On the convergence properties of the em algorithm", "author": ["C.J. Wu"], "venue": "The Annals of statistics, pp. 95\u2013103, 1983.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1983}, {"title": "A view of the em algorithm that justifies incremental, sparse, and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in graphical models. Springer, 1998, pp. 355\u2013368.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 229\u2013256, 1992.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient learning in spiking neural networks by dynamic perturbation of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical review letters, vol. 97, no. 4, p. 048104, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Noisy activation functions", "author": ["\u00c7. G\u00fcl\u00e7ehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": "CoRR, vol. abs/1603.00391, 2016. [Online]. Available: http://arxiv.org/abs/1603.00391", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation learning for systems with discrete-valued functions", "author": ["E. Wilson"], "venue": "Proceedings of the World Congress on Neural Networks, vol. 3, 1994, pp. 332\u2013339.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "The information capacity of amplitude and variance constrained scalar gaussian channel", "author": ["J.G. Smith"], "venue": "Information and Control, vol. 18, p. 203?219, 1971.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1971}, {"title": "The capacity-achieving input distribution for some ampli- tude-limited channels with additive noise", "author": ["W. Oettli"], "venue": "IEEE Trans. Inform. Theory, vol. IT?20, p. 372?374, May 1974.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1974}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 689\u2013692.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2015, pp. 448\u2013456.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic cores", "author": ["A. Amir", "P. Datta", "W.P. Risk", "A.S. Cassidy", "J.A. Kusnitz", "S.K. Esser", "A. Andreopoulos", "T.M. Wong", "M. Flickner", "R. Alvarez-Icaza"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u201310. 16", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Compass: a scalable simulator for an architecture for cognitive computing", "author": ["R. Preissl", "T.M. Wong", "P. Datta", "M. Flickner", "R. Singh", "S.K. Esser", "W.P. Risk", "H.D. Simon", "D.S. Modha"], "venue": "SC Conference on High Performance Computing Networking, Storage and Analysis, 2012, p. 54. 17", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1, 2].", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "1 Introduction Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1, 2].", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 3, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 4, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 5, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 6, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 7, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 8, "context": "Binary convolutional networks that use binary convolutional kernels, and binary neuron activations are ideally suited to be run on low-power neuromorphic architectures that use spike-based communication [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 9, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 10, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 11, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 12, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 13, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 14, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 6, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 15, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 16, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 17, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 18, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 19, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 20, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 8, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 21, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 22, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 23, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 24, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 25, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 20, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 8, "context": "By connecting the efficient weight representation schema used in neuromorphic architectures [9, 21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are ar X iv :1 60 6.", "startOffset": 92, "endOffset": 99}, {"referenceID": 20, "context": "By connecting the efficient weight representation schema used in neuromorphic architectures [9, 21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are ar X iv :1 60 6.", "startOffset": 92, "endOffset": 99}, {"referenceID": 26, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 27, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 28, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 29, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 30, "context": "We incorporate learning structured convolutional matrices into traditional stochastic gradient descent [31] so that the trained inference networks are hardware-ready.", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "We evaluate our system on Cifar10 data set and compare against best energy vs accuracy numbers reported on currently available hardware [3] \u2013 our approach reduces the number of TrueNorth cores required to achieve 87.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "5% on Cifar10 from 31872 TrueNoth cores in [3] to 13216 cores.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "We begin the next section by discussing binary convolutional networks and their suitability for neuromorphic hardware and introduce the weight representation mechanism used in TrueNorth architecture [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "2 Sparse Binary convolutional networks Binary convolutional networks [5, 3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks.", "startOffset": 69, "endOffset": 75}, {"referenceID": 2, "context": "2 Sparse Binary convolutional networks Binary convolutional networks [5, 3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks.", "startOffset": 69, "endOffset": 75}, {"referenceID": 31, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 3, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 4, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 7, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 5, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 2, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 8, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 22, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 23, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 24, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 25, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 20, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 8, "context": "A TrueNorth chip consists of a network of neurosynaptic cores with programmable connectivity, synapses, and neuron parameters [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 32, "context": "TrueNorth neurons use a variant of an integrateand-fire model with 23 configurable parameters [33] where a neuron\u2019s state variable updates each tick (typically at 1000 ticks per second, though higher rates are possible).", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "To account for the fact that a TrueNorth neuron can connect to at most 256 input features, as in [3], network structure in this paper constrained by partitioning features in each layer into one or more Which is simply the element-wise product between matrices 2", "startOffset": 97, "endOffset": 100}, {"referenceID": 33, "context": "3 Symmetric kernels When discrete convolution is written as a matrix multiplication, the resulting \u2018convolution matrix\u2019 has a block Toeplitz structure3 [34].", "startOffset": 152, "endOffset": 156}, {"referenceID": 33, "context": "Fact 1 (See for example, [34]): If W denotes the convolution matrix such that vecpX \u02c7Kq \u201c vecpXqW (2) then W is a block Toeplitz matrix of the form W \u201c \u00bb", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 35, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 36, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 37, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 38, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 39, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 40, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 41, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 42, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 43, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 44, "context": "An early attempt at using noisy ReLU with the objective of obtaining binary activations was reported in [45] for single-layer networks.", "startOffset": 104, "endOffset": 108}, {"referenceID": 45, "context": "It is well-known in information theory [46, 47] that the best way to communicate using an amplitudebounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal.", "startOffset": 39, "endOffset": 47}, {"referenceID": 46, "context": "It is well-known in information theory [46, 47] that the best way to communicate using an amplitudebounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal.", "startOffset": 39, "endOffset": 47}, {"referenceID": 47, "context": "3 Systems and methods The networks are trained on GPUs using MatConvNet deep learning libraries [48].", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "The stochastic gradient descent was used with dropout [49] layers, momentum (0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "9), weight decay (10 \u03016), and batch normalization [50].", "startOffset": 50, "endOffset": 54}, {"referenceID": 50, "context": "The parameters learned through training are mapped to hardware using reusable, composable network description functions called corelets [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 51, "context": "The CIFAR-10 dataset [52] consists of color natural images, 32 x 32 pixels in size, in 10 classes, with 50000 training images and 10000 test images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 7, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 2, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 52, "context": "The 4-Chip networks were run on simulation [53], and classification accuracies were measured for the dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The results are shown in Table 2 in comparison with Energy-efficient deep networks (EEDN) on TrueNorth [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "State of Multi-Chip Networks 1-Chip Networks the Art EEDN [3] Symmetric Kernel EEDN Symmetric Kernel Accuracy Accuracy #Cores Accuracy #Cores Accuracy #Cores FPS Accuracy #Cores FPS 91.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "By using symmetric kernels, we have been able to deploy a network with twice as many features on the same number of cores as was possible using the approach described in [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Thus, we have been able to obtain better accuracy for the same amount of hardware even though the set of allowed convolutional kernels are a strict subset of the set of t \u03011, 0, 1u-valued kernels allowed in [3].", "startOffset": 207, "endOffset": 210}], "year": 2016, "abstractText": "We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.", "creator": "LaTeX with hyperref package"}}}