{"id": "1606.02555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "abstract": "In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.", "histories": [["v1", "Wed, 8 Jun 2016 13:47:18 GMT  (188kb,D)", "http://arxiv.org/abs/1606.02555v1", "21 pages, 4 figures"]], "COMMENTS": "21 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marco dinarelli", "isabelle tellier"], "accepted": false, "id": "1606.02555"}, "pdf": {"name": "1606.02555.pdf", "metadata": {"source": "CRF", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "authors": ["Marco Dinarelli", "Isabelle Tellier"], "emails": ["marco.dinarelli@ens.fr,", "isabelle.tellier@univ-paris3.fr"], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "2 Improving Recurrent Neural Networks", "text": "The RNNs we are looking at in this paper have the same architecture that is used for Feedforward Neural Network Language Models (NNLM) as described in [21]. In this architecture, we have four layers: input, embedding, hidden, and output. Words are given as input to the network as indexes corresponding to their position in a dictionary V. The index of a word is used to choose its embedding (or distribution representation) in a real-valued matrix. We call E (v (wt) the embedding of the word w as input at position t of a sequence, | V | where the size of the dictionary and N the dimensionality of the embedding (which is a parameter to be selected). We call the embedding of the word w as input at position t of a sequence."}, {"heading": "2.1 RNN Learning", "text": "Learning the described RNNs consists in learning the parameters \u0442 = (E, H, O, R) between each pair of layers (see Figure 1) and avoiding distortions in order to keep the notation easier. We use a cross-entropy cost function between the expected label ct and the predicted label yt at position t in the sequence, plus an L2 regularization term [22]: C = \u2212 ct \u00b7 log (yt | I, \u0439) + \u03bb2 | \u0432 | 2 (1) \u03bb is a hyperparameter of the model. Since yt is a probability distribution over the output marks, we can also consider the output of an RNN as the probability of the predicted label yt: P (yt | I, \u0439). I am the input to the network plus the contextual information provided by the recurring connection. For Elman RNN IElman = wt \u2212 chaw \u2212 the preceding text, the input is \u2212 the word and the output."}, {"heading": "2.2 Learning Variants", "text": "One important choice for learning RNN models relates to the back-propagation quer quer algorithm (BPTT). In fact, due to the repeatability of their architecture to properly learn RNNs, the back-propagation Through-Time quantum algorithm (BPTT) should be used [23]. The BPTT algorithm essentially consists of unfolding the recurring architecture for a selected number of steps and then learning the network as the standard feed-forward network. This is intended to allow RNNs to learn arbitrarily long-forgotten contexts. However, [10] has shown that RNNs for voice modelling are best to learn quantum quers with only 5 time steps in the past. This could be due to the fact that at least in NLP quantum quer tasks the past information held by the network about the recurring architecture \"in the memory\" quer quer quer quer quer quer quers actually disappear after a few time steps. [10] In addition, RNNNs for language modelling the quer quer quer quer quer quer quer quer is best to be used with many times in the past quer quers."}, {"heading": "2.3 New Variants of RNN", "text": "As mentioned in the introduction, the new variants of RNN in this paper show two differences in relation to traditional RNNs, which have two different terms in relation to conventional RNNs: i) the recurring connection is from the output level to the input level, which means that the labels addressed are computed in the same way as the hidden layer activities in a slightly different way. Instead, in our variants of RNNs, words and labels are linked together and embedded in the hidden layer. The most interesting consequence of the changes in our RNN variants is the fact that the output labels are mapped in distributed representations, as is usually done for input items. In fact, the first layer of our network is just a mapping of distributed representations."}, {"heading": "2.4 Forward, Backward and Bidirectional RNNs", "text": "All RNNs described in this paper are examined in their forward, backward and bidirectional versions [3]. Reverse RNNs work as described before. Reverse RNNs have exactly the same architecture, the only difference being that they process sequences in reverse order, from end to beginning. Reverse RNNs can be used to predict future labels in a sequence. Bidirectional RNNs use both past and future information to predict the next label, both words and labels in our variants and in Jordanian RNNNs, or both words and hidden layers in Elman RNNNs. When labeling sequences with bidirectional RNNNNs, a backward-facing network is used to predict labels backwards, and the bidirectional RNN then processes sequences in the forward direction of past textual information as usual and in future RNNNNNs."}, {"heading": "2.5 Recurrent Neural Network Complexity", "text": "We offer here an analysis of the complexity in terms of the number of parameters that play a role in each model. | In Jordan RNN we count: | V | \u00d7 D + (2w + 1) D + c |) \u00d7 | H | + | H | | V | is roughly the size of the input dictionary, D the dimensionality of the embedding, | H | and | O | are the size of the hidden and output layers, respectively w the size of the window of the words that are used as context on the input site6 and c is the size of the context of the labels multiplied by the dimensionality of the output label dictionary. | O | Using the same symbols, in an Elman page of the words that are used as context."}, {"heading": "3 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Corpora", "text": "We used four different corporas: the task of the Air Travel Information System (ATIS) was designed to automatically provide flight information in SLU systems; the semantic representation is frame-based and the goal is to find the right frame and corresponding semantic slots. For example, the correct frame is FLIGHT and the words Boston, Philadelphia and today must be commented on using the concepts DEPARTURE.CITY, ARRIVALE and DEPARTURE.DATE, a relatively simple task dating back to 1993; the training consists of 4978 sentences drawn from the \"context-independent\" data in the areas of ATIS-2 and ATIS-3 companies; the test set consists of 893 sets taken from the ATIS-3 and DEC94 datasets."}, {"heading": "3.2 RNNs Parameters Settings", "text": "In fact, most people are able to decide whether they will be able to hold on to power or whether they will be able to hold their own."}, {"heading": "3.3 Training and Tagging Time", "text": "Since the implementations of the RNNs in this work are prototypes, it does not make sense to compare them with the state of the art in terms of training and meeting time. However, it is worth offering training times in order to have at least an idea and a comparison between the different RNNs. 11Our implementations are essentially written in the same order of magnitude as the size of the issue layer (i.e. the number of labels), including our first variant I-RNN and the Jordanian RNN, which has roughly the same complexity. The size of the hidden layer is also of the same magnitude as the size of the issue layer (i.e. the number of labels), and Elman RNN has approximately the same complexity as the Jordanian RNN. This is reflected in the training time. The training time for label embedding is always relatively short, as the size of the issue layer (i.e. the number of labels) is approximately the same."}, {"heading": "3.4 Sequence Labeling Results", "text": "This year, it will be able to introduce the aforementioned brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-consecrated brain-csrteeeaeFnl.n"}, {"heading": "4 Conclusions", "text": "We have proposed two new variants of RNNs to better model label dependencies, and we have compared these variants with the traditional architectures of Elman and Jordan RNNNs. We have explained the advantages that the proposed variants offer compared to previous RNNs. We have evaluated all RNNs, whether new or traditional, against four different tasks: two in language comprehension and two in POS tagging. Results show that our new variants of RNNNs always outperform traditional Elman and Jordan RNNNs, even if they do not always improve the state of the art."}], "references": [{"title": "Serial order: A parallel, distributed processing approach", "author": ["M.I. Jordan"], "venue": "Advances in Connectionist Theory: Speech", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "COGNITIVE SCIENCE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "Trans. Sig. Proc", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M.Y. Hwang", "Y. Shi", "D. Yu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "CCG supertagging with a recurrent neural network. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015", "author": ["W. Xu", "M. Auli", "S. Clark"], "venue": "July 26-31,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Unsupervised and lightly supervised partof-speech tagging using recurrent neural networks", "author": ["O. Zennaki", "N. Semmar", "L. Besacier"], "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, PACLIC 29,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Spoken language understanding: A survey", "author": ["R. De Mori", "F. Bechet", "D. Hakkani-Tur", "M. McTear", "G. Riccardi", "G. Tur"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Expanding the scope of the atis task: The atis-3 corpus", "author": ["D.A. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke-Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the Workshop on Human Language Technology. HLT \u201994,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Results of the french evalda-media evaluation campaign for literal understanding", "author": ["H. Bonneau-Maynard", "C. Ayache", "F. Bechet", "A. Denis", "A. Kuhn", "F. Lef\u00e8vre", "D. Mostefa", "M. Qugnard", "S. Rosset", "Servan", "J.S. Vilaneau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Building a Treebank for French. In: Treebanks : Building and Using Parsed Corpora", "author": ["A. Abeill\u00e9", "L. Cl\u00e9ment", "F. Toussenel"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Coupling an annotated corpus and a lexicon for state-of-theart pos tagging", "author": ["P. Denis", "B. Sagot"], "venue": "Lang. Resour. Eval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "CoRR abs/1206.5533", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE. Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Text chunking using transformation-based learning", "author": ["L. Ramshaw", "M. Marcus"], "venue": "Proceedings of the 3rd Workshop on Very Large Corpora,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "In Kremer, Kolen, eds.: A Field Guide", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 3, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 4, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 5, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 6, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 7, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 1, "context": "In the literature about RNNs for NLP, two main variants have been proposed, also called \u201csimple\u201d RNNs: the Elman [2] and the Jordan [1] RNN models.", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "In the literature about RNNs for NLP, two main variants have been proposed, also called \u201csimple\u201d RNNs: the Elman [2] and the Jordan [1] RNN models.", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 95, "endOffset": 102}, {"referenceID": 9, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 95, "endOffset": 102}, {"referenceID": 5, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 6, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 7, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 10, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 11, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 12, "context": "with word2vec, have already shown their ability to capture very attractive syntactic and semantic properties [13,14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "with word2vec, have already shown their ability to capture very attractive syntactic and semantic properties [13,14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "This effect comes from the syntactic and semantic properties that embeddings can encode [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "All RNNs in this article are studied in their forward, backward and bidirectional versions [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 18, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 19, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The RNNs we consider in this work have the same architecture also used for Feedforward Neural Network Language Models (NNLM), described in [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 21, "context": "We use a cross-entropy cost function between the expected label ct and the predicted label yt at the position t in the sequence, plus a L2 regularization term [22]:", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "2We also find [7] quite easy to understand, even for readers not familiar with RNNs.", "startOffset": 14, "endOffset": 17}, {"referenceID": 21, "context": "We use the back-propagation algorithm and the stochastic gradient descent with momentum [22] for learning the weights \u0398.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "Indeed, because of the recurrent nature of their architecture, in order to properly learn RNNs the Back-Propagation Through Time algorithm (BPTT) should be used [23].", "startOffset": 161, "endOffset": 165}, {"referenceID": 9, "context": "However, [10] has shown that RNNs for language modelling learn best with just 5 time steps in the past.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Since BPTT is quite more expensive than the traditional back-propagation algorithm, [7] has preferred to use explicit output context in Jordan RNNs and to learn the model with the traditional back-propagation algorithm, not surprisingly without loosing performance.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "In this work we use the same variant as [7].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "features and attractive syntactic and semantic properties, as shown by word2vec and similar works [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "It is worth noting that the idea of using label embeddings has been introduced by [24] in the context of dependency parsing.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "The hidden layer activities are computed as: ht = \u03a3([ItLt] \u00b7H) \u03a3 is the sigmoid activation function [22], [\u00b7] means the concatenation of the two matrices and we omit biases to keep notations lighter.", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "distributed representations [14], wrong labels have very similar representations to the correct ones.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Taking an example cited in [14]: if we use Paris instead of Rome, it has no effect for many NLP tasks, as they are both proper nouns for POS-tagging, locations for named entity recognition etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Mixing input processing It \u00b7 H and label processing y[t \u2212 i] \u00b7 R with a sum, can make sense for the tasks for which the Jordan network was designed [1], as output units were of the same nature as input units (speech signal).", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "All RNNs described in this work are studied in their forward, backward and bidirectional versions [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "It is very similar for our second variant, refer to [3] for details.", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "We used four distinct corpora: The Air Travel Information System (ATIS) task [16] has been designed to automatically provide flight information in SLU systems.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "8 The French corpus MEDIA [17] has been created to evaluate SLU systems providing tourist information, in particular hotel information in France.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "8 Please see [16] for more details on the ATIS corpus.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Both ATIS and MEDIA can be modelled as sequence labeling tasks using the BIO chunking notation [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 6, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 7, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 25, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 7, "context": "[8] is the only work providing results on MEDIA with RNNs, it also provides results obtained with CRFs [27], allowing an interesting comparison.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[8] is the only work providing results on MEDIA with RNNs, it also provides results obtained with CRFs [27], allowing an interesting comparison.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "The French Treebank (FTB) corpus is presented in [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "The version we use for POS-tagging is exactly the same as in [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "The Penn Treebank (FTB) corpus is presented in [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 16, "endOffset": 19}, {"referenceID": 25, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "10 [19] also provides results without using the external lexicon", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In contrast to [19], which has used several features of words (prefixes, suffixes, capitalisation information etc.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "This language model is like the one in [21], except it uses both words/labels in the past and in the future to predict next word/label.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "The results in the higher part of table 4 show that the best model on the ATIS task, with these settings, is the Elman RNN of [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Note that it is not clear how the improvements of [26] with respect to [7] (in part due to same authors) have been obtained.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "Note that it is not clear how the improvements of [26] with respect to [7] (in part due to same authors) have been obtained.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Indeed, in [7] authors obtain the best result with a Jordan RNN, while in [26] an Elman RNN gets the best performance.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Indeed, in [7] authors obtain the best result with a Jordan RNN, while in [26] an Elman RNN gets the best performance.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "During our experiments, using the same experimentation protocol as [26], we could not reach the same performances.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "We conclude that the difference between our results and those in [26] are due to reasons mentioned above.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "Beyond this, we note that our Elman and Jordan RNN implementations are equivalent to those of [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "This limitation is confirmed by the results obtained by [8] and [26] using CRFs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 25, "context": "This limitation is confirmed by the results obtained by [8] and [26] using CRFs.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "We can also see that our implementations of Elman and Jordan RNNs are comparable, even better in the case of Elman RNN, with state-of-the-art RNNs of [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "More importantly, results on the MEDIA task shows that in this particular experimental settings where taking label dependencies into account is crucial, our new variants are remarkably more effective than both our implementations and state-of-the-art implementations [8] of Elman and Jordan RNNs.", "startOffset": 267, "endOffset": 270}, {"referenceID": 18, "context": "On this task, we compare our RNN implementations to the state-of-the-art results obtained in [19] with the modelMEltfr.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "We would like to underline thatMElt 0 fr, when it does not use external resources like in the model obtaining the best absolute result in [19], nevertheless uses several features associated with words that provide an advantage over features used in our RNNs.", "startOffset": 138, "endOffset": 142}, {"referenceID": 27, "context": "We also provide the results of [28] and [29] for comparison with the state-of-the-art.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "We also provide the results of [28] and [29] for comparison with the state-of-the-art.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Instead, we can roughly compare our results with those of [5], which were also obtained with neural networks.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Note that this is a rough comparison as the model of [5], though not a RNN, integrates capitalisation features and uses a convolution and a max-over-time layer to encode large context information.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Our RNNs don\u2019t improve the state-of-the-art, but are all more effective than the model of [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "This result is particularly important, as it shows that RNNs, even without using a sophisticated encoding of the context like the model NN+SLL in [5],", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "This claim is enforced also by the fact that NN+SLL of [5] implements a global probability computation strategy similar to CRF (SLL stands for Sentence Level Likelihood), while all RNNs presented here use a local decision function (see equation 2).", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "[8] also shows results obtained with embeddings pre-trained with", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "A similar idea of hybrid RNN model has been tested in [26] without showing a clear advantage on Elman and Jordan RNNs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "Words [7] E-RNN 93.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "12% \u2013 [7] J-RNN 93.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "98% [26] E-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "Classes [8] E-RNN 96.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "16% \u2013 \u2013 [8] CRF \u2013 \u2013 95.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "23% [6] E-RNN 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 25, "context": "04% \u2013 \u2013 [26] E-RNN 96.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "29% [26] CRF \u2013 \u2013 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "forward backward bidirectional [8] E-RNN 81.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "94% \u2013 \u2013 [8] J-RNN 83.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "25% \u2013 \u2013 [8] CRF \u2013 \u2013 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "[19] MElt0fr \u2013 \u2013 97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "forward backward bidirectional [28] \u2013 \u2013 97.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "24% [29] \u2013 \u2013 97.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "33% [5] NN+SLL \u2013 \u2013 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 30, "context": "This problem is someway similar to the \u201cvanishing gradient\u201d problem [31]: as the network learns, the probability gets concentrated on few dimensions and all the other values get very small, limiting network learning.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.", "creator": "LaTeX with hyperref package"}}}