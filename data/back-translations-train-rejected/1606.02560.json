{"id": "1606.02560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", "histories": [["v1", "Wed, 8 Jun 2016 14:03:25 GMT  (2922kb,D)", "http://arxiv.org/abs/1606.02560v1", "8 pages. Under peer review of to SigDial 2016"], ["v2", "Thu, 15 Sep 2016 21:50:30 GMT  (3036kb,D)", "http://arxiv.org/abs/1606.02560v2", "In proceeding of SIGDIAL 2016. Added changes based-on peer review, including: 1. Added references, 2. fixed typos in text and figures, 3. added minor change to introduction"]], "COMMENTS": "8 pages. Under peer review of to SigDial 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["tiancheng zhao", "maxine eskenazi"], "accepted": false, "id": "1606.02560"}, "pdf": {"name": "1606.02560.pdf", "metadata": {"source": "CRF", "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "authors": ["Tiancheng Zhao"], "emails": ["max+}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us will be able to feel the way they are, and that they will be able to survive on their own, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to do what we are doing to change the world. \""}, {"heading": "2 Related Work", "text": "In fact, it is as if most of them will be able to abide by the rules that they have applied in practice. (...) In fact, it is as if they are able to determine how they have behaved. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to obey the rules. (...) It is as if they were able to break the rules. (...) It is as if they were able to break the rules. (...) It is as if they were able to break the rules. (...)"}, {"heading": "3 Deep Reinforcement Learning", "text": "Before describing the proposed algorithms, let us briefly consider deep amplification learning (RL). RL models are based on the Markov decision-making process (MDP); an MDP is a tuple (S, A, P, \u03b3, R), where S is a series of states; A is a series of actions; P defines the transition probability P (s \u2032 | s, a); R defines the expected immediate reward R (s, a); and \u03b3 [0, 1) is the discounting factor. The goal of amplification learning is to find the optimal policy p (s) so as to maximize the expected cumulative return (Sutton and Barto, 1998). MDPs assume the full observability of the internal states of the world, which is rarely true for real-world applications; the partially observable Markov decision-making process (POMDP) takes into account uncertainty in the state variable."}, {"heading": "3.1 Deep Q-Network", "text": "The deep Q network (DQN) introduced by Mnih (2015) uses a deep neural network (DNN) to parameterise the Q value function Q (s, a; \u03b8), and achieves performance on a human level in many Atari games. DQN retains two separate models: a target network \u03b8 \u2212 i and a behavioural network \u03b8i. For each new K sample, DQN \u03b8 \u2212 i uses the target values yDQN to calculate and update the parameters in Phenomeni. Only after each C update are the new weights of the phenomena copied to \u03b8. Furthermore, DQN uses the experience reproduction to store all previous experience levels (s, a, r, s \u2032). Before a new model update, the algorithm stitches a minibatch of experiences of size M from the memory and calculates the gradient of the following loss function: L (previ) (E \u2032 r)."}, {"heading": "3.2 Deep Recurrent Q-Network", "text": "An extension of the DQN is a Deep Recurrent QNetwork (DRQN) that introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) over the revolutionary layer of the original DQN model (Hausknecht and Stone, 2015) that enables DRQN to solve POMDPs. Therefore, the recursive neural network can be considered an approximation of the faith state that can aggregate information from a sequence of observations. Hausknecht (2015) shows that DRQN performs significantly better than DQN when an agent observes only partial states. A similar model was proposed by Narasimhan and Kulkarni (2015) and learns to play multi-user dungeons (Curtis, 1992) with game states hidden in paragraphs of natural language."}, {"heading": "4 Proposed Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Overview", "text": "End-to-end learning refers to models that can propagate error signals from the final output to the raw data. Before working in end-to-end state tracking (Henderson et al., 2014), we learn a sequential classifier that estimates the dialogue status based on the ASR output without requiring an NLU. Rather than treating state tracking as a standard supervised learning task, we propose to unify dialogue state tracking with the dialogue policy so that both are treated as measures available to an enhanced learning agent. Specifically, we learn an optimal policy that either elicits verbal response or modifies the current estimated dialogue state based on the new observations. This formulation allows a state tracker to be obtained without the marked data required for the DSTC, as long as the rewards of users and databases are available. Furthermore, in cases where these modes are available with minimal state tracker speed, its unified labels can be used significantly."}, {"heading": "4.2 Learning from the Users and Databases", "text": "Figure 2 shows an overview of the framework. We look at a task-oriented dialog task in which there are S slots, each with cardinality Ci, i, i, S. The environment consists of a user, Eu, and a database Edb. The agent can send verbal actions to the user, av, Av, and the user will respond with natural language answers ou and rewards ru. In order to interact with the database environment Edb, the agent can perform special actions ah, ah, Ah that can modify a query hypothesis h. The hypothesis is a slot fill-in form that represents the most likely slot values in light of the evidence observed. Considering this hypothesis, h, the database can perform a normal query and the results as observations, odb, and rewards rdb. At each turn, the agent applies his selected action. {Av, Ah} and receives the observations from either the user or the database."}, {"heading": "4.3 Incorporating State Tracking Labels", "text": "The pure RL approach described in the previous section may suffer from slow convergence if the cardinality of slots b = b = expensive. This is due to the nature of reinforcement learning: that it has to try different actions (possible values of a slot) to estimate the expected long-term benefit. On the other hand, a supervised classifier can learn much more efficiently (a typical multi-class classification loss function (e.g. categorical cross entropy) assumes that there is a single correct label that promotes the likelihood of the correct label and suppresses the probabilities of the wrong one. Modelling dialogue state tracking as a Q-value function has advantages over a local classifier. For example, let's take the situation where a user wants to send an email and the state tracker has to estimate the user's goal from three possible values: send, edit, and delete Qnamics values are treated as undesirable in a classification task equally."}, {"heading": "4.4 Implementation Details", "text": "We can optimize the network architecture in several ways to improve its efficiency: Common State Tracking Policy: It is more efficient to bind the weights of political networks for similar slots and to use the index of slots as input, which can reduce the number of parameters to be learned and promote common structures; the studies in Section 5 illustrate an example; Limited Action Mask: We can restrict the available actions at each turn to force the agent to switch between verbal response and slot filling; We define Amask as a function that takes the state and displays a set of available actions for: Amask (s) = Ah if there are new inputs (6) = Av otherwise (7) reward forms based on the database: Users \"reward signals are usually sparse (at the end of a dialogue), but the database can provide frequent rewards for the agent. Reward forms are a technique used to accelerate learning."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 20Q Game as Task-oriented Dialog", "text": "To test the proposed framework, we chose the 20-question game (20Q). The game rules are as follows: At the beginning of each game, the user thinks of a famous person. Then, the user asks the user a series of yes / no questions. The user answers honestly, using one of three answers: Yes, No or I don't know. To resemble a dialogue, our user can respond with any natural utterance that represents one of the three intentions. The user can make guesses at any point, but a false assumption leads to a negative reward.... Sys: I value the right person within a maximum number of rounds with the least number of false guesses. A sample game conversation is as follows: Sys: Is that person male? User: Yes, I think so. Sys: Is that person an artist? User: He is not an artist.... Sys: I guess that person is Bill Gates. User: Correct. We can formulate the game as a slot filling dialog."}, {"heading": "5.2 Simulator Construction", "text": "We have designed a simulator for 20Q. The simulator consists of two parts: a database of 100 famous people and a user simulator. We have selected 100 people from Freebase (Bollacker et al., 2008), each of whom has 6 attributes: place of birth, degree, gender, profession and birthday. We have manually designed several yes / no questions for each attribute available to the agent. Each question covers a different set of possible values for a particular attribute and thus carries a different discriminatory force to determine the person the user is thinking of. Consequently, the agent must carefully select the question based on the context of the game in order to limit the range of valid persons. There are 31 questions. Table 1 shows a summary. At the beginning of each game, the simulator will first select a person from the database as the person he is thinking of."}, {"heading": "5.3 Training Details", "text": "The Eu user environment is the simulator that accepts only verbal actions, either a yes / no question or a guess, and responds with a natural linguistic expression. Therefore, Av | Q | + 1 contains actions where the first | Q | actions are questions and the last act is a guess, given the results from the database. The database environment reads h in a query hypothesis and provides a list of people who meet the limitations in the query. h has a size of | Q | and each dimension can be one of the three values: yes / no / unknown. Since the cardinality is the same for all slots, we only need 1 slot-filling political network with 3 Q value outputs for yes / no / unknown to change the value of the most recent question asked, which is the common policy approach mentioned in Section 4."}, {"heading": "5.4 Dialog Policy Analysis", "text": "We compare the performance of three models: a strong modular baseline, RL and hybrid RL. The baseline has an independently trained state tracker and dialogue policy. The state tracker is also an LSTM-based classifier that enters a dialogue history and predicts the slot value of the current question. Dialogue policy is a DRQN that presupposes the perfect filling of the slots during training and simply controls the next verbal action. Therefore, the main difference between the baseline and the proposed models is that the state tracker and dialogue policy are not developed jointly. As hybrid RL effectively changes the reward function, the typical average cumulative reward size is not applicable to the performance comparison. Therefore, we directly compare the win rate and the average game time in later discussions. Table 3 shows that both proposed models achieve a significantly higher win rate than the base function by asking more questions before making assumptions about the learning process, but that the learning speed of the three models is insufficient because the understanding of the speed of the learning process is sufficient."}, {"heading": "5.5 State Tracking Analysis", "text": "One of the hypotheses is that the RL approach can only learn a good status tracker with the help of success signals in dialogue. We conducted the best trained models with a greedy policy and collected 10,000 samples. Table 4 reports on the precision and recall of filling slots in these trajectories. The results suggest that the RL model learns a completely different strategy from the baseline. The RL model aims for high precision so that it unknowingly predicts when the input is ambiguous, which is a safer option than predicting yes / no, as confusion between yes and no can potentially lead to contradiction and gaming failure. This is very different from the baseline, which does not differentiate between false labels. Therefore, although the baseline achieves better classification metrics, it does not take into account the long-term payout and leads to suboptimal overall performance."}, {"heading": "5.6 Dialog State Representation Analysis", "text": "The tracking of the state over several rounds is crucial because the optimal action of the agent depends on the story, e.g. the question he has already asked, the number of assumptions he has issued. In addition, one of the assumptions is that the output of the LSTM network is an approximation of the state of belief in the POMDP. We conducted two studies to test these hypotheses. For both studies, we conducted the hybrid RL models stored at 20K, 50K and 100K against the simulator with a greedy policy and collected 10,000 samples for each model. The first study verifies whether we can reconstruct an important state feature: the number of assumptions the agent made from the dialogue-state embedding. We divide the collected 10,000 samples into 80% for training and 20% for testing. We used the LSTM output as an input characteristic to a linear regression model with l2 regulation, the second study shows that the correlation of the state is increased."}, {"heading": "6 Conclusion", "text": "This paper identifies the limitations of the conventional SDS pipeline and describes a novel end-to-end framework for a task-oriented dialog system using in-depth reinforcement learning. We evaluated the model on the 20Q Game. The proposed models demonstrate superior performance in understanding both natural language and dialog strategies. Furthermore, our analysis confirms our hypotheses that the proposed models implicitly capture essential information in latent dialog states. Future studies will include the development of full-fledged task-oriented dialog systems using the proposed approach and the exploration of methods that allow easy integration of domain knowledge so that the system can be debugged and corrected more easily."}, {"heading": "7 Acknowledgements", "text": "This work was funded by the NSF grant CNS1512973. Opinions expressed in this essay do not necessarily reflect those of the NSF. We would also like to thank Alan W Black for the discussions on this essay."}], "references": [{"title": "Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda", "author": ["Dan Bohus", "Alexander I Rudnicky"], "venue": null, "citeRegEx": "Bohus and Rudnicky.,? \\Q2003\\E", "shortCiteRegEx": "Bohus and Rudnicky.", "year": 2003}, {"title": "A k hypotheses+ otherbelief updating model", "author": ["Dan Bohus", "Alex Rudnicky."], "venue": "Proc. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems.", "citeRegEx": "Bohus and Rudnicky.,? 2006", "shortCiteRegEx": "Bohus and Rudnicky.", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Mudding: Social phenomena in text-based virtual realities", "author": ["Pavel Curtis."], "venue": "High noon on the electronic frontier: Conceptual issues in cyberspace, pages 347\u2013374.", "citeRegEx": "Curtis.,? 1992", "shortCiteRegEx": "Curtis.", "year": 1992}, {"title": "Gaussian processes for fast policy optimisation of pomdp-based dialogue managers", "author": ["M Ga\u0161i\u0107", "F Jur\u010d\u0131\u0301\u010dek", "Simon Keizer", "Fran\u00e7ois Mairesse", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2010}, {"title": "Reinforcement learning of argumentation dialogue policies in negotiation", "author": ["Kallirroi Georgila", "David R Traum."], "venue": "INTERSPEECH, pages 2073\u2013 2076.", "citeRegEx": "Georgila and Traum.,? 2011", "shortCiteRegEx": "Georgila and Traum.", "year": 2011}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew Hausknecht", "Peter Stone."], "venue": "arXiv preprint arXiv:1507.06527.", "citeRegEx": "Hausknecht and Stone.,? 2015", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 292\u2013", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Switchboard swbd-damsl shallow-discoursefunction annotation coders manual", "author": ["Dan Jurafsky", "Elizabeth Shriberg", "Debra Biasca."], "venue": "Institute of Cognitive Science Technical Report, pages 97\u2013102.", "citeRegEx": "Jurafsky et al\\.,? 1997", "shortCiteRegEx": "Jurafsky et al\\.", "year": 1997}, {"title": "Pomdpbased let\u2019s go system for spoken dialog challenge", "author": ["Sungjin Lee", "Maxine Eskenazi."], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 61\u201366. IEEE.", "citeRegEx": "Lee and Eskenazi.,? 2012", "shortCiteRegEx": "Lee and Eskenazi.", "year": 2012}, {"title": "Extrinsic evaluation of dialog state tracking and predictive metrics for dialog policy optimization", "author": ["Sungjin Lee."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 310.", "citeRegEx": "Lee.,? 2014", "shortCiteRegEx": "Lee.", "year": 2014}, {"title": "Temporal supervised learning for inferring a dialog policy from example conversations", "author": ["Lihong Li", "He He", "Jason D Williams."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 312\u2013317. IEEE.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "State of the arta survey of partially observable markov decision processes: theory, models, and algorithms", "author": ["George E Monahan."], "venue": "Management Science, 28(1):1\u201316.", "citeRegEx": "Monahan.,? 1982", "shortCiteRegEx": "Monahan.", "year": 1982}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "arXiv preprint arXiv:1506.08941.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell."], "venue": "ICML, volume 99, pages 278\u2013287.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Lets go public! taking a spoken dialog system to the real world", "author": ["Antoine Raux", "Brian Langner", "Dan Bohus", "Alan W Black", "Maxine Eskenazi."], "venue": "in Proc. of Interspeech 2005. Citeseer.", "citeRegEx": "Raux et al\\.,? 2005", "shortCiteRegEx": "Raux et al\\.", "year": 2005}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver."], "venue": "arXiv preprint arXiv:1511.05952.", "citeRegEx": "Schaul et al\\.,? 2015", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research, pages 105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Richard S Sutton."], "venue": "Proceedings of the seventh international conference on machine learning, pages 216\u2013224.", "citeRegEx": "Sutton.,? 1990", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems", "author": ["Blaise Thomson", "Steve Young."], "venue": "Computer Speech & Language, 24(4):562\u2013588.", "citeRegEx": "Thomson and Young.,? 2010", "shortCiteRegEx": "Thomson and Young.", "year": 2010}, {"title": "Deep reinforcement learning with double qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver."], "venue": "arXiv preprint arXiv:1509.06461.", "citeRegEx": "Hasselt et al\\.,? 2015", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Jason D Williams", "Steve Young."], "venue": "Computer Speech & Language, 21(2):393\u2013422.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "The dialog state tracking challenge", "author": ["Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black."], "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 404\u2013413.", "citeRegEx": "Williams et al\\.,? 2013", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "Using pomdps for dialog management", "author": ["Steve J Young."], "venue": "SLT, pages 8\u201313.", "citeRegEx": "Young.,? 2006", "shortCiteRegEx": "Young.", "year": 2006}], "referenceMentions": [{"referenceID": 17, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 30, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 0, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 30, "context": "The typical structure of a task-oriented dialog system is outlined in Figure 1 (Young, 2006).", "startOffset": 79, "endOffset": 92}, {"referenceID": 12, "context": "The foremost challenge is that a task-oriented system must learn a strategic dialog policy that can achieve the goal of a given task which is beyond the ability of standard supervised learning (Li et al., 2014).", "startOffset": 193, "endOffset": 210}, {"referenceID": 29, "context": "Most industrial systems use rule-based heuristics to update the dialog state by selecting a high-confidence output from the NLU (Williams et al., 2013).", "startOffset": 128, "endOffset": 151}, {"referenceID": 1, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010).", "startOffset": 205, "endOffset": 256}, {"referenceID": 24, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010).", "startOffset": 205, "endOffset": 256}, {"referenceID": 29, "context": "The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs.", "startOffset": 43, "endOffset": 66}, {"referenceID": 29, "context": "In practice the output of the state tracker is used by a different dialog policy, so that the distribution in the training data and in the live data are mismatched (Williams et al., 2013).", "startOffset": 164, "endOffset": 187}, {"referenceID": 0, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010). The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs. In practice the output of the state tracker is used by a different dialog policy, so that the distribution in the training data and in the live data are mismatched (Williams et al., 2013). Therefore one of the basic assumptions of DSTC is that the state tracker\u2019s performance will translate to better dialog policy performance. Lee (2014) showed positive results following this assumption by showing a positive correlation between end-to-end dialog performance and state tracking performance.", "startOffset": 206, "endOffset": 820}, {"referenceID": 20, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 28, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 5, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 10, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 26, "context": "End-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015).", "startOffset": 183, "endOffset": 226}, {"referenceID": 19, "context": "End-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015).", "startOffset": 183, "endOffset": 226}, {"referenceID": 21, "context": "These methods train sequence-to-sequence models (Sutskever et al., 2014) on large human-human conversation corpora.", "startOffset": 48, "endOffset": 72}, {"referenceID": 22, "context": "The goal of reinforcement learning is to find the optimal policy \u03c0\u2217, such that the expected cumulative return is maximized (Sutton and Barto, 1998).", "startOffset": 123, "endOffset": 147}, {"referenceID": 14, "context": "It has been shown that the belief state is sufficient for optimal control (Monahan, 1982), so that the objective is to find \u03c0\u2217 : b \u2192 a", "startOffset": 74, "endOffset": 89}, {"referenceID": 24, "context": "L(\u03b8i) = E(s,a,r,s\u2032)[(y \u2212Q(s, a; \u03b8i))] (1) y = r + \u03b3max a\u2032 Q(s\u2032, a\u2032; \u03b8\u2212 i ) (2) Recently, Hasselt et al. (2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 18, "context": "(2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al. (2015) improves the convergence speed of DQN via prioritized experience replay.", "startOffset": 97, "endOffset": 118}, {"referenceID": 8, "context": "An extension to DQN is a Deep Recurrent QNetwork (DRQN) which introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs.", "startOffset": 110, "endOffset": 144}, {"referenceID": 6, "context": "An extension to DQN is a Deep Recurrent QNetwork (DRQN) which introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs.", "startOffset": 205, "endOffset": 233}, {"referenceID": 3, "context": "Kulkarni (2015) and learns to play Multi-User Dungeon (MUD) games (Curtis, 1992) with game states hidden in natural language paragraphs.", "startOffset": 66, "endOffset": 80}, {"referenceID": 7, "context": "tracking (Henderson et al., 2014) learns a sequential classifier that estimates the dialog state based on ASR output without the need of an NLU.", "startOffset": 9, "endOffset": 33}, {"referenceID": 23, "context": "This approach is closely related to the Dyna Q-Learning proposed in (Sutton, 1990).", "startOffset": 68, "endOffset": 82}, {"referenceID": 16, "context": "Ng et al. (1999) showed that potential-based re-", "startOffset": 0, "endOffset": 17}, {"referenceID": 2, "context": "We selected 100 people from Freebase (Bollacker et al., 2008), each of them has 6 attributes:", "startOffset": 37, "endOffset": 61}, {"referenceID": 9, "context": "In order to generate realistic natural language with the yes/no/unknown intent, we collected utterances from the Switchboard Dialog Act (SWDA) Corpus (Jurafsky et al., 1997).", "startOffset": 150, "endOffset": 173}], "year": 2016, "abstractText": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent QNetworks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", "creator": "TeX"}}}