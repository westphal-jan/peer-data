{"id": "1606.02580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Convolution by Evolution: Differentiable Pattern Producing Networks", "abstract": "In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution.", "histories": [["v1", "Wed, 8 Jun 2016 14:37:39 GMT  (1356kb,D)", "http://arxiv.org/abs/1606.02580v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["chrisantha fernando", "dylan banarse", "malcolm reynolds", "frederic besse", "david pfau", "max jaderberg", "marc lanctot", "daan wierstra"], "accepted": false, "id": "1606.02580"}, "pdf": {"name": "1606.02580.pdf", "metadata": {"source": "CRF", "title": "Convolution by Evolution Differentiable Pattern Producing Networks", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Malcolm Reynolds", "Frederic Besse", "David Pfau", "Max Jaderberg", "Marc Lanctot", "Daan Wierstra"], "emails": ["chrisantha@google.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords CPPNs, Compositional Pattern Producing Networks, Denoising Autoencoder, MNIST"}, {"heading": "1. INTRODUCTION", "text": "The idea behind it is that it is a way in which most people are able to put themselves in the world in which they are able to understand the world, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand and understand."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3. METHODS", "text": "Here we start with the introduction of the DPPN. Then we describe the general algorithm for optimizing a DPPN, which consists of an evolutionary part that contains a learning part in its inner loop."}, {"heading": "3.1 DPPNs", "text": "The DPPN is a modified implementation of a CPPN that can calculate the gradients of the functions in terms of weights. A CPPN is a function d that maps a coordinate vector ~ c to a vector of output values ~ p = (p1, p2,.., pn). The function is defined as a directed acyclic graph G = {N, E}, where N is a set of nodes and E is a set of edges between nodes. The amount of input and output nodes is usually fixed - one for each dimension of coordinate and output vectors. Each node has a set of input edges egg that can be changed by evolution, and a transfer function from a fixed list of nonlinearities associated with it. Each edge ej-ej-E has a weight node wj as well as input and output nodes inj, n-out function is defined by a fixed activation at the node."}, {"heading": "3.2 Denoising Autoencoders", "text": "The network enters a noisy version of the training data x, guides it through a series of layers f\u03b8 (x) = f n \u03b8n (f n \u2212 1 \u03b8n \u2212 1 (... f 1 \u03b81 (x)...)) with the parameters \u03b8 = {\u03b81,..., \u03b8n} and calculates a loss (x, f\u03b8 (x)) between the noiseless data and the prediction of the network. In our experiments we use the mean square error and the binary cross entropy. The loss function is usually minimized by some variant of the gradient drop. In the DPPN, the output parameters p are mapped directly into the parameters of the denosizing autoencoder."}, {"heading": "3.3 Optimisation Algorithm", "text": "It is not only the way in which the acquired knowledge is inherited from the parents, but also the way in which the acquired knowledge is inherited from the parents. We now describe the evolutionary algorithms, and the embedded learning effects are inherited from the children in more detail than the acquired knowledge is inherited from the parents. It is the way in which the acquired knowledge is inherited from the parents. We describe the evolutionary algorithms and the embedded learning effects from the children in more detail. 3.3.1 The evolutionary algorithms used by the parents are the microbial genetic algorithms (mGA) with a population of 50. Two random agents are selected, their weights are trained (see also the next evolutionary simple evolutionary algorithms)."}, {"heading": "3.4 Experiments", "text": "In this section we describe experiments on image reconstruction, character denoising, compression ratios, and generalization from MNIST to Omniglot.3.4.1 Image Reconstruction Experiments A single randomly chosen 28 \u00d7 28 MNIST digit is chosento be reconstructed. This is a simple benchmark task used to test various hyperparameter settings of the DPPN. The input batch to the DPPN is a 28 x 28 matrix of length 8 vectors. Each vector is constructed as (x, y, \u221a x2 + y2, 1, xN, x mod N, y mod N), withx and y normalized to values in [\u2212 1, 1], in evenly sampled steps across the image, and with the target output for each input data point is the ISN, x, y mod N, y mod N, y mod N, y mod N, and n mod N)."}, {"heading": "3.4.3 Indirect encoding of a fully connected network", "text": "The task is to reconstruct MNIST digits after 10% of the pixels in the image are set to zero. Figure 2 shows the logic of the training. We learn to indirectly encode a fully connected feedforward that denotes the autoencoder with an encoding layer with sigmoid activation functions and a decoding layer with sigmoid activation functions. The hidden layer has 100 units arranged on a 10-10 grid. Thus, there are 28 x 28 x 10 x 2 + 28 x 10 = 157684 parameters (weights and distortions) that have three orders of magnitude more parameters than the convenNet that performs the same task. The DPPPN that encodes these parameters has two outputs, one for the encoding layer and one for the decoding layer (weights and distortions) that have three orders of magnitude more parameters than the convenNet that performs the same task. The DPPPPN that encodes these parameters has two outputs, one for the coding layer X76N, and one for the next X76N layer that has one decoding layer, the XNIST layer that has one parameter than the conventional network that performs this task for the PN, and one for the next X76N that encodes the X76th layer."}, {"heading": "4. RESULTS", "text": "In this section we present the experimental results."}, {"heading": "4.1 Image Reconstruction Experiments", "text": "Figure 3 (right) shows the details of an evolutionary process in which a population of 50 DPPNs with a crossover probability of 0.2, initialized with 4-node DPPNs, is developed to reconstruct the handwritten number 2, which was developed using the complete Lamarck algorithm, i.e., where learned weights are inherited from offspring. Figure 3 (center) shows the same composition with the Cauchy mutation of weights with a coefficient of 0.001, in which there is no learning of weights at all, but where there is no inheritance of acquired properties. Finally, Figure 3 (left) shows the same composition with the pure Darwinian evolution with the Cauchy mutation of weights with a coefficient of 0.001, in which there is no learning of weights at all. This final setting is closest to a CPPN. In the examples shown that Lamarckian inheritance with a mean MSE with a MSE of 0.0006.0001 and MSE with a mean MSE of 0.0006.000.0001, we show a MSE with an MSE of M36.00000006 and MSE with a 0.000000000SE."}, {"heading": "4.2 The Effect of Crossover", "text": "Figure 4 shows the same setup as a run in Figure 3 (right), but without crossover. There is a size difference in the MSE of 0.003 with crossover compared to 0.03 without crossover. The reconstruction is qualitatively worse without crossover. Batch runs of size 10 show a size advantage of the crossover, with MSE of 0.005 (\u00b1 0.001) with crossover probability of 0.2 compared to an MSE of 0.021 (\u00b1 0.006) without crossover. A trivial reason for the advantage of the crossover could be that it merely enlarges the size of the networks (717 compared to 112 parameters) so that a greater number of parameters can be optimized by gradient drop, possibly reducing the risk of getting stuck in a local optimum."}, {"heading": "4.3 Can the DPPN efficiently compress the weights of denoising autoencoders?", "text": "Figure 5 shows the 2 encodings (left column) and 2 decodings (right column) developed by the DPPN for the Convolutionary Denocialization Autoencoder, along with the digits reconstructions and fitness graphs showing that 1000 tournaments are sufficient for a MSE of 0.01 on the test kit. Figure 10 compares the performance of a DPPN with (top) and without crossover (bottom) in the production of the 157684 parameters of a fully connected denosifying autoencoder, which recovers most of the uncorrelated failure noise from the reconstruction. Figure 10 compares the performance of a DPPN with (top) and without crossover (bottom) in the production of the 157684 parameters of a fully connected denosifying autoencoder. In both cases, the DPPN rediscovers windings by learning the on / off center cores and then encoding them via the 28 x 28 receptive fields of the hidden units."}, {"heading": "5. DISCUSSION AND CONCLUSION", "text": "The results show that DPPNs and the learning algorithms described here are able to massively compress the parameters of larger neural networks and improve the performance of Darwinian-trained CPPNs. Since the hidden layer has a 10 \u00d7 10 grid structure, we can visualize the activations in the hidden layer for each digit, see Figure 9, which shows the hidden layer activations of a fully connected autoencoder encoded by a DPPN with an identity node as input versus a DPN with a fully connected linear node as input. Bothproduce comparable BCEs with approximately the same number of parameters. One of the benefits of this symbiosis between evolutionary and gradient-based learning is that it allows optimization to better avoid getting trapped in local optimization or saddle points."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "Thanks to John Agapiou for helping with the code and Ken Stanley, Jason Yosinski and Jeff Clune for useful discussions."}, {"heading": "7. REFERENCES", "text": "[1] J. Bayer, D. Wierstra, J. Togelius, andJ. Schmidukhuber. Evolving memory cell structures for sequence learning. In Artificial Neural Networks-ICANN 2009, pages 755-764. Springer, 2009. [2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv: 1412.7062, 2014. [3] M. Denil, B. Shakibi, L. Dinh, N. de Freitas, et al. XiXiXi. in deep learning. In Advances in Neural Information Processing Systems, pages 2148-2156, 2013. [4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization."}], "references": [{"title": "Evolving memory cell structures for sequence learning", "author": ["J. Bayer", "D. Wierstra", "J. Togelius", "J. Schmidhuber"], "venue": "Artificial Neural Networks\u2013ICANN 2009, pages 755\u2013764. Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed hierarchical processing in the primate cerebral cortex", "author": ["D.J. Felleman", "D.C. Van Essen"], "venue": "Cerebral cortex, 1(1):1\u201347,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics, 36(4):193\u2013202,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1980}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The microbial genetic algorithm", "author": ["I. Harvey"], "venue": "Advances in artificial life. Darwin Meets von Neumann, pages 126\u2013133. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology, 160(1):106,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1962}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural gpus learn algorithms", "author": ["L. Kaiser", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, 350(6266):1332\u20131338,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, 1(4):541\u2013551,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Single-unit pattern generators for quadruped locomotion", "author": ["G. Morse", "S. Risi", "C.R. Snyder", "K.O. Stanley"], "venue": "Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 719\u2013726. ACM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving multimodal controllers with hyperneat", "author": ["J.K. Pugh", "K.O. Stanley"], "venue": "Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 735\u2013742. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling, 5:3,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1988}, {"title": "Phenotypic plasticity, the baldwin effect, and the speeding up of evolution: The computational roots of an illusion", "author": ["M. Santos", "E. Szathm\u00e1ry", "J.F. Fontanari"], "venue": "Journal of theoretical biology, 371:127\u2013136,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Picbreeder: A case study in collaborative evolutionary exploration of design space", "author": ["J. Secretan", "N. Beato", "D.B. D\u2019Ambrosio", "A. Rodriguez", "A. Campbell", "J.T. Folsom-Kovarik", "K.O. Stanley"], "venue": "Evolutionary Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Compositional pattern producing networks: A novel abstraction of development", "author": ["K.O. Stanley"], "venue": "Genetic programming and evolvable machines, 8(2):131\u2013162,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A hypercube-based encoding for evolving large-scale neural networks", "author": ["K.O. Stanley", "D.B. D\u2019Ambrosio", "J. Gauci"], "venue": "Artificial life,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Evolving neural networks through augmenting topologies", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Evolutionary computation, 10(2):99\u2013127,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Generative neuroevolution for deep learning", "author": ["P. Verbancsics", "J. Harguess"], "venue": "arXiv preprint arXiv:1312.5355,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Evolutionary programming made faster", "author": ["X. Yao", "Y. Liu", "G. Lin"], "venue": "Evolutionary Computation, IEEE Transactions on, 3(2):82\u2013102,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 24, "context": "Compositional Pattern Producing Networks (CPPN) [25] were a major advance in evolutionary computation because they permitted evolution to efficiently optimize a model incrementally starting from a small number of parameters.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "This means that one can evolve CPPNs to represent images; as in Picbreeder [24], where a crowd of Internet users evolve images by selecting which CPPNs to breed.", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "2908890 The way CPPNs are currently optimized is by evolving the topology and the weights with NEAT [27].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "However, one can train neural networks with millions of parameters by exploiting gradient-based learning methods [22].", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "We show that using DPPNs we can rapidly reconstruct images by using fewer parameters than the number of pixels, and that DPPNs can be used in a HyperNEAT-like framework [26] as an indirect encoding of a larger neural network.", "startOffset": 169, "endOffset": 173}, {"referenceID": 22, "context": "DPPNs also work in a Darwinian/Baldwinian framework [23] in which the learned weights are not inherited directly, only the initial weights of the DPPN are inherited.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "For example we show that when a DPPN is trained using our algorithm to produce the 157684 parameters of a fully connected denoising autoencoder for MNIST digit reconstruction (MNIST is a standard benchmark for supervised learning consisting of labelled handwritten digits) [29], it generates a convolutional architecture embedded within the fully connected feedforward network, in which each hidden unit contains a blob-like 28\u00d728 weight matrix where the blob is smoothly moved over the receptive fields of hidden nodes.", "startOffset": 273, "endOffset": 277}, {"referenceID": 14, "context": "Generalization to the Omniglot character set [15] is also demonstrated to be superior to an equivalent directly encoded network.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "CPPNs were invented by Ken Stanley [25] as an abstraction of natural development.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "A good example of this methodology is Picbreeder [24], in which a crowd of Internet users evolve images by selecting which CPPNs to breed.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "The NeuroEvolution of Augmented Topologies (NEAT) algorithm [27] is used to constrain crossover to homologous parts of the CPPN, and to maintain topology diversity.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "In HyperNEAT [26], CPPNs are used as indirect compressed encodings of the weights of a larger neural network.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "If a single CPPN must encode multiple layers of a deeper neural network then there are two possibilities, either an extra input is given signaling which layer of weights the CPPN is outputting [19], or the CPPN is constrained to always have multiple output nodes, with a specific node outputting the weight for its assigned layer [21].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "If a single CPPN must encode multiple layers of a deeper neural network then there are two possibilities, either an extra input is given signaling which layer of weights the CPPN is outputting [19], or the CPPN is constrained to always have multiple output nodes, with a specific node outputting the weight for its assigned layer [21].", "startOffset": 330, "endOffset": 334}, {"referenceID": 13, "context": "for convolutional neural networks for performing object classification on ImageNet [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "CPPNs and convolutional neural networks have previously been studied with CPPNs being used to evolve adversarial examples for convolutional network classifiers on ImageNet [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 5, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 30, "endOffset": 37}, {"referenceID": 15, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 30, "endOffset": 37}, {"referenceID": 13, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 236, "endOffset": 250}, {"referenceID": 1, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 236, "endOffset": 250}, {"referenceID": 17, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 236, "endOffset": 250}, {"referenceID": 11, "context": "Convolutional neural networks [6, 16] have made great strides in recent years in practical performance [14], to the point where they are now a critical component of many of the best systems for challenging artificial intelligence tasks [9, 2, 18, 12].", "startOffset": 236, "endOffset": 250}, {"referenceID": 4, "context": "These architectures were historically inspired by the structure of the mammalian visual system, namely the hierarchical structure of the ventral stream for visual processing [5] and the local, tiled nature of \u201creceptive fields\u201d in the primary visual cortex [10].", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "These architectures were historically inspired by the structure of the mammalian visual system, namely the hierarchical structure of the ventral stream for visual processing [5] and the local, tiled nature of \u201creceptive fields\u201d in the primary visual cortex [10].", "startOffset": 257, "endOffset": 261}, {"referenceID": 2, "context": "It has also been shown that even greater improvements in the compression of neural network weights should be possible - even after removing most of the weights from the filters of a trained convolutional neural network, it is possible to predict the missing weights with high accuracy [3].", "startOffset": 285, "endOffset": 288}, {"referenceID": 29, "context": "This allows compression of the weights of convolutional neural networks in order to make them computationally more efficient [30, 11].", "startOffset": 125, "endOffset": 133}, {"referenceID": 10, "context": "This allows compression of the weights of convolutional neural networks in order to make them computationally more efficient [30, 11].", "startOffset": 125, "endOffset": 133}, {"referenceID": 27, "context": "Recent work applied CPPNs in the HyperNEAT framework to evolve the weights of the 5 layer LeNet-5 convolutional neural network for MNIST character recognition [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 16, "context": "8% were obtained with backpropagation alone [17].", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "For example Bayer et al evolved the topology of the cells of an LSTM (Long Short Term Memory) recurrent neural network for sequence learning [1], and more recently Jozefowicz et al explored the topology of LSTMs and GRUs improving on both [32].", "startOffset": 141, "endOffset": 144}, {"referenceID": 24, "context": "For the DPPN, the node types used are as in previous CPPN papers [25], i.", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "A denoising autoencoder [29] is an unsupervised learning framework for neural networks.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "The simpler evolutionary algorithm is the microbial genetic algorithm (mGA) [8] with a population size of 50.", "startOffset": 76, "endOffset": 79}, {"referenceID": 30, "context": "001 [31].", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "weights of the DPPN we use Adam (adaptive moment estimation) [13] which is a momentum-based flavor of SGD that adaptively computes individual learning rates for each parameter by keeping an estimate of the first and second moments of the gradients.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "It combines the advantages of two popular methods: AdaGrad [4], which behaves well in presence of sparse gradients, and RMSProp [7], which is able to cope with non-stationary objectives.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "It combines the advantages of two popular methods: AdaGrad [4], which behaves well in presence of sparse gradients, and RMSProp [7], which is able to cope with non-stationary objectives.", "startOffset": 128, "endOffset": 131}], "year": 2016, "abstractText": "In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution.", "creator": "LaTeX with hyperref package"}}}