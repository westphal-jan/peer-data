{"id": "1606.02601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "A Joint Model for Word Embedding and Word Morphology", "abstract": "This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.", "histories": [["v1", "Wed, 8 Jun 2016 15:24:22 GMT  (386kb,D)", "http://arxiv.org/abs/1606.02601v1", "Submission for first Representation Learning for NLP workshop at ACL2016"]], "COMMENTS": "Submission for first Representation Learning for NLP workshop at ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kris cao", "marek rei"], "accepted": false, "id": "1606.02601"}, "pdf": {"name": "1606.02601.pdf", "metadata": {"source": "CRF", "title": "A Joint Model for Word Embedding and Word Morphology", "authors": ["Kris Cao"], "emails": ["kc391@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "The smallest semantic unit is morphine, while the smallest unit of orthography is graphics or signs. Both have been used as a method to go beyond models at the word level."}, {"heading": "2.1 Morphemic analysis and semantics", "text": "Since word semantics is compositional, one might wonder whether it is possible to learn morphology representations and compose them to obtain good word representations. Lazaridou et al. (2013) demonstrated just this: good morphology representations can be derived from distributional techniques and tools from compositional distribution semantics can be used to obtain good word representations. Luong et al. (2013) also trained a morphological composition model based on recursive neural networks. Botha and Blunsom (2014) built a language model that incorporates morphemy and demonstrated improvements in language modeling and machine translation. All of these approaches integrated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX (Baayen et al., 1995) or an external morphological analyzer such as Morfessor (Creutz and Lagus, 2007) to decide whether two morphological or unrelated words are morphological."}, {"heading": "2.2 Character-level models", "text": "Another approach that goes beyond words is based on models of neural networks at the sign level. Both recurring and revolutionary architectures for deriving word representations from characters have been used, and the results of downstream tasks such as speech modeling and POS tagging have been promising, with word confusion in speech modeling and the accuracy of English POS tagging accuracy being state-of-the-art (Ling et al., 2015; Kim et al., 2016). Ballesteros et al. (2015) typically form a character model for parsing. Zhang et al. (2015) completely eliminate words and train a revolutionary neural network to perform text classifications directly from words. Exciting, character models seem to capture morphological effects. Studying the closest neighbors of morphologically complex words in sign-conscious models often shows other words with the same morphology (Ling et al., 2015; Kim et al., 2016)."}, {"heading": "3 The Char2Vec model", "text": "We can improve the ability of character level models to learn word semantics. (We have put both hypotheses directly into the world that we have not usually put into the world. (SGNS) The starting point for our model is the Sketchbook with Negative Sampling (SGNS) Target by Mikolov et al. (2013b) For a vocabulary V of size and embedding size N, SGNS learns two embedding tables. (SGNS) Target and context vectors. Each time a word w is seen in the context of the word c, the tables are updated to maximizelog. (SGNS) + k."}, {"heading": "3.1 Capturing morphology via attention", "text": "Previous work on bidirectional character level models of the LSTM used both LSTMs to read the whole word (Ling et al., 2015; Ballesteros et al., 2015), which can lead to redundancy as both LSTMs are used to capture the whole word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves, meaning that one of the LSTMs can specialize in word prefixes and roots, while the other memorizes possible suffixes. In addition, when handling an unknown word, it can be divided into known and unknown components, and the model can then use its learned semantic knowledge for a known component to predict a representation of the unknown word as a whole. We assume that the natural place to split words is at morph boundaries, as morphems are the smallest linguistic unit that carries semantic meaning."}, {"heading": "4 Experiments", "text": "We evaluated our model based on three tasks: morphological analysis (\u00a7 4.1), semantic similarity (\u00a7 4.2) and analogy retrieval (\u00a7 4.3). We trained all models once and then used the same trained model for all three tasks - we do not perform hyperparameter tuning to optimize performance on each task. We trained our Char2Vec model on one side of the Text8 corpus, consisting of the first 100 MB of a 2006 purified dump from Wikipedia1. We trained only on words that appeared more than 5 times in our corpus. We used a context window size of 3 words on the other side of the target and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam Optimizer (Kingma and Ba, 2015). All experiments were weighted using Bergas (2015), Chollet (2015) and the other levels."}, {"heading": "4.1 Morphological awareness", "text": "rE \"s tis,\" so sasd re \"i.\" rf\u00fc"}, {"heading": "4.2 Capturing semantic similarity", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said.\" We, \"he said,\" will be able to annoy ourselves. \""}, {"heading": "4.3 Capturing syntactic and semantic regularity", "text": "Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space. To do this, we use the Google analogy dataset (Mikolov et al., 2013a). However, this consists of questions of the form \"A is to B as C is to X.\" We divide this collection into semantic and syntactic sections based on whether the analogies between words are driven by morphological changes or deeper semantic shifts. Examples of semantic questions concern relationships between capitals (\"Paris is to France as Berlin is to X\") and currency tactical relationships. Examples of syntactic questions are adjective-adjective-adverb relationships (\"amazing is too obvious to X\") and opposites formed by prefixing a negative particle (\"acceptable is too unacceptable as conscious is to X\")."}, {"heading": "5 Discussion", "text": "This is reflected in our morphology test, where more than half of the words consist of a simple morphem and more than 90% have a maximum of 2 morphemes.This is unfortunate for our model, as it performs better for words with richer morphology. It consistently provides more accurate morphological analysis for these words compared to standard baselines and is consistent with word-level models for semantic similarity of rare words with richer morphology. Furthermore, it appears to learn morphosyntactic features to solve the syntactic analogy task. Above all, it is language agnostic and easy to port across languages. Therefore, we expect our model to perform even better for languages with richer morphology than English, such as Turkish and German."}, {"heading": "6 Conclusion", "text": "In this paper, we present a model that learns morphology and word embedding together. Given a word, it splits the word into segments and arranges the segments based on its context-dependent predictive power. Our model can segment words into morphemes and also embed the word into a representation space.We show that our model is competitive in the task of restoring the morpheme boundary compared to a dedicated morphological analyzer, and suggests dedicated analyzers for words with rich morphology. We also show that word appendages correspond to linear shifts in the representation space, which shows that our model can learn morphological characteristics. Finally, we show that character-level models, although generally outperform them in the task of semantic similarity of word-level models, are competitive in the representation of rare morphologically rich words. Furthermore, the character-level models can predict good qualitative representations of invisible words, with a slightly more conscious morphologically characterized model."}], "references": [{"title": "Richard Piepenbrock", "author": ["Harald R. Baayen"], "venue": "and Leon Gulikers.", "citeRegEx": "Baayen et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Chris Dyer", "author": ["Miguel Ballesteros"], "venue": "and Noah A Smith.", "citeRegEx": "Ballesteros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Simpson", "author": ["David A. Balota", "Melvin J. Yap", "Keith A. Hutchison", "Michael J. Cortese", "Brett Kessler", "Bjorn Loftis", "James H. Neely", "Douglas L. Nelson", "Greg B"], "venue": "and Rebecca Treiman.", "citeRegEx": "Balota et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Janvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "David WardeFarley", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": null, "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Nam Khanh Tran", "author": ["Elia Bruni"], "venue": "and Marco Baroni.", "citeRegEx": "Bruni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bullinaria and Joseph P", "author": ["A John"], "venue": "Levy.", "citeRegEx": "Bullinaria and Levy2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Tim Vieira", "author": ["Ryan Cotterell"], "venue": "and Hinrich Sch\u00fctze.", "citeRegEx": "Cotterell et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Lagus2007] Mathias Creutz", "Krista Lagus"], "venue": "ACM Trans. Speech Lang. Process.,", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Gadi Wolfman", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan"], "venue": "and Eytan Ruppin.", "citeRegEx": "Finkelstein et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["John Goldsmith"], "venue": "Computational Linguistics,", "citeRegEx": "Goldsmith.,? \\Q2001\\E", "shortCiteRegEx": "Goldsmith.", "year": 2001}, {"title": "and Thomas L", "author": ["Sharon Goldwater", "Mark Johnson"], "venue": "Griffiths.", "citeRegEx": "Goldwater et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "and Alexander M", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag"], "venue": "Rush.", "citeRegEx": "Kim et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. ICLR", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Roberto Zamparelli", "author": ["Angeliki Lazaridou", "Marco Marelli"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Yoav Goldberg", "author": ["Omer Levy"], "venue": "and Ido Dagan.", "citeRegEx": "Levy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, Ram\u00f3n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W. Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Minh-Thang Luong", "Richard Socher"], "venue": "Manning.", "citeRegEx": "Luong et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "ICLR Workshop", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Ochs2015] Radu Soricut", "Franz Ochs"], "venue": null, "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Manning", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D"], "venue": "and Yoram Singer.", "citeRegEx": "Toutanova et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Michael Auli", "author": ["Wenduan Xu"], "venue": "and Stephen Clark.", "citeRegEx": "Xu et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Junbo Zhao", "author": ["Xiang Zhang"], "venue": "and Yann LeCun.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.", "creator": "LaTeX with hyperref package"}}}