{"id": "1606.02608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "abstract": "We present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90\\% less memory than the closest state-of-the-art counterpart.", "histories": [["v1", "Wed, 8 Jun 2016 15:39:17 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02608v1", "17 pages, 1 figure, 7 tables, submission to Pattern Recognition Letters, review version"]], "COMMENTS": "17 pages, 1 figure, 7 tables, submission to Pattern Recognition Letters, review version", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jaime ferreira", "david martins de matos", "ricardo ribeiro"], "accepted": false, "id": "1606.02608"}, "pdf": {"name": "1606.02608.pdf", "metadata": {"source": "META", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "authors": ["Jaime Ferreiraa", "David Martins de Matosa", "Ricardo Ribeiroa"], "emails": ["david.matos@inesc-id.pt"], "sections": [{"heading": null, "text": "In this paper, we introduce xokde + +, a state-of-the-art online kernel density estimation approach that models input data streams through Gaussian mixs.This approach follows the current state of the art research on online density estimation, but has been redesigned with respect to computing efficiency, numerical robustness and expandability in mind. Our approach delivers comparable or better results than the current state of the art, while achieving significant gains in computing power and improved numerical stability, and also examines the use of Gaussian cores with diagonal covariance, which further improve performance and stability with little loss of modeling quality. Our approach is up to 40 times faster and requires 90% less memory than the next modern counterpart. c \u00a9 2016 Elsevier Ltd. All rights reserved."}, {"heading": "1. Introduction", "text": "It is necessary that the data are not able to come to the fore, but to come to the fore."}, {"heading": "2. Online Kernel Density Estimation", "text": "oKDE was not developed with high dimensionality in mind. Although it allows any number of dimensions, the computational complexity scales quadratically with the number of dimensions, making the approach mathematically prohibitive. In addition, problems arise due to overflows, underflows and precision problems, which worsen as the number of dimensions increases, limiting the usefulness of the approach. Before describing our approach, we will introduce oKDE in detail.3"}, {"heading": "2.1. Model Definition", "text": "Sample models are defined as d-dimensional, N-component GMMs (equivalent 1): x are d-dimensional observations; wi are mixed weights, so \u03b5Ni = 1wi = 1; and \u03c6\u044bi (x \u2212 \u00b5i) are d-variable components (equivalent 2), with mean \u00b5i and covariance \u0439i.ps (x) = N \u2211 i = 1 wi\u0445 i (x \u2212 xi) (1) throu\u0445 i (x \u2212 \u00b5i) = 12\u03c0D / 2 | 1 / 2 exp {12 (x \u2212 \u00b5i) T\u03a3 \u2212 1i (x \u2212 \u00b5i) (2) In order to maintain low complexity, ps (x) is compressed when a threshold is reached. In order to restore miscompression, a model of observations is also retained for each component: Smodel = {ps (x \u2212)} i = 1: N}, where qi (x) is a mixture of (x) with most of the order of H (p)."}, {"heading": "2.2. Optimal Bandwidth", "text": "KDE approaches determine a bandwidth H that minimizes the distance between p-KDE (x) and (the unknown) p (x) that generates the data. Bandwidth can be written as H = \u03b22F (\u03b2 is the scale and F is the structure), determining H corresponds to determining the optimal scale \u03b2opt (Eq. 4), where R (p, F) \u2248 R (p, F, G) (Eq. 5) and Aij = (gj + sj) \u2212 1, ij = \u00b5i \u2212 \u00b5j, mij = \u2206 TijAij \u0445 ij, gj = G + sj and sj the covariance of the j component of ps (x).\u03b2opt = [d (4\u03c0) d / 2NR (p, F) \u2212 1 / (d + 4) R (p, F, G) = N-th component of ps (x)."}, {"heading": "2.3. Model Compression", "text": "Compression approaches the original N component ps (x) by an M component, M < N, p component (x) (Eq. 7), so that the compressed KDE does not change significantly. p component (x) = M component j = 1 w component (x-x-j) (7) Clustering approaches can be used to compress ps (x). The goal is to find clusters so that each cluster can be approached by a single component in p component in p-s (x)."}, {"heading": "2.4. Local Clustering Error", "text": "Consider p1 (x), a submixture of ps (x), and p0 (x), its simple Gaussian approximation (Eq. 9). The local cluster error is the distance (Eq. 11) between the KDEs. It can be quantified using the Hellinger distance [22] (Eq. 12). E-value (p1 (x), Hopt) = D (p1KDE (x), p0KDE (x)) (11) D2 (p1KDE (x), p0KDE (x)), 1 2 E-value (p1KDE (x) 1 / 2 \u2212 p0KDE (x) 1 / 2) 2 dx (12) 5"}, {"heading": "2.5. Distance between mixture models", "text": "The Hellinger distance cannot be analytically calculated for mixture models and is approximated by the odourless Transform [12] (Eq.13), where {(j) Xi, (j) Wi} j = 0: 2d + 1 weighted sets of sigma points of the i-th Gaussian phenomenon (x \u2212 xi).D2 (p1, p2) \u2248 12 N \u2211 i = 1 wi 2d + 1 \u2211 j = 0 g (j) Xi) (j) Wi (13) (0) Xi = xi, (0) Wi = kd + k, k = max ([0, m \u2212 d]) (j) Xi = xi + sj \u221a (d + k) \u0432j, (j) Wi = 12 (d + k) sj = 1, j \u2264 d \u2212 1, otherwise it should be noted that the two points have to be counted individually."}, {"heading": "2.6. Hierarchical Compression", "text": "A hierarchical approach avoids the evaluation of all possible assignments \u0430 (M) [7, 11]. ps (x) is first divided into two submixtures by Goldberger's K-means [7]. To avoid singularities related to the Dirac delta components of ps (x), K-means are applied to p-KDE (x). Subsequently, each submixture is approximated by a single Gauss (Section 2.3) and the submixture that yields the greatest local error is further divided into two submixtures. The process continues until E (B) \u2264 Dth, whereby a binary tree is created in which each of the M-sheets embodies the cluster assignments."}, {"heading": "2.7. Revitalization", "text": "These components are removed from ps (x) and replaced by the components of their detailed models. Each new component needs a detailed model: new components are generated based on their covariances. Let's consider the extension i (x \u2212 \u00b5i) as one of the new components. If the determinant of \u0435i is zero, then the component is a single data point and its detailed model is just the component itself. Otherwise, the component was created by clustering in previous compression steps: its detailed model is initialized by splitting exaci (x \u2212 \u00b5i) along its main axis [10] into a two-component mixture whose first two moments coincide with those of the original component. This has two advantages: firstly, since the component is symmetrical around the mean, splitting the process means reducing the error and thus preserving and reducing the total (second) error."}, {"heading": "3. Improving Numeric Stability", "text": "3.1. Degenerated covariance matrices Some data dimensions may seldom change or there may not have been sufficient observations with different values, causing covariance values of 0 along these axes. One way to detect and correct singular or near-singular matrices is to calculate their self-decomposition 94 = Q2 = Q2, testing eigenvalues smaller than 10 \u2212 9 and then correcting these eigenvalues by 1% of the mean of eigenvalues (Eq.15, 16, 17). The corrected covariance then results from the following statement: T = Q2 = Q3 = Q3, and checking for eigenvalues smaller than 10 \u2212 9. These eigenvalues are then corrected by 1% of the mean of eigenvalues (Eq.15, 16, 17)."}, {"heading": "3.2. Determinant Computation", "text": "The Gaussian probability density function (pdf) (Equation 2) uses the determinant (| \u03a3 |) and the inversion (\u03a3 \u2212 1) of the covariance (\u03a3). One way to efficiently calculate the inversion of a non-singular matrix is to first calculate its LU decomposition and use it to calculate the inversion. Since we guarantee that \u03a3 is positively defined, the determinant can be calculated by multiplying the diagonal entries of U. However, if we work in high dimensionality, overflows or underflows will inevitably occur even if we use the double precision flow point. To avoid this problem, we calculate instead the logarithm of the determinant: This is simply the sum of the logarithm of each input into the diagonal of the matrix U. 3.3. The whiteness transformation of the transformation A single hopt means that all components are equally scaled in all dimensions."}, {"heading": "3.4. Optimal Bandwidth Stability", "text": "Let us remember that we assume that the structure of Hopt is well approximated by the covariance of the whole model and that the determination of Hopt amounts to the determination \u03b2opt (Section 2.2). If this scaling factor is zero, e.g. due to precision problems or the data distribution in the model, Hopt will be zero. This means that no smoothing is applied to the KDE. In this case, Dirac deltas remain at zero covariance, which does not allow the use of the standard probability function. Since Hopt is calculated on the basis of white data, one solution to this problem is to detect these occurrences and set the bandwidth as an identity matrix and then calculate the backward transformation."}, {"heading": "4. Using Diagonal Covariance Matrices", "text": "Since KDE is a linear combination of components, full covariances may not be necessary, even if the features are not statistically independent: A second linear combination (a higher number of) components with diagonal covariances is able to model the correlations between features. In diagonal covariances, the need for computing grows linearly with the number of dimensions, a very practical aspect for high-dimensionality applications. This is in contrast to square growth for full covariances. In addition, diagonal covariances can improve stability in non-normalized data by ignoring certain relationships that could lead to singular covariances.Our numerical stability strategies have a stronger impact on diagonal covariances: this is particularly relevant for data with flat or near-flat dimensions (Section 7)."}, {"heading": "5. Lazy Operation and Result Buffering", "text": "Theoretically, hops should be updated every time a sample is added, but hops are only needed for the compression phase or to assess the probability of a given sample. In other cases, the calculation of hops can be deferred, meaning that the cost of adding a sample is simply to add a Dirac delta component to the mix and update the current weights, without affecting model quality; the determinant and inverse of the covariance matrices are needed to calculate the probability function (which is often cited), so it makes sense to store these calculations once they have been calculated, and keep them until the covariance changes; then 9 previous values are marked invalid, but not immediately recalculated; as with hops, it is advantageous to postpone these calculations until they are actually needed, which speeds up the process of adding components to the mix."}, {"heading": "6. Evaluation Setup", "text": "This year it has come to the point where one is able to experience the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brain.n"}, {"heading": "7. Results", "text": "The tests were carried out on an Intel Xeon E5530 @ 2.40GHz, 48GB RAM, Linux openSUSE 13.1. oKDE and OISVM were run on a MATLAB R2013a with -nosplash -nodisplay -nodisplay -nodisplay -nojvm. However, our approach (xokde +) was compiled and allows all vectorization options available on our CPU architecture. In the tables, \"-\" shows that the model could not be built. Model QualityAs a proxy for estimation quality, we use the average negative log-likelihood and the average datxxity of the models. Tables 2and 3 present results after observing all samples.Relative to oKDE, xokde + produces models of similar complexity, but with lower average negative log-likelihood (better fits).This is clear in datasets where some dimensions have very large variance."}, {"heading": "8. Conclusions", "text": "xokde is a state-of-the-art online KDE approach that is efficient, numerically robust, and capable of handling high dimensionality, with model quality comparable to that of oKDE, while requiring significantly less memory and computing time. Numerical stability improvements allow xokde + + to deal with non-normalized data and achieve an average classifier accuracy of 78% (versus oKDE 73%), which is an average improvement of 6.5% above the baseline. Furthermore, it continues to achieve good results where other approaches cannot even convection (Section 7.2). From a software standpoint, xokde + is expandable due to its pure template library nature, as demonstrated by the use of diagonal covariants that have been easily implemented and provide additional time and memory efficiency, while sacrificing little model quality.Besides speed and memory efficiency, a key contribution of our approach is its adaptability and expandability."}, {"heading": "9. Acknowledgements", "text": "This work was supported by national funding from Fundac and the Cie Ncia e a Tecnologia (FCT) with reference number UID / CEC / 50021 / 2013."}], "references": [{"title": "The OpenCV Library", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation", "author": ["T.T. Cai", "Z. Ren", "H.H. Zhou"], "venue": "The Annals of Statistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Probability density estimation with tunable kernels using orthogonal forward regression. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["S. Chen", "X. Hong", "C.J. Harris"], "venue": "IEEE Trans. on", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Online learning of Gaussian mixture models - a two-level approach", "author": ["A. Declercq", "J.H. Piater"], "venue": "VISAPP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Plug-in bandwidth matrices for bivariate kernel density estimation", "author": ["T. Duong", "M. Hazelton"], "venue": "Journal of Nonparametric Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Introduction to statistical pattern recognition. 2 ed", "author": ["K. Fukunaga"], "venue": "Academic press", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Hierarchical clustering of a mixture model, in: Advances in Neural Information", "author": ["J. Goldberger", "S.T. Roweis"], "venue": "Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Sequential kernel density approximation and its application to real-time visual tracking", "author": ["B. Han", "D. Comaniciu", "Y. Zhu", "L.S. Davis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Trans", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Nonlinear Gaussian Filtering: Theory, Algorithms, and Applications", "author": ["M. Huber"], "venue": "KIT Scientific Publishing", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Inference in sensor networks: Graphical models and particle methods", "author": ["A.T. Ihler"], "venue": "Ph.D. thesis. Massachusetts Institute of Technology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "A general method for approximating nonlinear transformations of probability distributions", "author": ["S.J. Julier", "J.K. Uhlmann"], "venue": "Technical Report. RRG,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Online discriminative kernel density estimator with gaussian kernels", "author": ["M. Kristan", "A. Leonardis"], "venue": "Cybernetics, IEEE Trans. on", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Multivariate online kernel density estimation with Gaussian kernels", "author": ["M. Kristan", "A. Leonardis", "D. Sko\u010daj"], "venue": "Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Approximating the covariance matrix of GMMs with low-rank perturbations", "author": ["M. Magdon-Ismail", "J.T. Purnell"], "venue": "Int. J. Data Mining and Bioinformatics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Multidimensional gaussian probability density and its applications in the degenerate case", "author": ["P. Mikheev"], "venue": "Radiophysics and quantum electronics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "DOGMA: a MATLAB toolbox for Online Learning. http://dogma.sourceforge.net", "author": ["F. Orabona"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "On-line independent support vector machines", "author": ["F. Orabona", "C. Castellini", "B. Caputo", "L. Jie", "G. Sandini"], "venue": "Pattern Recognition", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Mean shift spectral clustering", "author": ["U. Ozertem", "D. Erdogmus", "R. Jenssen"], "venue": "Pattern Recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "The gaussian toeplitz matrix. Linear algebra and its applications", "author": ["J. Pasupathy", "R. Damodar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "A user\u2019s guide to measure theoretic probability", "author": ["D. Pollard"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Kullback-leibler approach to gaussian mixture reduction", "author": ["A.R. Runnalls"], "venue": "Aerospace and Electronic Systems, IEEE Trans. on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Density estimation for statistics and data analysis. volume 26", "author": ["B.W. Silverman"], "venue": "CRC press", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "in: Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Kernel smoothing. volume 60 of Monographs on statistics and applied probability", "author": ["M.P. Wand", "M.C. Jones"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}], "referenceMentions": [{"referenceID": 6, "context": "In order to manage complexity, some approaches compress the model to a predefined number of components [7], or optimize some data-driven choice [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "In order to manage complexity, some approaches compress the model to a predefined number of components [7], or optimize some data-driven choice [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 17, "context": "A different approach [20] is to view model compression as a clustering", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, it is sensitive to non-Gaussian areas due to skewed or heavy tailed data [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "A two-level approach [4], based on the idea that each component of the (non-overfitting) mixture is in turn represented by an underlying mixture that represents data very precisely (possibly overfitting), allows the model to be refined without sacrificing accuracy, as a merge in the upper level can later be accurately split.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "The online kernel density estimation (oKDE) approach [13, 14] builds a two-level model of the target distribution.", "startOffset": 53, "endOffset": 61}, {"referenceID": 12, "context": "The online kernel density estimation (oKDE) approach [13, 14] builds a two-level model of the target distribution.", "startOffset": 53, "endOffset": 61}, {"referenceID": 23, "context": "The structure of H can be approximated by the structure of the covariance of the observations [26, 5], i.", "startOffset": 94, "endOffset": 101}, {"referenceID": 4, "context": "The structure of H can be approximated by the structure of the covariance of the observations [26, 5], i.", "startOffset": 94, "endOffset": 101}, {"referenceID": 19, "context": "It may be quantified by using the Hellinger distance [22] (Eq.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "The Hellinger distance cannot be calculated analytically for mixture models and is approximated by the unscented transform [12] (Eq.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "In line with [12], m was set to 3.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "A hierachical approach avoids evaluating all possible assignments \u039e(M) [7, 11].", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "A hierachical approach avoids evaluating all possible assignments \u039e(M) [7, 11].", "startOffset": 71, "endOffset": 78}, {"referenceID": 6, "context": "ps(x) is first split into two sub-mixtures using Goldberger\u2019s K-means [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Otherwise, the component has been generated through clustering in previous compression steps: its detailed model is initialized by splitting \u03c6\u03a3i(x\u2212\u03bci) along its principal axis [10] into a two-component mixture, whose first two moments match those of the original component.", "startOffset": 176, "endOffset": 180}, {"referenceID": 5, "context": "This can be avoided by first whitening the data, by transforming it to have unit covariance, smoothing it using a radially symmetric kernel, and, finally, transforming it back [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 21, "context": "Whitening allows Hopt to better fit the global distribution of the data, leading to better models [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "To evaluate our approach and to critically compare our results, we follow the same evaluation strategies of the original oKDE paper [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "We used the online independent support vector machine (OISVM) [18, 19].", "startOffset": 62, "endOffset": 70}, {"referenceID": 16, "context": "We used the online independent support vector machine (OISVM) [18, 19].", "startOffset": 62, "endOffset": 70}, {"referenceID": 22, "context": "A Haar cascade [25, 1] detected frontal face poses, but also some high inclination and head rotation poses.", "startOffset": 15, "endOffset": 22}, {"referenceID": 0, "context": "A Haar cascade [25, 1] detected frontal face poses, but also some high inclination and head rotation poses.", "startOffset": 15, "endOffset": 22}, {"referenceID": 20, "context": "Other improvements include changing the hierarchical compression approach, to another, more paralelizable approach such as pairwise merging of components [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 255, "endOffset": 262}, {"referenceID": 1, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 255, "endOffset": 262}, {"referenceID": 14, "context": "Finally, improvements on numerical stability and model quality may be obtained by avoiding correction of degenerate covariances and, instead, compute pseudo-inverses and pseudo-logdeterminants [17].", "startOffset": 193, "endOffset": 197}], "year": 2016, "abstractText": "In this paper we present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90% less memory than the closest state-of-the-art counterpart. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "creator": "LaTeX with hyperref package"}}}