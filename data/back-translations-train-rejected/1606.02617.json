{"id": "1606.02617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Efficient Estimation of k for the Nearest Neighbors Class of Methods", "abstract": "The k Nearest Neighbors (kNN) method has received much attention in the past decades, where some theoretical bounds on its performance were identified and where practical optimizations were proposed for making it work fairly well in high dimensional spaces and on large datasets. From countless experiments of the past it became widely accepted that the value of k has a significant impact on the performance of this method. However, the efficient optimization of this parameter has not received so much attention in literature. Today, the most common approach is to cross-validate or bootstrap this value for all values in question. This approach forces distances to be recomputed many times, even if efficient methods are used. Hence, estimating the optimal k can become expensive even on modern systems. Frequently, this circumstance leads to a sparse manual search of k. In this paper we want to point out that a systematic and thorough estimation of the parameter k can be performed efficiently. The discussed approach relies on large matrices, but we want to argue, that in practice a higher space complexity is often much less of a problem than repetetive distance computations.", "histories": [["v1", "Wed, 8 Jun 2016 16:11:53 GMT  (248kb,D)", "http://arxiv.org/abs/1606.02617v1", "Technical Report, 16p, alternative source:this http URL"], ["v2", "Mon, 13 Jun 2016 11:34:59 GMT  (248kb,D)", "http://arxiv.org/abs/1606.02617v2", "Technical Report, 16p, alternative source:this http URL"]], "COMMENTS": "Technical Report, 16p, alternative source:this http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aleksander lodwich", "faisal shafait", "thomas breuel"], "accepted": false, "id": "1606.02617"}, "pdf": {"name": "1606.02617.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Aleksander Lodwich", "Faisal Shafait", "Thomas Breuel"], "emails": [], "sections": [{"heading": null, "text": "Tags: Next Neighbor, Optimization, Benchmarking"}, {"heading": "1 Introduction", "text": "iD eeisrcnllhsrc\u00fceegnllhsrc\u00fceegnllhsrc\u00fceegnlllllrrrrlrrteeegln rf\u00fc ide eeisrrrrrlrrrlrrrlrrrrrrlrrrrrrrlrrrrrrrrrrrlrrrrlrrrlrrrrrrrrrrlrrrrrrrrrlrrrrrrrrrlrrrrrrrlrrrrrrrlrrrrrrlrrrrrrrrrrlrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Stating the Problem", "text": "eeisrVnree\u00fcgr rf\u00fc eid rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "3 Experiments", "text": "The method for fast k-calculation (AutoNN) was tested against three other algorithms from the ANN library 1.1, while further 35k search results are required. [16]: Brute, kd-tree and bd-tree kNN with default settings. The AutoNN and its competitors performed a full cross-validation on the display, diabetes, gene, glass, heart, heart, horse, ionosphere, iris, mushrooms, soybeans, STATLOG Australian, STATLOG heart, STATLOG SAT, STATLOG segment STATLOG shuttle, STATLOG vehicle, thyroid, waveform and wine records with 3, 5, 10 and 20 folds. The aim of the experiment was to measure the time required to complete the test run with different k.The data were divided into the different caric mode, into layered partitions that were used in different configurations, 10 and 20 folds. The experiment was designed to measure the time required to complete the test run with different k.The data were divided into layered partitions, waveform and wine records with 3, 5, 10 and 20 folds."}, {"heading": "4 Discussion and Conclusion", "text": "It is embedded in many automatic environments that take advantage of the flexibility of kNN. Although kNN has been used, analyzed and advanced for almost six decades, a repetitive question cannot be answered by the current literature. The advantage is that additional information can be provided about the data. This additional information allows to pre-calculate the distances between all vectors without waste and to use them repeatedly. Of this design change, which is not discussed in the literature, a decrease in time complexity can be observed (f \u2212 1 f).nbsp"}, {"heading": "5 Acknowledgments", "text": "This work was made possible by the funding of the PaREn project (Pattern Recognition and Engineering [18]) by the German Federal Ministry of Education and Research (BMBF)."}, {"heading": "19. Janick V. Frasch and Aleksander Lodwich and Faisal Shafait and Thomas M.", "text": "Breuel A Bayes-true data generator for evaluation of supervised and unsupervised learning methods. Pattern Recognition Letters, Volume 32: 11, p.1523-1531, 2011, doi: 2011.04.010APPENDIXA Influence of k on the kNN PerformanceThe following diagrams are the results of kNN natural and synthetic datasets."}], "references": [{"title": "Discriminatory analysis, nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L. Hodges"], "venue": "Technical Report 4, USAF School of Aviation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1951}, {"title": "Adaptive Metric nearest Neighbor Classification", "author": ["Carlotta Domeniconi", "Dimitrios Gunopulos", "Jing Peng"], "venue": "CVPR, Vol. 1,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Adaptive Kernel Metric Nearest Neighbor Classification", "author": ["Jing Peng", "Douglas R. Heisterkamp", "H.K. Dai"], "venue": "ICPR, Vol. 3,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning Weighted Distances for Relevance Feedback in Image Retrieval", "author": ["T. Deselaers", "R. Paredes", "E. Vidal", "H. Ney"], "venue": "ICPR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "On Packing R-trees", "author": ["I. Kamel", "C. Faloutsos"], "venue": "Proceedings of the 2nd Conference on Information and Knowledge Management (CIKM), Washington DC", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest"], "venue": "ch. 10, MIT Press and McGraw-Hill", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "An index structure for highdimensional data", "author": ["S. Berchtold", "D. Keim", "H.-P. Kriegel. The X-tree"], "venue": "In Proceedings of 22th International Conference on Very Large Databases (VLDB96), pp 2839, Morgan Kaufmann", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "An Efficient Technique for Nearest-Neighbor Query Processing on the SPY-TEC", "author": ["Dong-Ho Lee", "Hyoung-Joo Kim"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Prototype Selection for the Nearest-Neighbor Rule Through Proximity Graphs", "author": ["Jose Salvador-Sanchez", "Filiberto Pla", "Francesco J. Ferri"], "venue": "PRL, Vol. 18,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Selection of the optimal prototype subset for 1-NN classification PRL", "author": ["U. Lipowezky"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "The Omnipresence of Case-Based Reasoning in Science and Application", "author": ["David W. Aha"], "venue": "Journal on Knowledge-Based Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Secure K-NN Algorithm for Distributed Databases", "author": ["B. Young", "R. Bhatnagar"], "venue": "University of Cincinnati", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Privacy Preserving Nearest Neighbor Search", "author": ["M. Shaneck", "K. Yongdae", "V. Kumar"], "venue": "Dept. of Computer Science Minneapolis", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Privately Computing a Distributed k-nn Classifier", "author": ["M. Kantarcoglu", "C. Clifton"], "venue": "LNCS, Vol. 3202, 279\u2013290", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximate Nearest Neighbor Library 1.1, Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland", "author": ["D.M. Mount"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Bayes-true data generator for evaluation of supervised and unsupervised learning methods", "author": ["Janick V. Frasch", "Aleksander Lodwich", "Faisal Shafait", "Thomas M. Breuel"], "venue": "Pattern Recognition Letters, volume 32:11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Since the introduction of the k Nearest Neighbor (kNN) method by Fix and Hodges in 1951 [1] a lot of different variants of it have appeared in order to make it suitable to different scenarios.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 179, "endOffset": 182}, {"referenceID": 8, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 221, "endOffset": 224}, {"referenceID": 9, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 225, "endOffset": 229}, {"referenceID": 11, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 286, "endOffset": 290}, {"referenceID": 12, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 291, "endOffset": 295}, {"referenceID": 13, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 296, "endOffset": 300}, {"referenceID": 10, "context": "The continuing richness of investigation work into nearest neighbor can be explained with the omnipresence of CBR (Case Based Reasoning) type of problems [12] or just from the practical point of view of its massive parallelizability or simply populartiy.", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "1[16]: brute, kd-tree and bd-tree kNN with default settings.", "startOffset": 1, "endOffset": 5}], "year": 2017, "abstractText": "The k Nearest Neighbors (kNN) method has received much attention in the past decades, where some theoretical bounds on its performance were identified and where practical optimizations were proposed for making it work fairly well in high dimensional spaces and on large datasets. From countless experiments of the past it became widely accepted that the value of k has a significant impact on the performance of this method. However, the efficient optimization of this parameter has not received so much attention in literature. Today, the most common approach is to cross-validate or bootstrap this value for all values in question. This approach forces distances to be recomputed many times, even if efficient methods are used. Hence, estimating the optimal k can become expensive even on modern systems. Frequently, this circumstance leads to a sparse manual search of k. In this paper we want to point out that a systematic and thorough estimation of the parameter k can be performed efficiently. The discussed approach relies on large matrices, but we want to argue, that in practice a higher space complexity is often much less of a problem than repetetive distance computations.", "creator": "LaTeX with hyperref package"}}}