{"id": "1606.02680", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "First Result on Arabic Neural Machine Translation", "abstract": "Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar&lt;-&gt;En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.", "histories": [["v1", "Wed, 8 Jun 2016 18:36:09 GMT  (25kb,D)", "http://arxiv.org/abs/1606.02680v1", "EMNLP submission"]], "COMMENTS": "EMNLP submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amjad almahairi", "kyunghyun cho", "nizar habash", "aaron courville"], "accepted": false, "id": "1606.02680"}, "pdf": {"name": "1606.02680.pdf", "metadata": {"source": "CRF", "title": "First Result on Arabic Neural Machine Translation", "authors": ["Amjad Almahairi", "Kyunghyun Cho", "Nizar Habash", "Aaron Courville"], "emails": ["amjad.almahairi@umontreal.ca", "kyunghyun.cho@nyu.edu", "nizar.habash@nyu.edu", "aaron.courville@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become an important alternative to the widely used statistical phrase translation system (Koehn et al., 2003), as evidenced by the successful entries in WMT '15 and WMT' 16. Previous work on the use of neural networks for Arabic translation has mainly focused on the use of neural networks to induce an additional feature for phrase-based statistical machine translation systems (see e.g. (Devlin et al., 2014; Setiawan et al., 2015). This hybrid approach has resulted in an impressive improvement over other systems without a neural network, raising hopes that a fully neural translation system can achieve even higher translation quality."}, {"heading": "2 Neural Machine Translation", "text": "An essential factor behind neural machine translation is an attention-based encoder decoder model (Bahdanau et al., 2015; Cho et al., 2015). This attention-based encoder decoder model consists of an encoder, decoder and attention mechanism. The encoder, often implemented as a bi-directional recursive network, reads a source code Xiv: 160 6.02 680v 1 [cs.C L] 8J un2 01sentence X = (x1,., xTx) and provides a set of context vectors C = (h1,.,.,., hTx). The decoder is a recursive language model. At any time it calculates the new hidden state bytes \u2032 = (z-1, y-t-1, ct-1)."}, {"heading": "3 Processing of Arabic for Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Characteristics of Arabic Language", "text": "Arabic has a rich morphology, which makes Arabic difficult to process natural language and machine translation. For example, a single Arabic character \"\u00e9 J J\" Q \u00d6\u00cf\u00f0 \"(\" and to its vehicle \"in English) is preceded by\" \u00f0 \"(\" and \") and\" \u0445 \"(\" to \") for the basic lexeme\" \u00e9 J. \"\" Q\u00d3 \"(\" vehicle \") is formed by appending\" \u00e8 \"(\" to be \") and replacing the female suffix\" \u00e8 \"(ta marbuta) of the basic lexeme with\" H. \"This feature of Arabic is a challenge because (1) it increases the number of non-vocabulary characters, (2) it thus exacerbates the problem of data spareness 1 and (3) the verbatim correspondence between Arabic and another language in translation, which is often exacerbated by the orthographic ambiguity in Arabic scripts, as well as the inconsistency between the spelling in Arabic and the other language (3)."}, {"heading": "3.2 Morphology-Aware Tokenization", "text": "The goal of morphology-conscious tokenization or morpheme segmentation (Creutz and Lagus, 2005) is to divide a word in its surface form into a sequence of linguistically flawless subunits. In contrast to simple string-based tokenization methods, morphology-conscious tokenization relies on linguistic knowledge of a target language (in our case Arabic) and applies, for example, various morphological or orthographic adjustments to the resulting subunits. In this paper, we examine the tokenization scheme used in the Penn Arabic Treebank (ATB, (Maamouri et al., 2004), which works well with a phrase-based translation system in (El Kholy and Habash, 2012). This tokenization separates all clich\u00e9s except defined articles. When translating into Arabic, the decoded sequence of tokenized symbols is deciphered."}, {"heading": "3.3 Orthographic Normalization", "text": "Since the sources of greater orthographic ambiguity lie in the letters \"alif\" and \"ya,\" we normalize these letters (and their contradictory substitutions), replace parentheses \"(\" and \")\" with special characters \"-LRB-\" and \"-RRB-\" and remove diacritics."}, {"heading": "4 Experimental Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Preparation", "text": "Training Corpus We combine LDC2004T18, LDC2004T17 and LDC2007T08 into a parallel training corpus. The combined corpus contains approximately 1.2 million sentence pairs with 33 million tokens on the Arabic side. Most sentences are from news articles. We ignore sentence pairs that have more than 100 tokens on both sides. In-Domain Evaluation Sets We use the assessment sets of NIST 2004 (MT04) and 2005 (MT05) as development and test sets. In Ar \u2192 En we use all four English reference translations to measure translation quality. We only use the first English sentence of four as source while En \u2192 Ar. Both sentences are derived from news articles, just as the training corpus is. Out-of-Domain Evaluation Set of En \u2192 Ar, we evaluate the corpses of En \u2192 Ar, we evaluate the phrases of four as source while Er is derived from both sets of news articles."}, {"heading": "4.2 Machine Translation Systems", "text": "Phrase-based machine translation We use Moses (Koehn et al., 2007) to build a standardized phrase-based statistical machine translation system. Text alignment was extracted from GIZA + + (Och and Ney, 2003), and we used phrases up to 8 words to create a phrase table. We use the following options for alignment symmetry and reordering model: Grow-diag-final-and-and-msd-bidirectional. KenLM (Heafield et al., 2013) is used as a language model and trained on the target side of the training corpus. Neural machine translation We use a publicly available implementation of attention-based neural machine translation."}, {"heading": "4.3 Normalization and Tokenization", "text": "Arabic We test simple tokenization (Tok) based on the script of Moses, orthographic normalization (Norm), and morphology-conscious tokenization (ATB) using MADAMIRA (Pasha et al., 2014). In the latter scenario, we reverse tokenization before calculating BLEU. Note that ATB 2 contains https: / / github.com / nyu-dl / dl4mt-tutorial standard, and both contain simple tokenization performed by MADAMIRA.English. We test simple tokenization (Tok), reduction (Lowercasing) for En \u2192 Ar, and truecasing (True, (Lita et al., 2003)) for Ar \u2192 En.Byte pair coding As mentioned in September 2, we use byte-pair coding (BPE) for neural machine translation. We apply BPE to the readaltooled script we have at our disposal until we extract the 20pus."}, {"heading": "5 Result and Analysis", "text": "En \u2192 Ar From Table 2, we note that the translation quality improves as a better pre-processing routine is used. By using normalization and morphology-aware tokenization (Tok + Norm + ATB), the phrase-based and neural systems each achieve as much as + 4.46 and + 4.98 BLEU across baselines, on MT05. Even more significant is the improvement on MEDAR, whose domain differs from the training corpus, which confirms that proper pre-processing of the Arabic script actually helps deal with word marks that are not present in a training corpus. We note that the tokenization strategies tested have almost identical effects on phrase and neural translation systems. The translation quality of both systems is largely effective through the tokenization strategy used for Arabic, and is largely insensitive to the question of whether source sentences in English are subordinate to Arabic."}, {"heading": "6 Conclusion", "text": "We presented initial results on Arabic neural MT and conducted extensive experiments comparing it to a standard phrase system. We concluded that neural MT benefits from morphological tokenization and is resilient to domain changes."}], "references": [{"title": "Segmentation for english-to-arabic", "author": ["Badr et al.2008] Ibrahim Badr", "Rabih Zbib", "James Glass"], "venue": null, "citeRegEx": "Badr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Badr et al\\.", "year": 2008}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks. Multimedia", "author": ["Cho et al.2015] Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0", "author": ["Creutz", "Lagus2005] Mathias Creutz", "Krista Lagus"], "venue": "Helsinki University of Technology", "citeRegEx": "Creutz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2005}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Orthographic and morphological processing for english\u2013arabic statistical machine translation", "author": ["El Kholy", "Habash2012] Ahmed El Kholy", "Nizar Habash"], "venue": "Machine Translation,", "citeRegEx": "Kholy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kholy et al\\.", "year": 2012}, {"title": "Arabic preprocessing schemes for statistical machine translation", "author": ["Habash", "Sadat2006] Nizar Habash", "Fatiha Sadat"], "venue": null, "citeRegEx": "Habash et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2006}, {"title": "Evaluation methodology and results for english-to-arabic mt", "author": ["Hamon", "Choukri2011] Olivier Hamon", "Khalid Choukri"], "venue": "Proceedings of MT Summit XIII,", "citeRegEx": "Hamon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hamon et al\\.", "year": 2011}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "The penn arabic treebank: Building a large-scale annotated arabic corpus", "author": ["Ann Bies", "Tim Buckwalter", "Wigdan Mekki"], "venue": "In NEMLAR conference on Arabic language resources and tools,", "citeRegEx": "Maamouri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2004}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis", "author": ["Pasha et al.2014] Arfath Pasha", "Mohamed AlBadrashiny", "Mona T Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "What matters most in morphologically segmented smt models. Syntax, Semantics and Structure in Statistical Translation, page 65", "author": ["Colin Cherry", "Grzegorz Kondrak"], "venue": null, "citeRegEx": "Salameh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salameh et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Statistical machine translation features with multitask tensor networks. arXiv:1506.00698", "author": ["Zhongqiang Huang", "Jacob Devlin", "Thomas Lamar", "Rabih Zbib", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Setiawan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Setiawan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al.", "startOffset": 27, "endOffset": 101}, {"referenceID": 2, "context": "Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al.", "startOffset": 27, "endOffset": 101}, {"referenceID": 13, "context": ", 2014) has become a major alternative to the widely used statistical phrase-based translation system (Koehn et al., 2003), evidenced by the successful entries in WMT\u201915 and WMT\u201916.", "startOffset": 102, "endOffset": 122}, {"referenceID": 7, "context": ", (Devlin et al., 2014; Setiawan et al., 2015)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 21, "context": ", (Devlin et al., 2014; Setiawan et al., 2015)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 1, "context": "vanilla attention-based neural machine translation system (Bahdanau et al., 2015) against a vanilla phrase-based system (Moses, (Koehn et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 13, "context": ", 2015) against a vanilla phrase-based system (Moses, (Koehn et al., 2003)), while varying pre-/post-processing routines, including morphology-aware tokenization and orthographic normalization, which were found to be crucial in Arabic translation (Habash and Sadat, 2006;", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "A major workforce behind neural machine translation is an attention-based encoder-decoder model (Bahdanau et al., 2015; Cho et al., 2015).", "startOffset": 96, "endOffset": 137}, {"referenceID": 3, "context": "A major workforce behind neural machine translation is an attention-based encoder-decoder model (Bahdanau et al., 2015; Cho et al., 2015).", "startOffset": 96, "endOffset": 137}, {"referenceID": 20, "context": "Subword Symbols Sennrich et al. (2015), Chung", "startOffset": 16, "endOffset": 39}, {"referenceID": 20, "context": "Therefore, in our experiments, we use character n-grams selected by byte pair encoding (Sennrich et al., 2015).", "startOffset": 87, "endOffset": 110}, {"referenceID": 16, "context": "In this paper, we investigate the tokenization scheme used in the Penn Arabic Treebank (ATB, (Maamouri et al., 2004)) which was found to work well with phrase-based translation system in (El Kholy and Habash, 2012).", "startOffset": 93, "endOffset": 116}, {"referenceID": 4, "context": "1 of (Cho, 2015) for detailed discussion.", "startOffset": 5, "endOffset": 16}, {"referenceID": 0, "context": "proposed in (Badr et al., 2008; Salameh et al., 2015).", "startOffset": 12, "endOffset": 53}, {"referenceID": 19, "context": "proposed in (Badr et al., 2008; Salameh et al., 2015).", "startOffset": 12, "endOffset": 53}, {"referenceID": 14, "context": "Phrase-based Machine Translation We use Moses (Koehn et al., 2007) to build a standard", "startOffset": 46, "endOffset": 66}, {"referenceID": 11, "context": "KenLM (Heafield et al., 2013) is used as a", "startOffset": 6, "endOffset": 29}, {"referenceID": 2, "context": "2 For both directions\u2013En\u2192Ar and Ar\u2192En\u2013, the encoder is a bidirectional recurrent network with two layers of 512\u00d72 gated recurrent units (GRU, (Cho et al., 2014)), and the decoder a unidirectional recurrent network with 512 GRU\u2019s.", "startOffset": 142, "endOffset": 160}, {"referenceID": 24, "context": "Each model is trained for approximately seven days using Adadelta (Zeiler, 2012) until the cost on the development set stops improving.", "startOffset": 66, "endOffset": 80}, {"referenceID": 22, "context": "We regularize each model by applying dropout (Srivastava et al., 2014) to the output layer and penalizing the L2 norm of the parameters (coefficient 10\u22124).", "startOffset": 45, "endOffset": 70}, {"referenceID": 18, "context": "Arabic We test simple tokenization (Tok) based on the script from Moses, and orthographic normalization (Norm), and morphology-aware tokenization (ATB) using MADAMIRA (Pasha et al., 2014), .", "startOffset": 167, "endOffset": 187}, {"referenceID": 20, "context": "available script released by Sennrich et al. (2015).", "startOffset": 29, "endOffset": 52}], "year": 2016, "abstractText": "Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar\u2194En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.", "creator": "LaTeX with hyperref package"}}}