{"id": "1606.02689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Continuously Learning Neural Dialogue Management", "abstract": "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model. The experiments demonstrate the supervised model's effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model's performance in both interactive settings, especially under higher-noise conditions.", "histories": [["v1", "Wed, 8 Jun 2016 19:03:06 GMT  (749kb,D)", "http://arxiv.org/abs/1606.02689v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pei-hao su", "milica gasic", "nikola mrksic", "lina rojas-barahona", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve young"], "accepted": false, "id": "1606.02689"}, "pdf": {"name": "1606.02689.pdf", "metadata": {"source": "CRF", "title": "Continuously Learning Neural Dialogue Management", "authors": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen"], "emails": ["sjy}@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Developing a robust spoken dialogue system (SDS) traditionally requires a considerable amount of handcrafted rules combined with various statistical components. In a task-oriented SDS, teaching a system how to respond appropriately is not trivial. However, more recently, this dialogue management task has been formulated as an enhanced learning (RL) problem that can be automatically optimized through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jurc \"c\" ek et al., 2011; Young et al., 2013). Within this framework, the system learns a reward function that determines dialogue success through a trial and error process guided by a potentially delayed learning goal (El Asri et al., 2015; Vandyke et al., 2015; Vandyke et al al al al., 2016). Efficient learning algorithms are trained to enable the system."}, {"heading": "2 Neural Dialogue Management", "text": "The proposed framework addresses the dialog management component in a modular SDS. As illustrated in Figure 1, input to the model is the state of belief s, which encodes the intentions of the understood user along with the dialog history (Henderson et al., 2014b; Mrks \u0441ic \u0301 et al., 2015), and the output is the master dialog action a, which determines the semantic response, which is then passed on to the natural speech generator (Wen et al., 2015).The dialog management is presented as a political network, a neural network with a hidden layer exploiting tanh nonlinearities, an output layer consisting of two Softmax partitions and six sigmoid partitions. For Softmax outputs, one is used to predict DiaAct, a multi-class designation over five dialog files: {Request, Offer, Confirm, Selection, Selection, bye}, and the other prediction modules used in each one."}, {"heading": "2.1 Phase I: Supervised Learning", "text": "In the first phase, the political network is trained on the basis of corpus data, which can come from the WoZ survey or from user interactions with an existing policy, such as a handmade baseline. The goal is to \"imitate\" the reaction behavior within the monitored data. The training goal for each sample is to minimize a common cross-entropy loss L (\u03b8) between the model action marks y defined in \u00a7 2 and the predictions p: L: L (\u03b8) = \u2211 k: da, q, Os: H (yk, pk), (1) where DiaAct da and Query q outputs are categorical distributions and the Offer Set Os contain six binary offer slots. \u03b8 are the network parameters."}, {"heading": "2.2 Phase II: Reinforcement Learning", "text": "The policy that is trained in Phase I on a fixed data set cannot generalize well. In spoken dialogue, the sound level may vary according to conditions and thus significantly affect performance. Therefore, the second phase of the training pipeline aims at improving the SL-trained political network by further training using a political gradient based on RL. The training goal is to find a parameterized policy approach that maximizes the expected reward. J (\u03b8) of a dialogue with T turns: J (\u03b8) = E [\u2211 Tt = 1 \u03b3 tr (st, at).master action is where the discount factor and r (st, at) is the reward that maximizes the J (\u03b8) of a dialogue with T turns. The structure and initial weights of the policy are fixed by the SL preparation phase, as the RL training explicitly aims to improve the SL unit."}, {"heading": "3 Experimental Results", "text": "The target application is a live telephone SDS that provides restaurant information for the Cambridge (UK) area. The domain consists of about 150 venues, each of which has 6 slots, 3 of which can be used by the system to narrow the search (food type, area and price range) and 3 are informable properties (phone number, address and zip code) that are available as soon as a database unit is found. The model was implemented using the Theano library (Theano Development Team, 2016), the hidden layer size was set to 32 and all weights were randomly initialized between -0.1 and 0.1."}, {"heading": "3.1 Supervised Learning on Corpus Data", "text": "A corpus consisting of 720 user dialogs in the Cambridge restaurant area was divided into 4: 1: 1 for training, validation and testing. This corpus was collected via the Amazon Mechanical Turk (AMT) service, in which paid subjects interacted via language with a well-behaved dialogue system, as proposed in (Su et al., 2016).The raw data contains the best N-language recognition results (ASR), which were passed on to a rule-based semantic decoder and the focus-faith state tracker (Henderson et al., 2014a) to maintain the faith state that serves as an input mark for the proposed policy network.The turn-level labels were tagged in accordance with \u00a7 2. Adagrad (Duchi et al., 2011) per dialogue was used during the re-propagation to train each model based on the target in Equation 1. To prevent overmatch, the weighted results were applied to the overall validation process based on the 1, the validation method was applied to the predicate the result of the 1."}, {"heading": "3.2 Policy Network in Simulation", "text": "The political network was tested with a simulated user (Schatzmann et al., 2006) who provided the interaction at the semantic level. As shown in Figure 2, the first grid points designated \"SL: 0\" represent the performance of the SL model under various semantic error rates (SER), which averaged over 500 dialogs.The SL model was then trained on various SERs using RL. As the SL model already works well, the exploration parameter was set to 0.1. The size of the experience replay pool L was 2,000, and the minichmargin size was set to 32. For each update, the natural gradient of eNAC was calculated to update the model weights of the size \u0445 2600. The total yield for each dialogue was set to 20 \u00d7 1 (D) \u2212 T, with T being the dialog speed and 1 (D) being the success indicator for the dialogue."}, {"heading": "3.3 Policy Network with Real Users", "text": "Based on the same SL policy network as in paragraph 3.2, the model was improved using RL, using human subjects recruited via AMT. The policy network was connected to a modular SDS that included Microsoft's Bing speech recognition 3, a rules-based semantic decoder, the Focus Faith State Tracker, and a template-based natural language generator. To ensure dialogue quality, only those dialogs were considered whose objective system check matched the user assessment (Gas ic \"et al., 2013). Building on this, two parallel policies were trained with 200 dialogues. To evaluate the resulting policies, political learning was deactivated, and a further 110 dialogues were collected with both the SL and SL + RL models. AMT users were asked to rate 3www.microsoft.com / cognitive-services / en-us / api.the dialogue quality of the SL was also shown to be a good answer to the SL-6 question."}, {"heading": "4 Conclusion", "text": "In this paper, a two-step development for dialogue management in SDS was presented, in which the model can be trained on a fixed dialogue dataset using a uniform neural network framework using SL and then improved through simulated or spoken interactions using RL. Experiments demonstrated the efficiency of the proposed model based on only a few hundred monitored dialog examples. Further, the model was tested in simulated and real user settings. In a non-adapted environment, the model was able to change its behavior through a delayed reward signal and achieve a better success rate."}, {"heading": "Acknowledgments", "text": "Pei-Hao Su is supported by the Cambridge Trust and Taiwan's Ministry of Education."}], "references": [{"title": "Learning end-to-end goal-oriented dialog", "author": ["Bordes", "Weston2016] Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint:", "citeRegEx": "Bordes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2016}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["Simon Keizer", "Oliver Lemon"], "venue": "arXiv preprint arXiv:1511.08099", "citeRegEx": "Cuay\u00e1huitl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2015}, {"title": "Wizard of oz studies: why and how", "author": ["Arne J\u00f6nsson", "Lars Ahrenberg"], "venue": "In Proc of Intelligent user interfaces", "citeRegEx": "Dahlb\u00e4ck et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dahlb\u00e4ck et al\\.", "year": 1993}, {"title": "A comprehensive reinforcement learning framework for dialogue management optimisation", "author": ["Matthieu Geist", "Senthilkumar Chandramohan", "Olivier Pietquin"], "venue": "Journal of Selected Topics in Signal Processing,", "citeRegEx": "Daubigney et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daubigney et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Task completion transfer learning for reward inference", "author": ["Romain Laroche", "Olivier Pietquin"], "venue": "In Proc of MLIS", "citeRegEx": "Asri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asri et al\\.", "year": 2014}, {"title": "Gaussian processes for pomdp-based dialogue manager optimization", "author": ["Ga\u0161i\u0107", "Young2014] Milica Ga\u0161i\u0107", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Ga\u0161i\u0107 et al.2013] Milica Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve J. Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "The Second Dialog State Tracking Challenge", "author": ["B. Thomson", "J. Williams"], "venue": "In Proc of SIGdial", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based Dialog State Tracking with Recurrent Neural Networks", "author": ["B. Thomson", "S.J. Young"], "venue": "In Proc of SIGdial", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as pomdps", "author": ["Blaise Thomson", "Steve Young"], "venue": "TSLP,", "citeRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.", "year": 2011}, {"title": "An iterative design methodology for user-friendly natural language office information applications", "author": ["John F. Kelley"], "venue": "ACM Transaction on Information Systems", "citeRegEx": "Kelley.,? \\Q1984\\E", "shortCiteRegEx": "Kelley.", "year": 1984}, {"title": "A stochastic model of computerhuman interaction for learning dialogue strategies. Eurospeech", "author": ["Levin", "Pieraccini1997] Esther Levin", "Roberto Pieraccini"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine learning,", "citeRegEx": "Lin.,? \\Q1992\\E", "shortCiteRegEx": "Lin.", "year": 1992}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Mrk\u0161i\u0107 et al.2015] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of ACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Schaal2006] Jan Peters", "Stefan Schaal"], "venue": "In IEEE RSJ", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Spoken dialogue management using probabilistic reasoning", "author": ["Roy et al.2000] Nicholas Roy", "Joelle Pineau", "Sebastian Thrun"], "venue": "In Proc of SigDial", "citeRegEx": "Roy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2000}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Trust region policy optimization", "author": ["Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "Proc of ICML", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neu", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["Su et al.2015] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "On-line active reward learning for policy optimisation in spoken dialogue systems", "author": ["Su et al.2016] Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of ACL", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Multi-domain dialogue success classifiers for policy training", "author": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": "ASRU", "citeRegEx": "Vandyke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vandyke et al\\.", "year": 2015}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In Proceedings of the 2015 Conference", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "Pomdp-based statistical spoken dialogue systems: a review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161ic", "Blaise Thomson", "Jason Williams"], "venue": "In Proc of IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 10, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 28, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 21, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 23, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 22, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 3, "context": "To enable the system to be trained on-line, sample-efficient learning algorithms have been proposed (Ga\u0161i\u0107 and Young, 2014; Daubigney et al., 2014) which can learn policies from a minimal number of dialogues.", "startOffset": 100, "endOffset": 147}, {"referenceID": 11, "context": "Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlb\u00e4ck et al., 1993) have been widely used for collecting domain-specific training corpora.", "startOffset": 27, "endOffset": 64}, {"referenceID": 2, "context": "Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlb\u00e4ck et al., 1993) have been widely used for collecting domain-specific training corpora.", "startOffset": 27, "endOffset": 64}, {"referenceID": 19, "context": "training a network-based dialogue model, mostly in text-input schemes (Vinyals and Le, 2015; Serban et al., 2015; Wen et al., 2016; Bordes and Weston, 2016).", "startOffset": 70, "endOffset": 156}, {"referenceID": 26, "context": "training a network-based dialogue model, mostly in text-input schemes (Vinyals and Le, 2015; Serban et al., 2015; Wen et al., 2016; Bordes and Weston, 2016).", "startOffset": 70, "endOffset": 156}, {"referenceID": 20, "context": "This resembles the training process used in AlphaGo (Silver et al., 2016) for the game of Go.", "startOffset": 52, "endOffset": 73}, {"referenceID": 1, "context": "In addition, unlike most of thestate-of-the-art RL-based dialogue systems (Ga\u0161i\u0107 and Young, 2014; Cuay\u00e1huitl et al., 2015) which operate on a constrained set of summary actions to", "startOffset": 74, "endOffset": 122}, {"referenceID": 25, "context": "This is subsequently passed to the natural language generator (Wen et al., 2015).", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "For larger models with more parameters, a truncated variant (Schulman et al., 2015) can also be used to practically calculate the natural gradient.", "startOffset": 60, "endOffset": 83}, {"referenceID": 13, "context": "Experience replay (Lin, 1992) is utilised to randomly sample mini-batches of experiences from a reply pool P .", "startOffset": 18, "endOffset": 29}, {"referenceID": 22, "context": "with a well-behaved dialogue system as proposed in (Su et al., 2016).", "startOffset": 51, "endOffset": 68}, {"referenceID": 4, "context": "Adagrad (Duchi et al., 2011) per dialogue was used during backpropagation to train each model based on the objective in", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "The policy network was tested with a simulated user (Schatzmann et al., 2006) which provides the interaction at the semantic level.", "startOffset": 52, "endOffset": 77}, {"referenceID": 7, "context": "To ensure the dialogue quality, only those dialogues whose objective system check matched with the user rating were considered (Ga\u0161i\u0107 et al., 2013).", "startOffset": 127, "endOffset": 147}], "year": 2016, "abstractText": "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradientbased algorithms on one single model. The experiments demonstrate the supervised model\u2019s effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model\u2019s performance in both interactive settings, especially under higher-noise conditions.", "creator": "LaTeX with hyperref package"}}}