{"id": "1606.02807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Face valuing: Training user interfaces with facial expressions and reinforcement learning", "abstract": "An important application of interactive machine learning is extending or amplifying the cognitive and physical capabilities of a human. To accomplish this, machines need to learn about their human users' intentions and adapt to their preferences. In most current research, a user has conveyed preferences to a machine using explicit corrective or instructive feedback; explicit feedback imposes a cognitive load on the user and is expensive in terms of human effort. The primary objective of the current work is to demonstrate that a learning agent can reduce the amount of explicit feedback required for adapting to the user's preferences pertaining to a task by learning to perceive a value of its behavior from the human user, particularly from the user's facial expressions---we call this face valuing. We empirically evaluate face valuing on a grip selection task. Our preliminary results suggest that an agent can quickly adapt to a user's changing preferences with minimal explicit feedback by learning a value function that maps facial features extracted from a camera image to expected future reward. We believe that an agent learning to perceive a value from the body language of its human user is complementary to existing interactive machine learning approaches and will help in creating successful human-machine interactive applications.", "histories": [["v1", "Thu, 9 Jun 2016 03:06:46 GMT  (4210kb,D)", "http://arxiv.org/abs/1606.02807v1", "7 pages, 4 figures, IJCAI 2016 - Interactive Machine Learning Workshop"]], "COMMENTS": "7 pages, 4 figures, IJCAI 2016 - Interactive Machine Learning Workshop", "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["vivek veeriah", "patrick m pilarski", "richard s sutton"], "accepted": false, "id": "1606.02807"}, "pdf": {"name": "1606.02807.pdf", "metadata": {"source": "CRF", "title": "Face valuing: Training user interfaces with facial expressions and reinforcement learning", "authors": ["Vivek Veeriah", "Patrick M. Pilarski", "Richard S. Sutton"], "emails": ["rsutton}@ualberta.ca"], "sections": [{"heading": "1 Introduction", "text": "An important goal of human-machine interaction is to expand existing human capabilities, which requires machines and their human users to work closely together and form a productive partnership. To achieve this, it is critical that machines learn interactively from their users, especially their intentions and preferences. In current research trends, the user's preferences are communicated through explicit instructions (Kuhlmann et al., 2004) or through expensive corrective feedback (Knox & Stone, 2015) - in the form of predefined words or sentences, push buttons, mouse clicks, etc. In many real-world scenarios, these methods of feedback impose a cognitive burden on human users."}, {"heading": "2 Related Methods", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "3 Reinforcement Learning", "text": "In a constellation in which learning is enhanced (Sutton & Barto, 1998), a learning agent interacts with an unknown environment to achieve a goal. \u00b7 In this constellation, the goal is to maximize cumulative reward by adjusting its behavior within the environment. \u00b7 Markov Decision Processes (MDPs) are the mathematical notations used to formalize an affirmation problem. An MDP consists of a tuple S, A, p, r, \u03b3, consisting of S, set of all states; A, set of all actions; p (s, a), a transition probability function that indicates the probability of transition to state S in the next time step given for the current state S; and action a; r (s, a, s), the reward function that gives the expected reward for a transition from state S to a state."}, {"heading": "4 Grip Selection Task", "text": "To evaluate the face evaluation approach, we introduce a handle selection task inspired by a natural problem in an area of the prosthetic arm, in which the agent must select a suitable grip pattern to capture a particular object. The task could also consist of a series of n grips and m objects, depending on the experiment, there could be many possible grips for a particular object, defining the right grip according to the user's preferences. The task could also consist of countless objects, making the selection of a handle with pure trial error a non-trivial problem. this task was formulated as an undiscounted episodic MDP, with a reward of 0 for each time step and with 0 reward for ending an episode. At the beginning of each episode, a single object was randomly selected from a large group of objects, and the agent was instructed to select a handle from a limited series of grips; once the agent had selected a handle to complete an object, he had to move a fixed number of steps in the direction of the object."}, {"heading": "5 Experimental Setup", "text": "For our experiments, two Sarsa (\u03bb) (Rummery & Niranjan, 1994; Sutton & Barto, 1998) agents (one using face evaluation and one without face evaluation) are compared to the task described above. All the experimental results in this work were carried out by a well-trained user in a blind environment, i.e. the user did not know which of the two methods is currently being evaluated; the user provided both learning agents with the same form of reward by pressing a button; the two most important instructions we gave the human user were 1) to express their joy or displeasure with the agent through simple, repeatable and meticulously distinguishable facial expressions; and 2) to press the button when the learning agent did not behave in accordance with the user's expectations."}, {"heading": "5.1 State space", "text": "The state spaces for both agents are briefly described in Table 1. The non-face assessment agent has the ID of the current handle and the ID of the current object in its state space together with a bias term. The ID of the current handle chosen by the agent is encoded unilaterally to form a vector of length n. Likewise, the ID of the object is also encoded unilaterally and linked to the vector corresponding to the current grip. Thus, the total state space for this method is of length m + n + 1, where m is the total number of objects during the entire experiment and n is the total number of handles available to the agent during an experiment. For the face assessment agent, 68 key points from a frame containing a human face are recognized by a popular face recognition algorithm (Kazemi et al., 2014) These key points are simple two-dimensional coordinates indicating the position of specific locations of the human face."}, {"heading": "5.2 Action space", "text": "The complete action space for both agents consisted of {grip1, grip2, \u00b7 \u00b7, gripn, \u2191, \u2193} actions in which the first n actions implied the choice of the respective grip. The remaining two actions were the movement one step forward to the object and the movement one step back to the Grip changing station. The actions available to the agent depended on its position relative to the object and the Grip changing station. If the agent was in the Grip changing station, the available actions were {grip1, grip2, \u00b7, gripn, \u2191}, whereas when the agent left the Grip changing station, only the actions were available. If the agent pressed the reward button, the agent lost all his actions except {\u2193} until he reached the Grip changing station. The agent observed the state space once every tenth of a second and had to perform an action at each step. However, the agent had the freedom to choose the same number of human steps to allow the following action."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experiment 1: Different object-grip settings", "text": "The first experiment compared the two agents with the preferences of the user. The plots of the total time steps and the total rewards collected by the agents were shown in Fig. 3 (a), (b), \u00b7 \u00b7 \u00b7 (i) represent the total time a learner needs to complete a successful capture of episodes. Plots (Fig. 3), (k), show the total number of generated rewards given to an agent in order to successfully adapt to the preferences of the user. These graphs (Fig. 3) were performed by the same users in a blind way."}, {"heading": "6.2 Experiment 2: Infinite objects and finite grips", "text": "A second experiment (Fig. 4) showed the performance improvement achieved by our approach in a different and more difficult environment: one in which a new object was generated for each episode and no object was seen by the agent more than once during the experiment. Thus, the ability to evaluate the face to solve new or modified tasks was examined in this experiment, and the importance of quickly adapting to the preferences of a user was emphasized. Fig. 4 describes the total time steps taken by the agents to complete this experiment, while Table 2 shows the total number of explicit feedback required by the agent to successfully complete this task. Data were generated from experiments with a single user. Fig. 4 shows that the face evaluator was much quicker to adapt to the preferences of the user. It was learned to complete the task faster than an agent without face evaluation was able. From Table 2, it can be determined that the face evaluator was able to adapt to the preferences of the user much faster than an agent without face evaluation was able to perform the task."}, {"heading": "7 Discussion", "text": "In fact, it is the case that most people who are able to move are able to move, to move and to move, to move."}, {"heading": "8 Conclusions", "text": "We introduced a new and promising approach called Face Assessment to adjust an agent to a user's preferences, and demonstrated that he can achieve great performance improvements over a conventional agent who only learns through human-made rewards. By allowing the agent to perceive a value based on a user's facial expressions, the total number of expensive human-generated rewards delivered during a task was substantially reduced, and the agent was quickly able to adjust to the preferences of his user. Face Assessment learns a mapping from facial expression to user satisfaction; it formalizes satisfaction as a value function and learns this value function through time difference methods. Most work on using facial features in human-machine interaction uses facial features as control signals for an agent; surprisingly, our work seems to be the first to use facial expressions as a value function and learn this value function through time difference methods."}], "references": [{"title": "Learning from demonstrations with partially observable task parameters", "author": ["T. Alizadeh", "S. Calinon", "D.G. Caldwell", "May"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Alizadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alizadeh et al\\.", "year": 2014}, {"title": "Regulating human-robot interaction using emotions,drives, and facial expressions", "author": ["C. Breazeal", "May"], "venue": "In Proceedings of Autonomous Agents (Vol", "citeRegEx": "Breazeal et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breazeal et al\\.", "year": 1998}, {"title": "Learning from human-generated reward", "author": ["C. Breazeal", "B.C. Love", "R.J. Mooney"], "venue": null, "citeRegEx": "Breazeal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Breazeal et al\\.", "year": 2012}, {"title": "Application of real-time machine learning to myoelectric prosthesis control: A case series in adaptive switching", "author": ["A.L. Edwards", "M.R. Dawson", "J.S. Hebert", "C. Sherstan", "R.S. Sutton", "K.M. Chan", "P.M. Pilarski"], "venue": "Prosthetics and orthotics international,", "citeRegEx": "Edwards et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2015}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kazemi and Sullivan,? \\Q2014\\E", "shortCiteRegEx": "Kazemi and Sullivan", "year": 2014}, {"title": "Design Principles for Creating Human-Shapable Agents. In AAAI Spring Symposium: Agents that Learn from Human Teachers", "author": ["W.B. Knox", "I. Fasel", "P. Stone", "March"], "venue": null, "citeRegEx": "Knox et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2009}, {"title": "Teaching agents with human feedback: a demonstration of the tamer framework", "author": ["W.B. Knox", "P. Stone", "C. Breazeal", "March"], "venue": "In Proceedings of the companion publication of the 2013 international conference on Intelligent user interfaces companion (pp. 65-66)", "citeRegEx": "Knox et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2013}, {"title": "Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance", "author": ["W.B. Knox", "P. Stone"], "venue": "Artificial Intelligence,", "citeRegEx": "Knox and Stone,? \\Q2015\\E", "shortCiteRegEx": "Knox and Stone", "year": 2015}, {"title": "Training Wheels for the Robot: Learning from Demonstration Using Simulation", "author": ["N.P. Koenig", "M.J. Mataric", "October"], "venue": "In AAAI Fall Symposium: Robots Learning Interactively from Human Teachers", "citeRegEx": "Koenig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koenig et al\\.", "year": 2012}, {"title": "July. Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer. In The AAAI-2004 workshop on supervisory control of learning and adaptive systems", "author": ["G. Kuhlmann", "P. Stone", "R. Mooney", "J. Shavlik"], "venue": null, "citeRegEx": "Kuhlmann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2004}, {"title": "Subtle expressivity in a robotic computer", "author": ["K. Liu", "R.W. Picard", "April"], "venue": "In CHI 2003 Workshop on Subtle Expressiveness in Characters and Robots", "citeRegEx": "Liu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2003}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R. Loftin", "B. Peng", "J. MacGlashan", "M.L. Littman", "M.E. Taylor", "J. Huang", "D.L. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Loftin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Loftin et al\\.", "year": 2016}, {"title": "Automated proxemic feature extraction and behavior recognition: Applications in human-robot interaction", "author": ["R. Mead", "A. Atrash", "M.J. Mataric"], "venue": "International Journal of Social Robotics,", "citeRegEx": "Mead et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mead et al\\.", "year": 2013}, {"title": "A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans", "author": ["B. Peng", "J. MacGlashan", "R. Loftin", "M.L. Littman", "D.L. Roberts", "M.E. Taylor", "May"], "venue": "In Proceedings of the 2016 International Conference on Autonomous", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}, {"title": "Between Instruction and Reward: Human-Prompted Switching", "author": ["P.M. Pilarski", "R.S. Sutton", "October"], "venue": "In AAAI Fall Symposium: Robots Learning Interactively from Human Teachers", "citeRegEx": "Pilarski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pilarski et al\\.", "year": 2012}, {"title": "Prosthetic Devices as Goal-Seeking Agents", "author": ["P.M. Pilarski", "R.S. Sutton", "K.W. Mathewson"], "venue": null, "citeRegEx": "Pilarski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilarski et al\\.", "year": 2015}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": null, "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Learning from demonstrations through the use of non-rigid registration", "author": ["J. Schulman", "J. Ho", "C. Lee", "P. Abbeel"], "venue": "In Proceedings of the 16th international symposium on robotics research (ISRR)", "citeRegEx": "Schulman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2013}, {"title": "Introduction to reinforcement learning (Vol. 135)", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence,", "citeRegEx": "Thomaz and Breazeal,? \\Q2008\\E", "shortCiteRegEx": "Thomaz and Breazeal", "year": 2008}, {"title": "Guest editorial: progress on stabilizing and controlling powered upper-limb prostheses", "author": ["T.W. Williams"], "venue": "Journal of Rehabilitation Research and Development", "citeRegEx": "Williams,? \\Q2011\\E", "shortCiteRegEx": "Williams", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In current research trends, the user\u2019s preferences are conveyed via explicit instructions (Kuhlmann et al., 2004) or expensive corrective feedback (Knox & Stone, 2015)\u2014which can be in the form of predefined words or sentences, push buttons, mouse clicks etc.", "startOffset": 90, "endOffset": 113}, {"referenceID": 7, "context": "In current research trends, the user\u2019s preferences are conveyed via explicit instructions (Kuhlmann et al., 2004) or expensive corrective feedback (Knox & Stone, 2015)\u2014which can be in the form of predefined words or sentences, push buttons, mouse clicks etc. In many real-world, ongoing scenarios, these methods of feedback impose a cognitive load on human users. Moreover, in complex domains like prosthetic limbs, it is demanding for the user to provide these kinds of explicit feedback. It is important to have an alternative approach that is both scalable and would allow the machines to learn their human users\u2019intents and preferences via ongoing interactions. In this paper, we explore the idea that a reinforcement learning agent can learn a value function that relates a user\u2019s body language, specifically from the user\u2019s facial expressions, to expectations of future reward. The agent can use this value function to adapt its actions to a user\u2019s preferences quickly with minimal explicit feedback. This approach is analogous to an agent learning to understand the body language of its human user. It could also be imagined as building a form of communicative capital between a human user and a learning agent (c.f., Pilarski et al., 2015). Learning from interactions with a human user tend to be continual; reinforcement learning methods are therefore naturally suited for this purpose. To the best of our knowledge, our system is the first to learn a value function in real-time for a user\u2019s body language, specifically a value function that relates future reward to the facial features of the user. Additionally, this work is the first example of how such a value function can be used to guide the learning process of an agent interacting with a human user. Importantly, our approach does not utilize explicit reward channels, for example those discussed by Thomaz & Breazeal (2012) and by Knox et al.", "startOffset": 91, "endOffset": 1894}, {"referenceID": 5, "context": "Importantly, our approach does not utilize explicit reward channels, for example those discussed by Thomaz & Breazeal (2012) and by Knox et al. (2009). As it operates in real time, we believe that our approach is well suited for realistic human-machine interaction tasks and complements existing interactive machine learning approaches.", "startOffset": 132, "endOffset": 151}, {"referenceID": 12, "context": "Significant research effort has been directed toward creating successful human-robot partnerships (e.g., as summarized in Knox & Stone, 2015; Mead et al., 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015).", "startOffset": 98, "endOffset": 230}, {"referenceID": 2, "context": "Significant research effort has been directed toward creating successful human-robot partnerships (e.g., as summarized in Knox & Stone, 2015; Mead et al., 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015).", "startOffset": 98, "endOffset": 230}, {"referenceID": 3, "context": "Significant research effort has been directed toward creating successful human-robot partnerships (e.g., as summarized in Knox & Stone, 2015; Mead et al., 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015).", "startOffset": 98, "endOffset": 230}, {"referenceID": 1, "context": ", 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015). A natural approach is for an agent to learn from ongoing interactions with a human user via human-delivered rewards. Research by Thomaz & Breazeal (2008), Knox & Stone (2009), Breazeal et al.", "startOffset": 8, "endOffset": 233}, {"referenceID": 1, "context": ", 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015). A natural approach is for an agent to learn from ongoing interactions with a human user via human-delivered rewards. Research by Thomaz & Breazeal (2008), Knox & Stone (2009), Breazeal et al.", "startOffset": 8, "endOffset": 254}, {"referenceID": 1, "context": ", 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015). A natural approach is for an agent to learn from ongoing interactions with a human user via human-delivered rewards. Research by Thomaz & Breazeal (2008), Knox & Stone (2009), Breazeal et al. (2012), Loftin et al.", "startOffset": 8, "endOffset": 278}, {"referenceID": 1, "context": ", 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015). A natural approach is for an agent to learn from ongoing interactions with a human user via human-delivered rewards. Research by Thomaz & Breazeal (2008), Knox & Stone (2009), Breazeal et al. (2012), Loftin et al. (2015), and Peng et al.", "startOffset": 8, "endOffset": 300}, {"referenceID": 1, "context": ", 2013; Breazeal et al., 2012; Pilarski & Sutton, 2012; Edwards et al., 2015). A natural approach is for an agent to learn from ongoing interactions with a human user via human-delivered rewards. Research by Thomaz & Breazeal (2008), Knox & Stone (2009), Breazeal et al. (2012), Loftin et al. (2015), and Peng et al. (2016) adopts this perspective, and it has been extensively studied ar X iv :1 60 6.", "startOffset": 8, "endOffset": 324}, {"referenceID": 7, "context": "In the TAMER approach of Knox and Stone (2015), a system was able to learn a predictive model of a human user\u2019s shaping rewards, such that the model could be used to successfully train an agent even in the presence of human-related delays and inconsistencies.", "startOffset": 25, "endOffset": 47}, {"referenceID": 17, "context": "There are numerous works exploring learning from demonstration (e.g., Koenig & Mataric, 2012; Schulman et al., 2013; Alizadeh et al., 2014).", "startOffset": 63, "endOffset": 139}, {"referenceID": 0, "context": "There are numerous works exploring learning from demonstration (e.g., Koenig & Mataric, 2012; Schulman et al., 2013; Alizadeh et al., 2014).", "startOffset": 63, "endOffset": 139}, {"referenceID": 15, "context": "For evaluating our approach, we introduced a grip selection task wherein the learning agent had to figure out the goal through its interaction with the user; this agent can be readily termed a goal-seeking agent (Pilarski et al., 2015).", "startOffset": 212, "endOffset": 235}], "year": 2016, "abstractText": "An important application of interactive machine learning is extending or amplifying the cognitive and physical capabilities of a human. To accomplish this, machines need to learn about their human users\u2019 intentions and adapt to their preferences. In most current research, a user has conveyed preferences to a machine using explicit corrective or instructive feedback; explicit feedback imposes a cognitive load on the user and is expensive in terms of human effort. The primary objective of the current work is to demonstrate that a learning agent can reduce the amount of explicit feedback required for adapting to the user\u2019s preferences pertaining to a task by learning to perceive a value of its behavior from the human user, particularly from the user\u2019s facial expressions\u2014we call this face valuing. We empirically evaluate face valuing on a grip selection task. Our preliminary results suggest that an agent can quickly adapt to a user\u2019s changing preferences with minimal explicit feedback by learning a value function that maps facial features extracted from a camera image to expected future reward. We believe that an agent learning to perceive a value from the body language of its human user is complementary to existing interactive machine learning approaches and will help in creating successful human-machine interactive applications.", "creator": "LaTeX with hyperref package"}}}