{"id": "1606.02854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "e-Commerce product classification: our participation at cDiscount 2015 challenge", "abstract": "This report describes our participation in the cDiscount 2015 challenge where the goal was to classify product items in a predefined taxonomy of products. Our best submission yielded an accuracy score of 64.20\\% in the private part of the leaderboard and we were ranked 10th out of 175 participating teams. We followed a text classification approach employing mainly linear models. The final solution was a weighted voting system which combined a variety of trained models.", "histories": [["v1", "Thu, 9 Jun 2016 08:06:00 GMT  (24kb)", "http://arxiv.org/abs/1606.02854v1", "Technical report"]], "COMMENTS": "Technical report", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ioannis partalas", "georgios balikas"], "accepted": false, "id": "1606.02854"}, "pdf": {"name": "1606.02854.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ioannis.partalas@viseo.com", "georgios.balikas@imag.fr"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 854v 1 [cs.L G] 9J un2 016"}, {"heading": "1 Introduction", "text": "In this report, we present our participation in the cDicount 2015 Product Classification Challenge, organized on the platform www.datascience.net. The organizers provided a large collection of articles that mainly contained the text description; we adopted a text classification approach that mainly used linear models (such as SVMs) as base models; our final solution consisted of a weighted voting system that combined a variety of base models; in Section 2, we give a brief description of the cDiscount task and data; Section 3 describes our implementations and the results we achieved; and finally, Section 4 concludes with a discussion and lessons learned."}, {"heading": "2 e-Commerce Product Classification", "text": "Article categorization is fundamental to many aspects of an article lifecycle for e-commerce sites such as search, recommendation, cataloging, etc. It can be formulated as a supervised classification problem, with the categories being the target classes and the characteristics being the words that make up a textual description of the articles. As part of the cDiscount 2015 challenge, the organizers provided the descriptions of e-commerce articles and the goal was to develop a system that would perform an automatic classification of these products. Table 1 presents some training cases. There were 15,786,886 product descriptions in the training test and 35,066 cases that were to be classified in the test kit. The target categories (classes) were organized into a hierarchy consisting of 3 levels: the top level had 52 nodes, the middle level 536 and the bottom level 5,789. The challenge was aimed at predicting the lowest category for each test instance. It is noted that most classes were represented in the 1st with only a few examples and there were only 1"}, {"heading": "3 Our approach", "text": "We used a text classification approach that worked with the text information contained in the training data. In this context, x-Rd represents a document in a vector space, and y-Y = {1... K} its associated class name, where | Y | > 2. The large number of classes of the problem being addressed, as well as the lack of data for minority classes, is a typical situation in large systems. For example, we list challenges in the same line, where the available text is in contrast much larger, such as LSHTC (Partalas et al., 2015) and BioASQ (Balikas et al., 2014).1https: / / www.datascience.net / fr / challenge / 20 / details # tab"}, {"heading": "3.1 Setup", "text": "Since we used only the textual information provided for each training / test instance, our first task was to clean the data. We also cleaned the data pipeline by removing non-ascii characters, removing non-printable characters, removing HTML tags, punctuation, and the underbody. We also divided words that consisted of a piece of text and a numerical part into two different parts, such as \"12cm\" becoming \"12\" and \"cm.\" Furthermore, we did not perform mending, lemmatization, and stop-word removal due to the fact that the text span was small and such operations resulted in a loss of information. Finally, we used the remaining text in words with the white space as a delimitation.Vectorization. After cleaning the text, we generated the vectors using a uniform approach."}, {"heading": "3.2 Training and Prediction", "text": "We relied on linear models as the basis for our selection of predictions due to their efficiency in high-dimensional tasks such as text classification. Support vector machines (SVMs) are known for achieving modern performance in such tasks. To learn the basic models, we used the non-linear library that can support learning linear models in high-dimensional datasets (Fan et al., 2008). We tried two main strategies: a) flat classifiers that ignore the hierarchical structure between classes and b) hierarchical top-down classification. For flat classification, we followed the OneVersus-All (OVA) approach with a complexity O (K) in the number of classes. For the top-down classification, we trained a multi-class classifier for each parent in the hierarchy of products and during the prediction, we chose from the root and selected the best class according to the current multi-class classifier."}, {"heading": "3.3 Ensembling", "text": "We experimented with simple voting as well as weighted voting models. Simple voting always improved performance when calculated in a fraction of the overall ensemble that included only the most powerful models. A simple approach was to rank the models according to their performance (in terms of accuracy) relative to the current best model on the leaderboard. Subsequently, a simple majority decision was applied to approximately 20% -30% of the classifiers ordered, which would create a homogeneous sub-ensemble that would likely reduce the variance. We also used a weighted voting scheme that weighted fewer of our best individual models. Our final best submission (64.20% in the private board) was a weighted voting ensemble that gave greater weighting to the two best models. Weighted voting consistently improved accuracy by 1.2% -1.8%."}, {"heading": "3.4 Software and Hardware", "text": "During the task, we used scikit-learn (Pedregosa et al., 2011) as well as our own scripts to pre-process the raw data. To train the models, we experimented with Liblinear, scikitlearn and Vowpal Wabbit. We had full access to a machine with 4 cores at 2.4Ghz and 16Gb RAM and limited access to a common machine with 24 cores at 3.3Ghz and 128Gb RAM. In the first machine, we conducted our experiments with up to 270,000 features, and in the second, experiments with more features that required more memory."}, {"heading": "4 Results", "text": "When comparing the pairs of inputs (1), (2) and (9), it becomes clear that by increasing the number of unique characteristics, performance is increased. However, with about 300,000 characteristics, the improvements become negligible. Input pairs (3), (5) and (2), (6) have the advantage that bigrams are included in the attribute set alongside unigrams. Going further and adding up, apart from unigrams and bigrams, trigrams also improve performance as shown by comparing the inputs (8), (9) and (10) with the rest of the table."}, {"heading": "5 Conclusion and Discussion", "text": "Participating in cDiscount 2015 was a nice and interesting experience with several lessons we learned. We briefly discuss the most important ones: \u2022 Data preparation. Although short text classification for e-commerce products is not a new task, the cDiscount problems had two peculiarities: the enormous number of training instances and the large vocabulary even after careful cleaning of the data set. We found that the selection of features and feature engineering played an important role by studying the statistics of the data set, predicting base models and trying to identify patterns for the classes. As an example, we would like to highlight the \"Marque\" field. In the late stages of the challenge, we found that our models could benefit significantly from using this information. \u2022 Learning tools. From the early stages of the challenge, we decided to use SVMs that are known to perform well in such problems."}, {"heading": "Acknowledgements", "text": "We would like to thank the AMA team at the University of Grenoble-Alpes for providing us with the machines on which we run our algorithms.Description Public Private Coverage1 270K unique specimens, \u03b1 = 0.5, half data 63.56 63.11 3.208 2 Weighted voting 1 64.55 64.20 3.128 3 Weighted voting 2 64.57 64.14 3.116"}], "references": [{"title": "Results of the bioasq track of the question answering lab at clef", "author": ["Ioannis Partalas", "Axel-Cyrille Ngonga Ngomo", "Anastasia Krithara", "Eric Gaussier", "George Paliouras"], "venue": null, "citeRegEx": "Balikas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balikas et al\\.", "year": 2014}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "ChoJui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Aggregating local image descriptors into compact codes", "author": ["Jegou et al.2012] H. Jegou", "F. Perronnin", "M. Douze", "J. Sanchez", "P. Perez", "C. Schmid"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Jegou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Lshtc: A benchmark for large-scale text", "author": ["Aris Kosmopoulos", "Nicolas Baskiotis", "Thierry Artieres", "George Paliouras", "Eric Gaussier", "Ion Androutsopoulos", "Massih-Reza Amini", "Patrick Galinari"], "venue": null, "citeRegEx": "Partalas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Partalas et al\\.", "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Blondel et al\\.,? 2011", "shortCiteRegEx": "Blondel et al\\.", "year": 2011}, {"title": "Large-scale item categorization for e-commerce", "author": ["Shen et al.2012] Dan Shen", "Jean-David Ruvini", "Badrul Sarwar"], "venue": "In Proceedings of the 21st ACM international conference on Information and knowledge management,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "like LSHTC (Partalas et al., 2015) and BioASQ (Balikas et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": ", 2015) and BioASQ (Balikas et al., 2014).", "startOffset": 19, "endOffset": 41}, {"referenceID": 6, "context": "In addition, we did not perform stemming, lemmatization and stop-word removal due to the fact that the text spans were small and such operations would result in loss of information (Shen et al., 2012).", "startOffset": 181, "endOffset": 200}, {"referenceID": 2, "context": "This normalisation has been used in computer vision tasks (Jegou et al., 2012).", "startOffset": 58, "endOffset": 78}, {"referenceID": 1, "context": "For learning the base models we used the Liblinear library which can support linear models learning in high dimensional datasets (Fan et al., 2008).", "startOffset": 129, "endOffset": 147}, {"referenceID": 3, "context": "We also experimented with text embeddings using the word2vec tool (Mikolov et al., 2013); generating text representations in a low dimensional space (200 dimensions) and 3-NN as a category prediction approach improved over using the tf \u2212 idf representation but was still away from performing competitively.", "startOffset": 66, "endOffset": 88}], "year": 2016, "abstractText": "This report describes our participation in the cDiscount 2015 challenge where the goal was to classify product items in a predefined taxonomy of products. Our best submission yielded an accuracy score of 64.20% in the private part of the leaderboard and we were ranked 10th out of 175 participating teams. We followed a text classification approach employing mainly linear models. The final solution was a weighted voting system which combined a variety of trained models.", "creator": "LaTeX with hyperref package"}}}