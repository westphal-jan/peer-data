{"id": "1606.02891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Edinburgh Neural Machine Translation Systems for WMT 16", "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English&lt;-&gt;Czech, English&lt;-&gt;German, English&lt;-&gt;Romanian and English&lt;-&gt;Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems.", "histories": [["v1", "Thu, 9 Jun 2016 10:06:28 GMT  (21kb)", "http://arxiv.org/abs/1606.02891v1", "WMT16 shared task system description"], ["v2", "Mon, 27 Jun 2016 23:02:24 GMT  (21kb)", "http://arxiv.org/abs/1606.02891v2", "WMT16 shared task system description - final version with human evaluation results"]], "COMMENTS": "WMT16 shared task system description", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow", "alexandra birch"], "accepted": false, "id": "1606.02891"}, "pdf": {"name": "1606.02891.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "a.birch@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 891v 1 [cs.C L] 9J un2 016We participated in the WMT 2016 joint message translation task by building neural translation systems for four language pairs, each trained in two directions: English \u2194 Czech, English \u2194 German, English \u2194 Romanian and English \u2194 Russian. Our systems are based on an attentive encoder decoder and use BPE subtext segmentation for open vocabulary translations with fixed vocabulary. We experimented with automatic translations of the monolingual news corpus as additional training data, ubiquitous dropout, and target-bidirectional models. All reported methods show significant improvements, and we see improvements of 4.3-11.2 BLEU over our base systems."}, {"heading": "1 Introduction", "text": "We participated in the WMT 2016 Shared News Translation Task by building neural translation systems for four language pairs: English \u2194 Czech, English \u2194 German, English \u2194 Romanian and English \u2194 Russian. Our systems are based on an attentive encoder decoder (Bahdanau et al., 2015), using BPE subword segmentation for open vocabulary translations with a fixed vocabulary (Sennrich et al., 2016b), experimenting with automatic retranslations of the monolingual news corpus as additional training data (Sennrich et al., 2016a), ubiquitous dropout (Gal, 2015), and targetbidirectional models."}, {"heading": "2 Baseline System", "text": "We use minibatches of size 80, a maximum set length of 50, word embedding of size 500 and hidden layers of size 1024. We cut the gradient standard to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012) by redeploying the training corpus between epochs. We validate the model every 10,000 minibatches via BLEU on a validation set (Latest 2013, Latest 2014 or half of the Newspaper 2016 for EN-RO). We perform early stops for individual models and use the 4 most recently stored models (with models storing all 30,000 minibatches) for the ensemble results. Note: The ensemble result is the result of a varied search for individual components that are not carried out in a resource-saving manner."}, {"heading": "2.1 Byte-pair encoding (BPE)", "text": "To enable translation of the open vocabulary, we segment words using byte-pair encoding (BPE) 3 (Sennrich et al., 2016b). BPE, originally conceived as a compression algorithm (Gage, 1994), is adapted to word segmentation as follows: First, each word in the training vocabulary is presented as a sequence of characters, plus an end-of-word symbol. All characters are added to the symbol vocabulary. Then, the most common symbol pair is identified and all its occurrences are merged, creating a new symbol, added1https: / / github.com / nyu-dl / dl4mt-tutorial 2https: / github.com / emjotde / amunmt 3https: / / / github.com / rsennrich / subword-nmtto the vocabulary."}, {"heading": "3 Experimental Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Synthetic Training Data", "text": "WMT provides participants with large amounts of monolingual data, both in the domain and outside the domain. We use this monolingual data for training, as described in (Sennrich et al., 2016a). Specifically, we take a subset of the available target-side monolingual corpora, translate it automatically to the source page of the respective language pair, and then use this synthetic parallel data for training. For example, for EN \u2192 RO, the translation is done using an RO \u2192 EN system and vice versa. Sennrich et al. (2016a) motivates the use of monolingual data by domain adaptation, reducing revision and better modeling of language fluency. We sample monolingual data from the News Crawl corpora4, which is in the domain, and vice versa."}, {"heading": "3.2 Pervasive Dropout", "text": "To mitigate this, we apply dropouts to all levels of the network, including recurring ones. Previous work failed to apply dropouts from different units at each step. In the case of recurring connections, this has the disadvantage of obstructing the flow of information over long distances, and Pham et al. (2014) propose to apply dropouts only to non-recurring connections. Instead, we follow the approach suggested by Gal (2015) and use the same dropdown mask at each step. Our implementation differs from the recommendations of Gal (2015) in one respect: We also drop random words, but not at a type level. In other words, if a word occurs several times in a sentence, we can omit as many of its occurrences as we like, and not just none or all of them. In our Anglo-Romanian experiments, we drop full words (both on the source and on the target side) with a probability of 0.2 on all other layers."}, {"heading": "3.3 Target-bidirectional Translation", "text": "We found that when decoding, the model would occasionally assign a high probability to words based solely on the target context, ignoring the source sentence. We speculate that this is an instance of the label bias problem (Lafferty et al., 2001). To mitigate this problem, we are experimenting with separate models that generate the target text from right to left (r2l), and evaluating the nbest lists created by the most important (left-wing) models with these r2l models. Since the right-to-left model will see a complementary target context at each step, we expect the averaged probabilities to be more robust. In parallel to our experiments, this idea was published by Liu et al. (2016). We are increasing the size of the n-ranking to 50 for ranking experiments. A possible criticism of the l-r / r-l ranking approach is that we are actually creating the total gain from the addition of two systems in 2016."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 English\u2194German", "text": "Table 2 shows results for English \u2194 German. We observe improvements of 3.4-5.7 BLEU from the training with a mixture of parallel and synthetic data compared to the baseline, which is only trained on parallel data. Using an ensemble of the last 4 checkpoints leads to further improvements (1.3-1.7 BLEU). Our submission system includes a recalculation of the 50 best results of the left-right model with a right-left model - also an ensemble of the last 4 checkpoints - with uniform weights. This results in an improvement of 0.6-1.1 BLEU."}, {"heading": "4.2 English\u2194Czech", "text": "For us, it is important that we are able to achieve our goals."}, {"heading": "4.3 English\u2194Romanian", "text": "The results of our English and Romanian experiments are shown in Table 5. This language pair has the least amount of parallel training data, and we found that the drop-out was very effective and improvements of 4-5 BLEU.5We found that the use of diacritics in the Romanian training (and development) data was inconsistent. Therefore, for Romanian \u2192 English, we removed diacritics from the Romanian source and made improvements of 1.3-1.4 BLEU.Synthetic training data resulted in improvements of 4.1-5.1 BLEU. For English \u2192 Romanian, we found that the best single system outperformed the overall ensemble of the last 4 checkpoints at Dev, and we submitted the best single system as the primary system."}, {"heading": "4.4 English\u2194Russian", "text": "Therefore, we follow the approach described in (Sennrich et al., 2016b) by first mapping the Russian text into Latin letters using ISO-9 transliteration, then learning the BPE operations via the concatenation of English and Latinized Russian training data, and then mapping the BPE operations back into Cyrillic alphabet. We apply the Latin BPE operations to the English data (training data and input), and both the Cyrillic and Latin BPE operations to the Russian data. Translation results are shown in Table 6. As with the other language pairs, we observe strong improvements from synthetic training data (4-4.4 BLEU). Ensembles yield another 1.1-1.7 BLEU."}, {"heading": "5 Shared Task Results", "text": "Table 7 shows a preliminary, automatic ranking of our submission systems to the WMT165We also tested dropout for EN \u2192 DE with 8 million set pairs of training data, but found no improvement after 10 days of training. We speculate that dropout might still be helpful for records of this size with longer training periods and / or larger networks.6The final version of this paper will report official human rankings. For RU \u2192 EN and EN \u2192 RO, the best-placed systems (according to BLEU) use the neural models described in this paper as components (Junczys-Dowmunt et al., 2016; Peter et al., 2016). According to preliminary BLEU rankings, our submission for 7 of 8 translation directions we participated in is the best-placed system or one component thereof."}, {"heading": "6 Conclusion", "text": "We describe Edinburgh's neural machine translation systems for the WMT16 shared news translation task. For all translation genres, we observe major improvements in translation quality through the use of synthetic parallel training data obtained from the back translation of target monolingual data from the domain domain. Pervasive dropouts at all levels were used for English \u2194 Romanian and resulted in significant improvements. For English \u2194 German and English \u2192 Czech, we trained a right-to-left model with reversed target page and found that restoring system output with these reversed models was beneficial. We released the implementation used for the experiments.7 We also released scripts, sample configurations, synthetic training data and trained models.8"}, {"heading": "Acknowledgments", "text": "This project was funded under funding agreements 645452 (QT21), 644333 (TraMOOC) and 644402 (HimL) from the European Union's Horizon 2020 research and innovation programme. 7https: / / github.com / rsennrich / nematus 8https: / / github.com / rsennrich / wmt16-scripts"}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A New Algorithm for Data Compression", "author": ["Philip Gage"], "venue": "C Users J.,", "citeRegEx": "Gage.,? \\Q1994\\E", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. ArXiv e-prints", "author": ["Yarin Gal"], "venue": null, "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based", "author": ["Tomasz Dwojak", "Rico Sennrich"], "venue": null, "citeRegEx": "JunczysDowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "JunczysDowmunt et al\\.", "year": 2016}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Ma-", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Agreement on Target-bidirectional Neural Machine Translation", "author": ["Liu et al.2016] Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "venue": "NAACL HLT 16,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "The QT21/HimL Combined Machine Translation System", "author": ["dre Allauzen", "Lauriane Aufrant", "Franck Burlot", "Elena Knyazeva", "Thomas Lavergne", "Fran\u00e7ois Yvon", "Joachim Daiber", "Marcis Pinnis"], "venue": "In Proceedings of the First Conference on Ma-", "citeRegEx": "Allauzen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2016}, {"title": "Dropout Improves Recurrent Neural Networks for Handwriting Recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In 14th International Conference on Frontiers in Handwriting Recogni-", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al.", "startOffset": 56, "endOffset": 79}, {"referenceID": 2, "context": ", 2016a), pervasive dropout (Gal, 2015), and targetbidirectional models.", "startOffset": 28, "endOffset": 39}, {"referenceID": 0, "context": "Our systems are attentional encoder-decoder networks (Bahdanau et al., 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.", "startOffset": 34, "endOffset": 48}, {"referenceID": 1, "context": "BPE, originally devised as a compression algorithm (Gage, 1994), is adapted to word segmentation as follows:", "startOffset": 51, "endOffset": 63}, {"referenceID": 8, "context": "When applied to recurrent connections, this has the downside that it impedes the information flow over long distances, and Pham et al. (2014) propose to only apply dropout to non-recurrent connections.", "startOffset": 123, "endOffset": 142}, {"referenceID": 2, "context": "Instead, we follow the approach suggested by Gal (2015), and use the same dropout mask at each time step.", "startOffset": 45, "endOffset": 56}, {"referenceID": 2, "context": "Instead, we follow the approach suggested by Gal (2015), and use the same dropout mask at each time step. Our implementation differs from the recommendations by Gal (2015) in one respect: we also drop words at random, but we do so on a token level, not on a type level.", "startOffset": 45, "endOffset": 172}, {"referenceID": 4, "context": "We speculate that this is an instance of the label bias problem (Lafferty et al., 2001).", "startOffset": 64, "endOffset": 87}, {"referenceID": 5, "context": "In parallel to our experiments, this idea was published by Liu et al. (2016).", "startOffset": 59, "endOffset": 77}, {"referenceID": 5, "context": "However experiments in (Liu et al., 2016) show that a l-r/r-l reranking systems is stronger than an ensemble created from two independent l-r runs.", "startOffset": 23, "endOffset": 41}], "year": 2016, "abstractText": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English\u2194Czech, English\u2194German, English\u2194Romanian and English\u2194Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3\u201311.2 BLEU over our baseline systems.", "creator": "LaTeX with hyperref package"}}}