{"id": "1606.02892", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Linguistic Input Features Improve Neural Machine Translation", "abstract": "Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English&lt;-&gt;German neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3.", "histories": [["v1", "Thu, 9 Jun 2016 10:12:36 GMT  (32kb)", "https://arxiv.org/abs/1606.02892v1", "accepted to WMT16"], ["v2", "Mon, 27 Jun 2016 23:11:51 GMT  (32kb)", "http://arxiv.org/abs/1606.02892v2", "WMT16 final version; new EN-RO results"]], "COMMENTS": "accepted to WMT16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow"], "accepted": false, "id": "1606.02892"}, "pdf": {"name": "1606.02892.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 892v 2 [cs.C L] 27 Jun 2016"}, {"heading": "1 Introduction", "text": "This year, it will be ready to leave the country in order to enter the EU."}, {"heading": "2 Neural Machine Translation", "text": "We follow the architecture of neural machine translation by Bahdanau et al. (2015), which we briefly summarize here. Neural machine translation system is implemented as an attentive encoder decoder network with recursive neural networks.The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1,..., xm) and calculates a forward sequence of hidden states (\u2212 \u2192 h 1, \u2212 h m) and a backward sequence (\u2190 \u2212 h 1,..., \u2190 \u2212 hm).The hidden states \u2212 \u2192 h j and \u2190 \u2212 h j are linked to obtain the annotation vector hj. The decoder is a recursive neural network that predicts a target sequence y = (y1,..., yn).Each word yi is found on the basis of a recursive network, predicting the precursive allocated word \u2212 the precursive precursive alli is a precursor, where the precursor is a precursor word \u2212 the precursor."}, {"heading": "2.1 Adding Input Features", "text": "Our most important innovation over the standard encoder architecture is that we present the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006). Here we show the equation for the forward states of the encoder (for the simple RNN case; consider (Bahdanau et al., 2015) for GRU): \u2212 \u2192 h j = tanh (\u2212 \u2192 WExj + \u2212 \u2192 U \u2212 h j \u2212 1) (1), where E-Rm \u00b7 Kx is a word embedding the matrix, \u2212 \u2192 W-Rn \u00b7 m, \u2212 \u2192 U-Rn \u00b7 n are weight matrices where m and n are the word embedding size or number of hidden units, and Kx is the vocabulary size of the source language. We generalize this to any number of features | F |: \u2192 h j = tanh (| F | nk = 1Ekkimk) + j \u2212 h (1) where the two are linked."}, {"heading": "3 Linguistic Input Features", "text": "Our generalized model of the previous section supports any number of input functions. 4https: / / github.com / nyu-dl / dl4mt-tutorialIn this article, we focus on a number of well-known linguistic features. Our main empirical question is whether providing linguistic features to the encoder improves the translation quality of neural machine translation systems or whether the information comes from training encoder models for raw text, making their inclusion via explicit features superfluous. All linguistic features are automatically predicted; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to comment on the English inputs \u2192 German and ParZu (Sennrich et al., 2013) to comment on the German inputs for German \u2192 English. Here we discuss the individual features in detail."}, {"heading": "3.1 Lemma", "text": "In principle, neural models can learn that inflationary variants are semantically related and represent them as similar points in continuous vector space (Mikolov et al., 2013). Although this has been shown for high-frequency words, we expect lemmatized representation to increase data efficiency; low-frequency variants may even be unknown to dictionary-level models. In character or subword-level models, it is unclear to what extent they can learn the similarity between low-frequency word forms that have a problem in common, especially when the word forms are superficially dissimilar. Consider the following two German word forms that have the problem \"lie\" in common: \u2022 lies \"(3.p.sg. available) \u2022 lies\" lay \"(3.p.sg. subjunctive II) The lemmatisers we use are based on finite state methods that ensure a wide range even for infrequent word forms in German (We\" lay, \"Sennal,\" Satizer, \"2004, Satizer, et)."}, {"heading": "3.2 Subword Tags", "text": "In our experiments, we operate at the subword level to achieve an open vocabulary translation with a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous and can be either a separate word or a subword segment of a larger word. In addition, text is presented as a sequence of subwords without explicit word boundaries, but word boundaries are potentially helpful for learning which symbols to pay attention to and when to forget information in the recurring layers. We propose an annotation of the subword structure that resembles the common IOB format for chunking and entity recognition, and mark whether a symbol in the text forms the beginning (B), within (I) or the end (E) of a word."}, {"heading": "3.3 Morphological Features", "text": "For German \u2192 English, the parser comments on the German input with morphological characteristics. Different word types have different characteristics - nouns, for example, have uppercase letters, number and gender, while verbs have person, number, tense and aspect - and characteristics can be under-specified. We treat the concatenation of all morphological characteristics of a word as a string, using a special symbol for underspecified characteristics, and treat each such string as a separate attribute value."}, {"heading": "3.4 POS Tags and Dependency Labels", "text": "In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with a POS tag and a dependency label, the latter being the edge label that connects a word to its syntactic head, or \"ROOT,\" if the word does not have a syntactic head."}, {"heading": "3.5 On Using Word-level Features in a Subword Model", "text": "We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units and do not require any further modification. All other attributes are originally attributes at word level. To assign attributes to the segmented source code, we copy the attribute value of the word into all subword units. An example is shown in Figure 1."}, {"heading": "4 Evaluation", "text": "We have made it our mission to open up a wide range of topics that move us, \"he told the\" World. \"\" We haven't made it easy, \"he said,\" but we've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" We've made it. \"\" It's. \"\" It. \"\" \"It.\" \"\" It. \"\" It. \"\" It. \"\" It. \"\" \"It.\" \"It.\" \"It.\" \"\" It. \"\" \"It.\" \"It.\" \"\" It. \"\" \"It.\" \"\" It. \"\" It. \"\" \"\" It. \""}, {"heading": "4.1 Results", "text": "In fact, it is such that we are able to put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we are able to live in another world, in which we are in another world, in which we live in another world, in which we live in another world, in which we are able to live in another world, in which we are able to live in another world, in which we live in which we live in another world, in which we live in which we live in another world, in which we live in which we live in which we live in which we live, in which we live in which we live ourselves, in which we live in which we live ourselves, in which we live in which we live, in which we live ourselves, in which we live in which we live, in which we live in which we live, in which we live, in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live, in which we live, in which we live which we live in which we"}, {"heading": "5 Related Work", "text": "Linguistic characteristics have been used in neural language modeling (Alexandrescu and Kirchhoff, 2006) and are also used in other tasks recently using neural models, such as syntactic parsing (Chen and Manning, 2014).This paper addresses the question of whether source-side linguistic features are advantageous for neural machine translation. On the target side, linguistic characteristics are more difficult to obtain for a generational task such as machine translation, since this would require incremental parsing of hypotheses at the time of testing, and will be possible in the future.Among other things, our model incorporates information from dependency annotation, but is still a sequence tactical model. Eriguchi et al. (2016) suggest a tree-to-sequence model whose encoder creates vector representations for each phrase in the source tree, with the binding ability of the (unlabeled) structure of a syntactical annotation often relying on us during the discourse."}, {"heading": "6 Conclusion", "text": "In this paper, we investigate whether linguistic input functions are beneficial for neural machine translation, and our empirical evidence suggests that this is the case. We describe a generalization of the encoder in the popular encoder decoder architecture for neural machine translation, which allows the inclusion of any number of input functions. We empirically test the inclusion of various linguistic features, including lemmats, part-of-speech tags, syntactic dependency designations and morphological features in English \u2194 German and English \u2192 Romanian neural MT systems. Our experiments show that the linguistic features bring improvements over our baseline, resulting in improvements over newest testing 2016 of 1.5 BLEU for German, 0.6 BLEU for English \u2192 German, and 1.0 BLEU for English \u2192 Romanian. In the future, we expect several developments that will shed more light on the usefulness of linguistic input functions (or other input functions) than those that we might shed for central machine translation itself."}, {"heading": "Acknowledgments", "text": "This project was funded by the European Union's Horizon 2020 research and innovation programme under funding agreements 645452 (QT21) and 644402 (HimL)."}], "references": [{"title": "Factored Neural Language Models", "author": ["Alexandrescu", "Katrin Kirchhoff"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Alexandrescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu et al\\.", "year": 2006}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kazuma Hashimoto", "author": ["Akiko Eriguchi"], "venue": "and Yoshimasa Tsuruoka.", "citeRegEx": "Eriguchi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Holger Schwenk", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u00efc Barrault", "HueiChi Lin", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "G\u00fcl\u00e7ehre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Factored Translation Models", "author": ["Koehn", "Hoang2007] Philipp Koehn", "Hieu Hoang"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Wen-tau Yih", "author": ["Tomas Mikolov"], "venue": "and Geoffrey Zweig.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Carroll", "author": ["Guido Minnen", "John A"], "venue": "and Darren Pearce.", "citeRegEx": "Minnen et al.2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The QT21/HimL Combined Machine Translation System", "author": ["Allauzen", "Lauriane Aufrant", "Franck Burlot", "Elena Knyazeva", "Thomas Lavergne", "Fran\u00e7ois Yvon", "Marcis Pinnis"], "venue": "In Proceedings of the First Conference on Machine Translation (WMT16),", "citeRegEx": "Allauzen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2016}, {"title": "chrF: character ngram F-score for automatic MT evaluation", "author": ["Maja Popovi\u0107"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Popovi\u0107.,? \\Q2015\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "On Some Pitfalls in Automatic Evaluation and Significance Testing for MT", "author": ["Riezler", "Maxwell2005] Stefan Riezler", "John T. Maxwell"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-", "citeRegEx": "Riezler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2005}, {"title": "Arne Fitschen", "author": ["Helmut Schmid"], "venue": "and Ulrich Heid.", "citeRegEx": "Schmid et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Zmorge: A German Morphological Lexicon Extracted from Wiktionary", "author": ["Sennrich", "Kunz2014] Rico Sennrich", "Beat Kunz"], "venue": "In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Sennrich et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2014}, {"title": "Martin Volk", "author": ["Rico Sennrich"], "venue": "and Gerold Schneider.", "citeRegEx": "Sennrich et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "2016a. Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Philipp Koehn", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran"], "venue": "and Ond\u0159ej Bojar.", "citeRegEx": "Stanojevi\u0107 et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Manning", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D"], "venue": "and Yoram Singer.", "citeRegEx": "Toutanova et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Barry Haddow", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck"], "venue": "and Ond\u0159ej Bojar.", "citeRegEx": "Williams et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "2016", "author": ["Barret Zoph", "Kevin Knight"], "venue": "Multi-Source Neural Translation. In NAACL HLT", "citeRegEx": "Zoph and Knight2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder\u2013decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to English\u2194German and English\u2192Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available1 , as are sample files and configurations2 .", "creator": "LaTeX with hyperref package"}}}