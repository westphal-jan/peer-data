{"id": "1606.03002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "MuFuRU: The Multi-Function Recurrent Unit", "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.", "histories": [["v1", "Thu, 9 Jun 2016 15:41:17 GMT  (38kb,D)", "http://arxiv.org/abs/1606.03002v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL", "authors": ["dirk weissenborn", "tim rockt\\\"aschel"], "accepted": false, "id": "1606.03002"}, "pdf": {"name": "1606.03002.pdf", "metadata": {"source": "CRF", "title": "MuFuRU: The Multi-Function Recurrent Unit", "authors": ["Dirk Weissenborn", "Tim Rockt\u00e4schel"], "emails": ["dirk.weissenborn@dfki.de", "t.rocktaschel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Impressive results have been achieved for tasks related to text, such as modeling the language [12], translating into the foreign language [17], or answering questions into the foreign language [16]. At the core of each RNN is a recursive cell - a function that states that the current input factors and the previous state must be combined to form a new state."}, {"heading": "2 Recurrent Neural Networks", "text": "A recursive neural network is fully specified by a recursive function f\u03b8: RN \u00b7 RM \u2192 RM \u00b7 RH parameterized by \u03b8, where M is the size of the state vector, H is the size of the output vector, and N is the size of the input vector. In view of an input sequence X = (x1,..., xT) and a start state s0, the state and output at the time of the input vector is calculated as (st, ht) = f (xt, st \u2212 1). Finally, an RNN is defined as the recursive application of the cell function f to inputs and previous states from time step 1 to T. \u2212 Vanilla RNN The simplest cell function is the tanh cell, in which the new state is calculated at any time by a nonlinear projection of the current input function f on inputs and previous states."}, {"heading": "3 Multi-Function Recurrent Unit", "text": "The Multi-Function Recurrent Unit (MuFuRU) applies predefined composition operations (such as elemental max, min, absolute difference, etc.) to a new feature vector and the previous state and decides which composition should be used separately for each feature dimension."}, {"heading": "3.1 Architecture", "text": "For each step t, the MuFuRU calculates normalized weights pjt for all composition operations opj: RM \u00b7 RM \u2192 RM, j [1, l], via an operation controller kt (Eq.2).p \u0441jt = W j pkt + b j p [p1t,..., p l t] = Softmax ([p \u04411t,..., p \u0441l t]) (2) In this work, the operation controller is the concatenation of st \u2212 1 and the current input text (Eq.3).1kt = [xt st \u2212 1] (3) The MuFuRU (like the GRU) uses a reset to calculate the new function vector vt and combines it with the previous hidden state by a convex combination of the l different composition operations opj (Eq.4).rt = \u03c3 (Wr [xt st \u2212 1] + br) vt tanh = Wv \u00b7 jxt = 1 st = 1 st = 1 st = 1 st 1 st = 1 st."}, {"heading": "3.2 Composition Functions", "text": "In Table 1 we list the composition functions used in this work. Note that this list can easily be expanded to include other differentiated functions suitable for a specific task. Input for each operation is the previous state s and a feature vector v."}, {"heading": "3.3 Relations to existing Architectures", "text": "The MuFuRU is a generalization of existing architectures. Therefore, the MuFuRU becomes the same or similar to existing architectures with minor adjustments and a correct choice of composition functions. Vanilla RNN The MuFuRU becomes a standard tanh cell if only reset operation is allowed and the reset gate is set to 1.GRU. A GRU is a MuFuRU where only the club and replacement operation is allowed. It follows that the MuFuRU should in principle be able to perform sequence modeling tasks at least as well as these architectures, since it can learn how to work them."}, {"heading": "4 Experiments", "text": "In the following experiments, we compare performance with the GRU as a starting point. Each model is trained with the same task-specific hyperparameters to ensure comparability. Although the MuFuRU contains a larger set of parameters than a GRU via the operation controller, we believe that this does not affect comparability, since the additional parameters relate only to the selection of a single process in each time step and our goal is to investigate whether the introduction of new operations is advantageous or not. In addition, all models easily fit into most of our experiments, giving models with better generating capability an advantage. We perform a stochastic gradient descent in the minibatch using ADAM [11] with \u03b21 = 0.0 (no impulse) and \u03b22 = 0.999 for optimization in all experiments."}, {"heading": "4.1 Propositional Logic", "text": "For example, [1, 0, 0, 0, 1, \u21d2] represents the Boolean expression ((1, 0, 0, 0, 0, 0) \u21d2 1. We train the GRU and MuFuRU on 1000 Boolean formulas with 5-10 gates and test on 1000 invisible longer formulas with 11-20 gates. Figure 1a shows the test accuracy of a GRU and MuFuRU with different hidden dimensions trained for 100 epochs. While the GRU struggles to generalize to longer sequences, the MuFuRU learns to evaluate Boolean formulas with a memory size of only 8. The GRU can emulate the operations necessary to evaluate Boolean gates when they are provided with a much larger hidden dimension."}, {"heading": "4.2 Language Modeling", "text": "For this experiment we used the PTB data set [12] with a limited vocabulary of 10k words. We trained single-layer models with 200 hidden units. The GRU with a test set perplexity of 123.0 was surpassed by the MuFuRU with a perplexity of 119.7. This result supports our claim that the MuFuRU should model sequences at least as well as the GRU."}, {"heading": "4.3 Sentiment Analysis", "text": "In this experiment, we trained our models using the Sentiment Treebank [15], which contains 215,154 annotated phrases from 11,855 sentences. We trained the models with 100 hidden units, using mini-batches of 25 phrases each, and fed the final output vector at the end of each phrase as input into a logistical regression classifier. Word embedding is matched and initialized using Glove [13] or uniformly scanned between -0.05 and 0.05 for unknown words. We selected the model with the best accuracy on the development set in each run, which evaluated all 200 minibatches. The results of our experiments together with state-of-the-art technology are presented in Figure 1c. Both the GRU and the MuFuRU achieve high accuracy. The MuFuRU performs better than the GRU, suggesting that the introduction of these new methods into the complex operations of the NU can be better even without the structural processes of the RU structures."}, {"heading": "5 Conclusion", "text": "We introduced the Multi-Function Recurrent Unit (MuFuRU), a new recurrent neural network architecture that learns to select composition functions for combining computed features with an existing state at each step, generalizing over existing models that are limited to a very small set of such compositions. We demonstrate their theoretical advantages over a toy task that evaluates simple statement formulas and provides empirical evidence for a voice modeling task that additional compositional functions are useful. Since MuFuRUs are able in principle to learn the same or similar behavior to GRUs or LSTMs, they can be used instead of these cell functions in RNNNs. Furthermore, other task-specific, differentiated composition functions can be easily integrated."}, {"heading": "Acknowledgments", "text": "This research was partly supported by Microsoft Research as part of its doctoral scholarship programme and by the German Federal Ministry of Education and Research (BMBF) in the projects ALL SIDES (01IW14002), BBDC (01IS14013E) and Software Campus (01IS12050, subproject GeNIE)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["P. Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "In ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Kout\u0144\u0131k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In EMNLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTER- SPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "In STARSEM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 179, "endOffset": 182}, {"referenceID": 13, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 101, "endOffset": 108}, {"referenceID": 15, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 101, "endOffset": 108}, {"referenceID": 7, "context": "Different cell-functions have been proposed in the past, including the traditional tanh-cell (Vanilla), the Long-Short-TermMemory (LSTM [8]) or more recently the Gated Recurrent Unit (GRU [3]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Different cell-functions have been proposed in the past, including the traditional tanh-cell (Vanilla), the Long-Short-TermMemory (LSTM [8]) or more recently the Gated Recurrent Unit (GRU [3]).", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "Though different extensions and variations to GRUs and LSTMs have been investigated recently [6, 9], none of them outperform standard GRUs or LSTMs significantly on a range of different tasks.", "startOffset": 93, "endOffset": 99}, {"referenceID": 8, "context": "Though different extensions and variations to GRUs and LSTMs have been investigated recently [6, 9], none of them outperform standard GRUs or LSTMs significantly on a range of different tasks.", "startOffset": 93, "endOffset": 99}, {"referenceID": 10, "context": "We perform mini-batch stochastic gradient descent using ADAM [11] with \u03b21 = 0.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "See [4] for a description of the gates.", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "RNTN [15] 85.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "4 DCNN [2] 86.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "8 CNN-MC [10] 88.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "1 CT-LSTM [18] 88.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "0 LSTM [18] 84.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "For this experiment we use the PTB dataset [12] with a limited vocabulary of 10k words.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "we train our models on the Sentiment Treebank [15], which contains 215,154 annotated phrases collected from 11,855 sentences.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Word embeddings are tuned and initialized with Glove [13] or sampled uniformly between -0.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-ofthe-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an inputand state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.", "creator": "LaTeX with hyperref package"}}}