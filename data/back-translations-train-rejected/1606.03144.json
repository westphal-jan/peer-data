{"id": "1606.03144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical Relevance in Learner Essays", "abstract": "We investigate the task of assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines.", "histories": [["v1", "Thu, 9 Jun 2016 23:42:45 GMT  (22kb,D)", "http://arxiv.org/abs/1606.03144v1", "Accepted for publication at BEA-2016"]], "COMMENTS": "Accepted for publication at BEA-2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei", "ronan cummins"], "accepted": false, "id": "1606.03144"}, "pdf": {"name": "1606.03144.pdf", "metadata": {"source": "CRF", "title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical Relevance in Learner Essays", "authors": ["Marek Rei", "Ronan Cummins"], "emails": ["marek.rei@cl.cam.ac.uk", "ronan.cummins@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Relevance Scoring Methods", "text": "The systems receive the prompt and a single sentence as input and strive to generate a score that represents the current relevance of the sentence, with a higher value being relevant as a result of greater confidence in the relevance of the sentence. In most of the following methods, both the sentence and the prompt are mapped into vector representations and the cosine is used to measure their similarity."}, {"heading": "2.1 Baseline methods", "text": "The simplest baseline we use is a random system where the score between each record and each prompt is randomly assigned. In addition, we evaluate the baseline of the majority class, where the highest score is always assigned to the prompt in the record that contains the most records associated with it. It is important that any technical system outperforms the performance of these trivial baseline."}, {"heading": "2.2 TF-IDF", "text": "TF-IDF (Spa \ufffd rck Jones, 1972) is an established method of constructing document vectors for obtaining information. The weight of each word results from the multiplication of its term frequency and inverse document frequency (IDF). We adapt IDF to the sentence similarity by using the following formula: IDF (w) = log (N1 + nw), where N is the total number of sentences in a corpus and nw the number of sentences in which the target word w occurs. Intuitively, very common words such as determinators and prepositions are assigned low weights and rare words are assigned higher weights. To obtain reliable sentence frequency counts, we use the British National Corpus (BNC, Burnard (2007)), which contains 100 million words of English from various sources."}, {"heading": "2.3 Word2Vec", "text": "Word2Vec (Mikolov et al., 2013) is a useful tool for efficiently learning distributed vector representations of words from a large body of pure text. We use the CBOW variant, which maps each word to a vector space and uses the vectors of the surrounding words to predict the target word. As a result, words that often occur in similar contexts also have more similar vectors. To create a vector for a sentence or document, each word in the document is assigned to a corresponding vector, and these vectors are then grouped together. While the TF-IDF vectors are sparse and essentially measure a weighted word between the input prompt and the sentence, Word2vectors are able to capture the semantics of similar words without requiring perfect matches. In the experiments, we use the publicly available vectors of 100 billion words, containing millions of message text vectors, and the 300 unique message text dimension.1"}, {"heading": "2.4 IDF-Embeddings", "text": "We are experimenting with the combination of the advantages of Word2Vec and TF-IDF. While Word2Vec vectors can better understand the generalized meaning of each word, their summary assigns the same weight to all words. This is not ideal for our task - function words, for example, are likely to have less of an impact on the timeliness than more specific rare words. We assume that the weighting of all word vectors during addition can better reflect the contribution of certain words. To achieve this, we scale each word vector by the corresponding weight of the IDF for that word according to the formula in Section 2.2. This still maps the sentence to a distributed semantic vector, but more frequent words have less of an impact on the result."}, {"heading": "2.5 Skip-Thoughts", "text": "Skip-Thoughts (Kiros et al., 2015) is a more advanced neural network model for learning distributed sentence representations. A single sentence 1https: / / code.google.com / archive / p / word2vec / is first assigned to a vector by using a gated recurrent unit (Cho et al., 2014) that learns a composition function to assign individual word embeddings to a single sentence representation, and the resulting vector is used as input for a decoder that attempts to predict words in the previous and next sentence. The model is trained as a single network, and the GRU encoder learns to assign each sentence to a vector that is useful for predicting the content of surrounding sentences. We use the publicly available pre-schooled model2 to generate sentence vectors based on 985 million words of unpublished literature from the BookCorpu Zhal (2015)."}, {"heading": "2.6 Weighted-Embeddings", "text": "We have the idea that words must be weighted differently to scale their vector."}, {"heading": "3 Evaluation", "text": "Since there is no publicly available data set that contains manually annotated relevance values at the record level, we measure the accuracy of the methods used to determine the original prompt used to generate each sentence in a study essay. While3http: / / www.marekrei.com / projects / weighted-embeddingsnot all sentences in an essay are intended to directly convey the prompt, every sound in the data set disadvantages all systems equally, and the ability to assign a higher score to the correct command prompt directly reflects the model's ability to capture current relevance. Two separate publicly available corpora of essays written by upper-middle-level learners were used for the evaluation. The first certificate in the English dataset (FCE, Yannakoudakis et al. (2011)), consisting of different publicly available corpora of learners written in response to 60 input sentences, included in the English dataset (IC8of Learner), contains a large set of data sets (IC8of Learner)."}, {"heading": "4 Results", "text": "The results for all systems can be seen in Table 1. TF-IDF achieves good results and the best performance in FCE essays. The instructions in this dataset are long and detailed, with specific keywords and names that are expected to be used in the essay, which is why this method for measuring word overlaps achieves the highest accuracy. In contrast, the TF-IDF method is achieved on the ICLE dataset with more general and open statements. We used the same ICLE subset as Persing and Ng (2014).mid-level performance and is outranking by several embedding-based methods.Word2Vec is designed to capture more general word semantics, rather than to identify specific tokens, and achieves better performance on the ICLE dataset."}, {"heading": "5 Discussion", "text": "In Table 2, we see some examples of learning sentences from the ICLE dataset, along with results from the system of weighted embeddings; the method manages to capture an intuitive relevance assessment for all three sentences, even though none of them contain meaningful keywords from the prompt; the second sentence receives a slightly lower score than the first because it introduces a somewhat tangential government topic; the third sentence is rated very low because it does not contain specific information for the prompt; automatic rating systems based solely on the recognition of grammatical errors would likely assign similar scores to all of them; the method maps sentences into the same vector space as individual words, so we are also able to display the most relevant words for each command prompt, which is useful as a writing guide for low-level learners.Table 3 contains words with the highest and lowest weights, which can be weighted by paragraphs, as well as the paragraphs we have learned during the training, as the highest paragraph, and the paragraphs we can assign independently."}, {"heading": "6 Conclusion", "text": "In this paper, we explored the task of assessing the relevance of text blocks in study essays. Frameworks for assessing the topic of individual sentences would be useful for recording inappropriate topic shifts in writing, giving students more detailed feedback, and detecting subversion attacks on automated assessment systems. We found that measuring word overlaps, weighted by TF-IDF, is the best option when the text blocks contain many details that the student is likely to include. However, if the inputs are relatively short and designed to stimulate discussion, which is common in higher level tests, then measuring vector similarity by word embedding consistently leads to better results. We expanded the well-known Word2Vec embeddings by weighting them with IDF, which resulted in improvements in sentence representation. On this basis, we constructed the model of weighted embeddings for automatic learning of single data weights using only one method, and using the 2DF as the subordinated ones."}], "references": [{"title": "Developing and testing a self-assessment and tutoring system", "author": ["\u00d8istein E. Andersen", "Helen Yannakoudakis", "Fiona Barker", "Tim Parish."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Andersen et al\\.,? 2013", "shortCiteRegEx": "Andersen et al\\.", "year": 2013}, {"title": "Automated Assessment of ESOL Free Text Examinations", "author": ["Ted Briscoe", "Ben Medlock", "\u00d8istein Andersen."], "venue": "Technical report.", "citeRegEx": "Briscoe et al\\.,? 2010", "shortCiteRegEx": "Briscoe et al\\.", "year": 2010}, {"title": "Reference Guide for the British National Corpus (XML Edition)", "author": ["Lou Burnard."], "venue": "Technical report.", "citeRegEx": "Burnard.,? 2007", "shortCiteRegEx": "Burnard.", "year": 2007}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Conference on Empirical", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "International Corpus of Learner English v2", "author": ["Sylviane Granger", "Estelle Dagneaux", "Fanny Meunier", "Magali Paquot."], "venue": "Technical report.", "citeRegEx": "Granger et al\\.,? 2009", "shortCiteRegEx": "Granger et al\\.", "year": 2009}, {"title": "Evaluating Multiple Aspects of Coherence in Student Essays", "author": ["Derrick Higgins", "Jill Burstein", "Daniel Marcu", "Claudia Gentile."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Higgins et al\\.,? 2004", "shortCiteRegEx": "Higgins et al\\.", "year": 2004}, {"title": "Identifying Off-topic Student Essays Without Topicspecific Training Data", "author": ["Derrick Higgins", "Jill Burstein", "Yigal Attali."], "venue": "Natural Language Engineering, 12.", "citeRegEx": "Higgins et al\\.,? 2006", "shortCiteRegEx": "Higgins et al\\.", "year": 2006}, {"title": "Learning Distributed Representations of Sentences from Unlabelled Data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015).", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Off-topic essay detection using short prompt texts", "author": ["Annie Louis", "Derrick Higgins."], "venue": "NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Louis and Higgins.,? 2010", "shortCiteRegEx": "Louis and Higgins.", "year": 2010}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tom\u00e1\u0161 Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Modeling prompt adherence in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "52nd Annual Meeting", "citeRegEx": "Persing and Ng.,? 2014", "shortCiteRegEx": "Persing and Ng.", "year": 2014}, {"title": "A Statistical Interpretation of Term Specificity and its Retrieval", "author": ["Karen Sp\u00e4rck Jones."], "venue": "Journal of Documentation, 28.", "citeRegEx": "Jones.,? 1972", "shortCiteRegEx": "Jones.", "year": 1972}, {"title": "Overview of the Eighth Text REtrieval Conference (TREC-8)", "author": ["Ellen M. Voorhees", "Donna Harman."], "venue": "Text REtrieval Conference (TREC-8).", "citeRegEx": "Voorhees and Harman.,? 1999", "shortCiteRegEx": "Voorhees and Harman.", "year": 1999}, {"title": "A New Dataset and Method for Automatically Grading ESOL Texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}, {"title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "arXiv preprint.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Evaluating the relevance of learner essays with respect to the assigned prompt is an important part of automated writing assessment (Higgins et al., 2006; Briscoe et al., 2010).", "startOffset": 132, "endOffset": 176}, {"referenceID": 1, "context": "Evaluating the relevance of learner essays with respect to the assigned prompt is an important part of automated writing assessment (Higgins et al., 2006; Briscoe et al., 2010).", "startOffset": 132, "endOffset": 176}, {"referenceID": 1, "context": ", 2006; Briscoe et al., 2010). Students with limited relevant vocabulary may attempt to shift the topic of the essay in a more familiar direction, which grammatical error detection systems are not able to capture. In an automated examination framework, this weakness could be further exploited by memorising a grammatically correct essay and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. Most existing work on assigning topical relevance scores has been done using supervised methods. Persing and Ng (2014) trained a linear regression model for detecting relevance to each prompt, but this approach requires substantial training data for all the possible prompts.", "startOffset": 8, "endOffset": 696}, {"referenceID": 1, "context": ", 2006; Briscoe et al., 2010). Students with limited relevant vocabulary may attempt to shift the topic of the essay in a more familiar direction, which grammatical error detection systems are not able to capture. In an automated examination framework, this weakness could be further exploited by memorising a grammatically correct essay and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. Most existing work on assigning topical relevance scores has been done using supervised methods. Persing and Ng (2014) trained a linear regression model for detecting relevance to each prompt, but this approach requires substantial training data for all the possible prompts. Higgins et al. (2006) addressed off-topic detection by measuring the cosine similarity between tf-idf vector representations of the prompt and the entire essay.", "startOffset": 8, "endOffset": 875}, {"referenceID": 1, "context": ", 2006; Briscoe et al., 2010). Students with limited relevant vocabulary may attempt to shift the topic of the essay in a more familiar direction, which grammatical error detection systems are not able to capture. In an automated examination framework, this weakness could be further exploited by memorising a grammatically correct essay and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. Most existing work on assigning topical relevance scores has been done using supervised methods. Persing and Ng (2014) trained a linear regression model for detecting relevance to each prompt, but this approach requires substantial training data for all the possible prompts. Higgins et al. (2006) addressed off-topic detection by measuring the cosine similarity between tf-idf vector representations of the prompt and the entire essay. However, as this method only captures similarity using exact matching at the word-level, it can miss many topically relevant word occurrences in the essay. In order to overcome this limitation, Louis and Higgins (2010) investigated a number of methods that expand the prompt with related words, such as morphological variations.", "startOffset": 8, "endOffset": 1233}, {"referenceID": 0, "context": "Sentencebased relevance scores could also be used for estimating coherence in an essay, or be combined with a more general score for indicating sentence quality (Andersen et al., 2013).", "startOffset": 161, "endOffset": 184}, {"referenceID": 4, "context": "Higgins et al. (2004) used a supervised SVM classifier to train a binary sentence-based relevance model with 18 sentencelevel features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "In order to obtain reliable sentence-level frequency counts, we use the British National Corpus (BNC, Burnard (2007)) which contains 100 million words of English from various sources.", "startOffset": 102, "endOffset": 117}, {"referenceID": 10, "context": "Word2Vec (Mikolov et al., 2013) is a useful tool for efficiently learning distributed vector representations of words from a large corpus of plain text.", "startOffset": 9, "endOffset": 31}, {"referenceID": 8, "context": "Skip-Thoughts (Kiros et al., 2015) is a more advanced neural network model for learning distributed sentence representations.", "startOffset": 14, "endOffset": 34}, {"referenceID": 3, "context": "is first mapped to a vector by applying a Gated Recurrent Unit (Cho et al., 2014), which learns a composition function for mapping individual word embeddings to a single sentence representation.", "startOffset": 63, "endOffset": 81}, {"referenceID": 15, "context": "We make use of the publicly available pretrained model2 for generating sentence vectors, which is trained on 985 million words of unpublished literature from the BookCorpus (Zhu et al., 2015).", "startOffset": 173, "endOffset": 191}, {"referenceID": 13, "context": "The First Certificate in English dataset (FCE, Yannakoudakis et al. (2011)), consisting of 30,899 sentences written in response to 60 prompts; and the International Copus of Learner English dataset (ICLE, Granger et al.", "startOffset": 47, "endOffset": 75}, {"referenceID": 4, "context": "(2011)), consisting of 30,899 sentences written in response to 60 prompts; and the International Copus of Learner English dataset (ICLE, Granger et al. (2009)) containing 20,883 sentences, written in response to 13 prompts.", "startOffset": 137, "endOffset": 159}, {"referenceID": 13, "context": "Performance is evaluated through classification accuracy and mean reciprocal rank (Voorhees and Harman, 1999).", "startOffset": 82, "endOffset": 109}, {"referenceID": 11, "context": "We used the same ICLE subset as Persing and Ng (2014). FCE ICLE", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "Our results are consistent with those of Hill et al. (2016), who found that SkipThoughts performed very well when the vectors were used as features in a separate supervised classifier, but gave low results when used for unsupervised similarity tasks.", "startOffset": 41, "endOffset": 60}], "year": 2016, "abstractText": "We investigate the task of assessing sentencelevel prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentencelevel similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines.", "creator": "TeX"}}}