{"id": "1606.03152", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Policy Networks with Two-Stage Training for Dialogue Systems", "abstract": "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.", "histories": [["v1", "Fri, 10 Jun 2016 01:02:19 GMT  (53kb,D)", "http://arxiv.org/abs/1606.03152v1", "Submitted to SIGDial -- May 2016"], ["v2", "Tue, 19 Jul 2016 16:20:18 GMT  (58kb,D)", "http://arxiv.org/abs/1606.03152v2", "SIGDial 2016 (Submitted: May 2016; Accepted: Jun 30, 2016)"], ["v3", "Sat, 20 Aug 2016 21:20:21 GMT  (58kb,D)", "http://arxiv.org/abs/1606.03152v3", "SIGDial 2016 (Submitted: May 2016; Accepted: Jun 30, 2016)"], ["v4", "Mon, 12 Sep 2016 16:23:42 GMT  (59kb,D)", "http://arxiv.org/abs/1606.03152v4", "SIGDial 2016 (Submitted: May 2016; Accepted: Jun 30, 2016)"]], "COMMENTS": "Submitted to SIGDial -- May 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mehdi fatemi", "layla el asri", "hannes schulz", "jing he", "kaheer suleman"], "accepted": false, "id": "1606.03152"}, "pdf": {"name": "1606.03152.pdf", "metadata": {"source": "CRF", "title": "Policy Networks with Two-Stage Training for Dialogue Systems", "authors": ["Mehdi Fatemi", "Layla El Asri", "Hannes Schulz", "Jing He", "Kaheer Suleman"], "emails": ["first.last@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "However, statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of re-search for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gas and al., 2012; Daubigney et al., 2012; Dialog management has been successful as a partially observable Markov Decision Process (POMDP) (Williams and Young, 2007; Gas and al, 2012) that leads to systems that can learn from data and that rely on noise. In this context, a dialogue between a user and a dialogue system is conceived as a sequential process in which the system must act on each turn of what it has understood so far from users. Unfortunately, POMDP-based dialogue managers are unsuitable for online use."}, {"heading": "2 Preliminaries", "text": "The amplification learning problem consists of an environment (the user) and an agent (the system) (Sutton and Barto, 1998).The environment is de-1Teacher DA2Cscribed as a set of continuous or discrete states S and at each state s, the system can perform an action from an action spaceA (s).The actions may be continuous, but in our case they are considered discrete and finite. In time t, as a result of an action At = a (s), the state transition from St = s to St + 1 = s \"s.\" In addition, a reward signal is Rt + 1 = R (St, St + 1)."}, {"heading": "3 Value-Based Deep Reinforcement Learning", "text": "Broadly speaking, there are two main streams of methods in the RL literature: Value Approach and Political Gradients. As their names suggest, the former seeks to approximate the value function, while the latter seeks to approximate politics directly. Approaches are necessary for large or continuous faith and action spaces. If the faith space is large or continuous, it may not be possible to store a value for each state in a table, so a generalization across the state space is required. In this context, some of the advantages of deep RL techniques are the following: \u2022 Generalization across the faith space is efficient and the need for aggregated space is eliminated. \u2022 Storage requirements are limited and can be determined in advance unlike methods such as GPSARSA. \u2022 Deep architectures with multiple hidden layers can be used efficiently for complex tasks and environments."}, {"heading": "3.1 Deep Q Networks", "text": "A Deep Q Network (DQN) is a multi-layer neural network that maps a faith state Bt to the values of the possible actions At-A (Bt = b) in this state, Q\u03c0 (Bt, At; wt), where wt is the weight vector of the neural network. Neural networks to approximate the value functions have been studied for a long time (Bertsekas and Tsitsiklis, 1996). However, these methods were previously quite unstable (Mnih et al., 2013). In DQN, Mnih et al. (2013, 2015) proposed two techniques to overcome this instability - namely repetition of experiences and the use of a target network. Experience repetition is a method in which all transitions are placed in a finite pool D (Lin, 1993). Once the pool has reached its predefined maximum size, the addition of a new transition leads to a deletion of the oldest transition in the pool, W1. In the mini-Q dialog, the transitions of the Q-1 is replaced by a miniature of the Q-1, where only the Q-1 is."}, {"heading": "3.2 Double DQN: Overcoming Overestimation and Instability of DQN", "text": "The maximum operator in Eq.4 uses the same value network (i.e. the target network) to select and evaluate actions, which increases the probability of overestimating the value of the state share pairs (van Hasselt, 2010; van Hasselt et al., 2015).To see this more clearly, the target part of the loss in Eq.4 can be rewritten as follows: Rt + 1 + \u03b3Q \u03c0 (Bt + 1, argmax a Q\u03c0 (Bt + 1, a; w \u2212 t); w \u2212 t).In this equation, the target network is used twice. Decoupling is possible by using the Q network for action selection as follows (van Hasselt et al., 2015): Rt + 1 + \u03b3Q \u03c0 (Bt + 1, argmax a Q\u03c0 (Bt + 1, a; wt); w \u2212 t)."}, {"heading": "4 Policy Networks and Deep Advantage Actor-Critic (DA2C)", "text": "Another way is to calculate the discount amount for the respective repositories."}, {"heading": "5 Two-stage Training of the Policy Network", "text": "By definition, the political network provides a probability distribution across the action space. Consequently, and unlike value-based methods such as DQN, a political network can also be trained with direct supervised learning (Silver et al., 2016). Supervised learning of RL agents has been well studied in the context of Imitation Learning (IL). In IL, an agent learns to reproduce the behavior of an expert. If this condition is not met, IL can be directly applied at the level of value functions instead of politics (Pomerleau, 1989; Amit and Mataric, 2002). This direct type of imitation learning requires that the learner and the expert share the same characteristics. If this condition is not met, IL can be directly applied as one of the policies (Piot et al., 2015). In this paper, the data we use (DSTC2) is collected using a dialogue system similar to that we train in our case."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Comparison of DQN and GPSARSA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Experimental Protocol", "text": "In this paragraph, the user asks for a specific type of food in a particular area of the system in which the system requirements are made.) We must specify the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for the requirement for"}, {"heading": "6.1.2 Results", "text": "Figure 1 illustrates the performance of DQN compared to GPSARSA. In our experiments with GPSARSA, we found that it was difficult to find a good compromise between precision and efficiency.7This act consists of proposing a restaurant to the user. In fact, in order to be consistent with the DSTC2 dataset, an offer always includes the values for all the limitations understood by the system, e.g. offer (name = super ramen, food = Japanese, price range = cheap).In fact, the algorithm learns quickly with low precision but does not achieve optimal behavior, whereas higher precision makes learning extremely slow but results in better final performance. In summary, DQN outperforms GPSARSA in terms of convergence. In fact, GPSARSA requires twice as many dialogs to come together. It is also worth noting that the training time of the GPSARSA wall clock is considerably longer than that of the QDN based on the core rating."}, {"heading": "6.2 Comparison of the Deep RL Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Experimental Protocol", "text": "Similar to the previous example, we work on a restaurant domain and use the DSTC2 specifications. We use \u2212 greedy exploration for all four algorithms with an initial value of 0.5 and are annealed linearly at a rate of \u03bb = 0.99995. To speed up the learning process, the actions select-pricerange, select-area, and select-food are excluded from exploration. Note that this sentence does not depend on the state and is only intended for exploration. All actions can be executed by the system at any time. We derived two datasets from DSTC2. The first dataset contains the 2118 dialogs of DSTC2. We had evaluated these dialogs by a human expert, based on the quality of the dialogue management and on a scale of 0 to 3. The second dataset contains only the dialogs with a rating of 3 (706 dialogs of DSTC2).The underlying assumption is that we compare these dialogs with the optimal QE policies."}, {"heading": "6.2.2 Results", "text": "As expected, DDQN converges faster than DQN for all experiments. Figure 1b shows that DA2C without pre-training is the one that converges fastest (6000 dialogs vs. 10,000 dialogs for the other models). Figure 2a provides consistent results and shows that TDA2C converges significantly faster than the other models in initial training on the 2118 dialogs of DSTC2. Figure 2b focuses on DA2C and TDA2C. Compared to batch training on DSTC2, supervised training on DSTC2 accelerates convergence by 2000 dialogs (3000 dialogs vs. 5000 dialogs). Interestingly, there does not seem to be much difference between supervised training on the expert data and on DSTC2. Expert data consists of only 706 dialogs from the 2118 dialogs. Our observation is that in the dialogs selected by the system many dialogs are still appropriate, which explains that the system relieves the overall school table for performing 2118 only if it performs the optimal behavior."}, {"heading": "7 Concluding Remarks", "text": "An important feature of political networks is that they provide a direct probability distribution across the action space, enabling supervised training. We compared the results with other deep amplification learning algorithms, namely Deep Q Networks and Double Deep Q Networks. Combining supervised and enhanced learning is the main advantage of our method, which paves the way for the development of viable end-to-end dialog systems. Supervised training on a small dataset significantly accelerates the learning process and can be used to significantly improve the convergence rate of enhanced learning in statistically optimized dialog systems."}, {"heading": "A Specifications of restaurant search in DTSC2", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Learning movement sequences from demonstration", "author": ["R. Amit", "M. Mataric."], "venue": "Proc. Int. Conf. on Development and Learning. pages 203\u2013208.", "citeRegEx": "Amit and Mataric.,? 2002", "shortCiteRegEx": "Amit and Mataric.", "year": 2002}, {"title": "Biped dynamic walking using reinforcement learning", "author": ["H. Benbrahim", "J.A. Franklin."], "venue": "Robotics and Autonomous Systems 22:283\u2013302.", "citeRegEx": "Benbrahim and Franklin.,? 1997", "shortCiteRegEx": "Benbrahim and Franklin.", "year": 1997}, {"title": "NeuroDynamic Programming", "author": ["D.P. Bertsekas", "J. Tsitsiklis."], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis.,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "Natural Actor-Critic Algorithms", "author": ["S. Bhatnagar", "R. Sutton", "M. Ghavamzadeh", "M. Lee."], "venue": "Automatica 45(11).", "citeRegEx": "Bhatnagar et al\\.,? 2009", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2009}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon."], "venue": "arXiv:1511.08099 [cs.AI].", "citeRegEx": "Cuay\u00e1huitl et al\\.,? 2015", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2015}, {"title": "A Comprehensive Reinforcement Learning Framework for Dialogue Management Optimisation", "author": ["L. Daubigney", "M. Geist", "S. Chandramohan", "O. Pietquin."], "venue": "IEEE Journal of Selected Topics in Signal Processing 6(8):891\u2013", "citeRegEx": "Daubigney et al\\.,? 2012", "shortCiteRegEx": "Daubigney et al\\.", "year": 2012}, {"title": "Reinforcement learning with gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir."], "venue": "Proc. of ICML.", "citeRegEx": "Engel et al\\.,? 2005", "shortCiteRegEx": "Engel et al\\.", "year": 2005}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["M. Ga\u0161i\u0107", "C. Breslin", "M. Henderson", "D. Kim", "M. Szummer", "B. Thomson", "P. Tsiakoulis", "S.J. Young."], "venue": "Proc. of ICASSP. pages 8367\u2013", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? 2013", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Policy optimisation of POMDP-based dialogue systems without state space compression", "author": ["M. Ga\u0161i\u0107", "M. Henderson", "B. Thomson", "P. Tsiakoulis", "S. Young."], "venue": "Proc. of SLT .", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? 2012", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2012}, {"title": "Gaussian processes for fast policy optimisation of POMDP-based dialogue managers", "author": ["M. Ga\u0161i\u0107", "F. Jur\u010d\u0131\u0301\u010dek", "S. Keizer", "F. Mairesse", "B. Thomson", "K. Yu", "S. Young"], "venue": "In Proc. of SIGDIAL", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2010}, {"title": "The Second Dialog State Tracking Challenge", "author": ["M. Henderson", "B. Thomson", "J. Williams."], "venue": "Proc. of SIGDIAL.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Optimising a handcrafted dialogue system design", "author": ["R. Laroche", "G. Putois", "P. Bretier."], "venue": "Proc. of Interspeech.", "citeRegEx": "Laroche et al\\.,? 2010", "shortCiteRegEx": "Laroche et al\\.", "year": 2010}, {"title": "Machine learning for spoken dialogue systems", "author": ["O. Lemon", "O. Pietquin."], "venue": "Proc. of Interspeech. pages 2685\u20132688.", "citeRegEx": "Lemon and Pietquin.,? 2007", "shortCiteRegEx": "Lemon and Pietquin.", "year": 2007}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L-J Lin."], "venue": "Ph.D. thesis, Carnegie Mellon University.", "citeRegEx": "Lin.,? 1993", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu."], "venue": "Arxiv:1602.01783.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Playing Atari with deep reinforcement learning", "author": ["V Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I Antonoglou", "D. Wierstra", "M. Riedmiller."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["D. Hassabis"], "venue": "Nature", "citeRegEx": "Hassabis.,? \\Q2015\\E", "shortCiteRegEx": "Hassabis.", "year": 2015}, {"title": "Imitation Learning Applied to Embodied Conversational Agents", "author": ["B. Piot", "M. Geist", "O. Pietquin."], "venue": "Proc. of MLIS.", "citeRegEx": "Piot et al\\.,? 2015", "shortCiteRegEx": "Piot et al\\.", "year": 2015}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["D.A. Pomerleau."], "venue": "Proc. of NIPS. pages 305\u2013313.", "citeRegEx": "Pomerleau.,? 1989", "shortCiteRegEx": "Pomerleau.", "year": 1989}, {"title": "Agenda-based user simulation for bootstrapping a POMDP dialogue system", "author": ["J. Schatzmann", "B. Thomson", "K. Weilhammer", "H. Ye", "S. Young."], "venue": "Proc. of NAACL HLT . pages 149\u2013152.", "citeRegEx": "Schatzmann et al\\.,? 2007", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2007}, {"title": "The hidden agenda user simulation model", "author": ["J. Schatzmann", "S. Young."], "venue": "Proc. of TASLP 17(4):733\u2013747.", "citeRegEx": "Schatzmann and Young.,? 2009", "shortCiteRegEx": "Schatzmann and Young.", "year": 2009}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["K. Kavukcuoglu", "T. Graepel", "D. Hassabis."], "venue": "Nature 529(7587):484\u2013489.", "citeRegEx": "Kavukcuoglu et al\\.,? 2016", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2016}, {"title": "Mazebase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus."], "venue": "arxiv.org/pdf/1511.07401 [cs.LG].", "citeRegEx": "Sukhbaatar et al\\.,? 2016", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["R.S. Sutton."], "venue": "Ph.D. thesis, University of Massachusetts at Amherst, Amherst, MA, USA.", "citeRegEx": "Sutton.,? 1984", "shortCiteRegEx": "Sutton.", "year": 1984}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour."], "venue": "Proc. of NIPS. volume 12, pages 1057\u2013 1063.", "citeRegEx": "Sutton et al\\.,? 2000", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto."], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Double q-learning", "author": ["H. van Hasselt."], "venue": "Proc. of NIPS. pages 2613\u20132621.", "citeRegEx": "Hasselt.,? 2010", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "Deep reinforcement learning with double Qlearning", "author": ["H. van Hasselt", "A. Guez", "D. Silver."], "venue": "arXiv:1509.06461v3 [cs.LG].", "citeRegEx": "Hasselt et al\\.,? 2015", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["J.D. Williams", "S. Young."], "venue": "Proc. of CSL 21:231\u2013422.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["R.J. Williams."], "venue": "Machine Learning 8:229\u2013 256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J. Williams."], "venue": "Proc. IEEE 101(5):1160\u2013 1179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Ga\u0161i\u0107 et al., 2012; Daubigney et al., 2012).", "startOffset": 172, "endOffset": 284}, {"referenceID": 11, "context": "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Ga\u0161i\u0107 et al., 2012; Daubigney et al., 2012).", "startOffset": 172, "endOffset": 284}, {"referenceID": 8, "context": "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Ga\u0161i\u0107 et al., 2012; Daubigney et al., 2012).", "startOffset": 172, "endOffset": 284}, {"referenceID": 5, "context": "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Ga\u0161i\u0107 et al., 2012; Daubigney et al., 2012).", "startOffset": 172, "endOffset": 284}, {"referenceID": 28, "context": "Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007; Ga\u0161i\u0107 et al., 2012), which leads to systems that can learn from data and which are robust to noise.", "startOffset": 109, "endOffset": 155}, {"referenceID": 8, "context": "Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007; Ga\u0161i\u0107 et al., 2012), which leads to systems that can learn from data and which are robust to noise.", "startOffset": 109, "endOffset": 155}, {"referenceID": 7, "context": "Nevertheless, recent work has shown that it is possible to train a POMDP-based dialogue system on just a few hundred dialogues corresponding to online interactions with users (Ga\u0161i\u0107 et al., 2013).", "startOffset": 175, "endOffset": 195}, {"referenceID": 15, "context": "In order to alleviate the need for a summary state space, deep RL has recently been applied to dialogue management (Mnih et al., 2013; Cuay\u00e1huitl et al., 2015) in the context of negotiations.", "startOffset": 115, "endOffset": 159}, {"referenceID": 4, "context": "In order to alleviate the need for a summary state space, deep RL has recently been applied to dialogue management (Mnih et al., 2013; Cuay\u00e1huitl et al., 2015) in the context of negotiations.", "startOffset": 115, "endOffset": 159}, {"referenceID": 15, "context": "We analyse four deep RL models: Deep Q Networks (DQN) (Mnih et al., 2013), Double DQN (DDQN) (van Hasselt et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": ", 2015), Deep Advantage Actor-Critic (DA2C) (Mnih et al., 2016) and a version of DA2C initialized with supervised learning (TDA2C)1 (Silver et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 20, "context": "We use the Dialogue State Tracking Challenge 2 (DSTC2) dataset to train an agenda-based user simulator (Schatzmann and Young, 2009) for online learning and to perform batch RL and supervised learning.", "startOffset": 103, "endOffset": 131}, {"referenceID": 9, "context": "We first show that, on summary state and action spaces, deep RL converges faster than Gaussian Processes SARSA (GPSARSA) (Ga\u0161i\u0107 et al., 2010).", "startOffset": 121, "endOffset": 141}, {"referenceID": 25, "context": "The reinforcement learning problem consists of an environment (the user) and an agent (the system) (Sutton and Barto, 1998).", "startOffset": 99, "endOffset": 123}, {"referenceID": 28, "context": "This perception process injects noise in the state of the system and it has been shown that modelling dialogue management as a POMDP helps to overcome this noise (Williams and Young, 2007; Young et al., 2013).", "startOffset": 162, "endOffset": 208}, {"referenceID": 30, "context": "This perception process injects noise in the state of the system and it has been shown that modelling dialogue management as a POMDP helps to overcome this noise (Williams and Young, 2007; Young et al., 2013).", "startOffset": 162, "endOffset": 208}, {"referenceID": 6, "context": "In this paper, we use GPSARSA as a baseline as it has been proved to be a successful algorithm for training POMDP-based dialogue managers (Engel et al., 2005; Ga\u0161i\u0107 et al., 2010).", "startOffset": 138, "endOffset": 178}, {"referenceID": 9, "context": "In this paper, we use GPSARSA as a baseline as it has been proved to be a successful algorithm for training POMDP-based dialogue managers (Engel et al., 2005; Ga\u0161i\u0107 et al., 2010).", "startOffset": 138, "endOffset": 178}, {"referenceID": 6, "context": "In order to avoid intractability in the number of experiments, we use kernel span sparsification (Engel et al., 2005).", "startOffset": 97, "endOffset": 117}, {"referenceID": 2, "context": "Neural networks for the approximation of value functions have long been investigated (Bertsekas and Tsitsiklis, 1996).", "startOffset": 85, "endOffset": 117}, {"referenceID": 15, "context": "However, these methods were previously quite unstable (Mnih et al., 2013).", "startOffset": 54, "endOffset": 73}, {"referenceID": 13, "context": "Experience replay is a method in which all the transitions are put in a finite pool D (Lin, 1993).", "startOffset": 86, "endOffset": 97}, {"referenceID": 29, "context": "5 In order to train policy networks, policy gradient algorithms have been developed (Williams, 1992; Sutton et al., 2000).", "startOffset": 84, "endOffset": 121}, {"referenceID": 24, "context": "5 In order to train policy networks, policy gradient algorithms have been developed (Williams, 1992; Sutton et al., 2000).", "startOffset": 84, "endOffset": 121}, {"referenceID": 22, "context": ", 2016) and MazeBase by Facebook AI (Sukhbaatar et al., 2016).", "startOffset": 36, "endOffset": 61}, {"referenceID": 23, "context": "Such algorithms are known in general as actor-critic algorithms, where theQ approximator is the critic and \u03c0\u03b8 is the actor (Sutton, 1984; Barto et al., 1990; Bhatnagar et al., 2009).", "startOffset": 123, "endOffset": 181}, {"referenceID": 3, "context": "Such algorithms are known in general as actor-critic algorithms, where theQ approximator is the critic and \u03c0\u03b8 is the actor (Sutton, 1984; Barto et al., 1990; Bhatnagar et al., 2009).", "startOffset": 123, "endOffset": 181}, {"referenceID": 29, "context": "However, a direct use of Equation 5 with Q as critic is known to cause high variance (Williams, 1992).", "startOffset": 85, "endOffset": 101}, {"referenceID": 25, "context": "A good selection of Ba, which is called the baseline, can reduce the variance dramatically (Sutton and Barto, 1998).", "startOffset": 91, "endOffset": 115}, {"referenceID": 18, "context": "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002).", "startOffset": 93, "endOffset": 134}, {"referenceID": 0, "context": "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002).", "startOffset": 93, "endOffset": 134}, {"referenceID": 17, "context": "If this condition is not met, IL can be done at the level of the value functions rather than the policy directly (Piot et al., 2015).", "startOffset": 113, "endOffset": 132}, {"referenceID": 0, "context": "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002). This direct type of imitation learning requires that the learning agent and the expert share the same characteristics. If this condition is not met, IL can be done at the level of the value functions rather than the policy directly (Piot et al., 2015). In this paper, the data that we use (DSTC2) was collected with a dialogue system similar to the one we train so in our case, the demonstrator and the learner share the same characteristics. Similarly to Silver et al. (2016), here, we initialize both the policy network and the value network on the data.", "startOffset": 111, "endOffset": 613}, {"referenceID": 1, "context": "Supervised actor-critic architectures following this idea have been proposed in the past (Benbrahim and Franklin, 1997; Si et al., 2004); the actor works together with a human supervisor to gain competence on its task even if the critic\u2019s estimations are poor.", "startOffset": 89, "endOffset": 136}, {"referenceID": 19, "context": "We trained an agenda-based user simulator which at each dialogue turn, provides one or several dialogue act(s) in response to the latest machine act (Schatzmann et al., 2007; Schatzmann and Young, 2009).", "startOffset": 149, "endOffset": 202}, {"referenceID": 20, "context": "We trained an agenda-based user simulator which at each dialogue turn, provides one or several dialogue act(s) in response to the latest machine act (Schatzmann et al., 2007; Schatzmann and Young, 2009).", "startOffset": 149, "endOffset": 202}, {"referenceID": 10, "context": "The dataset used for training this user-simulator is the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) dataset.", "startOffset": 101, "endOffset": 125}], "year": 2017, "abstractText": "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actorcritic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.", "creator": "LaTeX with hyperref package"}}}