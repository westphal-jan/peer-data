{"id": "1606.03153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition", "abstract": "Text embeddings have played a key role in obtaining state-of-the-art results in natural language processing. Word2Vec and its variants have successfully mapped words with similar syntactic or semantic meanings to nearby vectors. However, extracting universal embeddings of longer word-sequences remains a challenging task. We employ the convolutional dictionary model for unsupervised learning of embeddings for variable length word-sequences. We propose a two-phase ConvDic+DeconvDec framework that first learns dictionary elements (i.e., phrase templates), and then employs them for decoding the activations. The estimated activations are then used as embeddings for downstream tasks such as sentiment analysis, paraphrase detection, and semantic textual similarity estimation. We propose a convolutional tensor decomposition algorithm for learning the phrase templates. It is shown to be more accurate, and much more efficient than the popular alternating minimization in dictionary learning literature. Our word-sequence embeddings achieve state-of-the-art performance in sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains, without requiring pre-training or additional features.", "histories": [["v1", "Fri, 10 Jun 2016 01:22:32 GMT  (67kb)", "http://arxiv.org/abs/1606.03153v1", null], ["v2", "Thu, 4 May 2017 22:32:17 GMT  (60kb)", "http://arxiv.org/abs/1606.03153v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["furong huang", "animashree anandkumar"], "accepted": false, "id": "1606.03153"}, "pdf": {"name": "1606.03153.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition", "authors": ["Furong Huang", "Animashree Anandkumar"], "emails": ["furongh@uci.edu", "a.anandkumar@uci.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.03 153v 1 [cs.C L] 10 Ju"}, {"heading": "1. Introduction", "text": "We have recently experienced an enormous success of word combinations or word combinations, which are of decisive importance in natural language processing. It is about defining words in vector representations in such a way that they have similar semantic or syntactic meanings as other words do. (2006) It is about the question of whether these words are about terms. (2011) It is about the question of whether these terms are about terms or not. (2013) It is about terms that mirror each other in the respective terms. (2012) It is about terms that differ in the language. (2012) It is about terms that are about terms. (2013) It is about terms that are about terms that are about terms. (2013) Terms that are about terms that are about terms. (2013) Terms, concepts, concepts, concepts, concepts, concepts, concepts, concepts. (2012) It is about terms that are different in the language. (2012) Terms, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts. (2012) It is about terms, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts. (2013) It is about, concepts, concepts, concepts that are about, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, concepts, in which are about, concepts, concepts, concepts, concepts, concepts, concepts, concepts, in which are about, concepts, in"}, {"heading": "1.1 Convolutional Dictionary Model and Tensor Decomposition Algorithm", "text": "In fact, it is the case that most people are in a position to feel as if they are able to move, and that they are in a position to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2. Word-Sequence Modeling and Formulation", "text": "Our ConvDic + DeconvenDec framework focuses on a revolutionary dictionary model for summarizing phrase templates, and then decodes word sequence signals to get the word sequence embedded.The first question is how to encode the word sequence into a signal that is entered into the revolutionary model, and we discuss this below."}, {"heading": "2.1 From raw text to signals", "text": "We have the ability to identify with the concepts we have made our own. (...) We have the concepts we have made our own. (...) We have the concepts we have taken to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have the concepts we take to ourselves. (...) We have. (...) We. (...) We. (...) We. (...) We. (...) We. (...) We. (...) We. (...) We. (...) We. (...) We."}, {"heading": "2.3 Feature-extraction Phase \u2013 Word-sequence Embeddings", "text": "Activation maps in one coordination: After learning a good set of phrase templates {f1,.., fL} and thus F, we use the deconvolutionary decoding (DeconvDec) to get the activation maps for the jth coordinate. For each observed coordinate of the word sequence y (j) i, the activation map w (j) l in (2) indicates the locations where the i th template file f (j) l is activated and w (j) is the line-stacked vector w: = [w * 1; w * 2;. w * L]. An estimate of w *, w (j) i, will result in (j) i = F \u2020 y (j) i. Note that the estimated phrase templates are zero padded to match the length of the word sequence."}, {"heading": "3. Convolutional Dictionary Model Learning", "text": "In this section, we focus on the conceptual phrase and propose a tensor decomposition dictional learning method for learning the phrase. (As we have already shown, the generative model for a patch of a coordinate is x, in which the superposition of the L phrase is generated as an overlay of phrases.) We show that the third phrase will include the third phrase of the L phrase. (F) The third phrase has a nice tensor decomposition of Huang and Anandkumar (2015), like the third phrase of the other phrase, where the composition of cumulants (L) is shown the unfolded third phrase of the ICA phrase. (6) We show that the third phrase of the third phrase is the third phrase of the F phrase. (F)"}, {"heading": "4. Experiments", "text": "We evaluate the quality of our word sequence embedding using three challenging natural language processing tasks: sentiment classification, paraphrase recognition and semantic text similarity estimation. Eight sets of data covering different areas are used as in Table 1. For all data sets, we train a simple logistical regression model for the training samples and report on the accuracy of the test classification using a 10-fold cross-validation. Sentiment analysis and paraphrase recognition are binary classification tasks. In a binary classification task, either accuracy or F-score is used as a benchmark. Remember that F-score is the harmonious means of precision and memory, i.e. F = 2 \u00b7 (precision \u00b7 memory) / precision + memory. Precision is the number of true positives divided by the total number of elements belonging to the positive class, and recall is the number of true positives divided by the total number of elements belonging to the positive class."}, {"heading": "4.1 Evaluation Task: Sentiment Classification", "text": "Emotional analysis is an important task in the natural language process, as the automated labeling of word sequences into positive and negative opinions is used in different settings. We rate our sentence embeddings on two sets of data from different areas, such as film criticism and subjective and objective comments, as in Table 1. By using word sequence embeddings combined with NB characteristics, we get the latest classification results for both sets of data as in Table 2.1. The information about word similarities they use is either trained in Wikipedia (4.4 million articles as opposed to the 4076 sentences of the paraphrase dataset we use) or by WordNet with expert knowledge."}, {"heading": "4.2 Evaluation Task: Paraphrase Detection", "text": "We look at the paraphrase recognition task on the Microsoft paraphrase Corpus Quirk et al. (2004); Dolan et al. (2004). We use 4076 sentence pairs as training data to learn sentence embeddings and to trace the truth of binary labels back to the ground with our learned sentence embeddings. The remaining test data is used to calculate classification errors. As discussed in Tai et al. (2015), we combine the two sentence embeddings that previously produced wL and wR, i.e. the embeddings for the right and left sentences. We generate classification characteristics both by distance (absolute difference) and by product between the pair (wL, wR): [wL] wR, where wR refers to the elemental multiplication of the right and left sentences. Unlike other unverifiable methods that are ordered and analyzed using external information such as WR and trees, we still do not use Wiki and obtain comparable results with 3. \""}, {"heading": "4.3 Evaluation Task: Semantic Textual Similarity Estimation", "text": "For the semantic textual similarity (STS) task, the goal is to predict a real value of similarity in a range [1, K] in which a sentence pair is given. We include datasets of the STS task in various ranges, including messages, image and video descriptions, glosses from WordNet / OntoNotes, the output of machine translation systems with reference translation. In order to integrate semantic test similarity estimation task into the multi-level classification framework, the gold evaluation as p-K2-K2] is discredited, and pi = 0-K1 in the sequence Tai et al. (2015), pi = semantic test similarity estimation task is included in the multi-level classification framework, the gold evaluation as p-K2-K2-K2-K2] is then discredited, and pi = 0-K1 in the sequence Tai et al. (This is reduced to determining a predicted p-K2-K1-Ki-Ki-1, Ki-1-1-Ki-1)."}, {"heading": "5. Conclusion", "text": "Our unattended, efficient ConvDic + DeconvenDec delivers word sequence representations that cover a wide range of NLP tasks well across datasets from different areas. At the same time, our efficient tensor learning algorithm requires a relatively small amount of data and calculations. In the future, we plan to explore the use of ConvDic + DeconvenDec for other areas such as images and videos and achieve common text-image embedding."}, {"heading": "Appendix A. Convolutional Tensor Decomposition For Learning", "text": "In fact, we are in a position to be in a position to move to another world, in a position to move to another world, in a position to move to another world, in a position to move to another world, in a position to move to another world, in a position to move to another world. \""}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A linear dynamical system model for text", "author": ["David Belanger", "Sham Kakade"], "venue": "arXiv preprint arXiv:1502.04081,", "citeRegEx": "Belanger and Kakade.,? \\Q2015\\E", "shortCiteRegEx": "Belanger and Kakade.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Optimization methods for convolutional sparse coding", "author": ["Hilton Bristow", "Simon Lucey"], "venue": "arXiv preprint arXiv:1406.2407,", "citeRegEx": "Bristow and Lucey.,? \\Q2014\\E", "shortCiteRegEx": "Bristow and Lucey.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Measuring semantic relatedness using salient encyclopedic concepts", "author": ["Samer Hassan"], "venue": "University of North Texas,", "citeRegEx": "Hassan.,? \\Q2011\\E", "shortCiteRegEx": "Hassan.", "year": 2011}, {"title": "Convolutional dictionary learning through tensor factorization", "author": ["Furong Huang", "Animashree Anandkumar"], "venue": "In Conference and Workshop Proceedings of JMLR,", "citeRegEx": "Huang and Anandkumar.,? \\Q2015\\E", "shortCiteRegEx": "Huang and Anandkumar.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 655\u2013665", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "The Association for Computer Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Corpus-based and knowledgebased measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava"], "venue": "In AAAI,", "citeRegEx": "Mihalcea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Chris Quirk", "Chris Brockett", "William B Dolan"], "venue": "In EMNLP,", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Paraphrase identification with lexico-syntactic graph subsumption", "author": ["Vasile Rus", "Philip M McCarthy", "Mihai C Lintean", "Danielle S McNamara", "Arthur C Graesser"], "venue": "In FLAIRS conference,", "citeRegEx": "Rus et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Mo Yu", "Mark Dredze"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu and Dredze.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Dredze.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "venue": "arXiv preprint arXiv:1504.05070,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al.", "startOffset": 166, "endOffset": 216}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al.", "startOffset": 166, "endOffset": 241}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al. (2013); Pennington et al.", "startOffset": 166, "endOffset": 264}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al. (2013); Pennington et al. (2014). Word embeddings have attained state-of-the-art performance in tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling.", "startOffset": 166, "endOffset": 290}, {"referenceID": 17, "context": "(2010); Yu and Dredze (2015). The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 13, "context": "The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 13, "context": "The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 94, "endOffset": 143}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al.", "startOffset": 14, "endOffset": 177}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al.", "startOffset": 14, "endOffset": 239}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al. (2014a). Since the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning.", "startOffset": 14, "endOffset": 428}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al. (2014a). Since the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning. The state-of-the-art unsupervised framework is Skip-thought Kiros et al. (2015), based on an objective function that abstracts the skip-gram model to the sentence level, and encodes a sentence to predict the sentences around it.", "startOffset": 14, "endOffset": 736}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa.", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently since it is linear in each of the variables. However, there are two main drawbacks: computational inefficiency and sub-optimality. AM requires a pass over all the samples in each iteration and is therefore computationally expensive in the large sample setting. Moreover, due to the non-convexity of the objective function as in (1), obtaining the global optimum of (1) is NP-hard in general. AM has no local or global convergence guarantees even in usual dictionary learning setting (multiplicative model). This problem is severely amplified in the convolutional setting due to additional symmetries. Due to shift invariance of the convolutional operator, shifting a phrase embedding fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1) unchanged. Thus, solving (1) is fundamentally ill-posed and has a large number of equivalent solutions. To solve the computational inefficiency and sub-optimality problem, we propose a convolutional tensor decomposition method Huang and Anandkumar (2015). Our convolutional tensor decomposition method employs the inverse method of moments, and decompose a data cumulant (empirically computed from aggregate statistics or data moments) as phrase embeddings and shifted versions of phrase embeddings.", "startOffset": 76, "endOffset": 1305}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently since it is linear in each of the variables. However, there are two main drawbacks: computational inefficiency and sub-optimality. AM requires a pass over all the samples in each iteration and is therefore computationally expensive in the large sample setting. Moreover, due to the non-convexity of the objective function as in (1), obtaining the global optimum of (1) is NP-hard in general. AM has no local or global convergence guarantees even in usual dictionary learning setting (multiplicative model). This problem is severely amplified in the convolutional setting due to additional symmetries. Due to shift invariance of the convolutional operator, shifting a phrase embedding fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1) unchanged. Thus, solving (1) is fundamentally ill-posed and has a large number of equivalent solutions. To solve the computational inefficiency and sub-optimality problem, we propose a convolutional tensor decomposition method Huang and Anandkumar (2015). Our convolutional tensor decomposition method employs the inverse method of moments, and decompose a data cumulant (empirically computed from aggregate statistics or data moments) as phrase embeddings and shifted versions of phrase embeddings. The entire process requires one pass of data to compute the cumulant whereas AM requires data passes in each iteration. The reason why our tensor decomposition framework avoids multiple passes of the data samples is that we only estimate the phrase embeddings fi in the learning step. Moreover, the algorithm is carefully implemented and algorithmically optimized that it requires only simple operations such as Fast Fourier Transforms (FFT) and matrix multiplications. These operations have a high degree of parallelism: for estimating L phrase embeddings, each of length n, we require O(log n+logL) time and O(L2n3) processors. Our convolutional tensor decomposition yields optimization problems (in each iteration) that can be solved in closed form and it converges much faster compared to AM Huang and Anandkumar (2015).", "startOffset": 76, "endOffset": 2374}, {"referenceID": 9, "context": "Figure 3: Convolutional tensor decomposition for learning convolutional ICA models Huang and Anandkumar (2015).(a) The convolutional generative model with template phrases.", "startOffset": 83, "endOffset": 111}, {"referenceID": 3, "context": "We can also extend to dependent distributions such as Dirichlet for w\u2217, along the lines of Blei et al. (2003), but limit ourselves to ICA model for simplicity.", "startOffset": 91, "endOffset": 110}, {"referenceID": 9, "context": "Under the convolution ICA model, we show that the third order cumulant has a nice tensor decomposition form Huang and Anandkumar (2015), as given below.", "startOffset": 108, "endOffset": 136}, {"referenceID": 0, "context": "The decomposition form in (4) is known as the CANDECOMP/PARAFAC (CP) decomposition form Anandkumar et al. (2014) (the usual form has the decomposition of the tensor and not its unfolding, as above).", "startOffset": 88, "endOffset": 113}, {"referenceID": 9, "context": "The objective function is defined in Huang and Anandkumar (2015), refer to appendix A for details.", "startOffset": 37, "endOffset": 65}, {"referenceID": 13, "context": "Our approach is different from skip thoughts, where universal phrase embeddings are generated Kiros et al. (2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.", "startOffset": 22, "endOffset": 46}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.", "startOffset": 22, "endOffset": 84}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.", "startOffset": 22, "endOffset": 118}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.", "startOffset": 22, "endOffset": 154}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.3 89.5 RNN Zhao et al. (2015) 77.", "startOffset": 22, "endOffset": 187}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.3 89.5 RNN Zhao et al. (2015) 77.2 93.7 BRNN Zhao et al. (2015) 82.", "startOffset": 22, "endOffset": 221}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.", "startOffset": 6, "endOffset": 17}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.", "startOffset": 6, "endOffset": 54}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.1 95.5 Paragraph-vector Le and Mikolov (2014) 74.", "startOffset": 6, "endOffset": 103}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.1 95.5 Paragraph-vector Le and Mikolov (2014) 74.8 90.5 Skip-thought Kiros et al. (2015) 75.", "startOffset": 6, "endOffset": 146}, {"referenceID": 13, "context": "Method Outside Information 1 F score Vector Similarity Mihalcea et al. (2006) word similarity 75.", "startOffset": 55, "endOffset": 78}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.", "startOffset": 7, "endOffset": 21}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.", "startOffset": 7, "endOffset": 68}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.9% RMLMG Rus et al. (2008) syntacticinfo 80.", "startOffset": 7, "endOffset": 121}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.9% RMLMG Rus et al. (2008) syntacticinfo 80.5% ConvDic+DeconvDec none 80.7% Skip-thought Kiros et al. (2015) train large book corpus 81.", "startOffset": 7, "endOffset": 203}, {"referenceID": 18, "context": "2 Evaluation Task: Paraphrase Detection We consider the paraphrase detection task on the Microsoft paraphrase corpus Quirk et al. (2004); Dolan et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings.", "startOffset": 8, "endOffset": 28}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error. As discussed in Tai et al. (2015), we combine the pair of sentence embeddings produced earlier wL and wR, i.", "startOffset": 8, "endOffset": 294}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error. As discussed in Tai et al. (2015), we combine the pair of sentence embeddings produced earlier wL and wR, i.e., the embedding for the right and the left sentences. We generate features for classification using both the distance (absolute difference) and the product between the pair (wL, wR): [wL\u2299wR, \u2016wL \u2212wR\u2016], where \u2299 denotes the element-wise multiplication. In contrast to other unsupervised methods which are trained using outside information such as wordnet and parse trees, our unsupervised approach use no extra information, and still achieves comparable results with the state of art Wiki (2014) as in table 3.", "startOffset": 8, "endOffset": 864}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise.", "startOffset": 176, "endOffset": 194}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, .", "startOffset": 176, "endOffset": 425}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, . . . ,K2]p\u0302. Results on STS task datasets are illustrated in Table 4. As in Wieting et al. (2015), Pearson\u2019s r of the median, 75th percentile, and highest score from the official task rankings are showed.", "startOffset": 176, "endOffset": 604}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, . . . ,K2]p\u0302. Results on STS task datasets are illustrated in Table 4. As in Wieting et al. (2015), Pearson\u2019s r of the median, 75th percentile, and highest score from the official task rankings are showed. We then compare our method against the performance of supervised models in Wieting et al. (2015): PARAGRAM-PHRASE (PP), projection (proj.", "startOffset": 176, "endOffset": 808}, {"referenceID": 13, "context": "), deep-averaging network (DAN), recurrent neural network (RNN) and LSTM; as well as the state-of-the-art unsupervised model skip-thought vectors Kiros et al. (2015). As we can see from the table, LST is performing poorly even though a back-propagation after seeing the training labelings is carried out for sequence embedding learning.", "startOffset": 146, "endOffset": 166}, {"referenceID": 25, "context": "The second three columns are reported by Wieting et al. (2015). Our comparison against the state-ofthe-art unsupervised word-sequence embedding method is in the last two columns.", "startOffset": 41, "endOffset": 63}], "year": 2016, "abstractText": "Text embeddings have played a key role in obtaining state-of-the-art results in natural language processing. Word2Vec and its variants have successfully mapped words with similar syntactic or semantic meanings to nearby vectors. However, extracting universal embeddings of longer word-sequences remains a challenging task. We employ the convolutional dictionary model for unsupervised learning of embeddings for variable length word-sequences. We propose a two-phase ConvDic+DeconvDec framework that first learns dictionary elements (i.e., phrase templates), and then employs them for decoding the activations. The estimated activations are then used as embeddings for downstream tasks such as sentiment analysis, paraphrase detection, and semantic textual similarity estimation. We propose a convolutional tensor decomposition algorithm for learning the phrase templates. It is shown to be more accurate, and much more efficient than the popular alternating minimization in dictionary learning literature. Our word-sequence embeddings achieve state-of-the-art performance in sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains, without requiring pre-training or additional features.", "creator": "LaTeX with hyperref package"}}}