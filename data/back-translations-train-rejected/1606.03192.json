{"id": "1606.03192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding", "abstract": "PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners.", "histories": [["v1", "Fri, 10 Jun 2016 05:55:58 GMT  (28kb)", "http://arxiv.org/abs/1606.03192v1", "12 pages, accepted by Neurocomputing, Software Track on Original Software Publications"]], "COMMENTS": "12 pages, accepted by Neurocomputing, Software Track on Original Software Publications", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shaohua li", "jun zhu", "chunyan miao"], "accepted": false, "id": "1606.03192"}, "pdf": {"name": "1606.03192.pdf", "metadata": {"source": "CRF", "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding", "authors": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "emails": ["shaohua@gmail.com", "dcszj@tsinghua.edu.cn", "ascymiao@ntu.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.03 192v 1 [cs.C L] 1PSDVec is a Python / Perl toolbox that learns word embedding, i.e. mapping words in a natural language to continuous vectors that encode the semantic / syntactic laws between words. PSDVec implements a word embedding method based on a weighted positive, low-level semi-defined approach. To expand the learning process, we implement a block-by-block online learning algorithm to gradually learn embedding, which significantly shortens the learning time of word embedding in a large vocabulary and can learn the embedding of new words without relearning the entire vocabulary."}, {"heading": "1. Introduction", "text": "It is not in such a way as if it were about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way, in which it was about a way in which it was about a way, in which it was about a way in which it was about a way, and in which it was about a way in which it was about a way, in which it was about a way in which it was about a way, and in which it was about which it was about a way in which it was about a way, and it was about which it was about a way in which it was about a way in which it was about a way, and it was about a way in which it was about a way in which it was about a way and it was about a way in which it was about a way and it was about a way in which it was about a way, and it was about a way in which it was about a way in which it was about a way and it was about a way in which it was about a way and it was about a way in which it was about a way and it was about a way in which it was about a way and it was about a way in which it was about a way in which it was about a way in which it was about a way in which it was about a way and"}, {"heading": "2. Problem and Solution", "text": "V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-"}, {"heading": "3. Software Architecture and Functionalities", "text": "Our toolbox consists of 4 Python / Perl scripts: extractwiki.py, gramcount.pl, factorize.py, and evaluate.py. Figure 1 represents the entire architecture.1. extractwiki.py first takes a Wikipedia snapshot as input; it then removes non-textual elements, non-English words, and punctuation; after all letters are converted to lowercase letters, it eventually produces a clean stream of English words; 2. gramcount.pl counts the frequency of unigrams or bigrams in a word stream and stores them in a file. In Unigram mode (-m1), unigrams that appear less than a certain frequency threshold are discarded. In Bigram mode (-m2), each word pair is factorily set in a text window (the size of which is specified by -n)."}, {"heading": "4. Implementation and Empirical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Implementation Details", "text": "The Python scripts use Numpy for the matrix calculation. Numpy automatically parallels the calculation to take full advantage of a multi-core machine. Perl script gramcount.pl implements an embedded C + + engine to speed up processing with less memory."}, {"heading": "4.2. Empirical results", "text": "Do you see how people in the U.S. and other countries of the world are able to survive themselves?, \"he said in an interview with the\" New York Times. \"\" I believe that the world is able to change the world. \"\" I don't believe that it will be able to change the world. \"\" I believe that the world is able to change the world, \"he said.\" I don't believe that it is able to change the world. \"\" I don't believe that it is able to change the world. \"\" I don't believe that it is able to change the world. \"\" I don't believe that it will be able to change the world. \"\" I don't believe that it is able to change the world. \""}, {"heading": "5. Illustrative Example: Training on English Wikipedia", "text": "In this example, we train embedding on the April 2015 Wikipedia snapshot, and the training procedure is as follows: 1. Use extractwiki.py to clean a Wikipedia snapshot, and create cleanwiki.txt, which is a stream of 2.1 billion words; 2. Use gramcount.plwith cleanwiki.txt as input to generate top1grams-wiki.txt; 3. Use gramcount.pl with top1grams-wiki.txt and cleanwiki.txt as input to generate top2grams-wiki.txt; 4. Use factorize.py with top2grams-wiki.txt as input to get 25,000 core embedding, stored in 25000-500-EM.ve.txt and cleanwiki.txt as input to get top2grams-wiki.txt and 25500-EM.twiki.txt as input, and 25000-kixki.txt as input."}, {"heading": "6. Conclusions", "text": "We have developed a Python / Perl toolkit PSDVec for learning word embedding from a corpus. This cross-platform open source software is easy to use, easy to expand, scales up to large vocabulary and can learn new words step by step without retraining the entire vocabulary. The embedding produced has performed well in various test tasks, achieving the best average of 7 state-of-the-art methods."}, {"heading": "Acknowledgements", "text": "This research is supported by the National Research Foundation Singapore as part of its Interactive Digital Media (IDM) Strategic Research Program."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Neural word embeddings as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of NIPS 2014,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan"], "venue": "In Proceedings of CoNLL-2014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Topic embedding: a continuous representation of documents", "author": ["Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the The 54th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A generative word embedding model and its low rank positive semidefinite solution", "author": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1965}, {"title": "Weighted low-rank approximations", "author": ["Nathan Srebro", "Tommi Jaakkola"], "venue": "In Proceedings of ICML 2003,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "One of the best known tools is word2vec [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 52, "endOffset": 58}, {"referenceID": 10, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "Representative methods/toolboxes include Hyperwords [4, 5], GloVe [11], Singular [14], and Sparse [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "An empirical comparison of popular methods is presented in [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "The toolbox presented in this paper is an implementation of our previous work [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "This toolbox is based on [9], where we estabilish a Bayesian generative model of word embedding, derive a weighted low-rank positive semidefinite approximation problem to the Pointwise Mutual Information (PMI) matrix, and finally solve it using eigendecomposition.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "PSDVec is established as a Bayesian generative model [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "For example, global factors like topics can be naturally incorporated, resulting in a hybrid model [8] of word embedding and Latent Dirichlet Allocation [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "For example, global factors like topics can be naturally incorporated, resulting in a hybrid model [8] of word embedding and Latent Dirichlet Allocation [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "A Block Coordinate Descent (BCD) algorithm [13] is used to approach (1), which requires eigendecomposition of G.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "The columns in V 2 are independent, thus for each vsi, it is a separate weighted ridge regression problem, which has a closed-form solution [9]; 5.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "7 out of the 9 testsets are used in [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 13, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 1, "context": "The hyperparameter settings of other methods and evaluation criteria are detailed in [5, 14, 2].", "startOffset": 85, "endOffset": 95}, {"referenceID": 2, "context": "The other 2 tasks are TOEFL Synonym Questions (TFL) [3] and Rubenstein & Goodenough (RG) dataset [12].", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "The other 2 tasks are TOEFL Synonym Questions (TFL) [3] and Rubenstein & Goodenough (RG) dataset [12].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "In analogy tasks Google and MSR, embeddings were evaluated using 3CosMul [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 14, "context": "The second type of testsets are 2 practical NLP tasks for evaluating word embedding methods as used in [15], i.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Following settings in [15], the embeddings for NLP tasks were trained on Reuters Corpus, Volume 1 [7], and the embedding dimensionality was set to 50 (\u201cSparse\u201d had a dimensionality of 500).", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "Following settings in [15], the embeddings for NLP tasks were trained on Reuters Corpus, Volume 1 [7], and the embedding dimensionality was set to 50 (\u201cSparse\u201d had a dimensionality of 500).", "startOffset": 98, "endOffset": 101}], "year": 2016, "abstractText": "PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping of words in a natural language to continuous vectors which encode the semantic/syntactic regularities between the words. PSDVec implements a word embedding learning method based on a weighted low-rank positive semidefinite approximation. To scale up the learning process, we implement a blockwise online learning algorithm to learn the embeddings incrementally. This strategy greatly reduces the learning time of word embeddings on a large vocabulary, and can learn the embeddings of new words without re-learning the whole vocabulary. On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing (NLP) tasks, PSDVec produces embeddings that has the best average performance among popular word embedding tools. PSDVec provides a new option for NLP practitioners.", "creator": "LaTeX with hyperref package"}}}