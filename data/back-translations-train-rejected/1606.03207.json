{"id": "1606.03207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations", "abstract": "Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7% on the SWB part of the Hub5'2000 evaluation test set, which is competitive with other state-of-the-art methods.", "histories": [["v1", "Fri, 10 Jun 2016 06:44:21 GMT  (8488kb)", "https://arxiv.org/abs/1606.03207v1", "5 pagegs, 4 figures, 5 tables"], ["v2", "Tue, 12 Jul 2016 07:23:53 GMT  (9113kb)", "http://arxiv.org/abs/1606.03207v2", "Submitted to IEEE Signal Processing Letters"]], "COMMENTS": "5 pagegs, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hwaran lee", "geonmin kim", "ho-gyeong kim", "sang-hoon oh", "soo-young lee"], "accepted": false, "id": "1606.03207"}, "pdf": {"name": "1606.03207.pdf", "metadata": {"source": "CRF", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations", "authors": ["Hwaran Lee", "Geonmin Kim", "Ho-Gyeong Kim", "Sang-Hoon Oh", "Soo-Young Lee"], "emails": ["sylee}@kaist.ac.kr)", "shoh@mokwon.ac.kr)"], "sections": [{"heading": null, "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "II. CONVOLUTION NEURAL NETWORKS", "text": "It is not the first time that the number of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the knowledge of the"}, {"heading": "IV. DEEP CNN ARCHITECTURE FOR ACOUSTIC MODELING", "text": "Since the number of filters in each frame of a given language sample is divided, the sharing of filters along the folding layer is the pooling layer (max-pooling 1: 2: 1). However, the intermap pooling layer (max-pooling 1: 1: 2) requires a detailed look at the characteristics of each layer. The mapping of a constellation layer followed by an intramap pooling layer or an intermap pooling layer requires a close look at the individual layers."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experiments setup", "text": "We conducted experiments with the 300-hour SwitchboardI Release 2 (SWBD) dataset [26], which is the conversation task, as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database, which is provided with different noise types and / or convoluted microphone distortions. The following results are 3 Pool, 1: 2 x2x512 @ 512. The Aurora4 database is a subset of the WSJ in which clean expressions with different noise types and / or convoluted with microphone distortions are added. 3 Pool, 1: 2 x2x512 @ 512."}, {"heading": "B. Convolution axis and depth of CNNs", "text": "Fig. 3 shows the decoding results of CNNs on SWBD weighting sets with different depths, from 6 layers to 15 layers (configurations are described in Fig. 2) Lower CNNs resulted in lower values, with the 15-layer CNN having a WER of 12.8% for CNB and 18.6% for the overall rating (a) SWB 6L 9L 12L 15LW ER (%) 12,51313,51414,51515.5 16 log-mel (time) CNLLR (time) fMLR (time) (b) Total 6L 9L 12L 15L1818,51919,52020,52121,5Fig. 3. Decoding results of CNNs on the switchboard weighting sets (Hub5 '2000) CNNs with different configuration axes (time, frequency) and input features (log-mel, fMLLLR) are compared."}, {"heading": "9L 3.36 7.05 8.14 18.05 11.58", "text": "It is noteworthy that IMP layers contribute to spectral fluctuations under both clean and loud conditions."}, {"heading": "D. Analysis on learnt filters", "text": "However, there are five categories of spectrotemporal characteristics in the filters. (1) Harmonic characteristics are narrow in the low frequency region and (2) wide in the high frequency region. (3) The characteristics of formant filters are temporally selective, but also sensitive to multiple frequencies. (4) The characteristics of gabor-like filters are centered on some frequency bands that are likely to recognize formants. (5) The characteristics of formant changes are directed diagonal lines, spectrotemporal modulations in the middle frequency bands. Note that different characteristics occur in different frequency bands, and that local characteristics of each type have different bandwidth sizes. The trained filters in each group of intercard frequency layers are presented as physical filters."}, {"heading": "E. Comparison of the IMP CNN", "text": "Comparison of decoding results is summarized in Table V. The \"9LIMP (512, 4)\" -IMP-CNN improved over the GMM-HMM baseline (19.5%) and the max-out network (14.6%), showing relative improvements of 34.87% and 13.01%, respectively. It is also on par with a 15-layer CNN, i.e. a non-IMP-CNN with six additional folding layers. Finally, the IMP-CNN is compared to the TDNN [16] and the CNNs that used 2-dimensional rotations [34], [35]. Note that we are only comparing other previous results, without any sequence training such as sMBR. Although our deep CNN did not use speaker matching techniques, it yielded a comparative word error rate by simply applying interpooling and increasing the depths."}, {"heading": "VI. CONCLUSION", "text": "In this paper, the present experiments show that folding along the timeline is more effective than folding along the frequency axis in language processing. Depth in folding layers is critical to sufficiently illustrate the complex temporal dynamics inherent in the acoustic characteristics of speech. To achieve greater robustness of spectral variations in speech recognition, we proposed adding CNN's Intermap Pooling (IMP). By visualizing the trained filters, we ensured that filters that are combined learn similar spectrotemporal characteristics and form a topological map. In the end, the proposed IMP provided CNN with competitive performance on the switchboard, WSJ and Aurora4 databases even without adaptation techniques. 5"}], "references": [{"title": "Acoustic Modeling using Deep Belief Networks", "author": ["A.-R. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "Audio, Speech, Lang. Process. IEEE Trans., vol. 20, no. 1, pp. 14\u201322, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMs", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "INTER- SPEECH, pp. 4688\u20134691, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning of speech features for improved phonetic recognition", "author": ["J. Lee", "S.-Y. Lee"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2011 IEEE Int. Conf. on., no. August, pp. 1249\u20131252, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-R. Mohamed", "N. Jaitly", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved feature processing for Deep Neural Networks", "author": ["S.P. Rath", "D. Povey", "K. Vesel", "J.H. Cernock"], "venue": "INTERSPEECH, pp. 1\u20135, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "2013 IEEE Work. Autom. Speech Recognit. Underst., pp. 55\u201359, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving DNN Speaker Independence With I-Vector Inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "2014 IEEE Int. Conf. Acoust. Speech Signal Process., pp. 225\u2013229, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-R. Mohamed", "H. Jiang", "G. Penn"], "venue": "IEEE Int. Conf. Acoust. Speech Signal Process., 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH, no. August, pp. 3366\u20133370, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Neural Networks for Speech Recognition", "author": ["O. Abdel-Hamid", "A.-R. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "Audio, Speech, Lang. Process. IEEE/ACM Trans., vol. 22, no. 10, pp. 1533\u2013 1545, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Recognition With Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2013 IEEE Int. Conf. on. IEEE, no. 3, pp. 6645\u20136649, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with Deep Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.R. Mohamed"], "venue": "Autom. Speech Recognit. Underst. (ASRU), 2013 IEEE Work. on. IEEE, pp. 273\u2013278, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling Has", "author": ["F. Beaufays", "H. Sak", "A. Senior"], "venue": "INTERSPEECH, no. September, pp. 338\u2013342, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G.E. Hinton", "K. Shikano", "K.J. Lang"], "venue": "Acoust. Speech Signal Process. IEEE Trans., vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks.", "author": ["H. Lee", "P. Pham", "Y. Largman", "A. Ng"], "venue": "Adv. Neural Inf. Process. Syst., pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "INTER- SPEECH, pp. 2\u20136, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convolutional neural networks for LVCSPR", "author": ["T.N. Sainath", "A.-R. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoust. Speech Signal Process. (ICASSP), 2013 IEEE Int. Conf. on., pp. 10\u201314, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces", "author": ["A. Hyv\u00e4rinen", "H. Patrik"], "venue": "Neural Comput., vol. 12, no. 7, pp. 1705\u2014-1720, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P.O. Hoyer", "M. Inki"], "venue": "Neural Comput., vol. 13, no. 7, pp. 1527\u20131558, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Invariant Features through Topographic Filter Maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. Le-Cun"], "venue": "Comput. Vis. Pattern Recognit., pp. 1605\u20131612, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning self-organized topology-preserving complex speech features at primary auditory cortex", "author": ["T. Kim", "S.-Y. Lee."], "venue": "Neurocomputing, vol. 65-66, pp. 793\u2013800, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "The topographic unsupervised learning of natural sounds in the auditory cortex", "author": ["H. Terashima", "M. Okada"], "venue": "Adv. Neural Inf. Process. Syst., pp. 1\u20139, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional maxout neural networks for low-resource speech recognition", "author": ["M. Cai", "Y. Shi", "J. Kang", "J. Liu", "T. Su"], "venue": "Proc. 9th Int. Symp. Chinese Spok. Lang. Process. ISC SLP 2014, pp. 133\u2013137, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv Prepr., pp. 1319\u20131327, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recoginition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv Prepr., pp. 1\u201314, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "SWITCHBOARD telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "Proc. 1992 IEEE Int. Conf. Acoust. Speech, Signal Process., vol. 1, pp. 517\u2013520, 1992.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "The Design for the Wall Street Journalbased CSR Corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proc. Work. Speech Nat. Languae, 1994.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Autom. Speech Recognit. Underst. (ASRU), 2011 IEEE Work. on. IEEE, pp. 1\u20134, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "The Fisher corpus: a Resource for the Next Generations of Speech-to-Text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "Proc. Lr., vol. 4, pp. 69\u201371, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Tonotopic mapping of human auditory cortex", "author": ["M. Saenz", "D.R. Langers"], "venue": "Hear. Res., vol. 307, pp. 42\u201352, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Orthogonal acoustic dimensions de fi ne auditory fi eld maps in human cortex", "author": ["B. Barton", "J.H. Venezia", "K. Saberi", "G. Hickok", "A.A. Brewer"], "venue": "Proc. Natl. Acad. Sci., 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Spatial representations of temporal and spectral sound cues in human auditory cortex", "author": ["M. Herdener", "F. Esposito", "K. Scheffler", "P. Schneider", "N.K. Logothetis", "K. Uludag", "C. Kayser"], "venue": "Cortex, vol. 49, no. 10, pp. 2822\u20132833, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the Human Auditory Cortex", "author": ["R. Santoro", "M. Moerel", "F. De Martino", "R. Goebel", "K. Ugurbil", "E. Yacoub", "E. Formisano"], "venue": "PLoS Comput. Biol., vol. 10, no. 1, p. e1003412, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "Interspeech 2015, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "pp. 2\u20136, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "ACOUSTIC modeling with deep learning has demonstrated remarkable performance improvements in automatic speech recognition [1]\u2013[4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "ACOUSTIC modeling with deep learning has demonstrated remarkable performance improvements in automatic speech recognition [1]\u2013[4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "Amongst feature-level approaches, speaker-adapted methods such as fMLLR [5] have been proposed.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Acoustic features concatenated with i-vectors, which represent speaker information, also have been employed as input for DNNs [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Acoustic features concatenated with i-vectors, which represent speaker information, also have been employed as input for DNNs [6], [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Model-level approaches have employed hybrid NN-HMM systems with convolutional neural networks (CNNs) [8]\u2013[10] and recurrent neural networks (RNNs) [11]\u2013[13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "Previous researchers introduced time-delayed neural networks (TDNNs), which are CNNs with convolutions along the time axis to learn the temporal dynamics of features [14]\u2013[16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "Previous researchers introduced time-delayed neural networks (TDNNs), which are CNNs with convolutions along the time axis to learn the temporal dynamics of features [14]\u2013[16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "Other researchers have applied convolutions along the frequency axis to attain invariance to frequency-shifts [8], [17].", "startOffset": 110, "endOffset": 113}, {"referenceID": 16, "context": "Other researchers have applied convolutions along the frequency axis to attain invariance to frequency-shifts [8], [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "The limited weight sharing method in which weights are convolved only within a subsection of frequencybands has been employed in efforts to overcome this problem [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "For unsupervised feature extraction, previous researchers imposed sparsity terms over small groups or neighborhoods in feature maps of image [18]\u2013 [20] and speech data [21], [22].", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "Previously, a convolutional maxout network has been proposed [23], however, it applied convolutions along the frequency axis.", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "Like the maxout networks [24], this layer groups the filters, and pools the feature maps inside a group.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Furthermore, motivated by the performance of very deep CNNs [25], we inserted convolution layers with small filters (of size 1x3) between two intramap pooling layers.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "We conducted experiments using the 300 hour SwitchboardI Release 2 (SWBD) dataset [26] which is conversational telephone speech task as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database which are read speech.", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "We conducted experiments using the 300 hour SwitchboardI Release 2 (SWBD) dataset [26] which is conversational telephone speech task as well as the Wall Street Journal (WSJ) corpus [27] and Aurora4 database which are read speech.", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": "Our implementation is developed upon the KALDI toolkit [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "For the SWBD task, we decode speech using a trigram language model (LM) of 30k vocabularies which is trained on 3M words, and then we rescore the decoding results using 4gram LM which is trained on Fisher English Part 1 transcripts [29].", "startOffset": 232, "endOffset": 236}, {"referenceID": 29, "context": "In recent neurophysiological studies, there is consensus that multiple tonotopic maps exist in the human auditory system [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "However, few studies suggest that this topography includes other sound features, such as temporal, spectral, and joint modulations [31]\u2013[33].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "However, few studies suggest that this topography includes other sound features, such as temporal, spectral, and joint modulations [31]\u2013[33].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "MFCC + i-vectors TDNN 4L [16] 12.", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "VTL-warped log-mel CNN 8L (2conv+6fc) [34] 12.", "startOffset": 38, "endOffset": 42}, {"referenceID": 34, "context": "6 CNN 13L (10conv+3fc) [35] 11.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Finally, the IMP CNN is compared with the TDNN [16] and the CNNs which employed 2-dimensional convolutions [34], [35].", "startOffset": 113, "endOffset": 117}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7% on the SWB part of the Hub5\u20192000 evaluation test set, which is competitive with other state-of-the-art methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}