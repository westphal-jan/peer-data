{"id": "1606.03212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Discovery of Latent Factors in High-dimensional Data Using Tensor Methods", "abstract": "Unsupervised learning aims at the discovery of hidden structure that drives the observations in the real world. It is essential for success in modern machine learning. Latent variable models are versatile in unsupervised learning and have applications in almost every domain. Training latent variable models is challenging due to the non-convexity of the likelihood objective. An alternative method is based on the spectral decomposition of low order moment tensors. This versatile framework is guaranteed to estimate the correct model consistently. My thesis spans both theoretical analysis of tensor decomposition framework and practical implementation of various applications. This thesis presents theoretical results on convergence to globally optimal solution of tensor decomposition using the stochastic gradient descent, despite non-convexity of the objective. This is the first work that gives global convergence guarantees for the stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. This thesis also presents large-scale deployment of spectral methods carried out on various platforms. Dimensionality reduction techniques such as random projection are incorporated for a highly parallel and scalable tensor decomposition algorithm. We obtain a gain in both accuracies and in running times by several orders of magnitude compared to the state-of-art variational methods. To solve real world problems, more advanced models and learning algorithms are proposed. This thesis discusses generalization of LDA model to mixed membership stochastic block model for learning user communities in social network, convolutional dictionary model for learning word-sequence embeddings, hierarchical tensor decomposition and latent tree structure model for learning disease hierarchy, and spatial point process mixture model for detecting cell types in neuroscience.", "histories": [["v1", "Fri, 10 Jun 2016 07:17:00 GMT  (4605kb)", "http://arxiv.org/abs/1606.03212v1", "Ph.D. Thesis"]], "COMMENTS": "Ph.D. Thesis", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["furong huang"], "accepted": false, "id": "1606.03212"}, "pdf": {"name": "1606.03212.pdf", "metadata": {"source": "META", "title": "Discovery of Latent Factors in High-dimensional Data Using Tensor Methods", "authors": ["Furong Huang", "Shaoyun Liu"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 6.03 212v 1 [cs.L G] 10 Jun 2016"}, {"heading": "UNIVERSITY OF CALIFORNIA, IRVINE", "text": "Discovery of latent factors in high-dimensional data using tensor methods"}, {"heading": "DISSERTATION", "text": "in partial fulfillment of the requirements for the degree of"}, {"heading": "DOCTOR OF PHILOSOPHY", "text": "in Electrical Engineering and Computer Engineering by Furong Huang Dissertation Committee: Assistant Professor Animashree Anandkumar, Chairman Professor Carter Butts Associate Professor Athina Markopoulou2016All materials c \u00a9 2016 Furong Huang"}, {"heading": "DEDICATION", "text": "An Jinsong Huang and Shaoyun LiuiiTABLE OF CONTENTS"}, {"heading": "LIST OF FIGURES vii", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LIST OF TABLES ix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LIST OF ALGORITHMS x", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "ACKNOWLEDGMENTS xi", "text": "CURRICULUM VITAE xiii"}, {"heading": "ABSTRACT OF THE DISSERTATION xvi", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "1.1 Summary of Posts.............................. 31.1.1. Global Guaranteed Online Tensor Decomposition...... 3 1.1.2 Use of Scalable Tensor Decomposition Framework..... 4 1.1.3 Learning Invariant Models using Convolutional Tensor Decomposition 6 1.1.4. Learning Latent Tree Models using Hierarchical Tensor Decomposition 7 1.1.5 Discovering Neuronal Cell Types using Spectral Methods............ 81.2 Tensor Preliminaries............... Appree................ Applied......... 9 1.3. Background and Related Works.... Tree....."}, {"heading": "2 Online Stochastic Gradient for Tensor Decomposition 25", "text": "Precursors include:..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Applying Online Tensor Methods for Learning Latent Variable Models 43", "text": "....,..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Dictionary Learning through Convolutional Tensor Decomposition 78", "text": "4.1 Model and Formulation. 4.1. Model. 4.1. Formulation. 4.2. Shape of the Cumulant Moment Tensor......... 804.1.1. Convolutional Dictionary Learning. 82 4.3. Alternating Least Squares for Convolutional Tensor Decomposition... 81 4.2. Shape of the Cumulant Moment Tensor. 84 4.4. Challenge to Reduce Memory and Computational Costs. 874.4.1 Challenge: Computing ((H'H). Alternately Least Squares for Convolutional Tensor Decomposition.. 84 4.4 Challenge: Optimization to Reduce Memory and Computational Costs. 874.4.1 Challenge: Computing (H'G)."}, {"heading": "5 Latent Tree Model Learning through Hierarchical Tensor Decomposition103", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "6 Discovering Cell Types with Spatial Point Process Mixture Model 123", "text": ". 134. Motives. Objectives.... 1266.2. Modeling Cell Types. Using Spatial Point Process Features. 124.. 1.2. Previous Work... 129. 6.2.1. Marked Spatial Point Process Process. Representing ISH Images. 129 6.2.2 Representing Spatial Point Processes Using Common Feature Histograms 130 6.3 Non-Mixing Features. Decesses to Discover Cell-types. 135. Gatial Images. 129 6.2.2 Representing Spatial Point Processes Using Common Feature Histograms 130 6.3 Un-Mixing Deponial Species."}, {"heading": "7 Conclusion and Outlook 141", "text": "7.1 Conclusion............................................. 141."}, {"heading": "A Appendix for Online Stochastic Gradient for Tensor Decomposition 156", "text": "A.1. Detailed Analysis for Section 2.2 in Unconfined Case. 172A.2.1. Preliminary. 178 A.2.3. Main Law. 156 A.2. Detailed Analysis for Section 2.2. Confined Case. 174 A.2.2. 172A.2.1. Preliminary. 178 A.2.3. Main Law. 176 A.2."}, {"heading": "B Appendix for Applying Online Tensor Methods for Learning LVMs 212", "text": "B.1 Stochastic updates.................................................................................................................................................................................."}, {"heading": "C Appendix for Dictionary Learning via Convolutional Tensor Method 224", "text": "........................................................................................................................."}, {"heading": "D Appendix for Latent Tree Learning via Hierarchical Tensor Method 228", "text": "D.1 Additivity of Multivariate Information Distance......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "E Appendix for Spatial Point Process Mixture model Learning 240", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "ACKNOWLEDGMENTS", "text": "It was an honor to be her first Ph.D.. I appreciate all the efforts they have made to build my trust, to make my study experience productive and stimulating. Her endless enthusiasm for research has been contagious and a source of motivation. She also always has a spirit of exploration and stimulation. She is not only a role model, but also a friend who shares and offers her life experience."}, {"heading": "EDUCATION", "text": "Doctor of Philosophy in ECE 2016 University of California Irvine Irvine, CA, USA Master of Science in ECE 2012 University of California Irvine Irvine, CA, USA Bachelor of Science in EECS 2010 Zhejiang University Hangzhou, Zhejiang, China"}, {"heading": "RESEARCH EXPERIENCE", "text": "Research Assistant 2010-2016 University of California Irvine Irvine, California Research Intern 2014.3-2014.5 Microsoft Research Redmond, WashingtonResearch Intern 2014.6-2014.12 Microsoft Research New England Cambridge, Massachusetts"}, {"heading": "REFEREED JOURNAL PUBLICATIONS", "text": "F. Huang, UN Niranjan, M.U. Hakeem and A. Anandkumar, \"Online Tensor Methods for Learning Latent Variable Models\" 2014"}, {"heading": "Journal of Machine Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Anandkumar, V.Y.F Tan, F. Huang and A.S. Willsky, \u201cHighDimensional Structure Learning of Ising Models: Local Separation", "text": "Criterion \"2012"}, {"heading": "Annals of Statistics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Anandkumar, V.Y.F Tan, F. Huang and A.S. Willsky, \u201cHighDimensional Gaussian Graphical Model Selection: Walk-Summability", "text": "and local separation criterion \"2012 Journal of Machine Learningxiii"}, {"heading": "REFEREED CONFERENCE PUBLICATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F. Huang, A. Anandkumar, C. Borgs, J. Chayes, E. Fraenkel, M. Hawrylycz, E. Lein, A. Ingrosso, S. Turaga, \u201cDiscovering Neuronal Cell Types and Their Gene Expression Profiles Using a Spatial Point", "text": "Model of Process Mixing \"2015"}, {"heading": "NIPS BigNeuro workshop 2015", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F. Huang, U.N. Niranjan, J. Perros, R. Chen, J. Sun, A. Anandkumar,\u201cScalable Latent Tree Model and its Application to Health", "text": "Analytics \"2015NIPS 2015 Workshop on Machine Learning in HealthcareF. Huang, A. Anandkumar,\" Convolutional Dictionary Learning through Tensor Factorization \"2015JMLR Conference and Workshop"}, {"heading": "F. Arabshahi, F. Huang, A. Anandkumar, C. Butts, \u201cAre you going to the party: depends, who else is coming? \u2013Learning hidden group", "text": "Dynamics through conditional latent tree models \"2015"}, {"heading": "2015 IEEE International Conference on Data Mining (ICDM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F. Huang, S. Matusevych, A.Anandkumar, N. Karampatziakism and", "text": "P. Mineiro, \"Distributed Latent Dirichlet Allocation via Tensor Factorization\" 2014"}, {"heading": "NIPS Optimization for Machine Learning workshop", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Anandkumar, D. Hsu, F. Huang and S.M. Kakade, \u201cLearning", "text": "High-Dimensional Mixtures of Graphic Models \"2012"}, {"heading": "Proc. of NIPS 2012", "text": "F. Huang and A. Anandkumar, \"FCD: Fast-Concurrent-Distributed Load Balancing under Switching Costs and Imperfect Observations\" 2013"}, {"heading": "In Proc. of the 32nd IEEE INFOCOM", "text": "F. Huang, W. Wang and Z. Zhang, \"Prediction-based Spectrum Aggregation with Hardware Limitation in Cognitive Radio Networks\" 2010IEEE Vehicular Technology ConferenceSOFTWARExivTensorDecom4TopicModeling Link to Github repository C + + Algorithm that solves the topic modeling of LDA by means of tensor decomposition at individual node workstations. OnlineTensorCommunity Link to Github repository C + + and CUDA algorithms that solve community recognition problems by means of tensor decomposition on CPU and GPU. SpectralLDA-TensorSpark Link to Github repository Spark spectral LDA algorithms in Scala that solve large-scale tensor decomposition. ConvDicLearnTensorFactor Link to Github repository Tensor decomposition algorithms that learn counter-revolutionary models."}, {"heading": "AWARDS", "text": "MLconf Industry Impact Student Research Winner 2015 Google San Francisco, CaliforniaTravel Grant 2015 NIPS Montreal, CanadaTravel Grant 2013 WiML Lake Tahoe, NevadaFellowship 2010 University of California Irvine Irvine, Californiaxv"}, {"heading": "ABSTRACT OF THE DISSERTATION", "text": "Discovery of latent factors in high-dimensional data Using Tensor MethodsByFurong HuangDoctor of Philosophy in Electrical Engineering and Computer Engineering University of California, Irvine, 2016 Assistant Professor Animashree Anandkumar, ChairUnsupervised Learning aims at discovering hidden structures that drive observations in the real world. It is essential for the success of modern machine learning and artificial intelligence. Latent variable models are versatile in unattended learning and have applications in almost all areas, such as social network analysis, natural language processing, computer vision and computational biology. Formation of latent variable models is a challenge due to the non-convential expansion of probability function. An alternative method is based on the spectral decomposition of low-order moments, matrices and tensors. This multi-layered framework ensures that the correct model is uniform."}, {"heading": "Introduction", "text": "In fact, it is as if most people are able to understand themselves and what they do. (...) It is as if they were not able to understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if. (...) It is as if. (...). (...). (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.).). (). (.). (.).). (.).). (.).). (). (.). (). (). (). (.).). (). (.). (). (). (). (). (). ().). (). (). (). ().). (). (). ().). (). ().). (). (). (). (). (). (). (). ().).). ().). (). (). (). ().). (). (). ().). (). ().). (). (). ().).). ().). (). ().). (). ().). (). ().). (). ().).). ().). ()."}, {"heading": "1.1 Summary of Contributions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1.1 Globally Guaranteed Online Tensor Decomposition", "text": "Learning latent variable models using the moment method involves a challenging non-convex optimization problem in the high-dimensional system, since tensor degradation is generally NP-hard. We identify strict saddle properties for non-convex problems that allow efficient optimization, and using this property, we show that from an arbitrary starting point, noisy stochastic gradient degradation converges in a polynomial number of iterations to a local minimum. To the best of our knowledge, this is the first work to provide global convergence3guarantees for stochastic gradient degradation to non-convex functions with exponentially many local minima and saddle points. Our analysis is applied to orthogonal tensor decomposition, and we propose a new optimization formulation for the problem of tensor decomposition that exhibits strict saddle properties. [As a result, we obtain the first online algorithm for this wide algorithm]"}, {"heading": "1.1.2 Deployment of Scalable Tensor Decomposition Framework", "text": "The Tensor Decomposition Framework is tailor-made for automated document categorization (i.e. finding the hidden themes of articles) and efficient prediction of common interests or communities of social actors (using the connectivity graph) in social networks, see Figure 1.2. Compared to the state-of-the-art variation inference that optimizes the lower limit of probability, our results are surprisingly accurate and much faster [84, 86]. For example, we implemented our tensor decomposition on Spark for Learning in the PubMed data, which consists of 8 million documents and 700 million words. Tensor method achieves much more accurate results (better probability) than variation conclusions, although we never calculate or optimize the probability function. Moreover, the Tensor method requires much less computing time and is at least an order of magnitude faster. Another comparison is performed on graph data to evaluate the performance of detecting hidden communities."}, {"heading": "1.1.3 Learning Invariant Models Using Convolutional Tensor De-", "text": "The data is modeled as linear combinations of filters / templates in conjunction with activation cards. Filters are shift-invariant dictionary elements due to folding. A tensor decomposition algorithm with additional constraints on shift invariance on factors is introduced, converges with models with better reconstruction errors, and is much faster compared to popular alternating minimization euristics, where filters and activation maps are updated alternately. This framework for decomposition of tensors successfully solves challenging natural language processing tasks such as learning phrase templates and extracting sequence embeddings, as shown in Figure 1.5."}, {"heading": "1.1.4 Learning Latent Tree Models Using Hierarchical Tensor De-", "text": "This thesis represents an integrated approach to structural and parameter estimation in latent tree models. In the meantime, the proposed algorithm automatically learns the latent variables and their locations and achieves consistent structural estimation with logarithmic computational complexity. Meanwhile, the inverse method of moments is performed in intelligently selected local neighborhoods with linear computational complexity. Rigorous evidence for the global consistency of structure and parameter estimation within the framework of the divide-and-conquer framework is presented. Consistency guarantees apply to a broad class of linear multivariate latent tree models, including discrete distributions, continuous multivariate distributions (e.g. Gaussian), and mixed distributions such as Gaussian mixtures [88]. This model class is much more general than discrete models, which prevail in most previous work on latent tree models."}, {"heading": "1.1.5 Discovering Neuronal Cell Types Using Spectral Methods", "text": "The above advances in unattended learning have rich applications in neuroscience. Using the framework of spectral cell decomposition, we analyze challenging tasks. For example, cataloging neuronal cell types in the brain, which was the primary goal of the Brain Initiative and modern neuroscience. It is an extremely difficult task, partly due to the brain-wide resolution of individual cells at the petabyte scale. Previous methods capture the average image intensity in local voxels for a rough estimate of gene expression levels. The success of these methods is based on precise image alignment at the neuron level between different brains, which is compatible with prohibitive. In this thesis, we solve the above problem using a spatial point process model. We measure the spatial distribution of the neurons marked in the ISH image for each gene and model it as a spatial point process mixture, the mixing weights of which are expressed by the cell types."}, {"heading": "1.2 Tensor Preliminaries", "text": "This year, we have reached the stage where no such process has ever taken place."}, {"heading": "1.3 Background and Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.3.1 Online Stochastic Gradient for Tensor Decomposition", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "1.3.2 Applying Online Tensor Methods for Learning Latent Vari-", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country."}, {"heading": "1.3.3 Dictionary Learning through Convolutional Tensor Decom-", "text": "In fact, it is as if most people are able to know and understand themselves. (...) It is not as if they were able to identify themselves. (...) It is not as if they were able to do it. (...) It is as if they were doing it. (...) It is not as if they were doing it. (...) It is as if they were not doing it. (...) \"\" It is as if they do not want to do it. (...) \"(...)\" (...) \"(...)\" (() \"(() (() () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () ()) () () () () ()) () () () ()) () () () () () () () () () ()) () () () () ()) () () () () ()) () () () ()) () () ()) () () ()) () ()) () ()) () () ())) () () ()) () () ()) () ()) () () ()) () () ()) () () ()) () () () () ()) () () ()) () () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () ("}, {"heading": "1.3.4 Latent Tree Model Learning through Hierarchical Tensor", "text": "Decomposient variable graphical models span flat models and hierarchical models, see Figure 1.10 for a flat multi-view model and a hierarchical model. Latent tree graphical models are a popular class of latent variable models, where a probability distribution with observed and hidden variables are Markovian on a tree. Due to the fact that the structure of (observable and hidden) variable interactions are approximated as a tree, conclusions on latent trees can be made precisely by a simple propagation of belief [134]. Therefore, latent tree graphical models present a good trade-off between model accuracy and computational complexity. They are applicable in many areas where it is natural to expect hierarchical or sequential relationships between variables (through a hidden Markov model). For example, latent tree models have been used for phylogenetic reconstruction [56], object recognition [42] and human pose estimation [157].The task of a model is to learn from two parts:"}, {"heading": "1.4 Thesis Structure", "text": "Based on the theoretical guarantees, I will show in Chapter 3 how to make the degradation of tensors highly scalable and highly parallel. Furthermore, we expand the scope to learn dictionaries or templates with additional constraints such as displacement invariance in the image or text dictionary by using Convolutionary Tensors or decomposition in Chapter 4. I will therefore not limit myself to flat models in which observations are independent of the hidden dimension. On the contrary, I expand the scope for the degradation of tensors from multiple angles to a hierarchical tensor to analyze data with complicated hierarchical structure. A latent tree model is therefore proposed in Chapter 5, where latent variable graphical model structure learning techniques are combined with hierarchical tensors or decomposition to enable consistent learning of the hierarchical model structure and parameters."}, {"heading": "Online Stochastic Gradient for Tensor", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Decomposition", "text": "In the previous paper [13], it was found that a broad class of latently variable graphical models can be learned by decomposition of tensors, and model parameters can be achieved by decomposition of higher order aggregates or by modified data moments. Therefore, learning latently variable graphical models is reduced to the problem of decomposition of tensors. Decomposition of tensors is a non-convex optimization problem, and it is known that non-convex optimization problems are generally difficult NP. Now, the question arises whether we can use efficient methods such as stochastic gradients to achieve a local optimum for a class of functions under mild conditions? Could we integrate tensor decomposition problems into the class of function? We analyze stochastic gradient wastes to optimize non-convex functions. In many cases, the goal of non-convex functions is to find a reasonable local minimum, and that gradient updates are the most important concern, trapped in tele-functions."}, {"heading": "Strict saddle functions", "text": "Considering a function f (w) that is doubly differentiable, we call a stationary point if we call a strict saddle point. To these functions, the Hessian of each saddle point has a negative eigenvalue. In particular, this means that local second algorithms similar to those in [53] can always make some progress. It may seem counterproductive why stochastic gradients can work in these cases: especially if we let the basic gradients descend from a stationary point, then it will not move. However, we show that the saddle points are not stable and that randomness in stochastic gradients contributes to the algorithm being able to escape from the saddle point."}, {"heading": "2.1 Preliminaries", "text": "The stochastic gradient aims to solve the stochastic optimization problem (1,2), which we repeat here: w = arg min w = Rd \u03b2 (w), where f (w) = Ex \u00b2 D [\u03c6 (w, x)].Recall \u03c6 (w, x) denotes the loss function that is evaluated for the sample x at point w.The algorithm follows a stochastic gradient draft + 1 = wt \u2212 p \u2212 p \u2212 p (wt, xt), (2,1) where xt is a random sample from the distribution D and \u03b7 is the learning rate.In the more general environment, stochastic gradient descent can be considered an optimization of any function f \u2212 w \u2212 p \u2212 p, if a stochastic gradient orakel.27Definition 2.1. For a function f (w): Rd \u2192 R, a function SG (w), which represents a variable to a random vector in Rd, is a chastic gradient."}, {"heading": "2.2 Stochastic Gradient Descent for Strict saddle Func-", "text": "In this section we discuss the properties of saddle points and show if all saddle points are well behaved, then the stochastic gradient descent finds a local minimum for a non-convex function in polynomial time. Notation In the course of the chapter we use [d] to denote sentence {1, 2,..., d}. We use [2] to denote the vector norm and the spectral norm of matrices. For a matrix we use [min] to denote its smallest eigenvalue. For a function f: Rd \u2192 R, the function f and [2f] denote its gradient vector and the Hessian matrix."}, {"heading": "2.2.1 Strict saddle Property", "text": "For a twofold differentiable function f (w) strict statutes are required if its inclination angles are equal to 0. (f) For a twofold differentiable function f (w) we are sound. (f) For a twofold differentiable function f (w) we are not responsible. (f) For a twofold differentiable function f (w) we are not responsible. (f) For a twofold differentiable function f (w) we are not responsible. (f) It is not possible that a point w has both positive and negative values. (f) These criteria do not cover all cases, since degenerated scenarios could occur. (w) Positive semidefined with a eigenvalue equal to 0, in which case the point of a local could give a local minimum or a saddle point.If a function does not have this, then we say strict saddle points, then the function is: Definition 2.2. (f) A twofold function is (f) differentiable."}, {"heading": "2.2.2 Proof Sketch", "text": "In order to prove that this is a point at which a minimum is reached, we must analyse the three cases in definition 2.3 in more detail. (...) If the gradient is large (...), we will show the mode of operation in one step (...). (...) If we see the mode of operation in one step (...), we will show the mode of operation in one step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...), in another step (...). (...), in another step (...). (...), in another step (...). (...), in another step (...), in another step (...). (...), in another step (...)."}, {"heading": "2.2.3 Constrained Problems", "text": "In many cases, the problems we face are limited optimization problems. In this part, we briefly describe how the analysis can be adapted to problems with equality constraints (sufficient to apply the tensor), dealing with general inequality constraints is left as a future work problem, especially in the algorithm after each step we have to project back to this diversity (see algorithm 2, where \"W\" is the projection to this diversity). Method 2 Projected noisy stochastic gradient input: Stochastic gradient oracle SG (w), starting point w0, desired accuracy: WT, which is close to a local minimum."}, {"heading": "2.3 Online Tensor Decomposition", "text": "In this section we describe how to apply our stochastic gradient descendant analysis to tensor decomposition problems. First, we present a new formulation of the tensor decomposition problem as an optimization problem and show that it fulfills the strict saddle characteristics. Then, we explain in a simple example of Independent Component Analysis (ICA) [91] how to calculate the stochastic gradient."}, {"heading": "2.3.1 Optimization Problem for Tensor Decomposition", "text": ", u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u"}, {"heading": "2.3.2 Implementing Stochastic Gradient Oracle", "text": "To design a simple ICA example, we need an implementation for the stochastic gradient oracle. In other words, the tensor is responsible for any x-arbitrary x-arbitrary example. [1] Using the linearity of the multilinear map, we can know that E [g) (ui, uj, uj) = E [g) (x) (ui, uj, uj, uj] s can define the loss function (ui, ui, ui, uj, uj, uj), and the stochastic gradient oracle (u, x).38For the simple ICA example, we consider an unknown signal."}, {"heading": "2.4 Experiments", "text": "The results show that the algorithm converges efficiently from random starting points (as predicted by the theorems), and that our new formulation (2,11) performs better than the reconstruction error (2,10).The step size is carefully selected for the respective objective functions.The performance is determined by the normalized reconstruction error E = (3), the input sensor T is a random tensor in R10 4 that exhibits orthogonal decomposition (1,1).The step size is determined by the normalized reconstruction error E = (4) and the value T-2F. Samples and stochastic gradients We use two ways to generate samples and calculate stochastic gradients."}, {"heading": "2.5 Conclusion", "text": "In this chapter, we identify the strict saddle characteristic and show that stochastic gradient degradation converges to a local minimum under this assumption, leading to a new online orthogonal tensor decomposition algorithm. We hope that this is a first step towards understanding stochastic gradients for further classes of non-convex functions. We believe that the strict saddle characteristic can be extended to include more functions, especially those with similar symmetry characteristics."}, {"heading": "Applying Online Tensor Methods for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning Latent Variable Models", "text": "This year is the highest in the history of the country."}, {"heading": "3.1 Tensor Forms for Topic and Community Models", "text": "In this section we briefly recapitulate the theme derived in [10, 8] and the community models as well as the tensor forms for their precise moments."}, {"heading": "3.1.1 Topic Modeling", "text": "The discussion concerns the question whether the words are words used in a particular document. (h1, h2,., hk) The words in a particular document are represented. (1, x2,.) The words are drawn independently of each other and are interchangeable. (1, h2,.) The words in the document are represented by d-dimensional random vectors x1, x2,. xl, with the words in Rd and d the 1https: / archive.ics.uci.edu / ml / datasets + of + Words45size of the word vocabulary. Conditioned on h, the words in a document satisfy E xi | h = \u00b5h, where."}, {"heading": "3.1.2 Mixed Membership Model", "text": "In the mixed membership stochastic block model (MMSB), introduced by [5], the edges in a social network refer to the hidden communities of nodes (1). A batch tensor decomposition technology for learning MMSB was derived in [8].Let n denotes the number of nodes, and the vectors are contained in a simplex, i.e., in which each node has an associated community membership vector, which is a latent variable, and the vectors are contained in a simplex, i.e., i.e."}, {"heading": "3.2 Learning using Third Order Moment", "text": "Our learning algorithm uses up to the moment of the third order to estimate the topic word matrix \u00b5 or the community membership matrix \u0442. First, we obtain the simultaneous occurrence of triplet words or subgraph counts (implicit). Then, we perform pre-processing with second order Moment50M2. Then, we perform tensor decomposition efficiently using stochastic gradient descent [111] on M3. We find that in our implementation of the algorithm on the Graphics Processing Unit (GPU) linear algebraic operations are extremely fast. We implement our algorithm on the CPU for large data sets exceeding the memory capacity of the GPU, and use sparse matrix operations that result in large increases in both memory and runtime requirements. The general approach is summarized in Algorithm 3.Procedure 3: Total approach to learning a variable model over a moment."}, {"heading": "3.2.1 Dimensionality Reduction and Whitening", "text": "Whitening step uses linear algebraic manipulations to make the tensor symmetric and orthogonal (in anticipation). Furthermore, it leads to a reduction in dimensionality since it (implicitly) reduces the tensor M3 of size O (n 3) to a tensor of size k3, where k is the number of communities. Typically, we have k n. The whitening step also converts the tensor M3 into a symmetric orthogonal tensor. The whitening matrix W RnA \u00b7 k fulfills W M2W = I. The idea is that if the two-dimensional projection of the second moment on W yields the identity matrix, then a trilinear projection of the third moment on W would lead to an orthogonal tensor."}, {"heading": "3.2.2 Stochastic Tensor Gradient Descent", "text": "In [8] and [10], the deflation potential method is used for tensor decomposition, where eigenvectors are restored in serial fashion by iterating over multiple loops. In addition, batch data is used in its iterative performance method, making this algorithm slower than its stochastic counterpart. In addition to implementing a stochastic spectral optimization algorithm, we achieve further acceleration by efficiently paralleling stochastic updates. Let v = [v1 | v2 |. | vk] be the true eigenvectors. Denote the cardinality of the sample as nX, i.e. nX: = | X | |. Now that we have the white tensor, we propose stochastic tensor descensus (STGD)."}, {"heading": "3.2.3 Post-processing", "text": "Property values: = [\u03bb1, \u03bb2] as the norm of property vectors \u03bbi = estestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestestest"}, {"heading": "3.3 Implementation Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Symmetrization Step to Compute M2", "text": "Note that for the theme model, the second order moment M2 can easily be calculated from the word-frequency vector (3,7). For the community setting, the calculation M2 requires additional linear algebraic operations. It therefore requires the calculation of the matrices ZB and ZC in equation (3,7), which requires the calculation of pseudo-inverses of \"pairs\" matrices. Note now that the pseudo-inverses of (pairs (B, C) in equation (3,7) with rank k-SVD: k-SVD (pairs (B, C)) = UB (: 1: k) BC (:, 1: k) VC (: k).We use the property of low rank to have efficient runtimes and storage. First, we implement the k-SVD of pairs (pairs (B, C)) = pairs (RX, CX, B)."}, {"heading": "3.3.2 Efficient Randomized SVD Computations", "text": "rE \"s rf\u00fc swear by the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "3.3.3 Stochastic Updates", "text": "STGD can potentially be the most computationally intensive task if performed naively, since storing and manipulating an O (n3) -sized tensor makes the method unscalable. However, we overcome this problem because we never explicitly form the tensor; instead, we implicitly break down the tensor modes. By optimizing the implementation of STGD, we get white vectors yA, yB, and yC, and efficiently manipulate these vectors to obtain tensor eigenvector updates by scaling the gradient at an appropriate learning rate. Efficient STGD via stacked vector operations: We convert the BLAS II into BLAS III operations to obtain tensor vector updates by stacking the vectors into matrices efficiently."}, {"heading": "Module BLAS I BLAS II BLAS III SVD QR", "text": "Reducing Communication in the GPU Implementation: In STGD, note that the amount of memory required for the iterative part does not depend on the number of nodes in the dataset, but on the parameter k, i.e. the number of communities to be estimated, since the whitening performed before STGD results in a reduction in dimensions, making it suitable for storing the required buffers in the GPU memory and using the CULA device interface for the BLAS operations. In Figure 3.2, we illustrate the data transfer involved in the GPU standard and device interface codes. While the default interface includes the data transfer (including the welded neighborhood vectors and property vectors) in each stochastic iteration between the CPU memory and the GPU memory, the device interface includes the assignment and maintenance of the property characteristics in each iterative iterator."}, {"heading": "3.3.4 Computational Complexity", "text": "We divide the execution of our algorithm into three main modules, namely pre-processing, STGD and post-processing, the various matrix operations of which are listed in Table 3.1.1 above. Theoretical asymptotic complexity of our method is summarized in Table 3.2 and is best addressed by taking into account the parallel model of calculation [94], i.e., where a number of processors or computing cores work with the data at the same time. This is justified considering that we implement our method on GPUs and matrix products are embarrassingly parallel. Note that this differs from serial computing complexity. We will now encode the entries in Table 3.2. First, we remember a fundamental problem regarding the lower time complexity for parallel addition along with the number of cores required to achieve a speed - up.Lemma 3.4] Adding s numbers in the O series (time); this can be significantly improved with time (essence log)."}, {"heading": "Pre-processing", "text": "We multiply an O (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n) x (n"}, {"heading": "STGD", "text": "In STGD we perform implicit stochastic updates, consisting of a constant number of matrix matrix and matrix vector products, on the set of eigenvectors and white samples of the size k \u00b7 k. If c [1, k3 / log k] is, we get a runtime of O (k3 / c) for the calculation of internal products parallel to c computing cores, since each core can perform an internal products64 to calculate an element in the resulting matrix independently of other cores in linear time. for c \u0441 (k3 / log k, \u221e] we get a runtime of O (log k) using Lemma 3.4. Note that the STGD time complexity is calculated per iteration."}, {"heading": "Post-processing", "text": "After all, post-processing also consists of sparse matrix products. Similar to pre-processing, this consists of multiplications with the sparse matrices. Given the number of non-zeros per column of an O (n) \u00b7 O (k) matrix, the effective number of elements on O (sk) decreases. Therefore, with c (nks / log s) cores O (nsk / c) time, we need to perform the internal products for each entry of the resulting matrix. For c (nks / log s), using Lemma 3.4, we get a runtime of O (log s).Note that nk2 is the complexity of calculating the exact SVD, and we reduce it to O (k) if there are enough cores available. This is meant for the setting where k is small. This k3 complexity of the SVD is related to O (k) < < n) algorithms with distributed points 99, for example, SVD [71] and SVD [n]."}, {"heading": "3.4 Validation methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1 P -value Testing", "text": "Remember that the true matrix of community membership is a set of random variables, and we look at data sets in which the basic truth is available. To confirm the results, we need to find a good match between the rows of random variables. We use the term p-values to test statistically significant dependencies between a set of random variables. The p-value denotes the probability not to refute the null hypothesis that the random variables under consideration are independent, and we use the term p-values to test the statistically significant dependencies between a set of random variables. The p-value denotes the probability not to refute the null hypothesis that the random variables under consideration are independent, and we use the student's s3 t test to calculate the p-value."}, {"heading": "3.4.2 Evaluation Metrics", "text": "The question that arises is whether it is an error where the two communities are not correlated to each other in order to find an error that exceeds a specified p-value threshold, and we designate such pairs that use a split p-value account for the fact that two communities are not correlated with each other. Therefore, it should be noted that in the special case of the block model, where the estimated communities are only a permutated version of the basic truth communities, the mating results match each other with perfect matching accuracy. [G {Pval} s becomes asG {V} = (1) {Pval}, V {2) {Pval}, E {Pval}, where the nodes in the two node sets are defined."}, {"heading": "3.5 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Results on Synthetic Datasets:", "text": "This year we have reached the point where we will be able to put ourselves at the forefront to pave the way for the future, \"he said."}, {"heading": "DBLP Dataset:", "text": "The DBLP data contains bibliographic records5 with different publication sites such as journals and conferences, which we model as communities. We then consider authors who have published at least one paper in a community (place of publication) to be members of it. Co-authorship is therefore modeled as a link in the graph in which authors are represented as nodes. In this context, we could win back the best authors in communities and bridge authors."}, {"heading": "3.6 Conclusion", "text": "In this chapter, we have presented a fast and unified moment-based learning framework that encompasses overlapping communities and topics in one corpus.5http: / / dblp.uni-trier.de / xml / Dblp.xml76Second, the implementation of implicit \"tensor\" operations may not have strong statistical recovery warranties.5http: / / dblp.uni-trier.de / xml / Dblp.xml76Second, the implementation of implicit \"tensor\" operations leads to significant acceleration of the algorithm, even though the use of a moment-based formulation may at first glance appear to be mathematically expensive. Third, the use of randomized methods for spectral methods in the computorial domain is promising, as the runtime can then be significantly shortened, paving the way for several interesting directions for further research. While our current use will extend the detection of communities in a single cloud by incorporating the number of multiperplier systems and extending them to larger ones."}, {"heading": "Dictionary Learning through", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Convolutional Tensor Decomposition", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "4.1 Model and Formulation", "text": "For a matrix M is a matrix M with equal number of columns M (with equal number of rows) is M: = [M1, M2, M2,..] a matrix M is with equal number of rows (with equal number of rows) M: = [M1, M2,..) a matrix F of M (with equal number of columns) is M: = [M1; M2;.; ML].Cyclic Convolution The 1-dimensional (1-D) n-cyclic convolution f: w between vectors f and w is defined."}, {"heading": "4.1.1 Convolutional Dictionary Learning/ICA Model", "text": "We assume that the input x-Rn is generated to characterize F values. We assume that the input x-Rn asx values are generated. [L] f-J values are treated additively. [L] f values are activated. [L] f values are activated. [L] f values are activated. [L] f values are activated. [L] Cir (f values) is the concatenation or column stacked version of circular matrices, and w values are the stacked vector w values: = [w values 1; w values 2),... w values L values indicate that Cir (f values L) is the circular matrix corresponding to the filter f values. [4,2). Note that although81F values are an n value of nL values. [w values], there are only nL values free parameters. We never explicitly build the estimates of F values."}, {"heading": "4.2 Form of Cumulant Moment Tensors", "text": "Tensor (1).u (1).u (1).u (1).u (1).u (1).u (1).u (1).u (1).u (1).u (1).u (2).u (1).u (2).u (2).u (3).u (3).u (3).u (3).u (3).u (3).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u).u (4).u (4).u (4).u (4).u (4).u).u (4).u (4).u (4).u).u (4).u (4).u (4).u).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u (4).u"}, {"heading": "4.3 Alternating Least Squares for Convolutional Ten-", "text": "sor Decompositionation is updated by normalization.We now ask ourselves the question whether we are able to solve the problem. (D) We first consider the asymmetric solution of (4.6) and then ask ourselves whether we can adapt the different variables F, G and H for Filter-84timates along each of the modes. (D) D (F) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S S S S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S S (S) S S (S) S (S) S S (S) S S) S (S) S (S) S (S) S (S) S S S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S (S) S (S) S (S) S (S (S) S (S) S (S) S (S) S (S (S) S (S) S (S) S (S) S (S (S) S (S (S) S) S (S (S) S (S (S) S) S (S (S) S (S (S) S) S (S) S (S (S) S (S) S (S) S (S (S) S (S) S (S (S) S) S (S (S) S) S (S (S (S) S (S (S) S) S (S (S) S) S (S (S) S (S (S (S) S) S (S) S (S (S"}, {"heading": "4.4 Algorithm Optimization to Reduce Memory and", "text": "The computing power of the computing power of the computing power is, however, very expensive when n (and L) are large. Instead, we use the properties of the circular matrices and the Khatri-Rao product to perform these calculations implicitly. We present our end result on the computational complexity of the proposed method. Recall that n is the filter size and L is the number of filters.Lemma 4.2. [Computational Complexity] Using multi-threading, the runtime of our algorithm for n dimensional input and L number of filters."}, {"heading": "4.4.1 Challenge: Computing ((H\u22a4H). \u22c6 (G\u22a4G))\u2020", "text": "A naive implementation to find the matrix inversion (H'H) is very expensive. However, we integrate the stacked circular structure of G and H to reduce the calculation. Note that this is not easy, because although G and H are stacked circular matrices, the resulting product, whose inversion is required, is not circular. In the following, we show that it is partially circulating along different rows and columns. Property 2 (Block of circulating matrices): The matrix (H'H'H). The matrix (G'G) consists of a row and a column of circulating matrices. We make the above property precise by introducing some new notations. Define column stacked identity matrix I: = [I,.., I]."}, {"heading": "4.4.2 Challenge: Computing M = C3(H\u2299 G) \u00b7 ((H\u22a4H). \u22c6 (G\u22a4G))\u2020", "text": "Now that we (H H) n (G G) \u2020 efficiently, we must calculate the resulting matrix with C3 (H G) to get M. We observe that the mth line of the result M given89byMm = [FFT (g1);.; FFT (gL)], z: [FFT (h1);.; FFT (hL)] are concatenated FFT coefficients of the filters, andn (m): UHI (m) IU, (gL)], z: [FFT (h1);.; FFT (hL)] are concatenated FFT coefficients of the filters, andn (m).UHI (m) IU, (m) ij: [FFT) m (FFT (h1)."}, {"heading": "4.5 Experiments: Comparison with Alternating Mini-", "text": "The error comparison between our proposed revolutionary tensor algorithm and the alternating minimization algorithm is shown in Figure 4.2a. We evaluate the errors for both algorithms by comparing the reconstruction of error and filter recovery errors3. Our algorithm approaches the solution much faster than the alternating minimization algorithm. In fact, the alternating minimization results in an incorrect solution where the reconstruction error is significantly greater than the error achieved by the tensor method. The error gap in the reconstruction error curve in Figure 4.2a for the tensor method is due to the random initialization after deflation of a filter and the estimation of the second filter size. Runtime is also shown in Figure 4.2b and 4.2c between our proposed conversion error curve in Figure 4.2a for the tensor method."}, {"heading": "4.6 Application: LearningWord-sequence Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.6.1 Word-Sequence Modeling and Formulation", "text": "Our ConvDic + DeconvenDec framework focuses on a revolutionary dictionary model for summarizing phrase templates, and then decodes word sequence signals to get the word sequence embedded.The first question is how to encode the word sequence into a signal that is entered into the revolutionary model, and we discuss this below."}, {"heading": "From raw text to signals", "text": "But in this case, we will be able to solve the problem by getting it under control, \"he told the German Press Agency.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules that we have set ourselves. \""}, {"heading": "Comprehension Phase \u2013 Learning Phrase Templates", "text": "A word sequence consists of overlapping patches, so we are interested in learning a generative model of overlapping patches. We can also consider these patches as phrases. A length n Patch x is created as an overlay of L phrase embedding f * l, which is involved in L activation cards w * l. Due to the property of folding, the folding is reformulated as a multiplication of F * and w *, with F *: [Cir (f * 1), Cir (f * 2),.., Cir (f * L)] being the concatenation of circular matrices, and w \u00b2 the concatenation of stacked vectors w *: = w * 1 w \u00b2 2... w \u00b2 2... RnL. To be precise: a patchx = f * l \u00b2 l = F * w \u00b2."}, {"heading": "Feature-extraction Phase \u2013 Word-sequence Embeddings", "text": "Activation maps in a coordination: After learning a good set of phrase templates {f1,.., fL} and thus F, we use deconvolutionary decoding (DeconvDec) to get the activation maps for the jth coordinate. For each observed coordinate of the word sequence y (j) i, the activation map w * l in (4.17) indicates the locations where the i th template phrase f * l is activated and w * is the line-stacked vector w *: = [w * 1; w * 2;. w * L]. An estimate of the w *, w (j) i is obtained as a sequence (j) i = F * y (j) i. (4.18) Note that the estimated phrase templates are zero padded to match the length of the word sequence."}, {"heading": "4.6.2 Evaluating Embeddings through Downstream Tasks", "text": "We evaluate the quality of our word sequence embedding using three challenging natural language processing tasks: sentiment classification, paraphrase recognition and semantic text similarity estimation.Eight sets of data covering different areas are used as shown in Table 4.1. For all data sets, we train a simple logistic regression model on the training samples and report on the accuracy of the test classification using a 10-fold cross-validation. Sentiment analysis and paraphrase recognition are binary classification tasks. In a binary classification task, either accuracy or F-score is used as the evaluation criterion. Remember that F-score is the harmonic 97 means of precision and memory, i.e. F = 2 \u00b7 (precision \u00b7 memory) / precision + memory. Precision is the number of true positives divided by the total number of elements belonging to the positive phrase."}, {"heading": "Evaluation Task: Sentiment Classification", "text": "Sentiment analysis is an important task in the natural language process, as the automatic labeling of word sequences into positive and negative opinions is used in different contexts. We evaluate our sentence embedding in two sets of data from different areas, such as film reviews and subjective and objective comments, as shown in Table 4.1. By using word sequence embedding combined with NB characteristics, we obtain the current classification results for these two sets of data as shown in Table 4.2."}, {"heading": "Evaluation Task: Paraphrase Detection", "text": "We look at the task of paraphrase recognition on the Microsoft paraphrase corpus [137, 55]. We use 4076 sentence pairs as training data to learn sentence embeddings and use our learned sentence embeddings to undo the binary names of truth on the ground. The remaining test data is used to calculate classification errors.4The word similarity information they use is either in Wikipedia (4.4 million articles as opposed to the 4076 sentences of the paraphrase dataset we use) or from WordNet with expertise.98"}, {"heading": "Method MR SUBJ", "text": "As discussed in [154], we combine the two sentence embeddings that have produced previous wL and wR sentences, i.e. the embedding of the right and left sentences. We generate characteristics for classification both by distance (absolute difference) and by product between the pair (wL, wR): [wL, wL \u2212 wR], where \"element-wise multiplication\" is used. Unlike other uncontrolled methods trained with external information such as word networks and parse trees, our uncontrolled approach does not use additional information and still achieves results comparable to the state of the art [162] as in Table 4.3. We show some examples of circumscriptions and non-circumscriptions that we have identified. Recognized circumscriptions: (1) Amrozi accused his brother, whom he called \"the witness,\" of intentionally distorting his evidence by calling his evidence \"the witness.\""}, {"heading": "Evaluation Task: Semantic Textual Similarity Estimation", "text": "For the semantic textual similarity (STS) task, the goal is to predict a real value of similarity in a range [1, K] in which a sentence pair is given. We include datasets from the STS task in various ranges, including messages, image and video descriptions, glosses from WordNet / OntoNotes, the output of machine translation systems with reference translation. [154] To integrate semantic test similarity estimation task into the multi-level classification framework, the gold rating as p-K2-K1 is discredited in the following way [154], pi = semantic test similarity estimation task is placed within the multi-level classification framework. [K1, K2] the gold rating is discredited as p-K2-K2, and pi = 0 otherwise. This is reduced to determining a predicted p-K2-K1 model."}, {"heading": "4.7 Conclusion", "text": "Contrary to popular alternate minimization, our method avoids expensive decoding of activation cards at each step and can achieve better solutions with faster runtimes. We have derived efficient updates for tensor degradation based on modified alternating smallest squares and it consists of simple operations such as FFTs and matrix multiplications. Our framework easily extends to conventional models for higher-dimensional signals (such as images), where the circulating matrix is replaced by block-circulating matrices [73]. More generally, our framework can deal with general group structure by replacing FFT operation with the corresponding group FFT [106]. By combining the advantages of tensor methods with a general class of invariant representations, we thus have a powerful paradigm for learning efficient latent variable models and embedding in a variety of areas."}, {"heading": "Latent Tree Model Learning through", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hierarchical Tensor Decomposition", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "5.1 Latent Tree Graphical Model Preliminaries", "text": "There are two types of variables at the nodes, namely the observable variables referred to as X..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5.2 Overview of Approach", "text": "The general approach is illustrated in Figure 5.2, where (a) and (b) show the step of data preprocessing, (c) - (e) illustrate the step of sharing and learning structure and parameters. Specifically, we start with the parallel calculation of pairwise multivariate information distances. The information distances roughly measure the extent of the correlation between different pairs of observed variables and require SVD calculations in step (a). Subsequently, in step (b) a Minimum Spanning Tree (MST) is constructed in parallel using observable variables [24] using the multivariate information distances. The local groups are also determined by MST, so that they are available for the next step of learning structure and parameters. Structure and parameter learning is carried out jointly by a strategy of sharing and conquest. Step (c) illustrates the split step (or local learning) in which local structure and parameters are estimated."}, {"heading": "5.3 Structure Learning", "text": "It is a way of comparing the different types of words and words that take into account the paired correlation between the different types of words and words. (Expectation is beyond the sample) Note that their precedence differs from the hidden variables. (The multivariate distance between the i and yb nodes is defined as asdist (va, vb): \u2212 lok = 1 (yay). (E (yay)). (E (yay))). (E (E)). (E)"}, {"heading": "5.4 Parameter Estimation", "text": "This is a guaranteed and fast approach to restoring parameters in the third order of the observed data. In contrast, traditional approaches such as maximizing expectations (EM) suffer from a false local optima and cannot demonstrably restore the parameters. A latent tree with three leaves: We will first consider an example of three observable leaves x1, x2, x3 (i.e. a triplet) with a common hidden parent. We will then clarify how this can be generalized to learn the parameters of the latent tree model."}, {"heading": "5.5 Integrated Structure and Parameter Estimation", "text": "So far, we have described high-level methods of structural estimation by local recursive grouping (LRG) or parameter estimation by tensor degradation using triplets of variables. We now describe an integrated and efficient approach that brings all these components together. Furthermore, we offer merging steps to obtain a global model, using the sub-trees and parameters we have learned from local groups."}, {"heading": "5.5.1 Local Recursive Grouping with Tensor Decomposition", "text": "The next steps towards an integrated process where full algorithm assessment is applied in practice are as follows: \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\" \"There is only one way.\""}, {"heading": "5.5.2 Merging and Alignment Correction", "text": "This year, it has reached the point where it will be able to retaliate in order to pave the way for the future."}, {"heading": "5.6 Theoretical Gaurantees", "text": "Theorem 5.1. Given samples from an identifiable latent tree model, the proposed method consistently restores the structure with O (log p) sample complexity and parameters with O (poly p) sample complexity; the sketch of evidence is in Appendix D.3.Calculation Complexity: We remember some notations here: d is the observable node dimension, k is the hidden node dimension, k is the number of samples, p is the number of observable nodes, and z is the number of non-zero elements in each sample.Let us specify the maximum size of the groups through which we operate the local recursive grouping method, k is the number of samples, p is the number of observable nodes, and z is the number of non-zero elements in each sample.Let us specify the maximum size of the groups through which we operate the local recursive grouping method."}, {"heading": "5.7 Experiments", "text": "The results of the study are written in C + +, coupled with the multithreading capabilities of the OpenMP environment [52] (version 1.8.1).The goal of our analysis is to discover a disease hierarchy based on their cooperative relationships in patient records. Generally, longitudinal medical records store the diagnosed diseases over time, where the diseases are coded with the International Classification of Diseases (ICD)."}, {"heading": "5.7.1 Validation", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "5.8 Conclusion", "text": "We present an integrated approach to structural and parameter estimation in latent tree models. Our method addresses challenges such as uncertainty of location and number of hidden variables, the problem of local optima without consistency guarantees, the difficulty of scalability in terms of number of variables. The proposed algorithm is ideal for parallel calculations and highly scalable. We have successfully applied the algorithm to a real-world application to detect disease hierarchy by using large patient data for 1.6 m patients. 122Chapter 6"}, {"heading": "Discovering Cell Types with Spatial", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Point Process Mixture Model", "text": "Cataloging the neural cell types that encompass the circuits of individual brain regions is one of the main goals of modern neuroscience and the BRAIN initiative. Single cell RNA sequencing can now be used to measure the gene expression profiles of individual neurons and to categorize neurons based on their gene expression profiles. Although the single cell techniques are extremely powerful and promising, they are currently still labor-intensive, have high cost per cell and, most importantly, do not provide information about the spatial distribution of cell types in specific regions of the brain. We propose a complementary approach using computer-aided methods to determine the cell types and their gene expression profiles by analyzing the brain-wide resolution of individual cells in situ-hybridization (ISH)."}, {"heading": "6.1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Motivations and Goals", "text": "It is indeed the case that we are able to go in search of a solution that puts us in the position in which we find ourselves."}, {"heading": "6.1.2 Previous Work", "text": "Allen Brain Atlas (ABA) [115] is a groundbreaking study that maps the gene expression of approximately 20,000 genes across the entire mouse brain. ABA data set consists of high-resolution 2D cellular images of in-situ hybridized sections of the brain digitally aligned to a common reference atlas. However, since the in-situ images for each gene originate from different mouse brains, and since there is significant variability in the individual locations of labeled cells, it is not possible to register brain-wide gene expression with a resolution greater than about 250 micrometers. Therefore, cellular resolution is detailed to determine the 3d generative representation of the average gene expression level in 250 micrometers."}, {"heading": "6.2 Modeling the Spatial Distribution of Cell-types Us-", "text": "Most analyses of the ABA in situ hybridization data set use a simple measure of the average expression level in relatively large 250\u00b5m x 250\u00b5m x 250\u00b5m voxels of brain tissue. Due to the large volume over which the expression level is averaged, such an imaging cannot distinguish between a large number of cells expressing small amounts of RNA and a small number of cells expressing large amounts of RNA. All information about the spatial organization of labeled cells, their shapes, sizes, and spatial density is lost and summarized by a single scalar number."}, {"heading": "6.2.1 The Marked Spatial Point Process Representation of ISH", "text": "Image Our approach requires processing the high-resolution ISH images to identify individual marked cells and their visual characteristics. We developed a cell recognition algorithm described in the Supplementary Section. Our algorithm additionally estimates the expression level of each identified cell, its shape, size and orientation. Figure 6.1 (a) and Figure 6.1 (b) illustrate the results of our cell recognition algorithm. Since cell types not only differ in terms of gene expression patterns, but also have a variety of shapes, sizes and spatial densities, we attempted to characterize these properties. We measured: (1) cell size s = [r1, r2]: the radius in two main directions of an ellipse matches each cell; (2) cell orientation o: the orientation of the first main axis of the ellipse; (3) gene intensity density p: the intensity of the labeling of a cell relative to the image background; (4) the cell distribution is within a 129local point."}, {"heading": "6.2.2 A Model-free Approach to Representing Spatial Point Pro-", "text": "Statistical modeling of repellent spatial point processes, as occurs in biology, is not trivial, and many generative models, such as determinant point processes [110] and mater point processes, have a high computational complexity. However, since we are not interested in directly modeling the individual marked cells, but only in modelling their aggregated spatial statistics and deriving their gene expression profiles from them, we can take a simpler approach. We use a common histogram of simple statistics on the collection of detected cells to characterize the underlying point process from which they are derived.This is an empirical moment approach that requires a careful definition of a generative point process distribution.As we describe in the next section, we propose to model the point process measured from the ISH image for each gene as a mixture of point processes belonging to individual cell types."}, {"heading": "6.3 Un-mixing Spatial Point Processes to Discover Cell-", "text": ""}, {"heading": "6.3.1 Generative Model: A Variation of Latent Dirichlet Alloca-", "text": "The spatial process histogram representation of the ABA ISH dataset for each brain region is an NF \u00b7 NG matrix [xmn], where NF is the total number of histogram vessels (henceforth, the number of histogram features is estimated) 2, NG is the number of genes, and xmn is the number of cells that the gene n in the histogram vessels m.We model the gene-space histogram matrix [xmn] by assuming that it is supported by a variation of the latent dirichlet allocation (vLDA) [32] model of the cell types histogram histogramm.This matrix factorization-based variable model assumes that the ISH histograms are associated with a small number of cell types, K, and each cell type i with a type-dependent spatial histogram."}, {"heading": "6.3.2 Estimating the Cell-type Dependent Gene Expression Pro-", "text": "After testing several estimation methods for the parameters of our model, we found that the non-negative matrix factorization (NMF) performs well in estimating the cell-type specific 132 gene expression profiles \u03b2, see Figure 6.2a. We solve the following optimization problem: min \u03b2, hNF \u2211 mNG \u2211 n (xmn \u2212 K \u0445 thmt \u03b2 t nL m) 2, s.t. \u03b2tn \u2265 0, NG \u2211 n\u03b2tn = 1, h m t \u2265 0, K \u2211 thmt = 1 (6.1) Here, the non-negativity and sum-to-one constraints on hmt and \u03b2 t n ensure that h and \u03b2 result in properly normalized multinomic distributions. While this estimation method results in common estimates for h and \u03b2, it does not pre-set the guide h h. Therefore, we refine our NMF derived estimates for h by variation inference [32]."}, {"heading": "6.3.3 Estimating the Cell-type Dependent Spatial Point Process", "text": "Histogram hWe use a standardized method to estimate the maximum probability for h [32]. Iteratively, we refine the conclusion of cell type membership hm \u2206 k under each common histogram characteristic m. We update hmi until convergence [148].hmi \u2190 1Lm + \u2211 Kt \u03b1tNG \u2211 n = 1xmn hmi \u03b2 i nK \u2211 l = 1 hml \u03b2 l + \u03b1i, \u0192i [K], m [NF] (6.2) Remember that the dirichlet before \u03b1 encodes the number of cell types that we expect on average to express each gene. We use \u03b1 on a symmetrical dirichlet with \u03b11 = \u03b12 =. = \u03b1K, namely t = 0.01 for all cell types. In practice, we find that our estimates of h are relatively insensitive to the specific choice for \u03b1 as long as it is small enough."}, {"heading": "6.4 Results and Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.1 Implementation Details", "text": "We tested our proposed cell type discovery algorithm using the high-resolution in situ hybridization image series \u03b2 for 1743 of the most reliably mapped and annotated genes in the ABA. Single cells were detected in the ISH images of cell resolution using tailor-made algorithms (detailed in Supplementary Information). For each detected cell, we adapt ellipses and extract several local features: (a) size and shape represented as the diameters along the principle axes of the ellipse, (b) orientation of the first principle axis, (c) gene intensity level measured by the intensity of cell body marking, and (d) the number of cells detected with a radius of 100 micrometers around the cell, which is a measurement of local cell density. We aligned the ISH images to the ABA reference atlas and focused our attention on cells in the coroscope focus for this paper."}, {"heading": "6.4.2 Evaluating Cell-type Gene Expression Profile Predictions", "text": "A recent study conducted on 1691 neurons isolated from the somatosensory cortex of the mouse uses this dataset to evaluate the quality of the cell types we detected. The cell RNA seq data, G: = [g1 | g2 |... | gNC], contains the gene expression profiles for NC = 1691 cells. We then infer the cell types h i for these cells using the equation (6.2) and then calculate the likelihood Li of observing each cell according to our estimated cell type-dependent gene expression profile Matrix \u03b2 using the equation (6.4). Then we can calculate the 135 perplexity, a commonly used measure of fit under the vLDA model, the RNA seq data of each cell, based on the model we learned from our spatial point process."}, {"heading": "6.4.3 Comparison to Standard Average Gene Expression Features", "text": "Baseline and a Permutation Test for SignificanceHere we demonstrate the superiority of our method and its statistical significance in two respects. First, we compared the helplessness of the unicellular RNA seq dataset G in our model (Figure 6.2b, monochrome blue) with the helplessness of a replacement dataset with the same marginal statistics, but whose gene-cell correlations were destroyed (Figure 6.2b, blue dashed).We created this replacement dataset by randomly permutating the gene expression levels for each gene across cells. This permutated dataset showed significantly higher (worse) perplexity than the true standard single cell dataset. This shows that our model, which was trained to untangle the ISH-derived spatial point processes, discovered cell types whose gene expression profiles correspond significantly better to single cells than the gene expression of our standard single cell dataset."}, {"heading": "6.4.4 A Brief Analysis of Recovered Cell Types in Somatosensory", "text": "CortexIn this section, we describe the representative spatial process statistics and gene expressions for 8 cell types that we have recovered. We tried to align our 8 clusters with cell types defined by [168] in the single-celled RNA sequencing paper. In Figure 6.3, we found high overlap in gene expression profiles for all 8 clusters of known cell types defined in [168], interneurons, S1138 pyramidal, mural, endothelial, microglia, ependymal, astrocytes, and oligodendrocytes. Estimation of \u03b2 was combined with MLE to conclude the cell-type spatial point-process representation hml. In investigating the spatial process distributions that we predict for each of these cell types, we discover that while the distribution of cell body orientations is fairly broad and similar across neural cell types, the distribution of cells is a previously unknown number of excitations for one cell type to another, from 6.6% to 6.6%."}, {"heading": "6.5 Conclusion", "text": "We developed a computational method for detecting cell types in a brain region by analyzing the high-resolution in situ hybridization image series from the Allen Brain Atlas. Assuming that cell types exhibit unique spatial distributions and gene expression profiles, we used a diverse latent dirichlet allocation (vLDA) based on a spatial process mixing model to simultaneously infer the spatial distribution and gene expression profiles of cell types. By comparing our gene expression profile with a single-celled RNA sequence dataset, we showed that the performance of our model significantly improves compared to the state of the art. The accuracy of our method is largely based on the assumption that cell types differ in their spatial distribution and that our point process characteristics do a good job of distinguishing these differences, so that the performance of our method can be improved by better estimates of better properties."}, {"heading": "Conclusion and Outlook", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Conclusion", "text": "Now that we have reached the end of our dissertation, we are convinced that spectral methods, including tensor degradation, are good candidates for unattended learning, revealing hidden structures through transformations and extracting useful and clean information to characterize complicated data. Spectral methods are potentially applicable in a variety of applications such as text and image processing, social networking, health analytics, and neuroscience. Spectral methods, especially matrix / tensor degradation frames, are versatile; they are easy to apply to flat models such as interchangeable model, multi-view model, and hidden Markov model, but they are also modifiable to learn models with a hierarchy such as a mixture of trees and latent tree model. Spectral methods not only work well on traditional multiplicative, sparse coding models, but also exceed state-of-the-art group invariance models."}, {"heading": "7.2 Outlook", "text": "Could we push the boundaries of spectral methods further? Can we have a tensor library with optimal hardware support for tensor operations? Could we develop approximate algorithms that are more computationally efficient in the region of high-dimensional hidden space? Could we have tensor sketches where decomposition occurs in a sketching vector space and the tensor is never explicitly shaped? Furthermore, could we use tensor decomposition to train models with other inventory (such as rotation invariance and scaling invariance) or general inventory limitations? In the real world, we could push forward our framework for more challenging tasks. In neuroscience, we want to understand the brain; that is, systematically model and learn the neural system of the brain and clarify its relationship to bodily functions. We know that a deep neural network system inspired by the architecture of neural circuits is hugely successful."}, {"heading": "Appendix A", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix for Online Stochastic", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Gradient for Tensor Decomposition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Detailed Analysis for Section 2.2 in Unconstrained", "text": "(A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A"}, {"heading": "A.2 Detailed Analysis for Section 2.2 in Constrained", "text": "In this section, we extend our result to equilibrium limitation problems under some mild conditions. 172Let us consider the equilibrium limitation problem: min wf (w) (A.63) s.t. ci (w) = 0, i = 1, \u00b7 \u00b7, mDefine the practicable sentence as the set of points that meet all constraints W = {w | ci (w) = 0; i = 1, \u00b7 \u00b7, m}.In this case, the algorithm we are executing is Projected Noisy Gradient Descent. Let us consider the function W (v) as a projection on the practicable sentence in which the projection is defined as a global solution of minw \u00b2 \u2212 v \u2212 w \u00b2. With the same argument as in the unrestricted case, we could simplify the approach somewhat and convert it into the standard projected stochastic gradient descend (PSGD) with updated equation: vt = wt \u2212 f \u2212.With the same argument as in this unrestricted case, we can simplify the Ejvt Z = 64 with the first step \u2212 T."}, {"heading": "A.2.1 Preliminaries", "text": "It is not as if we are able to solve an unrestricted problem by transforming it into an unrestricted problem by making it an unrestricted problem. Langrangian L can be written asL (w) -m) -m (w) -m) -i (A.66) Then, if LICQ applies to all, we can properly define the function."}, {"heading": "A.2.2 Geometrical Lemmas Regarding Constraint Manifold", "text": "In this section we provide some technical lemmas which refer to the geometry of the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in the condition in"}, {"heading": "A.2.3 Main Theorem", "text": "We are now able to examine the main theorems. (...) We are ready to examine the definition of the strict saddle definition. (...) We are ready to examine the main theorems. (...) We are ready to review the definition of the strict saddle definition. (...) We are ready to examine the definition of the strict saddle definition. (...) We are not ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition. (...) We are ready to accept the definition."}, {"heading": "A.3 Detailed Proofs for Section 2.3", "text": "In this section we show two optimization problems (2,9) and (2,11), which satisfy the strict saddle characteristics (\u03b1, \u03b3, \u0442, \u03b4)."}, {"heading": "A.3.1 Warm Up: Maximum Eigenvalue Formulation", "text": "We remember that we try to solve the optimization (2.9), which we first restate here.max T (u, u, u), (A.140), (A.140), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A, A, A, A, A, A, A, A (A), A, A, A, A, A, A (A), A, A, A (A), A, A, A, A, A, A, A (A), A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A (A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A"}, {"heading": "A.3.2 New Formulation", "text": "In this section we consider our new formulation (2,11). We repeat in the beginning the optimization problem here: min + i = j = j = j = j = j (i), u (i), u (i), u (j), u (j), u (j), u (j), u (u), u (u (u), u (j), u (j), u (j), u (j), u (u), u (a), u (u), u (u (u), u (u (u), u (u (u), u (u (u), u (u (u), u (u (u), u (u (u), u (u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (u), A.u (u), A.u (u), A.u (u), A.u (u), A.u (u), A.u (u), A.u (u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A.u), A.u (A.u), A.u (A.u), A.u (A.u (A."}, {"heading": "A.3.3 Extending to Tensors of Different Order", "text": "This year, it is up to us to put ourselves at the top of the ranking, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "Appendix B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix for Applying Online Tensor", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Methods for Learning Latent Variable", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Stochastic Updates", "text": "After reaching the whitening matrix, the data G x, A, G x, B and G x, C are updated by linear operations to ytA, y t B and y t C Rk: ytA: = G x, A, W, ytB: = ZBG x, B, W, ytC: = ZCG x, C.wo x X and t the index of online data.212The stochastic gradient descending algorithm is determined by deriving the loss function L t (v) t vi (v) v vi = k j j j j = 1 < vj, vi > 2 vj \u2212 (0 + 1) (vi + 2 vi vi, y tvi t vi, y ti ti ti ti ti ti ti ti ti, i i i s)."}, {"heading": "B.2 Proof of Algorithm Correctness", "text": "First we calculate M2 as justEx [G x, C G x, B A, C], where we get G x, B: = Ex [G x, A G x, C x, B x, B x, B x, B x, C x, B x, B \u2212 C]) \u2020 G x, BG x, C x, C: = Ex [G x, A G x, B x, B (G x, B x, B \u2212 C]) \u2020 G x, C.Define FA as FA: = = FA AP, we get M2 = E [G x, A x, A x, A] = AP (A x, A x)."}, {"heading": "B.3 GPU Architecture", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "B.4 Results on Synthetic Datasets", "text": "Homophily is an important factor in social interactions [119]; the term homophily refers to the tendency for actors in the same community to interact more than between different communities. Therefore, we assume the diagonally dominated connectivity matrix P with diagonal elements equal to 0.9 and extra-diagonal elements equal to 0.1. Note that P does not have to be stochastical or symmetrical. Our algorithm allows the randomly generated connectivity matrix P with support [0, 1]. In this way, we look at general directed social bonds between communities. We conduct experiments for both the stochastic block model (\u03b10 = 0) and the mixed membership model. For the mixed membership model, we set the concentration parameter \u03b10 = 1.219We find that the error is about 8% -14% and the maturities are less than one minute when n \u2264 10000 and n \u0445 k. The results are given in Table B.1."}, {"heading": "B.5 Comparison of Error Scores", "text": "(1) The empirical distribution of the basic truths is easy to obtain. (1) Similarly, the empirical distribution of the estimated membership in the community is categorizing. (1) The empirical distribution of the basic truths is easy to obtain. (1) The empirical distribution of the estimated membership in the community is categorizing. (1) The empirical distribution of the basic truths is categorizing. (1) Categorizing the empirical distribution of the basic truths. (2) Categorizing the empirical distribution of the estimated membership. (1) Categorizing the NMI for block building. (2) Categorizing the empirical distribution of the basic truths. (1) Categorizing the empirical distribution of the basic truths. (2) Categorizing the empirical distribution of the basic truths. (1) Categorizing the empirical distribution of the basic truths."}, {"heading": "Appendix C", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix for Dictionary Learning via", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Convolutional Tensor Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Cumulant Form", "text": "In [12] it is proven that in the ICA model the cumulant of observation x is transformed into a multilinear transformation of a diagonal cumulant of the h. Therefore, we strive to use the cumulant of third order for input x. As we know, the moment of the rth order for variable x is defined as a micro: = E [x r] cumulant of third order Rn \u00b7 n \u00b7 n (C.1) Let us use [\u00b53] i, j, k \u2212 [\u00b52] i, j [\u00b51] k \u2212 [\u00b52] th entry of the moment of third order. The relationship between cumulant moment of third order 3 is [3] i, j, k \u2212 [\u00b53] i, j \u2212 [\u00b51] k \u2212 [C.2] i \u2212 [\u00b52] i, k [\u00b51] j \u2212 C \u2212 [\u00b52] of the cumulant third order (\u00b52] J \u2212 E, [4] of the cumulant F [4] b] b] b] b] that the format \u2212 si shifts."}, {"heading": "C.2 Proof for Main Theorem 4.1", "text": "Our optimization problem ismin F-C3-FB (H-G), (C.6), giving for simplicity D-value (H-G). Therefore, the goal is to minimize the SVD from D-value to D-value (PQ). Since the Frobenius standard remains invariant under orthogonal transformations and the full diagonal matrix (57), it is 225 that the SVD can be achieved from D-value C3-value (FD-value) 2F = PQ-value C3QE-value 2F = orthogonal transformations and full diagonal matrix (57). Therefore, the optimization problem is equivalent in (4,7)."}, {"heading": "C.3 Parallel Inversion of \u03a8", "text": "We propose an efficient iterative algorithm for calculating the problem using block matrix inversion theorem (68).Lemma C.1. (Parallel inversion of row and column diagonal matrix) Let JL = 1 be divided into a block form: JL = JL \u2212 1 OR blkLL (B), (C.10) 226where O: = blk1L (B)... blkL \u2212 1L (B), and R: [blk1L \u2212 1 (B),.., blk L \u2212 1 (B), blkLL \u2212 1), the O (n) time with O (n) processors is achieved there inverse time."}, {"heading": "Appendix D", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix for Latent Tree Learning", "text": "about Hierarchical Tensor Method"}, {"heading": "D.1 Additivity of the Multivariate Information Dis-", "text": "Remember that the additive information gap between nodes two categorical variables xi and xj was defined in [41]. We extend the notation of the information gap to high-dimensional variables by definition 5.1 and present the proof of its additivity in Lemma 5.1 here."}, {"heading": "Proof.", "text": "E [xax c] = E [xax c | xb] = b = b (b) b = b (b) (xbx b] B Let's look at three nodes a, b \u2212 c so that there are edges between a and b \u2212 and b \u2212 and c. Let's keep A = E (xa | xb) and B = E (xc | xb) fully ranked. () (E) (xax a), 228E (xax b) and E (xcx c c c) are fully ranked ((E (xax c)) (E (xax a))) (dist (c), vax c) ((), vc) (((), v (E) (), v (e), v (e)), vax (e \u2212 b \u2212 ax b))))) ((((((), v (E), vc), v () (), v (())"}, {"heading": "D.2 Local Recursive Grouping", "text": "The local recursive grouping (LRG) algorithm is a local division and conquer procedure for learning the structure and parameters of the latent tree (algorithm 6). We perform recursive grouping simultaneously on the sub-trees of the MST. Each of the sub-trees consists of an internal node and its adjacent nodes. We track the internal nodes of the MST and its neighbors. The resulting latent sub-trees according to LRG can easily be merged to recover the last latent tree. Let's consider a pair of adjacent sub-trees in the MST. They have two common nodes (the internal nodes) that are neighbors on MST. First, we identify the path from one internal node to the other in the trees, and then compose the multivariate information between the internal nodes and the introduced hidden nodes."}, {"heading": "D.3 Proof Sketch for Theorem 5.1", "text": "We argue for the accuracy of the method under exact moments. The example complexity follows from the previous work. To clarify the evidence ideas, we define the concept of the replacement node [41] as the following. 231Definition D.1. In other words, the replacement node for a hidden node is an observable node that has the minimum multivariate information removal from the hidden node. See Figure 5.2 (a), the replacement node of h1, Sg (h1; T), is a node that has the minimum multivariate information removal from the hidden node. See Figure 5.2 (a), the replacement node of h1, Sg (h1; T), is v3, Sg (h3; T) = Sg (h3; T) = Sg (h3; T) is the probability of the hidden node."}, {"heading": "D.4 Proof of Correctness for LRG", "text": "(D) D \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o. \"o\" o \"o\" o. \"o\" o. \"o\" o \"o.\" o \"o.\" o \"o\" o. \"o\" o \"o\" o. \"o\" o \"o\" o \"o.\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o. \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o.\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o"}, {"heading": "D.5 Cross Group Alignment Correction", "text": "The first triplet is also formed by three nodes: reference nodes in group 1, x1, non-reference nodes in group 1, x2 and reference nodes in group 2, x3. Let us use h1 to designate the parent node in group 1, and h2 the parent node in group 2. From trip (x1, x2, x3) we get P (h1 | x1) = A, P (x2 | h1) = B and P (x3 | h1) = EA (x3 | h2) P (h2) = DE. From mutation (x3, x4, x1) we can know that P (x3 | h1) and P (x3 | h2) = D (1) = D = D = D = D = D = D (1)."}, {"heading": "D.6 Computational Complexity", "text": "We remember some notations here: d is the observable node dimension, k is the hidden node dimension (k), N is the number of samples, p is the number of observable nodes, and z is the number of non-zero elements in each sample. Calculating the product x1x T 2 from a single sample for nodes 1 and 2 requires sparse matrix multiplications to calculate the paired second moments. Each observable node has a d \u00b7 N sample-like matrix with z-zeros per column. Calculating the product x1x T 2 from a single sample requires O (z) time, and there are N such sample-pair products that lead to O (Nz) time. There are O (p2) node pairs, and therefore the degree of parallelism is O (p2)."}, {"heading": "D.7 Sample Complexity", "text": "From [6] we remember the number of samples required to restore the tree structure that corresponds to the soil truth (for a precise definition of consistency see definition 2 of [41])."}, {"heading": "Lemma D.4. If", "text": "N > 200k2B2t (1 \u2212 distmax)) 2 + 7kM2t (1 \u2212 distmax), (D1) then with a probability of at least 1 \u2212 \u03b7, suggested algorithm results T = T, whereas B: = max xi, xj, X {\u221a max, xj, xj, X {4 ln (4E, xi, 2xjx, j], max {E, 2xix, 2xix, i], M: = max xi, xi, xj, X {4 ln (4E, xi, 2, xj, 2) \u2212 Tr (E, xi, 2xix, j, i), max {E, xi, 2xix, 2xix, 2xix, 2xix, i)."}, {"heading": "D.8 Efficient SVD Using Sparsity and Dimensionality", "text": "ReductionWithout loss of generality, we assume that a matrix whose SVD we want to calculate has no row or column that is completely zeros, because if it has zero entries, such rows and columns can be dropped. 238Let a row Rn \u00b7 n be the matrix to do SVD. Let's leave the row Rd \u00b7 k, where k = \u03b1k is a scalar, normally in the range [2, 3]. If the ith line of [i,:) 6 = 0 and [i,:) 6 = 0 and [:, i) 6 = 0, then there is only one entry that is not zero, and that entry is uniformly selected from [k]. If either i | (i,:) = 0 or i |."}, {"heading": "Appendix E", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix for Spatial Point Process", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Mixture model Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1 Morphological Basis Extraction", "text": "We aim to characterize the morphological basis for all cells with different size, orientation, expression profiles and spatial distribution."}, {"heading": "E.1.1 Gaussian Prior Convolutional Sparse Coding", "text": "The popular alternating approach between matching M and k-SVD to learn F is generally applicable to all object recognition problems in image processing. However, this approach causes an inaccurate estimate of cell count as filters with multimodality (i.e. multiple cells) are learned. We solve this problem by proposing a Gaussian probability density function before the filters to guarantee the detection of individual cells and achieve an accurate estimate of cell count. Support for M is also limited to the local maxima specified by cell centers. Note that our cell is not shaped, and it is reasonable to assume that the darkest point is the cell center. 241Therefore, we optimize the objective minimum image frequency. n In \u2212 ig m Fm, m, m, m, m, m, m, m, m, Mnm, 0 so that our cell is not shaped, and the point is reasonable to assume that it is the cell center."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "OF THE DISSERTATION<lb>Discovery of Latent Factors in High-dimensional Data Using Tensor Methods<lb>By<lb>Furong Huang<lb>Doctor of Philosophy in Electrical and Computer Engineering<lb>University of California, Irvine, 2016<lb>Assistant Professor Animashree Anandkumar, Chair Unsupervised learning aims at the discovery of hidden structure that drives the observations<lb>in the real world. It is essential for success in modern machine learning and artificial intel-<lb>ligence. Latent variable models are versatile in unsupervised learning and have applications<lb>in almost every domain, e.g., social network analysis, natural language processing, computer<lb>vision and computational biology. Training latent variable models is challenging due to the<lb>non-convexity of the likelihood objective function. An alternative method is based on the<lb>spectral decomposition of low order moment matrices and tensors. This versatile framework<lb>is guaranteed to estimate the correct model consistently. My thesis spans both theoretical<lb>analysis of tensor decomposition framework and practical implementation of various appli-<lb>cations. This thesis presents theoretical results on convergence to globally optimal solution of tensor<lb>decomposition using the stochastic gradient descent, despite non-convexity of the objective.<lb>This is the first work that gives global convergence guarantees for the stochastic gradient<lb>descent on non-convex functions with exponentially many local minima and saddle points. This thesis also presents large-scale deployment of spectral methods (matrix and tensor<lb>decomposition) carried out on CPU, GPU and Spark platforms. Dimensionality reduction<lb>techniques such as random projection are incorporated for a highly parallel and scalable<lb>xvi tensor decomposition algorithm. We obtain a gain in both accuracies and in running times<lb>by several orders of magnitude compared to the state-of-art variational methods. To solve real world problems, more advanced models and learning algorithms are proposed.<lb>After introducing tensor decomposition framework under latent Dirichlet allocation (LDA)<lb>model, this thesis discusses generalization of LDA model to mixed membership stochastic<lb>block model for learning hidden user commonalities or communities in social network, con-<lb>volutional dictionary model for learning phrase templates and word-sequence embeddings,<lb>hierarchical tensor decomposition and latent tree structure model for learning disease hierar-<lb>chy in healthcare analytics, and spatial point process mixture model for detecting cell types<lb>in neuroscience.", "creator": "LaTeX with hyperref package"}}}