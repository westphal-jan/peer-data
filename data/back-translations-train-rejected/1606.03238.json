{"id": "1606.03238", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks", "abstract": "Here, we present IDNet, an original user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from her/his way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. Our design features several innovations including: a robust and smartphone-orientation-independent walking cycle extraction block, a novel feature extractor based on convolutional neural networks, a one-class support vector machine to classify walking cycles, and the coherent integration of these into a multi-stage authentication system. To the best of our knowledge, our system is the first exploiting convolutional neural networks as universal feature extractors for gait recognition, and using classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% in fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the authentication performance.", "histories": [["v1", "Fri, 10 Jun 2016 09:14:28 GMT  (1410kb)", "https://arxiv.org/abs/1606.03238v1", null], ["v2", "Wed, 15 Jun 2016 13:15:53 GMT  (1624kb)", "http://arxiv.org/abs/1606.03238v2", null], ["v3", "Wed, 19 Oct 2016 12:11:57 GMT  (3094kb)", "http://arxiv.org/abs/1606.03238v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["matteo gadaleta", "michele rossi"], "accepted": false, "id": "1606.03238"}, "pdf": {"name": "1606.03238.pdf", "metadata": {"source": "CRF", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks", "authors": ["Matteo Gadaleta", "Michele Rossi"], "emails": ["gadaleta@dei.unipd.it", "rossi@dei.unipd.it"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.03 238v 3 [cs.C V] 19 OHere we introduce IDNet, a framework for user authentication based on smartphone-acquired motion signals. Its goal is to identify a target user from his or her walkway using the accelerometer and gyroscope (inertial signals) provided by a commercial smartphone that is carried in the front pocket of the user. IDNet has several innovations, including: i) a robust and smartphone-oriented walking cycle extraction block, ii) a novel feature extractor based on Convolutionary Neural Networks, iii) a single-class support vector machine for classifying walking cycles and their coherent integration into iv) a multi-level authentication technology. IDNet is the first system to combine a deep learning approach as an extractor for subsequent cycle classification and subsequent decision-making."}, {"heading": "1. Introduction", "text": "The idea behind this is that people throughout the world who are in a position to realize themselves, to put themselves in another world, in which they can take something away from themselves and others, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which, in which they, in which they, in which, in which they, in which they, in which, in which they, in which they, in which, in which they, in which they, in which, in which they, in which, in which they, in which, in which, in which they live, in which"}, {"heading": "2. Related Work", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "3. Signal Processing Framework", "text": "The aim of IDNet is to correctly recognize a subject from his / her way of walking, by acquiring inertial signals from a standard smartphone. The proposed processing method is presented in Figure 1. The proposed processing method is first captured, then we perform some pre-processing methods: 1. pre-filtering to remove motion artifacts (Section 3.1), 2. extracting walking cycles (Section 3.2), 3. a transformation to move the raw walking data into a new orientation system (Section 3.3), 4. a normalization to represent each walking cycle (accelerometer and gyroscope data) by fixed length, zero mean and unit variance vectors (Section 3.4).The walking cycles are ready to identify the user. The standard approach, called \"Classical Machine Learning,\" is the compilation of a number of pre-established statistical features, most of which are used to learn informative techniques and classification."}, {"heading": "3.1. Data Acquisition and Filtering", "text": "In fact, most of us are able to survive on our own without having to move to another world."}, {"heading": "3.2. Extraction of Walking Cycles", "text": "This minimum is then refined by searching for the first minimum of this signal. (1) To identify the first minimum of this signal, a reference point must be located in amag (i). (1) To do this, we must first amag (i) 2 + 1 (i) (2 + 2 (i) 2 + 2 (1) 2 + 2 (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "3.3. Orientation Independent Transformation", "text": "To explore the new orientation, we must adopt the acceleration signals obtained during the path of the acceleration signals. (...) We assume that the acceleration signals we receive during the path from the acceleration signal acquisition to the acceleration signals from the acceleration signal acquisition to the acceleration signal acquisition to the acceleration signals from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration signal to the acceleration signal acquisition from the acceleration to the acceleration signal acquisition from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration to the acceleration signal acquisition from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration to the acceleration signal acquisition from the acceleration to the acceleration signal acquisition from the acceleration signal acquisition from the acceleration signal acquisition to the acceleration signal acquisition from the acceleration to the acceleration from the acceleration direction from the acceleration direction of the acceleration direction to the acceleration signal acquisition from the acceleration to the acceleration of the acceleration signal acquisition."}, {"heading": "3.4. Normalization", "text": "Each gait cycle has a different duration, which depends on the stride speed and stride length. Considering the acceleration and gyroscopic data collected during a complete walking cycle, we therefore stick to variable-size acceleration and gyroscopic vectors, which are now expressed in the new orientation invariant coordinate system discussed in Section 3.3. However, since extraction and classification algorithms for features require N-size vectors for each cycle where N must be fixed, further adjustment is needed. We manage this cycle length variability by means of further spline interpolation to represent all walking cycles by vectors of N = 200 samples each. This specific value of N was selected to avoid aliasing. In fact, assuming a maximum cycle duration of 1 = 2 seconds, which corresponds to a very slow gait, and a signal bandwidth of B = 40 Hz, a number of samples of > N = 2160 samples was selected."}, {"heading": "4. Convolutional Neural Network", "text": "In this section, we present the chosen architecture of the Convolutional Neural Network (CNN) for IDNet (Section 4.1), its optimization, training, and its quantitative comparison with literature (Section 4.2)."}, {"heading": "4.1. CNN Architecture", "text": "In fact, most people are able to understand themselves and what they are doing to change the world."}, {"heading": "4.2. CNN Optimization and Results", "text": "In this context, it is important that we are able to set out in search of new ways. (...) In this sense, it is important that we set out in search of new ways. (...) We must set out in search of new ways. (...) We must set out in search of new ways. (...) We must set out in search of new ways. (...) We must set out in search of new ways. (...) We must set out in search of new ways. (...) We must set out in search of new ways. (...) \"(...) We must set out in search of new ways. (...) We must set out in search of new ways.\" (...) We must set out in search of new ways. \"(...)\" We must set out in search of new ways. \"(...)\" (...) We must set out in search of new ways. \"(...) We must set out in search of new ways.\" (...) We must set out in search of new ways. \"(...) We must set out in search of new ways.\""}, {"heading": "5. One-Class Support Vector Machine Training", "text": "In this section, we extend the IDNet CNN-based authentication chain by designing an SVM classifier that is trained solely on the target's motion data. This is known as a One-Class Classification (OCC) and is important for practical applications where motion signals from the target user are available but those from other subjects are not. More importantly, this approach can extend the classification system to users not included in the CNN training."}, {"heading": "5.1. Revised Classification Architecture", "text": "To take advantage of this, we discard the output neurons of FL2 and use CNN as a tool to reduce dimensionality, which, given an input matrix X, provides a user-dependent feature vector f. CNN is then trained only once, taking into account the optimizations of Section 4.2. All of its weights and distortions are then pre-calculated and are not modified during classification. In view of the diagram of Fig. 6, we receive the feature vector f at the output of CNN. We then apply a feature selection block to reduce the number of features from F to S \u2264 F (Dimensionality Reduction). PCA is used to accomplish this task and the new feature vector is called s. Therefore, we have s = (f), where RF \u2192 RS is the PCA target (Dimensionality Reduction)."}, {"heading": "5.2. One-Class SVM Design", "text": "s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,"}, {"heading": "6. Multi-Stage Authentication", "text": "The processing pipeline discussed so far provides a score for each walking cycle. However, as shown in Figure 14, when a score falls close to the point at which the two pdfs intersect, there is a high degree of uncertainty about the identity of the user who created it. In IDNet, we resolve this uncertainty by collectively taking into account the values from successive walking cycles. Let O = (o1, o2,.) be a sequence of subsequent OSVM values from the same subject, where oi = h (si) and i = 1, 2,. is the walking cycle index. From our previous analysis, we can assume that oi is a random process with a probability density function (h (si)) = p\u03b8 (oi), 0 (si), and our goal is to reliably obtain the results from the values in O. In the direction of this assumption, we assume that subsequent values belong to the same user and that they are distributed identically (i.i.e)."}, {"heading": "6.1. Experimental Results", "text": "The movement data of K = 35 subjects were used to train the CNN feature extractor, with Nc = 40, F = 40 and S = 20. One user of the remaining 15 was considered a target user and 14 as negatives for the final tests. The following results are obtained through a leave-one-out cross-validation approach for the target user's sessions, i.e., out of twelve sessions, eleven are used for training and one for the final tests. The session that is omitted is rotated and the final results are averaged across all studies. The multi-level framework authentication results are shown in Fig. 15. Incorrect positive rates (i.e. a user is falsely authenticated as a target) and false negative values (i.e. the target is not recognized) are less than 0.15% for an appropriate selection of SPRT thresholds (\u03b1 and \u03b2)."}, {"heading": "7. Conclusions", "text": "In this paper, we proposed IDNet, a framework for user authentication for inertial signals captured by smartphones. In sharp contrast, IDNet uses Convolutionary Neural Networks because they enable automatic feature engineering and have excellent generalization.These deep neural networks are then used as universal feature extractors to feed classification techniques, combining them with single-class support vector machines and a novel multi-step decision algorithm.With our framework, the neural network is trained once and for all and then used for new users.The one-class classifier is trained exclusively on the movement data of the target; it provides a score that weighs the deviation of newly acquired data from the target, and subsequent scores are then accumulated through a multi-step decision approach. Experimental results show the superiority of IDNet over previous work, resulting in mitigation rates that are less than five percent compared to existing optimization cycles."}], "references": [{"title": "Inertial sensor-based gait recognition: A review", "author": ["S. Sprager", "M.B. Juric"], "venue": "Sensors 15 (9) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of biometric gait recognition: Approaches", "author": ["D. Gafurov"], "venue": "security and challenges, in: Annual Norwegian computer science conference, Oslo, Norway", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition 47 (11) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust arbitrary view gait recognition based on parametric 3D human body reconstruction and virtual posture synthesis", "author": ["J. Luo", "J. Tang", "T. Tjahjadi", "X. Xiao"], "venue": "Pattern Recognition 60 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Complete canonical correlation analysis with application to multi-view gait recognition", "author": ["X. Xing", "K. Wang", "T. Yan", "Z. Lv"], "venue": "Pattern Recognition 50 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncooperative gait recognition: Re-ranking based on sparse coding and multi-view hypergraph learning", "author": ["X. Chen", "J. Xu"], "venue": "Pattern Recognition 53 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust view-invariant multiscale gait recognition", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 48 (3) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Gait Analysis: An Introduction", "author": ["M.W. Whittle"], "venue": "4th ed., Elsevier: Edinburgh, UK", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating and overcoming the challenges in utilizing smart mobile phones and standalone accelerometer for gait analysis", "author": ["H. Chan", "H. Zheng", "H. Wang", "R. Sterritt"], "venue": "in: IET Irish Signals and Systems Conference ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "CNN Features Off-the- Shelf: An Astounding Baseline for Recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition Workshops, Columbus, Ohio, US", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Gait identification using accelerometer on mobile phone", "author": ["H.M. Thang", "V.Q. Viet", "N.D. Thuc", "D. Choi"], "venue": "in: International Conference on Control, Automation and Information Sciences (ICCAIS), Saigon, Vietnam", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Authentication of smartphone users based on the way they walk using k-nn algorithm", "author": ["C. Nickel", "T. Wirtl", "C. Busch"], "venue": "in: International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), Piraeus-Athens, Greece", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Influence of Holding Smart Phone for Acceleration-Based Gait Authentication", "author": ["Y. Watanabe"], "venue": "in: International Conference on Emerging Security Technologies (EST), Houston, Texas, US", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Biometric gait recognition based on wireless acceleration sensor using k-nearest neighbor classification", "author": ["S. Choi", "I.H. Youn", "R. LeMay", "S. Burns", "J.H. Youn"], "venue": "in: International Conference on Computing, Networking and Communications (ICNC), Honolulu, Hawaii, US", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Smartphone based user verification leveraging gait recognition for mobile healthcare systems", "author": ["Y. Ren", "Y. Chen", "M.C. Chuah", "J. Yang"], "venue": "in: IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks (SECON), New Orleans, Louisiana, US", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "An Efficient HOS-Based Gait Authentication of Accelerometer Data", "author": ["S. Sprager", "M.B. Juric"], "venue": "IEEE Transactions on Information Forensics and Security 10 (7) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Smart mobile phone based gait assessment of patients with low back pain", "author": ["H. Chan", "H. Zheng", "H. Wang", "R. Sterritt", "D. Newell"], "venue": "in: Ninth International Conference on Natural Computation (ICNC), San Diego, California, US", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Gait analysis by using tri-axial accelerometer of smart phones", "author": ["G.-S. Huang", "C.C. Wu", "J. Lin"], "venue": "in: International Conference on Computerized Healthcare (ICCH), Hong Kong, China", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Scenario test of accelerometerbased biometric gait recognition", "author": ["C. Nickel", "M.O. Derawi", "P. Bours", "C. Busch"], "venue": "in: International Workshop on Security and Communication Networks (IWSCN), Gj\u00f8vik, Norway", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Using hidden markov models for accelerometer-based biometric gait recognition", "author": ["C. Nickel", "C. Busch", "S. Rangarajan", "M. Mobius"], "venue": "in: IEEE 7th International Colloquium on Signal Processing and its Applications (CSPA), Penang, Malaysia", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Rotation invariant feature extraction from 3-D acceleration signals", "author": ["T. Kobayashi", "K. Hasida", "N. Otsu"], "venue": "in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Prague, Czech Republic", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J.C. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation 13 (7) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Walking patterns of normal men", "author": ["M.P. Murray", "A.B. Drought", "R.C. Kory"], "venue": "The Journal of Bone & Joint Surgery 46 (2) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1964}, {"title": "Gait as a total pattern of movement: Including a bibliography on gait", "author": ["M.P. Murray"], "venue": "American Journal of Physical Medicine & Rehabilitation", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1967}, {"title": "M", "author": ["T. Nixon"], "venue": "S. ans Tieniu, C. Rama, Human identification based on gait, Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Cell phone-based biometric identification", "author": ["J.R. Kwapisz", "G.M. Weiss", "S.A. Moore"], "venue": "in: Fourth IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying users of portable devices from gait pattern with accelerometers", "author": ["J. Mantyjarvi", "M. Lindholm", "E. Vildjiounaite", "S.M. Makela", "H.A. Ailisto"], "venue": "in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Philadelphia, Pennsylvania, US", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Unobtrusive user-authentication on mobile phones using biometric gait recognition", "author": ["M.O. Derawi", "C. Nickel", "P. Bours", "C. Busch"], "venue": "in: 6th International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), Darmstadt, Germany", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact indexing of dynamic time warping", "author": ["E. Keogh", "C. Ratanamahatana"], "venue": "Knowledge and Information Systems 7 (3) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Gait-id on the move: Pace independent human identification using cell phone accelerometer dynamics", "author": ["F. Juefei-Xu", "C. Bhagavatula", "A. Jaech", "U. Prasad", "M. Savvides"], "venue": "in: Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS), Washington DC, US", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "The possibility of normal gait analysis based on a smart phone for healthcare", "author": ["S. Jiang", "B. Zhang", "G. Zou", "D. Wei"], "venue": "in: IEEE International Conference on Green Computing and Communications (GreenCom), Internet of Things (iThings), and Cyber, Physical and Social Computing (CPSCom), Beijing, China", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Sensor orientation invariant mobile gait biometrics", "author": ["Y. Zhong", "Y. Deng"], "venue": "in: IEEE International Joint Conference on Biometrics (IJCB), Clearwater, FL, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Largescale Video Classification with Convolutional Neural Networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, US", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "The largest inertial sensor-based gait database and performance evaluation of gait-based personal authentication", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 47 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Personalization and user verification in wearable systems using biometric walking patterns", "author": ["P. Casale", "O. Pujol", "P. Radeva"], "venue": "Personal and Ubiquitous Computing 16 (5) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "T", "author": ["J. Tilmanne", "R. Sebbe"], "venue": "Dutoit, A database for stylistic human gait modeling and synthesis ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "D", "author": ["J. Frank", "S. Mannor"], "venue": "Precup, Data sets: Mobile phone gait recognition data ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short", "author": ["P.D. Welch"], "venue": "modified periodograms, IEEE Transactions on Audio and Electroacoustics 15 (2) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1967}, {"title": "Pem-id: Identifying people by gait-matching using cameras and wearable accelerometers", "author": ["T. Teixeira", "D. Jung", "G. Dublon", "A. Savvides"], "venue": "in: ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC), Como, Italy", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Which Way Am I Facing: Inferring Horizontal Device Orientation from an Accelerometer Signal", "author": ["K. Kunze", "P. Lukowicz", "K. Partridge", "B. Begole"], "venue": "in: IEEE International Symposium on Wearable Computers, Linz, Austria", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Heading Estimation for Indoor Pedestrian Navigation Using a Smartphone in the Pocket", "author": ["Z.-A. Deng", "G. Wang", "Y. Hu", "D. Wu"], "venue": "MDPI Sensors 15 (9) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "The Use and Interpretation of Principal Component Analysis in Applied Research", "author": ["C.R. Rao"], "venue": "Sankhy\u0101: The Indian Journal of Statistics 26 (4) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1964}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series, in: The Handbook of Brain Theory and Neural Networks, MIT Press", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in Neural Information Processing Systems 25", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. M\u00fcller", "S. Behnke"], "venue": "in: 20th International Conference on Artificial Neural Networks (ICANN), Thessaloniki, Greece", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Computer Intensive Methods in Control and Signal Processing: The Curse of Dimensionality", "author": ["R. Hanka", "T.P. Harte"], "venue": "Birkh\u00e4user Boston", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1993}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning 29 (2) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1997}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Transactions on Information Theory 13 (1) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1967}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning 20 (3) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1995}, {"title": "J", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor"], "venue": "C. Platt, et al., Support vector method for novelty detection, Neural Information Processing Systems (NIPS) 12 ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimizing F-Measure with Support Vector Machines", "author": ["D.R. Musicant", "V. Kumar", "A. Ozgur"], "venue": "in: 16-th International FLAIRS Conference, FLAIRS, St. Augustine, Florida, US", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}, {"title": "Artificial Neural Networks and Neural Information Processing", "author": ["D.M.J. Tax", "K.R. M\u00fcller"], "venue": "Springer, Berlin, Heidelberg", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Sequential analysis", "author": ["A. Wald"], "venue": "Dover, New York, NY, US", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1947}, {"title": "Sequential Analysis Hypothesis Testing and Changepoint Detection", "author": ["A. Tartakovsky", "I. Nikiforov", "M. Basseville"], "venue": "CRC Press", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A great deal of work has been carried out on gait recognition in the last decade [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "In general, biometric gait recognition can be grouped into three main categories: 1) computer vision based, 2) floor sensor based and 3) wearable sensor based [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 3, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 4, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 5, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 6, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 7, "context": "As shown in [8, 9], modern phones possess highly accurate inertial sensors, which allow for non-obtrusive gait biometrics.", "startOffset": 12, "endOffset": 18}, {"referenceID": 8, "context": "As shown in [8, 9], modern phones possess highly accurate inertial sensors, which allow for non-obtrusive gait biometrics.", "startOffset": 12, "endOffset": 18}, {"referenceID": 9, "context": "IDNet leverages deep Convolutional Neural Networks (CNN) [10] and tools from machine learning, such as Support Vector Machines (SVM) [11], combining them in an innovative fashion.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "IDNet leverages deep Convolutional Neural Networks (CNN) [10] and tools from machine learning, such as Support Vector Machines (SVM) [11], combining them in an innovative fashion.", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 13, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 14, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 15, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 16, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 14, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 15, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 17, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 11, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 12, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 18, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 19, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 20, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 21, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 285, "endOffset": 289}, {"referenceID": 13, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 14, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 17, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 12, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 22, "context": "\u2022 The combination of CNN-extracted features with a one-class SVM classifier [23], which is solely trained on the target subject, see Section 5.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Murray compared normal gait parameters against those from pathologic gaits [25] and showed that gait is unique to each individual.", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "Most recent works are based on computer vision [26, 2].", "startOffset": 47, "endOffset": 54}, {"referenceID": 1, "context": "Most recent works are based on computer vision [26, 2].", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": ") are of special interest [7].", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 26, "context": ", for task identification [27].", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "A thorough review of the latest developments in this area can be found in Sprager\u2019s work [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "[28] were the first to look at this problem and they did it through accelerometer data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], Derawi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Dynamic Time Warping (DTW) [30] was used as the distance measure, to ensure robustness against non-linear temporal shifts.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "This scheme was also tested in [20], where majority voting and cyclic rotation were compared as inference rules.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "In a further paper [21], Hidden", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "Either gait cycles extraction [31] or fixed windows lengths [13] are possible signal segmentation methods.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Either gait cycles extraction [31] or fixed windows lengths [13] are possible signal segmentation methods.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "However, more advanced features are required for better results, like cepstral coefficients, which are widely used for speech recognition [13], or features extracted through frequency analysis, i.", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": ", using Fourier [12] or wavelet transforms [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": ", using Fourier [12] or wavelet transforms [31].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 12, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 16, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 30, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 230, "endOffset": 238}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 230, "endOffset": 238}, {"referenceID": 18, "context": "showed that signals acquired by a waist-worn device on a patient with cervical disc herniation differed before and after the surgery [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "In [18], classification algorithms were used to discriminate a group of subjects with non-specific chronic low back pain from healthy subjects.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": ", correlation matrices of Fourier transforms [22] or gait dynamic images [33]).", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": ", correlation matrices of Fourier transforms [22] or gait dynamic images [33]).", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The second relies on the transformation of inertial signals [14], projecting them into a new orientation invariant three-dimensional reference system,", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "These have been successfully used by the video processing community [34] but to the best of our knowledge have never been exploited for the analysis of inertial data from wearable devices.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "The largest one was acquired by the Institute of Scientific and Industrial Research (ISIR) at Osaka University (OU) [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "collected accelerometer data from a smartphone positioned in the chest pocket from 22 users walking over a predefined path [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "In [37], a motion capture suit was used to acquire data from 40 subjects walking in a small area at different speeds and with direction changes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "collected data from a mobile phone in the pocket of 20 individuals at McGill University, performing two separate 15 minute walks on two different days [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "2, we plot the power of accelerometer and gyroscope signals at different frequencies through the Welch\u2019s method [39], considering a full walking trace and setting the Hanning window length to 1 s, with half window overlap.", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "To do so, inspired by [16] we first pass amag(i) through a low-pass filter with cutoff frequency", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "Thus, we detect the first minimum of this filtered signal, which corresponds to the heel strike [40], and the corresponding index is called \u0129.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "A template matching approach exploiting a similar rationale was used in [16], where the authors employed the Pearson product-moment correlation coefficient between template and amag(i).", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The main differences between [16] and our approach are: we obtain the template T in a neighborhood of i, using a fixed number of samples Ns, whereas they take the samples between two adjacent minima of \u03c6(i) (which may then differ in size for different cycles).", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "In previous work [16], the template is instead kept unchanged up to a point when minima cannot be longer detected, and a new template is to be obtained.", "startOffset": 17, "endOffset": 21}, {"referenceID": 40, "context": "To this end, we adopt a technique similar to those of [41, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 41, "context": "To this end, we adopt a technique similar to those of [41, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 40, "context": ", it is parallel to the direction of motion, as it was also observed and verified in previous research [41].", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "This is done by applying the Principal Component Analysis (PCA) [43] on the projected points, which finds the direction along which the variance of the measurements is maximized.", "startOffset": 64, "endOffset": 68}, {"referenceID": 43, "context": "For more details the reader is referred to [44].", "startOffset": 43, "endOffset": 47}, {"referenceID": 44, "context": "CNNs have been proven to be excellent feature extractors for images [45] and here we prove their effectiveness for motion data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "Max pooling is applied to the output of CL2 to further reduce its dimensionality and increase the spatial invariance of features [46].", "startOffset": 129, "endOffset": 133}, {"referenceID": 46, "context": "In general, a too small F can lead to poor classification results; too many features, instead, would make the state space too big to be effectively dealt with (curse of dimensionality) [47].", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 187, "endOffset": 191}, {"referenceID": 48, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 210, "endOffset": 214}, {"referenceID": 49, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 243, "endOffset": 247}, {"referenceID": 50, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 12, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 30, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 13, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 51, "context": "The strategy proposed by Sch\u00f6lkopf is to map the data into the feature space of a kernel, and to separate them from the origin with maximum margin [52].", "startOffset": 147, "endOffset": 151}, {"referenceID": 50, "context": "The corresponding minimization problem is similar to that of the original SVM formulation [51].", "startOffset": 90, "endOffset": 94}, {"referenceID": 51, "context": "\u03bd \u2208 (0, 1) is one of the most important parameters and sets an upper bound on the fraction of outliers and a lower bound on the fraction of Support Vectors (SV) [52].", "startOffset": 161, "endOffset": 165}, {"referenceID": 52, "context": ", the fraction of patterns identified of the target class that in fact belong to the target user, while the recall corresponds to the fraction of target patterns that are correctly classified out of the entire positive class of samples [53].", "startOffset": 236, "endOffset": 240}, {"referenceID": 53, "context": "In fact, as pointed out in [54], two options are possible to go from the CNN-extracted feature vector f to s.", "startOffset": 27, "endOffset": 31}, {"referenceID": 53, "context": "This is in accordance with [54].", "startOffset": 27, "endOffset": 31}, {"referenceID": 54, "context": "For the estimation of \u03b8 we use Wald\u2019s probability ratio test (SPRT) [55, 56].", "startOffset": 68, "endOffset": 76}, {"referenceID": 55, "context": "For the estimation of \u03b8 we use Wald\u2019s probability ratio test (SPRT) [55, 56].", "startOffset": 68, "endOffset": 76}, {"referenceID": 54, "context": "Moreover, defining \u03b1 as the probability of accepting H1 when H0 is true and \u03b2 that of accepting H0 when H1 is true, A and B can be approximated as: A = log(\u03b2/(1 \u2212 \u03b1)) and B = log((1\u2212 \u03b2)/\u03b1), see [55].", "startOffset": 194, "endOffset": 198}, {"referenceID": 11, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 12, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 13, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 14, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 15, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 16, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}], "year": 2016, "abstractText": "Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user\u2019s trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.", "creator": "LaTeX with hyperref package"}}}