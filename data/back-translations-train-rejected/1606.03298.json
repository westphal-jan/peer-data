{"id": "1606.03298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages", "abstract": "Reasoning on large and complex real-world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub-models, apply an inference algorithm to each sub-model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods.", "histories": [["v1", "Fri, 10 Jun 2016 12:53:01 GMT  (1033kb,D)", "http://arxiv.org/abs/1606.03298v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["avi pfeffer", "brian ruttenberg", "william kretschmer"], "accepted": false, "id": "1606.03298"}, "pdf": {"name": "1606.03298.pdf", "metadata": {"source": "CRF", "title": "Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages", "authors": ["Avi Pfeffer", "Brian Ruttenberg", "William Kretschmer"], "emails": ["apfeffer@cra.com", "bruttenberg@cra.com", "kretsch@mit.edu"], "sections": [{"heading": null, "text": "Thinking about large and complex models of the real world is a mathematically difficult task, but one that is necessary for the effective use of many AI applications. A wealth of inference algorithms have been developed that work well on certain models or only on parts of general models. Therefore, a system that can intelligently apply these inference algorithms to different parts of a model is highly desirable for quick thinking. We are introducing a new framework called structured factored inference (SFI), which forms the basis for such a system. Using models encoded in a likely programming language, SFI provides a solid means of splitting a model into sub-models, applying an inference algorithm to each sub-model, and combining the resulting information to answer a query. Our results show that SFI is almost as accurate as exact inferences, but retains the advantages of approximate inference methods."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are living, in which they are able to live, in which they are able to live, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in"}, {"heading": "2 RELATED WORK", "text": "Automated algorithm selection is a long-awaited goal in computer science, with possibly Rice's first formulation [7]. As such, it has been applied to a variety of disciplines in this field, such as scientific computing [8], game theory [9], and artificial intelligence issues such as satisfaction [10]. However, most efforts focus on methods to analyze a problem and learn how to apply the single best algorithm to solve a problem. Our SFI framework is complementary to much of this existing work. SFI can break down complex models into smaller sub-models to which these complex learning algorithms can be applied, and potentially perform even better than learning on a single model.Probabilistic inference is unique in that the autonomy of models provides the ability to apply many algorithms to different parts of the problem, with a significant community approach not significant."}, {"heading": "3 PROBABILISTIC PROGRAMMING LANGUAGE", "text": "The basic concepts of SFI are strongly tied to probabilistic programming languages (PPL), and SFI was implemented in a publicly available PPL. Therefore, understanding the PPL semantics is critical to understanding SFI. However, since the focus of this paper is on introducing the SFI concept, we present a simplified and abstract PPL as an explanation. We call this abstract language SimplePPL."}, {"heading": "3.1 SimplePPL Language", "text": "The central concept in SimplePPL is a random variable (RV). Intuitively, an RV is a random process that stochastically produces a value. For the sake of simplicity, we use an atypical language so that an RV can generate any value in the value set T, where T is a countable finite set. A program Q has a set of free RVs FQ and consists of a sequence of definitions of the format RV = expr. The set of RVs defined by Q is called RVQ. An RV is available if it is either in FQ or previously in Q. The set of available RVs in relation to an RV r is called Ar.An expression that defines an RV r is one of the following: \u2022 A value v \u2022 A primitive one that defines a probability distribution, overvalues. Examples are Flip (p), which matches the probability p, and Uniform (x, y)."}, {"heading": "3.2 SimplePPL Semantics", "text": "Although there are clear semantics for recursive programs in SimplePPL, it will suffice for simplicity in this essay to assume that the extensions of a program Q are not recursive and therefore finite. Within the framework of SimplePPL semantics, each program Q defines a conditional probability distribution P (RVQ | FQ). This is achieved by defining for each program Q a conditional distribution P (r | Ar) and then using the chain rule so that P (RVQ | FQ) = equative distribution P (r | Ar) P (r | Ar) P (r | Ar) is defined as follows: \u2022 For r defined by value v, P (r | Ar) assigns probability 1 to v. \u2022 For r defined by a primitive distribution, P (r | Ar) is P (r | VleQP). \u2022 For r defined by applicability (r1, rn, f), Chaery 1."}, {"heading": "4 STRUCTURED FACTORED INFERENCE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Overview", "text": "There is a simple intuition behind SFI: If a model can be divided into smaller sub-models (i.e. programs) that can be solved independently (i.e., marginalize non-relevant variables), then different algorithms can be applied to different parts of a model. Combined with methods for intelligently selecting inference algorithms, this framework could then lead to improved conclusions on a variety of issues. SFI is basically a framework for applying two strategies: a decomposition strategy that splits a model into smaller sub-models, and an inference strategy that appropriately applies an inference algorithm to each sub-model. SFI uses factors to combine information from solved sub-models to answer queries. For example, consider the following model, written in SimplePPL, shown in Figures 1 and 2. We have three RVs defined in Q, a, b, and c. RFI Vc is Vc and a value is generated by L, with the result being generated by L."}, {"heading": "4.2 Model Decomposition", "text": "The following discussion of the SFI is to be seen in the context of a SimplePPL program that aims exclusively at a common distribution of the relevant variables. First, we define two key terms: The SFI uses an RV x when: x-Ar-P (r) 6 = P (r-Ar-X) We rename the set of variables Ur for a variable r as a set of all variables r, either directly or recursively, and this definition can be difficult in a program that resorts to the semantics of SimplePPPPL."}, {"heading": "4.3 Factored Representation", "text": "The use of factors in the SFI has several advantages. First, it provides an interface to communicate the common distribution of a sub-model with other parts of the model. Second, factors make the SFI algorithm agnostic; any algorithm that can calculate a common distribution and return a factor can be used. For example, sample algorithms can also be used that can convert a common distribution into a factor."}, {"heading": "4.3.1 Factor Creation in SimplePPL", "text": "If the variables in Q have been divided into two groups via a decay point, we convert the decay into a factorized representation. Each variable r = RVQ can, using the probability distribution that defines the variable, be converted into a series of factors that describe the generative semantics of the variable. For variables defined as values or primitives, we generate a factor above the support of the variable, using the probability distribution that defines the variable. Finally, for variables defined by r = applying (r1,.., rn, f), we generate a single factor above {r, r1,., rn}, the value of which is 1 if r = f (r1,., rn) and 0 otherwise. Finally, for variables defined by r = chain (r1, f), we generate a set of factors that represent the common distribution of {r, r1, r1,.,.,.)."}, {"heading": "4.3.2 SFI with Factors", "text": "We refer to the set of factors created from a program Q as \"Q\" as \"Q\" and \"Q\" as \"Q\" as \"Q.\" We refer to the set of factors created from a program Q as \"Q\" as \"Q\" as \"Q\" and \"Q\" as \"Q.\" We refer to the set of factors created from a program Q as \"Q.\" Q \"(Q) as\" Q. \"Q\" (Q) as \"Q.\" Q \"(Q) as\" Q. \"Q\" (Q) as \"Q\" (Q) as \"Q.\" Q \"(Q) as\" Q. \"Q\" (Q) as \"Q.\" Q \"(Q) as\" Q. \"Q\" (Q) as \"Q.\" Q \"(Q) as\" Q \"(Q) as Q\" (Q) as Q \"(Q),\" Q \"(Q) as Q\" (Q)."}, {"heading": "5 USING SFI", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 SFI Operation", "text": "Algorithm 1 outlines conclusions in SFI. To query the distribution via a Q factor, a user calls the SFI function with program Q (written in SimplePPL), q, a decomposition strategy DS, and a consequence strategy IS. DS and IS are functions that guide the decomposition and conclusion of the model, and are explained in more detail below. SFI function calls the decomposition function, and the resulting factor over q is normalized to calculate P (q). The decomposition function visits each decomposition point in Q, applies DS to the submodel (i.e. the program) that has been defined. Algorithm 2 A recursive decomposition strategy function RECURSIVEDECOMPOSITION (program Q, variables E, istrategy IS) yields decomposition (Q, E, dies, IS) end functions in each point and marginalizes the internal variables using IS."}, {"heading": "5.2 Strategies for Decomposition", "text": "A decomposition strategy DS is a method that defines how a program should be decomposed. It is a function that receives a program Q \"and a set of relevant variables EQ\" and returns a set of factors \"EQ\" over at least EQ. \"The simplest DS is what we call\" Increase \": if each d is raised, we get a\" flat \"strategy. Thus, the typical conclusion works: Factors are created for all variables and all non-query variables are marginalized in a flat operation. In order to take advantage of various inference algorithms, it is clearly beneficial to have a DS that actually reduces the number of variables in the returned factor set by defining the composition of the variables in the returned unit EQ.\" As such, we define a recursive strategy as one that applies the hierarchically."}, {"heading": "5.3 Strategies for Inference", "text": "A strategy of inference applies an inference algorithm to a number of factors defined by program Q, and provides a common distribution via E, the set of external variables in the factors. While SFI uses factors that pass the common distribution on to other programs, there is no restriction that an algorithm acts on factors. As long as the algorithm can absorb factors from other decompositions and output a common factor via E, any algorithm can be used. SimplePPPL's implementation of SFI uses factor-based algorithms. There are three algorithms available: Variable Elimination (VE) [2], Faith Propagation (BP) [15], and Gibbs Sampling (GS) [16]. VE and BP are standard implementations of these algorithms based on factors, and as such we do not provide details. GS is implemented based on a number of factors, but integration into SFI is not vial."}, {"heading": "5.3.1 Choosing an Algorithm", "text": "SFI provides the ability to develop schemes that dynamically select the best inference algorithm for a decomposition point that serves as the basis for an automated inference frame. By applying the inference strategy, it is possible to analyze and estimate the complexity of different algorithms applied to factors, and the algorithm with the least estimated runtime can chose.We created a simple heuristics to choose an inference algorithm, but nevertheless it shows the potential of the approach. As VE is an exact algorithm, it is always preferred to other algorithms when it is not too expensive, but unfortunately it is impractical for most problems."}, {"heading": "6 EXPERIMENTS", "text": "We are dealing with three models. First, we have developed a version of the QMR model."}, {"heading": "6.1 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 QMR and Mixed Model Results", "text": "We have two strategies, flat and hierarchical, based on the QMR model with BP and VE. For the hierarchical version of BP, each instance of BP ran for 10 iterations, while the results of the hierarchical strategy are usually faster than the flat ones. Mathematically, both strategies are able to pursue the same strategy."}, {"heading": "6.1.2 Seismic Monitoring Results", "text": "On the seismic monitoring model, we performed two experiments in which we either varied the number of false detection factors (Fig. 6) or discretization in the models (Fig. 7). Fig. 6 shows the duration of our hybrid VE / BP-15 strategy, the hierarchical VE, and the flat versions of VE and BP-30 as the number of false detections in the seismic model increased. That is, each test increased noise in the model, making it more difficult to detect the true seismic events and significantly increase the complexity of the conclusions. On the hybrid strategy test, the SFI framework ran for most of the submodels with the exception of BP on some submodels with large tree widths. As the two versions of VE ran out of storage, we are unable to determine the truth values (variation in accuracy between VE / BP-15 and BP-30)."}, {"heading": "7 CONCLUSION", "text": "Using simple heuristics to analyze the complexity of sub-models, we have shown that SFI can be used to implement a basic automated inference scheme that is faster than approximate conclusions and almost as accurate as exact inference methods. This work serves as a starting point for a more robust automated inference scheme, but more analysis is needed. First, there needs to be a more theoretical analysis of the criteria to explain that a sub-model is \"solved,\" that is, how many iterations of GS or BP should be applied to each sub-model."}, {"heading": "Acknowledgements", "text": "This work was supported by the DARPA contract FA8750-14C-0011."}], "references": [{"title": "Understanding the metropolis-hastings algorithm", "author": ["S. Chib", "E. Greenberg"], "venue": "The american statistician, vol. 49, no. 4, pp. 327\u2013335, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 1, no. 1-2, pp. 1\u2013305, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Rao-blackwellised particle filtering for dynamic bayesian networks", "author": ["A. Doucet", "N. De Freitas", "K. Murphy", "S. Russell"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 2000, pp. 176\u2013 183.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Effective bayesian inference for stochastic programs", "author": ["D. Koller", "D. McAllester", "A. Pfeffer"], "venue": "AAAI/IAAI, 1997, pp. 740\u2013747.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "The principles and practice of probabilistic programming", "author": ["N.D. Goodman"], "venue": "Proceedings of the 40th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages. ACM, 2013, pp. 399\u2013402.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "The algorithm selection problem", "author": ["J.R. Rice"], "venue": "Advances in Computers, vol. 15, pp. 65\u2013118, 1976.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1976}, {"title": "Pythia-ii: a knowledge/database system for managing performance data and recommending scientific software", "author": ["E.N. Houstis", "A.C. Catlin", "J.R. Rice", "V.S. Verykios", "N. Ramakrishnan", "C.E. Houstis"], "venue": "ACM Transactions on Mathematical Software (TOMS), vol. 26, no. 2, pp. 227\u2013253, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning techniques for automatic algorithm portfolio selection", "author": ["A. Guerri", "M. Milano"], "venue": "ECAI, vol. 16, 2004, p. 475.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Satzilla: portfolio-based algorithm selection for sat", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Journal of Artificial Intelligence Research, pp. 565\u2013 606, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A bayesian approach for automatic algorithm selection", "author": ["H. Guo"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI03), Workshop on AI and Autonomic Computing, Acapulco, Mexico, 2003, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "arXiv preprint arXiv:1401.0118, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Object-oriented bayesian networks", "author": ["D. Koller", "A. Pfeffer"], "venue": "Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1997, pp. 302\u2013313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Using combinatorial optimization within max-product belief propagation", "author": ["J. Duchi", "D. Tarlow", "G. Elidan", "D. Koller"], "venue": "Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, vol. 19. MIT Press, 2007, p. 369.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding belief propagation and its generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Exploring artificial intelligence in the new millennium, vol. 8, pp. 236\u2013239, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 6, pp. 721\u2013741, 1984.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1984}, {"title": "Predicting the size of depth-first branch and bound search trees", "author": ["L.H. Lelis", "L. Otten", "R. Dechter"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 594\u2013600.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["A.T. Ihler", "J. Iii", "A.S. Willsky"], "venue": "Journal of Machine Learning Research, 2005, pp. 905\u2013936.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Variational probabilistic inference and the qmr-dt network", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Journal of artificial intelligence research, pp. 291\u2013322, 1999.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Time-dependent statistics of the ising model", "author": ["R.J. Glauber"], "venue": "Journal of mathematical physics, vol. 4, no. 2, pp. 294\u2013307, 1963.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1963}, {"title": "Netvisa: Network processing vertically integrated seismic analysis", "author": ["N.S. Arora", "S. Russell", "E. Sudderth"], "venue": "Bulletin of the Seismological Society of America, vol. 103, no. 2A, pp. 709\u2013729, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Preliminary empirical evaluation of anytime weighted and/or best-first search for map", "author": ["N. Flerova", "R. Marinescu", "R. Dechter"], "venue": "Proceedings of 4th NIPS workshop on Discrete Optimization in Machine Learning. Citeseer, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Slicing probabilistic programs", "author": ["C.-K. Hur", "A.V. Nori", "S.K. Rajamani", "S. Samuel"], "venue": "ACM SIGPLAN Notices, vol. 49, no. 6. ACM, 2014, pp. 133\u2013144.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For example, sampling methods such as Metropolis-Hastings [1] are often the \u201cgo to\u201d algorithms for reasoning on continuous models, but convergence can be painfully slow and they suffer from high variance estimates.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Exact methods such as variable elimination [2] work well on discrete problems, but are intractable for all but the simplest models.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Recent work on generalized variational inference [3] shows promise, but still requires some hand\u2013tuning to work effectively.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "For example, one algorithm might be appropriate for a continuous portion of the model while another is used for a discrete portion; indeed, the Rao\u2013Blackwellization algorithm [4] exploits this fact.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "PP [5, 6] provides expressive and general purpose languages to encode a probabilistic model as an executable ar X iv :1 60 6.", "startOffset": 3, "endOffset": 9}, {"referenceID": 5, "context": "PP [5, 6] provides expressive and general purpose languages to encode a probabilistic model as an executable ar X iv :1 60 6.", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "Automated algorithm selection has been a long desired goal in computer science, with possibly the first formulation by Rice [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "For example, Guo [11] uses Bayesian networks to learn and select the best algorithm to solve a problem; neither the problems or algorithms are specific, and can be applied generally to a variety of problems.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Our approach is similar in spirit to the current work on black box variational inference [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "The decomposition strategies in SFI also bears similarity to structured variable elimination (SVE) [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "For example, methods that estimate the running time of various inference algorithms on a model [17] can be encoded into an inference strategy, and the algorithm with the lowest estimated running time can be chosen.", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "relates with the convergence rate of BP [18], we use the amount of determinism in the model as a choice between using BP or GS.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "First, we encoded a version of the QMR medical diagnosis model [19] in SimplePPL.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "The undirected portion of the model is an Ising model [20], where each Boolean variable v in an n \u00d7 n grid has a potential to its four vertical and horizontal neighbors.", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "The last model we used is a simplified version of the Bayesian seismic monitoring system presented in [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": ", by using Chains) finds a better elimination order than the heuristics used to solve this NP\u2013hard problem [22].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "These results are consistent with previous work on structured VE [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 48, "endOffset": 56}, {"referenceID": 16, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 165, "endOffset": 169}], "year": 2016, "abstractText": "Reasoning on large and complex real\u2013world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub\u2013models, apply an inference algorithm to each sub\u2013model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods.", "creator": "LaTeX with hyperref package"}}}