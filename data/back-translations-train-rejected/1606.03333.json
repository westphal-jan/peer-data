{"id": "1606.03333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Automatic Genre and Show Identification of Broadcast Media", "abstract": "Huge amounts of digital videos are being produced and broadcast every day, leading to giant media archives. Effective techniques are needed to make such data accessible further. Automatic meta-data labelling of broadcast media is an essential task for multimedia indexing, where it is standard to use multi-modal input for such purposes. This paper describes a novel method for automatic detection of media genre and show identities using acoustic features, textual features or a combination thereof. Furthermore the inclusion of available meta-data, such as time of broadcast, is shown to lead to very high performance. Latent Dirichlet Allocation is used to model both acoustics and text, yielding fixed dimensional representations of media recordings that can then be used in Support Vector Machines based classification. Experiments are conducted on more than 1200 hours of TV broadcasts from the British Broadcasting Corporation (BBC), where the task is to categorise the broadcasts into 8 genres or 133 show identities. On a 200-hour test set, accuracies of 98.6% and 85.7% were achieved for genre and show identification respectively, using a combination of acoustic and textual features with meta-data.", "histories": [["v1", "Fri, 10 Jun 2016 14:09:32 GMT  (74kb,D)", "http://arxiv.org/abs/1606.03333v1", "Proc. of 17th Interspeech (2016), San Francisco, California, USA"]], "COMMENTS": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.IR", "authors": ["mortaza doulaty", "oscar saz", "raymond w m ng", "thomas hain"], "accepted": false, "id": "1606.03333"}, "pdf": {"name": "1606.03333.pdf", "metadata": {"source": "CRF", "title": "Automatic Genre and Show Identification of Broadcast Media", "authors": ["Mortaza Doulaty", "Oscar Saz", "Raymond W. M. Ng", "Thomas Hain"], "emails": ["t.hain}@sheffield.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "With the ever-increasing volume of digital media and the demands on media archive processing, automatic identification and classification of media recordings is becoming increasingly important. Multimedia data can be grouped by genres such as sports, news and comedy, which are categories that also imply information other than purely semantic. As such classification is easier for viewers to understand, challenges are being advanced within the Multimedia Grand Challenges of ACM Multimedia Conference. Initiatives such as \"MediaEval Benchmarking for Multimedia Evaluation\" [1], or \"Robust,\" such as \"Human Genre Classification for Video,\" present challenges within the Multimedia Grand Challenges of ACM Conference. Genre identification and identification of shows can be considered a core task in multimedia processing."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to move to a different world in which they are capable than the world in which they are in."}, {"heading": "3. Acoustic Latent Dirichlet Allocation", "text": "As shown in our previous work [18], the acoustic LDA domain posterior is a unique distribution across all genres and time levels. In this work, we use the acoustic LDA domain posterior features to facilitate the use of other data sources such as subtitles, automatic speech recognition (ASR), and meta-data.LDA is an unattended generative model for collecting discrete data. Since speech observations are continuous data, it must first be represented by some discrete symbols referred to here as acoustic words. A GMM with N mixing components is used for this purpose. The index of the Gaussian component with the highest probability is then used to represent each frame with a discrete symbol. Frames of any acoustic document of length T, di u1..., ut, uT} are represented as: vt = arg nP (Gn)."}, {"heading": "4. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "The data provided by the British Broadcasting Corporation (BBC) is identical to the data provided for the Multi-Genre Broadcast (MGB) Challenge 2015 [22] with different definitions of training / test sets. Shows were selected to cover the full range of programme types and divided into 8 genres: Counselling, Children, Comedy, Competition, Documentary, Drama, Events and News. All shows were broadcast by the BBC for 6 weeks in April and May 2008, with more than 2,000 shows included in the original MGB Challenge data, of which 1,789 shows were selected for the experiments, 1,501 shows for the training set and 288 shows for the test set, with a total of 133 unique shows. The distribution of shows (time and number) across the genres for the training and test data is shown in Table 1. Figure 2 shows the distribution of the 133 unique shows for both the training set and the test set, representing the largest axis and the largest number of horizontal shows."}, {"heading": "4.2. Baseline", "text": "For the data described above, genre ID tasks with 8 target classes and show ID tasks with 133 target classes were used. 13 dimensional PLP [23] characteristics plus their first and second derivatives were used to train the genre-based and show-based GMMs using the expectation maximisation algorithm and mix-up procedures to achieve 512 mixtures. The optimal number of mixtures for a similar task was determined in our previous experiments with 512 [13]. Table 2 shows the classification accuracy for both tasks. As there are fewer target classes, genre ID should be a simpler classification task than ID. However, GMMs prove to be more powerful than genres in classifying shows (70.1% versus 61.5%), one reason for this could be the variety of data discussed in the introduction, and the fact that PLP characteristics are good at representing language-specific characteristics [13] and the genre ID can be used for the show-learning task, if the GMP task can be adopted as a bad one."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Whole Show and Segment Based Acoustic LDA", "text": "The performance of these models can be compared to the proposed segment-based LDA models. Classification accuracy for the genre ID and show ID tasks is shown in Table 3. For segment-level models, the rear estimates can be loud on short segments. Selecting the domain with the highest rear probability and displaying the rear vector as a one-hot vector can reduce the rear estimate noise and it was found that it slightly exceeds the base case and was used in the experiments. As the performance of segment-level models was better than the entire show models, they were also used in the other experiments. Segment-based models also showed higher accuracy with less latent domains. E.g. the highest accuracy for segment-based models for genre ID was 86.4%, achieved with an LDA model with 256 latents."}, {"heading": "5.2. Text Based LDA", "text": "This section examines classification based solely on textual characteristics. BBC TVs provide subtitles to the TV soundtrack, mostly to support deaf and hard of hearing viewers, and the quality of these subtitles varies significantly by genre. For example, subtitles of live events and news are mostly spoken live and have higher errors, but for other genres that are not live, the quality is higher. For a detailed analysis of subtitle quality, they refer to [22] and [24]. Subtitles were used without pre-processing to train classifiers for both tasks. Although subtitles may be of different quality, their correctness is still high. In a second experiment, ASR output is used instead of subtitles. ASR systems used here were trained to participate in the MGB Challenge."}, {"heading": "5.3. Using Meta-Data", "text": "The data used in the experiments also includes some metadata, such as the number of BBC stations, the date and time of broadcast and other unstructured information. The use of some of the structured metadata will be investigated next to learn how to further improve classification accuracy. As these programs were broadcast for 6 weeks in April and May 2008, the use of the date was probably not helpful, which we verified in the experiments. Instead, the transmission time, the division of 24 hours into 8 blocks and the number of channels in this setup 1-4, which corresponds to BBC1, BBC2, BBC3 and BBC4, were examined as one-hot vectors to the inputs of the SVM classifiers and their effect. Table 5 summarizes the results of the use of the metadata along with acoustic LDA characteristics. Adding these metadata helps in both tasks to compare the channel and time, and the difference is greater in the case of Show ID along with acoustic LDA characteristics."}, {"heading": "5.4. System Fusion", "text": "For the two systems based on acoustic and textual characteristics, a combination of the two can be used, provided that they make different classification errors and their results are complementary. To combine the values of the systems, logistic regression is used to find a linear combination of individual system evaluations to maximize the probability of correct classification [26]. Table 6 shows classification accuracy with system fusion. The combination of acoustic and text-based systems improves classification accuracy for both tasks, 97.2% and 85.0% for genre ID and show ID respectively, showing the complementarity of each system. In addition, the inclusion of metadata improves accuracy to 98.6% and 85.7%, respectively, which is nearly perfect for the genre ID task."}, {"heading": "6. Conclusions", "text": "This paper proposed new methods for classifying broadcast media on the basis of audio. Furthermore, information sources needed to achieve a very high level of performance were examined in this paper. Also, for the first time, a task for classifying broadcasts on very large data sets was investigated; the experiments used more than 1,200 hours of data from more than 1,500 BBC television broadcasts broadcast in 2008, which were part of the MGB Challenge 2015 [22]. There were 8 classes for the task of classifying genre IDs, and 133 classes for the show IDs. Acoustic and textual LDA models were trained with audio and subtitles to derive the rear dirichlet parameters, which were then used in SVM classifiers to classify genres and shows. On a 200-hour test set, the combination of acoustic and text-based classifiers showed an accuracy of 97.2% and 85.0% for genre IDs."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EPSRC Programme Grant EP / I031022 / 1 (Natural Speech Technology) and the audio and subtitle data used in these experiments were distributed under a licence with the BBC as part of the MGB Challenge (mgb-challenge.org) [22]."}, {"heading": "8. References", "text": "[1] M. Larson, X. Anguera, T. Reuter, G. Jones, B. Ionescu, M. Schedl, T. Piatrik, C. Hauff, and M. Soleymani, \"Indexing multimedia documents with acoustic concept recognition lattices\" in Proc. of MediaEval 2013 Multimedia Benchmark Workshop, Barcelona, Spain, 2013. [2] \"Multimedia Grand Challenge (2009, 2010).\" [Online] Available: http: / / comminfo.rutgers.edu / conferences / mmchallenge [3]. Liu, J. Huang, and Y. Wang, \"Classification of TV programs based on audio using hidden Markov model, in Proc. of Multimedia Signal Processing Workshop, 1998, pp. 27-32. [4] M. Roach and J. S. Mason,\" Classification of video genre using audio,., \"in Proc. of Interspeech, Aalborg, Denmark, 2001."}], "references": [{"title": "Indexing multimedia documents with acoustic concept recognition lattices.", "author": ["M. Larson", "X. Anguera", "T. Reuter", "G. Jones", "B. Ionescu", "M. Schedl", "T. Piatrik", "C. Hauff", "M. Soleymani"], "venue": "in Proc. of MediaEval 2013 Multimedia Benchmark Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Classification of TV programs based on audio information using hidden Markov model", "author": ["Z. Liu", "J. Huang", "Y. Wang"], "venue": "Proc. of Multimedia Signal Processing Workshop, 1998, pp. 27\u201332.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Classification of video genre using audio", "author": ["M. Roach", "J.S. Mason"], "venue": "Proc. of Interspeech, Aalborg, Denmark, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal genre classification of TV programs and YouTube videos", "author": ["H.K. Ekenel", "T. Semela"], "venue": "Multimedia tools and applications, vol. 63, no. 2, pp. 547\u2013567, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel neural networks for multimodal video genre classification", "author": ["M. Montagnuolo", "A. Messina"], "venue": "Multimedia Tools and Applications, vol. 41, no. 1, pp. 125\u2013159, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "An indepth evaluation of multimodal video genre categorization", "author": ["I. Mironica", "B. Ionescu", "P. Knees", "P. Lambert"], "venue": "Proc. of Content-Based Multimedia Indexing (CBMI) Workshop, Veszprem, Hungary, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "On-line genre classification of TV programs using audio content", "author": ["S. Kim", "P. Georgiou", "S. Narayanan"], "venue": "Proc. of ICASSP, Vancouver, Canada, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "TV genre classification using multimodal information and multilayer perceptrons", "author": ["M. Montagnuolo", "A. Messina"], "venue": "AI*IA 2007: Artificial Intelligence and Human-Oriented Computing. Springer, 2007, pp. 730\u2013741.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Data-selective transfer learning for multi-domain speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.M. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Proc. of IWSLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Background-tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Proc. of SLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic topic model for audio information retrieval", "author": ["S. Kim", "S. Narayanan", "S. Sundaram"], "venue": "Proc. of WASPAA, New Paltz NY, USA, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Group feature selection for audio-based video genre classification", "author": ["G. Sageder", "M. Zaharieva", "C. Breiteneder"], "venue": "MultiMedia Modeling. Springer, 2016, pp. 29\u201341.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-based semantic concept classification for consumer video", "author": ["K. Lee", "D.P. Ellis"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1406\u20131416, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Indexing multimedia documents with acoustic concept recognition lattices.", "author": ["D. Castan", "M. Akbacak"], "venue": "in Proc. of Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation", "author": ["M. Doulaty", "O. Saz", "R.W.M. Ng", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Factor analysis for audio-based video genre classification.", "author": ["M. Rouvier", "D. Matrouf", "G. Linares"], "venue": "in Proc. of Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Unsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The MGB Challenge: Evaluating multi-genre broadcast media recognition", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P. Woodland"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "The 2015 Sheffield system for transcription of multi-genre broadcast media", "author": ["O. Saz", "M. Doulaty", "S. Deena", "R. Milner", "R.W.M. Ng", "M. Hasan", "Y. Liu", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The 2015 Sheffield system for longitudinal diarisation of broadcast media", "author": ["R. Milner", "O. Saz", "S. Deena", "M. Doulaty", "R. Ng", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "FoCal toolkit for evaluation, fusion and calibration of statistical pattern recognisers", "author": ["N. Brummer"], "venue": "2010. [Online]. Available: https://sites.google.com/site/nikobrummer/focal", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Research in this field is pushed forward by initiatives such as the \u201cMediaEval Benchmarking for Multimedia Evaluation\u201d [1], or the \u201cRobust, as Accurate as Human Genre Classification for Video\u201d challenges within the Multimedia Grand Challenges of ACM Multimedia Conference [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "For audio-based classification mostly short-term features are used [3], such as Mel-Frequency Cepstral Coefficients (MFCC) [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "For audio-based classification mostly short-term features are used [3], such as Mel-Frequency Cepstral Coefficients (MFCC) [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "The use of other features such as average speech rate, signal energy, zero crossing rate, duration of silence, noise and speech have also been studied [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 3, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 4, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "Typical features extracted from video include colour statistics, camera motion and cut detection [5, 6, 7].", "startOffset": 97, "endOffset": 106}, {"referenceID": 6, "context": "In the literature, audio based features usually have very similar performance compared to the video-based features [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "title, tag, video description) contain semantic information and are believed to give promising results in genre ID [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 4, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 6, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 7, "context": "Research on genre ID tasks typically report accuracies of over 90% [5, 6, 8, 9].", "startOffset": 67, "endOffset": 79}, {"referenceID": 7, "context": "Typical datasets are the RAI dataset [9], Quaero dataset [10] and some custom YouTube videos.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "The proposed method in [8] uses acoustic features and using the RAI dataset, they reported accuracy of 94.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "2% was reported in [5] for the same dataset.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "5% [5]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "On a custom YouTube dataset [5], 87.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "[8] reported an accuracy of 93.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In smaller and more homogeneous datasets where the same shows and speakers might often reoccur, the classification performance with those features are usually much better than the accuracies obtained on larger and more heterogeneous datasets [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 6, "context": "[8] had the accuracy improved by 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "bols and trained Latent Dirichlet Allocation (LDA) models [14] followed by Support Vector Machine (SVM) classifiers.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "However when the amount of data is more and thus the dataset is more diverse, the same baseline models performs much worse [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "[15] tried to pool various types of features and then group and select a subset using canonical correlation analysis in order to identify low-correlated and complementary features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Other approaches try to identify certain audio-visual events, with the objective to model the semantics of the broadcast shows or YouTube videos [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 14, "context": "Other approaches try to identify certain audio-visual events, with the objective to model the semantics of the broadcast shows or YouTube videos [16, 17].", "startOffset": 145, "endOffset": 153}, {"referenceID": 15, "context": "As shown in our previous work [18], acoustic LDA domain posteriors have a unique distribution across genres and shows.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Training minimises the Kullback-Leiber Divergence between the real and the approximated joint probabilities (equations 4 and 5) [19]:", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 106, "endOffset": 113}, {"referenceID": 17, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 106, "endOffset": 113}, {"referenceID": 10, "context": "Discriminative classifiers such as SVMs have been used successfully for genre classification tasks before [8, 20] including our previous work [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "[8] used the whole shows to train the LDA models and used the domain posteriors as features for an SVM classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In this work we followed our previous setup [18, 21] where only speech segments are used to train the LDA model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 18, "context": "In this work we followed our previous setup [18, 21] where only speech segments are used to train the LDA model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 19, "context": "The data is identical to the one defined and provided for the 2015 MultiGenre Broadcast (MGB) Challenge [22] with a different training/testing set definitions.", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 3, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 4, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 6, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 12, "context": "It is important to note that this dataset is by orders of magnitude larger than most of the datasets used in the literature for the genre ID task [2, 4, 5, 6, 8, 15].", "startOffset": 146, "endOffset": 165}, {"referenceID": 20, "context": "13 dimensional PLP [23] features plus their first and second derivatives were used to train the genre-based and showbased GMMs using Expectation Maximisation algorithm and mix-up procedure to reach 512 mixtures.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "The optimal number of mixtures for a similar task was found to be 512 in our previous experiments [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "5%), one reason for this could be the diversity of data as discussed in the introduction and the fact that PLP features are good for representing speaker specific characteristics [13] and for the show ID task the GMMs are learning speakers in re-occurring episodes.", "startOffset": 179, "endOffset": 183}, {"referenceID": 19, "context": "For a detailed analysis of the subtitles quality refer to [22] and [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "For a detailed analysis of the subtitles quality refer to [22] and [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "For more details about these ASR systems, refer to [24] and [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "For more details about these ASR systems, refer to [24] and [25].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "However here the LDA model reduces the dimensionality of the tf-idf features to the number of latent domains, which is known to work better than tf-idf only features for document classification [19].", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "To combine the scores of the systems, logistic regression is used to find a linear combination of individual system scores to maximize the probability of correct classification [26].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "These data was a part of the MGB 2015 challenge [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "org) [22] through a licence with the BBC.", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "Huge amounts of digital videos are being produced and broadcast every day, leading to giant media archives. Effective techniques are needed to make such data accessible further. Automatic meta-data labelling of broadcast media is an essential task for multimedia indexing, where it is standard to use multi-modal input for such purposes. This paper describes a novel method for automatic detection of media genre and show identities using acoustic features, textual features or a combination thereof. Furthermore the inclusion of available meta-data, such as time of broadcast, is shown to lead to very high performance. Latent Dirichlet Allocation is used to model both acoustics and text, yielding fixed dimensional representations of media recordings that can then be used in Support Vector Machines based classification. Experiments are conducted on more than 1200 hours of TV broadcasts from the British Broadcasting Corporation (BBC), where the task is to categorise the broadcasts into 8 genres or 133 show identities. On a 200-hour test set, accuracies of 98.6% and 85.7% were achieved for genre and show identification respectively, using a combination of acoustic and textual features with meta-data.", "creator": "LaTeX with hyperref package"}}}