{"id": "1606.03335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "abstract": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position - role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset.", "histories": [["v1", "Fri, 10 Jun 2016 14:12:47 GMT  (3411kb,D)", "http://arxiv.org/abs/1606.03335v1", "29 pages, 16 figures, submitted to journal"]], "COMMENTS": "29 pages, 16 figures, submitted to journal", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.DC", "authors": ["roman bartusiak", "{\\l}ukasz augustyniak", "tomasz kajdanowicz", "przemys{\\l}aw kazienko", "maciej piasecki"], "accepted": false, "id": "1606.03335"}, "pdf": {"name": "1606.03335.pdf", "metadata": {"source": "CRF", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "authors": ["Roman Bartusiaka", "Lukasz Augustyniak", "Tomasz Kajdanowicz", "Przemys law Kazienko", "Maciej Piasecki"], "emails": ["roman.bartusiak@pwr.edu.pl", "lukasz.augustyniak@pwr.edu.pl", "tomasz.kajdanowicz@pwr.edu.pl", "kazienko@pwr.edu.pl", "maciej.piasecki@pwr.edu.pl"], "sections": [{"heading": null, "text": "A complex nature of large data resources requires new methods of structuring especially textual content. WordNet is a good source of knowledge for a comprehensive abstraction of natural language, as there are good implementations for many languages. Since WordNet embeds the natural language in the form of a complex network, the paper proposes a transformation mechanism WordNet2Vec, which creates vectors for every word from WordNet. These vectors summarize the general position - the role of a given word over all other words in the natural language. Each list or set of such vectors contains knowledge of the context of its components within the entire language. Such word representation can easily be applied to many analytical tasks such as classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in Mood Analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset. Keywords: Structuring natural language, WordNet, Word2Vec, Netord2Vec, Network Sensitivity Analysis, Transformation."}, {"heading": "1. Introduction", "text": "This year, the time has come for us to be able to find a new home in the city, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home."}, {"heading": "2. Related Work", "text": "The representation of knowledge is an area of Artificial Intelligence that deals with the question of how knowledge can be symbolically represented and manipulated automatically [4]. Text documents are inherently unstructured, and to ensure their robust processing, they must be resolved, aggregated, integrated and abstracted - through the various methods. From such treatment of textual data, we expect accurate estimation and prediction, data mining, social network analysis, and semantic search and visualization. Derived knowledge can be represented in various structures, including: semantic networks, frames, rules, and ontologies [5]. Due to the fact that the majority of machine learning and supervised learning approaches deal with vector space, text representation should also be placed in a vector space."}, {"heading": "2.1. Word Embedding Techniques", "text": "This year, the time has come for it to be able to find a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution."}, {"heading": "2.2. WordNet", "text": "WordNet is a large lexical database of natural language. There are separate WordNetworks for many different languages, e.g. English [2, 17]. Nouns, verbs, adjectives and adverbs are grouped into groups of cognitive synonyms (synsets), each of which expresses a specific concept. Synsets are linked by conceptual semantic and lexical relationships. Different types of linkages can be distinguished in WordNet: \u2022 There are 285,348 semantic linkages between all words: - 178,323 both hypernym and hyponym, - 21,434 similarities, - 18,215 both holonym and meronym, - 67,376 other linkages. \u2022 There are 92,508 lexical linkages between all words: - 74,656 derivatives, - 7,981 antonyms, - 9,871 other linkages."}, {"heading": "2.3. All-pairs Shortest Paths", "text": "All-pairs shortways (APSP) is one of the most basic problems in graph theory. The goal of this task is to calculate the distances between all vertices in a graph. Calculations can be performed for all types of graphs, i.e. directed, undirected, weighted, unweighted, etc. The field is extensively studied and analyzed because the complexity of the task is an open problem. Several algorithms have been proposed to solve the problem. Their complexity may vary depending on the graph type that is being considered. Currently, the best known solution to the problem has been proposed by Robert Floyd [18]. Following the success in optimizing complexity, several new approaches have emerged that attempt to minimize the computational and memory complexity. Simple geometric optimization has allowed to reduce the complexity to O (n 3log (n)), which is the best distributed complexity in the environment."}, {"heading": "2.4. Sentiment Analysis", "text": "Nowadays, the most commonly used methods for sentiment analysis are classification approaches using classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28]. Furthermore, the selection of characteristics can improve classification accuracy by reducing high dimensionality to a low-dimensional feature space. Yousefpour et al. [30] proposed a hybrid method and two meta-heuristic algorithms to find an optimal feature underlay. Another family of solutions for classifying feelings are neural network-based approaches. Socher et al. [31] proposed a recursive depth model for feelings using tree structure of sentences. Zhang and LeCun [32] used deep learning to grasp text understandings from character layers to textual concepts (using them as abstract concepts)."}, {"heading": "2.5. Transfer Learning", "text": "Interestingly, it is based on human behavior during learning. Often, we can transfer what we have learned in a situation and adapt it to the new situation. Yoshida et al. [35] proposed a model in which each word is linked to three factors: domain name, domain dependence / independence, and word polarity. The most important part of their method is Gibbs Sampling to derive the parameters of the model from both labeled and unlabeled texts. In addition, the method they proposed could also determine whether the polarity of each word is domain-independent or domain-independent. Zhou et al. [36] developed a solution for cross-domain mood classification of unlabeled data. To bridge the gap between domains, they proposed an algorithm called topical correspondence transfer (TCT)."}, {"heading": "3. WordNet2Vec: WordNet-based Natural Language Representation", "text": "The general idea of the WordNet2Vec method is to transfer knowledge of the natural language that is encapsulated by the WordNet network database into word-based vector structures that are suitable for further processing. Its principal steps are presented in Figure 1.WordNet, which is developed by linguists for a particular language, consists of many synsets - meanings of words and lexical and semantic relationships that link them together. If it is large enough - more than one hundred thousand synsets - it can be treated as a reliable and comprehensive representation of the general vocabulary used by people who speak a particular language. As WordNet has a complex network form, it is hardly suitable for commonly used analytical methods such as machine learning reasoning. To overcome this limitation, we propose the WordNet2Vec method, which provides a number of word vectors that tie the whole WordNet together."}, {"heading": "4. WordNet2Vec Implementation \u2013 Distributed Calculation of All-", "text": "In order to demonstrate the WordNet2Vec method, it was applied to the English WordNet [2]. We created a simplified network based on the semantic and lexical relationships present in WordNet. Therefore, we obtained a graph consisting of 147,478 words connected by 1,695,623 links. The simplified network was used to calculate all the pairwise shortest paths in this network, so that we obtained 147, 4782, i.e. over 21 billion paths. Due to the size of the simplified network, as well as the computing and memory complexity of the path computation task, we decided to use heterogeneous computing clusters as the environment for our experiments. Nevertheless, further optimization had to be carried out. In our approach, we used distributed computing paths from Dijkistra [37] algorithms available in our SparklingGraph library."}, {"heading": "5. Use Case: WordNet2Vec Application to Sentiment Analysis", "text": "Among a variety of possible applications, the application of the WordNet2Vec method is presented with a use case for sentiment analysis. In general, sentiment analysis of the texts consists in the assignment of a positive, neutral or negative measure to the text. Using the WordNet2Vec representation, we use a supervised learning approach to generalize the sentiment classes assigned to the document. Depending on the content of the new document, it is then possible to derive its sentiment class with a trained model. In such a scenario, one can determine the attitudes, opinions and emotions expressed within a text. Specifically, sentiment analysis can be performed with a suitable sequence of processing steps that includes: text segmentation and lepatization, vector lookup in the WordNet2Vec matrix for each word in the document, aggregation of vectors for all words within a document, train / data set splitting and data set splitting, and finally, file fixing."}, {"heading": "5.1. Text segmentation and lematization", "text": "First, some basic methods of processing natural language must be applied: segmentation and lemmatization. In order to process each document, it must be segmented into words. Since the WordNet2Vec matrix is prepared for words in their lemmatization form, each word from each document must be lemmatized. At this point, it must be emphasized that the method has a disadvantage, as it only works for words that are present in WordNet. However, since WordNet is an authoritative and reliable representation of language, it may be suitable for almost all possible applications."}, {"heading": "5.2. Vector look-up in the WordNet2Vec Matrix for a word", "text": "The word vectorization method proposed in the paper provides a pre-calculated WordNet2Vec matrix. It contains vector representations for each word from WordNet. In the next step of the flow, vectors for all words for all documents with O (1) time are retrieved from the mentioned matrix."}, {"heading": "5.3. Aggregation of vectors for all words within documents", "text": "To calculate a single vector representing the document, we suggest summarizing vectors of each word that appears in the document, see Equation 1), v (d) = | | d | \u2211 i \u2212 \u2212 \u2212 \u2212 \u2192 v (di) (1), where \u2212 \u2192 v (\u00b7) represents a vector from the WordNet2Vec matrix, | | d | | denotes a number of lemmas in a document, and di represents an ith lem from document d."}, {"heading": "5.4. Dataset split, classifier learning and testing", "text": "Once all documents have a single vector representation, they form a data set suitable for training and testing classifiers. To test the generalization capabilities of classifiers, two different scenarios are proposed: learning and testing within the same text domain (the same type of documents) or transfer of learning and testing (learning in one domain and testing in another)."}, {"heading": "6. Experiments and Results", "text": "Following the scenario presented in Section 5, we performed validity tests by examining the mood mapping task. In the following sections, we describe the data used, the details of the experiments, and the results received."}, {"heading": "6.1. Datasets", "text": "We selected data from a single data source - the online trading platform Amazon [39]. The data covers a period of 18 years, including 35 million reviews through March 2013. Reviews include product and user information, reviews, and a review in plain text. Some basic statistical information about the data set is in Table 1. The entire experiment was conducted on a selected portion of Amazon data consisting of 7 areas, namely: automotive, sports, and outdoor, books, health, video games, toys, and games, movies, and television. Due to the fact that the distribution of the classes is important in interpreting the results of classification validation, the correct histogram is shown in Figure 4. The areas of the review data set selected for the experiment are listed in Table 2. To verify the accuracy of the proposed methods, we extracted the mood orientation from the ratings expressed with stars. The ratings were assigned to the following classes: \"positive,\" \"neutral,\" \"\" negative, \"\" \"see 2,\" and \"sterner\" and \"3."}, {"heading": "6.2. Experimental setup", "text": "Our experiments were divided into two different groups. First, the model consisted of classical machine learning evaluation by using train / test split on each of the seven data sets mentioned. In the second group of experiments, we used one against all transfer learning evaluation. Whenever a domain was used as a training set, it was evaluated on the rest of the domains. Thanks to this, we experienced the quality of the two classical sensation analysis task based on specific domain data sets and on the transfer of learning between different domains. In both experiment groups, we used two methods of text vectorization: proposed by us WordNet2Vec and for reference - Doc2Vec.Doc2Vec is the generalization of Word2Vec algorithm at the document level. Word2Vec model shows how a word is normally used in a window context after other words (how words occur together)."}, {"heading": "6.3. Generalization ability with regards to in-domain classification", "text": "For each domain out of a total of seven investigated in the experiments, Doc2Vec is slightly better than the approach (Figure 5).The difference between the results obtained by Doc2Vec and WordNet2Vec is not as large as can be seen in Figure 6. Nevertheless, it is statistically significant, as shown by the Wilcoxon ranking sum test (Table 4)."}, {"heading": "6.4. Generalization ability with regards to transfer learning", "text": "We note that our method achieves better results than Doc2Vec (Figure 7, 8).It is important to note that the differences in results in favor of WordNet2Vec are much greater compared to the results from the standard classification (Figure 9).Analyzing the variance of transfer learning results for both methods shows that WordNet2Vec is more stable and the results are less different compared to Doc2Vec (Figure 10).In addition, we have used statistical tests to verify the statistical significance of differences in results (Table 5).The superiority of WordNet2Vec over Doc2Vec is statistically significant. Table 5: Wilcoxon rank-sum test results for transfer learning.Method 1 Method 2 Ha p-values Doc2Vec WordNet2Vec Historically, F1WordNet2Vec > F1Doc2V 86.01Vec 100.01Vec - Vec 01c 01c Vec 1c 1c 1c 1c 1c Vec 1c 1- 0.1c 1c 0c 1c 1c 1c 1- 0.1c 1c Vec 0c 1c 1- 0.1c 0c 1c 1c 1- 0.1c Vec 1c 1c 1c 1- 0.1c 1c 1c 0c 1c 1c 1- 0.0.1c 1c Vec 1c 1c 1c 1c 1c 1c 1c 1- 0.0.1c 1c 1c 1c 1c 1c 1c 1c - 0.0.0.1c."}, {"heading": "7. Conclusions and Future Work", "text": "A novel method - WordNet2Vec - for word vectorization, which enables the construction of a more general knowledge representation of texts using WordNets, was presented in the thesis. It provides word representation in vector space using its distance from every other word in the network. To represent the paired word distance, the method calculates in WordNet.The usefulness of the WordNet2Vec method was demonstrated in a sentiment analysis problem, i.e. when classifying with transfer learning settings using Amazon ratings. We compared the WordNet2Vec-based classification of sentiment with the Doc2Vec approach. Doc2Vec proved to be more accurate in homogeneous settings (learning and testing within the same domain). However, in the case of cross-domain application (transfer learning), our method performed better than the Doc2Vec-based classification of sentient."}], "references": [{"title": "A Wordnet from the Ground Up", "author": ["M. Piasecki", "S. Szpakowicz", "B. Broda"], "venue": "Oficyna Wydawnicza Politechniki Wroclawskiej, Wroc  law", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "Commun. ACM 38 (11) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Knowledge Representation and Reasoning", "author": ["R. Brachman", "H. Levesque"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": "2nd Edition, Pearson Education", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado"], "venue": "Dean, Distributed representations of words and phrases and their compositionality ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research 12 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in: Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Lebret"], "venue": "Collobert, Word Emdeddings through Hellinger PCA ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Rehabilitation of count-based models for word vector representations", "author": ["R. Lebret", "R. Collobert"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9041 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Don\u2019t count", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Baltimore, Maryland", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorization of Latent Variables in Distributional Semantic Models", "author": ["A. \u00d6Sterlund", "D. \u00d6dling", "M. Sahlgren"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015 (September) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems (NIPS) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithm 97: shortest path", "author": ["R. Floyd"], "venue": "Communications of the ACM 5 (6) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1962}, {"title": "All-pairs shortest paths with real weights in o (n 3/log n) time", "author": ["T.M. Chan"], "venue": "in: Algorithms and Data Structures, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "An o (n 3 (loglogn/logn) 5/4) time algorithm for all pairs shortest paths", "author": ["Y. Han"], "venue": "in: Algorithms\u2013ESA 2006, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Thumbs up?: sentiment classification using machine learning techniques, in: Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10", "author": ["P. Bo", "L. Lillian", "V. Shivakumar"], "venue": "Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Sentiment classification of online reviews to travel destinations by supervised machine learning approaches", "author": ["Q. Ye", "Z. Zhang", "R. Law"], "venue": "Expert Systems with Applications 36 (3, Part 2) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "C", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng"], "venue": "Potts, Learning Word Vectors for Sentiment Analysis ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Predicting consumer sentiments from online text", "author": ["X. Bai"], "venue": "Decision Support Systems 50 (4) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Sentiment classification on customer feedback data: Noisy data", "author": ["M. Gamon"], "venue": "large feature vectors, and the role of linguistic analysis, in: Proceedings of 27  the 20th International Conference on Computational Linguistics, COLING \u201904", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "The importance of neutral examples for learning sentiment", "author": ["J. Schler", "J. Schler"], "venue": "in: In Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Comprehensive study on lexicon-based ensemble classification sentiment analysis", "author": ["L. Augustyniak", "P. Szymanski", "T. Kajdanowicz", "W. Tuliglowicz"], "venue": "Entropy 18 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Intelligent Information and Database Systems: 8th Asian Conference", "author": ["A. Yousefpour", "R. Ibrahim", "H.N.A. Hamed", "T. Yokoi"], "venue": "ACIIDS 2016, Da Nang, Vietnam, March 14-16, 2016, Proceedings, Part I, Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "in: Proceedings of the conference on empirical methods in natural language processing (EMNLP), Vol. 1631, Citeseer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Rosemerry: A baseline message-level sentiment classification system", "author": ["H. Liang", "R. Fothergill", "T. Baldwin"], "venue": "SemEval-2015 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowl. and Data Eng. 22 (10) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Y", "author": ["Y. Yoshida", "T. Hirao", "T. Iwata", "M. Nagata"], "venue": "Matsumoto, Transfer learning for multiple-domain sentiment analysis - identifying domain dependent/independent word polarity., in: W. Burgard, D. Roth (Eds.), AAAI, AAAI Press", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische mathematik 1 (1) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1959}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "in: Proceedings of the 7th ACM Conference on Recommender Systems, RecSys \u201913, ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming 45 (1-3) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "The largest WordNet is for Polish [1] and English [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The largest WordNet is for Polish [1] and English [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "Representation of the knowledge is an area of Artificial Intelligence concerned with how knowledge can be represented symbolically and manipulated in an automated way [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Derived knowledge can be represented in various structures including: semantic nets, frames, rules, and ontologies [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Word2Vec (Word-2-Vector) [6, 7] is the most successful method from this group.", "startOffset": 25, "endOffset": 31}, {"referenceID": 5, "context": "[8, 9], but Word2Vec outperformed them and gained much more attention.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[10], is one of them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Lebret and Collobert [11] and Dhillon et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Lebret and Collobert [13] presented alternative model based on counts.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "In conclusion, Word2Vec is a \u201dpredictive\u201d model, whereas GloVe is a \u201dcountbased\u201d model [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "They are different computational methods that produce a very similar type of a semantic models [15, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 11, "context": "They are different computational methods that produce a very similar type of a semantic models [15, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 1, "context": "for English [2, 17].", "startOffset": 12, "endOffset": 19}, {"referenceID": 12, "context": "Current most known solution for the problem have been proposed by Robert Floyd [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Simple geometrical optimization allowed to decrease complexity to O( n 3 log(n) ) [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Work of Yijie Han [20] exceeds, unbreakable for long time, result of O( n 3 log(n) ).", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Method proposed by Robert Floyd [18] thanks to its simplicity can be distributed in easiest way, thanks to what it can be easily used in distributed environment for big datasets .", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 153, "endOffset": 165}, {"referenceID": 16, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 153, "endOffset": 165}, {"referenceID": 17, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 18, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 19, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 20, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 21, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 21, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 277, "endOffset": 281}, {"referenceID": 22, "context": "[30] proposed a hybrid method and two meta-heuristic algorithms are employed to find an optimal feature subset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[31] proposed recursive deep model for sentiment using treebank structure of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "What is more, some systems leverages both hand-crafted features and word level embedding features, like Do2Vec, with the usage of classifiers such as SVM [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 25, "context": "Transfer Learning Transfer learning provides system ability to recognize and apply knowledge extraction (learn in the previous tasks) to the novel tasks (in new domains) [34].", "startOffset": 170, "endOffset": 174}, {"referenceID": 26, "context": "[35] proposed a model, where each word is associated with three factors: domain label, domain dependence/independence and word polarity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "To demonstrate the WordNet2Vec method, it was applied to English WordNet [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 27, "context": "In our approach, we have used distributed implementation of Dijkistra [37] algorithm available in our SparklingGraph library [38].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Most the known solutions for the all-pair shortest paths (APSP) problem has memory complexity of O(n), where n is the number of nodes in the graph [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 28, "context": "Datasets We chose data from one data source - Amazon e-commerce platform [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Logistic regression was selected as a supervised learning model and in order to train it we have used limited memory BFGS algorithm [40].", "startOffset": 132, "endOffset": 136}], "year": 2016, "abstractText": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset.", "creator": "LaTeX with hyperref package"}}}