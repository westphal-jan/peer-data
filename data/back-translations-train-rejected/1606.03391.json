{"id": "1606.03391", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Simple Question Answering by Attentive Convolutional Neural Network", "abstract": "This work focuses on answering single-relation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a two-step pipeline: entity linking and fact selection. In fact selection, we match the subject entity in fact with the entity mention in question by a character-level convolutional neural network (char-CNN), and match the predicate in fact with the question by a word-level CNN (word-CNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker of SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.", "histories": [["v1", "Fri, 10 Jun 2016 16:54:51 GMT  (261kb,D)", "http://arxiv.org/abs/1606.03391v1", "10 pages, 2 figures"], ["v2", "Tue, 11 Oct 2016 14:32:13 GMT  (259kb,D)", "http://arxiv.org/abs/1606.03391v2", "Accepted as an oral long paper by COLING'2016"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "mo yu", "bing xiang", "bowen zhou", "hinrich sch\\\"utze"], "accepted": false, "id": "1606.03391"}, "pdf": {"name": "1606.03391.pdf", "metadata": {"source": "CRF", "title": "Simple Question Answering by Attentive Convolutional Neural Network", "authors": ["Wenpeng Yin", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want. (...) It's not that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...)... (...) \"(...). (...). (...). (...). (...). (). (...). (). (). (). (). (). (). (). (). (). (). ().). (). (). (). ().). (). (). (). ().). (). ().). ().). (). ().). ().). ().). ().).). ().).). ().). ().).)."}, {"heading": "2 Related Work", "text": "In fact, most people who are able to determine what they want and what they don't want have to do it. In fact, it is that people are able to determine what they want and what they don't want. In fact, it is that people are able to determine what they want and what they want. In fact, it is that people are able to determine themselves what they want and what they don't want. In fact, it is that people are able to determine themselves what they want and what they want and what they want. In fact, it is that people are able to determine themselves what they want and what they want."}, {"heading": "3 Task Definition and Data Introduction", "text": "We first describe the SimpleQuestions task (Berant et al., 2013) and Freebase (Bollacker et al., 2008).The SimpleQuestions benchmark, a typical SimpleQA task, offers a series of questions with a single relation; each question is accompanied by a basic truth-fact. Indeed, the object entity is the default answer; the data set is divided into train (75,910), dev (10,845) and test (21,687) sentences. While questions with a single relation are easier to handle than questions with more complex and multiple relationships, answering questions with a single relation is still far from a solution. Even within this limited range, there are a large number of paraphrases of the same question. Therefore, the problem of assigning a question to a specific predicate and entity in the freebase is hard. Freebase is a structured knowledge base in which units are connected by predefined prediates or thieves \"all of which are directly related to a particular predicate and entity."}, {"heading": "4 Entity Linking and Mention Detection", "text": "The question we are asking is, \"What is the answer to this question?\" We are referring to a question that relates to the following three steps. (i) We are linking by deriving the longest consecutive common sequence (LCCS) between a question and a candidate, and we are pointing out that it is a question to which we are referring. (ii) We are referring to a question that we call all the names of entities from Freebase. (i) We are performing the following three steps. (ii) We are using every word of q to retrieve entities whose names contain this word. We are referring to the set of all these entities as Ce. (ii) For each entity candidates in Ce, their entity CCS."}, {"heading": "5 Fact Selection", "text": "Our entire system is illustrated in Figure 1 (a). It consists of two aspects: (i) a CNN at character level (char-CNN) to detect the similarity of the string and the mention string in surface form (the left column); (ii) a CNN with attentive maxpooling (AMP) at word level (word-AMPCNN) to detect whether the predicate is a paraphrase of the pattern. Word-AMPCNN is motivated by the observation that the name of the FB predicate is short and firm, while the corresponding pattern is highly variable in length and word choice. Our hypothesis is that the predicate pattern is best made on the basis of maxchons in the pattern (and perhaps humans do something similar) and that CNN should therefore identify helpful predictive words. Traditional MP-XP-XP programs, which all have the same dimensions, are likely higher."}, {"heading": "5.1 Framework of CNN-Maxpooling", "text": "Both char-CNN and word-CNN have two weighted CNNs, as they model two pieces of text. In the following, we will use \"entry\" as a generic term for letters and words. The input layer is a sequence of lengths, each entry being represented by a d-dimensional randomly initialized embedding; therefore, the sequence is used as a dimensionality characteristic map d \u00b7 s. Figure 1 (b) shows the input layer as the lower rectangle with multiple columns. The folding layer is used for the representation learned from sliding n-grams. For an input sequence with s entries: v1, v2,.., vs, let vector ci-Rnd, the concatenated embedding of n entries vi \u2212 n + 1,... vi, where n is the filter width and 0 < i < s + n. Embedding of vi, i < 1 or i > are the contiguous embedding of vi \u2212 n."}, {"heading": "5.2 AMPCNN: CNN-Attentive-Maxpooling", "text": "Figure 2 shows TMP (Traditional MaxPooling) and AMP (Attentive MaxPooling) as we apply them to SimpleQA. Remember that we use standard CNNs to generate (i) the predicate representation vp (see Figure 1 (a)) and (ii) a characteristic map of the pattern, i.e. a matrix with columns designating n-gram representations (shown in Figure 1 (b), the matrix below \"colwise (attentive) maxpooling\"). In Figure 2, we refer to the characteristic map as Fpattern and the predicate representation as vp.TMPCNN, i.e., traditional maxpooling, gives the vector shown as vTMP; the same vTMP is produced for different vp. AMPCNN's basic idea is to consider the predicates vp bias the selection and weighting of subsequences of the question of the prediction of the pattern."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Training Setup", "text": "Our fact pool consists of all facts whose subject unit is in the top N subject candidates; for the turn, 99 negative facts for each basic truth fact; for Dev and test, all fact candidates are kept.Figure 1 (a) shows a two-way match between a tuple t and a question q: entity-mention-match by charCNN (rate me), predicate-pattern match by wordAMPCNN (rate mr).The total ranking value of the pair is st (q, t \u2212) = me + mr + se, where se is the entity ranking value in phase linkage. Our goal is to minimize ranking losses: l (q, t +, t \u2212) = max (0, \u03bb + st (q, t \u2212) \u2212 st (q, t +)))) (4), where it is a constant, we build up word and vocare."}, {"heading": "6.2 Entity Linking", "text": "In Table 3, we compare our entity linker with the state-of-the-art entity linker (Golub and He, 2016) in this SimpleQA task. Golub and He (2016) report on the coverage of the truth by Top N cases (N-1, 10, 20, 50, 100). In addition, they investigate a reranking algorithm to refine the entity ranking list. In this work, we do not have a reranking step. Table 3 shows the superior performance of our entity linker and its performance without factor a, b or c (-a, -b, -c). Our entity linker exceeds the raw results of the baseline by large margins and is 2-3 percent above its reranking values values. This shows the outstanding performance of our entity linker despite its simplicity. The table also shows that all three factors (a, b, c) play a role. Observations: (Each factor i is more important if N.)"}, {"heading": "6.3 SimpleQuestions", "text": "Table 4 compares AMPCNN with two baselines. (i) MemNN (Bordes et al., 2015), an implementation of the storage network for SimpleQuestions task. (ii) Encoder decoder (Golub and He, 2016), the state-of-the-art system in this task, a character-level, attention-based encoder decoder LSTM (Hochreiter and Schmidhuber, 1997) model. Both MemNN and encoder decoder decoders use multiple data resources (WebQuestion, SimpleQuestions and Paraphrases (Fader et al., 2014) and they are ensembles with improvements over non-ensemble individual systems from 62.2 to 63.9 on FBB5M (Bordes et al., 2015) and from 65.9 to 66.2 on FB2M (Golub and He, 2016). Our system significantly exceeds both baselines, although it uses only a single dataset (Simpleons)."}, {"heading": "6.4 Effect of Attentive Maxpooling (AMP)", "text": "We compare AMP (a major contribution by us) with three CNN attention mechanisms that are representative of related work in modeling two parts of text: (i) HABCNN: Hierarchical attention-based CNN (Yin et al., 2016a); (ii) ABCNN: Attentionbased CNN (Yin et al., 2016b); (iii) APCNN: CNN with attentive pooling (Santos et al., 2016).7Stanford NER (Finkel et al., 2005) also performed a task from it. Since attentive matching of predicate patterns is only one part of our commonly trained system, it is difficult to judge whether an attentive CNN performs better than alternatives. We therefore create a relationship classification (RC) to compare AMP with baseline schemes. The RC task is created on the basis of SimpleQuestions: each question MP1: subjective AB2 gold labels are evaluated first with the AB739 basic predicate."}, {"heading": "7 Conclusion", "text": "In this paper, we examined CNNs for the SimpleQuestions task. We made two main contributions. (i) A simple and effective entity linker that provides greater coverage of ground truth entities. (ii) An attentive maxipooling that is stacked over the fold layer and more effectively models the relationship between predicate and question pattern."}], "references": [{"title": "More accurate question answering on freebase", "author": ["Bast", "Haussmann2015] Hannah Bast", "Elmar Haussmann"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Bast et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of SIGMOD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Question answering over freebase with multi-column convolutional neural networks", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of KDD,", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Incorporating nonlocal information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Character-level question answering with attention", "author": ["Golub", "He2016] David Golub", "Xiaodong He"], "venue": "arXiv preprint arXiv:1604.00727", "citeRegEx": "Golub et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Golub et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Attentive pooling networks. arXiv preprint arXiv:1602.03609", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "FLORS: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Alberto Garc\u0131\u0301aDur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generating quiz questions from knowledge graphs", "author": ["Mohamed Yahya", "Klaus Berberich"], "venue": "In Proceedings of WWW,", "citeRegEx": "Seyler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seyler et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "On the generalization error bounds of neural networks under diversity-inducing mutual angular regularization", "author": ["Xie et al.2015] Pengtao Xie", "Yuntian Deng", "Eric Xing"], "venue": "arXiv preprint arXiv:1511.07110", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Enhancing freebase question answering using textual evidence", "author": ["Xu et al.2016] Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao"], "venue": "arXiv preprint arXiv:1603.00957", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Robust question answering over the web of linked data", "author": ["Yahya et al.2013] Mohamed Yahya", "Klaus Berberich", "Shady Elbassuoni", "Gerhard Weikum"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Yahya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yahya et al\\.", "year": 2013}, {"title": "Information extraction over structured data: Question answering with Freebase", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Freebase QA: Information extraction or semantic parsing", "author": ["Yao et al.2014] X. Yao", "J. Berant", "B. Van Durme"], "venue": "In Proceedings of ACL Workshop on Semantic Parsing,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Lean question answering over freebase from scratch", "author": ["Xuchen Yao"], "venue": "In Proceedings of NAACLHLT,", "citeRegEx": "Yao.,? \\Q2015\\E", "shortCiteRegEx": "Yao.", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Online updating of word representations for part-of-speech tagging", "author": ["Yin et al.2015] Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "2016a. Attention-based convolutional neural network for machine comprehension", "author": ["Yin et al.2016a] Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "Proceedings of NAACL Human-Computer QA Workshop", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "2016b. ABCNN: Attentionbased convolutional neural network for modeling sentence pairs. TACL", "author": ["Yin et al.2016b] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Deep learning for answer sentence selection", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "Proceedings of ICLR workshop", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.", "startOffset": 70, "endOffset": 94}, {"referenceID": 4, "context": "This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts.", "startOffset": 94, "endOffset": 115}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al.", "startOffset": 71, "endOffset": 161}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al.", "startOffset": 71, "endOffset": 180}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al.", "startOffset": 71, "endOffset": 207}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al.", "startOffset": 71, "endOffset": 226}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)).", "startOffset": 71, "endOffset": 244}, {"referenceID": 8, "context": "munity QA sites (Fader et al., 2013) and in search query logs.", "startOffset": 16, "endOffset": 36}, {"referenceID": 32, "context": "This procedure resembles answer selection (Yu et al., 2014) in which a system is asked", "startOffset": 42, "endOffset": 59}, {"referenceID": 4, "context": "Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 14, "context": "These observations motivate us to include two kinds of convolutional neural networks (CNN, LeCun et al. (1998)) in our deep learning system.", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013).", "startOffset": 120, "endOffset": 141}, {"referenceID": 1, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 23, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 28, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 3, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 5, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 22, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 8, "context": "SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 7, "context": ", 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011).", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "(i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and", "startOffset": 73, "endOffset": 93}, {"referenceID": 7, "context": "SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours \u2013 they employ CNNs to match entity-mention and predicate-pattern.", "startOffset": 36, "endOffset": 150}, {"referenceID": 3, "context": "pleQA was introduced by Bordes et al. (2015). (Golub and He, 2016) is the state of the art.", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "pleQA was introduced by Bordes et al. (2015). (Golub and He, 2016) is the state of the art. Bordes et al. (2015) tackle this problem by an embeddingbased QA system developed under the framework of Memory Networks (Weston et al.", "startOffset": 24, "endOffset": 113}, {"referenceID": 32, "context": ", Yu et al. (2014), Yin", "startOffset": 2, "endOffset": 19}, {"referenceID": 15, "context": "(2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given FB2M FB5M", "startOffset": 9, "endOffset": 30}, {"referenceID": 18, "context": "We are also inspired by work that generates natural language questions given knowledge graph triples (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 17, "context": "We are also inspired by work that generates natural language questions given knowledge graph triples (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 2, "context": ", 2013) and Freebase (Bollacker et al., 2008).", "startOffset": 21, "endOffset": 45}, {"referenceID": 10, "context": "We tried Stanford NER (Finkel et al., 2005), but it did not work well for our noisy text.", "startOffset": 22, "endOffset": 43}, {"referenceID": 20, "context": "We thus resort to three top-performing POS taggers: (i) Stanford POS tagger3 (Toutanova et al., 2003); (ii) FLORS4 (Schnabel and Sch\u00fctze, 2014; Yin et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 29, "context": ", 2003); (ii) FLORS4 (Schnabel and Sch\u00fctze, 2014; Yin et al., 2015), the state-of-the-art POS tagger in domain adaptation scenario (its strength in do-", "startOffset": 21, "endOffset": 67}, {"referenceID": 6, "context": "We employ Adagrad (Duchi et al., 2011), L2 regularization and diversity regularization (Xie et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 21, "context": ", 2011), L2 regularization and diversity regularization (Xie et al., 2015).", "startOffset": 56, "endOffset": 74}, {"referenceID": 4, "context": "(i) MemNN (Bordes et al., 2015), an implementation of memory network for SimpleQuestions task.", "startOffset": 10, "endOffset": 31}, {"referenceID": 9, "context": "Both MemNN and Encoder-Decoder use multiple data resources (WebQuestion, SimpleQuestions and Paraphrases (Fader et al., 2014)) and they are ensembles with improvements over non-ensemble single systems from 62.", "startOffset": 105, "endOffset": 125}, {"referenceID": 4, "context": "9 on FB5M (Bordes et al., 2015) and from 65.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": ", 2016b); (iii) APCNN: CNN with attentive pooling (Santos et al., 2016).", "startOffset": 50, "endOffset": 71}, {"referenceID": 10, "context": "Stanford NER (Finkel et al., 2005) performed worse.", "startOffset": 13, "endOffset": 34}, {"referenceID": 15, "context": "902 0 OWA-APCNN (Santos et al., 2016) .", "startOffset": 16, "endOffset": 37}], "year": 2016, "abstractText": "This work focuses on answering singlerelation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a twostep pipeline: entity linking and fact selection. In fact selection, we match the subject entity in fact with the entity mention in question by a character-level convolutional neural network (char-CNN), and match the predicate in fact with the question by a word-level CNN (wordCNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker of SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.", "creator": "LaTeX with hyperref package"}}}