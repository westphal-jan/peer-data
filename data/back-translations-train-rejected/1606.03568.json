{"id": "1606.03568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Word Sense Disambiguation using a Bidirectional LSTM", "abstract": "In this paper we present a model that leverages a bidirectional long short-term memory network to learn word sense disambiguation directly from data. The approach is end-to-end trainable and makes effective use of word order. Further, to improve the robustness of the model we introduce dropword, a regularization technique that randomly removes words from the text. The model is evaluated on two standard datasets and achieves state-of-the-art results on both datasets, using identical hyperparameter settings.", "histories": [["v1", "Sat, 11 Jun 2016 08:12:02 GMT  (24kb)", "http://arxiv.org/abs/1606.03568v1", null], ["v2", "Fri, 18 Nov 2016 22:47:07 GMT  (37kb)", "http://arxiv.org/abs/1606.03568v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mikael k{\\aa}geb\\\"ack", "hans salomonsson"], "accepted": false, "id": "1606.03568"}, "pdf": {"name": "1606.03568.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kageback@chalmers.se", "hans.salomonsson@chalmers.se"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.03 568v 1 [cs.C L] 11 Jun 2016"}, {"heading": "1 Introduction", "text": "For example, the word rock refers to both a stone and a genre of music, but in the phrase \"Without the guitar, there would be no rock music,\" the meaning is no longer ambiguous. From this example, it is easy to see that the context surrounding the word defines the meaning. However, it is not so obvious that this is a difficult task. To see this, one must instead look at the phrase \"hard rock crushes heavy metal,\" in which all the words themselves point to stone, but together they define the word token as music."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Long short-term memory", "text": "Long-term memory (LSTM) is a gated type of recurrent neural network (RNN). LSTMs were introduced by Hochreiter and Schmidhuber (1997) to enable RNNs to better capture long-term dependencies when used to model sequences, which is achieved by allowing the model to copy the state between periods without forcing the state through non-linearity. Instead, the flow of information is regulated by multiplicative gates, in = \u03c3 (W xixn + W hihn \u2212 1 + W cicn \u2212 1 + bi) fn = \u03c3 (W xfxn + W hen \u2212 1 + W cfcf) on = \u03c3 (W xoxn + W hollow \u2212 1 + W cocn \u2212 1 + bo) the state of the network is stored in a memory cell, cn = fncn \u2212 1 + W spring \u2212 in tanh (W xxxn + W xn + 1 chn \u2212 1 cfnn), \u2212 1 W chnn \u2212 1 chnn is stored in a memory cell."}, {"heading": "2.2 Bidirectional LSTM", "text": "The bidirectional variant of the LSTM (BLSTM) (Graves and Schmidhuber, 2005) is a simple adjustment of the LSTM described in Section 2.1, where the state consists at any given time of two LSTMs, one to the left and one to the right. For WSD, this means that the state has information about both preceding and subsequent words, which in many cases is absolutely necessary to properly classify the meaning."}, {"heading": "2.3 Global vectors for word representation", "text": "Global Vectors for Word Representation (GloVe), introduced by Pennington et al. (2014), is a hybrid approach to word representation that combines a log-linear model popularized by Mikolov et al. (2013) with count-based random statistics to more efficiently capture global statistics. GloVe vectors are trained unsupervised, typically on large amounts of data, and are able to capture fine-grained semantic and syntactic information about words, which can then be used to initialize the input layer of a neural network or other NLP model."}, {"heading": "2.4 Regularization", "text": "One of the most basic control techniques is to add Gaussian noise to the input (Sietsma and Dow, 1991), which can be regarded as a kind of data augmentation, i.e. distortion of training data to produce slightly different examples for each run through the data, making it difficult for the model to simply memorize the training set. Another, newer, noise-based control technology is the dropout introduced by Srivastava et al. (2014). Dropout reduces the capacity of the model by randomly deleting information from a layer of neurons by setting a subset of them to zero. This forces the network to learn independent subnetworks, which similarly improves generalization by training as many separate models and combining the results."}, {"heading": "3 Dropword", "text": "In addition to the regularization techniques described in Section 2.4, we introduce a novel regularization system called dropword. The idea behind dropword is similar to that of dropout, but instead of randomly removing a portion of the activations, we randomly replace some words in context with a < drop > tag. The tag is treated like any other word in the vocabulary and has a corresponding word embedding that is trained. This process repeats itself over time, so that the words change over time. The motivation for the dropword is to reduce dependence on individual words in the training context. This technique can be generalized to other types of sequential input, not just words."}, {"heading": "4 The Model", "text": "The architecture of the model shown in Figure 1 consists of a Softmax layer, a hidden layer, and a BLSTM. See Section 2.2 for more details on BLSTM. The BLSTM and the hidden layer share parameters across all word types and senses, while the Softmax is parameterized by word type and selects the appropriate weight matrix and bias vector for each word type. This structure allows the model to divide statistical strength across different word types while remaining computationally efficient even for a large number of senses."}, {"heading": "4.1 Model definition", "text": "The input to the BLSTM at position n in document D is calculated with asxn = W xv (wn), n \u044e {1,..., | D |}. (2) Here v (wn) is the uniform representation of the word type corresponding to the wn-D. This has the effect of selecting the column of W x that corresponds to this word type. The resulting vector is called word embedding. Furthermore, W x can be initialized by means of pre-trained word embedding in order to use large uncommented datasets. In this thesis, GloVe vectors were used for this purpose.The model output y (n) is a predicted distribution of meaning for a word at position n and is verbalized via the hidden layer a = W ha [hn \u2212 1; hn + 1] + b ha (3) y (n) = softmax (W aywna + b ay wn), where biometric dimensions of the word a = W hen [1] and the right layer (Sat) are the (Sat) and the right layer (a)."}, {"heading": "4.2 Loss function", "text": "The parameters of the model are adjusted by minimising the cross entropy error L (I) = \u2212 \u2211 i-i-j-j-S (wi) ti, j-log yj (i) (5) via a series of symbols with indices I-1,.., | C |} within a training corpus C, each labelled with a target sense distribution ti-i-I."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data and Task", "text": "To evaluate our proposed model, we performed the example lexical task of SensEval 2 (SE2) and SensEval 3 (SE3). Training and test data as well as a standardized goal scorer are provided for both tasks to ensure fair comparisons."}, {"heading": "5.2 Experimental settings", "text": "A major challenge with these relatively small data sets is the risk of overadjustment of the model. To mitigate this problem, we use a 50% dropout, the hidden state h and the hidden plane to initialize the word embedding. Furthermore, GloVe vectors trained on Wikipedia and Gigaword are used, and we use 10% dropouts, 74 + 74 neurons in the BLSTM, embedding size of 100 and additive gaussian noise with the mean zero and a standard deviation of 0.2\u03c3i, where \u03c3i is the default deviation for embedding dimension i for all words in the embedding matrix, W x. \u03c3i is updated after each weight update. To speed up the training, we use a maximum context window length of 140 words. If we allow the model to use data further away than this does not improve the results, finally we use identical hyperparameter settings for the SE2 and SE3 model validity tests."}, {"heading": "5.3 Training details", "text": "The model was implemented in TensorFlow (Abadi et al., 2015) and optimized with Stochastic Gradient Descent with impulse (0.1) and batch size 100. Learning rate is initialized to 2.0 and then decreased exponentially by a decay factor of 0.96. All parameters of the model are initialized in U (\u2212 0.1, 0.1), except for distortions that are initialized to zero. Data is cleaned by removing various keywords. We do not remove unusual words, but to keep the vocabulary size manageable, we have replaced numbers with a < number > keyword. Words that do not occur in GloVe are initialized by N (0, 0.1)."}, {"heading": "5.4 Results", "text": "The results of the corresponding datasets can be seen in Table 1. 100JHU (R) was developed by Yarowsky et al. (2001) and achieved the best score of the example English lexical problem SE2 with an F1 score of 64.2. Their system used a rich feature space based on raw words, lemmas, POS tags, dictionaries, bi-grammar and tri-grammar collocations etc. as input to an ensemble classifier. Accordingly, htsa3 of Grozea (2004) won the lexical sample SE3 with an F1 score of 72.9. This system also used a rich set of features. These were used as inputs to a regulated least square classifier. IMS + adapted CW is a newer system of Taghipour and Ng (2015) that introduces word embedding into WSD, albeit individually, as a feature designed to support the SEOS system along with a series of effective POS tags."}, {"heading": "6 Conclusions & future work", "text": "We presented a BLSTM-based model for WSD that was able to effectively exploit word order and achieve up-to-date results on two sets of WSD data. It is in stark contrast to previous work, as it is a consistent traceable system, from text to sensory perception. Furthermore, we showed that the use of pre-formed word vectors improved the model's generalization capability. However, it is likely that this effect would decrease if the model were trained on more meaning-labeled data, which is an experiment we intend to conduct in the future. Partly due to the limited amount of data, we also developed the Dropword regulation technique, which we were able to improve under these limited circumstances. Another effect resulting from the limited data is that we had to greatly reduce and regulate the model, so we believe it is likely that it is possible to improve the reported results simply by scaling the model and training on more data."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the project Towards a knowledge-based culturomics, which is supported by a Framework Fellowship of the Swedish Research Council (2012-2016; dnr 2012-5738)."}], "references": [{"title": "Knowledge base unification via sense embeddings and disambiguation", "author": ["Luis EspinosaAnke", "Roberto Navigli"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Bovi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bovi et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Finding optimal parameter settings for high performance word sense disambiguation", "author": ["Cristian Grozea"], "venue": "In Proceedings of Senseval-3 Workshop", "citeRegEx": "Grozea.,? \\Q2004\\E", "shortCiteRegEx": "Grozea.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Combining relational and distributional knowledge for word sense disambiguation", "author": ["Johansson", "Pina2015] Richard Johansson", "Luis Nieto Pina"], "venue": "Nordic Conference of Computational Linguistics NODALIDA", "citeRegEx": "Johansson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Word Sense Disambiguation: a survey", "author": ["Roberto Navigli"], "venue": "ACM Computing Surveys,", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space. arXiv preprint arXiv:1504.06654", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Nieto Pi\u00f1a", "Johansson2015] Luis Nieto Pi\u00f1a", "Richard Johansson"], "venue": "In Proceedings of Recent Advances in Natural Language Processing,", "citeRegEx": "Pi\u00f1a et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pi\u00f1a et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Creating artificial neural networks that generalize", "author": ["Sietsma", "Dow1991] Jocelyn Sietsma", "Robert JF Dow"], "venue": "Neural networks,", "citeRegEx": "Sietsma et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Sietsma et al\\.", "year": 1991}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Semi-supervised word sense disambiguation using word embeddings in general and specific domains", "author": ["Taghipour", "Ng2015] Kaveh Taghipour", "Hwee Tou Ng"], "venue": "In The 2015 Annual Conference of the North American Chapter", "citeRegEx": "Taghipour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2015}, {"title": "Wordsense disambiguation for machine translation", "author": ["Luke Biewald", "Marc Teyssier", "Daphne Koller"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Lan-", "citeRegEx": "Vickrey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}, {"title": "The johns hopkins senseval2 system descriptions", "author": ["Silviu Cucerzan", "Radu Florian", "Charles Schafer", "Richard Wicentowski"], "venue": "In The Proceedings of the Second International Workshop", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "machine translation, information Retrieval, information Extraction (Vickrey et al., 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al.", "startOffset": 67, "endOffset": 104}, {"referenceID": 6, "context": "machine translation, information Retrieval, information Extraction (Vickrey et al., 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al.", "startOffset": 67, "endOffset": 104}, {"referenceID": 7, "context": ", 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al., 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015).", "startOffset": 82, "endOffset": 159}, {"referenceID": 0, "context": ", 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al., 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015).", "startOffset": 82, "endOffset": 159}, {"referenceID": 0, "context": ", 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015). However, for word representations the benefit goes two ways which was shown by Taghipour and Ng (2015) who used word embeddings to improve on WSD, and later by Johansson and Pina (2015) who combined word embeddings with a knowledge graph approach.", "startOffset": 40, "endOffset": 163}, {"referenceID": 0, "context": ", 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015). However, for word representations the benefit goes two ways which was shown by Taghipour and Ng (2015) who used word embeddings to improve on WSD, and later by Johansson and Pina (2015) who combined word embeddings with a knowledge graph approach.", "startOffset": 40, "endOffset": 246}, {"referenceID": 8, "context": "Global Vectors for Word Representation (GloVe), introduced by Pennington et al. (2014) is a hybrid approach to word representation that combine a log-linear model, made popular by Mikolov et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 5, "context": "(2014) is a hybrid approach to word representation that combine a log-linear model, made popular by Mikolov et al. (2013), with counting based cooccurrence statistics to more efficiently capture global statistics.", "startOffset": 100, "endOffset": 122}, {"referenceID": 11, "context": "Another, more recent noise based regularization technique is dropout, introduced by Srivastava et al. (2014). Dropout lowers the capacity of the model by randomly erasing information from a layer of neurons by setting a subset of them to zero.", "startOffset": 84, "endOffset": 109}, {"referenceID": 13, "context": "100JHU(R) was developed by Yarowsky et al. (2001) and achieved the best score on the English lexical sample task of SE2 with a F1 score of 64.", "startOffset": 27, "endOffset": 50}, {"referenceID": 2, "context": "Correspondingly, htsa3 by Grozea (2004) was the winner of the SE3 lexical sample task with a F1 score of 72.", "startOffset": 26, "endOffset": 40}, {"referenceID": 2, "context": "Correspondingly, htsa3 by Grozea (2004) was the winner of the SE3 lexical sample task with a F1 score of 72.9. This system also used a rich set of features. These were used as inputs to a regularized least square classifier. IMS+adapted CW is a more recent system by Taghipour and Ng (2015) that introduces word embeddings in WSD, albeit separately trained, as a feature together with, again, a rich set of features including POS tags, collocations and surrounding words.", "startOffset": 26, "endOffset": 291}], "year": 2016, "abstractText": "In this paper we present a model that leverages a bidirectional long short-term memory network to learn word sense disambiguation directly from data. The approach is end-to-end trainable and makes effective use of word order. Further, to improve the robustness of the model we introduce dropword, a regularization technique that randomly removes words from the text. The model is evaluated on two standard datasets and achieves stateof-the-art results on both datasets, using identical hyperparameter settings.", "creator": "LaTeX with hyperref package"}}}