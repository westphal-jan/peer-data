{"id": "1606.03628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "metricDTW: local distance metric learning in Dynamic Time Warping", "abstract": "We propose to learn multiple local Mahalanobis distance metrics to perform k-nearest neighbor (kNN) classification of temporal sequences. Temporal sequences are first aligned by dynamic time warping (DTW); given the alignment path, similarity between two sequences is measured by the DTW distance, which is computed as the accumulated distance between matched temporal point pairs along the alignment path. Traditionally, Euclidean metric is used for distance computation between matched pairs, which ignores the data regularities and might not be optimal for applications at hand. Here we propose to learn multiple Mahalanobis metrics, such that DTW distance becomes the sum of Mahalanobis distances. We adapt the large margin nearest neighbor (LMNN) framework to our case, and formulate multiple metric learning as a linear programming problem. Extensive sequence classification results show that our proposed multiple metrics learning approach is effective, insensitive to the preceding alignment qualities, and reaches the state-of-the-art performances on UCR time series datasets.", "histories": [["v1", "Sat, 11 Jun 2016 21:14:08 GMT  (1128kb,D)", "http://arxiv.org/abs/1606.03628v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiaping zhao", "zerong xi", "laurent itti"], "accepted": false, "id": "1606.03628"}, "pdf": {"name": "1606.03628.pdf", "metadata": {"source": "CRF", "title": "metricDTW: local distance metric learning in Dynamic Time Warping", "authors": ["Jiaping Zhao", "Zerong Xi", "Laurent Itti"], "emails": ["jiapingz@usc.edu", "zxi@usc.edu", "itti@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact it is so that most people are able to understand themselves and understand what they are doing. (...) In fact it is so that most people are able to understand themselves. (...) In fact it is so that most people are able to understand themselves. (...) It is so that people are able to understand themselves. (...) It is as if people are able to understand the world. (...) It is as if people are able to understand the world, to understand the world. (...) It is as if people are able to understand themselves. (...) It is as if people are able to understand the world. (...) It is as if people are able to understand the world, to understand the world. (...) It is as if people are able to understand the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the, in the world in the, in the world in the, in the world in the, in the world in the, in the world in the, in the world in the world in the, in the world in the, in the world in the, in the, in the world in the, in the world in the world in the, in the, in the world in the world in the, in the world in the, in the, in the world in the, in the world in the world in the, in the world in the, in the, in the world in the, in the world in the world in the, in the, in the, in the world in the, in the world in the world and in the world, in the, in the world and in the world in the world, in the, in the world in the, in the, in the world in the, in the world in the, in the world in the, in the, in the world in the, in the, in the world and in the, in the world, in the, in the world and in the, in the world in the"}, {"heading": "2 Related work", "text": "As already mentioned, our local metric learning framework essentially learns the meaning of different subsequences in an automatic and principled manner. There are several earlier works that focus on representative and discriminatory subsequences (image fields) from temporal sequences (images).The time series shapelet is introduced in [22], and it is a time series sub-sequence (pattern) that is discriminatory for class affiliation.The authors propose to list all possible subsequences, to evaluate their qualities by gaining information, and to form a decision tree classifier from the best-placed shapelets. Mining shapelets in their case is the search for more important subsequences, while neglecting less important subsequences. In the visual community there are several related works [19, 5, 4] all dedicated to the discovery of image fields at the middle level."}, {"heading": "3 Local distance metric learning in DTW", "text": "As mentioned above, local metric learning requires sequence alignment as input. While in most scenarios sequence alignment is intrinsically accurate or impossible to name, in experiments we first use DTW to align sequences and then use the calculated alignments for subsequentmetric learning. In this section, we first briefly review the DTW algorithm for sequence alignment and then introduce our multiple local metric learning algorithm for classifying time series."}, {"heading": "3.1 Dynamic Time Warping", "text": "DTW is an algorithm for aligning temporal sequences under certain constraints. In the face of two sequences P and Q of possible different lengths LP and LQ, namely P = (p1, p2,..., pLP) T and Q = (q1, q2,..., qLQ) T, and let d (P, Q), RLP \u00b7 LQ be the paired distance matrix, where d (i, j) is the distance between points pi and pj. A widely used distance measure is the square Euclidean distance (i.e. d (i, j) = pi \u2212 qj \u00b2 22. The goal of the temporal alignment between P and Q is to find two sequences of indices of equal length l corresponding index sources (i) in the time series P to index \u03b2 (i)."}, {"heading": "3.2 Local distance metric learning", "text": "After we have reached the path of the sub-sequence p through DTW, it is assumed that the environmental information is captured in two ways. (1) To measure the distance between a matched pair (pi, qj), we can use the distance between two matched pairs (pi, qj). (2) To measure the distance between a matched pair (pi, qj), we can use the distance between the two matched pairs, i.e., d (\u2212 pi, \u2212 qj), where \u2212 pi and \u2212 qj are each different points pi and qj. (2) In this way, the distance between P and Q is calculated as the accumulated descriptortions along p, i.e. (i, j) and p d (\u2212 pi, \u2212 qj). Here, the descriptor at some point is a feature that centered the representation of the sub-sequence and is the descriptor."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the performance of the proposed local metric learning method for classifying time series using 70 UCR datasets [3] that provide their standard training / test partitions for performance evaluation, empirically showing: (1) whether multiple local metric learning increases the accuracy of the time series classification of the 1NN classifier; (2) how the quality of the preceding alignments affects subsequent metric learning performance; (3) the impact of hyperparameter settings on metric learning performance."}, {"heading": "4.1 Experimental settings", "text": "When calculating the point-to-point distance in the DTW alignment (d (i, j) in (2)), we calculate the distance between their descriptors and use it as the distance between the original points in time. Obviously, the optimal alignment path sought in the DTW alignment (2) depends on the descriptor used for point-to-point distance calculations. Descriptors are used in the subsequent metric formation as well as the distance between the original points in time. Obviously, the optimal alignment path sought in the DTW series of VTW (2) depends on the descriptor used for point-to-distance calculations. Descriptors are also used in the subsequent metric formation to define the DTW distance (see Sec. 3.2).In experiments, we use three sub-sequence descriptors, including the raw sequence sequence, HOG-1D [23] and derivative class [2]."}, {"heading": "4.2 Effectiveness of local distance metric learning", "text": "First and foremost, it is about being able to achieve our goals, and being able to achieve our goals, \"he told the Deutsche Presse-Agentur.\" We have to be able to achieve our goals, \"he told the Deutsche Presse-Agentur.\" We have to play by the rules, \"he told the Deutsche Presse-Agentur."}, {"heading": "4.3 Effects of hyper-parameters", "text": "There is an important hyperparameter in metric learning: the number of clusters of descriptors. In experiments, we align local metrics and learn them under the gradient descriptor, as well as during metric learning, we specify a different number of descriptor clusters, i.e. k = {5, 10, 15, 20, 25, 30}, learn metrics by solving (3), and plot the performance improvements of 1NN in Fig. 5. Among different k's, the majority of improvements are above 0, and the signed Wilcoxon test delivers p-values of 0.003 / 0.026 / 0.005 / 0.021 / 0.002 / 0.017 under k = 5 / 10 / 15 / 20 / 25 / 30, which show significant improvements under different k's."}, {"heading": "4.4 Comparison with the state of the art algorithm", "text": "As shown in [15, 20, 1, 17], the 1NN classifier is very difficult to surpass with the DTW distance as a measure of similarity (1NNDTW). In this case, we use 1NN-DTW as the initial value and compare our algorithms with it. In 1NN-DTW, the alignment is also calculated by the DTW, however, no descriptor is used, i.e. the point-to-point distance is calculated directly by the square euclidean distance between these two points, rather than by their descriptor distance. The DTW distance between two aligned sequences is calculated as accumulated euclidean point-to-point distances, without the descriptor also being used. In our case, we use the HOG-1D descriptor to align the sequences and local metrics. \u2212 We draw the temporal classification performance in Figure 6: Our algorithm with (without learning rates), 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, and local metrics. \u2212 We draw the temporal classification performance in Figure 6: 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, and local metrics."}, {"heading": "5 Conclusion and discussion", "text": "We have shown empirically that the metric learning process always improves the accuracy of the 1NN time series classification, ignoring the qualities of previous DTW alignments. Our algorithm gains the 1NN-DTW algorithm significantly on 70 UCR time series datasets and sets a record for further comparisons. DTW time series classification consists of two consecutive steps: time series alignment and subsequent classification. In this essay, metric learning occurs after the alignment phase is complete, and information in metric learning does not migrate back to the previous alignment step. A naive extension is to perform alignment and metric learning in an iterative process."}], "references": [{"title": "An experimental evaluation of nearest neighbour time series classification", "author": ["A. Bagnall", "J. Lines"], "venue": "arXiv preprint arXiv:1406.4757,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The ucr time series classification", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": "www.cs.ucr.edu/~eamonn/time_series_data/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "What makes paris look like paris", "author": ["C. Doersch", "S. Singh", "A. Gupta", "J. Sivic", "A. Efros"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Dynamic time warp (dtw) in matlab", "author": ["D. Ellis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Metric learning for temporal sequence alignment", "author": ["D. Garreau", "R. Lajugie", "S. Arlot", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Computing and visualizing dynamic time warping alignments in r: the dtw package", "author": ["T. Giorgino"], "venue": "Journal of statistical Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["M. Grant", "S. Boyd"], "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version", "author": ["M. Grant", "S. Boyd"], "venue": "//cvxr.com/cvx,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Style translation for human motion", "author": ["E. Hsu", "K. Pulli", "J. Popovi\u0107"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Derivative dynamic time warping", "author": ["E. Keogh", "M. Pazzani"], "venue": "In SDM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Exact indexing of dynamic time warping", "author": ["E. Keogh", "C. Ratanamahatana"], "venue": "Knowledge and information systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Continuous action recognition based on sequence alignment", "author": ["K. Kulkarni", "G. Evangelidis", "J. Cech", "R. Horaud"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G. Webb", "A. Nicholson", "Y. Chen", "E. Keogh"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Fundamentals of speech recognition", "author": ["L. Rabiner", "B.-H. Juang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B. Campana", "A. Mueen", "G. Batista", "B. Westover", "Q. Zhu", "J. Zakaria", "E. Keogh"], "venue": "In SIGKDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1978}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A. Efros"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["X. Wang", "A. Mueen", "H. Ding", "G. Trajcevski", "P. Scheuermann", "E. Keogh"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "In SIGKDD,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 19, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 0, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 16, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 20, "context": "We adapt LMNN [21] to formulate our multiple metric learning in DTW.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "[21, 2].", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[21, 2].", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "We closely follow Large Margin Nearest Neighbor (LMNN) [21] to formulate local metric learning in DTW.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "In [21], the Mahalanobis metric is learned with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In our case, we use the same max margin framework, with the only difference that: examples in [21] are feature points in some fixed-dimension space, and distances between examples are squared Mahalanobis distances, while in our case, examples are temporal sequences, and distances between examples are DTW distances.", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "We extensively test the performance of metricDTW for time series classification on 70 UCR time series datasets [3], and experimental results show that (1) the learned local metrics, compared with the default Euclidean metric, improve the 1NN classification accuracies significantly; (2) given alignment paths of different qualities, the subsequent metric learning consistently boosts classification accuracies significantly, showing that the proposed metric learning approach is invariant to the preceding alignment step; (3) our metric learning algorithm outperforms the state-of-the-art time series classification algorithm (1NN-DTW) significantly on UCR datasets, therefore, we set a new record for future time series classification comparison.", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "Time series shapelet is introduced in [22], and it is a time series subsequence (patterns) which is discriminative of class-membership.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 4, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 3, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 18, "context": "They [19, 5] pose the discriminative patch search procedure as a discriminative clustering process, in which they selectively choose important patches but discarding other common patches.", "startOffset": 5, "endOffset": 12}, {"referenceID": 4, "context": "They [19, 5] pose the discriminative patch search procedure as a discriminative clustering process, in which they selectively choose important patches but discarding other common patches.", "startOffset": 5, "endOffset": 12}, {"referenceID": 20, "context": "Our work is most similar to and largely inspired by LMNN [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "In [21], Weinbergre and Saul extend LMNN to learn multiple local distance metrics, which is exploited in our work as well.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "However, we are still sufficiently different: first the labeled examples in our case are temporal sequences; second, the DTW distance between two examples is jointly defined by multiple metrics, while in [21], distance between two examples are determined by a single metric.", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "In [7], Garreau et al propose to learn a Mahalanobis distance metric to perform DTW sequence alignment.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 12, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 5, "context": "The above formula is a typical dynamic programming recursion, and can be solved efficiently in O(LP\u00d7LQ) time by a dp algorithm [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 17, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 146, "endOffset": 153}, {"referenceID": 7, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 146, "endOffset": 153}, {"referenceID": 11, "context": "In following experiments, we always adopt the second way to define the DTW distance, and we use three shape descriptors, namely the raw-subsequence, HOG-1D [23] and the gradient sequence [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 21, "context": "If the squared Euclidean distance is used, then DTW distance is calculated as D(P,Q) = \u2211 (i,j)\u2208p \u2016 \u2212 \u2192pi \u2212 \u2212 \u2192qj \u2016, which is essentially a equally weighted sum of distances between descriptors (subsequences), however, as shown in [22], some subsequences are more class-membership predictive, while others are less discriminative.", "startOffset": 230, "endOffset": 234}, {"referenceID": 20, "context": "In order to learn these local metrics from labeled sequence data, we follow LMNN [21] closely and pose our problem as a max margin problem: the local Mahalanobis metrics are trained such that the k-nearest neighbors of any sequence always belong to the same class while sequences of different classes are separated by a large margin.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "We use the exact notations in LMNN, and the only place to change is to replace the squared Mahalanobis point-to-point distance in [21] by the DTW distance.", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "We refer readers to [21] for notation meanings.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "In this section, we evaluate the performances of the proposed local metric learning method for time series classification on 70 UCR datasets [3], which provide their standard training/test partitions for performance evaluation.", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "In experiments, we use three subsequence descriptors, including raw-subsequence, HOG-1D [23] and the derivative sequence [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "We follow [12] exactly to compute derivative at each point, and the derivative descriptor is 30D by definition.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "The linear program (3) is solved by the CVX package [10, 9].", "startOffset": 52, "endOffset": 59}, {"referenceID": 8, "context": "The linear program (3) is solved by the CVX package [10, 9].", "startOffset": 52, "endOffset": 59}, {"referenceID": 14, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 16, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}], "year": 2016, "abstractText": "We propose to learn multiple local Mahalanobis distance metrics to perform knearest neighbor (kNN) classification of temporal sequences. Temporal sequences are first aligned by dynamic time warping (DTW); given the alignment path, similarity between two sequences is measured by the DTW distance, which is computed as the accumulated distance between matched temporal point pairs along the alignment path. Traditionally, Euclidean metric is used for distance computation between matched pairs, which ignores the data regularities and might not be optimal for applications at hand. Here we propose to learn multiple Mahalanobis metrics, such that DTW distance becomes the sum of Mahalanobis distances. We adapt the large margin nearest neighbor (LMNN) framework to our case, and formulate multiple metric learning as a linear programming problem. Extensive sequence classification results show that our proposed multiple metrics learning approach is effective, insensitive to the preceding alignment qualities, and reaches the state-ofthe-art performances on UCR time series datasets.", "creator": "LaTeX with hyperref package"}}}