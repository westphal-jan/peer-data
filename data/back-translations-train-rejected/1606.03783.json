{"id": "1606.03783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression", "abstract": "Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.", "histories": [["v1", "Sun, 12 Jun 2016 23:50:19 GMT  (67kb,D)", "http://arxiv.org/abs/1606.03783v1", "International Conference on Theory and Practice of Digital Libraries 2016 (accepted)"]], "COMMENTS": "International Conference on Theory and Practice of Digital Libraries 2016 (accepted)", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["pedro chahuara", "thomas lampert", "pierre gancarski"], "accepted": false, "id": "1606.03783"}, "pdf": {"name": "1606.03783.pdf", "metadata": {"source": "CRF", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression", "authors": ["Pedro Chahuara", "Thomas Lampert", "Pierre Gan\u00e7arski", "P. Gan\u00e7arski"], "emails": ["pedro.chahuara@unistra.fr", "lampert@unistra.fr", "pierre.gancarski@unistra.fr"], "sections": [{"heading": null, "text": "Keywords: Collaborative Question Answering, Question and Answer Retrieval, LDA, Neural Network, Topic Modeling, Regression"}, {"heading": "1 Introduction", "text": "Over the last decade, the Internet based on answers (CQA) has grown in popularity. These platforms provide a social environment where people seek answers to questions, and where the answers are offered by other members of the community. Users ask questions in the natural language, while similar systems are found in the industry, for example in retail and business websites, where users can ask questions about the product and a group of specialists. This content has drawn the attention of researchers to a number of domains."}, {"heading": "2 Related Work", "text": "The main challenge in retrieving related questions and answers in a QA database in the face of a new question is the lexical gap that can exist between two semantically similar questions. In general, a method aimed at solving the problem of the question retrievally is composed of at least two main parts: a document representation that can properly express the semantics and context of the QAs in the database; and a mechanism for comparing the similarity of documents containing their representations. The most common topic criteria for document representation in the literature are those based on Word Pocket (BOW), which explicitly represents each of the words in the document. Comparison is achieved by calculating the number of matching words between two BOW representations. There are several variations of this method class, with each word weighted having specific properties in the datasets, such as tf-idf and BM25. This method class is capable of measuring the two documents against each other."}, {"heading": "3 Methodology", "text": "A corpora C of the size L = | C | consists of many question-and-answer pairs: C = {(q1, a1), (q2, a2),..., (qL, aL), where Q = {q1, q2,.., qL} and A = {a1, a2,.., (q2, ai),. (qL, ai),................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.1 Latent Dirichlet Allocation & Distributed Word Representation", "text": "In this paper, we claim that theme modeling provides a representation of the elements in C that facilitates the discovery of semantically similar questions, especially if these similar questions do not have words in universally valid terms. [17] It is a generative probabilistic model that enables us to describe a collection of discrete observations regarding latent variables. [17] The disk notation that LDA represents is presented in Figure 2. [17] With respect to a corpora, the generation of each document is modeled using two stochastic independent processes and can be summarized as follows. [17] For each document d in Collection D, a distribution overtopic is randomly selected in which the Dir theme is applied. [17] For each word in document d: (a), we select a theme from the theme distribution in Step 1. zd: (n) Mult (b) we select a word from the vocabulary distribution wd, n (n)."}, {"heading": "3.2 Nonlinear Multinomial Regression", "text": "When a quantum question q * is entered into the space of the linked questions, the Qi distribution of the topic Q * is compared. A regression model is therefore required to obtain an estimate of the phenomenon Q *, which is associated with a distribution of the topics in the QA theorem \u03b8 QA *. Mapping the distribution of question topics to the distribution of question-answer topics avoids problems that arise when limited vocabulary is used in a question. This information is supplemented by the information derived from the set of answer terms, i.e. by mapping a quantum question to the space of question-answers, it is possible to calculate its similarity with words that do not exist in the question vocabulary (and are therefore not represented in the topic distribution TQ). Performing this mapping also provides a means of modelling the relationship between question semantics and existing question-answer systems."}, {"heading": "4 Evaluation", "text": "This section describes the data, experimental setup and comparison algorithms used to evaluate the proposed approach."}, {"heading": "4.1 Data", "text": "The first two categories are the categories Health and Computer & Internet (hereinafter referred to as Computers) in the publicly available Yahoo! Questions L6 (Yahoo! Answers Comprehensive Questions and Answers version 1.0) dataset1; the second two categories are the categories Physics and Geographic Information Systems (GIS) taken from the publicly available StackExchange (SE) dataset2; the questionnaire 1 available at http: / / webscope.sandbox.yahoo.com / catalog.php? datatype = l 2 Available from https: / / archive.org / details / stackexchangesets extracted from the Yahoo! dataset.sandbox.sandbox.sandbox.yahoo.com / catalog.php? datatype = l 2 available from http: / / archive.org / stackexchangegebensets extracted from the Yahoo! dataset by concatenating the text and description (if available), the etype and the dataset were extracted from the SE."}, {"heading": "4.2 Results", "text": "The proposed method, henceforth referred to as LDA +, was compared against four state-of-the-art algorithms: Translation1, the IBM translation approach proposed by Jeon et al. [1]; Translation2, the language model of combined translation and probability query proposed by Xue et al. [5]; an autoencoder-based method proposed by Socher et al. [26]; the benefits of word2vec, LDA * (as described in Section 3 without word2vec); and the benefits of the LDA \u2020 regression level (as described in Section 3 without the regression step). Mean average precision (MAP) and precision at N (P @ N) are used to summarize the retrieval performance within each category."}, {"heading": "5 Discussion", "text": "In contrast, the LDA \u2020, the LDA +, which is based on common themes, and the words discussed in this section are characterized by the decrease in their probability in the health category. To illustrate this, Table 3 provides an example of retrieved questions. (The points discussed in this section have been observed in all categories, but we present examples from the health category.) In the first half of the table, the retrieved questions are presented with the words LDA + that are related to this."}, {"heading": "6 Conclusions", "text": "This paper introduces a novel model that combines theme modeling with Word2vec and a regression level for the ranking of relevant question-answer pairs within collaborative question-answering platforms. The performance of the proposed method has been evaluated on the basis of several real-world data sets, and it has been shown that it separately outperforms translation-based methods and LDA for each innovation, especially when the data set contains long questions and answers. This is achieved by enabling the model to overcome the differences in vocabulary used in questions and answers, by helping to deal with the scarcity (due to its relatively short length) that often occurs in questions, and by enabling the method to use all available information."}], "references": [{"title": "Finding similar questions in large question and answer archives", "author": ["J. Jeon", "B.W. Croft", "J. Ho Lee"], "venue": "CIKM", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "A topic clustering approach to finding similar questions from large question and answer archives", "author": ["Zhang", "W.N"], "venue": "PLoS ONE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A syntactic tree matching approach to finding similar questions in community-based QA services", "author": ["K. Wang", "Z. Ming", "T.S. Chua"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Question answering passage retrieval using dependency relations", "author": ["H. Cui", "R. Sun", "K. Li", "M.Y. Kan", "T.S. Chua"], "venue": "In: SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Retrieval models for question and answer archives", "author": ["X. Xue", "J. Jeon", "W.B. Croft"], "venue": "In: SIGIR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bridging lexical gaps between queries and questions on large online Q&A collections with compact translation models", "author": ["Lee", "J.T"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Combining lexical semantic resources with question & answer archives for translation-based answer finding", "author": ["D. Bernhard", "I. Gurevych"], "venue": "In: ACL-IJCNLP. Volume", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "CQArank: Jointly model topics and expertise in community question answering", "author": ["L Yang"], "venue": "CIKM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Bridging the lexical chasm: statistical approaches to answer-finding", "author": ["A. Berger", "R. Caruana", "D. Cohn", "D. Freitag", "V. Mittal"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Learning the latent topics for question retrieval in community QA", "author": ["L. Cai", "G. Zhou", "K. Liu", "J. Zhao"], "venue": "IJCNLP", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The application of the topic modeling to question answer retrieval", "author": ["J. Vasiljevi\u0107", "M. Ivanovi\u0107", "T. Lampert"], "venue": "ICIST", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["S.E. Robertson", "S. Walker"], "venue": "In: SIGIR", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "The mathematics of statistical machine translation: paramter estimation", "author": ["P Brown"], "venue": "Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Improving question retrieval in community question answering using world knowledge", "author": ["G Zhou"], "venue": "In: IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Entity based Q&A retrieval", "author": ["A. Singh"], "venue": "EMNLP", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Statistical machine translation improves question retrieval in community question answering via matrix factorization", "author": ["G Zhou"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei", "D.M"], "venue": "JMLR", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Modeling community questionanswering archives", "author": ["Z. Zolaktaf", "F. Riahi", "M. Shafiei", "E. Milios"], "venue": "Proceedings of the Workshop on Computational Social Science and the Wisdom of Crowds at NIPS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Word features for latent dirichlet allocation", "author": ["J Petterson"], "venue": "In: NIPS. Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["L. Yao", "D. Mimno", "A. McCallum"], "venue": "In: SIGKDD", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Evaluation methods for topic models", "author": ["H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "In: ICML", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Pattern recognition and neural networks", "author": ["B. Ripley"], "venue": "Camb UP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Neural networks and the multinomial logit for brand choice modelling: a hybrid approach", "author": ["Y. Bentz", "D. Merunka"], "venue": "J. Forec", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R Socher"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 1, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 2, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 3, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 5, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 6, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 7, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 98, "endOffset": 104}, {"referenceID": 8, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 4, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 7, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 0, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 119, "endOffset": 125}, {"referenceID": 8, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].", "startOffset": 211, "endOffset": 216}, {"referenceID": 4, "context": "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].", "startOffset": 211, "endOffset": 216}, {"referenceID": 4, "context": "demonstrated that combining information derived from existing questions and their answers outperforms the other strategies [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 9, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 10, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 11, "context": "There exist several variations of this class of methods, each weighting words that have specific properties in the dataset, such as tf-idf and BM25 [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Their method consists of two stages: first a set of semantically similar questions are found by matching their answers using a query-likelihood language model; and subsequently, word translation probabilities are estimated using the IBM translation model 1 [13].", "startOffset": 257, "endOffset": 261}, {"referenceID": 4, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 5, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 6, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 13, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 82, "endOffset": 89}, {"referenceID": 14, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 82, "endOffset": 89}, {"referenceID": 4, "context": "[5] propose an extension that combines the IBM translation model (applied to the questions) with a query likelihood language model (applied to the answers).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.", "startOffset": 76, "endOffset": 84}, {"referenceID": 15, "context": "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.", "startOffset": 76, "endOffset": 84}, {"referenceID": 16, "context": "Since the topics that characterise a document can be considered a semantic representation, it is possible to use topic distributions inferred using a method such as Latent Dirichlet Allocation (LDA) [17] to measure the semantic similarity between documents in a corpora.", "startOffset": 199, "endOffset": 203}, {"referenceID": 1, "context": "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "[10] combine the result of LDA and translation models; Vasiljevi\u0107 et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] explore combining a document\u2019s word count and topic model similarity into one measure; and Yang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] form a generative probabilistic method to jointly model QA topic distributions and user expertise.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] model the question topics and then use them to condition the answer topics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Latent Dirichlet Allocation (LDA) [17] is a generative probabilistic model that enables us to describe a collection of discrete observations in terms of latent variables.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "A solution is to treat words as features [19] and the method used to calculate a word\u2019s features then influences its topic membership.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "[20] introduced the continuous bag-of-words and Skip-gram neural network models that produce a continuous-valued vectorial word representation by exploiting the content of large textual databases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In order to implement this modification, the Gibbs Sampling-based algorithm proposed by [21] was adapted so that at each step the probability of topic t being present in document d given word w is estimated as follows:", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "01 [21, 22].", "startOffset": 3, "endOffset": 11}, {"referenceID": 20, "context": "Small values of \u03b1 and \u03b2 result in a fine-grained decomposition into topics that address specific areas [21,22].", "startOffset": 103, "endOffset": 110}, {"referenceID": 21, "context": "When a query question q\u2217 is entered the left-to-right method [23] is used to infer its topic distribution, \u03b8 \u2217 .", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 23, "context": "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 20, "context": "[21], was used for Topic Modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1]; Translation2, the combined translation and querylikelihood language model proposed by Xue et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]; an autoencoder based method proposed by Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[26]; to establish the benefit of word2vec, LDA\u2217 (as described within Section 3 excluding word2vec); and to establish the benefit of the regression stage, LDA\u2020 (as described within Section 3 excluding the regression step).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 9, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 13, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 13, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 14, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 0, "context": "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.", "startOffset": 40, "endOffset": 46}, {"referenceID": 4, "context": "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.", "startOffset": 40, "endOffset": 46}], "year": 2016, "abstractText": "Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.", "creator": "LaTeX with hyperref package"}}}