{"id": "1606.03864", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Neural Associative Memory for Dual-Sequence Modeling", "abstract": "Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.", "histories": [["v1", "Mon, 13 Jun 2016 09:08:04 GMT  (225kb,D)", "http://arxiv.org/abs/1606.03864v1", "To appear in RepL4NLP at ACL 2016"], ["v2", "Tue, 14 Jun 2016 07:59:18 GMT  (224kb,D)", "http://arxiv.org/abs/1606.03864v2", "To appear in RepL4NLP at ACL 2016"]], "COMMENTS": "To appear in RepL4NLP at ACL 2016", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["dirk weissenborn"], "accepted": false, "id": "1606.03864"}, "pdf": {"name": "1606.03864.pdf", "metadata": {"source": "CRF", "title": "Neural Associative Memory for Dual-Sequence Modeling", "authors": ["Dirk Weissenborn"], "emails": ["dirk.weissenborn@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the city, where it is as far as never before that it is a place where it is a place, it is a place."}, {"heading": "2 Related Work", "text": "In fact, it is a reactionary act, it is a reactionary diversion, it is a reactionary diversion, it is a reactionary diversion, it is a reactionary diversion, it is a reactionary diversion, it is a reactionary diversion."}, {"heading": "3 Associative Memory RNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Redundant Associative Memory", "text": "In the following, we will use the terminology of Danihelka et al. (2016) to introduce redundant associative memories and holographic reduced representations (HRR) (Plate, 1995). HRRs provide a mechanism for encoding an element x with a key r, which can be written into a fixed-size memory and which, via r.In HRR, keys r and values x refer to complex vectors consisting of a real and imaginary part: r = rre + i \u00b7 rim, x = xre + i \u00b7 xim, where i is the imaginary unit. We present these complex vectors as a concatenation of their respective real and imaginary parts, e.g. r = [rre; rim]. The encoding and retrieval operation used by Danihelka et al is the complex multiplication (equation (1) of a key with its value."}, {"heading": "3.2 Augmenting RNNs with Associative Memory", "text": "A recursive neural network (RNN) can be defined by a parameterized cell function f\u03b8 = 1.0: RN \u00b7 RM \u2192 RM \u00b7 RH, which is repeatedly applied to an input sequence. At any time, there is output ht and a state st, which is used as an additional input in the following time step (Equation (5) \u2212 \u2212 f\u03b8 (xt, st \u2212 1) = (st, ht) x-RN, s-RM, h-RH (5) In this work, we supplement RNNs, or more precisely, their cell function state f\u03b8 \u2212 with associative memory RNNNs (AM-RNN) f-RN as follows. Let us let st = [ct; nt] be the concatenation of a memory key and not the concatenation of a memory key and, if applicable, the concatenation of a memory key."}, {"heading": "4 Associative Memory RNNs for Dual Sequence Modeling", "text": "Important NLP tasks such as machine translation (MT) or textual sequence recognition (TE) therefore involve two distinct sequences as input, a source sequence and a target sequence. In MT, a system predicts the target sequence based on the source, whereas in TE source and target are given and an origin class should be predicted. Recently, both tasks have been successfully modeled using an attention mechanism that can accompany the positions in the source sequence to any step in the target sentence (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2016; Cheng et al., 2016). These models are able to learn important task-specific correlations between words or phrases of the two sentences, such as word / phrase sequence translations or origin problems or contradictions at the word / phrase level. The success of these models is mainly based on the fact that long ranges dependencies can be directly bridged over attention, rather than retained over a long state of memory."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "Data set We conducted experiments at the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) consisting of approximately 500k pairs of sentences (premise hypothesis), which are provided with textual labels, the task being to predict whether a premise implies, contradicts or is neutral. Training We perform mini-batch (B = 50) stochastic gradient descent with ADAM (Kingma and Ba, 2015) with \u03b21 = 0, \u03b22 = 0.999 and an initial learning rate of 10 \u2212 3 for small models (H \u2248 100) and 10 \u2212 4 (H = 500) for our large model. The learning rate was halved when accuracy was decreased over an epoch. Performance on the development set was checked every 1000 mini-batches and the best model is used for testing. We employ dropout with a probability of 0.1 or 0.2 for the small and large models."}, {"heading": "5.2 Results", "text": "The results are presented in Table 1. They show that the H = 100-dimensional Dual AM-GRU and the conditional AM-GRU significantly exceed our GRU base system. Specifically, the Dual AMGRU achieves an accuracy of 84.4% in this task, which shows that it is important to use the associative memory of the premise separately only for reading. Most noteworthy is that in this constellation it achieves even better results than a comparable LSTM architecture with bidirectional attention between all assumptions and hypotheses (LSTM Attention), which indicates that our Dual AM-GRU architecture is at least capable of achieving similar or even better results than an attention-based model in this constellation. We qualitatively examined this finding using selected examples by retrieving heatmaps of cosmic similarities between the contents written into memory at any time in the premise and what was derived from it."}, {"heading": "5.3 Sequence-to-Sequence Modeling", "text": "End-to-end differentiable sequence-to-sequence models consist of an encoder that encodes the source sequence and a decoder that generates the target sequence based on the encoded source. In a preliminary experiment, we applied the dual AM-GRU without common parameters to the task of automatic encoding, where source and target sequences are identical. Intuitively, we want the AMGRU to write phrase-level information with different keys to associative memory. However, we found that the AM-GRU encoder quickly learned to write everything into memory with the same key, making it work very similarly to a standard RNN-based encoder decoder architecture, where the encoder state is only used to initialize the decoder state. This finding is illustrated in Figure 3. The presented heatmap shows similarities between content retrieved while the target sequence was being written by the memory and what was predicted by the encoder."}, {"heading": "5.4 Discussion", "text": "We believe that the main difficulty in calculating a suitable key lies in each step of the target sequence to retrieve related content. Furthermore, the encoder should be forced not to use the same key all the time. Keys, for example, could be based on syntactic and semantic cues that could eventually lead to the capture of some form of frame semantics (Fillmore and Baker, 2001), which could greatly facilitate encoding. We believe this could be achieved by means of via-regularization or curriculum learning (Bengio et al., 2009)."}, {"heading": "6 Conclusion", "text": "The Dual AM-RNN expands the AM-RNN with a second read memory. Its ability to detect long-term dependencies enables effective learning of dual sequence modeling tasks, such as text sequence detection. Our models achieve highly competitive results and outperform a comparable attention-based model while maintaining constant computing and memory resources. Applying the Dual AM-RNN to a sequence-to-sequence modeling task has shown that the benefits of bridging long-term dependencies for this type of problem cannot yet be achieved. However, both quantitative and qualitative results on textual issues are promising, and we believe that the Dual AM-RNN can be an important building block for NLP tasks involving two sequences."}, {"heading": "Acknowledgments", "text": "We would like to thank Sebastian Krause, Tim Rockt\u00e4schel and Leonard Hennig for comments on an early draft of this work, which was supported by the Federal Ministry of Education and Research (BMBF) in the projects ALL SIDES (01IW14002), BBDC (01IS14013E) and Software Campus (01IS12050, subproject GeNIE)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Software available from", "citeRegEx": "Vanhoucke et al\\.,? 2015", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In The International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Bengio et al.2009] Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Long short-term memorynetworks for machine reading", "author": ["Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Frame semantics for text understanding", "author": ["Fillmore", "Baker2001] Charles J Fillmore", "Collin F Baker"], "venue": "In Proceedings of WordNet and Other Lexical Resources Workshop,", "citeRegEx": "Fillmore et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fillmore et al\\.", "year": 2001}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1819\u20131827.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In The International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "The NLP engine: A universal turing machine for nlp", "author": ["Li", "Hovy2015] Jiwei Li", "Eduard Hovy"], "venue": "arXiv preprint arXiv:1503.00168", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents. In 53nd Annual Meeting of the Association for Computational Linguistics (ACL)", "author": ["Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "The International Conference on Learning Representations (ICLR)", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2016] Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of the 2016 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), recognizing textual entailment (Cheng et al.", "startOffset": 173, "endOffset": 220}, {"referenceID": 15, "context": "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), recognizing textual entailment (Cheng et al.", "startOffset": 173, "endOffset": 220}, {"referenceID": 4, "context": ", 2014), recognizing textual entailment (Cheng et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016), auto-encoding (Li et al.", "startOffset": 40, "endOffset": 108}, {"referenceID": 13, "context": ", 2014), recognizing textual entailment (Cheng et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016), auto-encoding (Li et al.", "startOffset": 40, "endOffset": 108}, {"referenceID": 10, "context": ", 2016; Wang and Jiang, 2016), auto-encoding (Li et al., 2015), syntactial parsing (Vinyals et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 16, "context": ", 2015), syntactial parsing (Vinyals et al., 2015) or document-level question answering (Hermann et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 7, "context": ", 2015) or document-level question answering (Hermann et al., 2015).", "startOffset": 45, "endOffset": 67}, {"referenceID": 14, "context": "For example, Sutskever et al. (2014) connected two LSTMs conditionally for machine translation where the memory state after processing the source was used as initialization for the memory state of the target LSTM.", "startOffset": 13, "endOffset": 37}, {"referenceID": 1, "context": "Bahdanau et al. (2015) proposed an architecture that resolved this issue by allowing the model to attend over all positions in the source sentence when predicting the target sentence, which enabled the model to automatically learn alignments of words and phrases of the source with the target sentence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Neural Turing Machines inspired subsequent work on using different kinds of external memory, like queues or stacks (Grefenstette et al., 2015).", "startOffset": 115, "endOffset": 142}, {"referenceID": 1, "context": "Most famously it has been applied to machine translation (MT) where attention models automatically learn soft word alignments between source and translation (Bahdanau et al., 2015).", "startOffset": 157, "endOffset": 180}, {"referenceID": 13, "context": ", states for every word in the source sentence of MT or textual entailment (Rockt\u00e4schel et al., 2016), or entire sentence states as", "startOffset": 75, "endOffset": 101}, {"referenceID": 14, "context": "in Sukhbaatar et al. (2015) which is an end-to-end memory network (Weston et al.", "startOffset": 3, "endOffset": 28}, {"referenceID": 4, "context": "AM-RNNs also have an interesting connection to LSTM-Networks (Cheng et al., 2016) which recently demonstrated impressive results on various text modeling tasks.", "startOffset": 61, "endOffset": 81}, {"referenceID": 1, "context": "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2016; Cheng et al., 2016).", "startOffset": 171, "endOffset": 240}, {"referenceID": 13, "context": "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2016; Cheng et al., 2016).", "startOffset": 171, "endOffset": 240}, {"referenceID": 4, "context": "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2016; Cheng et al., 2016).", "startOffset": 171, "endOffset": 240}, {"referenceID": 13, "context": "Note that this is basically the the conditional encoding architecture of Rockt\u00e4schel et al. (2016).", "startOffset": 73, "endOffset": 99}, {"referenceID": 3, "context": "Dataset We conducted experiments on the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) that consists of roughly 500k sentence pairs (premise-hypothesis).", "startOffset": 90, "endOffset": 111}, {"referenceID": 12, "context": "(2016), word embeddings are initialized with Glove (Pennington et al., 2014) or randomly for unknown words.", "startOffset": 51, "endOffset": 76}, {"referenceID": 4, "context": "Following Cheng et al. (2016), word embeddings are initialized with Glove (Pennington et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "Model In this experiment we compare the traditional GRU with the (Dual) AM-GRU using conditional encoding (Rockt\u00e4schel et al., 2016) using shared parameters between source and target RNNs.", "startOffset": 106, "endOffset": 132}, {"referenceID": 13, "context": "LSTM (Rockt\u00e4schel et al., 2016) 116/252k 80.", "startOffset": 5, "endOffset": 31}, {"referenceID": 13, "context": "9 LSTM shared (Rockt\u00e4schel et al., 2016) 159/252k 81.", "startOffset": 14, "endOffset": 40}, {"referenceID": 13, "context": "4 LSTM-Attention (Rockt\u00e4schel et al., 2016) 100/252k 83.", "startOffset": 17, "endOffset": 43}, {"referenceID": 4, "context": "4 LSTM Network (Cheng et al., 2016) 450/3.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "regularization or by curriculum learning (Bengio et al., 2009).", "startOffset": 41, "endOffset": 62}], "year": 2017, "abstractText": "Many important NLP problems can be posed as dual-sequence or sequence-tosequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on autoencoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.", "creator": "LaTeX with hyperref package"}}}