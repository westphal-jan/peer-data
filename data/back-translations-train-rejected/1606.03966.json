{"id": "1606.03966", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Making Contextual Decisions with Low Technical Debt", "abstract": "Applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. We create a service that uses machine learning to accomplish this goal. The service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle.", "histories": [["v1", "Mon, 13 Jun 2016 14:17:00 GMT  (1519kb,D)", "http://arxiv.org/abs/1606.03966v1", null], ["v2", "Tue, 9 May 2017 14:41:15 GMT  (1679kb,D)", "http://arxiv.org/abs/1606.03966v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["alekh agarwal", "sarah bird", "markus cozowicz", "luong hoang", "john langford", "stephen lee", "jiaji li", "dan melamed", "gal oshri", "oswaldo ribas", "siddhartha sen", "alex slivkins"], "accepted": false, "id": "1606.03966"}, "pdf": {"name": "1606.03966.pdf", "metadata": {"source": "CRF", "title": "A Multiworld Testing Decision Service", "authors": ["Alekh Agarwal", "Sarah Bird", "Markus Cozowicz", "Luong Hoang", "John Langford", "Stephen Lee", "Jiaji Li", "Dan Melamed", "Gal Oshri", "Oswaldo Ribas", "Siddhartha Sen", "Alex Slivkins"], "emails": [], "sections": [{"heading": null, "text": "The service has a simple API and has been designed to be modular and reproducible to simplify deployment and debugging, and we show how these features enable robust and secure learning systems.Our evaluation shows that the decision-making service makes decisions in real time and quickly integrates new data into learned strategies.A large-scale deployment of a personalized messaging website has handled all traffic since January 2016, resulting in a relative 25% increase in clicks. By making the decision-making service available externally, we hope to provide everyone with optimal decision-making."}, {"heading": "1 Introduction", "text": "Dre eeisrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Machine learning methodology", "text": "From the perspective of machine learning, we implement Multiworld Testing (MWT): The ability to test and optimize through K strategies is generally observed, using a lot of data and calculations that are logarithmically scaled in K without necessarily knowing these strategies during data collection. Our methodology synthesizes ideas from contextual bandits (e.g., [3, 34]) and policy assessment (e.g., [36, 37]). The methodology emphasizes modularity: exploration is separate from political learning, and policy learning is reduced to a cost-sensitive classification, a well-studied problem in machine learning. Modularity of methodology maps for modularity in system design. The following presentation makes a few simplistic assumptions for clarity and visits them later in the section. Contextual decisions that interact with their environment, like a news website with its users, a data center with job requirements."}, {"heading": "3 An API for Making Decisions", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Decision Service: System Design", "text": "In this section we present the architecture of the MWT Decision Service. Our implementation (Section 5) is based on Microsoft Azure [40], but the architecture is cloud agnostic and could easily be migrated to another provider."}, {"heading": "4.1 Challenges and design requirements", "text": "eDi eeisrteeGsrsrteeeeeeteeteeteeteeteerrrrrrrrlrlrlrteeGsrrteeeeeeeeeeeGrrrrrrsrteeeVrlrrrsrteeoiiiiiiugnnlrsrteeeeoiuiuuuiuiuiuiuiuuiueeeegnrsrsgteeoiiiiiiiiiiiiiiiiiiiiiiuiiiiiiuiiiuiiuiuiuiuiuiuiuiuiuiuiuiuiirrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrsrrlgteeoioioioioioioieteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeteeeeeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteerrrrrrrr"}, {"heading": "4.2 Architecture and Semantics", "text": "The high-level architecture of the decision-making process that has captured this data is about as it has been in the four steps of dealing with the online world. (\"It's about the way we put it into the world.\") \"It's about the way we put it into the world.\" (\"It's about the way we put it into the world.\") \"It's about the way we put it into the world.\" (\"It's about the way we put it into the world.\") \"It's about the way we put it into the world.\" (\"It's about the way we put it into the world.\") \"(\" It's about the way we brought it into the world. \")\""}, {"heading": "4.3 Robust logging and randomization", "text": "Most machine learning systems assume that reward information is available at the same time as the interaction, which is rarely the case in practice. As mentioned in Section 4.1, many errors arise due to incorrect linking of this information. Our design systematically eliminates such errors by ensuring that the (x, a, p) tuples are set aside for logging at the decision point and later correctly associated with the reward. This ensures that the characteristics used in policy assessment are consistent with the interaction, that the correct probabilities are recorded, and that the action chosen by randomization is correctly logged, even if the downstream logic overrides it. Collecting or using probabilities is the other major cause of many past errors we have observed. PRClient library uses a randomized ID and interaction between the different layers, but the interaction is not complete."}, {"heading": "4.4 Low latency learning", "text": "The decision service enables efficient, low-latency learning through a coordinated, cross-component design. In the client library, the current prediction policy can be invoked in multiple ways without duplicating its state, resulting in parallel sub-millisecond decisions with low memory usage. Context processing is optimized in two ways. As the same context class (e.g. MyContext in Section 3) is repeatedly sent to the join service, we construct and use an abstract syntax tree to speed up serialization of the class. In addition, we use a simple caching scheme for repeated fragments of a context to significantly reduce data transfer. Such fragments are explicitly sent at regular intervals and replaced by references. In addition to the experimental duration, the join service adds a negligible delay, but must be scaled to allow for the set of events to be newly configured by the libraries that may have numerous events."}, {"heading": "4.5 Full reproducibility", "text": "The decision-making service is unique in its ability to fully reproduce online results offline, despite randomized exploration and continuously updated policies, with each component playing a role. Online learner policies are identified by unique IDs and published in the data store in Fig. 2. The customer library records the ID of all policies used for each decision. As the event and application IDs initiate PRG for exploration, all results are reproducible. Together, this ensures that each trained policy is reproducible, so that it records the order in which interactions with the stored policies are processed. As we regularly reset the learning rate to favor current events, these resets are also recorded. Together, this ensures that each trained policy is reproducible. We have not yet implemented full reproducibility for systems with multiple online learners, as we have not yet required more than an additional set of values, however, as some of them may be easily matched."}, {"heading": "4.6 Safeguards", "text": "The Decision Service enables a fully automated online learning loop: Automated and machine learning are the service's main selling points, but they are also liabilities, as it can be difficult to statically verify the behavior. As a result, application engineers may not trust the behavior of such a system. Decision Service has two key characteristics that make it possible to protect an deployed system and mitigate these impairments, the first being compatibility with business logic and environmental limitations. Decision Service allows any business logic to exist downstream that can change, restrict or reverse its decisions. Such logic can be used to implement failsafe measures against business policy violations or human safety. As long as this logic remains stationary, it will not affect MWT's ability to provide unbiased counterfactual estimates of policy performance; the second feature is the ability to evaluate any policy in real time, as provided by the online learner."}, {"heading": "5 Implementation", "text": "The code is opensource [20]. The C # library is about 5K lines of code: 1.5K for the various exploration policies and another 1.5K that process bundled uploads to the join service. [10] The client library can be linked to the ML module to call policies, but the delay is limited to the experimental duration. We have implemented the join service with Azure Stream Analytics (ASA), which allows us to specify a delayed entry with 4 lines of Query Language."}, {"heading": "6 Deployment in MSN", "text": "This year it is more than ever before."}, {"heading": "7 Evaluation", "text": "In this section, we will use the data collected in this deployment to evaluate various aspects of the design of the Decision Service. We will answer the following questions: 1. How quickly are decisions made and policies learned? 2. Can high data rates be managed? 3. Are estimates of policy assessment reliable? Can policies be compared in real time? 4. How important is it to continuously train policies? 5. What impact does poor logging have on policies?"}, {"heading": "7.1 Experimental setup and methodology", "text": "The sample code distributed with the client library has been used on several large DIN-A-4 instances (8 cores, 14GB of memory, 1Gbps NIC); the join service uses ASA, which provides scalability through a configurable number of streaming units; the event hub feeds the join service and saves its output as needed; the online learner is a stand-alone workforce on a single D5 instance (16 cores, 56GB of memory); all of our experiments use exploration data from the MSN deployment for a three-day period in April; this data contains the real (x, a, p, r) tuples generated on those days, with x consisting of about 1000 features per action. By linking this data to browser click logs, we can determine both the time of the decision and the time of the click to ensure realistic reproduction of the data."}, {"heading": "7.2 Latency of learning and decisions", "text": "We are interested in two parameters: decision latency and learning latency. Decision latency is the time when a decision is made in the customer library (i.e. the call of ChooseAction).Learning latency is the time from the time when an interaction is completed (i.e., ReportReward was called) until it affects a policy used in the customer library. Measured average latency is 0.2 ms. To measure the learning latency, we initially removed configurable sources of delay: We disabled batching and caching (due to delays in reordering) in the customer library and configured it to query for new models every 100 seconds; we set the experimental duration of the join service to 1 second; and configured the online learner to publish an updated policy after each interaction; then we played an MN policy interaction to an average of 3 seconds."}, {"heading": "7.3 High data rates", "text": "Both our Join Service implementation (based on ASA) and the one used by MSN (based on Redis) are inherently scalable services, so we focus on the online learner. The data rates seen so far in production have been handled adequately by learning on a single core. To saturate the online learner, we loaded MSN data into the Join service output queue and processed it at full speed. We used the customer library compression scheme to mimic what MSN does. Throughput achieved by the learner was stable at 2000 interactions / sec, meaning that 100 million interaction / day applications are practicable. The buffer for newly ordered events whose compressed functions were not yet available was minimal and remained at 0 for most of the time with occasional peaks up to 2250 buffered interactions. In the case of MSN, the number of political actions directly influences the number of school actions."}, {"heading": "7.4 Reliable real-time policy comparison", "text": "We have chosen 3 simple strategies, each of which selects the first, second and third article from the editorial ranking for first place; the first policy follows the editorial baseline, while others are sensible alternatives; for each policy, we have calculated an ips estimate of its value from the data of one day for two different days in April; in addition, we have estimated the performance of each policy in the control flight with very high precision; across all strategies and days, the relative difference between the IPS and control values was no more than 2.5%; and all ips estimates were within a 95% confidence interval around the control values; next, we use similar ips estimates to measure the speed of learning; and the strategies currently in production in MSN have been trained for months (with daily adjustments to the learning rate)."}, {"heading": "7.5 Continuous policy training", "text": "To demonstrate the importance of continuous training, we used the guidelines of the previous experiment (trained on the first day) and tested them on the second and third days. We compared these guidelines with the guidelines of the corresponding day and recorded relative performance: guidelines of: Day 1 Day 2 Day 3 Day 1 1.0 1.0 1.0 1.0 0.73 0.46 In other words, the guidelines of Day 1 reach 73% of the CTR of the guidelines of Day 2 when tested on the second day, and 46% of the guidelines of Day 3 on the third day. This indicates that the environment and articles have changed and the guidelines of Day 1 are outdated. Continuous training solves this problem."}, {"heading": "7.6 Data collection failures", "text": "Our motivation for setting up the Decision Service came from the fact that we had experienced past failures in the application of MWT. Many of these failures were due to incorrect exploration data. We simulated two error modes using MSN's offline protocols. For each experiment, we took the data one day and randomly assigned it 80% for training and 20% for the test, while simulating the performance of this policy in its use online. This simulation is imperfect because the training takes place on fewer events and because the policies evaluated on the test set are not adaptive. If the training in train allows progressive validation techniques [13], an unbiased estimate of the political value deployed without delay."}, {"heading": "8 Related work", "text": "Here we discuss other approaches to machine learning in the context of MWT and experiments and machine learning systems in the context of the Decision Service."}, {"heading": "8.1 Machine learning with exploration", "text": "The simplest of these is active learning [12, 29, 30, 50], where the algorithm helps in selecting examples that can be labeled in partnership with a user to improve the performance of the model. A more general setting is reinforcement learning [53, 54], where an algorithm repeatedly selects between actions and receives rewards and other feedback depending on the actions chosen. A more minimal setting is multi-armed bandits (MABs), where only a single action influences the observed reward [15, 25]. Our methodology relies on context-based bandits with policy specifications [2, 3, 8, 22, 34] which reduce the policy optimization of exploration data to a monitored learning problem, allowing the inclusion of tools developed for supervised learning (see [11] for the background of learning reductions). Our methodology is based on context-based bandits with policy specifications [36, 34, 23, 23, 22, 23] which reduces policy specifications to policy specifications."}, {"heading": "8.2 Systems for ML and experimentation", "text": "A / B tests refer to randomized experiments with subjects who are not able to be used in medicine and social science, and which are standard in many areas (Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics, Google Analytics...)."}, {"heading": "9 The Future", "text": "We have introduced the Decision Service: a powerful tool to support the entire data lifecycle that automates many of the onerous tasks that data scientists face, such as collecting the right data and using it appropriately. Instead, a data scientist can focus on core tasks, such as finding the right features, representation, or signals for re-optimization. Data Lifecycle Support makes the basic application of the Decision Service feasible even without data scientists. To lower the entry barrier, we examine techniques based on expert learning [16] and hyperparameter search that could further automate the process. Since policy assessment techniques can provide accurate predictions of online performance, such automation is guaranteed to be statistically flawless. We also focus on making the decision service easy to use and use because we believe that this is the key to the goal of democratizing machine learning for all. Of course, the Decision Service can also expand to include a wider variety of problems that can be addressed by extending life-cycle data support, such as extending the ability of all."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["A. Agarwal", "O. Chapelle", "M. Dud\u00edk", "J. Langford"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Contextual bandit learning with predictable rewards", "author": ["A. Agarwal", "M. Dud\u00edk", "S. Kale", "J. Langford", "R.E. Schapire"], "venue": "In 15th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R. Schapire"], "venue": "In 31st Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Personalizing linkedin feed", "author": ["D. Agarwal", "B.-C. Chen", "Q. He", "Z. Hua", "G. Lebanon", "Y. Ma", "P. Shivaswamy", "H.-P. Tseng", "J. Yang", "L. Zhang"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "In 30th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Using confidence bounds for exploitationexploration trade-offs", "author": ["P. Auer"], "venue": "J. of Machine Learning Research (JMLR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Learning reductions that really work", "author": ["A. Beygelzimer", "H.D. III", "J. Langford", "P. Mineiro"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "J. Langford", "Z. Tong", "D.J. Hsu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Beating the hold-out: Bounds for k-fold and progressive cross-validation", "author": ["A. Blum", "A. Kalai", "J. Langford"], "venue": "In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L. Bottou", "J. Peters", "J. Quinonero-Candela", "D.X. Charles", "D.M. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson"], "venue": "J. of Machine Learning Research (JMLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Contextual Bandits with Linear Payoff Functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "In 14th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "The missing piece in complex analytics: Low latency, scalable model management and serving with velox", "author": ["D. Crankshaw", "P. Bailis", "J.E. Gonzalez", "H. Li", "Z. Zhang", "M.J. Franklin", "A. Ghodsi", "M.I. Jordan"], "venue": "CIDR", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Sampleefficient nonstationary policy evaluation for contextual bandits", "author": ["M. Dud\u00edk", "D. Erhan", "J. Langford", "L. Li"], "venue": "In 28th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Efficient optimal leanring for contextual bandits", "author": ["M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang"], "venue": "In 27th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dud\u00edk", "J. Langford", "L. Li"], "venue": "In 28th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Field Experiments: Design, Analysis, and Interpretation", "author": ["A.S. Gerber", "D.P. Green"], "venue": "W.W. Norton&Co, Inc.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Multi- Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Theory of disagreement-based active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Next: A system for real-world development, evaluation, and application of active learning", "author": ["K.G. Jamieson", "L. Jain", "C. Fernandez", "N.J. Glattard", "R. Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Online controlled experiments and a/b tests", "author": ["R. Kohavi", "R. Longbotham"], "venue": "Encyclopedia of Machine Learning and Data Mining. Springer,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Controlled experiments on the web: survey and practical guide", "author": ["R. Kohavi", "R. Longbotham", "D. Sommerfield", "R.M. Henne"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Efficient contextual semi-bandit learning", "author": ["A. Krishnamurthy", "A. Agarwal", "M. Dud\u00edk"], "venue": "arxiv.org, abs/1502.05890,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In 21st Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "In 19th Intl. World Wide Web Conf. (WWW),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["L. Li", "W. Chu", "J. Langford", "X. Wang"], "venue": "In 4th ACM Intl. Conf. on Web Search and Data Mining (WSDM),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.-Y. Su"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Showing Relevant Ads via Lipschitz Context Multi-Armed Bandits", "author": ["T. Lu", "D. P\u00e1l", "M. P\u00e1l"], "venue": "In 14th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "The user and business impact of server delays, additional bytes, and http chunking in web search", "author": ["E. Schurman", "J. Brutlag"], "venue": "In Velocity,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2009}, {"title": "Machine learning: The high-interest credit card of technical debt", "author": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young"], "venue": "In SE4ML: Software Engineering 4 Machine Learning,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Ice: enabling non-experts to build models interactively for large-scale lopsided problems", "author": ["P. Simard", "D. Chickering", "A. Lakshmiratan", "D. Charles", "L. Bottou", "C.G.J. Suarez", "D. Grangier", "S. Amershi", "J. Verwey", "J. Suh"], "venue": "arXiv preprint arXiv:1409.4814,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "J. of Machine Learning Research (JMLR),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1933}, {"title": "Minerva: A scalable and highly efficient training platform for deep learning", "author": ["M. Wang", "T. Xiao", "J. Li", "J. Zhang", "C. Hong", "Z. Zhang"], "venue": "In NIPS Workshop, Distributed Machine Learning and Matrix Computations,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Spark: Cluster computing with working sets", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Machine Learning has well-known high-value applications, yet effective deployment is fraught with difficulties in practice [14, 49].", "startOffset": 123, "endOffset": 131}, {"referenceID": 31, "context": "Machine Learning has well-known high-value applications, yet effective deployment is fraught with difficulties in practice [14, 49].", "startOffset": 123, "endOffset": 131}, {"referenceID": 10, "context": "Learning can be biased when the decisions made by an algorithm affect the data collected to train that algorithm [14, 49].", "startOffset": 113, "endOffset": 121}, {"referenceID": 31, "context": "Learning can be biased when the decisions made by an algorithm affect the data collected to train that algorithm [14, 49].", "startOffset": 113, "endOffset": 121}, {"referenceID": 22, "context": "A common methodology for exploration is A/B testing [31, 32]: policy A is tested against policy B by running both live on a small percentage of user traffic.", "startOffset": 52, "endOffset": 60}, {"referenceID": 23, "context": "A common methodology for exploration is A/B testing [31, 32]: policy A is tested against policy B by running both live on a small percentage of user traffic.", "startOffset": 52, "endOffset": 60}, {"referenceID": 2, "context": "Contextual bandits [3, 34] is a line of work in machine learning allowing testing and optimization over exponentially more policies for a given number of events2.", "startOffset": 19, "endOffset": 26}, {"referenceID": 25, "context": "Contextual bandits [3, 34] is a line of work in machine learning allowing testing and optimization over exponentially more policies for a given number of events2.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "3 of [14] contains a classic example of this phenomenon.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": ", [3, 34]) and policy evaluation (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 25, "context": ", [3, 34]) and policy evaluation (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [36, 37]) literatures.", "startOffset": 2, "endOffset": 10}, {"referenceID": 27, "context": ", [36, 37]) literatures.", "startOffset": 2, "endOffset": 10}, {"referenceID": 2, "context": "The randomization need not be uniform and for best performance should not be [3, 8].", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "The randomization need not be uniform and for best performance should not be [3, 8].", "startOffset": 77, "endOffset": 83}, {"referenceID": 17, "context": "A better approach is to use a reduction to cost-sensitive classification [23], for which many practical algorithms have been designed and implemented.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "[34]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "A more principled approach explores and learns for all slots at once [33].", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "For interactive high-value applications serving latency tends to be directly linked to user experience and revenue [48].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "The MWT methodology guarantees offline experimentation is counterfactually accurate [21, 23, 36, 37].", "startOffset": 84, "endOffset": 100}, {"referenceID": 17, "context": "The MWT methodology guarantees offline experimentation is counterfactually accurate [21, 23, 36, 37].", "startOffset": 84, "endOffset": 100}, {"referenceID": 26, "context": "The MWT methodology guarantees offline experimentation is counterfactually accurate [21, 23, 36, 37].", "startOffset": 84, "endOffset": 100}, {"referenceID": 27, "context": "The MWT methodology guarantees offline experimentation is counterfactually accurate [21, 23, 36, 37].", "startOffset": 84, "endOffset": 100}, {"referenceID": 0, "context": "VW also supports parallel online learning using the AllReduce communication primitive [1], which provides scaling across cores and machines.", "startOffset": 86, "endOffset": 89}, {"referenceID": 24, "context": "multaneously [33].", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "When training on the train set progressive validation techniques [13] allow an unbiased estimate of policy value deployed with zero delay.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "The simplest of these is active learning [12, 29, 30, 50] where the algorithm helps select examples to label in partnership with a user in order to improve model performance.", "startOffset": 41, "endOffset": 57}, {"referenceID": 20, "context": "The simplest of these is active learning [12, 29, 30, 50] where the algorithm helps select examples to label in partnership with a user in order to improve model performance.", "startOffset": 41, "endOffset": 57}, {"referenceID": 21, "context": "The simplest of these is active learning [12, 29, 30, 50] where the algorithm helps select examples to label in partnership with a user in order to improve model performance.", "startOffset": 41, "endOffset": 57}, {"referenceID": 32, "context": "The simplest of these is active learning [12, 29, 30, 50] where the algorithm helps select examples to label in partnership with a user in order to improve model performance.", "startOffset": 41, "endOffset": 57}, {"referenceID": 34, "context": "A maximally general setting is reinforcement learning [53, 54] where an algorithm repeatedly chooses among actions and receives rewards and other feedback depending on the chosen actions.", "startOffset": 54, "endOffset": 62}, {"referenceID": 35, "context": "A maximally general setting is reinforcement learning [53, 54] where an algorithm repeatedly chooses among actions and receives rewards and other feedback depending on the chosen actions.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "A more minimal setting is multi-armed bandits (MAB) where only a single action affects observed reward [15, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 19, "context": "A more minimal setting is multi-armed bandits (MAB) where only a single action affects observed reward [15, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 1, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 62, "endOffset": 79}, {"referenceID": 2, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 62, "endOffset": 79}, {"referenceID": 6, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 62, "endOffset": 79}, {"referenceID": 16, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 62, "endOffset": 79}, {"referenceID": 25, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 62, "endOffset": 79}, {"referenceID": 7, "context": "Our methodology builds on contextual bandits with policy sets [2, 3, 8, 22, 34] which reduces policy optimization on exploration data to a supervised learning problem, allowing incorporation of tools developed for supervised learning (see [11] for background on learning reductions).", "startOffset": 239, "endOffset": 243}, {"referenceID": 15, "context": "Our methodology also incorporates offline policy evaluation in contextual bandits [21, 23, 36, 37].", "startOffset": 82, "endOffset": 98}, {"referenceID": 17, "context": "Our methodology also incorporates offline policy evaluation in contextual bandits [21, 23, 36, 37].", "startOffset": 82, "endOffset": 98}, {"referenceID": 26, "context": "Our methodology also incorporates offline policy evaluation in contextual bandits [21, 23, 36, 37].", "startOffset": 82, "endOffset": 98}, {"referenceID": 27, "context": "Our methodology also incorporates offline policy evaluation in contextual bandits [21, 23, 36, 37].", "startOffset": 82, "endOffset": 98}, {"referenceID": 5, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 114, "endOffset": 125}, {"referenceID": 13, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 114, "endOffset": 125}, {"referenceID": 26, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 114, "endOffset": 125}, {"referenceID": 29, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 145, "endOffset": 153}, {"referenceID": 33, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 145, "endOffset": 153}, {"referenceID": 4, "context": "Alternative approaches for contextual bandits typically impose substantial modeling assumptions such as linearity [7, 17, 36], Lipschitz rewards [39, 51], or availability of (correct) Bayesian priors [5], which often limits applicability.", "startOffset": 200, "endOffset": 203}, {"referenceID": 23, "context": "It is routinely used in medicine and social science, and has become standard in many Internet services [32, 31], as well supported by statistical theory [24].", "startOffset": 103, "endOffset": 111}, {"referenceID": 22, "context": "It is routinely used in medicine and social science, and has become standard in many Internet services [32, 31], as well supported by statistical theory [24].", "startOffset": 103, "endOffset": 111}, {"referenceID": 18, "context": "It is routinely used in medicine and social science, and has become standard in many Internet services [32, 31], as well supported by statistical theory [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "Google Analytics [26] supports Thompson Sampling [56], a well-known algorithm for the basic Multi-Arm Bandit problem.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": ", news recommendation [4, 36, 37] and Advertising [14]); however, they all have been completely custom rather than a generalpurpose system like the Decision Service.", "startOffset": 22, "endOffset": 33}, {"referenceID": 26, "context": ", news recommendation [4, 36, 37] and Advertising [14]); however, they all have been completely custom rather than a generalpurpose system like the Decision Service.", "startOffset": 22, "endOffset": 33}, {"referenceID": 27, "context": ", news recommendation [4, 36, 37] and Advertising [14]); however, they all have been completely custom rather than a generalpurpose system like the Decision Service.", "startOffset": 22, "endOffset": 33}, {"referenceID": 10, "context": ", news recommendation [4, 36, 37] and Advertising [14]); however, they all have been completely custom rather than a generalpurpose system like the Decision Service.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "There are many systems designed for supervised machine learning such as CNTK [18], GraphLab [28], Parameter Server [38], MLlib [52], TensorFlow [55], Torch [57], Minerva [59] and Vowpal Wabbit [58] to name a few.", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "There are many systems designed for supervised machine learning such as CNTK [18], GraphLab [28], Parameter Server [38], MLlib [52], TensorFlow [55], Torch [57], Minerva [59] and Vowpal Wabbit [58] to name a few.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Velox [19] is an open-source system that supports model serving and batch training.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "It collects data that can be used to retrain models via Spark [61].", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "We know of two other systems designed to fully support data collection with exploration, model development, and deployment: LUIS [35] (based on ICE [50]), and Next [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "We know of two other systems designed to fully support data collection with exploration, model development, and deployment: LUIS [35] (based on ICE [50]), and Next [30].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "To assist in lowering the barrier to entry, we are exploring techniques based on expert learning [16] and hyperparameter search that may further automate the process.", "startOffset": 97, "endOffset": 101}], "year": 2016, "abstractText": "Applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. We create a service that uses machine learning to accomplish this goal. The service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle. The system is general: it works for any discrete choices, with respect to any reward metric, and can work with many learning algorithms and feature representations. The service has a simple API, and was designed to be modular and reproducible to ease deployment and debugging, respectively. We demonstrate how these properties enable learning systems that are robust and safe. Our evaluation shows that the Decision Service makes decisions in real time and incorporates new data quickly into learned policies. A large-scale deployment for a personalized news website has been handling all traffic since Jan. 2016, resulting in a 25% relative lift in clicks. By making the Decision Service externally available, we hope to make optimal decision making available to all.", "creator": "LaTeX with hyperref package"}}}