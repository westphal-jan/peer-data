{"id": "1606.03968", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Visual-Inertial-Semantic Scene Representation for 3-D Object Detection", "abstract": "We describe a representation of a scene that captures geometric and semantic attributes of objects within, along with their uncertainty. Objects are assumed persistent in the scene, and their likelihood computed from intermittent visual data using a convolutional architecture, integrated within a Bayesian filtering framework with inertials and a context model. Our method yields a posterior estimate of geometry (attributed point cloud and associated uncertainty), semantics (identities and co-occurrence), and a point-estimate of topology for a variable number of objects within the scene, implemented causally and in real-time on commodity hardware.", "histories": [["v1", "Mon, 13 Jun 2016 14:22:10 GMT  (32kb,D)", "http://arxiv.org/abs/1606.03968v1", "preliminary version, no figures"], ["v2", "Mon, 17 Apr 2017 20:25:26 GMT  (4651kb,D)", "http://arxiv.org/abs/1606.03968v2", "To appear in CVPR 2017"]], "COMMENTS": "preliminary version, no figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jingming dong", "xiaohan fei", "stefano soatto"], "accepted": false, "id": "1606.03968"}, "pdf": {"name": "1606.03968.pdf", "metadata": {"source": "CRF", "title": "Visual-Inertial Semantic Scene Representation UCLA TR CSD160005", "authors": ["Stefano Soatto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to determine for themselves what they want and what they do not want. (...) It is not so that they are able to determine for themselves. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is so that they do it. (...) It is so that they do it. (...) It is as if they do not want it. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (). (...). (). (...). (). (). (). (...). (). (). (...). (). ().). (...). (). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (). \"(...).\" ().). (...). \"(...). (...). ().). (...). (...). (). (). (). ().). (). (). ().). (...).).). (...). (). (...). (). (). (). (). ().). (). (). (). ().).). (). (). (). (). ().). (). (). (). ().).). (). (). (). ().). (). (). ().). (). (). ().). ().). (). ().).). (). ().). ().). (). (). ().). ().). (). (). (). ()."}, {"heading": "2 Modeling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Representations", "text": "A scene is populated by a number of objects, each updating a new object., \"\" zN, \"each with geometric (position, shape) and semantic (label) attributes zj, sj, lj).2 Measurements (e.g. images) up to the current time t, yt., yt) are captured by a sensor in the pose in which a representation of the scene in support of semantic tasks is the common posterior p., zj | yt) for up to the j-th objects seen up to the time., in which sensor and other nuances are marginalized. The common posteriors can be decomposed3 asp (zj, zj | yt) = p (1) p (6) p (zj | yt) p (2) p (2) p (2) p (2) with the first factor."}, {"heading": "2.2 Approximations", "text": "The measurement in (2) can be adapted to the second order by a simultaneous localization and Figure (SLAM). (2) The measurement in (2) can be adapted to the second order by a simultaneous localization and Figure (SLAM). (3) The measurement in (3) of the second order by a simultaneous localization and Figure (SLAM) is a system: p (4), the measurement in (4) of the third level is one (4), the one (3), (4), (4), (5), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, (5), (5, 5, 5, 5, 5, 5, (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5), (5, (5, 5, 5, 5, 5, 5, 5, 5), (5, (5), (5, (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), (5, 5, 5, (5), (5, 5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5), (5, (5, (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5"}, {"heading": "2.3 Information", "text": "For measurements up to the present time, yt, the information that a (future) measurement yt + 1 contains about the scene / objects, is formally I (z, yt + 1 | yt) = H (yt + 1 | yt) \u2212 H (yt + 1 | z, yt). (7) The first term is the uncertainty of future data in the face of the past. It indirectly depends on the true scene, through the future data it generates. The second term explicitly depends on the true collection of objects in the scene z, which is generally unidentifiable (Rem. 3). By definition, nuances are not meaningful for the scene (although see Rem. 4 in the case of controllable nuances), so that the information content of yt + 1 is identical with the information content of the scene z (yt + 1) if it is a maximum inventory for the scene (AI) [52]."}, {"heading": "2.4 Exploration", "text": "For the purpose of this section, we assume a data information model h, which, given the true (objects in) scene z + 1 (uncertainty), could generate the data y up to an uninformative / white residue / noise n: y = h (z, g) + n. In section 3.1, we could loosen this assumption to calculate the functions of the data sufficient to evaluate the probability. Before measuring yt + 1 (z, g) and p (gt, x), we can determine the action ut that would place the sensor at gt + 1 (ut) so that as hallucinating measurements y + 1 = h (z, gt + 1), most of the decreasing uncertainties (i.e., maximize information) to z (7) we haveut = argmax uH (yt) \u2212 H (u), yt (z), yt (z), yt (z), yt (9) where we maximize the information."}, {"heading": "A Turing postulate", "text": "We call Turing the assumption that for a compact scene there is a (possibly large but finite) set of data whose statistics are as good as the scene to generate future measurements yt + 1 (or maximum invariants thereof): p (yt + 1 | z) = p (yt + 1 | h (z (yt), g). The Turing test is passed if the innovation remnants yt + 1 \u2212 h (z | g), p (t), x | t, yt) are from the rear plane, which is a minimally sufficient (Bayesian) statistic of yt for z [53]. The Turing test is passed if the innovation remnants yt + 1 \u2212 h (yt), g) are uninformative, according to some empirical whiteness tests that are ineffective."}, {"heading": "2.5 Exploitation", "text": "In some cases, it is possible to derive a score from the representation of the scene represented in (3). (3) Focusing on the second term of (9), we have the following image: H (yt + 1 | z, yt) = H (yt + 1 | z) \u2264 H (yt + 1 | z) (12) for each estimation function z (\u00b7). We can therefore select the object that minimizes the right scene; this requires the postulation of a probability function p (yt + 1 | z), and the calculation of empirical entropy [ig] for the empirical entropy [ig].z = argmin z (\u00b7) Ep (yt + 1, yt). (yt + 1 | y t))). (13) or, in general, the minimization in relation to the predictor p (yt + 1 | yt)."}, {"heading": "3 Instantiation", "text": "It is raw data collected at a given time and described by a reference frame. It is the \"history\" of the data up to t. For example, an image It: D = [640 \u00b7 480] \u2192 [0, 255]., T with domain D on the pixel grid. Let x be a description of the (sparse) geometry of the scene, for example a point cloud x = [x1,., xNx], x x. Allow a statistical (deterministic) function of the raw data to exist. If the image is limited to a compact subset of images, it is sometimes referred to as a \"local (feature) descriptor.\" A revolutionary neural network is often used as a local descriptor (I | b), with the entire image b = D as a special case. The pose of the sensor is described by a reference frame."}, {"heading": "3.1 Abstracting the pixels", "text": "The above model allows us to evaluate the probability of an object of class l seen in a region from the vantage point g in image I by comparing the latter with an image hallucinating from the object that I (s, l, g) over the (log probelihood) '2 distance between pixels d2 (I, I) = I \u2212 I \u01092. But to evaluate the probability, we do not need to simulate the image image image image image forming process down to the pixels. At the very least, we do not want the comparison to be affected by the error variability. This could be done by defining a (geodesic) distance between functions (I) and (I), for example, which is designed to be maximally invariant descriptors. But, in general, we can design a descriptor for I that directly adjusts the probability function (11) by selecting from a parametric class of functions (I), this region-ib-score (that we empib-score)."}, {"heading": "3.2 Controlling nuisance variability", "text": "The effects of some annoying variables can be eliminated by replacing the data with a (maximum) invariant. For example, contrast transformations can be taken into account without loss by replacing the intensity I on xij with a maximum contrast invariant. (I) However, the linearized approach to nt as anecular Gaussian can be controlled. (I) The nuances can be controlled. (I) A robot can control the point of view to some extent. (I) The speed of the approach to nt as anecular Gaussian. (7) The nuances can be controlled."}, {"heading": "3.3 Measurement process", "text": "The SLAM system processes all past image measurements. It is a collection of sparse contrast invariant characteristic descriptions for Ni visible regions of the scene, which we collectively refer to as an instance, and produces a common posterior distribution of poses gt and a sparse geometric representation of the scene x = [x1]., xNi (t), conjectured uni-modal and approximate characteristics for Ni visible regions of the scene, and produces a common posterior distribution of poses gt and a sparse geometric representation of the scene x = [x1]., xNi (t), presupposed uni-modal and approximated classes described by a Gaussian: p SLAM (gt, x yt) \"N (gt, x, x, t, x)."}, {"heading": "3.4 Dependencies and co-visibility", "text": "To calculate the probability of an object present in the scene, it is necessary to determine whether it is visible in the image, which in turn depends on all other objects, so that the scene must be modeled holistically, rather than as an independent collection of objects. In addition, the presence of certain objects and their configuration affects the probability that other objects that are not visible can be presented. In addition, relationships between objects can be the object of interest itself, as in events or actions. To capture these dependencies, we note that the geometric representation p (gt, x | yt) can be used to ensure a common distribution across the position of all objects and cameras p (gt, x | yt) that provide co-visualization information, in particular the probability that each point in x is visible. However, it is of no use to determine the visibility of objects as it does not contain topological information."}, {"heading": "3.5 Scene context RNN", "text": "The last element we need to calculate (2) is a representation of the (abstract) scene. (2) This is necessary to produce sample objects, including those that have not yet been seen. (2) [3] [4] [5] + 1] p (zj + 1] p [5] p (zj + 1] p [6] p [6] p [7] p [7] p [7] p [7] p [8] p [8] p [8] p [8] p [8] p [8] p [8] p) p [8] p [8] p) p [8] p [8] p \"p\" p [8] p \"p [8] p\" p [8] p \"p\" p \"p\" p \"p [8] p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p) p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p) p) p) p) p) p) p) p) p) \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p) p) p) p) p) p) p) p) p) p) p) p) p) p)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Implementation", "text": "SLAM is borrowed from [56, 16]. CNN is implemented by changing the stock code [45] by removing the classification layer and introducing normalization. EKF and PMF are standard textbook materials and the RNN is trained by adopting manually labeled scene limitation boxes."}, {"heading": "4.2 Empirical evaluation", "text": "Fig. 5 shows a representative visualization of the MAP estimate of the object position, encoded by a gravitational ellipsoid (here visualized as a Bounding Box), and of the section of the point cloud contained therein. Furthermore, the topology is estimated (dense surface, Fig. 2) and the identity is inferred. Although objects in the scene are assumed to be static (or slowly moving), camera motion induces a non-trivial dynamic in the images of objects that would have to be tracked if object models were depicted in the image. However, if we have represented them in the scene, we can predict the (image) motion of objects, as shown in Fig. 7, even if the object is completely closed. We note that some datasets contain moving objects (people) that move slowly Fig. 7: Left to right: A chair is recognized by CNN (top) and initialized in the filter."}, {"heading": "5 Discussion", "text": "In fact, most people who are able to decide for themselves what they want and what they want have to take matters into their own hands. It's not as if they can take things into their own hands. It's as if they are able to take things into their own hands. It's as if they have to take things into their own hands. It's as if they don't have to take things into their own hands. It's as if they don't do it. It's as if they do it, but it's as if they do it. It's not as if they do it, as if they do it."}, {"heading": "Acknowledgments", "text": "Contributors to this work are Pratik Chaudhari, Alessandro Achille and Alessandro Chiuso for discussions leading to the development of the statistical framework, and Xiaohan Fei, Jingming Dong, Nikolaos Karianakis Konstantine Tsotsos for its implementation and testing. Research supported by ONR, ARO and AFOSR."}], "references": [{"title": "Visual common-sense for scene understanding using perception, semantic parsing and reasoning", "author": ["S. Aditya", "Y. Yang", "C. Baral", "C. Fermuller", "Y. Aloimonos"], "venue": "2015 AAAI Spring Symposium Series,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "On invariance and selectivity in representation learning", "author": ["F. Anselmi", "L. Rosasco", "T. Poggio"], "venue": "arXiv preprint arXiv:1503.05938,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Detachable object detection: Segmentation and depth ordering from shortbaseline video", "author": ["A. Ayvaci", "S. Soatto"], "venue": "IEEE Trans. on Patt. Anal. and Mach. Intell., 34(10):1942\u20131951,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Bold-binary online learned descriptor for efficient image matching", "author": ["V. Balntas", "L. Tang", "K. Mikolajczyk"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2367\u20132375,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Second-order constrained parametric proposals and sequential searchbased structured prediction for semantic segmentation in rgb-d images", "author": ["D. Banica", "C. Sminchisescu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3517\u20133526,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Scene segmentation using temporal clustering for accessing and re-using broadcast video", "author": ["L. Baraldi", "C. Grana", "R. Cucchiara"], "venue": "Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1\u20136. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation and recognition using structure from motion point clouds", "author": ["G. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": "Computer Vision\u2013ECCV 2008, pages 44\u201357. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Classification with scattering operators", "author": ["J. Bruna", "S. Mallat"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic parsing for priming object detection in rgb-d scenes", "author": ["C. Cadena", "J. Ko\u0161ecka"], "venue": "3rd Workshop on Semantic Perception, Mapping and Exploration,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting hierarchical context on a large database of object categories", "author": ["M. Choi", "J. Lim", "A. Torralba", "A. Willsky"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 129\u2013136. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "arXiv preprint arXiv:1604.01685,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "3d urban scene modeling integrating recognition and reconstruction", "author": ["N. Cornelis", "B. Leibe", "K. Cornelis", "L. Van Gool"], "venue": "International Journal of Computer Vision, 78(2-3):121\u2013141,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional nets and watershed cuts for real-time semantic labeling of rgbd videos", "author": ["C. Couprie", "C. Farabet", "L. Najman", "Y. Lecun"], "venue": "The Journal of Machine Learning Research, 15(1):3489\u20133511,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic segmentation of rgbd images with mutex constraints", "author": ["Z. Deng", "S. Todorovic", "L. Latecki"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1733\u20131741,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain size pooling in local descriptors: Dsp-sift", "author": ["J. Dong", "S. Soatto"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal state estimation for stochastic systems: An information theoretic approach", "author": ["X. Feng", "K. Loparo", "Y. Fang"], "venue": "Automatic Control, IEEE Transactions on, 42(6):771\u2013785,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Descriptor matching with convolutional neural networks: a comparison to sift", "author": ["P. Fischer", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv:1405.5769,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Single image 3d without a single 3d image", "author": ["D. F Fouhey", "W. Hussain", "A. Gupta", "M. Hebert"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1053\u20131061,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Invariance and selectivity in the ventral visual pathway", "author": ["S. Geman"], "venue": "Journal of Physiology-Paris, 100(4):212\u2013224,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on PAMI, 6:721\u2013741, November", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580\u2013587,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Online 3d reconstruction using convex optimization", "author": ["G. Graber", "T. Pock", "H. Bischof"], "venue": "Computer Vision Workshops, IEEE International Conference on, pages 708\u2013711. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Indoor scene understanding with rgb-d images: Bottomup segmentation, object detection and semantic segmentation", "author": ["S. Gupta", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "International Journal of Computer Vision, 112(2):133\u2013149,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 564\u2013571. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint 3d scene reconstruction and class segmentation", "author": ["C. Haene", "C. Zach", "A. Cohen", "R. Angst", "M. Pollefeys"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint 3D scene reconstruction and class segmentation", "author": ["C. Hane", "C. Zach", "A. Cohen", "R. Angst", "M. Pollefeys"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 97\u2013104,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Dense 3d semantic mapping of indoor scenes from rgb-d images", "author": ["A. Hermans", "G. Floros", "B. Leibe"], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 2631\u20132638. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards consistent vision-aided inertial navigation", "author": ["J. Hesch", "D. Kottas", "S. Bowman", "S. Roumeliotis"], "venue": "Algorithmic Foundations of Robotics X, pages 559\u2013574. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Guest editorial: Scene understanding", "author": ["D. Hoiem", "J. Hays", "J. Xiao", "A. Khosla"], "venue": "International Journal of Computer Vision, 112(2):131\u2013132,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view stereo beyond lambert", "author": ["H. Jin", "S. Soatto", "A.J. Yezzi"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., pages I\u2013171\u2013178, June", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Linear prediction, filtering, and smoothing: An information-theoretic approach", "author": ["P. Kalata", "R. Priemer"], "venue": "Information Sciences, 17(1):1\u201314,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1979}, {"title": "Object discovery in 3d scenes via shape analysis", "author": ["A. Karpathy", "S. Miller", "L. Fei-Fei"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 2088\u20132095. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "3d scene understanding by voxel-crf", "author": ["B. Kim", "P. Kohli", "S. Savarese"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1425\u20131432,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS, volume 1, page 4,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint semantic segmentation and 3d reconstruction from monocular video", "author": ["A. Kundu", "Y. Li", "F. Dellaert", "F. Li", "J. Rehg"], "venue": "Computer Vision\u2013ECCV 2014, pages 703\u2013718. Springer,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection-based object labeling in 3d scenes", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 1330\u20131337. IEEE,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Online temporal calibration for camera\u2013imu systems: Theory and algorithms", "author": ["M. Li", "A. Mourikis"], "venue": "The International Journal of Robotics Research, 33(7):947\u2013964,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Holistic scene understanding for 3d object detection with rgbd cameras", "author": ["D. Lin", "S. Fidler", "R. Urtasun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1417\u20131424,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring context with deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "A. Hengel", "I. Reid"], "venue": "arXiv preprint arXiv:1603.03183,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R.X. Mottaghi", "Chen", "X. Liu", "N. Cho", "S. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Monocular slam supported object recognition", "author": ["S. Pillai", "J. Leonard"], "venue": "Proceedings of Robotics: Science and Systems (RSS), Rome, Italy, July", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Slam++: Simultaneous localisation and mapping at the level of objects", "author": ["R. Salas-Moreno", "R. Newcombe", "H. Strasdat", "P. Kelly", "A. Davison"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1352\u20131359. IEEE,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete optimization of ray potentials for semantic 3d reconstruction", "author": ["N. Savinov", "C. Hane", "M. Pollefeys"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Semantic modelling of urban scenes", "author": ["S. Sengupta", "E. Greveson", "A. Shahrokni", "P. Torr"], "venue": "International Conference on Robotics and Automation,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Recursive context propagation network for semantic scene labeling", "author": ["A. Sharma", "O. Tuzel", "M. Liu"], "venue": "Advances in Neural Information Processing Systems, pages 2447\u20132455,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2012, pages 746\u2013760. Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric scene parsing with adaptive feature relevance and semantic context", "author": ["G. Singh", "J. Kosecka"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3151\u20133157,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Actionable information in vision", "author": ["S. Soatto"], "venue": "Proc. of the Intl. Conf. on Comp. Vision, October", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Visual representations: Defining properties and deep approximations", "author": ["S. Soatto", "A. Chiuso"], "venue": "Proc. of the Intl. Conf. on Learning Representations (ICLR); ArXiv: 1411.7676, May", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "On the set of images modulo viewpoint and contrast changes", "author": ["G. Sundaramoorthi", "P. Petersen", "V.S. Varadarajan", "S. Soatto"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., June", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Shape-based object detection via boundary structure segmentation", "author": ["A. Toshev", "B. Taskar", "K. Daniilidis"], "venue": "International journal of computer vision, 99(2):123\u2013146,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust filtering for visual inertial sensor fusion", "author": ["K. Tsotsos", "A. Chiuso", "S. Soatto"], "venue": "Proc. of the Intl. Conf. on Robotics and Automation (ICRA),", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction", "author": ["V. et al. Vineet"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "Understanding and generating scene descriptions", "author": ["D. Waltz"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1981}, {"title": "Hierarchical semantic labeling for task-relevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "Robotics: Science and systems (RSS),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Sun3d: A database of big spaces reconstructed using sfm and object labels", "author": ["J. Xiao", "A. Owens", "A. Torralba"], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1625\u20131632. IEEE,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 702\u2013709. IEEE,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Sensor fusion for semantic segmentation of urban scenes", "author": ["R. Zhang", "S. Candra", "K. Vetter", "A. Zakhor"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on, pages 1850\u20131857. IEEE,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 51, "context": "An ideal representation would be a minimal sufficient statistic (of the data, for the task) that is also invariant (to nuisance factors) [53].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "A gravity-aligned, sparse attributed Euclidean point cloud, with local illumination- and poseinvariant descriptors as attributes [15], is a minimal sufficient invariant for localization tasks, including relative localization of multiple sensor platforms/agents (Fig.", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 38, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 54, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 22, "context": "We leverage on existing work [24] to infer surface topology given images and (MAP) point-estimates of camera poses as above (Fig.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "This can approximated using a convolutional architecture (CNN) [23, 45], interpreted not as an object detector, but as a likelihood function for objects being present in the scene.", "startOffset": 63, "endOffset": 71}, {"referenceID": 43, "context": "This can approximated using a convolutional architecture (CNN) [23, 45], interpreted not as an object detector, but as a likelihood function for objects being present in the scene.", "startOffset": 63, "endOffset": 71}, {"referenceID": 51, "context": "This is in line with [53], where CNNs are seen as computing marginal/profile likelihoods, which are minimal sufficient invariant statistics of a single image.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Semantic relations, such as co-occurrence and relative pose and identities, are modeled by a graph with unary and pairwise potentials [10] with a Dirichlet prior.", "startOffset": 134, "endOffset": 138}, {"referenceID": 28, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 38, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 54, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 22, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 111, "endOffset": 119}, {"referenceID": 43, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 111, "endOffset": 119}, {"referenceID": 9, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "Alternatively, one could define objects in terms of functional or topological characteristics that yield measurable correlates, for instance being \u201cdetachable,\u201d [3], or use other sensory modality, such as touch.", "startOffset": 161, "endOffset": 164}, {"referenceID": 17, "context": "There is a sizable literature on semantic scene understanding from a single image ([19] and references therein).", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "There is also a vast literature on scene segmentation, including dedicated workshops and special issues ([31] and references therein), mostly using range (RGB-D) sensors, although also from video.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 47, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 41, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 12, "context": "Some focus on real-time operation [13], but most operate off-line.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 10, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 58, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 37, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 42, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 35, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 182, "endOffset": 186}, {"referenceID": 56, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 221, "endOffset": 225}, {"referenceID": 39, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 53, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 57, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 8, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 49, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 13, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 24, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 32, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 44, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 27, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 4, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 23, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 25, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 46, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 55, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 36, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 0, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 40, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 48, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 5, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 59, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 60, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 11, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 6, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 45, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 19, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 1, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 7, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 51, "context": "3 we will show how a discriminatively-trained CNN can be leveraged to this end [53].", "startOffset": 79, "endOffset": 83}, {"referenceID": 50, "context": "Actionable Information (AI) [52] captures the first term after uncertainty due to non-controllable nuisance variability \u03ba is removed: Ht(yt+1) .", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "This process is closely related to that proposed by [22].", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "where z(y) \u223c p(z|\u011dt|t, x\u0302t|t, y) are from the posterior, which is a minimal sufficient (Bayesian) statistic of y for z [53].", "startOffset": 119, "endOffset": 123}, {"referenceID": 50, "context": "Thus a representation is a statistic of past data y that is (minimal) sufficient to hallucinate (invariants of) future data yt+1 up to an uninformative residual [52, 53].", "startOffset": 161, "endOffset": 169}, {"referenceID": 51, "context": "Thus a representation is a statistic of past data y that is (minimal) sufficient to hallucinate (invariants of) future data yt+1 up to an uninformative residual [52, 53].", "startOffset": 161, "endOffset": 169}, {"referenceID": 31, "context": "More in general, the relation between the two are described in [33, 17] for linear systems.", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "More in general, the relation between the two are described in [33, 17] for linear systems.", "startOffset": 63, "endOffset": 71}, {"referenceID": 44, "context": "For objects where the posterior converges to a single mode, we can replace the coarse shape with a high-quality model from the prior [46], if specific object recognized, or an iconic prior from the class.", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "Topology estimated by an implicit surface following [24] for some representative objects is shown in Fig.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "\u201d A convolutional neural network is often used as a local descriptor \u03c6(I|b) [4, 18], with the entire image b = D as a special case.", "startOffset": 76, "endOffset": 83}, {"referenceID": 16, "context": "\u201d A convolutional neural network is often used as a local descriptor \u03c6(I|b) [4, 18], with the entire image b = D as a special case.", "startOffset": 76, "endOffset": 83}, {"referenceID": 52, "context": "7The same can, in theory, be done for viewpoint gt and for any other nuisance that acts as a group on the data (viewpoint changes induce diffeomorphic deformations of the domain of the image away from occlusions [54]).", "startOffset": 212, "endOffset": 216}, {"referenceID": 51, "context": "They can still be sufficient statistics for certain classes of tasks, which leads us to the notion of sufficient invariants [53], which can be computed by marginalization and/or profiling (max-out).", "startOffset": 124, "endOffset": 128}, {"referenceID": 54, "context": "This localization pipeline is described in [56], and is agnostic of the organization of the scene into objects and their identity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "Note that the resulting CNN is not an object detector, but a likelihood function [53]: It is not a discriminant for a random variable l that can take one of K+1 possible values, but instead a function of a K+1-dimensional (binary) vector l, which is not normalized with respect to l.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "10 To enable visibility computation, we can use the point cloud together with the images to compute the dense shape of objects in a maximum-likelihood sense: \u015dj = argmax p(sj |g, x, y) using generic regularizers [32].", "startOffset": 212, "endOffset": 216}, {"referenceID": 54, "context": "An alternative is to approximate the shape of objects with a parametric family, for instance bounding boxes or ellipsoids, and compute visibility accordingly, also leveraging the co-visibility graph computed as a corollary from the SLAM system [56], and priors on the size and aspect ratios of objects.", "startOffset": 244, "endOffset": 248}, {"referenceID": 34, "context": "In the absence of ground truth objects zi one can model directly the predictive density using a variational autoencoder [36].", "startOffset": 120, "endOffset": 124}, {"referenceID": 54, "context": "1 Implementation SLAM is borrowed from [56, 16].", "startOffset": 39, "endOffset": 47}, {"referenceID": 43, "context": "The CNN is implemented by modifying stock code [45] by removing the classification layer and normalization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "For the purpose of validation, we also tested components of our system on the KITTI dataset [20]; representative results are reported in Fig.", "startOffset": 92, "endOffset": 96}, {"referenceID": 51, "context": "The likelihood function computed by a CNN is (an approximation of) a minimal sufficient invariant statistic of a single datum yt+1, for the inference of semantic properties of the scene [53].", "startOffset": 186, "endOffset": 190}], "year": 2017, "abstractText": "We describe a representation of a scene that captures geometric and semantic attributes of objects within, along with their uncertainty. Objects are assumed persistent in the scene, and their likelihood computed from intermittent visual data using a convolutional architecture, integrated within a Bayesian filtering framework with inertials and a context model. Our method yields a posterior estimate of geometry (attributed point cloud and associated uncertainty), semantics (identities and co-occurrence), and a point-estimate of topology for a variable number of objects within the scene, implemented causally and in real-time on commodity hardware.", "creator": "LaTeX with hyperref package"}}}