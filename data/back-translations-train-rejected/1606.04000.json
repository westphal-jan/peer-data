{"id": "1606.04000", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Using a Distributional Semantic Vector Space with a Knowledge Base for Reasoning in Uncertain Conditions", "abstract": "The inherent inflexibility and incompleteness of commonsense knowledge bases (KB) has limited their usefulness. We describe a system called Displacer for performing KB queries extended with the analogical capabilities of the word2vec distributional semantic vector space (DSVS). This allows the system to answer queries with information which was not contained in the original KB in any form. By performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors, we are able to give approximate answers to many questions which, if posed to the KB alone, would return no results.", "histories": [["v1", "Mon, 13 Jun 2016 15:45:00 GMT  (115kb,D)", "http://arxiv.org/abs/1606.04000v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["douglas summers-stay", "clare voss", "taylor cassidy"], "accepted": false, "id": "1606.04000"}, "pdf": {"name": "1606.04000.pdf", "metadata": {"source": "CRF", "title": "Using a Distributional Semantic Vector Space with a Knowledge Base for Reasoning in Uncertain Conditions", "authors": ["Douglas Summers-Stay", "Clare Voss", "Taylor Cassidy"], "emails": ["douglas.a.summers-stay.civ@mail.mil"], "sections": [{"heading": null, "text": "We also show how the hand-curated knowledge in a KB can be used to increase the accuracy of a DSVS in solving analogy problems, allowing one KB and one DSVS to compensate for each other's weaknesses. Keywords: semantic vector space, knowledge base, analogy"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move to another world, in which they are able to live, in which they want to live."}, {"heading": "2 Biological Inspiration", "text": "In 1984, Geoff Hinton [12] outlined ways in which distributed representations, characteristic of a DSVS, were biologically more plausible than the local representations used in KBs. He pointed out that both the strengths of human memory (content-addressing, generalization, analogy creation) and its weaknesses (the difficulty of remembering any set of concepts) are similar to those of distributed representations. Brain imaging studies have also suggested that concepts in the brain are presented as distributed networks of neuronal activation [18]. A DSVS can be interpreted as a neural model of how memories might be encoded. [2] In particular, the analogical thinking ability of a DSVS can be understood as an example of the relational primation model of analogy outlined in the analogy [15]. There is evidence that object categories are represented as continuous seatic space on the surface of the brain."}, {"heading": "3 Background", "text": "Here we describe how to use a KB and a DSVS, and combine these efforts with previous research on combining the two. We have not solved the problem of parsing natural language queries in KB queries, although this would be an important component of a ready-made system."}, {"heading": "3.1 Answering Queries with the ResearchCyc KB", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves, and are able to play by the rules they have set for themselves."}, {"heading": "3.2 Solving Analogies with the word2vec DSVS", "text": "In the 1980s and early 1990s, researchers attempted to solve analogy problems with the help of KBs. [11] It is proposed to use the structure of concepts to find analogies, rather than the more obvious mapping of relationships. Unfortunately, as [13] pointed out, such a structure is heavily dependent on the preconcepts inherent in the design of the knowledge base. The use of distribution-based semantic information was a breakthrough that applied the system of [22] at the human level to multiple-choice questions of the SAT analogy. word2vec DSVS [16] uses an efficient method to build up the vector space so that it can be created with hundreds of billions of words of training data. Except for this ability to use a larger training set and some well-chosen parameters, however, word2vec is essentially optimizing the same goal as latent semantic analysis (LSA)."}, {"heading": "3.3 Using a KB with a DSVS", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "4 Experiments with Answering Queries", "text": "The first experiment (3.1) shows how semantic similarity can be used to estimate which of two responses to a query is more likely to be correct, based on the results of the query on similar3The sum and average are identical when the semantic distance is measured from a cosinal distance, as is the case in many of these papers.4A Matlab wrapper by Shai Bagon for a k-d tree ANN library by David Mount and Sunil Arya.terms. The second experiment (3.2) is an attempt to estimate the likelihood of finding the best answer to a query within the first few results returned by Displacer. The third experiment (3.3) shows how Displacer can be used on queries with many correct answers. The last two experiments (4.1 and 4.2) use the components of Displacer in a different arrangement to show how the KB can be used to improve the performance of the DSVS in solving analogies rather than answering queries."}, {"heading": "4.1 Experiment: Estimating the gender of given names", "text": "The purpose of this experiment was to test Displacer's ability to correctly answer a query about an unknown term when there are two probable possibilities for the result in the Knowledge Base. We used 1990 U.S. Census Bureau data on the 800 most popular names for men and women in the U.S. (who do not appear on the list for the opposite sex) and asked the system about the gender of the individual with each name. KB contains gender and name information for some celebrities and historical individuals. Of the 1,600 names, only 146 were identified as belonging to a person about whom the KB had sex information. Moreover, it estimated the answer based on responses to similar terms. Responses where more than 50% of the names were labeled male were classified as male, and similar for female names."}, {"heading": "4.1.1 Approach", "text": "Displacer was used for this experiment as follows: 1. Start with a list of the 800 most popular unique male and female names from U.S. Census data. For each name, do the following: 2. Map the name from an English word string to a vector using word2vec.3. Find the vector's closest neighbors in DSVS.4. Map the closest neighbors of vectors to English words using vec2word.5. Map the English words to Cyc constants using word2cyc.6. Search Cyc for famous individuals with the same first name and enter the gender of those famous persons.7. Map the resulting Cyc expressions into English words (\"male\" and \"female\") using cyc2word.8. Map the results from the previous step onto vectors using word2vec.9. Average vectors and compare the distance from the average vector to the vectors for \"male\" and \"female.\""}, {"heading": "4.1.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.3 Discussion", "text": "The names in the database come from all over the world (albeit with a strong Western bias), while in the test they only come from the United States. Nevertheless, in most cases, the system allows for clearly correct conclusions. The reason is simple: Female names are often used in sentences together with the word \"she\" in similar contexts, resulting in them being mapped closely together in the DSVS. The vector leading from one of these names to the vector for \"female\" is almost identical (and similar for male names and the vector for \"male\")."}, {"heading": "4.2 Experiment: Estimating probability of finding single correct answers", "text": "For other predicates such as capitalCity or animalTypeMakesSoundType, the query of any term yields a unique result. Displacer is able to take all of these unique results and create a combined estimate for queries on terms that are not in KB. In the word2vec DSVS, the vector from A (e.g. France) to its result B (Paris) can be applied to A (England) to estimate its result B '(London). By averaging several of these vectors, we can reduce noise and obtain a high-quality estimate. Choosing A as close as possible to A' will probably yield the best results, as the contexts are most similar and the vector between A and B are most similar and almost parallel to the vector between A 'and B. However, there will inevitably be noise in the context, and averaging multiple vectors together will tend to reduce this noise."}, {"heading": "4.2.1 Approach", "text": "In this experiment, we chose predicates for which each query yields only a single result, and most of these results were unique; the steps that Displacer followed in this experiment were similar to those in the previous experiment, but the results were backmapped using the displacement vectors in the last steps. As a concrete example, the following procedure was used for the first query: 1. Query the KB to obtain a list of countries. 5. For each country, perform the following steps: 5. In this experiment, we used a leave-one-out method, since the goal was to estimate how often the use of the full KB would give the correct answer. 2. Search the name of the country from an English word sequence to a vector with word2vec.3. The closest neighbors of the vector in the DSVS.4. Map the closest neighbors of vectors to English words with vec2word.5. Map the English words to Cyc expressions with word2cyc.6. Search Cyc for the capital of the 4th map of the closest neighbors of vectors to the original neighbors under the English neighbors using the Cyc expressions of the capital of the 4th vector."}, {"heading": "4.2.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.3 Discussion", "text": "These probabilities apply to other query terms contained in the DSVS, but not to the KB. The query (capitalCity Germany? X) does not provide results from the KB, because Germany was not yet a reunited country at the time the information was entered. However, since most other capitals and their corresponding countries are included in the KB, the analogy between many of these countries and their capitals, as well as between Germany and Berlin, is very well supported, and the correct answer, Berlin, has a lower sum of distances than other nearby terms, such as Frankfurt.This probability estimate is not perfect. It may, for example, be the case that certain facts not invoked in the KB occur more rarely, in which case the DSVS will also have a worse estimate of these facts. The process also requires knowing what general type of answer is acceptable for the given predicate and term in order to derive the sample from which the estimate is to be made. In some cases, the limitations of KB are sufficient."}, {"heading": "4.3 Experiment: Using a DSVS to support a KB in answering queries with many results", "text": "In both of the experiments above, each query yielded only a single result. For example, when queries are made for the parts of objects, there are many correct results for each query. In such cases, displacers can still be used, but the probability that a result is correct cannot be calculated in the same way. Instead, we use k-means to cluster the results and find the English word that corresponds to the mean of each cluster."}, {"heading": "4.3.1 Approach", "text": "USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "4.3.2 Results", "text": "The DSVS (and the overall system) were able to find an interpretation of 294 of these terms, with only 52 missing. (These tended to be older terms found in dictionaries, but rarely in newspaper articles such as Gangsaw, Pavoir and Triphammer.) For the 294 machines, we asked Displacers for physical parts of these machines. To make a fair comparison, we also inserted an incomplete version of our overall system. First, we used the system in semantic query mode for KB, similar to the system described in [9]. This performs a query for similar terms, but does not perform the final displacement step, which maps the answers into the original context using the analogy capability of word2vec. The results are summarized in the following table."}, {"heading": "4.3.3 Discussion", "text": "This shows that Displacer provides more correct answers than the KB alone or one of the systems that does not rely on the analog capability of word2vec. In all of the above experiments we apply a single predicate to a single term, but because Displacer is based on the full meaningfulness of the KB, it can be used for much more complex queries with logical expressions (\"and,\" or, \"not,\" \"it exists,\" etc.)."}, {"heading": "5 Experiments with Solving Analogies", "text": "The previous experiments showed how a KB can be used to extend the scope and precision of a KB term. In this experiment, we tested how a KB can be used to improve the analogue reasoning capability of a DSVS. In some cases, the relationship between the first two terms in a proportional analogy problem corresponds exactly to a predicate in a knowledge base. For example, in the Paris analogy: The KB will be correctly England for X. We can find such predicates using the following query in the Cyc KB (and (assertedPredicateArg France 1? X) (assertedPredicateArg CityOfParisFrance 2? X)."}, {"heading": "5.1 Experiment: Semantic Syntactic Word Relationship test set", "text": "For the condition \"DSVS alone,\" we calculated the vector from term 1 to term 2 and added this vector to term 3 in order to obtain the answer vector. We used the natural-language term with the minimum distance to this answer vector (if it differed from the three search terms.) For condition KB alone, we searched for results that corresponded to the analogy patterns discussed above. If the KB returned many results, an answer was randomly selected from the replies returned. For the combined system, the answer closest to the KB was selected."}, {"heading": "5.1.1 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.2 Discussion", "text": "It should come as no surprise that a database of countries and capitals is able to do a good job in finding the capital of a particular country. However, the displacer did better than either the KB8In practice, our identification of parts of the language is not perfect, so if no term can be found that matches parts of the language, we ignore this criterion in all categories. KB was not able to find patterns that match many analogies in some categories, resulting in a low score for opposites, superlatives, nationality and family. In these cases, the displacer relied on the DSVS and only put correct parts of the speech through. For other predictions, KB found too many potentially correct answers. Without a good way to judge these according to what should be the most natural answer, the system randomly chose between them, estimating the basic truth only a small fraction of the time. This explains the low score for capitals, the word, the word, the word, and the result is likely B, with the result being the word, the word and the GB."}, {"heading": "5.2 Experiment: SAT four-term analogy test set", "text": "The second set of 374 analogies for four semesters comes directly from SAT tests [22]. Unlike the analogies in the previous test, these are difficult for humans to solve. In the past, most tests of automated systems on these questions were carried out by selecting multiple-choice pairs of answers, but to make the results comparable to the previous test, we provide the first three terms of analogy and ask them to provide an appropriate fourth semester. 9"}, {"heading": "5.2.1 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.2 Discussion", "text": "The greatest benefit comes from enforcing language restrictions and rejecting synonyms and related forms of the three input terms when appropriate. In only ten cases did attempts to find an appropriate analogy in the KB succeed in improving the result over the DSVS alone, and in four cases this resulted in the system incorrectly rejecting a good answer. Clearly, the system's approach to finding analogies in the KB is too simplistic for more difficult analogies."}, {"heading": "6 Conclusion and Future Work", "text": "A KB and a DSVS can be used together to make the KB less brittle and with greater range (with approximate knowledge), and to provide enough analog examples of the DSVS to give good estimates of the likelihood of the accuracy of its own answers. Since these analogies allow for several possible fourth terms, there is a subjective element to the evaluation. Both query and analog tasks, the combination of the two either exceeds the individual. In this paper, all queries are expressed in Cyc syntax, but to be widely useful, a system must be able to handle queries in natural language. Displacer itself could potentially be used for some degree of literal clarification and reference resolution. We plan to look for ways to confirm or reject possible answers based on other information within the KB or DSVS that might alter the probability of the veracity of an answer. We also plan to explore ways to supplement the text, such as learning from video sources, or to use the information from it."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1502.03520,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A neurally plausible encoding of word order information into a semantic vector space", "author": ["Peter Blouw", "Chris Eliasmith"], "venue": "In 35th Annual Conference of the Cognitive Science Society,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "In AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A survey of automatic query expansion in information retrieval", "author": ["Claudio Carpineto", "Giovanni Romano"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Discovery at a distance: farther journeys in predication space", "author": ["Thomas Cohen", "Dominic Widdows", "Roger W Schvaneveldt", "Thomas C Rindflesch"], "venue": "In Bioinformatics and biomedicine workshops (BIBMW),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Predication-based semantic indexing: Permutations as a means to encode predications in semantic space", "author": ["Trevor Cohen", "Roger W Schvaneveldt", "Thomas C Rindflesch"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Experiences using the researchcyc upper level ontology", "author": ["Jordi Conesa", "Veda C Storey", "Vijayan Sugumaran"], "venue": "In Natural Language Processing and Information Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "arXiv preprint arXiv:1411.4166,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Natural language queries over heterogeneous linked data graphs: A distributional-compositional semantics approach", "author": ["Andre Freitas", "Edward Curry"], "venue": "In Proceedings of the 19th international conference on Intelligent User Interfaces,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A distributional semantics approach for selective reasoning on commonsense graph knowledge bases", "author": ["Andr\u00e9 Freitas", "Jo\u00e3o Carlos Pereira da Silva", "Edward Curry", "Paul Buitelaar"], "venue": "In Natural Language Processing and Information Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["Dedre Gentner"], "venue": "Cognitive science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}, {"title": "The copycat project: A model of mental fluidity and analogy-making", "author": ["Douglas R Hofstadter", "Melanie Mitchell"], "venue": "Advances in connectionist and neural computation theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "A continuous semantic space describes the representation of thousands of object and action categories across the human", "author": ["Alexander G Huth", "Shinji Nishimoto", "An T Vu", "Jack L Gallant"], "venue": "brain. Neuron,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Analogy as relational priming: A developmental and computational perspective on the origins of a complex cognitive skill", "author": ["Robert Leech", "Denis Mareschal", "Richard P Cooper"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Looking for hyponyms in vector space", "author": ["Marek Rei", "Ted Briscoe"], "venue": "In Proceedings of the 18th Conference on Computational Natural Language Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Distributed representations in memory: insights from functional brain imaging", "author": ["Jesse Rissman", "Anthony D Wagner"], "venue": "Annual review of psychology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Can we derive general world knowledge from texts? In Proceedings of the second international conference on Human Language Technology Research, pages 94\u201397", "author": ["Lenhart Schubert"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Analogyspace: Reducing the dimensionality of common sense knowledge", "author": ["Robert Speer", "Catherine Havasi", "Henry Lieberman"], "venue": "In AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Analogyspace: Reducing the dimensionality of common sense knowledge", "author": ["Robert Speer", "Catherine Havasi", "Henry Lieberman"], "venue": "In AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Corpus-based learning of analogies and semantic relations", "author": ["Peter D Turney", "Michael L Littman"], "venue": "Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Knowledge graph and text jointly embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "and research into the area has exploded since the word2vec DSVS was introduced in [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Brain-imaging studies have likewise suggested that concepts are represented in the brain as distributed networks of neural activation [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "In particular, the analogical-reasoning capability of a DSVS can be understood as an example of the relational priming model of analogy making outlined in [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "[14] The brain\u2019s slow operating speed and massive paralellism (compared to a CPU) also hint that whatever operations are being performed must be very short, simple programs operating on large vectors, more characteristic of A DSVS than a KB.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Using the ResearchCyc KB to support web queries is explored in [7].", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "[11] suggested using the structure of concepts to find analogies, rather than the more obvious matching of relations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Unfortunately, as [13] pointed out, such structure is heavily dependent on the preconceptions inherent in the design of the knowledge base.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "The use of distributional semantic information was a breakthrough, allowing the system of [22] to score at human level on multiple-choice SAT analogy questions.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "The word2vec DSVS [16] uses an efficient method to build the vector space, allowing it to be created with hundreds of billions of words of training data.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "2The basic reason why word2vec is able to solve these kinds of analogies is explained clearly in [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "Others, such as the ones encoding hypernyms, vary a lot from one term to another [17].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "In [16] the creators of word2vec write, \u201cOur ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "A good survey of query expansion is [4].", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "The idea of using a DSVS to extend the capability of a KB as a query-KB semantic matching technique is explored in [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "[17] for example, explores how hyponyms swarm around a term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The possibility has also been explored of reshaping a DSVS according to verified facts in [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 21, "context": "[23] also explores putting a knowledge graph (that is, a KB) into the space of a DSVS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "AnalogySpace, built from the large KB ConceptNet[20], is one such example.", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "Knowledge graph embedding seems promising, especially when combined with knowledge extraction efforts such as KNEXT[19] and NELL[3].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Knowledge graph embedding seems promising, especially when combined with knowledge extraction efforts such as KNEXT[19] and NELL[3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "First we used the system in a query-KB semantic matching mode, similar to the system described in [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "Dedre Gentner called such matches with a predefined relation vector relating terms trivial, and felt that very few analogies would be captured by such relationships [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "8 We tested analogy-finding ability on two test sets: the Semantic Syntactic Word Relationship test set introduced in [16] to test word2vec, and the SAT four-term analogy test set from (Turney, 2005)", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "The second set of 374 four-term analogies comes directly from SAT tests [22].", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "The inherent inflexibility and incompleteness of commonsense knowledge bases (KB) has limited their usefulness. We describe a system called Displacer for performing KB queries extended with the analogical capabilities of the word2vec distributional semantic vector space (DSVS). This allows the system to answer queries with information which was not contained in the original KB in any form. By performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors, we are able to give approximate answers to many questions which, if posed to the KB alone, would return no results. We also show how the hand-curated knowledge in a KB can be used to increase the accuracy of a DSVS in solving analogy problems. In these ways, a KB and a DSVS can make up for each other\u2019s weaknesses.", "creator": "easychair.cls-3.4"}}}