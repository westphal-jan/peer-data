{"id": "1606.04038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Trace Norm Regularised Deep Multi-Task Learning", "abstract": "In this paper, we propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that one neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning work, we do not predefine a parameter sharing strategy by tying some (usually bottom) layers' parameters, instead, our framework allows the sharing for all shareable layers thus the sharing strategy is learned from a pure data-driven way.", "histories": [["v1", "Mon, 13 Jun 2016 17:15:43 GMT  (15kb)", "http://arxiv.org/abs/1606.04038v1", "Technical Report, 5 pages"], ["v2", "Fri, 17 Feb 2017 01:33:17 GMT  (42kb,D)", "http://arxiv.org/abs/1606.04038v2", "Submission to Workshop track - ICLR 2017"]], "COMMENTS": "Technical Report, 5 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy m hospedales"], "accepted": false, "id": "1606.04038"}, "pdf": {"name": "1606.04038.pdf", "metadata": {"source": "CRF", "title": "Trace Norm Regularised Deep Multi-Task Learning", "authors": ["Yongxin Yang", "Timothy M. Hospedales"], "emails": ["t.hospedales}@qmul.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.04 038v 1 [cs.L G] 13 Jun 2016"}, {"heading": "1 Introduction and Related Work", "text": "The paradigm of Multi-Task Learning (MTL) [Caruana, 1997] is to learn several tasks together, and the knowledge gained from one task could be reused by others. In this section we will briefly discuss some studies in this area."}, {"heading": "1.1 Matrix-based Multi-Task Learning", "text": "Matrix-based MTL is normally based on a linear model, i.e. each task is parameterized by a D-dimensional weighting vector w, and the model prediction is y = x \u00b7 w = xTw, where x is a D-dimensional feature vector representing an instance. Here, the objective function for matrix-based MTL can be described as follows: T \u2211 i = 1N (i) \u2211 j = 1 (y) j, x (i) j \u00b7 w (i) + (W) (1) Here, there is a loss function for the true designation y and the predicted designation y. T is the number of tasks, and for the i-th task there are N (i) training instances. If the dimensionality of each task is equal, the models - w (i) s - are of the same size. Then, the collection of w (i) s forms a D matrix W, of which the i-matrix is a] task, the W-ou is a matrix."}, {"heading": "1.2 Tensor-based Multi-Task Learning", "text": "Although it is a classic setting for each task to be indexed by a single factor, tasks in many real problems are indexed by multiple factors. For example, to build a restaurant recommendation system, we want a regression model that predicts the values for different aspects (food quality, environment) by different customers. Afterwards, the task is indexed by aspects \u00d7 customers. In this case, the collection of all linear models for all tasks is then a 3-way tensor W of size D \u00d7 T1 \u00d7 T2, where T1 and T2 are the number of aspects and the number of customers respectively. Consequently, it must be a kind of tensor regulation [Tomioka et al., 2010]. An alternative solution is, for example, the sum of trace standards for all matrices 1 [Romera-paredes et al., 2013] and a scaled latent trace standard [Wimalawarne et al., 2014]."}, {"heading": "1.3 Multi-Task Learning for Neural Networks", "text": "Zhang et al. [2014] use a revolutionary neural network to find landmarks in the face and recognize facial features (e.g. emotions). Liu et al. [2015] propose a deep neural network for classifying queries and gathering information (ranking for web search). One of the main similarities of this work is that they all use a kind of predefined parameter-sharing strategy. A typical design consists of using the same parameters for the lower layers of the deep neural network and using task-specific parameters for the upper layers. This type of architectural design can be traced back to the 2000s [Bakker and Heskes, 2003]. Modern models of neural networks usually contain a large number of layers, making it extremely difficult to decide by which layer the neural network will be divided for different tasks."}, {"heading": "2 Methodology", "text": "Instead of predefining a strategy for parameter sharing, we propose the following framework: For T tasks, each task is modeled by a neural network of the same architecture2. We collect the parameters in layers and set a tensor norm for each collection. We illustrate our framework using a simple example: assuming we have T = 2 tasks, each of which is modeled by a neural 4-layer folding network (CNN). CNN architecture is: (1) layer (\"Conv1\") of size 5 x 5 x 32, (2) layer (\"Conv2\") of size 3 x 32 x 64, (3) layer (\"fc1\") of size 256 x 256, (4) layer (\"Conv1\") of size 5 x 32 x 32, \"(2) layer (\"), \"layer (\" fc2 \") of the first layer,\" and \"(\" c2 \") layer."}, {"heading": "2.1 Tensor Norm", "text": "We opt for the trace standard, which is defined as the sum of the singular values of the matrix. | | X | | \u043a = \u2211 i = 1\u03c3i (2) 1Matriciation is also known as tensor unfolding or flattening. 2In the case that each task has a different number of outputs, the parameters of the top layers of neural networks are of different sizes, so they are chosen for splitting. The trace standard is named by the fact that if X is a positive semidefinitive matrix, it is the track of X. It is sometimes referred to as the nuclear standard. Trace standard is the closest convex relationship of the matrix rank [Recht et al., 2010]. The extension of the trace standard from matrix to tensor is not unique, just like the rank of tensor definitions."}, {"heading": "2.2 Optimisation", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3 Experiment", "text": "We implement the proposed method of TensorFlow [Abadi et al., 2015], and the code is available on Github3."}, {"heading": "3.1 Omniglot", "text": "We demonstrate the proposed method using the Omniglot dataset [Lake et al., 2015]. Omniglot contains handwritten characters in 50 different alphabets (e.g. Cyrillic, Korean, Tengwar), each with its own number of unique characters (14 \u0445 55). In total, there are 1623 unique characters, and each has exactly 20 instances. Here, each task corresponds to an alphabet, and the goal is to recognize their characters. The images are monochrome in size 105 \u00d7 105. We design a CNN with 3 Convolutionary and 2 FC layers. The first Conv layer has 8 filters of size 5 \u00d7 5; the second Conv layer has 12 filters of size 3 \u00d7 3, and the third Convolutionary layer has 16 filters of size 3 \u00d7 3. Each Convolutionary layer is followed by a 2 \u00d7 2 max-pooling layer (the first FC layer has 64 neurons, and the second FC layer has a size corresponding to the unique class in the alphabet)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org", "author": ["Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "Yu and Zheng.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Zheng.", "year": 2015}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Task clustering and gating for Bayesian multitask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Bakker and Heskes.,? \\Q2003\\E", "shortCiteRegEx": "Bakker and Heskes.", "year": 2003}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ji and Ye.,? \\Q2009\\E", "shortCiteRegEx": "Ji and Ye.", "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daum\u00e9 III"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Rev.,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Multilinear multitask learning", "author": ["B. Romera-paredes", "H. Aung", "N. Bianchi-berthouze", "M. Pontil"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Romera.paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.paredes et al\\.", "year": 2013}, {"title": "On the extension of trace norm to tensors", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima"], "venue": "In NIPS Workshop on Tensors, Kernels, and Machine Learning,", "citeRegEx": "Tomioka et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2010}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": null, "citeRegEx": "Tucker.,? \\Q1966\\E", "shortCiteRegEx": "Tucker.", "year": 1966}, {"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "author": ["K. Wimalawarne", "M. Sugiyama", "R. Tomioka"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "A unified perspective on multi-domain and multi-task learning", "author": ["Y. Yang", "T.M. Hospedales"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yang and Hospedales.,? \\Q2015\\E", "shortCiteRegEx": "Yang and Hospedales.", "year": 2015}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "The paradigm of multi-task learning (MTL) [Caruana, 1997] is to learn multiple tasks jointly, and the knowledge obtained from one task could be reused by others.", "startOffset": 42, "endOffset": 57}, {"referenceID": 1, "context": "Some choices could be the l2,1 norm [Argyriou et al., 2008], and the trace norm [Ji and Ye, 2009].", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": ", 2008], and the trace norm [Ji and Ye, 2009].", "startOffset": 28, "endOffset": 45}, {"referenceID": 11, "context": "Consequently \u03a9(W) has to be a kind of tensor regularisation [Tomioka et al., 2010].", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "For example, sum of the trace norms on all matriciations [Romera-paredes et al., 2013], and scaled latent trace norm [Wimalawarne et al.", "startOffset": 57, "endOffset": 86}, {"referenceID": 13, "context": ", 2013], and scaled latent trace norm [Wimalawarne et al., 2014].", "startOffset": 38, "endOffset": 64}, {"referenceID": 14, "context": "An alternative solution is to concatenate the one-hot encodings of task factors and feed it as input into a two-branch neural network model [Yang and Hospedales, 2015], in which there are two input channels for feature vector and encoded task factor.", "startOffset": 140, "endOffset": 167}, {"referenceID": 2, "context": "This kind of architecture design can be traced back to 2000s [Bakker and Heskes, 2003].", "startOffset": 61, "endOffset": 86}, {"referenceID": 13, "context": "Zhang et al. [2014] use a convolutional neural network to find facial landmarks as well as recognise face attributes (e.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Liu et al. [2015] propose a deep neural network for query classification and information retrieval (ranking for web search).", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Trace norm is the tightest convex relation of matrix rank [Recht et al., 2010].", "startOffset": 58, "endOffset": 78}, {"referenceID": 12, "context": ", Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011].", "startOffset": 23, "endOffset": 37}, {"referenceID": 8, "context": ", Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011]. We propose three tensor trace norm designs here, which are corresponding to three variants of the proposed method.", "startOffset": 69, "endOffset": 86}, {"referenceID": 6, "context": "1 Omniglot We demonstrate the proposed method using the Omniglot dataset [Lake et al., 2015].", "startOffset": 73, "endOffset": 92}], "year": 2017, "abstractText": "In this paper, we propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that one neural network is encouraged to reuse others\u2019 parameters if possible \u2013 this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning work, we do not predefine a parameter sharing strategy by tying some (usually bottom) layers\u2019 parameters, instead, our framework allows the sharing for all shareable layers thus the sharing strategy is learned from a pure data-driven way.", "creator": "LaTeX with hyperref package"}}}