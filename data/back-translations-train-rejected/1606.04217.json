{"id": "1606.04217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "abstract": "Dealing with the complex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.", "histories": [["v1", "Tue, 14 Jun 2016 07:04:37 GMT  (382kb,D)", "http://arxiv.org/abs/1606.04217v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["ekaterina vylomova", "trevor cohn", "xuanli he", "gholamreza haffari"], "accepted": false, "id": "1606.04217"}, "pdf": {"name": "1606.04217.pdf", "metadata": {"source": "CRF", "title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "authors": ["Ekaterina Vylomova", "Trevor Cohn", "Xuanli He", "Gholamreza Haffari"], "emails": ["evylomova@gmail.com", "tcohn@unimelb.edu.au", "xuanlih@student.unimelb.edu.au", "gholamreza.haffari@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution. \""}, {"heading": "2 Related Work", "text": "Most neural models for NLP rely on words as their basic units and therefore face the problem of how to treat tokens in the test sentence that are outside the vocabulary (OOV), i.e. they do not appear in the training set (or are considered too rare in the training set to be included in the vocabulary.) Often, these words are assigned either to a special UNK token that allows them to apply to all data, but at the expense of modelling accuracy, especially in structured problems such as language modelling and translation, where the identity of the word is crucial in making the next decision. One solution to the OOV problem is the modelling of sub-word units based on their composite morphemes. (2013) suggested a recursive combination of morphoses with affine transformations that are unable to distinguish between compositional and non-compositional cases."}, {"heading": "3 Operation sequence model", "text": "The first contribution in this paper is a neural network variant of the Operational Sequence Model (OSM), in which the terms of the target sentence are broken down into a number of different factors, in which each factor is modelled separately and conditioned on a rich history of recent translation decisions. (Durrani et al., 2011; Feng and Cohn, 2013) the sequence of operations is modelled as a Markov chain with a limited history in which each translation decision is conditioned. (Feng and Cohn, 2013) The sequence of operations is described as a Markov chain with a limited history in which each translation decision is conditioned. (Feng and Cohn, 2013) The sequence of operations is presented as a Markov chain with a limited history in each translation decision."}, {"heading": "4 Word Representation Models", "text": "To test this hypothesis, we need to characterize words by their subword units in order to capture the lemmas and morphological affixes, thus allowing for a better generalization between similar word formulas. To test this hypothesis, we look at both morphemes and character encoding methods.3More generally, \u03a6 (.) can capture all aspects of past alignment decisions, so it can be used to impose structural distortions to restrict the alignment space in neural OSM, e.g. symmetry, fertility, and positioning bias. We compare this with the subword embedding approach. For each type of subword unit, we will learn two word expressions: one from the subunits and the word embedding."}, {"heading": "4.1 Bag of Sub-word Units", "text": "This method is inspired by (Botha and Blunsom, 2014), where the embedding of subword units is simply added, ew = \u2211 u-Uw mu, where mu is the embedding of subword unit u."}, {"heading": "4.2 Bidirectional LSTM Encoder", "text": "The encoding of the word is formulated using a pair of LSTMs (Bi-LSTM) that operates from left to right via the input sequence, and another that operates from right to left, h \u2192 j = LSTM (h \u2192 j \u2212 1, muj) and h \u2190 j = LSTM (h \u2192 j + 1, muj), where h \u2192 j and h \u2190 j are the hidden states of the LSTM. 5 The initial word is then presented as a pair of hidden states, from the left- and right-most states of the LSTMs. These are fed into the multi-layer perception (MLP) with a single hidden level and a Tanh activation function to form the word representation, ew = MLP (h \u2192 | Uw |, h \u2190 1).4We only include word embedding for common words; rare words share a UNK embedding. 5The memory cells are compressed as part of the recurrence, here."}, {"heading": "4.3 Convolutional Encoder", "text": "The last word coder we are looking at is a revolutionary neural network inspired by a similar approach to speech modeling (Kim et al., 2016).The idea of unitlevel CNN is to apply a kernel Ql-REu-kl with the width kl-kl to Uw to get a function board fl-R-U-kl-kl. More formally, for the jth element of the function card the revolutionary representation is fl (j) = tanh (< Uw, j, Ql > + b), where Uw, j-REu-kl is a disk of Uw that spans the representation of the jth element of the function card and its preceding unit olent."}, {"heading": "5 Experiments", "text": "We compare the various word representation models based on three morphologically rich languages with both external and intrinsic assessments. For external assessments, we examine their impact on translation into English from Estonian, Romanian and Russian using our neural OSM. For intrinsic assessments, we examine how exactly the models become semantically / syntactically related words into a set of predefined words. Datasets, we use parallel bilingual data from Estonian-English and Romanian-English (Koehn, 2005), and web-crawled parallel data for Russian-English (Antonova and Misyurev, 2011). For preprocessing, we use bilingual, and filter sentences longer than 30 words. We also apply a frequency threshold of 5, and replace all low-frequency words with a special UNK token."}, {"heading": "5.1 Extrinsic Evaluation: MT", "text": "This year, the number of work-related incapacity days is many times higher than in previous years."}, {"heading": "5.2 Intrinsic Evaluation", "text": "We examine how well the closest neighbors are interchangeable with a query word in the translation process, thus defining the term transformation. We take a closer look at the embeddings learned from the models, based on how well they capture thesemantic and morphological information in the closest neighboring words. We therefore divide the test dictionary into 6 subgroups whose frequency is determined in the training: [0-4], [10-14], [15-19], [20-50], and 50 +. Since we have set the word frequency threshold to 5 for training, all words in the frequency bandwidth [0.4] actually appear OOOVs for the test group. We take each word of the test set as the 20 closest neighbors from the entire training (without threshold) using cosmic metrics. We examine how well the closest neighbors are interchangeable with a query word in the translation process."}, {"heading": "6 Conclusion", "text": "In this context, we compared four different models of word representation at the morphology and character level for the source language. These models lead to more robust encodings of words in morphologically rich languages and overall better translations than simple word embeddings. Our detailed analyses have shown that word embeddings are superior to frequent words, whereas conventional methods are best suited to dealing with rare words. Comparing the revolutionary and recurrent methods over strings has shown that the revolutionary method better captures the crucial problem for the translation of words from the vocabulary and would also be crucial in many other semantic applications."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Bannard", "Chris Callison-Burch"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bannard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bannard et al\\.", "year": 2005}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A Botha", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1405.4273", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Character-based neural machine translation. arXiv preprint arXiv:1603.00810", "author": ["Costa-juss\u00e0", "Fonollosa2016] Marta Costa-juss\u00e0", "Jose Fonollosa"], "venue": null, "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Lagus2007] Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "A joint sequence translation model with integrated reordering", "author": ["Helmut Schmid", "Alexander M. Fraser"], "venue": "In The 49th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Durrani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2011}, {"title": "A markov model of machine translation using nonparametric bayesian inference", "author": ["Feng", "Cohn2013] Yang Feng", "Trevor Cohn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015a] Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015b] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black"], "venue": "arXiv preprint arXiv:1511:04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "Models of end-to-end machine translation based on neural networks have been shown to produce excellent translations, rivalling or surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 193, "endOffset": 272}, {"referenceID": 0, "context": "Models of end-to-end machine translation based on neural networks have been shown to produce excellent translations, rivalling or surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 193, "endOffset": 272}, {"referenceID": 21, "context": "Accordingly sentences containing rare words tend to be translated much more poorly than those containing only common words (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 123, "endOffset": 170}, {"referenceID": 0, "context": "Accordingly sentences containing rare words tend to be translated much more poorly than those containing only common words (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 123, "endOffset": 170}, {"referenceID": 19, "context": "(2016)), with a few notable exceptions (Ling et al., 2015b; Sennrich et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016), these approaches have not been applied to the more challenging problem of translation.", "startOffset": 39, "endOffset": 115}, {"referenceID": 10, "context": ", Botha and Blunsom (2014), Kim et al. (2016)), with a few notable exceptions (Ling et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 7, "context": "Our novel neural MT model, is based on the operation sequence model (OSM; Durrani et al. (2011), ar X iv :1 60 6.", "startOffset": 74, "endOffset": 96}, {"referenceID": 0, "context": "Our OSM can be considered as a form of attentional encoder-decoder Bahdanau et al. (2015) with hard attention in which each decision is contextualised by at most one source word, contrasting with the soft attention in Bahdanau et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "Our OSM can be considered as a form of attentional encoder-decoder Bahdanau et al. (2015) with hard attention in which each decision is contextualised by at most one source word, contrasting with the soft attention in Bahdanau et al. (2015).", "startOffset": 67, "endOffset": 241}, {"referenceID": 15, "context": "Luong et al. (2013) proposed a recursive combination of morphs using affine transformation, however this is unable to differentiate between the compositional and non-compositional cases.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "Luong et al. (2013) proposed a recursive combination of morphs using affine transformation, however this is unable to differentiate between the compositional and non-compositional cases. Botha and Blunsom (2014) aim to address this problem by forming word representations from adding a sum of each word\u2019s morpheme embeddings to its word embedding.", "startOffset": 0, "endOffset": 212}, {"referenceID": 10, "context": "Several authors have proposed convolutional neural networks over character sequences, as part of models of part of speech tagging (Santos and Zadrozny, 2014), language models (Kim et al., 2015) and machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 175, "endOffset": 193}, {"referenceID": 1, "context": "Another strand of research has looked at recurrent architectures, using long-short term memory units (Ling et al., 2015a; Ballesteros et al., 2015) which can capture long orthographic patterns in the character sequence, as well as non-compositionality.", "startOffset": 101, "endOffset": 147}, {"referenceID": 7, "context": "The first contribution of this paper is a neural network variant of the Operational Sequence Model (OSM) (Durrani et al., 2011; Feng and Cohn, 2013).", "startOffset": 105, "endOffset": 148}, {"referenceID": 7, "context": "In previous work (Durrani et al., 2011; Feng and Cohn, 2013), the sequence of operations is modelled as Markov chain with a bounded history, where each translation decision is conditioned on a finite history of past decisions.", "startOffset": 17, "endOffset": 60}, {"referenceID": 0, "context": "Note that the neural OSM can be considered as a hard attentional model, as opposed to the soft attentional neural translation model (Bahdanau et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 11, "context": "The last word encoder we consider is a convolutional neural network, inspired by a similar approach in language modelling (Kim et al., 2016).", "startOffset": 122, "endOffset": 140}, {"referenceID": 20, "context": "In order to capture interactions between the character n-grams obtained by the filters, a highway network (Srivastava et al., 2015) is applied after the max pooling layer, ew = t MLP(rw) + (1 \u2212 t) rw, where t = MLP\u03c3(rw) is a sigmoid gating function which modulates between a tanh MLP transformation of the input (left component) and preserving the input as is (right component).", "startOffset": 106, "endOffset": 131}, {"referenceID": 12, "context": "We use parallel bilingual data from Europarl for Estonian-English and Romanian-English (Koehn, 2005), and web-crawled parallel data for Russian-English (Antonova and Misyurev, 2011).", "startOffset": 87, "endOffset": 100}, {"referenceID": 16, "context": "We train the reranker using MERT (Och, 2003) with 100 restarts.", "startOffset": 33, "endOffset": 44}, {"referenceID": 17, "context": "We use BLEU (Papineni et al., 2002) and METEOR9 (Denkowski and Lavie, 2014) to measure the translation quality against the reference.", "startOffset": 12, "endOffset": 35}], "year": 2016, "abstractText": "Dealing with the co mplex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.", "creator": "LaTeX with hyperref package"}}}