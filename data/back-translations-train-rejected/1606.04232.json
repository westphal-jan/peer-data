{"id": "1606.04232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "abstract": "Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner. We use convex optimization methods, based on an objective criterion and feedback from the current performance of the classifier, to efficiently identify informative samples to train on. We propose an algorithm to decompose the optimization problem into smaller per-class problems, which can be solved in parallel. We test our approach on standard classification tasks and demonstrate its effectiveness in decreasing the training set size without compromising performance. We also show that our approach can make the classifier more robust in the presence of label noise and class imbalance.", "histories": [["v1", "Tue, 14 Jun 2016 07:38:13 GMT  (922kb,D)", "http://arxiv.org/abs/1606.04232v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["maya kabkab", "azadeh alavi", "rama chellappa"], "accepted": false, "id": "1606.04232"}, "pdf": {"name": "1606.04232.pdf", "metadata": {"source": "META", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "authors": ["Maya Kabkab", "Azadeh Alavi", "Rama Chellappa"], "emails": [], "sections": [{"heading": null, "text": "Extensive monitored classification algorithms, especially those based on deep Convolutionary Neural Networks (DCNNs), require enormous amounts of state-of-the-art training data to be up to date. Reducing this data requirement would significantly speed up the training process and potentially improve generalization. Motivated by this goal, we are looking at the task of finding concise training subgroups to be presented iteratively to the learner. We are using convex optimization methods based on an objective criterion and feedback from the current performance of the classifier to efficiently identify informative samples to train on. We are proposing an algorithm to break down the optimization problem into smaller problems per class that can be solved in parallel. We are testing our approach to standard classification tasks and demonstrating its effectiveness in reducing the size of the training set without compromising performance. We are also demonstrating that our approach can make the classifier more robust, if available and more robust."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that they are both very complex matters, and this is a very complex matter."}, {"heading": "2 Problem statement", "text": "We assume that we get a fixed classification architecture and a set of labeled training data points: X = SLk = 1 Xk, where Xk = {X1, k, X2, k,..., XNk, k} are the training samples belonging to class k, and L is the number of classes. At any time, we select a subset of Bt-X, so that the classifier (who was previously trained to Bt 1) performs a good generalization performance when trained to Bt. To this end, we formulate a criterion for selecting new training examples that serve the following objectives: (O1) The samples in Bt must be such that the classifier is unsure to classify them (or safe, but wrong in its classification). (O2) Bt should have a balanced selection from all classes. (O3) Bt should be sufficiently diverse. (O4) Bt should be representative of X. We will mathematically formulate each of these objectives in the following sections."}, {"heading": "2.1 Classifier uncertainty and error", "text": "We assume that at the time of instance t, the classifier for each training sample designates Xi, k from class k, the bypt (Xi, k) = [p t 1 (Xi, k), p t 2 (Xi, k),.., p t L (Xi, k), (1), where ptl (X) is interpreted as an estimate of the probability of the classifier that X 2 X belongs to class l and ptl (X) 0 8l and PL l = 1 p t l (X) = 1. To quantify the uncertainty and error of the classifier, we define: ct (Xi, k) = LXl = 1t \u00b7 1 [l = k] + (1 t) \u00b7 ptl (Xi, k) log ptl (Xi, k), (2) where t 2 [0, 1] is a selected parameter."}, {"heading": "2.2 Class balance", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2.3 Subset diversity", "text": "According to (O3), we want to select a diverse subset, i.e. one that does not have too much redundancy. To this end, we assume that we have a distance metric d (\u00b7, \u00b7) so that d (Xi, Xj) represents the distance between the samples Xi and Xj. This may be, for example, the Euclidean distance between Xi and Xj, or the Euclidean distance between their character vectors in a predefined character space. To maximize diversity, we try to maximize the average distance between all selected samples, i.e., we find B like this: 1M2XX2BXX02B d (X, X 0) (4) is maximized, 1 where M is the budget assigned by the water fill algorithm. Let us find N = | X |, the training set size of the class in question: We insert a binary variable s 2 {0, 1}, so that all are si = 2, even if the dimensions are Xi and D = 2."}, {"heading": "2.4 Subset representativeness", "text": "Per (O4) we also want to select a representative subset B, i.e., the unselected samples must be well represented by the set B. For this purpose we try to minimize the average distance between selected and unselected samples. As before, this can be rewritten as asmin s2 {0,1} N1M (N M) (1 s) | Ds, (6), where 1 is the vector of all samples in RN."}, {"heading": "2.5 Joint formulation", "text": "As already mentioned, when the partial problem of allocating budgets to each class is solved, we try to solve the L-independent problems by finding a diverse, representative subset in which the classifier performs poorly. We therefore combine the subset diversity, representativeness and uncertainty criteria. We define the vector c, [c (X1),.., c (XN) | where c (\u00b7) is defined in such a way that other objective functions can be formulated, such as maximizing the minimum distance between selected samples. While we guarantee less redundancy, such objective functions are more difficult to solve. Section 2.1. To make the sets comparable, we normalize D and c so that all their elements are in [0, 1]. We designate the normalized quantities by D and c, respectively optimized. Our objective function is: max s2 {0.1} N1 \u00b7 1 M2 s | D diversity (N)."}, {"heading": "2.6 Proposed solution", "text": "The problem in (8) is not convex for two reasons: (i) the set {0, 1} N is finite and therefore not convex, and (ii) the objective function is generally not convex. We change the constraint 1 | s = M to its equivalent (1 | s M) 2 = 0 (since this guarantees a zero duality gap [17]) and change the variable x = 2s 1, where x 2 {1,1} Nx | Ax + b | s s s s s s s s s t. (1 | 2M + N) 2 = 0. (10) This problem is known as forced binary programming and is NP-hard [17]."}, {"heading": "3 Experiments", "text": "In this section, we will test the proposed method on several real-world classification tasks. We will look at numerical and facial recognition problems. We will compare our approach with the random selection of training samples used in ordinary training algorithms. Our formulation is not based on a specific classification structure. However, we will illustrate our results on deep neural networks as they are state-of-the-art. We will use the Caffe framework [22] for the implementation of conventional neural networks (CNN) and the SDPA framework [23] to solve the SDP problem in (12). We will also calculate the distances between samples based on local binary pattern (LBP) characteristics. For each of these experiments, unless otherwise specified, we will assume a randomly initialized CNN with a fixed architecture. First, we will test this CNN on the entire pool of training examples to obtain the initial average uncertainty levels."}, {"heading": "3.1 MNIST digit recognition", "text": "For the problem of digit recognition on the known MNIST dataset, we use the LeNet architecture [28]. We conduct our experiments with a randomly selected subset of the MNIST dataset, which consists of 1000 images from each class. We use a total budget of 50 training images per loop of our algorithm (see Figure 2)."}, {"heading": "3.1.1 Diversity vs. representativeness", "text": "First, we illustrate the effect of the weights 1, 2 defined in (7) on the selection process. We set 3 = 0. Figure 3 shows the selected samples at 2 = 20 1 (top) and 2 = 1 (bottom). If 2 is large, more representative samples are selected, as in Figure 3, above. At 1 = 2, more diverse samples are selected, confirming our original objective formulation in (7)."}, {"heading": "3.1.2 Clean labels", "text": "We compare our method of adaptive selection of training lots with the baseline of random selection. As discussed in [11], we first introduce \"simpler\" samples and gradually increase the difficulty. We achieve this by holding 1 tight and starting with 2 = 10 1 and 3 = 0. Selecting a large 2 puts more emphasis on the notion of representativeness in (7), thus ensuring that outliers are not selected. We gradually decrease 2 and increase 3 to allow for more difficult samples. We present our results in Figure 4 (A). Our approach outperforms random selection by a fraction of 4%. Furthermore, the number of samples required by our proposed method to achieve a target performance level is much lower than random samples. For example, for a target accuracy of 94%, approximately 700 random samples are required, as opposed to less than 350 random samples for our approach. In order to assess the quality of the local minimum reached, the random samples that are achieved by all of us exercising on our network at the same time, we are using one of the selected samples."}, {"heading": "3.1.3 Noisy labels", "text": "The results are shown in Figures 4 (B) and 4 (C), respectively. It turns out that our approach exceeds random selection by more than 5%. To combat label noise, we reduce the diversity weight1 and take a more \"cautious\" approach by increasing 3 at a slower pace, resulting in a slower but safer update of the network. In fact, the total number of noise images selected by our algorithm in the case of 20% label noise is 93 images in the twelfth loop (or 6.5% of the selected images), while the random sample appears to take about 20% of noise samples."}, {"heading": "3.1.4 Data imbalance", "text": "Finally, we test our method against a scenario where there is a significant data imbalance between different classes, which can happen when collecting marked data is much more difficult for some classes than for others. We artificially introduce a data imbalance by randomly selecting 4 classes and reducing their training size to 10 to 20 images per class. Our approach achieves a test accuracy of 90.14% after only 9 rounds of the algorithm (i.e. 450 selected samples), while the network can revisit certain training samples if required, which is especially critical in the case of data imbalances, as random selection has a very low probability of selecting images from the classes sampled below. Figure 4 (D) shows the number of classification errors made by a CNN using our algorithm, and significantly compared to 6 classes randomly."}, {"heading": "3.2 VGG Face dataset", "text": "For the problem of facial recognition, we decided to start with a pre-trained CNN to illustrate the use of our transfer learning algorithm to fine-tune the sampling strategy. Using the methods and network described in [29], a CNN was pre-trained on the CASIA WebFace dataset [30]. Instead of random initialization, we start with the pre-trained weights for the first 15 layers (up to the fifth pooling layer) and add two randomly connected layers connected by a dropout layer. We train and test on the VGG Face dataset [31]. Since CASIA-WebFace and VGG Face have significant overlaps, we select 20 of the non-overlapping subjects. The VGG Face dataset consists of a large number of images, part of which has been selected as part of the final curated set."}, {"heading": "4 Conclusion", "text": "In this thesis, we addressed the problem of reducing the training data requirement of deep neural networks. We proposed an efficient iterative and adaptive algorithm based on convex optimization, demonstrated its effectiveness on real datasets and demonstrated its robustness to identify noise and class imbalances."}, {"heading": "Acknowledgments", "text": "This research is based on work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through IARPA R & D Contract No. 2014-14071600012. The views and conclusions contained therein are those of the authors and should not necessarily be interpreted to represent the official guidelines or recommendations of the ODNI, IARPA or the US Government."}], "references": [{"title": "A recipe for semidefinite relaxation for (0", "author": ["S. Poljak", "F. Rendl", "H. Wolkowicz"], "venue": "1)-quadratic programming,\u201d Journal of Global Optimization, vol. 7, no. 1, pp. 51\u201373", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Quadratic optimization problems,", "author": ["N. Shor"], "venue": "Soviet Journal of Computer and and Systems Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Conic and robust optimization,", "author": ["A. Ben-Tal"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "and Y", "author": ["X. Zheng", "X. Sun", "D. Li"], "venue": "Xia, \u201cDuality gap estimation of linear equality constrained binary quadratic programming,\u201d Mathematics of Operations Research, vol. 35, no. 4, pp. 864\u2013880", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 1, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 2, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 3, "context": "One simple approach [4] repeatedly presents the same example if the network error exceeds a threshold.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "In [5], this problem is addressed in the context of feedforward neural networks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "where t 2 [0, 1] is a chosen parameter.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "To make the quantities comparable, we normalize D and c such that all their elements lie in [0, 1].", "startOffset": 92, "endOffset": 98}], "year": 2016, "abstractText": "Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner. We use convex optimization methods, based on an objective criterion and feedback from the current performance of the classifier, to efficiently identify informative samples to train on. We propose an algorithm to decompose the optimization problem into smaller per-class problems, which can be solved in parallel. We test our approach on standard classification tasks and demonstrate its effectiveness in decreasing the training set size without compromising performance. We also show that our approach can make the classifier more robust in the presence of label noise and class imbalance.", "creator": "LaTeX with hyperref package"}}}