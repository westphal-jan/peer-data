{"id": "1606.04275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Efficient Pairwise Learning Using Kernel Ridge Regression: an Exact Two-Step Method", "abstract": "Pairwise learning or dyadic prediction concerns the prediction of properties for pairs of objects. It can be seen as an umbrella covering various machine learning problems such as matrix completion, collaborative filtering, multi-task learning, transfer learning, network prediction and zero-shot learning. In this work we analyze kernel-based methods for pairwise learning, with a particular focus on a recently-suggested two-step method. We show that this method offers an appealing alternative for commonly-applied Kronecker-based methods that model dyads by means of pairwise feature representations and pairwise kernels. In a series of theoretical results, we establish correspondences between the two types of methods in terms of linear algebra and spectral filtering, and we analyze their statistical consistency. In addition, the two-step method allows us to establish novel algorithmic shortcuts for efficient training and validation on very large datasets. Putting those properties together, we believe that this simple, yet powerful method can become a standard tool for many problems. Extensive experimental results for a range of practical settings are reported.", "histories": [["v1", "Tue, 14 Jun 2016 09:38:18 GMT  (826kb,D)", "http://arxiv.org/abs/1606.04275v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michiel stock", "tapio pahikkala", "antti airola", "bernard de baets", "willem waegeman"], "accepted": false, "id": "1606.04275"}, "pdf": {"name": "1606.04275.pdf", "metadata": {"source": "CRF", "title": "Efficient Pairwise Learning Using Kernel Ridge Regression: an Exact Two-Step Method", "authors": ["Michiel Stock", "Tapio Pahikkala", "Antti Airola", "Bernard De Baets", "Willem Waegeman"], "emails": ["michiel.stock@ugent.be", "aatapa@utu.fi", "ajairo@utu.fi", "bernard.debaets@ugent.be", "willem.waegeman@ugent.be"], "sections": [{"heading": null, "text": "Keywords: dyadic prediction, pair learning, transfer learning, kernel ridge regression, kernel methods, zero shot learning"}, {"heading": "1. Introduction to pairwise prediction", "text": "In contrast to more traditional learning environments, the goal here is to make predictions for pairs of objects, each of which is characterized by a characteristic representation. Applications of this kind occur in biology (e.g. predicting mRNA-miRNA interactions), medicine (e.g. predicting host-parasite interactions), social network analysis (e.g. predicting linkages between people) and recommendation systems (e.g. recommending personalized products to users), chemistry (e.g. predicting the binding between two types of molecules), ecology (e.g. predicting host-parasite interactions), social network analysis (e.g. finding linkages between people) and recommendation systems). Pairwise Learning has strong connections to many other machine learning environments. In particular, linking with multi-task learning task \"can be the first instance of setting and object."}, {"heading": "2. The different prediction settings in pairwise learning", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "3. Pairwise learning using ridge-regression-based methods", "text": "This section gives an overview of the core methods for pair learning, with particular emphasis on the two-step method, which will be further analyzed in the coming sections. To this end, we want to expand the previously introduced mathematical notation further. Let's call D = {di | i = 1,..., m} and T = {tj | j = 1,.., q} the sets of different instances and tasks encountered in the training set with m = | D | and q = | T |. We say that the training set is complete if it contains each instance pair of tasks with instance in D and task in T. For complete training sets, we introduce another notation for the matrix of labels Y = Rm \u00d7 q, so that its lines are indexed by the instances in D and the columns by the tasks in T. The prevention function is called f (d, t), with the model for task tj."}, {"heading": "3.1 Independent-task kernel ridge regression", "text": "Suppose the training set is complete and for each task we have m labeled dyads D = {di} mi = 1. Since for each task a separate and independent model is trained, we call this setting independent task (IT) kernel comb regression. In the case of kernel comb regression (KRR) we want a function of the format ITj (d) = m \u2211 i = 1 aITij k (d, di), with aITij parameters that minimize a suitable objective function. In the case of kernel comb regression (KRR), this objective function is the square loss with an L2 complexity penalty. The parameters for the individual tasks using KRR can be found together by minimizing the following objective function (Bischof, 2006): J (AIT) = tr complexity penalty."}, {"heading": "3.2 Pairwise and Kronecker kernel ridge regression", "text": "Suppose you have a prior knowledge of what tasks are of a similar nature, quantified by a core function g = q = 4, \u00b7) of the tasks defined (see Alvarez et al. (2012); Baldassarre et al. (2012) and references to it have extended the task correlations over matrix-weighted kernels (2012). (Most of this literature, however, concerns kernels for which tasks are specified during the training period.) An alternative approach that allows generalization of new tasks is the use of a paired kernel (d, t), (d, t). (Pairwise kernels provide a predictive function of typef (d, t) = n (d, t). (d, t)."}, {"heading": "3.3 Two-step kernel ridge regression", "text": "It is therefore not possible to develop a model for new tasks that prevents us from new tasks that can be predicted indirectly for new tasks. Formally, one wants to make a prediction for the dyad (d, t). Let k, Rm denote the vector of the instance between instances and an instance that contains the instance between instances and an instance in the test set, i.e. k, k, d, d1)."}, {"heading": "4. Theoretical considerations", "text": "In this section we will show that the two-stage kernel burr regression can be considered a kernel burr regression with special types of paired kernel matrices, depending on the predictive setting. We will show that Setting A is a transductive setting, while Setting D is only a special case of the (Kronecker) kernel burr regression. Furthermore, we will examine the different learning algorithms from a spectral filter perspective and show that the two-stage kernel burr regression uses a special decompressible filter. From these observations, we will prove the universality and permissibility of the methods."}, {"heading": "4.1 Equivalence between two-step and other kernel ridge regression methods", "text": "The relationship between the two steps KKR () follows from Eq. (11) thatfTSj (d) = [k > (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 3). (K + 3). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 3). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2). (K + 2)."}, {"heading": "4.2 Universality of the Kronecker product pairwise kernel", "text": "This is a necessary step to demonstrate the consistency of this method. (i.e.) Let us first remember the concept of universal core functions. (i.e.) Definition 1 (Steinwart, 2002) A continuous kernel k (\u00b7) on a compact metric space X (i.e. X) is the space of all continuous functions. (i.e.) The universality property indicates that the hypotheses space induced by a universal kernel can be approximated to any continuous function. (i.e.) X (X) is the space of all continuous functions f: X \u2192 R.H property indicates that the hypotheses space can be approximated by a universal kernel. (Steinwart, 2002) In other words, the learning algorithms are very easy to learn as the available set of training data is large and representative enough, and the learning algorithms can efficiently find this approximation from the hypotheses space (Steinwart, 2002)."}, {"heading": "4.3 Spectral interpretation", "text": "In this section we will examine the difference between single task, Kronecker and two-step system regression from the point of view of spectral regularization. The universal appropriation properties of this kernel described above are also related to the consistency properties of the two-step KRR, as elaborated in more detail in this section. Learning through spectral regularization comes from the theory of emerging problems. This paradigm is also present in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning - e.g. Lo Gerfo et al. (2008) Here we want to find the parameters of the data generation process, which thatured a series of noisy measurements y, (19) with a set of individual values and a set of individuals."}, {"heading": "5. Efficient algorithms for two-step kernel ridge regression", "text": "In this section, we derive some computational abbreviations for the two-step kernel ridge regression. For the Kronecker kernel ridge regression, it is known that the huge system of Eq. (5) can be efficiently solved because the Gram matrix can be decomposed. Our two-step method takes this decompression a step further, as the model can be considered an application of two successive regression steps. Thus, efficient algorithms for cross-validation can be derived for each of the four settings shown in Figure 1, while the original Kronecker kernel ridge regression only allows an abbreviation for Setting A. The same linear algebra can also be used to implement a scheme for the online update of the model with new instances or tasks. These cross-validation abbreviations for Settings B, C and D and the online update cannot be derived for the Kronecker ridge regression in general."}, {"heading": "5.1 Efficient hold-out computation", "text": "It's very well known that it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way, and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and a way and it's about a way and it's about a way and it's about a way and a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it"}, {"heading": "5.2 Learning with mini batches", "text": "In some cases, it is desirable to train a model online, as opposed to standard batch training. For example, if new instances are available for the various tasks, it is more desirable to update the model parameters rather than to completely retrain the model. Similarly, new tasks could be added to the model that also affect the performance of the older tasks. This is common in online applications, such as systems where the data is added dynamically so that systems should be updated quickly. In connection with the handling of the required data, the required matrices may simply be too large to fit into the main memory as a whole. In addition to gradient-based methods such as stochastic gradient descent and conjugated gradients, solutions for updating the model parameters can be derived. As we assume this is of particular interest for large-scale data applications, we deduce a shortening for the basic functions."}, {"heading": "6. Experiments", "text": "The experiments illustrate the efficient algorithms for training and evaluating the two-stage KRR model, as opposed to the more limited toolkit for the Kronecker KRR model. \u2022 To be more precise: Algorithm 2 Update primary parameter WTSold of the two-stage KRR model with batches l of new tasks, where l < q requires new task features and labels in addition to the old weight matrix, the algorithm requires precompressed matrices Nold = (1 > old + new tasks, assuming l < q)."}, {"heading": "6.1 Study of regularization for the different settings", "text": "In this context, we have examined the influences of the regulatory parameters \u03bb, \u03bbd and \u03bbt, of which we have the similarity with other countries. In this context, we have also developed a series of approaches that relate to the different areas of the economy. (...) To this end, we have developed four different approaches that relate to the different areas of the economy. (...) \"We have the same approaches and approaches that relate to the individual areas. (...)\" \"We have the same approaches.\" (...) \"We have the same approaches.\" (...) \"We.\" (...) \"We have the same approaches.\" (...) \"We have the same approaches.\" (...) \"(...\" We have. \"(...)\" We have. \"(...)\" We have. \"(...)\" We have the same approaches. \"(...)\" We have the same. \"(...). (...\" We have the same. \"(...).\" We have the same. (...) \"We have the same.\" (... \"We have the same approaches.\" (...). \"We have the same.\" (... \"We have the same.\" (). () We have the same. (). () We have the same. (). (). (We. () We have the same. (). (We have the same. (). (). (We. () We have the same. (). (). (We have the same. (). (We have the same. (). (We have the same. (). (We have the same.). (). (We have the same. (). (We have the same.). (We have the same. () We have the same. (). (). (We have the same. (). (We have the same.). (). (We have the same. (We have the same.). (). (We have the same. (). (). (We have the same. (). (). (We have the same. (We have the same. (). (). (We have the same.). (We have the same. (). (). (We have the same"}, {"heading": "6.2 Comparison of different transfer learning settings", "text": "This year, it will be able to drown the aforementioned lcihsrcnlrVo in order to drown them."}, {"heading": "6.3 Online-learning of hierarchical text classification", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "7. Conclusions and perspectives", "text": "We have shown that these methods are very similar: Kronecker KRR is a special case of ordinary kernel KRR regression, two-step KRR is a special case of (Kronecker) KRR interpretation for all these methods. Two-step KRR interpretation, which works very well both theoretically and experimentally, has additional component-specific advantages. Since modelling can be conceptually divided into two independent steps, efficient novel hold-out tricks and algorithms for online learning can be achieved."}, {"heading": "Acknowledgements", "text": "Part of this work was carried out with the help of the Stevin supercomputer infrastructure of the University of Ghent, financed by the University of Ghent, the Hercules Foundation and the Flemish Government - EMI Department."}], "references": [{"title": "A new approach to collaborative filtering: operator estimation with spectral regularization", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Incorporating side information in probabilistic matrix factorization with Gaussian processes", "author": ["R.P. Adams", "G.E. Dahl", "I. Murray"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Kernels for vector-valued functions: a review", "author": ["M.A. Alvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Alvarez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alvarez et al\\.", "year": 2012}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Proceedings of the 21st Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "On spectral learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Massimiliano"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Argyriou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2010}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning,", "citeRegEx": "Baldassarre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2012}, {"title": "A joint framework for collaborative and content filtering", "author": ["J. Basilico", "T. Hofmann"], "venue": "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Basilico and Hofmann.,? \\Q2004\\E", "shortCiteRegEx": "Basilico and Hofmann.", "year": 2004}, {"title": "On regularization algorithms in learning theory", "author": ["F. Bauer", "S. Pereverzev", "L. Rosasco"], "venue": "Journal of Complexity,", "citeRegEx": "Bauer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bauer et al\\.", "year": 2007}, {"title": "Kernel methods for predicting protein-protein interactions", "author": ["A. Ben-Hur", "W. Noble"], "venue": "Suppl 1:38\u201346,", "citeRegEx": "Ben.Hur and Noble.,? \\Q2005\\E", "shortCiteRegEx": "Ben.Hur and Noble.", "year": 2005}, {"title": "Introduction to Inverse Problems in Imaging", "author": ["M. Bertero", "P. Boccacci"], "venue": "CRC Press,", "citeRegEx": "Bertero and Boccacci.,? \\Q1998\\E", "shortCiteRegEx": "Bertero and Boccacci.", "year": 1998}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Kernel multi-task learning using task-specific features", "author": ["E.V. Bonilla", "F.V. Agakov", "C.K.I. Williams"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Pairwise support vector machines and their application to large scale problems", "author": ["C. Brunner", "A. Fischer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brunner and Fischer.,? \\Q2012\\E", "shortCiteRegEx": "Brunner and Fischer.", "year": 2012}, {"title": "Exact low-rank matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Candes and Recht.,? \\Q2008\\E", "shortCiteRegEx": "Candes and Recht.", "year": 2008}, {"title": "Comprehensive analysis of kinase inhibitor selectivity", "author": ["M.I. Davis", "J.P. Hunt", "S. Herrgard", "P. Ciceri", "L.M. Wodicka", "G. Pallares", "M. Hocker", "D.K. Treiber", "P.P. Zarrinkar"], "venue": "Nature Biotechnology,", "citeRegEx": "Davis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2011}, {"title": "Data-driven recipe completion using machine learning methods", "author": ["M. De Clercq", "M. Stock", "B. De Baets", "W. Waegeman"], "venue": "Trends in Food Science & Technology,", "citeRegEx": "Clercq et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clercq et al\\.", "year": 2015}, {"title": "On label dependence and loss minimization in multi-label classification", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Matrix co-factorization for recommendation with rich side information and implicit feedback", "author": ["Y. Fang", "L. Si"], "venue": "In The 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems,", "citeRegEx": "Fang and Si.,? \\Q2011\\E", "shortCiteRegEx": "Fang and Si.", "year": 2011}, {"title": "Concordance probability and discriminatory power in proportional hazards", "author": ["M. G\u00f6nen", "G. Heller"], "venue": "regression. Biometrika,", "citeRegEx": "G\u00f6nen and Heller.,? \\Q2005\\E", "shortCiteRegEx": "G\u00f6nen and Heller.", "year": 2005}, {"title": "A tale of two phylogenies: comparative analyses of ecological interactions", "author": ["J.D. Hadfield", "B.R. Krasnov", "R. Poulin", "S. Nakagawa"], "venue": "The American Naturalist,", "citeRegEx": "Hadfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hadfield et al\\.", "year": 2014}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Self-measuring similarity for multi-task Gaussian processes", "author": ["K. Hayashi", "T. Takenouchi", "R. Tomioka", "H. Kashima"], "venue": "Transactions of the Japanese Society for Artificial Intelligence,", "citeRegEx": "Hayashi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hayashi et al\\.", "year": 2012}, {"title": "Protein-ligand interaction prediction: an improved chemogenomics", "author": ["L. Jacob", "J.-P. Vert"], "venue": "approach. Bioinformatics,", "citeRegEx": "Jacob and Vert.,? \\Q2008\\E", "shortCiteRegEx": "Jacob and Vert.", "year": 2008}, {"title": "Zero shot recognition with unreliable attributes", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jayaraman and Grauman.,? \\Q2014\\E", "shortCiteRegEx": "Jayaraman and Grauman.", "year": 2014}, {"title": "Graph-based semi-supervised learning and spectral kernel design", "author": ["R. Johnson", "T. Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Johnson and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2008}, {"title": "Link propagation: a fast semi-supervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "In Proceedings of the SIAM International Conference on Data Mining,", "citeRegEx": "Kashima et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2009}, {"title": "Cartesian kernel: an efficient alternative to the pairwise kernel", "author": ["H. Kashima", "S. Oyama", "Y. Yamanishi", "K. Tsuda"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Kashima et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2010}, {"title": "Attribute-based classification for zeroshot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lampert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2014}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "In Proceedings of the 23rd National Conference on Artificial Intelligence,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Bipartite edge prediction via transductive learning over product graphs", "author": ["H. Liu", "Y. Yang"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Liu and Yang.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Yang.", "year": 2015}, {"title": "Spectral algorithms for supervised learning", "author": ["L. Lo Gerfo", "L. Rosasco", "F. Odone", "E. De Vito", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "Gerfo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gerfo et al\\.", "year": 2008}, {"title": "Shifted Kronecker product systems", "author": ["C.D. Martin", "C.F. Van Loan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Martin and Loan.,? \\Q2006\\E", "shortCiteRegEx": "Martin and Loan.", "year": 2006}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["R. Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Link prediction via matrix factorization", "author": ["A. Menon", "C. Elkan"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Menon and Elkan.,? \\Q2011\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2011}, {"title": "A log-linear model with latent features for dyadic prediction", "author": ["A.K. Menon", "C. Elkan"], "venue": "In Proceedings of the 10th International Conference on Data Mining,", "citeRegEx": "Menon and Elkan.,? \\Q2010\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2010}, {"title": "Using feature conjunctions across examples for learning pairwise classifiers", "author": ["S. Oyama", "C. Manning"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Oyama and Manning.,? \\Q2004\\E", "shortCiteRegEx": "Oyama and Manning.", "year": 2004}, {"title": "Fast $n$-fold cross-validation for regularized least-squares", "author": ["T. Pahikkala", "J. Boberg", "T. Salakoski"], "venue": "In Proceedings of the Nineth Scandinavian Conference on Artificial Intelligence,", "citeRegEx": "Pahikkala et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2006}, {"title": "Learning intransitive reciprocal relations with kernel methods", "author": ["T. Pahikkala", "W. Waegeman", "E. Tsivtsivadze", "T. Salakoski", "B. De Baets"], "venue": "European Journal of Operational Research,", "citeRegEx": "Pahikkala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2010}, {"title": "Efficient regularized least-squares algorithms for conditional ranking on relational data", "author": ["T. Pahikkala", "A. Airola", "M. Stock", "B. De Baets", "W. Waegeman"], "venue": "Machine Learning,", "citeRegEx": "Pahikkala et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2013}, {"title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "author": ["T. Pahikkala", "M. Stock", "A. Airola", "T. Aittokallio", "B. De Baets", "W. Waegeman"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Pahikkala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2014}, {"title": "Toward more realistic drug-target interaction predictions", "author": ["T. Pahikkala", "A. Airola", "S. Pietila", "S. Shakyawar", "A. Szwajda", "J. Tang", "T. Aittokallio"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "Pahikkala et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2015}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "G. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.-T. Park", "W. Chu"], "venue": "In Proceedings of the Third ACM Conference on Recommender Systems,", "citeRegEx": "Park and Chu.,? \\Q2009\\E", "shortCiteRegEx": "Park and Chu.", "year": 2009}, {"title": "Flaws in evaluation schemes for pair-input computational predictions", "author": ["Y. Park", "E.M. Marcotte"], "venue": "Nature Methods,", "citeRegEx": "Park and Marcotte.,? \\Q2012\\E", "shortCiteRegEx": "Park and Marcotte.", "year": 2012}, {"title": "LSHTC: a benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "rXivpreprint arXiv:1503.08581,", "citeRegEx": "Partalas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Partalas et al\\.", "year": 2015}, {"title": "Affinity regression predicts the recognition code of nucleic acid-binding proteins", "author": ["R. Pelossof", "I. Singh", "J.L. Yang", "M.T. Weirauch", "T.R. Hughes", "C.S. Leslie"], "venue": "Nature Biotechnology,", "citeRegEx": "Pelossof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pelossof et al\\.", "year": 2015}, {"title": "Phylogenetic trait-based analyses of ecological networks. Ecology", "author": ["N.E. Rafferty", "A.R. Ives"], "venue": null, "citeRegEx": "Rafferty and Ives.,? \\Q2013\\E", "shortCiteRegEx": "Rafferty and Ives.", "year": 2013}, {"title": "Fast and scalable algorithms for semi-supervised link prediction on static and dynamic graphs", "author": ["R. Raymond", "H. Kashima"], "venue": "Proceedings of the European Conference on Machine learning and Knowledge Discovery in Databases,", "citeRegEx": "Raymond and Kashima.,? \\Q2010\\E", "shortCiteRegEx": "Raymond and Kashima.", "year": 2010}, {"title": "Value regularization and Fenchel duality", "author": ["R. Rifkin", "R. Lippert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Lippert.,? \\Q2007\\E", "shortCiteRegEx": "Rifkin and Lippert.", "year": 2007}, {"title": "Notes on regularized least squares", "author": ["R. Rifkin", "R. Lippert"], "venue": "Technical Report MIT-CSAILTR-2007-025, Massachusetts Institute of Technology,", "citeRegEx": "Rifkin and Lippert.,? \\Q2007\\E", "shortCiteRegEx": "Rifkin and Lippert.", "year": 2007}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Rohrbach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["B. Romera-Paredes", "P. Torr"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Romera.Paredes and Torr.,? \\Q2015\\E", "shortCiteRegEx": "Romera.Paredes and Torr.", "year": 2015}, {"title": "On protocols and measures for the validation of supervised methods for the inference of biological networks", "author": ["M. Schrynemackers", "R. K\u00fcffner", "P. Geurts"], "venue": "Frontiers in Genetics,", "citeRegEx": "Schrynemackers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schrynemackers et al\\.", "year": 2013}, {"title": "Classifying pairs with trees for supervised biological network inference", "author": ["M. Schrynemackers", "L. Wehenkel", "M.M. Babu", "P. Geurts"], "venue": "Molecular Biosystems,", "citeRegEx": "Schrynemackers et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schrynemackers et al\\.", "year": 2015}, {"title": "Probabilistic matrix factorization for collaborative filtering", "author": ["H. Shan", "A. Banerjee"], "venue": "In Proceedings of the 10th IEEE International Conference on Data Mining,", "citeRegEx": "Shan and Banerjee.,? \\Q2010\\E", "shortCiteRegEx": "Shan and Banerjee.", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Steinwart.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart.", "year": 2002}, {"title": "Identification of functionally related enzymes by learning-torank methods", "author": ["M. Stock", "T. Fober", "E. H\u00fcllermeier", "S. Glinca", "G. Klebe", "T. Pahikkala", "A. Airola", "B. De Baets", "W. Waegeman"], "venue": "IEEE Transactions on Computational Biology and Bioinformatics,", "citeRegEx": "Stock et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stock et al\\.", "year": 2014}, {"title": "The ubiquitous Kronecker product", "author": ["C.F. Van Loan"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Loan.,? \\Q2000\\E", "shortCiteRegEx": "Loan.", "year": 2000}, {"title": "A new pairwise kernel for biological network inference with support vector machines", "author": ["J.-P. Vert", "J. Qiu", "W.S. Noble"], "venue": "BMC Bioinformatics,", "citeRegEx": "Vert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vert et al\\.", "year": 2007}, {"title": "A kernelbased framework for learning graded relations from data", "author": ["W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "Waegeman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Waegeman et al\\.", "year": 2012}, {"title": "Spline Models for Observational Data", "author": ["G. Wahba"], "venue": null, "citeRegEx": "Wahba.,? \\Q1990\\E", "shortCiteRegEx": "Wahba.", "year": 1990}, {"title": "Prediction of drugtarget interaction networks from the integration of chemical and genomic", "author": ["Y. Yamanishi", "M. Araki", "A. Gutteridge", "W. Honda", "M. Kanehisa"], "venue": "spaces. Bioinformatics,", "citeRegEx": "Yamanishi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2008}, {"title": "Alternating least-squares for low-rank matrix reconstruction", "author": ["D. Zachariah", "M. Sundin"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Zachariah and Sundin.,? \\Q2012\\E", "shortCiteRegEx": "Zachariah and Sundin.", "year": 2012}, {"title": "Kernelized probabilistic matrix factorization: exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 40, "context": "We first presented this method in a conference paper (Pahikkala et al., 2014).", "startOffset": 53, "endOffset": 77}, {"referenceID": 54, "context": "From a different but related perspective, Schrynemackers et al. (2015) recently also proposed a similar method for biological network inference, but here tree-based methods instead of kernel methods were used as base learners.", "startOffset": 42, "endOffset": 71}, {"referenceID": 45, "context": "For example, in a large-scale meta-study about biological network identification it was found that these concepts are vital to correctly evaluate pairwise learning models (Park and Marcotte, 2012).", "startOffset": 171, "endOffset": 196}, {"referenceID": 13, "context": "Many of these matrix completion algorithms do not incorporate side features (features of the instances and tasks) and make assumptions on the structure of the true label matrix by, for example, assuming that the completed matrix is low rank or has a low nuclear norm (Candes and Recht, 2008; Mazumder et al., 2010).", "startOffset": 267, "endOffset": 314}, {"referenceID": 33, "context": "Many of these matrix completion algorithms do not incorporate side features (features of the instances and tasks) and make assumptions on the structure of the true label matrix by, for example, assuming that the completed matrix is low rank or has a low nuclear norm (Candes and Recht, 2008; Mazumder et al., 2010).", "startOffset": 267, "endOffset": 314}, {"referenceID": 30, "context": "Recently, a framework based on bipartite graphs was proposed, which exploits the network structure for transductive link prediction (Liu and Yang, 2015).", "startOffset": 132, "endOffset": 152}, {"referenceID": 28, "context": "Larochelle et al. (2008) for a review.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Basilico and Hofmann (2004); Abernethy et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 0, "context": "Basilico and Hofmann (2004); Abernethy et al. (2008); Menon and Elkan (2011).", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": "Basilico and Hofmann (2004); Abernethy et al. (2008); Menon and Elkan (2011).", "startOffset": 29, "endOffset": 77}, {"referenceID": 16, "context": "Here, most techniques encode dependency in the tasks by means of a suitable loss function or by jointly regularizing the different tasks (Dembczynski et al., 2012).", "startOffset": 137, "endOffset": 163}, {"referenceID": 43, "context": "Here as well, a large number of applicable methods is available in the literature (Pan and Yang, 2010).", "startOffset": 82, "endOffset": 102}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 90}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al. (2012) for a non-exhaustive list.", "startOffset": 0, "endOffset": 110}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al. (2012) for a non-exhaustive list. From a bioinformatics viewpoint, Settings B and C are often analyzed using graph-based methods that take the structure of a biological network into account \u2013 see e.g. Schrynemackers et al. (2013) for a recent review.", "startOffset": 0, "endOffset": 333}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 46}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 70}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al.", "startOffset": 0, "endOffset": 95}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al. (2013); Rohrbach et al.", "startOffset": 0, "endOffset": 120}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al. (2013); Rohrbach et al. (2011)).", "startOffset": 0, "endOffset": 144}, {"referenceID": 60, "context": "For Setting D Kronecker-based kernel methods are often employed (Vert et al., 2007; Brunner and Fischer, 2012).", "startOffset": 64, "endOffset": 110}, {"referenceID": 12, "context": "For Setting D Kronecker-based kernel methods are often employed (Vert et al., 2007; Brunner and Fischer, 2012).", "startOffset": 64, "endOffset": 110}, {"referenceID": 6, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 44, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 58, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al., 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 8, "context": ", 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al., 2009) or protein-nucleic acid (Pelossof et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 26, "context": ", 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al., 2009) or protein-nucleic acid (Pelossof et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 47, "context": ", 2009) or protein-nucleic acid (Pelossof et al., 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 23, "context": ", 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 38, "context": ", 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al., 2010) and document retrieval (Pahikkala et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 39, "context": ", 2010) and document retrieval (Pahikkala et al., 2013).", "startOffset": 31, "endOffset": 55}, {"referenceID": 44, "context": "Efficient optimization approaches based on gradient descent (Park and Chu, 2009; Kashima et al., 2009) and closed-form solutions (Pahikkala et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 26, "context": "Efficient optimization approaches based on gradient descent (Park and Chu, 2009; Kashima et al., 2009) and closed-form solutions (Pahikkala et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 39, "context": ", 2009) and closed-form solutions (Pahikkala et al., 2013) have been proposed.", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "The parameters for the individual tasks using KRR can be found jointly by minimizing the following objective function (Bishop, 2006): J(A) = tr[(KA \u2212Y)>(KAIT \u2212Y)] + \u03bbdtr[A > KA] , (1) with AIT = [aIT ij ] \u2208 Rm\u00d7q and K \u2208 Rm\u00d7m the Gram matrix associated with the kernel function k(\u00b7, \u00b7) for the instances.", "startOffset": 118, "endOffset": 132}, {"referenceID": 2, "context": "Several authors (see Alvarez et al. (2012); Baldassarre et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "Several authors (see Alvarez et al. (2012); Baldassarre et al. (2012) and references therein) have extended KRR to incorporate task correlations via matrix-valued kernels.", "startOffset": 21, "endOffset": 70}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 36, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 8, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 44, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 22, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 11, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 39, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 27, "context": "(Kashima et al., 2010).", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al.", "startOffset": 81, "endOffset": 543}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al.", "startOffset": 81, "endOffset": 568}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al. (2012); Pahikkala et al.", "startOffset": 81, "endOffset": 592}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al. (2012); Pahikkala et al. (2013)) or for more efficient calculations in certain settings, e.", "startOffset": 81, "endOffset": 617}, {"referenceID": 21, "context": "In the statistical literature H\u0393 = \u0393 (\u0393 + \u03bbI)\u22121 is denoted as the so-called hat matrix (Hastie et al., 2001), which transforms the measurements into estimates.", "startOffset": 87, "endOffset": 108}, {"referenceID": 26, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 49, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 39, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 2, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 44, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 26, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 39, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 64, "context": "Superficially, this approach resembles alternating least-squares (Zachariah and Sundin, 2012), though the latter is an iteratively trained model mainly used for Setting A to obtain a low-rank representation.", "startOffset": 65, "endOffset": 93}, {"referenceID": 49, "context": "See Rifkin and Lippert (2007a); Johnson and Zhang (2008) for a more in-depth discussion.", "startOffset": 4, "endOffset": 31}, {"referenceID": 25, "context": "See Rifkin and Lippert (2007a); Johnson and Zhang (2008) for a more in-depth discussion.", "startOffset": 32, "endOffset": 57}, {"referenceID": 57, "context": "Definition 1 (Steinwart, 2002) A continuous kernel k(\u00b7, \u00b7) on a compact metric space X (i.", "startOffset": 13, "endOffset": 30}, {"referenceID": 57, "context": "The universality property indicates that the hypothesis space induced by a universal kernel can approximate any continuous function on the input space X to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find this approximation from the hypothesis space (Steinwart, 2002).", "startOffset": 360, "endOffset": 377}, {"referenceID": 57, "context": "Definition 1 (Steinwart, 2002) A continuous kernel k(\u00b7, \u00b7) on a compact metric space X (i.e. X is closed and bounded) is called universal if the reproducing kernel Hilbert space (RKHS) induced by k(\u00b7, \u00b7) is dense in C(X ), where C(X ) is the space of all continuous functions f : X \u2192 R. The universality property indicates that the hypothesis space induced by a universal kernel can approximate any continuous function on the input space X to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find this approximation from the hypothesis space (Steinwart, 2002). In other words, the learning algorithm is consistent in the sense that, informally put, the hypothesis learned by it gets closer to the function to be learned while the size of the training set gets larger. The consistency properties of two-step KRR are considered in more detail in Section 4.3. Next, we consider the universality of the Kronecker product pairwise kernel. The following result is a straightforward modification of some of the existing results in the literature (e.g. Waegeman et al. (2012)), but we present it here for self-sufficiency.", "startOffset": 14, "endOffset": 1173}, {"referenceID": 9, "context": "This paradigm is well studied in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning \u2013 e.", "startOffset": 64, "endOffset": 92}, {"referenceID": 9, "context": "This paradigm is well studied in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning \u2013 e.g. Lo Gerfo et al. (2008). Here, one wants to find the parameters \u03b1 of the data-generating process given a set of noisy measurements y such that", "startOffset": 65, "endOffset": 163}, {"referenceID": 31, "context": "Lo Gerfo et al. (2008) and references therein).", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "Argyriou et al. (2007, 2010); Baldassarre et al. (2012). We will translate the pairwise learning methods from Section 3 to this spectral regularization context.", "startOffset": 0, "endOffset": 56}, {"referenceID": 6, "context": "Following Bauer et al. (2007); Lo Gerfo et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 6, "context": "Following Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al.", "startOffset": 10, "endOffset": 54}, {"referenceID": 5, "context": "(2008); Baldassarre et al. (2012), we characterize the quality of a learning algorithm via its consistency properties.", "startOffset": 8, "endOffset": 34}, {"referenceID": 6, "context": "For the exact details and further elaboration, we refer to Bauer et al. (2007); Lo Gerfo et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 6, "context": "For the exact details and further elaboration, we refer to Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al.", "startOffset": 59, "endOffset": 103}, {"referenceID": 5, "context": "(2008); Baldassarre et al. (2012). Theorem 6 If the filter function is admissible and the kernel function is universal, then the learning algorithm is consistent in the sense of Def.", "startOffset": 8, "endOffset": 34}, {"referenceID": 45, "context": "(Park and Marcotte, 2012; Pahikkala et al., 2015).", "startOffset": 0, "endOffset": 49}, {"referenceID": 41, "context": "(Park and Marcotte, 2012; Pahikkala et al., 2015).", "startOffset": 0, "endOffset": 49}, {"referenceID": 55, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al.", "startOffset": 99, "endOffset": 112}, {"referenceID": 37, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al. (2006); Rifkin and Lippert (2007b).", "startOffset": 113, "endOffset": 137}, {"referenceID": 37, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al. (2006); Rifkin and Lippert (2007b).", "startOffset": 113, "endOffset": 165}, {"referenceID": 20, "context": "(2013); Hadfield et al. (2014) for applications of pairwise learning in such a setting).", "startOffset": 8, "endOffset": 31}, {"referenceID": 20, "context": "(2013); Hadfield et al. (2014) for applications of pairwise learning in such a setting). Such datasets are often plagued with false negatives or false positives, which make them difficult to analyze, see Schrynemackers et al. (2013) for a discussion.", "startOffset": 8, "endOffset": 233}, {"referenceID": 17, "context": "These can either be the primal features or be obtained from a decomposition of the kernel matrices, for example by means of the Nystr\u00f6m method (Drineas and Mahoney, 2005).", "startOffset": 143, "endOffset": 170}, {"referenceID": 10, "context": "where we made use of the matrix inversion lemma (Bishop (2006), Eq.", "startOffset": 49, "endOffset": 63}, {"referenceID": 28, "context": "We refer to Romera-Paredes and Torr (2015) for some experimental results which show that two-step KRR is a competitive method compared to established zero-shot learning methods, including DAP (Lampert et al., 2014) and ZSRwUA (Jayaraman and Grauman, 2014), for some zero-shot learning benchmark datasets.", "startOffset": 192, "endOffset": 214}, {"referenceID": 24, "context": ", 2014) and ZSRwUA (Jayaraman and Grauman, 2014), for some zero-shot learning benchmark datasets.", "startOffset": 19, "endOffset": 48}, {"referenceID": 51, "context": "We refer to Romera-Paredes and Torr (2015) for some experimental results which show that two-step KRR is a competitive method compared to established zero-shot learning methods, including DAP (Lampert et al.", "startOffset": 12, "endOffset": 43}, {"referenceID": 63, "context": "To this end, we use four drug-target classification datasets collected by Yamanishi et al. (2008)1.", "startOffset": 74, "endOffset": 98}, {"referenceID": 10, "context": "By using this relabeling, minimizing the squared loss becomes equivalent with Fisher discriminant analysis (Bishop, 2006), making our method more suitable for classification.", "startOffset": 107, "endOffset": 121}, {"referenceID": 14, "context": "We also used a different drug-target interaction prediction dataset2 (Davis et al., 2011; Pahikkala et al., 2015) consisting of 68 drug compounds and 442 protein targets.", "startOffset": 69, "endOffset": 113}, {"referenceID": 41, "context": "We also used a different drug-target interaction prediction dataset2 (Davis et al., 2011; Pahikkala et al., 2015) consisting of 68 drug compounds and 442 protein targets.", "startOffset": 69, "endOffset": 113}, {"referenceID": 19, "context": "The performances are averages over all repetitions and over all target tasks, and are measured using the concordance index (G\u00f6nen and Heller, 2005) (C-index), also known as the pairwise ranking accuracy:", "startOffset": 123, "endOffset": 147}, {"referenceID": 54, "context": "Previously, Schrynemackers et al. (2013) have, in their overview article on dyadic prediction in the biological domain, made the observation that in terms of predictive accuracy there does not seem to be a clear winner between the independent-task and multi-task type of learning approaches.", "startOffset": 12, "endOffset": 41}, {"referenceID": 46, "context": "We used the Wikipedia benchmark dataset (Partalas et al., 2015) of the Large Scale Hierarchical Text Classification Challenge4.", "startOffset": 40, "endOffset": 63}], "year": 2016, "abstractText": "Pairwise learning or dyadic prediction concerns the prediction of properties for pairs of objects. It can be seen as an umbrella covering various machine learning problems such as matrix completion, collaborative filtering, multi-task learning, transfer learning, network prediction and zero-shot learning. In this work we analyze kernel-based methods for pairwise learning, with a particular focus on a recently-suggested two-step method. We show that this method offers an appealing alternative for commonly-applied Kronecker-based methods that model dyads by means of pairwise feature representations and pairwise kernels. In a series of theoretical results, we establish correspondences between the two types of methods in terms of linear algebra and spectral filtering, and we analyze their statistical consistency. In addition, the two-step method allows us to establish novel algorithmic shortcuts for efficient training and validation on very large datasets. Putting those properties together, we believe that this simple, yet powerful method can become a standard tool for many problems. Extensive experimental results for a range of practical settings are reported.", "creator": "LaTeX with hyperref package"}}}