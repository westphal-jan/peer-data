{"id": "1606.04422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge", "abstract": "We propose real logic: a uniform framework for integrating automatic learning and reasoning. Real logic is defined on a full first-order language where formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as (feature) vectors of real numbers. Real logic promotes a well-founded integration of deductive reasoning on knowledge-bases with efficient, data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's TensorFlow primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.", "histories": [["v1", "Tue, 14 Jun 2016 15:25:28 GMT  (29kb)", "http://arxiv.org/abs/1606.04422v1", "12 pages, 2 figs, 1 table, 27 references"], ["v2", "Thu, 7 Jul 2016 12:28:57 GMT  (30kb)", "http://arxiv.org/abs/1606.04422v2", "12 pages, 2 figs, 1 table, 27 references"]], "COMMENTS": "12 pages, 2 figs, 1 table, 27 references", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO cs.NE", "authors": ["luciano serafini", "artur d'avila garcez"], "accepted": false, "id": "1606.04422"}, "pdf": {"name": "1606.04422.pdf", "metadata": {"source": "CRF", "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge", "authors": ["Luciano Serafini", "Artur d\u2019Avila Garcez"], "emails": ["serafini@fbk.eu", "a.garcez@city.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.04 422v 1 [cs.A I] 1 4Ju nKeywords: Knowledge Representation, Relational Learning, Tensor Networks, NeuralSymbolic Computation, Data-driven Knowledge Completion."}, {"heading": "1 Introduction", "text": "The current situation in the USA and Europe is different: The USA, the USA, the EU, the USA, the USA, the EU, the USA, the USA, the USA, the USA, the USA, the USA, the EU, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the"}, {"heading": "2 Real Logic", "text": "The propositions of L are used to express relational knowledge, e.g. the atomic formula G (o1, o2) states that objects o1 and o2 are related to each other. (R, y) states that R is a reflexive relationship in which x and y are variables; y.R (o1, y) states that there is an (unknown) object related to object o1 throughR. For simplicity, we assume that all propositions of L are in prenex subjunctive, scolemised normal form [16], e.g. a proposition x (x) \u2192 yR (x, y) is converted into an equivalent clause."}, {"heading": "3 Learning as approximate satisfiability", "text": "We start with the definition of the ground theory and its satisfaction. < G > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p = p = p, p = p, p = p = p = p = p = p, p = p = p = p = p = p, p = p = p = p = p = p = p, p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p, p = p = p = p = p"}, {"heading": "4 Implementing Real Logic in Tensor Networks", "text": "In this section we describe a realization of real logic where G is the space of real tensor transformations of order k (where k is a parameter).In this space, function symbols are interpreted as linear transformations. more precisely, if f is a function symbol of arm m and v1,..., vm-Rn are real vectors corresponding to the grounding of m terms, then G (f) (v1,.., vm value) = Mfv + Nffor some n-mn matrix Mf and n-vector Nf, then G (f) (f) (v = < v1,., vn >. The grounding of the m predicate P, G (P), is defined as a matrix function Mf and n-vector Nf, where v = < v1,."}, {"heading": "5 An Example of Knowledge Completion", "text": "In fact, it is so that most people are able to recognize and understand themselves. (...) In the second half of the 20th century, it is so that people are able to identify themselves. (...) In the third half of the 20th century, it is so that people in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the fourth, in the 19th century."}, {"heading": "6 Related work", "text": "In fact, most people who are able to survive themselves are able to survive themselves, \"he said in an interview with the\" New York Times. \"\" I don't think the world is OK, \"he said.\" I don't think the world is OK. \"He added,\" I don't think the world is OK. \"He added,\" I don't think the world is OK. \"He added,\" I don't think the world is OK. \"He added,\" I don't think the world is OK, but it is OK. \""}, {"heading": "7 Conclusion and future work", "text": "We have proposed real logic: a uniform framework for learning and reasoning. Approximate satisfaction is defined as a learning task in which both knowledge and data are mapped to real value vectors. An inference-as-learning approach can be used to integrate relational knowledge constraints and state-of-the-art data-driven approaches. We have shown how real logic can be implemented and efficiently applied to knowledge completion and data forecasting tasks in Deep Tensor Networks, which we call Logic Tensor Networks (LTNs). As a future work, we will provide the implementation of LTN, written in TENSORFLOWTM, to the community so that LTN can be applied to large-scale experiments and relational learning benchmarks, and comparisons can be drawn with systems of statistical relational learning, neural symbolic computing and (probabilistic) inductive logic programming."}], "references": [{"title": "Random satisfiability. In Handbook of Satisfiability, pages 245\u2013270", "author": ["Dimitris Achlioptas"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A (somewhat) new solution to the variable binding problem", "author": ["Leon Barrett", "Jerome Feldman", "Liam MacDermed"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "An Introduction to Many-Valued and Fuzzy Logic: Semantics, Algebras, and Derivation Systems", "author": ["M. Bergmann"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "From machine learning to machine reasoning", "author": ["L\u00e9on Bottou"], "venue": "Technical report,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Neural-symbolic learning and reasoning (dagstuhl seminar 14381)", "author": ["Artur S. d\u2019Avila Garcez", "Marco Gori", "Pascal Hitzler", "Lu\u0131\u0301s C. Lamb"], "venue": "Dagstuhl Reports,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Neural-Symbolic Cognitive Reasoning", "author": ["Artur S. d\u2019Avila Garcez", "Lu\u0131\u0301s C. Lamb", "Dov M. Gabbay"], "venue": "Cognitive Technologies. Springer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Bridging logic and kernel machines", "author": ["Michelangelo Diligenti", "Marco Gori", "Marco Maggini", "Leonardo Rigutini"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver"], "venue": "search. Nature,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S. d\u2019Avila Garcez"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Towards a model theory for distributed representations", "author": ["Ramanathan Guha"], "venue": "In 2015 AAAI Spring Symposium Series,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Logic in Computer Science: Modelling and Reasoning About Systems", "author": ["Michael Huth", "Mark Ryan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "The vision of autonomic", "author": ["Jeffrey O. Kephart", "David M. Chess"], "venue": "computing. Computer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Douwe Kiela", "L\u00e9on Bottou"], "venue": "In Proceedings of EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine Intelligence", "author": ["Bart Kosko"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "BLOG: probabilistic models with unknown objects", "author": ["Brian Milch", "Bhaskara Marthi", "Stuart J. Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov"], "venue": "Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Meta-interpretive learning of higher-order dyadic datalog: predicate invention revisited", "author": ["Stephen H. Muggleton", "Dianhuan Lin", "Alireza Tamaddoni-Nezhad"], "venue": "Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning relational sum-product networks", "author": ["Aniruddh Nath", "Pedro M. Domingos"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Statistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Luc De Raedt", "Kristian Kersting", "Sriraam Natarajan", "David Poole"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Lecture 6.5 - RMSProp, COURSERA: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Hybrid markov logic networks", "author": ["Jue Wang", "Pedro M. Domingos"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 9, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 14, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 15, "context": "The recent availability of large-scale data combining multiple data modalities, such as image, text, audio and sensor data, has opened up various research and commercial opportunities, underpinned by machine learning methods and techniques [5, 12, 17, 18].", "startOffset": 240, "endOffset": 255}, {"referenceID": 12, "context": "Guha\u2019s recent position paper [15] is a case in point, as it advocates a new model theory for real-valued numbers.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 7, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 8, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 132, "endOffset": 143}, {"referenceID": 1, "context": "In this paper, we take inspiration from such recent work in AI, but also less recent work in the area of neuralsymbolic integration [8, 10, 11] and in semantic attachment and symbol grounding [4] to achieve a vector-based representation which can be shown adequate for integrating machine learning and reasoning in a principled way.", "startOffset": 192, "endOffset": 195}, {"referenceID": 21, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 173, "endOffset": 176}, {"referenceID": 10, "context": "This paper proposes a framework called logic tensor networks (LTN) which integrates learning based on tensor networks [25] with reasoning using first-order manyvalued logic [6], all implemented in TENSORFLOW [13].", "startOffset": 208, "endOffset": 212}, {"referenceID": 6, "context": "It is expected that, through an adequate integration of numerical properties and relational knowledge, differently from the immediate related literature [9, 2, 1], the framework introduced in this paper will be capable of combining in an effective way full first-order logical inference on open domains with efficient relational multi-class learning using tensor networks.", "startOffset": 153, "endOffset": 162}, {"referenceID": 13, "context": "For simplicity, we assume that all sentences of L are in prenex conjunctive, skolemised normal form [16], e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "An example of grounding is the one that associates to each document its bag-of-words vector [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "To limit the number of clause instantiations, which in general might be infinite since L admits function symbols, the usual approach is to consider the instantiations of each clause up to a certain depth [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 21, "context": "The grounding of m-ary predicate P , G(P ), is defined as a generalization of the neural tensor network [25] (which has been shown effective at performing the task of knowledge completion in the presence of simple logical constraints), as a function from R to [0, 1], as follows:", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "An estimation of the optimal grounding is obtained by 5,000 runs the RMSProp optimisation algorithm [26] available in TENSORFLOW .", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "In his recent note, [15], Guha advocates the need for a new model theory for distributed representations (such as those based on embeddings).", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Real logic shares with [15] the idea that terms must be interpreted in a geometric space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "the semantics proposed in [15] can be implemented within an ltn with a single layer (k = 1), since the operation of projection and comparison necessary to compute the truth-value of P (t1, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Real logic is orthogonal to the approach taken by (Hybrid) Markov Logic Networks (MLNs) and its variations [24, 27, 22].", "startOffset": 107, "endOffset": 119}, {"referenceID": 19, "context": "Real logic is orthogonal to the approach taken by (Hybrid) Markov Logic Networks (MLNs) and its variations [24, 27, 22].", "startOffset": 107, "endOffset": 119}, {"referenceID": 16, "context": "Much work has been done also on neuro-fuzzy approaches [19].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Bayesian logic (BLOG) [20] is open domain, and in this respect is similar to real logic and LTNs.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Other statistical AI and probabilistic approaches such as lifted inference fall in this same category, including probabilistic variations of inductive logic programming (ILP) [23], which are normally restricted to Horn clauses.", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Metainterpretive ILP [21], together with BLOG, seems closer to LTNs in what concerns the knowledge representation language, but without exploring tensor networks and its benefits in terms of computational efficiency.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "Finally, related work in the domain of neural-symbolic computing and neural network fibring [10] has sought to combine neural networks with ILP to gain efficiency [14] and other forms of knowledge representation, such as propositional modal logic and logic programming.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Finally, related work in the domain of neural-symbolic computing and neural network fibring [10] has sought to combine neural networks with ILP to gain efficiency [14] and other forms of knowledge representation, such as propositional modal logic and logic programming.", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "We propose real logic: a uniform framework for integrating automatic learning and reasoning. Real logic is defined on a full first-order language where formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as (feature) vectors of real numbers. Real logic promotes a well-founded integration of deductive reasoning on knowledge-bases with efficient, data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google\u2019s TENSORFLOW primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}