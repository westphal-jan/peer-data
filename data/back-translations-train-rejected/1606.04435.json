{"id": "1606.04435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "abstract": "Deep neural networks have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive the neural network into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since images with their high-entropy can be conveniently manipulated without changing the images' overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure.", "histories": [["v1", "Tue, 14 Jun 2016 16:01:52 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v1", "13 pages"], ["v2", "Thu, 16 Jun 2016 08:14:12 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v2", "version update: correcting typos, incorporating external feedback"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG cs.NE", "authors": ["kathrin grosse", "nicolas papernot", "praveen manoharan", "michael backes", "patrick mcdaniel"], "accepted": false, "id": "1606.04435"}, "pdf": {"name": "1606.04435.pdf", "metadata": {"source": "CRF", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "authors": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "emails": ["grosse@cs.uni-saarland.de", "ngp5056@cse.psu.edu", "manoharan@cs.uni-saarland.de", "backes@mpi-sws.org", "mcdaniel@cse.psu.edu"], "sections": [{"heading": null, "text": "In this paper, we show how to construct highly effective attacks on opposing patterns for neural networks that are used as malware classifiers. At this point, the ability to create suitable samples is severely limited when the problem of image classification is replaced by malware classification: (i) continuous, differentiated input domains are replaced by discrete, often binary inputs; and (ii) the loose state of leaving the visual appearance unchanged is replaced by equivalent functional behavior. We show the feasibility of these attacks on many different cases of malware classifiers that we have trained using the DREBIN Android malware data set. We also examine the extent to which potential defenses against opposing crafts can be used to discontinue malware classification."}, {"heading": "1. INTRODUCTION", "text": "This year, it will only be once before such a process takes place."}, {"heading": "2. BACKGROUND", "text": "In this section, we will explain the general concepts used in this essay. First, we will briefly cover the background to neural networks and explain in detail how to create opposing samples."}, {"heading": "2.1 Neural Networks", "text": "Neural networks are machine learning models that are capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] to the reduction of dimensionality [11] Each neuron applies an activation function, often nonlinear, to its input to generate an output. Figure 1, taken from [17], illustrates the general structure of such neural neutral networks and also introduces the notation used throughout the paper. Starting with the model input, each network layer generates an output that is used as input by the next layer. Networks with a single middle - hidden - layer are qualified as flat neural networks, while models with multiple hidden layers are deep neural networks. The use of multiple hidden layers is interpreted as hierarchical extraction of representations from the input [6], which are ultimately an ontonal learning task with multiple neural layers and a pre-sagonal output."}, {"heading": "2.2 Adversarial Machine Learning", "text": "These manipulations take the form of conflicting examples, which, by adding carefully selected and often indistinguishable approaches, are forced to input in order to force a targeted model to misclassify the sample. [7] These examples take advantage of the inadequacies in the training phase, as well as the underlying linearity of components used to learn models - even if the overall model is not linear, as is the case for deep neural networks [7]. Opponents \"space has been formalized for multi-class learning models in a taxonomy. [17] Adversarial targets can differ from simple misclassifications in a class that differs from the legitimate source class to the source targets of misclassification classes, in which examples from each source class are misclassified."}, {"heading": "3. METHODOLOGY", "text": "This section describes the adversarial crafting approach to detecting malware. We begin by describing how we present classification applications and how we train classifiers, detailing the configurations we choose for the neural networks, then describe in detail how we create adversarial samples based on the forward derivatives of the trained neural network, and explain the limitations of creating adversarial samples to detect malware (we just add features to ensure functionality is maintained)."}, {"heading": "3.1 Application Model", "text": "Before we can begin the formation of a neural network-based malware detection system, we must first decide on a representation of applications that we use as input in our classifier. In this work, we focus on statically defined features of applications. As a feature, we understand some of the properties that the statically evaluated code of the application has, such as whether the application uses a certain system call or not, as well as the use of certain hardware components or access to the Internet. A natural way to represent such features is the use of binary indicator vectors: given features 1,... we represent an application that uses the binary vector X or not, where Xi indicates whether the application has i, i.e. Xi = 1, or not, i.e. Xi = 0. Due to the diverse nature of the available applications, M will typically be very large, while each individual application has very few features. This leads to very sparse feature vectors and, overall, to a thin space that we can successfully build in our networks."}, {"heading": "3.2 Training the Malware Classifier", "text": "There is no publicly available malware detection system based on neural networks that takes static properties into account. [5] While we limit ourselves to the simpler, static features, in this case we stick to simpler, static detection. [6] The most common neural network architectures used in the literature are Convolutionary Networks for computer vision tasks and Recursive Neural Networks for natural speech processing and acquisition. However, these architectures use the special properties of their input domains to improve their classification performance. On the one hand, Constitutional Neural Networks well on input-invariant properties that can be found in images. [13] Recurrent neural Networks well with input data that needs to be processed sequentially."}, {"heading": "3.3 Crafting Adversarial Malware Samples", "text": "The aim of the classification is to mislead the classification system, which results in the classification for a particular application changing depending on whether the classification is changed to another category, depending on whether the classification is changed to another category, depending on whether the classification is changed to another category or not. We take the option that has the higher probability, i.e. y = argmaxi Fi (X).The aim of the classification now is that we find a small classification in another category in which the classification is changed to another category. We assume that the classification is changed to another category, that the classification is changed to another category."}, {"heading": "3.4 Restrictions on Adversarial Crafting", "text": "In order to ensure that the changes caused by the above algorithms do not alter the application in question too much, we impose a limit on the distortion we apply to the original example. As in the case of computer vision, we only allow distortions in the application we apply. However, we differ in the norm we apply: in computer vision, we usually use the L norm to limit the maximum change to a single pixel. In our case, any change to an entry will always change its value by exactly 1, rendering the L norm obsolete. Instead, we use the L1 norm to bind the total number of features we modify. While the main objective of the contrarian craft is to achieve a misclassification, in malware detection, this cannot happen at the expense of the functionality of the application: functional changes determined by the algorithm 1 may cause the application in question to lose its functionality in parts already in place, or we may only incorporate the additional limitations we already have in the following application to avoid the following ones."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "Through our evaluation, we aim to confirm the following two hypotheses: First, that the neural network-based malware classifier achieves a performance comparable to the state-of-the-art malware classifiers depicted in the literature; second, the contrarian pattern creation algorithm discussed in Section 3.3 allows us to successfully mislead the neural network we train; and, as a measure of success, we consider the misclassification rate achieved by the contrarian crafting algorithm, which is defined by the percentage of malware samples that are misclassified after applying the contrarian crafting algorithm but were previously correctly classified."}, {"heading": "4.1 Data set", "text": "We base our assessments on the DREBIN dataset originally introduced by Arp et al. [1]: DREBIN contains 129,013 android applications, 123,453 of which are benign and 5,560 malicious. The dataset already contains extracted static features for all applications. In total, the dataset contains 545,333 features divided into 8 feature classes, each of which is represented by a binary value indicating whether the feature is present in an application or not. This translates the binding indicator vector X that we use to represent applications that have a vector in {0, 1} M with M = 545,333.The feature classes in DREBIN encompass various aspects of android applications, including: A) permissions and hardware access to these applications."}, {"heading": "4.2 Malware Detection", "text": "We train numerous neural networks with different architectures according to the training procedure described in Section 3. Our basic architecture is the network with two hidden layers of 200 neurons each. From here, we vary the number of neurons per layer (to 10, 50, 100 and 300) and also the number of layers (to 1, 3 and 4). We also vary the malware ratio in each training part in steps from 0.1 to 0.5 and measure its impact on the overall performance of the neural network in the correct classification of applications in the DREBIN dataset.The results for the different neural networks can be found in Table 3. WE first list earlier classifiers from the literature, then the architecture (in neurons per shift) we have trained. As performance measurements, we consider the overall accuracy on the DREBIN dataset, as well as the false positive and false negative rates."}, {"heading": "4.3 Adversarial Malware Crafting", "text": "Next, we apply the hostile creation algorithm described in Section 3 and observe how often we can successfully mislead our neural network classifiers. We quantify the performance of our algorithm by the misclassification rate achieved, which measures the amount of previously correctly classified malware that is misclassified after adversarial creation. In addition, we also measure the average number of changes required to determine the measured misclassification rate, which architecture makes it more difficult to be misled. As discussed above, we allow a maximum of 20 changes to one of the malware applications. The performance results are in Table 4. As we can see, we achieve misclassification rates of at least 50% in the case of the two-layer neural network with 200 and 10 neurons in each layer, up to 84% in the case of the two-layer network with 10 neurons in both layers. Again, we cannot directly observe a rule that establishes a connection between the network architecture and malware."}, {"heading": "4.4 Discussion", "text": "As our evaluation results show, scanning objects also poses a real threat to malware-based neural networks. Therefore, we confirm that findings from computer vision applications can be successfully transferred to security-critical areas (depending on the architecture considered and ignoring cornerstones).Although this does not match the misclassification rates of up to 97% for numerical images reported by Papernot et al., a reduction in misclassification performance could be expected given the malware-detection domain. In the next part of this paper, we will consider possible defense mechanisms to harden our neural network against hostile machinations. To ensure comparability, we will limit ourselves to the malware-detection domain, which has a high comparability rate between this neural architecture and the neural laymen."}, {"heading": "5. DEFENSES", "text": "In this section, we examine potential defense mechanisms for hardening our neural network-based malware detector against enemy crafting. We measure the effectiveness of each of these mechanisms by determining the reduction in error classification rates, i.e. we determine the difference between the error classification rates achieved compared to regular networks and those to which we applied the mechanisms. As in the previous section, the error classification rate is defined as a percentage of malware samples that were incorrectly classified after applying the hostile crafting algorithm but were correctly classified beforehand. We first consider functional reduction as a potential defense mechanism: By reducing the number of features we consider for classification, we hope to reduce the neural network's sensitivity to changes in input as well as reduce the number of feature options the opponent has to distort an application and cause a misclassification."}, {"heading": "5.1 Simple Feature Reduction", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the r"}, {"heading": "5.2 Feature Reduction via Mutual Information", "text": "In fact, the situation is that most people are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5.3 Distillation", "text": "(It.). (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.\" (It.) \"(It.)\" (It.) \"(It.)\" (It.) \"(It.\" (It.) \"(It.)\" (It. (It.) \"(It.)\" (It. (It.) \"(It.)\" (It. (.) \"(It. (.)\" (It. (.) \"(It.)\" (It. (.) \"(It. (.)\" (It. (.). (It.) \"(It. (.)\" (It.). \"(It. (.).\" (It.). \"(It.\"). \"(It.\" (. \").\" (It. \"(.\"). \"(It.\"). \"(It.\" (. \").\" (It. \").\" (. \"(It.\"). \"(It.\" (. \").\" (It. \").\" (It. \"(.\"). \"(.\"). \"(It.\" (. \").\" (It. \"(.\"). (. (It. (.). \"). (It. (It. (.). (It. (.). (.). (It. (. (.). (.). (It. (.). (It. (.). (. (.). (.). (It. (It.). (It. (.).). (. (.). (It. (.). (.). (.)."}, {"heading": "5.4 Re-Training", "text": "In fact, most people are able to play by the rules they have imposed on themselves. \"We have to play by the rules,\" he says, \"but we have to play by the rules.\" \"We have to play by the rules,\" he says. \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules,\" he says. \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\""}, {"heading": "5.5 Summary of Results", "text": "We examined four potential defense mechanisms and assessed their effects on neural networks that are prone to enemy crafting. Feature reduction, both simple and mutual information, usually weakens the neural network against enemy crafting. Having fewer features of greater importance will facilitate the creation of enemy samples, caused by the greater impact each feature has on the output distribution of the network. At this point, we cannot recommend feature reduction as a defense mechanism. Future work will need to identify methods of feature reduction that are potentially more involved, which actually increase the resilience of a network against enemy crafting. We also examined distillation and re-education, both originally proposed as a defense mechanism against enemy crafting in computer vision. Distillation has a positive effect, but does not work as well as in computer vision."}, {"heading": "6. RELATED WORK", "text": "Barreno et al. [2] give a comprehensive overview of attacks on machine learning systems, distinguishing between exploratory attacks at the time of testing or causal attacks that affect the training data to achieve the desired result. Contrasting samples, such as those used here, are used at test dates. [9, 7] They can be constructed for different algorithms and in some cases generalized between different architectures [7]. Gu et al. [9] claim that cross-samples are mainly a product of the way in which supplying neural networks are trained and optimized. As a solution, they propose deep contractive networks, which are more difficult to optimize. These networks include a layer-based penalty that further limits their capacity. [7] Goodfellow proposed a linear explanation for the existence of cross-samples."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "In this work, we investigated the feasibility of adversarial crafting against neural networks in areas that differ from computer vision. By evaluating the DREBIN dataset, we were able to show that adversarial crafting also poses a real threat in security-critical areas such as malware detection: We achieved misclassification rates of up to 80% compared to classifiers based on neural networks that achieve state-of-the-art classification performance. As a second contribution, we investigated four potential defense mechanisms for hardening our neural networks against adversarial crafting. Our assessments of these mechanisms showed the following: First, functional reduction, a popular method for reducing input complexity and simplifying the training of the classifier, generally leads to the neural network becoming weaker against adversarial crafting. Second, distillation improves misclassification rates, but does not reduce them as strongly as computer settings should reduce, and third, defenses should be carefully investigated."}, {"heading": "8. REFERENCES", "text": "[1] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, andK. Rieck. 2016 Networks on 34ural 34 pages. DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket. In Proceedings of the 2014 Network and Distributed System Security Symposium (NDSS), 2014. [2] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. The Security of Machine Learning. Machine Learning, 81 (2): 121-148, 2010. [3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Xixi Xiyal. End of Learning for Self-Driving Cars. arXiv preprint arXiv: 1604.07316, 2016."}], "references": [{"title": "DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket", "author": ["D. Arp", "M. Spreitzenbarth", "M. Hubner", "H. Gascon", "K. Rieck"], "venue": "Proceedings of the 2014 Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning, 81(2):121\u2013148", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 3422\u20133426. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 3422\u20133426", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "CoRR, abs/1412.5068", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "ArXiv e-prints", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "arXiv preprint arXiv:1601.04920", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning in Adversarial Settings", "author": ["P. McDaniel", "N. Papernot"], "venue": "IEEE Security & Privacy,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Applied linear statistical models", "author": ["J. Neter", "M.H. Kutner", "C.J. Nachtsheim", "W. Wasserman"], "venue": "volume 4. Irwin Chicago", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "and al", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha"], "venue": "Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "Proceedings of the 1st IEEE European Symposium in Security and Privacy (EuroS&P)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient detection of zero-day android malware using normalized 12  bernoulli naive bayes", "author": ["L. Sayfullina", "E. Eirola", "D. Komashinsky", "P. Palumbo", "Y. Mich\u00e9", "A. Lendasse", "J. Karhunen"], "venue": "2015 IEEE TrustCom/BigDataSE/ISPA, Helsinki, Finland, August 20-22, 2015, Volume 1, pages 198\u2013205", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Intriguing properties of  neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "These inputs are derived from regular inputs by minor yet carefully selected perturbations [7, 17] that induce the neural network into adversary-desired misclassifications.", "startOffset": 91, "endOffset": 98}, {"referenceID": 16, "context": "These inputs are derived from regular inputs by minor yet carefully selected perturbations [7, 17] that induce the neural network into adversary-desired misclassifications.", "startOffset": 91, "endOffset": 98}, {"referenceID": 16, "context": "The approach that conceptually underlies many recent adversarial machine learning research effort involves gradients of the function F represented by the neural network in order to classify inputs: evaluating it on an input X, one can quickly (and fully automatically) either identify individual input features that should be perturbed iteratively to achieve misclassification [17] or compute a suitable minor change for each pixel all at once [7].", "startOffset": 377, "endOffset": 381}, {"referenceID": 6, "context": "The approach that conceptually underlies many recent adversarial machine learning research effort involves gradients of the function F represented by the neural network in order to classify inputs: evaluating it on an input X, one can quickly (and fully automatically) either identify individual input features that should be perturbed iteratively to achieve misclassification [17] or compute a suitable minor change for each pixel all at once [7].", "startOffset": 444, "endOffset": 447}, {"referenceID": 0, "context": "DREBIN data set [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "[17], but address challenges that appear in the transition from continuous and differentiable input domains in computer vision to discrete and restricted inputs in malware detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1], which contains more than 120,000 android applications samples, among them over 5,000 malware samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In a second step we then consider distillation [18] and re-training on adversarial samples [21] which have both been proposed as actual defensive mechanisms in the literature.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "In a second step we then consider distillation [18] and re-training on adversarial samples [21] which have both been proposed as actual defensive mechanisms in the literature.", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 110, "endOffset": 117}, {"referenceID": 3, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "Figure 1, taken from [17], illustrates the general structure of such neural neutworks and also introduces the notation that is used throughout the paper.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Using multiple hidden layers is interpreted as hierarchically extracting representations from the input [6], eventually producing a representation relevant to solve the machine learning task and output a prediction.", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "Neural networks, like numerous machine learning models, have been shown to be vulnerable to manipulations of their inputs [21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "These samples exploit imperfections in the training phase as well as the underlying linearity of components used to learn models\u2014even if the overall model is non-linear like is the case for deep neural networks [7].", "startOffset": 211, "endOffset": 214}, {"referenceID": 16, "context": "The space of adversaries was formalized for multi-class deep learning classifiers in a taxonomy [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "Crafting an adversarial sample ~ x\u2217\u2014 misclassified by model F\u2014from a legitimate sample ~x can be formalized as the following problem [21]:", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "[17]", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] linearizes the model\u2019s cost function around the input to be perturbed and selects a perturbation by differentiating this cost function with respect to the input itself and not the network parameters like is traditionally the case during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] evaluates the model\u2019s output sensitivity to each", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "However, a black-box attack leveraging both of these approaches to target unknown remotely hosted deep neural networks was proposed in [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Machine learning models deployed in adversarial settings therefore need to be robust to manipulations of their inputs [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "demonstrated that explicitly training with adversarial samples reduced the error rate of models on samples crafted against the resulting improved model [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 17, "context": "proposed the use of distillation\u2014training with class probabilities produced by a teacher model instead of labels\u2014as a defense mechanism, and showed that this effectively reduces the sensitivity of models to small perturbations [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 21, "context": "[22] evaluated a simplified variant of distillation training models on softened indicator vectors instead of probabilities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] use a neural", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "On the one hand, convolutional neural networks work well on input containing translation invariant properties, which can be found in images [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Recurrent neural networks, on the other hand, work well with input data that needs to be processed sequentially [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 16, "context": "[17] and which we already discussed in Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "It is largely similar to the algorithms presented in [17], with the difference in the discrete changes of the input vector due to the input domain and also the additional restrictions below.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "[1]: DREBIN contains 129.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] achieve a 6.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] even achieve a 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] \u2212 \u2212 6.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] \u2212 \u2212 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "06 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "06 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "74 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "74 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "90 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "90 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "11 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "04 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "6 [10, 200] 0.", "startOffset": 2, "endOffset": 11}, {"referenceID": 9, "context": "71 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "86 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "82 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "92 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "92 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "15 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "15 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "12 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "12 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "48 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "89 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "84 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "96 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "96 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "79 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 16, "context": "[17], a reduction in misclassification performance was to be expected given the challenge inherent to the malware detection domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], and re-training on adversarial samples, following methodology described by Szegedy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] as as a way to transfer knowledge from large neural networks to a smaller ones,", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] recently proposed using it as a defensive mechanism against adversarial crafting as well.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] motivate the use of distillation as a defensive mechanism through its capability to improve the second networks generalization performance (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Thus, instead of using distillation to train a second smaller network like was proposed in [10], they use the output of the first neural network F to train a second neural network F\u2032 with exactly the same architecture.", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "[18] reported rates around 5% after distillation for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] and involves the following steps:", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] propose an alternative approach to the general idea of adversarial re-training: instead of training on actually crafted adversarial samples, they formulate an adversarial loss function that incorporates the possibility of adversarial crafting through perturbations in direction of the network\u2019s gradient.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] give a broad overview of attacks against machine learning systems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There has already been done some work on why they exist in general[9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "There has already been done some work on why they exist in general[9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "They can be constructed for different algorithms and also generalize between different architectures in some cases [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "[9] claim that adversarial samples are mainly a product of the way feed-forward neural networks are trained and optimized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] proposed a linear explanation to the existence of adversarial samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "\u2019s [7] idea of using an adversarial loss functions during training should be more carefully examined in security relevant domains.", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "Deep neural networks have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive the neural network into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since images with their high-entropy can be conveniently manipulated without changing the images\u2019 overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection\u2013which may pose significant challenges in sample generation and arguably grave consequences for failure. In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. Here, we face severely constrained limits on crafting suitable samples when the problem of image classification is replaced by malware classification: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results.", "creator": "LaTeX with hyperref package"}}}