{"id": "1606.04486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Lifted Convex Quadratic Programming", "abstract": "Symmetry is the essential element of lifted inference that has recently demon- strated the possibility to perform very efficient inference in highly-connected, but symmetric probabilistic models models. This raises the question, whether this holds for optimisation problems in general. Here we show that for a large class of optimisation methods this is actually the case. More precisely, we introduce the concept of fractional symmetries of convex quadratic programs (QPs), which lie at the heart of many machine learning approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can then be tackled with the usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms, stochastic gradients etc.). If the original QP exhibits symmetry, then the lifted one will generally be more compact, and hence their optimization is likely to be more efficient.", "histories": [["v1", "Tue, 14 Jun 2016 18:18:58 GMT  (933kb,D)", "http://arxiv.org/abs/1606.04486v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["martin mladenov", "leonard kleinhans", "kristian kersting"], "accepted": false, "id": "1606.04486"}, "pdf": {"name": "1606.04486.pdf", "metadata": {"source": "CRF", "title": "Lifted Convex Quadratic Programming", "authors": ["Martin Mladenov", "Leonard Kleinhans"], "emails": ["martin.mladenov@cs.tu-dortmund.de", "leonard.kleinhans@tu-dortmund.de", "kristian.kersting@cs.tu-dortmund.de"], "sections": [{"heading": null, "text": "Symmetry is the essential element of upscale conclusions that has recently demonstrated the ability to execute very efficient conclusions in highly networked but symmetrical probability models, which raises the question of whether this is true of optimization problems in general. At this point, we show that this is indeed the case with a large class of optimization methods. Specifically, we introduce the concept of broken symmetries of convex square programs (QPs) that form the core of many machine learning approaches, and use it to raise, i.e. compress, QPs. These upscale QPs can then be tackled with the usual optimization toolbox (standard solvers, cutting algorithms, stochastic gradients, etc.). If the original QP shows symmetry, the upscale QP will generally be more compact, and its optimization is therefore likely to be more efficient."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that there will only be one such process, and that is the question of the extent to which such a process has taken place."}, {"heading": "2 Prior Art", "text": "Several expressive modeling languages for mathematical programming have been proposed, see e.g. [Wallace and Ziemba, 2005] for an up-to-date overview. These modeling languages are mixtures of declarative and imperative programming styles using object sets to index multidimensional parameters and LP variables. Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. Unfortunately, according to Kabjan et al. [2009], it can still be argued that there is a need for languages that not only facilitate natural algebraic modeling, but also provide integrated capabilities with logical programming. This is also due to the growing need for relational mathematical modeling in natural language processing [Yih and Roth, 2007, Riedel et al., 2012] and the recent urge to combine statistical analytical frameworks such as R and Python with relational databases. [Ret al, 2015] The current work shows the QPs and QPs are the first relational."}, {"heading": "3 Exact Symmetries of Convex Quadratic Programs", "text": "We start with exact symmetries of convex QPs. \"We start with exact symmetries of convex QPs.\" We start with exact symmetries of convex QPs. \"We start with exact symmetries of convex QPs.\" We start with exact symmetries of convex QPs. \"We start with exact symmetries of convex QPs.\" We start with exact QP. \"We start with exact QP.\" We start with exact QP. \"We start with exact QTX.\" We start with exact QTX. \"We start with exact QP.\" We start with exact QP. \"We start with exact QP.\" We start with exact QTX. \"We start with exact QTX.\" We start with exact QTX. \"We start with exact QP.\""}, {"heading": "4 Fractional Symmetry of Convex Quadratic Programs", "text": "\"We have to ask ourselves whether we will be able to establish a new system in which we can build a new system,\" he said. \"We have to ask ourselves whether we will be able to establish a new system in the new system.\" He added, \"We have to deal with the question of what we are going to do.\" He added, \"What we are going to do is that we are going to do it.\" He added, \"We are not going to do it.\" He added, \"We are not going to do it as if we are going to do it.\" He added, \"We are going to do it as if we are going to do it.\""}, {"heading": "5 Empirical Illustration", "text": "Our intention here is to investigate the following question (Q): Can we potentially benefit from fracture symmetries of QPs? In general, this is to be expected, for example, for classification as if all data points in an orbit share the same label, then this symmetry effectively lowers the VC dimension and the complexity of the samples of the classifier. In a first experiment, we looked at SVM classifiers for different amounts of overlap between two classes represented by spherical Gaussians. This dataset was chosen to represent the potential of approximate symmetries. We trained an increased SVM (LSVM) with 200 approximate color classes and a conventional SVM class, both with RBF cores, on 2500 training examples per class. We used a grid search together with CV to select the \"t0.25, 0.50, 2.00, 4.00u and C.\" Performance was measured."}, {"heading": "6 Conclusions", "text": "Specifically, we have introduced and studied a precise mathematical definition of the fracture symmetry of convex QPs. Using the fractional automorphism tool, orbits of optimization variables are obtained, and upscale solvers materialize as the corresponding optimization problem in the space of convex optimization variables, enabling the raising of a large class of machine learning tasks and approaches. We have illustrated this here for SVMs by developing the first upscale solver for SVMs and empirically demonstrating its potential. In the future, other ML settings should also be investigated. We could also deepen our theoretical results to more data sets, investigate the link to other methods of data reduction, develop approximate WL graphs and go beyond convex QPs. Most importantly, our framework provides a mathematical basis for symmetric machine learning [Gens and Domingos]."}], "references": [{"title": "Hints and the VC dimension", "author": ["Y.S. Abu-Mostafa"], "venue": "Neural Computation,", "citeRegEx": "Abu.Mostafa.,? \\Q1993\\E", "shortCiteRegEx": "Abu.Mostafa.", "year": 1993}, {"title": "Polyhedral representation conversion up to symmetries", "author": ["D. Bremner", "M. Dutour Sikri\u0107", "A. Sch\u00fcrmann"], "venue": "In Proceedings of the 2006 CRM Workshop on Polyhedral Computations. AMS, Providence,", "citeRegEx": "Bremner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bremner et al\\.", "year": 2009}, {"title": "Automorphism groups of graphical models and lifted variational inference", "author": ["H.H. Bui", "T.N. Huynh", "S. Riedel"], "venue": "In Proc. of the 29th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Bui et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2013}, {"title": "Conflict analysis and branching heuristics in the search for graph automorphisms", "author": ["P. Codenotti", "H. Katebi", "K.A. Sakallah", "I.L. Markov"], "venue": "In Proc. of ICATI,", "citeRegEx": "Codenotti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Codenotti et al\\.", "year": 2013}, {"title": "Statistical Relational Artificial Intelligence: Logic, Probability, and Computation", "author": ["L. De Raedt", "K. Kersting", "S. Natarajan", "D. Poole"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2016}, {"title": "CVXPY: A Python-embedded modeling language for convex optimization, version 0.2", "author": ["S. Diamond", "E. Chu", "S. Boyd"], "venue": "http://cvxpy.org/,", "citeRegEx": "Diamond et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diamond et al\\.", "year": 2014}, {"title": "Deep symmetry networks", "author": ["R. Gens", "P.M. Domingos"], "venue": "In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Gens and Domingos.,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2014}, {"title": "Dimension reduction via colour refinement", "author": ["M. Grohe", "K. Kersting", "M. Mladenov", "E. Selman"], "venue": "In Proceedings of the 22th Annual European Symposium on Algorithms (ESA),", "citeRegEx": "Grohe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Grohe et al\\.", "year": 2014}, {"title": "Symmetry of convex sets and its applications to the extremal ellipsoids of convex bodies", "author": ["O. G\u00fcler", "F. G\u00fcrtuna"], "venue": "Optimization Methods and Software,", "citeRegEx": "G\u00fcler and G\u00fcrtuna.,? \\Q2012\\E", "shortCiteRegEx": "G\u00fcler and G\u00fcrtuna.", "year": 2012}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Y. Jernite", "S. Rush", "D. Sontag"], "venue": "In Proc. of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Jernite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Algebraic modeling in a deductive database language", "author": ["D. Kabjan", "R. Fourer", "J. Ma"], "venue": "http://dynresmanagement. com/uploads/3/3/2/9/3329212/datalog_modeling.pdf,", "citeRegEx": "Kabjan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kabjan et al\\.", "year": 2009}, {"title": "Relational linear programming", "author": ["K. Kersting", "M. Mladenov", "P. Tokmakov"], "venue": "Artificial Intelligence Journal (AIJ), OnlineFirst,", "citeRegEx": "Kersting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kersting et al\\.", "year": 2015}, {"title": "Symmetry in integer linear programming", "author": ["F. Margot"], "venue": "Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art,", "citeRegEx": "Margot.,? \\Q2010\\E", "shortCiteRegEx": "Margot.", "year": 2010}, {"title": "Lifted message passing as reparametrization of graphical models", "author": ["M. Mladenov", "A. Globerson", "K. Kersting"], "venue": "In Proc. of the 30th Int. Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Mladenov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mladenov et al\\.", "year": 2014}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "In Proc. of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Poole.,? \\Q2003\\E", "shortCiteRegEx": "Poole.", "year": 2003}, {"title": "Machine learning and databases: The sound of things to come or a cacophony of hype", "author": ["C. R\u00e9", "D. Agrawal", "M. Balazinska", "M.I. Cafarella", "M.I. Jordan", "T. Kraska", "R. Ramakrishnan"], "venue": "In Proc. of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "R\u00e9 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "R\u00e9 et al\\.", "year": 2015}, {"title": "Parse, Price and Cut\u2013Delayed Column and Row Generation for Graph Based Parsers", "author": ["S. Riedel", "D.A. Smith", "A. McCallum"], "venue": "In Proc. of the EMNLP-CoNLL,", "citeRegEx": "Riedel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2012}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Fast subtree kernels on graphs", "author": ["N. Shervashidze", "K.M. Borgwardt"], "venue": "In Proc. of the 23rd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Shervashidze and Borgwardt.,? \\Q2009\\E", "shortCiteRegEx": "Shervashidze and Borgwardt.", "year": 2009}, {"title": "On the complexity and approximation of binary evidence in lifted inference", "author": ["G. Van den Broeck", "A. Darwiche"], "venue": "In Proc. of the 27th Annual Conf. on Neural Information Processing Systems (NIPS),", "citeRegEx": "Broeck and Darwiche.,? \\Q2013\\E", "shortCiteRegEx": "Broeck and Darwiche.", "year": 2013}, {"title": "Statistical learning theory. Adaptive and learning systems for signal processing, communications and control series", "author": ["V.N. Vapnik"], "venue": "A Wiley-Interscience Publication,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["W.-t. Yih", "D. Roth"], "venue": null, "citeRegEx": "Yih and Roth.,? \\Q2007\\E", "shortCiteRegEx": "Yih and Roth.", "year": 2007}, {"title": "Linear programming support vector machines", "author": ["W. Zhou", "L. Zhang", "L. Jiao"], "venue": "Pattern recognition,", "citeRegEx": "Zhou et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "The language in which convex optimization problems are specified typically includes inequalities, matrix and tensor algebra, and software packages for convex optimization such as CVXPY [Diamond et al., 2014] recreate this language as an interface between the user and the solver.", "startOffset": 185, "endOffset": 207}, {"referenceID": 15, "context": ", 2012] and the recent push to marry statistical analytic frameworks like R and Python with relational databases [R\u00e9 et al., 2015].", "startOffset": 113, "endOffset": 130}, {"referenceID": 12, "context": "There are symmetry-breaking branch-and-bound approaches for (mixed\u2013)integer programming [Margot, 2010] that are also featured by commercial solvers.", "startOffset": 88, "endOffset": 102}, {"referenceID": 18, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al.", "startOffset": 80, "endOffset": 114}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems.", "startOffset": 10, "endOffset": 32}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. However, following Kabjan et al. [2009], one can still argue that there is a need for languages that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming.", "startOffset": 10, "endOffset": 147}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. However, following Kabjan et al. [2009], one can still argue that there is a need for languages that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming. This is also witnessed by the growing need for relational mathematical modeling e.g. in natural language processing [Yih and Roth, 2007, Riedel et al., 2012] and the recent push to marry statistical analytic frameworks like R and Python with relational databases [R\u00e9 et al., 2015]. The present work is the first that introduces relational convex QPs and studies their symmetries. There are symmetry-breaking branch-and-bound approaches for (mixed\u2013)integer programming [Margot, 2010] that are also featured by commercial solvers. QPs, however, do not feature branch-and-bound solvers. For the special fragment of LPs, Kersting et al. [2015] have introduced a relational language and shown how to exploit fractional symmetries.", "startOffset": 10, "endOffset": 962}, {"referenceID": 2, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al., 2013, Mladenov et al., 2014, Jernite et al., 2015]. Unfortunately, their techniques or proofs do not carry over to (convex) QPs. G\u00fcler and G\u00fcrtuna [2012] and references in there have studied automorphisms but not fractional ones of convex sets.", "startOffset": 151, "endOffset": 317}, {"referenceID": 2, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al., 2013, Mladenov et al., 2014, Jernite et al., 2015]. Unfortunately, their techniques or proofs do not carry over to (convex) QPs. G\u00fcler and G\u00fcrtuna [2012] and references in there have studied automorphisms but not fractional ones of convex sets. Finally, our approximate FA approach generalizes Van den Broeck and Darwiche\u2019s [2013] approach of approximating evidence in probabilistic relational models to QPs using real-valued low-rank factorizations.", "startOffset": 151, "endOffset": 494}, {"referenceID": 3, "context": "Such automorphism groups, or rather, the orbit partitions thereof, can be computed via packages such as Saucy Codenotti et al. [2013]. The reason why orbit partitions are lifting partitions of a convex problem, is that JpXPxq \u201c Jp 1 |Aut | \u0159 p\u03a3,\u03a0qPAut \u03a0xq \u010f 1 |Aut | \u0159", "startOffset": 110, "endOffset": 134}, {"referenceID": 7, "context": "For the special case of LPs, Grohe et al. [2014] have proven that equitable partitions act as lifting partitions.", "startOffset": 29, "endOffset": 49}, {"referenceID": 1, "context": "Moreover, if the right dimension (number of columns) B is held fixed, the converse is true as well Bremner et al. [2009]. That is, not only do rotational symmetries of B correspond to renaming symmetries of Q, but vice-versa, as for fixed k, the semidefinite factors of Q are unique up to rotations.", "startOffset": 99, "endOffset": 121}, {"referenceID": 8, "context": "[G\u00fcler and G\u00fcrtuna, 2012], and is to be expected, since the symmetry properties of a given dataset B can easily be destroyed by slightly perturbing the body.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "[Kersting et al., 2015].", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "for classification as if all the data points of an orbit share the same label, then this symmetry effectively lowers the VC-dimension and sample complexity of the classifier [Abu-Mostafa, 1993].", "startOffset": 174, "endOffset": 193}, {"referenceID": 17, "context": "In a second experiment, we considered a relational classification task on the Cora dataset [Sen et al., 2008].", "startOffset": 91, "endOffset": 109}, {"referenceID": 22, "context": "The base classifiers are an8-norm regularized SVM (LP-SVM) [Zhou et al., 2002] and a conventional SVM (QP-SVM) [Vapnik, 1998] formulated as a convex QP.", "startOffset": 59, "endOffset": 78}, {"referenceID": 20, "context": ", 2002] and a conventional SVM (QP-SVM) [Vapnik, 1998] formulated as a convex QP.", "startOffset": 40, "endOffset": 54}, {"referenceID": 11, "context": "Additionally we considered transductive, collective versions of both of them following Kersting et al. [2015], denoted as TC-LP-SVM resp.", "startOffset": 87, "endOffset": 110}, {"referenceID": 11, "context": "linked(I1, I2) = label(I1) & query(I2) & (cite(I1, I2) | cite(I2, I1)) # query for the transductive constraint slacks = sum{I in labeled(I)} slack(I); coslacks = sum{I1, I2 in linked(I1, I2)} slack(I1,I2) # inline definitions # QUADRATIC OBJECTIVE, the main novelty compared to [Kersting et al., 2015] minimize: sum{J in feature(I,J)} weight(J)**2 + c1 * slack + c2 * coslack; subject to forall {I in labeled(I)}: labeled(I)*predict(I) >= 1 - slack(I); # push labeled examples to the correct side subject to forall {I in labeled(I)}: slack(I) >= 0; # slacks are positive # TRANSDUCTIVE PART: cited instances should have the same labels.", "startOffset": 278, "endOffset": 301}, {"referenceID": 11, "context": "(a) TC-QP-SVM encoded in a novel QP extension of the relational LP language in [Kersting et al., 2015].", "startOffset": 79, "endOffset": 102}, {"referenceID": 6, "context": "Most significantly, our framework offers a mathematical foundation for symmetry-based machine learning [Gens and Domingos, 2014].", "startOffset": 103, "endOffset": 128}], "year": 2016, "abstractText": "Symmetry is the essential element of lifted inference that has recently demonstrated the possibility to perform very efficient inference in highly-connected, but symmetric probabilistic models models. This raises the question, whether this holds for optimisation problems in general. Here we show that for a large class of optimisation methods this is actually the case. More precisely, we introduce the concept of fractional symmetries of convex quadratic programs (QPs), which lie at the heart of many machine learning approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can then be tackled with the usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms, stochastic gradients etc.). If the original QP exhibits symmetry, then the lifted one will generally be more compact, and hence their optimization is likely to be more efficient.", "creator": "LaTeX with hyperref package"}}}