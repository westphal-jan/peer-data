{"id": "1606.04487", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs", "abstract": "We perform a study of the factors affecting training time in multi-device deep learning systems. Given a specification of a convolutional neural network, we study how to minimize the time to train this model on a cluster of commodity CPUs and GPUs. Our first contribution focuses on the single-node setting, in which we show that by using standard batching and data-parallel techniques throughput can be improved by at least 5.5x over state-of-the-art systems when training on CPUs. This ensures an end-to-end training time directly proportional to the throughput of a device regardless of its underlying hardware, allowing each node in the cluster to be treated as a black box. Our second contribution is a theoretical and empirical study of the tradeoffs affecting end-to-end training time in a multiple-device setting. We identify the degree of asynchronous parallelization as a key feature affecting both hardware and statistical efficiency. We show that asynchrony can be viewed as introducing a momentum parameter, which we use to limit our search space; in turn, this leads to a simpler optimizer, which is our third contribution. Our optimizer involves a predictive model for the total time to convergence and selects an allocation of resources to minimize that time. We demonstrate that the most popular distributed deep learning systems fall within our tradeoff space but do not optimize within the space. By doing such optimization, our prototype runs 1.9x to 12x faster than the fastest state-of-the-art systems.", "histories": [["v1", "Tue, 14 Jun 2016 18:21:04 GMT  (2439kb,D)", "http://arxiv.org/abs/1606.04487v1", null], ["v2", "Mon, 18 Jul 2016 00:05:49 GMT  (2439kb,D)", "http://arxiv.org/abs/1606.04487v2", null], ["v3", "Fri, 26 Aug 2016 13:04:00 GMT  (2439kb,D)", "http://arxiv.org/abs/1606.04487v3", null], ["v4", "Wed, 19 Oct 2016 04:26:03 GMT  (2549kb,D)", "http://arxiv.org/abs/1606.04487v4", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["stefan hadjis", "ce zhang", "ioannis mitliagkas", "dan iter", "christopher r\\'e"], "accepted": false, "id": "1606.04487"}, "pdf": {"name": "1606.04487.pdf", "metadata": {"source": "CRF", "title": "Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs", "authors": ["Stefan Hadjis", "Ce Zhang", "Ioannis Mitliagkas", "Christopher R\u00e9"], "emails": ["imit}@stanford.edu", "chrismre}@cs.stanford.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "An important aspect of deep learning is that the quality improves with the amount of data, and that advances in system efficiency and scalability lead directly to an improvement in quality. This observation has led to an arms race of distributed deep learning systems both in industry (e.g., \"Distief\") and in science. There are few studies that explore deep learning from the perspective of data producers. Each of these systems makes a series of design decisions, although they are not suitable for other tasks or hardware settings. To address this problem, we are conducting an initial study of the design space for deep learning systems, which implements a series of design decisions."}, {"heading": "Overview of Technical Contributions", "text": "In fact, most people who live and work in the United States are able to outdo themselves, and most of them are able to outlive themselves, in the way they live, in the way they live, in the way they live, in the way they live, in the way they live, in the way they live, in the way they live, in the way they live."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Convolutional Neural Networks (CNNs)", "text": "A revolutionary neural network (CNN, [17]) consists of layers L1, L2,.., LP. Each layer is an operator that takes a 3D data sensor D, Rn, n, din as input and converts it into a resulting 3D data sensor R, Rm, m, dout, i.e. LFWp (Dp) = Rp. FW indicates the layer that runs in the \"front\" direction to D in R. Layers have a second operation, backward or BW, which is described later. Leave the input D1 on the first CNN layer L1 an image I, Rn, n, n, x, 3 representing the RGB color channels. Layers after L1 (the input layer), the input tensor Dp results from the output of a previous layer (normally Dp = Rp, 1)."}, {"heading": "2.2 Stochastic Gradient Descent", "text": "\"The goal of the CNN training is to optimize the W model to minimize the loss function.\" (RP, C), also referred to as \"(W, I, C), to make the fact that RP is a function of W and I explicit. Low loss correlates with high predictive accuracy, and in this work we refer to both. The most popular training algorithm for CNNs is an iterative technique called stochastic gradient descent (SGD). Each SGD iteration consists of a forward and backward directed passage. The input for each SGD iteration is an image-label tuple (I, C), as described above. Forward computes the prediction RP of I using the Equation (2), and then the prediction error compared to C is used to calculate the gradient (or derived) of this RP.\""}, {"heading": "2.3 CNN Computation", "text": "Of all these layers, most are computationally intensive (conv) and fully networked (FC). A conventional layer performs many independent confrontations over an image, i.e. it leads to several sliding window operations. In our study, its data properties are more important, and we refer to Chetlur et al. [6] For a detailed description of its use in the machine, it is multiplied by a call to a BLAS or cuBLAS library."}, {"heading": "2.4 Problem Definition", "text": "We examine system compromises in order to build an optimizer for the most widely used networks / algorithms. We focus on SGD due to its popularity, although our optimizer is applicable to other algorithms as well. Specifically, we receive the following information as input: (i) a CNN architecture {L1,..., Lp}, including regulation, (ii) a data set D consisting of data packets, (iii) a device diagram G, in which vertices are hardware devices (specified by their throughput) and edges communication speeds between devices. Our goal is to design an optimizer that creates a plan for the physical mapping and execution strategy in order to train as quickly as possible. A plan for the physical mapping forms the calculation (both FW and BW) of each layer of CNN on vertices (e.g. GPUs or CPU cores) of the device starting from cluster 3. Initially, this solution is based on the cluster solution, rather than on the individual hardware mapping and the corresponding strategy, but rather on the conclusion that in case of an optimization occurs within the cluster alone, the strategy."}, {"heading": "3. SINGLE-DEVICE TRADEOFF", "text": "The first step in building a distributed optimizer is to understand the system compromises within a single device. We show that for each device (GPU or CPU), we can achieve throughput proportional to its maximum FLOPS. This will allow the distributed optimizer to treat each device in Section 4 as a black box. This is not a trivial feature for deep learning: Many CNN systems exist [1, 2, 16] that use either CPUs or GPUs, but these always report that GPU implementations are an order of magnitude faster than CPUs, even if the devices offer similar FLOPS. Therefore, the challenge is to use the FLOPS on the CPU. We examine the key core in CNN implementations that are computationally bound. We introduce a data batching technique that calculates the amount of memory for computing time and show that this compromise provides an acceleration of more than 5 CU over existing systems."}, {"heading": "3.1 Convolutional Layer Computation", "text": "As reported in the literature and confirmed by our experiments, the most computationally intensive layers in CNN are the revolutionary layers. Together, all the revolutionary layers in a CNN often consume between 70-90% of the total execution time. Remember that a revolutionary layer contains a model that we also call a kernel and will call K. A revolutionary layer accepts as input a 4D data sensor D-Rn \u00b7 n \u00b7 b, where recall b is the batch size. K is also a 4D tensor, K-Rk \u00b7 n \u00b7 dout. The output is a 4D data sensor, the R-Rm \u00d7 dout \u00d7 b, where: Rx, y = din \u2212 d \u2032 d \u00b2."}, {"heading": "3.2 Batching and Data Parallelism", "text": "The design trade-off between CPUs and GPUs arises as a result of this increase in data size. Suppose we get a fixed batch size of b images (e.g. b = 256, discussion of selecting b in Appendix E.1). GPUs cannot fit an entire batch of lowered data into the off-chip memory, so many CNN implementations on GPUs perform a lowering / GEMM series on one or a few images at the same time until all b have been processed. On the CPU, however, the off-chip memory is larger, allowing a lowering / GEMM to be performed on all b images at the same time. This results in significant CPU speedups compared to state-of-the-art tools that do not explore this trade-off, but use the same implementation that is suitable for the GPU for the CPU. Specifically, as Figure 3 shows, this allows us to view a CPU or GPU as simply a device that produces a LOPS."}, {"heading": "4. MULTI-DEVICE TRADEOFF", "text": "This year is the highest in the history of the country."}, {"heading": "4.1 Hardware Efficiency Model", "text": "In this case, it is as if it is a type of plague that will be able to regenerate, that will be able to regenerate, and that will be able to regenerate, \"he said in an interview with the New York Times."}, {"heading": "4.2 Statistical Efficiency Model", "text": "During our study, we discovered a theoretical characterization of this phenomenon that was surprising to us, since deep learning problems are seemingly impenetrable to the theory. We show that stalness can implicitly be considered a synchronous momentum - but with a stronger impulse term. We give a simplified version of this result that is sufficient to understand our optimizer, and describe a more detailed version in an accompanying theoretical paper [18]. More importantly, we validate the predictions of this model on a series of deep learning models, which in turn forms the basis of our optimizer. We make the following modeling assumptions that are not necessary, but help us to grasp the essence of the result and form the basis of our optimizer. The full model, detailed assumptions and evidence can be found in [18]. We make the three following simplifying assumptions. (A0) The batch used for each step is explicitly drawn by the replacement."}, {"heading": "5. DISTRIBUTED OPTIMIZER", "text": "This section uses the models and characterization of the target space of the two preceding sections to (i) create a physical mapping plan that assigns each server to a machine, and (ii) an execution strategy plan that defines the number of groups of calculations by assigning data packets to each server. As in previous sections, we assume a fixed number of machines. We first discuss the process of physical mapping and then describe our optimizer. We conclude with a theoretical and empirical justification of the optimizer and compare it with modern Bayesian approaches in Section 6."}, {"heading": "5.1 Physical Mapping", "text": "We note that in all CNNs, the architecture of Figure 5 (a) works best, and describe other physical maps and the search space in Appendix D.4. Omnivore maps the FC computing and model servers to the same machine, an approach we call merged FC. Merging FC computing and model servers to the same devices was shown in [7] to reduce the communication effort in a CPU cluster (improvement of HU), but it was not known that (1) this is also useful HU for a GPU cluster, and (2) this technique also benefits SE by eliminating instability in the FC model. These are both observations we are making. The remaining machines are used for the Conv servers. The Conv model servers are mapped to one of the Conv computing machines. These optimizations are critical: on a cluster of 33 EC2 c4.4xlarge machines, the merging of FC servers causes additional hardware communication from 1.2, as well as increased efficiency in a cluster of 2.5."}, {"heading": "5.2 Optimizer", "text": "Several factors affect the performance of the training method: (1) the number of computing groups; (2) the dynamics; (3) the dynamics; (3) the learning rate. As we have illustrated in Section 4.2, all of these three factors are interdependent. Furthermore, the optimal setting of these parameters may change during the training so that our optimizer periodically runs in periods (e.g., every hour). Algorithm 1 shows the end-to-end optimizer running after the initial cold start period (described in more detail in Appendix E.4). Algorithm 1 Automatic TradeoffInput Optimizer: Time budget T and possible decisions of (1) # calculates the groups CG, (2) Dynamics M, and (3) explicitly the learning rate H. Output: Trained Model W.1: g = CG 2: while we do not meet the termination criteria 3: (9)."}, {"heading": "6. EXPERIMENTS", "text": "We evaluate the runtime performance of our system and find that Omnivore outperforms state-of-the-art tools by 1.9 x to 12 x in a number of training tasks. (i) our result applies to a wide range of hardware, including CPUs and GPUs, (ii) both for individual devices and for multiple device / machine settings. Importantly, Omnivore does not require hyperparameter values. Section 4 \"s compromise analysis leads to the automatic optimizer in Section 5 that takes care of these decisions. Our experiments confirm that our optimizer delivers better results than manually set schedules and is up to 6 x faster than state-of-the-art Bayesian optimizers."}, {"heading": "6.1 Experiment Setup", "text": "We begin by describing the data sets, models and tasks as well as the hardware we run on the.Datasets and Models.We validate the performance of Omnivore on a variety of data sets and models, as shown in Figure 8. The largest corpus we use is ImageNet [9], which contains 1.3 million images. ImageNet is de facto the benchmark for deep learning systems [7, 8, 17]. Training a model on ImageNet can take dozens of hours - even on the latest 7 TFLOPS Titan X GPU, NVIDIA reports that it took three days to train with a single GPU.3. For some of our experiments, which require many configurations to converge, we used a reduced version, ImageNet8, which includes the first 8 tiles.We train the standard CaffeNet4 on both datasets. We also use smaller, but standard, Dataset CaffIAR and NetFIST to measure the network.We train the Omnivore required for both Networks."}, {"heading": "6.2 Performance Evaluation", "text": "This year, it is so far that it only takes one year to get there like never before."}, {"heading": "6.3 Tradeoff and Optimizer of Omnivore", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "7. RELATED WORK", "text": "In recent years, CNN's performance has become a well-researched problem. Popular libraries include Caffe [16], cuDNN [6], TensorFlow [1], Theano [2], and Torch10. To calculate confrontations, many of these frameworks use a lowering, an idea developed by Chellapilla et al. [4] This makes use of highly optimized BLAS libraries. Our work follows from this line of research and shows how to optimize the lowering of CPUs to build a system that relies on different types of hardware. Deep learning, that is, distributed systems for deep learning, is a popular topic, including SINGA [25], FireCaffe [15], Sparkutation, DL4J11, Sparkutation, Distance-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Belief-Below-Below-Below-Below-Below-Below-Strategies-Each and other-Below-Below-Below-Belief-Optimization."}, {"heading": "8. CONCLUSIONS", "text": "We described the first explicit study of the target space for deep learning systems, a popular, high-quality type of industrial learning systems. We identified critical problems in mapping layers to devices, and are the first to systematically investigate widespread techniques such as asynchrony. We designed a new optimizer and demonstrated that it has excellent end-to-end performance and is independent of our implementation substructure."}, {"heading": "9. ACKNOWLEDGEMENTS", "text": "The authors thank Dan Iter, Chris Aberger, and the rest of the Hazy Group for their feedback and assistance, and HyoukJoong Lee, Nadathur Rajagopalan Satish, Peter Bailis, and Benjamin Recht for their thoughtful comments. We thank Intel, Toshiba, and the Moore Foundation for their support along with DARPA through MEMEMEX (FA8750-14-2-0240), SIMPLEX (N6600115-C-4043), and XDATA (FA8750-12-2-0335) programs, and the Office of Naval Research (N000141210041 and N000141310129). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, ONR, or the U.S. government."}, {"heading": "10. REFERENCES", "text": "[1] Abadi et al. TensorFlow: Large-scale machine learning onheterogeneous network onheterogeneous deep distributed systems. arXiv, 2015. [2] J. Bergstra et al. Theano: a CPU and GPU math expression compiler. In SciPy, 2010. [3] L. Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade (2nd ed.). Springer, 2012. [4] K. Chellapilla et al. High performance convolutional neural networks for document processing. ICFHR, 2006. [5] T. Chen et al. Mxnet: A dynamic and efficient machine learning library for heterogeneous distributed systems. arXiv, 2015. [6] S. Chetlur et al. cuDNN: Efficient Primitives for Deep Learning. ArXiv, 2014. [7] T. Chilimbi et al. Project adam. Building an efficient and scalable deep learning system."}, {"heading": "APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. APPENDIX FOR INTRODUCTION (SECTION 1)", "text": "Tradeoff table. While many distributed deep learning systems exist, each of them makes design decisions that are appropriate for a particular type of (1) computing cluster, (2) deep learning model, and (3) dataset, although these decisions may not be appropriate for other issues or hardware resources. This is because deep learning is complex from both a computational and machine learning perspective, and the development of a highly efficient and scalable deep learning engine involves a number of interconnected design decisions. Table 1 shows a number of factors to be taken into account when designing distributed deep learning systems. In view of a fixed number of machines, there are a number of trade-offs between the use of hardware on each node, the mapping of data packages to machines, the type of deep learning models to be used to minimize communication. In addition, these decisions affect each other - for example, the parallel stack size of machines, the number of machines that can be used, the number of machines that can be used, the number of machines that can be used."}, {"heading": "B.1 CNN Computation", "text": "AlexNet FLOPS. We approach the # floating point operations in AlexNet by adding the sum of all GEMM operations with batch size 256. Specifically, we add 1 GEMM in the forward pass for each Conv and FC layer, plus two GEMMs in the backward pass for each Conv and FC layer (although Conv1 backwards only has 1 GEMM because no gradient is required in relation to the data).Terminology. This section introduces model and data parallelism as two techniques to parallelise CNNs. (For a full description of these concepts, see Terminology in Appendix D.2.CNN Trends. This section considers CNNs as two phases, Conv and FC. Recent CNNs, e.g. Residual Networks (ResNets) and Inception Networks, can also be included in this partitioning."}, {"heading": "B.2 Problem Definition", "text": "Our study uses the SGD algorithm because of its popularity, although our optimizer is applicable to other algorithms as well. Similarly, we do not modify the CNN architecture, but assume that it is provided by the user. Terminology. The physical mapping forms the layer calculation to vertices in G. vertices in G may contain other vertices, such as GPUs or CPU cores within a machine. Section 3 first examines how to map CNN to hardware within a machine, and concludes that if optimized properly, only the throughput of each vertice is important, not the underlying hardware. Section 4 then examines how to map CNN to all G, i.e. across machines."}, {"heading": "C. APPENDIX FOR SINGLE-NODE TRADEOFFS (SECTION 3)", "text": "This section describes the physical plan of the optimizer (how to map CNN to hardware) at the level of a single machine. Given a machine that contains devices with different throughput (CPUs, GPUs), our goal is to perform the CNN calculation as quickly as possible by identifying two trade-offs: our first trade-off introduces a data processing technology that accounts for computing time. We show that this trade-off results in > fivefold CPU acceleration over existing systems. With this optimization, both CPUs and GPUs now offer throughput proportional to the FLOPS provided by the device, and our second trade-off partitions CNN calculation to both the CPU and the GPU to combine their FLOPS - a well-known HPC trick, but not applied to CNNs."}, {"heading": "C.1 Convolutional Layer Computation", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "C.2 Batching and Data Parallelism", "text": "This year it is more than ever before in the history of the city."}, {"heading": "C.3 Device Throughput", "text": "FLOPS experiments. Figure 3 shows throughput for CNN when using Caffe, Omnivore and also for reference a GEMM kernel with a single precision. For the GEMM experiment, we used OpenBLAS and 16384 \u00d7 16384 matrices. For the GPU GEMM, we used a GEMM kernel from the CUDA examples of NVIDIA. For Caffe and omnivores, we focus only on the folding layers, especially the time to run back and forth on all 5 layers of CaffeNet.FLOPS. The c4.4xlarge instance CPU FLOPS contains a Haswell CPU with 8 physical cores \u00d7 2.9 GHz \u00d7 32 = 0.742 TFLOPs, 32 of which are the Haswell instructions with a single precision per cycle (8-Float SIMD \u00d7 2 FMA per cycle, Grid \u00d7 6MA \u00d7 1.5PLOPs = 1.5PU)."}, {"heading": "C.4 FLOPS-Proportional Scheduling", "text": "Given that both processor and GPU speeds are now proportional to the device's FLOPS value, we next consider whether the CPU and GPU can be used to process each layer at the same time, using data parallelism (batch is partitioned, model is replicated) for all layers in the folding phase, which is computer-bound and contains small data; the trade-off is which fraction of the batch should be given to each device. We choose the simple but optimal option that a device should process a fraction p of the input, where p is the fraction of the total FLOPS that the device provides. For example, if a CPU provides 1 TFLOPS and a GPU 4 TFLOPS, 1 / 5 of the batch is processed by the CPU, giving an ideal acceleration of 20% over the GPU alone."}, {"heading": "C.5 Single-Node Experiments", "text": "It accepts the same input files as Caffe and produces the same output files as Caffe. Our experiments are comparable to Caffe and use the configuration files provided by Caffe. We remove the grouping for conversion layers because the full AlexNet fits into the memory of a single K520 (g2.2xlarge). We use the same external libraries for both Caffe and CUDA7.5. For Caffe, we report both cuDNN v3 and v4.We built the same time span in Caffe and Omnivore."}, {"heading": "D. APPENDIX FOR DISTRIBUTED", "text": "TRADEOFFS (SECTION 4) After examining the trade-offs for a single machine, this section examines the distributed setting. The goal of this section is to create an optimizer that (1) creates a physical plan P (A, G) that maps the CNN architecture A to machines in the device diagram G. This section begins with a description of why these two are the most important tasks for the optimizer. (2) Although many distributed CNN systems exist [5, 7, 8, 15, 19, 25] and each describes its own distribution techniques, when analyzing these strategies we find that they are different, but all described either (1) or (2) above. Given these two basic design dimensions, we then classify existing strategies into a target space and formulate the target within that space."}, {"heading": "D.1 Distributed CNN Tradeoff Space", "text": "In fact, most of them will be able to survive on their own, without being able to survive on their own."}, {"heading": "D.2 Terminology", "text": "This year, it is more than ever before in the history of the city, in which it has come as far as never before in the history of the city."}, {"heading": "D.3 Existing Systems", "text": "In fact, most of them are able to trump themselves by trumping themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves."}, {"heading": "D.4 Hardware Efficiency Model", "text": "The reason for this is that the number of people who are able, is able to move, is able to move, is able to move, is able to move, is able to move, is able to move, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to move, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to be, is able to decide itself."}, {"heading": "D.5 Statistical Efficiency Model", "text": "Since asynchrony can be regarded as an increase in the implicit impulse, asynchrony can be equated to synchronous execution by correctly decreasing the explicit impulse so that the total explicit + implicit impulse remains the same as the optimal impulse of the synchronous case. This is so long as the implicit impulse is smaller than the optimal impulse of the synchronous case. This is a key finding, because it means that durability can exist in the system without causing a statistical penalty, which is beneficial for hardware efficiency. Furthermore, it is important to leave the impulse equal (instead of simply ignoring this result and generating additional dynamics), because an excessively high total impulse deviates, which we show experimentally in Appendix E. This theory also successfully predicts the measured behavior of the system: Figure 6 shows the predicted and actually measured dynamics for several popular deep learning problems. In both cases, after decreasing the implicit impulse, the synchronous impulse is not imposed on the first SE."}, {"heading": "E. APPENDIX FOR DISTRIBUTED OPTIMIZER (SECTION 5)", "text": "This section describes how to use the models from the two previous sections to (1) select a physical mapping that maps each server to a machine, and (2) execute a strategy that defines the number of calculation groups by assigning data packets to each server. As in previous sections, we assume that the number of machines is set."}, {"heading": "E.1 Selecting the Batch Size", "text": "The x-axis varies the batch size, and the y-axis records the transitions from the dataset (or epochs) to convergence. For each batch size, we used an oracle to determine the optimal learning rate, depending on the divergence."}, {"heading": "E.2 Physical Mapping", "text": "As discussed in the text, for the physical map that maps servers to machines, we map the FC calculation and model servers to the same machine (i.e., we \"merge\" the FC servers, which, as [7] argues, reduces the communication effort because it avoids the communication of the large FC model) and use a computation group for the FC phase. The rest of the machines are used for the conv computation servers. The conv model servers are mapped to one of the conv computation machines. Empirically, we show in Appendix F. 3 that this mapping is best for both hardware and statistical efficiency: On a cluster of 33 EC2 c4.4xlarge machines, the non-merging of the FC servers results in an additional hardware efficiency loss of 1.2 \u00d7 due to increased communication and a statistical efficiency loss of 2.5 \u00d7 due to durability in the FC model. We describe this result further in the experimental results of Appendix F. Our current optimizer therefore always chooses this computation server for this 4.5 architecture (although this section is D5)."}, {"heading": "E.3 Optimizer Details", "text": "For each epoch, Algorithm 1 performs an adaptive grid search on the learning rate and impulse, starting with the current value of g. Specifically, we perform each learning rate and impulse (see below) for one minute and select the configuration with the lowest loss after 1 minute of execution. If after 1 minute all these configurations show the same loss, we continue for another minute until there is a clear winner in terms of loss (we determine this by a threshold of 5% from the loss of the past 50 iterations, although a statistical test can also be used). We then continue this best (\u00b5, \u0438) for one hour until there is a clear winner in terms of loss. One could use more complex parameter search routines, but this took less than 10% of the execution time. We seek the learning rate as follows. Let's leave the learning rate and the dynamics used in the previous 1 hour (i.e.) The result of the grid search from this epoch will be (in the last epoch)."}, {"heading": "E.4 Cold Start Phase", "text": "This is necessary to determine the appropriate scale for the weights. However, a fully synchronous execution can significantly increase the duration of the cold start phase due to lack of hardware efficiency, and thus a cold start run with slight asynchronicity can be completed faster. Therefore, a fully synchronous execution of the cold start phase is also important for the cold start phase. To do this, as in algorithm 1, we look for grid hyper parameters for any number of groups (1, 2, 4, for N machines)."}, {"heading": "F. APPENDIX FOR DISTRIBUTED EXPERIMENTS (SECTION 6)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 Single-Node Experiments", "text": "See Annex C.5."}, {"heading": "F.2 Small Cluster Experiments", "text": "As a matter of fact, most of us are in a position to abide by the rules which they have imposed on themselves. (...) It is not as if they are in a position to change the rules. (...) It is not as if they take themselves into their own hands. (...) It is not as if they are in a position to change the rules. (...) It is not as if they place themselves within the limits. (...) It is not as if they place themselves within the limits of the limits. (...) It is as if they place themselves within the limits of the limits. (...) It is as if they place themselves within the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits of the limits. \"(...)"}, {"heading": "F.3 Detailed Tradeoff Space Analysis", "text": "In fact, it is the case that this is a type and manner in which people are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "F.4 Scalability to a Larger Cluster", "text": "In this last experiment, we compare Omnivore with the best competitor from the small clusters, MXNet, on that larger cluster. We use the same dataset and network as the small cluster experiments and define convergence in the same way (99% accuracy).We try to open a cluster of 33 g2.8xlarge instances, but we continuously run into EC2 errors related to insufficiently available machines. InsufficientInstanceCapacity).As in Section 6.2, we repeat the same procedure to apply our optimizer to MXNet, i.e. we run each configuration for 10 minutes, select the best execution strategy and learning rate, and run that convergence is synchronized with convergence again. The best strategy was to synchronize again with Section 6.2, we repeat the procedure to apply our optimizer to MXNet, i.e."}, {"heading": "F.5 End-To-End Experiments", "text": "The end-to-end result is in Figure 10. We trained the standard CaffeNet (same setting as in Appendix F. 2) with ImageNet-1000 on both systems with CPU-L and GPU-S clusters. We suspended each run after 8 hours and reported on training accuracy vs. time.According to the official MXNet 22 performance optimization guideline, they recommend trying the sync strategy, but also noting that \"if the model size is quite large or you are using a large number of machines, you want to use dist async.\" Immediately above, they describe large models with the size > 100MB like AlexNet and VGG. Since we train AlexNet and use up to 33 machines that can be considered large, then async may be the best choice according to these instructions. As they do not provide an automatic mechanism to make this decision, we followed both strategies as in paragraph.2. This required adjusting the learning rate for each strategy we worked out."}, {"heading": "F.6 Preliminary RNN/LSTM Result", "text": "To understand whether our compromise is broader, we have implemented the Recurrent Neural Network Model and LSTM proposed by Graves [11]. Following the same protocol as Figure 28 and using the CPU-S cluster, Figure 32 shows that the compromise between statistical efficiency and hardware efficiency is comparable, and the choice of a fully synchronous or asynchronous configuration can be up to 2 x slower than the optimal configuration. 23"}, {"heading": "F.7 Comparison to Standard Schedules", "text": "Next, we confirm the hypothesis that the Omnivore Optimizer outperforms the standard tuning and parameter planning methods. To confirm this, we run Omnivore on the full ImageNet using the default CaffeNet. We run two versions of Omnivore: (1) Omnivore (Standard Schedule), the CaffeNet's default learning rate schedule23 We also see a similar trade-off in the LSTM variant proposed by Graves [11], which reduces the learning rate every 100,000 iterations tenfold; and (2) Omnivore, which uses the default Omnivore Optimizer. To be fair, both versions use the same search strategies to select the optimal learning rate, dynamics, and number of calculation groups at the beginning. In addition, we run Omnivore for 10 x the optimization time before optimising the default parameters again. Figure 33 shows the loss of training time compared to the Omnivore (The Omnivore responds faster to the training clock)."}, {"heading": "F.8 Comparison to Bayesian Optimizer", "text": "We compare our simple optimizer with the state-of-the-art Bayesian optimization approach, which explores the parameter space of CNNs. Results are presented in Figure 34. We follow Snoek et al. [22] to model the search space as (\u03b7, \u00b5, S, N), where N is the number of epochs to run. We use the same search space for (\u03b7, \u00b5, S) as in our optimizer, and measure both the number of configurations and the total number of epochs that the Bayesian optimizer must execute before we find a run that achieves an accuracy within 1% of the best loss after 1000 seconds. Our procedure is as follows: We run omnivors to achieve a run that achieves 99% convergence with the same dataset and cluster as in Figure 12 (b). This has taken 80 epochs during which an accuracy within 1% of the best loss is achieved, we then give a shorter interval of 80 years to achieve the bayesian loss = 1 N."}, {"heading": "G. APPENDIX STUDYING TOTAL COST", "text": "In fact, most of them will be able to get to the top without being able to get to the top."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We perform a study of the factors affecting training time in<lb>multi-device deep learning systems. Given a specification of<lb>a convolutional neural network, we study how to minimize<lb>the time to train this model on a cluster of commodity CPUs<lb>and GPUs. Our first contribution focuses on the single-node<lb>setting, in which we show that by using standard batching<lb>and data-parallel techniques throughput can be improved<lb>by at least 5.5\u00d7 over state-of-the-art systems when training<lb>on CPUs. This ensures an end-to-end training time directly<lb>proportional to the throughput of a device regardless of its<lb>underlying hardware, allowing each node in the cluster to be<lb>treated as a black box. Our second contribution is a theoret-<lb>ical and empirical study of the tradeoffs affecting end-to-end<lb>training time in a multiple-device setting. We identify the<lb>degree of asynchronous parallelization as a key feature af-<lb>fecting both hardware and statistical efficiency. We show<lb>that asynchrony can be viewed as introducing a momentum<lb>parameter, which we use to limit our search space; in turn,<lb>this leads to a simpler optimizer, which is our third contri-<lb>bution. Our optimizer involves a predictive model for the<lb>total time to convergence and selects an allocation of re-<lb>sources to minimize that time. We demonstrate that the<lb>most popular distributed deep learning systems fall within<lb>our tradeoff space but do not optimize within the space. By<lb>doing such optimization, our prototype runs 1.9\u00d7 to 12\u00d7<lb>faster than the fastest state-of-the-art systems.", "creator": "LaTeX with hyperref package"}}}