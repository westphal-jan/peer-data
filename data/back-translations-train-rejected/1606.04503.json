{"id": "1606.04503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization", "abstract": "This paper describes the Georgia Tech team's approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.", "histories": [["v1", "Tue, 14 Jun 2016 19:00:59 GMT  (61kb,D)", "http://arxiv.org/abs/1606.04503v1", "describes our system at the CoNLL 2016 shared task"]], "COMMENTS": "describes our system at the CoNLL 2016 shared task", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akanksha", "jacob eisenstein"], "accepted": false, "id": "1606.04503"}, "pdf": {"name": "1606.04503.pdf", "metadata": {"source": "CRF", "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization", "authors": ["Jacob Eisenstein"], "emails": ["akanksha271@gmail.com", "jacobe@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Our approach to classifying discourse relationships is to combine strong surface features with a distributed representation of each discourse unit, following previous work that has shown that distributed representations can generalize this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).We combine these two distinct representations in a neural network architecture.Our approach is shaped by two major design decisions: the use of recursive long-term memories (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayean optimization (Snoek et al., 2012) to optimize the neural network architecture."}, {"heading": "2 System Overview", "text": "The overall architecture is shown in Figure 1. The same architecture is used for both explicit and non-explicit relationships, but with different parameters. The output of the classifier is a softmax layer that takes as input a series of dense layers, which allow nonlinear interactions between surface features and elements of the distributed representation. Dropout is designed to reduce overadjustments (Srivastava et al., 2014). The overall architecture is trained to minimize crossentropy. Implementation is done in keras (Chollet, 2015), and training takes several hours on a standard CPU. We will now describe the subcomponents of the classifier in detail."}, {"heading": "2.1 Distributed representations for discourse units", "text": "This component of the model is shown in the dotted part of Figure 1 for the first discourse argument. Previous work has explored a variety of ways to induce representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015). We follow a recursive approach of neural networks that characterizes Xiv: 160 6.04 503v 1 [cs.C L] 14 Jun 2016each discourse unit through a recursively updated state vector (Li et al., 2015), with the input consisting of pre-formed word embedding GoogleNews vector negative 300.bin from the word2vec page."}, {"heading": "2.2 Surface features", "text": "In addition to the distributed representations of the discourse units, we use some of the most successful surface features of previous work. These features are implemented with the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al., 2011). Generally, these features were inspired by the Wang and Lan system (2015), which performed best in the PDTB test of the joint task 2015 (Xue et al., 2015)."}, {"heading": "2.2.1 Features for explicit relations", "text": "Linking Text The linking element itself is a strong feature for the sensual classification of explicit discourse relationships (Pitler et al., 2008), which alone gives our classification an F1 value of 0.8862. Sentiment Value The analysis package Vader Sentiment (Hutto and Gilbert, 2014) was used to calculate the sentiment score for both arguments, and the feature then reports whether the two arguments have the same sense.Trigram characteristics were used for the last three words of Arg1 and the first three words of Arg2."}, {"heading": "2.2.2 Features for non-explicit relations", "text": "We used the same trigram attributes from the explicit relationship classifier as well as the following 1https: / / code.google.com / archive / p / word2vec / additional attributes on pairs of linguistic elements in arg1 and arg2.Word PairWe created word pairs from the cross-product of all words appearing in arg1 and arg2, following much of the previous work in discourse sparsing (Marcu and Echihabi, 2003; Pitler et al., 2009), and then replaced the words in each pair with a cluster identity (Rutherford and Xue, 2014). Specifically, we used the GoogleNews vector negative word embedding in the skipgram to form 1000 clusters. Part-of-Speech Pair.Part-Speech Pairs We also formed partial repairs for non-relevant add-pairs from the tags appearing in the two arguments (Rutherford and Xue, 2014)."}, {"heading": "2.3 Hyperparameter tuning", "text": "The best set of hyperparameters for the classifiers was found using the search algorithm GPEIOptChooser Markov Chain Monte Carlo (Snoek et al., 2012), an algorithm that was sampled from the space of hyperparameters while trying to learn a function from hyperparameters on overall performance. We use cross-parameter (not formula 1) as a general performance target; due to the time required to train the model, we could only perform twenty samples that took several days. Progress of this search is illustrated in Figure 2, which shows that the best hyperparameter configuration was identified at the sixth sample. Further samples may have led to better performance, but this was not possible under the time constraints of the common task. The best set of hyperparameters for classifying non-explicit discourse relationships is listed in Table 1."}, {"heading": "3 Evaluation", "text": "Table 3 compares the performance of our system with the best systems from this year's joint task. Our system was particularly competitive in the blind test. (The best performance for non-explicit relationships in the blind test set came from ttr, but this system did not attempt to classify explicit relationships and therefore did not receive an overall score for all relationships.) This indicates that our approach suffered less from compliance with the dev data. On the other hand, the performance of our system for explicit relationships was further behind the best systems, indicating the need for additional features to handle this case. Results are broken down by relationship type in Table 4, also using the evaluation script for the joint task. In addition, the contribution of each feature is given in Table 2, where the features are gradually added to a base model that contains only the distributed representations of each argument.We thank the organizers of the joint task for formalizing this evaluation and thank Yang for the helpful phase of the project."}], "references": [{"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, California.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 2201\u20132211, Lisbon, September.", "citeRegEx": "Braud and Denis.,? 2015", "shortCiteRegEx": "Braud and Denis.", "year": 2015}, {"title": "Keras", "author": ["Franois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Vader: A parsimonious rule-based model for sentiment analysis", "author": ["Clayton J Hutto", "Eric Gilbert"], "venue": null, "citeRegEx": "Hutto and Gilbert.,? \\Q2014\\E", "shortCiteRegEx": "Hutto and Gilbert.", "year": 2014}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "One vector is not enough: Entity-augmented distributional semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Transactions of the Association for Computational Linguistics (TACL), June.", "citeRegEx": "Ji and Eisenstein.,? 2015", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2015}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 343\u2013351, Singapore.", "citeRegEx": "Lin et al\\.,? 2009", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["Daniel Marcu", "Abdessamad Echihabi."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 368\u2013375.", "citeRegEx": "Marcu and Echihabi.,? 2003", "shortCiteRegEx": "Marcu and Echihabi.", "year": 2003}, {"title": "Easily identifiable discourse relations", "author": ["Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind Joshi."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING), pages 87\u201390, Manchester,", "citeRegEx": "Pitler et al\\.,? 2008", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Emily Pitler", "Annie Louis", "Ani Nenkova."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Suntec, Singapore.", "citeRegEx": "Pitler et al\\.,? 2009", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Attapol T Rutherford", "Nianwen Xue."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Rutherford and Xue.,? 2014", "shortCiteRegEx": "Rutherford and Xue.", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan Prescott Adams."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A refined endto-end discourse parser", "author": ["Jianxiang Wang", "Man Lan."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task, pages 17\u201324, Beijing, China, July. Association for Computational", "citeRegEx": "Wang and Lan.,? 2015", "shortCiteRegEx": "Wang and Lan.", "year": 2015}, {"title": "The CoNLL-2015 shared task on shallow discourse parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol T Rutherford."], "venue": "Proceedings of the Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Xue et al\\.,? 2015", "shortCiteRegEx": "Xue et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 7, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 1, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 4, "context": "Our approach is shaped by two main design decisions: the use of long short-term memory recurrent networks (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayesian optimization (Snoek et al.", "startOffset": 106, "endOffset": 140}, {"referenceID": 15, "context": "Our approach is shaped by two main design decisions: the use of long short-term memory recurrent networks (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayesian optimization (Snoek et al., 2012) for tuning the neural network architecture.", "startOffset": 228, "endOffset": 248}, {"referenceID": 16, "context": "to reduce overfitting (Srivastava et al., 2014).", "startOffset": 22, "endOffset": 47}, {"referenceID": 2, "context": "The implementation is in Keras (Chollet, 2015), and training takes several hours on a standard CPU.", "startOffset": 31, "endOffset": 46}, {"referenceID": 6, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 1, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 8, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015).", "startOffset": 221, "endOffset": 263}, {"referenceID": 7, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015).", "startOffset": 221, "endOffset": 263}, {"referenceID": 9, "context": "each discourse unit by a recurrently-updated state vector (Li et al., 2015), with the input consisting of pre-trained word embeddings GoogleNews-vectors-negative300.", "startOffset": 58, "endOffset": 75}, {"referenceID": 4, "context": "1 Specifically, our recurrent architecture is a long short-term memory (LSTM), which uses a combination of gates to better handle long-term dependencies, as compared with the more straightforward recurrent neural network (Hochreiter and Schmidhuber, 1997).", "startOffset": 221, "endOffset": 255}, {"referenceID": 3, "context": "Following Graves and Schmidhuber (2005), we employ a bidirectional LSTM, in which each training sequence is presented forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer.", "startOffset": 10, "endOffset": 40}, {"referenceID": 0, "context": "These features are implemented using the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 18, "context": "In general, these features were inspired by the system from Wang and Lan (2015), which obtained best performance on the PDTB test set in the 2015 shared task (Xue et al., 2015).", "startOffset": 158, "endOffset": 176}, {"referenceID": 0, "context": "These features are implemented using the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al., 2011). In general, these features were inspired by the system from Wang and Lan (2015), which obtained best performance on the PDTB test set in the 2015 shared task (Xue et al.", "startOffset": 67, "endOffset": 209}, {"referenceID": 12, "context": "1 Features for explicit relations Connective Text The connective itself is a strong feature for sense classification of explicit discourse relations (Pitler et al., 2008).", "startOffset": 149, "endOffset": 170}, {"referenceID": 5, "context": "Sentiment Value The Vader Sentiment analysis package (Hutto and Gilbert, 2014) was used to calculate sentiment score for both arguments.", "startOffset": 53, "endOffset": 78}, {"referenceID": 11, "context": "Word Pairs We formed word pairs from the cross product of all words appearing in arg1 and arg2, following much of the prior work in discourse parsing (Marcu and Echihabi, 2003; Pitler et al., 2009).", "startOffset": 150, "endOffset": 197}, {"referenceID": 13, "context": "Word Pairs We formed word pairs from the cross product of all words appearing in arg1 and arg2, following much of the prior work in discourse parsing (Marcu and Echihabi, 2003; Pitler et al., 2009).", "startOffset": 150, "endOffset": 197}, {"referenceID": 14, "context": "We then replaced the words in each pair with a cluster identity (Rutherford and Xue, 2014).", "startOffset": 64, "endOffset": 90}, {"referenceID": 14, "context": "Part-of-Speech Pairs Similarly, we formed partof-speech pairs from the tags appearing in the two arguments (Rutherford and Xue, 2014).", "startOffset": 107, "endOffset": 133}, {"referenceID": 10, "context": "Production Rules Pairs Using the syntactic analysis of each argument, we form pairs of production rules appearing in the two arguments (Lin et al., 2009).", "startOffset": 135, "endOffset": 153}], "year": 2016, "abstractText": "This paper describes the Georgia Tech team\u2019s approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.", "creator": "TeX"}}}