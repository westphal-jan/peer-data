{"id": "1606.04506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Max-Margin Feature Selection", "abstract": "Many machine learning applications such as in vision, biology and social networking deal with data in high dimensions. Feature selection is typically employed to select a subset of features which im- proves generalization accuracy as well as reduces the computational cost of learning the model. One of the criteria used for feature selection is to jointly minimize the redundancy and maximize the rele- vance of the selected features. In this paper, we formulate the task of feature selection as a one class SVM problem in a space where features correspond to the data points and instances correspond to the dimensions. The goal is to look for a representative subset of the features (support vectors) which describes the boundary for the region where the set of the features (data points) exists. This leads to a joint optimization of relevance and redundancy in a principled max-margin framework. Additionally, our formulation enables us to leverage existing techniques for optimizing the SVM objective resulting in highly computationally efficient solutions for the task of feature selection. Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation. We use a sparse representation to deal with data in very high dimensions. Experiments on seven publicly available benchmark datasets from a variety of domains show that our approach results in orders of magnitude faster solutions even while retaining the same level of accuracy compared to the state of the art feature selection techniques.", "histories": [["v1", "Tue, 14 Jun 2016 19:05:01 GMT  (311kb)", "http://arxiv.org/abs/1606.04506v1", "submitted to PR Letters"]], "COMMENTS": "submitted to PR Letters", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yamuna prasad", "dinesh khandelwal", "k k biswas"], "accepted": false, "id": "1606.04506"}, "pdf": {"name": "1606.04506.pdf", "metadata": {"source": "META", "title": "Max-Margin Feature Selection", "authors": ["Yamuna Prasada", "Dinesh Khandelwala", "K. K. Biswasa"], "emails": ["yprasad@cse.iitd.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.04 506v 1 [cs.L G] 14 Jun 20Many machine learning applications, such as vision, biology and social networks, deal with high-dimensional data. Feature selection is typically used to select a subset of characteristics that improves generalization accuracy and reduces the computational cost of learning the model. One of the criteria for feature selection is to jointly minimize redundancy and maximize the relevance of the characteristics selected. In this paper, we formulate the task of feature selection as an SVM problem of a class in a space where characteristics correspond to data points and instances correspond to dimensions. The goal is to search for a representative subset of characteristics (support vectors) that describes the boundary for the region where the set of characteristics (data points) exists, resulting in a common optimization of relevance and redundancy in a principled max-point data framework that enables us to use existing data efficiently for our selection (2008 M)."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to set out in search of a solution that originates in the real world."}, {"heading": "2. Proposed Max-Margin Framework", "text": "The main objective in selecting the characteristics is to select a subset of characteristics that are highly relevant (i.e. highly predictable) and non-redundant (i.e. uncorrelated). Relevance is measured either using an explicit metric (e.g. the correlation between a characteristic and the target variables) or implicitly using the accuracy of the classifier on the selected subset of characteristics. Redundancy is measured using metrics such as correlation coefficient or mutual information. Most of the existing methods for selecting characteristics are based on a paired notion of similarity in order to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003). We try to answer the question \"Is there a principled approach to collectively assessing the relevance and redundancy between characteristics?.\" To do this, we leaf around the problem and examine the space in which characteristics themselves become hyperformality."}, {"heading": "2.1. Formulation", "text": "Let X represent the data matrix in which each row vector xiT (i-1. >. M) denotes an instance and each column vector f-j (j-1. \u2212 N) denotes a feature vector. We will use \u03c6 to denote a feature map in such a way that the dot product between the data points can be calculated by a kernel (xi, x-j) = properties of selected properties (xi) T\u03c6 (x-j), which can be interpreted as similar to xi and x-j. We will use Y to denote the vector of the class names yi (i-1.). M) Based on the above notations, we will present the following formulation for feature selection in the primal: min w, b 2wT w + C N, subject wT\u03c6 (fi) + b-a, ri-i, that is the differentiation of the plane."}, {"heading": "2.2. Dual Formulation", "text": "In order to solve the MMFS optimization efficiently by the dual coordinate descent strategy, we need both the primary and the dual formula. The dual formula for equation 1 can be derived using the Lagrange method. \u2212 The Lagrange function L (w, b, \u03b1, \u03b2) can be written as follows: L (w, b, \u03b1, \u03b2) = min w, b 2wT w + b + C N. \u2212 The Lagrange multipliers can now be written as follows: max \u03b1 + N = 1\u03b1i (ri \u2212 i \u2212 (wT\u03c6 (fi) + b))) \u2212 N [n] [n] [n] i = 1\u03b2i [n] and \u03b2i \"s are the Lagrange multipliers. \u2212 The Lagrange dual function can now be written as follows: max \u03b1, \u03b2: i [n]."}, {"heading": "2.3. Choice of Metrics", "text": "The relevance of a trait in our framework is measured by the correlation between the trait vector and the class identification vector. In our experiments, we normalized both the data and the target vector (class identifiers) so that the correlation between the trait vector and the target vector (normalized) has zero mean and unit variance. Therefore, the point product between the trait vector and the target vector (normalized) could also be used (Peng et al., 2005). Redundancy is usually measured by correlation or mutual information in trait selection tasks (Peng et al., 2005). Within our framework, the Dot product space (kernel) can detect the similarity (redundancy) between the traits and the required genetic equality."}, {"heading": "2.4. Dual Coordinate Descent for MMFS", "text": "The following equation (1), the number of variables and the number of constraints in the primary formulation are M + 1 and 2N, respectively, while from equation (6), it is seen that the corresponding numbers are N and 2N + 1, respectively, marginal. Solving the dual use requires O (N2) space and O (N3) time. Even solving the dual use for sequential minimum optimization (SMO) based methods in practice has the complexity of O (N2) (Fan et al., 2005). These high time and memory complexities limit the scalability of the primary or dual data with a very large number of instances and systems. In many cases, if the data is already present in a rich feature space, the performance of linear SVMs will be similar to that of inefficient data."}, {"heading": "2.5. Complexity", "text": "Following (Hsieh et al., 2008), the MMFS-DCD approach obtains an error-accurate solution in O (log (1 / \u0430) number of iterations. Time complexity of a single iteration is O (MN). Memory complexity of the DCD algorithm is O (NM). For sparse datasets, the complexity depends on N instead of N, where N \u0441 is the average number of characteristic values that do not deviate from zero. Details for proof of convergence are available in (Hsieh et al., 2008). 5"}, {"heading": "3. Relationship to Existing Filter Based Methods", "text": "The question whether these are reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary,"}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "We demonstrate our experiments on seven publicly available benchmark datasets of medium to large dimensions. Of these seven datasets, leukemia, RAOA and RAC are microarray datasets (Kumar et al., 2012), MNIST is a vision dataset (Tan et al., 2010) and REAL-SIM, webspam and Kddb are text classification datasets from the NLP domain (Chang et al., 2010; Yiteng et al., 2012). Table 1 describes the details of the datasets. The last column represents the sparseness representing the average number of non-zero features per instance in the dataset."}, {"heading": "4.2. Algorithms", "text": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al., 2010) and two other methods for selecting embedded features, namely Feature Generating Machine (FGM) (Tan et al., 2010) and Group Discovery Machine (GDM) (Yiteng et al., 2012). FGM also uses a cutting-level strategy for selecting features. GDM also tries to minimize redundancy in FGM by taking into account the correlation between the features. QPFS, FGM and GDM have shown that they exceed a variety of existing methods for selecting features, including mRMR and MaxRel (Peng et al., 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc. For QPFS, we used mutual information (MI as the best-scale MF)."}, {"heading": "4.3. Methodology", "text": "For all records except webspam and kddb, we report on the accuracies obtained for different number of top K attributes (K = {2, 3, 4,.., 100}) selected for each of the methods. For webspam and kddb records, we report on the accuracies obtained for different number of top K attributes (K = {5, 10, 20, 30,.., 200}) selected by FGM, GDM and MFS-DCD records. We also report on the best records obtained for all records at each default value of K. We have all records except webspam and kddb to zero mean and unit variance. The zero mean and unit variance of the records are standardized."}, {"heading": "4.4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1. Accuracy", "text": "Table 2 represents the best set of average accuracies (the number of selected Top-K characteristics varying) for all methods. QPFS method does not have results for RAOA and RAC datasets within 24 hours6. Therefore, we used Nystro-m approximation (Rodriguez-Lujan et al., 2010) with sampling rate (\u03c1 = 0.01) for these datasets. In Figure 2 (a), QPFS-N represents the QPFS with Nystro-m approximation. The QPFS and FCBF methods4http: / / www.csie.ntu.edu.tw / cjlin / liblinear 5http: / / www.c2i.ntu.edu.sg / mingkui / FGM.htm 6We put a dash \u2212 with corresponding entries in Table 2.can supplementable data of FGM, so we compare FGM, GDM-DaconCD for web.sg / FGM.htm 6We do not process the corresponding data in Table 2.can supplementable data of FGM."}, {"heading": "4.4.2. Time", "text": "The time required for MMFS-DCD, FCBF and QPFS is independent of the number of selected features. For FGM and GDM, the time required increases monotonously with K. For GDM, the time required increases sharply when K becomes greater than five7. Figure 2 shows that MMFS-DCD is several orders of magnitude faster than all other algorithms on all datasets 8."}, {"heading": "4.4.3. Parameter Sensitivity Analysis", "text": "Figure 3 shows the different accuracy of MMFS-DCD on the leukemia dataset because we vary the regularization parameter (\u03b3) with different number of top-k characteristics. Accuracy is not very sensitive to \u03b3, as a large flat region in the graph shows."}, {"heading": "5. Conclusion and Future Work", "text": "We have presented a novel Max Margin Framework for Feature Selection (MMFS), which resembles an SVM formulation of a class. Our framework offers a principled approach to maximizing relevance and minimizing redundancy together. Furthermore, us7For RAC allows GDM to perform up to 20 iterations. 8 plots for remaining data sets are available in complementary files. 7 To utilize existing SVM-based optimization techniques that lead to highly efficient solutions for feature selection task. Our experiments show that MMFS with a dual, decent approach is many orders of magnitude faster than the existing state of the art while maintaining the same level of accuracy. One of the most important future directions involves investigating whether there is any idea of a generalization tied to the feature selection task in our framework, as in the case of SVMs for the classification task. In other words, what we can say more about the quality and what we can say about the features selected."}, {"heading": "Acknowledgment", "text": "The authors thank Dr. Parag Singla, Dept. of CSE, I.I.T Delhi for his valuable suggestions and support in improving the paper."}], "references": [{"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J.C. Burges"], "venue": "Data Min. Knowl. Discov", "citeRegEx": "Burges,? \\Q1998\\E", "shortCiteRegEx": "Burges", "year": 1998}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Y.W. Chang", "C.J. Hsieh", "K.W. Chang", "M. Ringgaard", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Working set selection using second order information for training support vector machines", "author": ["R.E. Fan", "P.H. Chen", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Fan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2005}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A max margin framework on image annotation and multimodal image retrieval", "author": ["Z. Guo", "Z. Zhang", "E.P. Xing", "C. Faloutsos"], "venue": "in: ICME,", "citeRegEx": "Guo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2007}, {"title": "An intoduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C.J. Hsieh", "K.W. Chang", "C.J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "in: Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Design of fuzzy expert system for microarray data classification using a novel genetic swarm algorithm", "author": ["P.G. Kumar", "A.T.A. Victoire", "P. Renukadevi", "D. Devaraj"], "venue": "Expert Syst. Appl", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Feature selection using one class svm: A new perspective, in: MLCB", "author": ["Y. Prasad", "K. Biswas", "P. Singla"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2013}, {"title": "Quadratic programming feature selection", "author": ["I. Rodriguez-Lujan", "R. Huerta", "C. Elkan", "C.S. Cruz"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Rodriguez.Lujan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rodriguez.Lujan et al\\.", "year": 2010}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": "Advances in neural information processing systems", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "in: Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "in: Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Tan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Discovering support and affiliated features from very high dimensions", "author": ["Z. Yiteng", "T. Mingkui", "O. Yew S", "T. Ivor W"], "venue": "in: Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Yiteng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yiteng et al\\.", "year": 2012}, {"title": "Feature selection for high-dimensional data: A fast", "author": ["L. Yu", "H. Liu"], "venue": null, "citeRegEx": "Yu and Liu,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation.", "startOffset": 62, "endOffset": 82}, {"referenceID": 6, "context": "In filter based methods, features (or subset of the features) are ranked based on their statistical importance and are oblivious to the classifier being used (Guyon and Elisseeff, 2003; Peng et al., 2005).", "startOffset": 158, "endOffset": 204}, {"referenceID": 9, "context": "In filter based methods, features (or subset of the features) are ranked based on their statistical importance and are oblivious to the classifier being used (Guyon and Elisseeff, 2003; Peng et al., 2005).", "startOffset": 158, "endOffset": 204}, {"referenceID": 8, "context": "Wrapper based methods select subset of features heuristically and classification accuracy is used to estimate the goodness of the selected subset (Kumar et al., 2012).", "startOffset": 146, "endOffset": 166}, {"referenceID": 14, "context": "In the embedded methods, feature selection criteria is directly incorporated in the objective function of the classifier (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 121, "endOffset": 160}, {"referenceID": 15, "context": "In the embedded methods, feature selection criteria is directly incorporated in the objective function of the classifier (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 121, "endOffset": 160}, {"referenceID": 6, "context": "Many filter and wrapper based methods fail on very high dimensional datasets due to their high time and memory requirements, and also because of inapplicability on sparse datasets (Guyon and Elisseeff, 2003; Yiteng et al., 2012).", "startOffset": 180, "endOffset": 228}, {"referenceID": 15, "context": "Many filter and wrapper based methods fail on very high dimensional datasets due to their high time and memory requirements, and also because of inapplicability on sparse datasets (Guyon and Elisseeff, 2003; Yiteng et al., 2012).", "startOffset": 180, "endOffset": 228}, {"referenceID": 0, "context": "In the literature, various max-margin formulation had been developed for many applications (Burges, 1998; Guo et al., 2007).", "startOffset": 91, "endOffset": 123}, {"referenceID": 5, "context": "In the literature, various max-margin formulation had been developed for many applications (Burges, 1998; Guo et al., 2007).", "startOffset": 91, "endOffset": 123}, {"referenceID": 10, "context": "Recently, we have proposed a hard margin primal formulation for feature selection using quadratic program (QP) slover (Prasad et al., 2013).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "We have formulated the task of feature selection as a one class SVM problem (Sch\u00f6lkopf et al., 2000) in the dual space where f eatures correspond to the data points and instances correspond to the dimensions.", "startOffset": 76, "endOffset": 100}, {"referenceID": 12, "context": "This is equivalent to searching for a hyperplane which maximally separates the data points from the origin (Sch\u00f6lkopf et al., 2000).", "startOffset": 107, "endOffset": 131}, {"referenceID": 7, "context": "We employ the Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) for solving our formulation.", "startOffset": 54, "endOffset": 74}, {"referenceID": 2, "context": "We adapt the liblinear implementation (Fan et al., 2008) for our proposed framework so that our approach is scalable to data in very high dimensions.", "startOffset": 38, "endOffset": 56}, {"referenceID": 11, "context": "We also show that the Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) falls out as a special case of our formulation in the dual space when using a hard margin.", "startOffset": 69, "endOffset": 99}, {"referenceID": 9, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 11, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 16, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 12, "context": "Which boundary could describe well the set of features lying in this space? Locating the desired boundary is similar to one class SVM formulation (Sch\u00f6lkopf et al., 2000).", "startOffset": 146, "endOffset": 170}, {"referenceID": 12, "context": "Note that in this formulation the objective function is similar to the one class SVM (Sch\u00f6lkopf et al., 2000).", "startOffset": 85, "endOffset": 109}, {"referenceID": 12, "context": "This is similar to the standard SVM dual derivation (Sch\u00f6lkopf et al., 2000).", "startOffset": 52, "endOffset": 76}, {"referenceID": 11, "context": "Note that the first term in the objective captures the redundancy between the features and the second term captures the relevance as in the case of QPFS formulation of (Rodriguez-Lujan et al., 2010).", "startOffset": 168, "endOffset": 198}, {"referenceID": 9, "context": "Some other appropriate metric which captures the predictive accuracy of a feature (such as mutual information(MI)) could also be used (Peng et al., 2005).", "startOffset": 134, "endOffset": 153}, {"referenceID": 9, "context": "The redundancy is usually captured using correlation or mutual information in feature selection tasks (Peng et al., 2005).", "startOffset": 102, "endOffset": 121}, {"referenceID": 4, "context": "A Gaussian kernel can also be used to approximate the mutual information (MI) (Gretton et al., 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": ", 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al., 2005; Rodriguez-Lujan et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 11, "context": ", 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al., 2005; Rodriguez-Lujan et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 7, "context": "Next, we describe the use of Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) to obtain a highly computationally efficient solution for our feature selection formulation.", "startOffset": 69, "endOffset": 89}, {"referenceID": 13, "context": "Solving the primal (typically by using QP solvers) may be efficient (O(M3)) in the cases when M \u226a N (Shalev-Shwartz et al., 2007).", "startOffset": 100, "endOffset": 129}, {"referenceID": 3, "context": "Even solving the dual using sequential minimal optimization (SMO) based methods in practice has the complexity of O(N2) (Fan et al., 2005).", "startOffset": 120, "endOffset": 138}, {"referenceID": 7, "context": "The dual coordinate descent methods have been well studied for solving linear SVMs using unconstrained form of the primal as well as dual formulations (Hsieh et al., 2008) who have shown that dual coordinate descent algorithm is significantly faster than many other existing algorithms for solving the SVM problem.", "startOffset": 151, "endOffset": 171}, {"referenceID": 7, "context": "Following the unconstrained formulation for the SVM objective (Hsieh et al., 2008), the MMFS objective in the primal (using a linear kernel) can be written as:", "startOffset": 62, "endOffset": 82}, {"referenceID": 7, "context": "We adapt the Dual Coordinate Descent algorithm (Hsieh et al., 2008) for our MMFS problem.", "startOffset": 47, "endOffset": 67}, {"referenceID": 7, "context": "Following (Hsieh et al., 2008), the MMFS-DCD approach obtains an \u01eb-accurate solution in O(log(1/\u01eb)) number of iterations.", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "The details about the proof of convergence are available in (Hsieh et al., 2008).", "startOffset": 60, "endOffset": 80}, {"referenceID": 11, "context": "Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) is a filter based feature selection method which models the feature selection problem as a quadratic program jointly minimizing redundancy and maximizing relevance.", "startOffset": 47, "endOffset": 77}, {"referenceID": 9, "context": "QPFS objective closely resembles the minimal-redundancy-maximalrelevance (mRMR) (Peng et al., 2005) criterion.", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "QPFS has also been shown to outperform many existing feature selection methods including mRMR and maxRel (Rodriguez-Lujan et al., 2010).", "startOffset": 105, "endOffset": 135}, {"referenceID": 11, "context": "(Rodriguez-Lujan et al., 2010) do not give any strong justification for the particular form of the objective used, other than the fact that it makes intuitive sense and seems to work well in practice.", "startOffset": 0, "endOffset": 30}, {"referenceID": 11, "context": "(Rodriguez-Lujan et al., 2010), the proposed approach for solving the objective is to simply use any of the standard quadratic programming implementations.", "startOffset": 0, "endOffset": 30}, {"referenceID": 8, "context": "Out of these seven datasets Leukemia, RAOA and RAC are microarray datasets (Kumar et al., 2012), MNIST is a vision dataset (Tan et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 14, "context": ", 2012), MNIST is a vision dataset (Tan et al., 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": ", 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al., 2010; Yiteng et al., 2012).", "startOffset": 92, "endOffset": 133}, {"referenceID": 15, "context": ", 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al., 2010; Yiteng et al., 2012).", "startOffset": 92, "endOffset": 133}, {"referenceID": 16, "context": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al., 2010) and two other embedded feature selection methods, namely, Feature Generating Machine (FGM) (Tan et al.", "startOffset": 95, "endOffset": 125}, {"referenceID": 14, "context": ", 2010) and two other embedded feature selection methods, namely, Feature Generating Machine (FGM) (Tan et al., 2010) and Group Discovery Machine (GDM) (Yiteng et al.", "startOffset": 99, "endOffset": 117}, {"referenceID": 15, "context": ", 2010) and Group Discovery Machine (GDM) (Yiteng et al., 2012).", "startOffset": 42, "endOffset": 63}, {"referenceID": 9, "context": "QPFS, FGM and GDM have been shown to outperform a variety of existing feature selection methods including mRMR and MaxRel (Peng et al., 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 122, "endOffset": 141}, {"referenceID": 16, "context": ", 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 14, "endOffset": 32}, {"referenceID": 6, "context": ", 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 42, "endOffset": 69}, {"referenceID": 11, "context": "For QPFS, we used mutual information (MI) as the similarity metric as it has been shown to give the best set of results (Rodriguez-Lujan et al., 2010).", "startOffset": 120, "endOffset": 150}, {"referenceID": 15, "context": "We have normalized these two datasets with unit variance (Yiteng et al., 2012).", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": "For MNIST and REAL-SIM datasets, training and testing splits are provided in (Chang et al., 2010).", "startOffset": 77, "endOffset": 97}, {"referenceID": 15, "context": "We have followed the training and testing splits of (Yiteng et al., 2012) for webspam and kddb datasets.", "startOffset": 52, "endOffset": 73}, {"referenceID": 14, "context": "We used the default settings of the parameters for both FGM and GDM as reported in (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 83, "endOffset": 122}, {"referenceID": 15, "context": "We used the default settings of the parameters for both FGM and GDM as reported in (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 83, "endOffset": 122}, {"referenceID": 2, "context": "After the top K features are selected, we used L2-regularized L2loss SVM (Fan et al., 2008) with default settings (that is cost parameter C=1) for classification for each of the algorithms and for each of the datasets.", "startOffset": 73, "endOffset": 91}, {"referenceID": 7, "context": "This implementation uses shrinking strategy (Hsieh et al., 2008).", "startOffset": 44, "endOffset": 64}, {"referenceID": 11, "context": "We used the publicly available implementation of QPFS (Rodriguez-Lujan et al., 2010).", "startOffset": 54, "endOffset": 84}, {"referenceID": 15, "context": "al (Yiteng et al., 2012).", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": "So, we used Nystr\u00f6m approximation (Rodriguez-Lujan et al., 2010) with sampling rate(\u03c1=0.", "startOffset": 34, "endOffset": 64}], "year": 2016, "abstractText": "Many machine learning applications such as in vision, biology and social networking deal with data in high dimensions. Feature selection is typically employed to select a subset of features which improves generalization accuracy as well as reduces the computational cost of learning the model. One of the criteria used for feature selection is to jointly minimize the redundancy and maximize the relevance of the selected features. In this paper, we formulate the task of feature selection as a one class SVM problem in a space where features correspond to the data points and instances correspond to the dimensions. The goal is to look for a representative subset of the features (support vectors) which describes the boundary for the region where the set of the features (data points) exists. This leads to a joint optimization of relevance and redundancy in a principled max-margin framework. Additionally, our formulation enables us to leverage existing techniques for optimizing the SVM objective resulting in highly computationally efficient solutions for the task of feature selection. Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation. We use a sparse representation to deal with data in very high dimensions. Experiments on seven publicly available benchmark datasets from a variety of domains show that our approach results in orders of magnitude faster solutions even while retaining the same level of accuracy compared to the state of the art feature selection techniques. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "creator": "LaTeX with hyperref package"}}}