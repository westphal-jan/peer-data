{"id": "1606.04518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy", "abstract": "Observational studies are based on accurate assessment of human state. A behavior recognition system that models interlocutors' state in real-time can significantly aid the mental health domain. However, behavior recognition from speech remains a challenging task since it is difficult to find generalizable and representative features because of noisy and high-dimensional data, especially when data is limited and annotated coarsely and subjectively. Deep Neural Networks (DNN) have shown promise in a wide range of machine learning tasks, but for Behavioral Signal Processing (BSP) tasks their application has been constrained due to limited quantity of data. We propose a Sparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework to deal with limited data. First, we break the acoustic feature set into subsets and train multiple distinct classifiers. Then, the hidden layers of these classifiers become parts of a deeper network that integrates all feature streams. The overall system allows for full connectivity while limiting the number of parameters trained at any time and allows convergence possible with even limited data. We present results on multiple behavior codes in the couples' therapy domain and demonstrate the benefits in behavior classification accuracy. We also show the viability of this system towards live behavior annotations.", "histories": [["v1", "Tue, 14 Jun 2016 19:32:34 GMT  (178kb,D)", "http://arxiv.org/abs/1606.04518v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["haoqi li", "brian baucom", "panayiotis georgiou"], "accepted": false, "id": "1606.04518"}, "pdf": {"name": "1606.04518.pdf", "metadata": {"source": "CRF", "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples\u2019 Therapy", "authors": ["Haoqi Li", "Brian Baucom", "Panayiotis Georgiou"], "emails": ["haoqili@usc.edu,", "brian.baucom@utah.edu,", "georgiou@sipi.usc.edu"], "sections": [{"heading": null, "text": "To deal with limited data, we propose a Sparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework. First, we break down the acoustic characteristics into subsets and train several unique classifiers. Then, the hidden layers of these classifiers become part of a deeper network that integrates all the trait streams. The overall system enables complete connectivity while limiting the number of parameters trained at any given time, and allows convergence even with limited data. We present results on several codes of conduct in the couples \"therapy domain and demonstrate the advantages in the precision of behavioural classification. We also demonstrate the feasibility of this system over live behavioural comments. Index terms: Behavioral Signal Processing, Deep Neural Networks, Behavioral Classification, Data Sparsity."}, {"heading": "1. Introduction", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "2. Preprocessing and feature extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Audio preprocessing", "text": "Thisar Xiv: 160 6.04 518v 1 [cs.L G] 14 Jun 2016requires a series of pre-processing steps: Voice Activity Detection (VAD) to identify spoken regions, Speaker Diarization to identify regions with the same speakers, and then we perform feature extraction from language regions. In our thesis, we use the pre-processing steps described in [12]. In short, we use all available interactions with an SNR above 5dB and perform VAD and diarization, then we ignore language segments shorter than 1.5 seconds. Language segments from each session for the same speaker are then used to analyze behavior."}, {"heading": "2.2. Acoustic feature extraction", "text": "We extract acoustic features that characterize speech prosody (pitch and intensity), spectral envelopes (MFCCs, MFBs), and speech quality (jitter and shimmer), all of which are extracted every 10 ms with a 25 ms hamming window by OpenSMILE [17] and PRAAT [18]. We perform a session feature normalization for each of the speakers, as in [12], to reduce the impact of recording conditions and physical properties of different speakers.Unlike [12], we are interested in developing a fine-resolution behavioral assessment, rather than just a session-level classification system, and as such we use features with a sliding frame1. Within each frame, we calculate a number of functions: Min (1st percentile), Max (99th percentile), Range (9th percentile, 1st percentile), and Medium."}, {"heading": "3. Couples\u2019 Therapy Corpus", "text": "The database used in this paper is provided by the UCLA / UW Couple Therapy Research Project [19], in which 134 couples participated in video-documented problem-solving interactions. During each discussion, a relationship-related topic (e.g. \"Why can't you leave my stuff alone?\") was selected, the behavior of which was evaluated separately by human commentators using a set of 33 codes of conduct (e.g. \"blame,\" \"acceptance,\" etc.) using the Couples Interaction Rating System (CIRS) [20] and the Social Support Interaction Rating System (SSIRS). [21] Each human commentator gave a subjective score of 1 to 9, with 1 indicating the absence of behavior and 9 indicating a strong presence. For more information on this data set, see [12, 19]."}, {"heading": "4. Methodology", "text": "For example, a therapist can observe how a couple interacts for an hour and derive an assessment that one of the partners is negative while the other shows acceptance. Unfortunately, this often means that we are left without an immediate basic truth. In most cases, this results in building session systems by using all available data, for example [12], averaging local decisions toward meeting-level assessments [11], or creating interaction models such as those in [15, 16]. In this work, we will build a system that is able to estimate behaviors in short periods of time to implement a live behavioral assessment framework. We propose a SparselyConnected and Disjointly-Trained Deep Neural Network (SDDNN) that aims to address data sparseness in behavioral analyses."}, {"heading": "4.1. DNN training", "text": "In our analysis, and with a feature size of 168, this approach always leads to failure during training: DNN training immediately identifies a local minimum even for small neural networks, while objective function decreases in the training set, but not in the development set. The results of behavior detection during the test are largely unchanged and therefore of little significance in providing behavioral trajectories. It is likely that the system converges to different minimums in relation to other dimensions, such as speaker characteristics. To minimize overequipment, we can add a failure layer to the input [22]. This feature reduction avoids overfitting to some extent, but we still do not achieve the gains we expected from using a DNN framework."}, {"heading": "4.2. Reduced feature dimensionality DNN", "text": "One way to avoid overfitting problems is to use a reduced Measurement Input Function Set. We can do this by selecting a subset of features and training DNN on it, which means that we use these subfeatures to train multiple behavior detection systems. For each of these systems, the feature dimension is reduced by a significant factor compared to the full feature set, which also reduces the number of parameters in the resulting DNN. Using the same amount of training data, we get a robustly trained DNN. The sequence of this level is illustrated in Figure 2. As we expect, this does not work across base systems either, as we do not take all informative features into account."}, {"heading": "4.3. Sparsely-Connected and Disjointly-Trained DNN", "text": "To take advantage of both the benefits of small feature sets that converge to avoid overpass problems, and the redundancy between feature streams, we propose the SparselyConnected and Disjointly-Trained DNN (SD-DNN) Training Framework. In this framework, shown in Figure 3, we select sparse feature sets, train (as in DNN's Reduced Feature Dimensionality) individual DNN systems, then set the parameters of these DNN systems, remove the output layer, connect the top hidden layers to each other, and add new hidden layers as fusion layers. This framework allows both sparse connectivity on the lower layers (not all features are connected to all the hidden layers above) and disjointly training the various layers of the DNN, thereby reducing the degree of freedom and achieving convergence."}, {"heading": "4.4. Joint Optimization of Sparsely-Connected DNN", "text": "The system presented in the previous section and shown in Fig. 3 optimizes the sparse lower and upper fusion layers. Without increasing the dimensionality of the parameters of the SD-DNN, we can initialize the training from the disjoint optimization point and jointly optimize the system. We will call this sparse, jointly optimized system SJ-DNN."}, {"heading": "4.5. Local \u2013 Session mappings", "text": "As mentioned above, we only have session-level assessments for the corpus of couples therapy, which is not unusual given the cost and subjectivity of the notes for mental health applications. Due to issues of subjectivity and agreement between the annotators, we use a binary subset of the data set for our training, which represents 20% of the data set as in [12] above and below. We assign points 1 for high presence and 0 for low presence of a certain behavior. Educational-level training samples receive the same reference as the reference value at session level as in Fig. 1. At the test date, the output of the DNN system provides a score for the presence of behavior (as in Fig. 4), but does not provide a global rating. While there are a number of methods to merge decisions (e.g. [15, 16, 23]), in this work we use the simplest score: Average stragglers. We can treat the output of DNxy as the probability of the Qi, whereby the QNY-level is determined by fixing the QNY-1."}, {"heading": "5. Experiment Setup", "text": "For each code of conduct and gender, we use 70 sessions at one extreme of the code (e.g. high recrimination) and 70 sessions at the other extreme (e.g. low recrimination) 2. This is to achieve greater agreement between the annotators and provide training data with binary class labels.Temporal deviations in behavior are slower than fundamental emotions' and therefore their analysis requires a longer frame window size of the language segment. Previous work [16] compared the performance of the behavior classification using different image sizes and showed that a 20-s frame was sufficient to estimate meaningful behavioral variables with simultaneous high resolution, so we opted for a 20-s window with 1 s shift. In our experiments, we use 3 codes of conduct available to us: Acceptance, Negativity, Blame. We evaluate the use of an SVM base system and compare with the DNN system proposed above."}, {"heading": "6. Results and Discussion", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We've never waited so long, \"he said.\" We're not ready yet to be able to retaliate, \"he said."}, {"heading": "7. Conclusion and Future Work", "text": "Compared to other DNN-based machine learning tasks, data economy is a critical issue in the BSP domain due to its costly and complicated process of data generation. Sparsely Connected and Disjoint Training enables us to train more complex DNN systems with limited data sets, achieve increased session-level performance, and, most importantly, receive continuous time and evaluation comments on our data. For future work, we plan to include mutual or shared information between different codes of conduct in behavioral analysis, as some behaviors are strongly correlated. We will also adjust the SD DNN architecture and parameters, for example, different DNN learning systems with reduced dimensions can use different DNN architectures."}, {"heading": "8. References", "text": "This year, it's gotten to the point where it's able to retaliate, \"he said.\" It's not as if., \"he said.\" But it's as if., \"he said.\" It's as if., \"\" It's as if., \"he said.,\" It's as if., \"\" It's as if., \"\" It's as if., \"\" \"It's as if.,\" \"It's as if.,\" \"It's as if.\""}], "references": [{"title": "Survey on speech emotion recognition: Features, classification schemes, and databases", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Affect and Emotion in Human- Computer Interaction: From Theory to Applications", "author": ["T. Vogt", "E. Andr\u00e9", "J. Wagner"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge", "author": ["B. Schuller", "A. Batliner", "S. Steidl", "D. Seppi"], "venue": "Speech Communication, vol. 53, no. 9\u00e2\u201c10, pp. 1062 \u2013 1087, 2011, sensing Emotion and Affect - Facing Realism in Speech Processing.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks", "author": ["A. Stuhlsatz", "C. Meyer", "F. Eyben", "T. Zielke", "G. Meier", "B. Schuller"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, May 2011, pp. 5688\u20135691.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Advances in Neural Networks: Computational and Theoretical Issues", "author": ["B. Schuller"], "venue": "Cham: Springer International Publishing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine.", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "in Interspeech,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Emotion recognition from spontaneous speech using hidden markov models with deep belief networks", "author": ["D. Le", "E.M. Provost"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, Dec 2013, pp. 216\u2013221.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Behavioral signal processing: Deriving human behavioral informatics from speech and language", "author": ["S. Narayanan", "P.G. Georgiou"], "venue": "Proceedings of the IEEE, vol. 101, no. 5, pp. 1203\u20131233, May 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Behavioral signal processing for understanding (distressed) dyadic interactions: Some recent developments", "author": ["P.G. Georgiou", "M.P. Black", "S.S. Narayanan"], "venue": "Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding, ser. J-HGBU \u201911. New York, NY, USA: ACM, 2011, pp. 7\u201312.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011, Memphis", "author": ["P.G. Georgiou", "M.P. Black", "A.C. Lammert", "B.R. Baucom", "S.S. Narayanan"], "venue": "TN, USA, October", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Toward automating a human behavioral coding system for married couples\u2019 interactions using speech acoustic features", "author": ["M.P. Black", "A. Katsamanis", "B.R. Baucom", "C.-C. Lee", "A.C. Lammert", "A. Christensen", "P.G. Georgiou", "S.S. Narayanan"], "venue": "Speech Communication, vol. 55, no. 1, pp. 1 \u2013 21, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Head motion modeling for human behavior analysis in dyadic interaction", "author": ["B. Xiao", "P. Georgiou", "B. Baucom", "S.S. Narayanan"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 7, pp. 1107\u2013 1119, July 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Quantifying atypicality in affective facial expressions of children with autism spectrum disorders", "author": ["A. Metallinou", "R.B. Grossman", "S. Narayanan"], "venue": "Multimedia and Expo (ICME), 2013 IEEE International Conference on, July 2013, pp. 1\u20136.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A language-based generative model framework for behavioral analysis of couples\u2019 therapy", "author": ["S.N. Chakravarthula", "R. Gupta", "B. Baucom", "P. Georgiou"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 2090\u20132094.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A dynamic model for behavioral analysis of couple interactions using acoustic features", "author": ["W. Xia", "J. Gibson", "B. Xiao", "B. Baucom", "P.G. Georgiou"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "Proceedings of the 21st ACM International Conference on Multimedia, ser. MM \u201913. New York, NY, USA: ACM, 2013, pp. 835\u2013838.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Praat, a system for doing phonetics by computer", "author": ["P. Boersma"], "venue": "Glot international, vol. 5, no. 9/10, pp. 341\u2013345, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Traditional versus integrative behavioral couple therapy for significantly and chronically distressed married couples.", "author": ["A. Christensen", "D.C. Atkins", "S. Berns", "J. Wheeler", "D.H. Baucom", "L.E. Simpson"], "venue": "Journal of consulting and clinical psychology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Couples interaction rating system 2 (cirs2)", "author": ["C. Heavey", "D. Gill", "A. Christensen"], "venue": "University of California, Los Angeles, vol. 7, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Couples interaction study: Social support interaction rating system", "author": ["J. Jones", "A. Christensen"], "venue": "University of California, Los Angeles, vol. 7, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929\u20131958, Jan. 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1929}, {"title": "Based on isolated saliency or causal integration? toward a better understanding of human annotation process using multiple instance learning and sequential probability ratio test", "author": ["C.-C. Lee", "A. Katsamanis", "P.G. Georgiou", "S.S. Narayanan"], "venue": "Proceedings of InterSpeech, Sep. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example, speech emotion recognition works [1\u20133] have shown effectiveness of extracting emotional content from human speech signals.", "startOffset": 46, "endOffset": 51}, {"referenceID": 1, "context": "For example, speech emotion recognition works [1\u20133] have shown effectiveness of extracting emotional content from human speech signals.", "startOffset": 46, "endOffset": 51}, {"referenceID": 2, "context": "For example, speech emotion recognition works [1\u20133] have shown effectiveness of extracting emotional content from human speech signals.", "startOffset": 46, "endOffset": 51}, {"referenceID": 3, "context": "In addition, Deep Neural Networks (DNN) have been employed for many related speech tasks [4\u20136].", "startOffset": 89, "endOffset": 94}, {"referenceID": 4, "context": "In addition, Deep Neural Networks (DNN) have been employed for many related speech tasks [4\u20136].", "startOffset": 89, "endOffset": 94}, {"referenceID": 5, "context": "In addition, Deep Neural Networks (DNN) have been employed for many related speech tasks [4\u20136].", "startOffset": 89, "endOffset": 94}, {"referenceID": 6, "context": "[7] and Le et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] both utilized DNN to extract high level representative features to improve emotion classification accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Over the last few years Behavioral Signal Processing (BSP) [9, 10] has examined the analysis of such complex, domain specific behaviors.", "startOffset": 59, "endOffset": 66}, {"referenceID": 9, "context": "Over the last few years Behavioral Signal Processing (BSP) [9, 10] has examined the analysis of such complex, domain specific behaviors.", "startOffset": 59, "endOffset": 66}, {"referenceID": 10, "context": "Based on machine learning techniques, BSP employed lexical [11], acoustic [12], and visual [13, 14] information to analyze and model multimodal human behaviors.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "Based on machine learning techniques, BSP employed lexical [11], acoustic [12], and visual [13, 14] information to analyze and model multimodal human behaviors.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "Based on machine learning techniques, BSP employed lexical [11], acoustic [12], and visual [13, 14] information to analyze and model multimodal human behaviors.", "startOffset": 91, "endOffset": 99}, {"referenceID": 13, "context": "Based on machine learning techniques, BSP employed lexical [11], acoustic [12], and visual [13, 14] information to analyze and model multimodal human behaviors.", "startOffset": 91, "endOffset": 99}, {"referenceID": 11, "context": "[12] built an automatic human behavioral coding system for couples interaction by using acoustic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In [15, 16] the authors employed a top layer HMM to take dynamic behavior state transitions into consideration and thus achieved higher accuracy on session-level behavioral classification.", "startOffset": 3, "endOffset": 11}, {"referenceID": 15, "context": "In [15, 16] the authors employed a top layer HMM to take dynamic behavior state transitions into consideration and thus achieved higher accuracy on session-level behavioral classification.", "startOffset": 3, "endOffset": 11}, {"referenceID": 11, "context": "In our work we employ the preprocessing steps described in [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "All these LowLevel-Descriptors (LLDs) are extracted every 10 ms with a 25 ms Hamming window through openSMILE [17] and PRAAT [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "All these LowLevel-Descriptors (LLDs) are extracted every 10 ms with a 25 ms Hamming window through openSMILE [17] and PRAAT [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "We perform session level feature normalization for each of the speakers as in [12] to reduce the impact of recording conditions and physical characteristics of different speakers.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Unlike [12] we are interested in building a fine-resolution behavioral estimation, rather than session-level classificationonly system, and as such we employ features with a sliding frame.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "The database used in this paper is provided by UCLA/UW Couple Therapy Research Project [19], in which 134 couples participated in video-taped problem-solving interactions.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": ") based on the Couples Interaction Rating System (CIRS) [20] and the Social Support Interaction Rating System (SSIRS) [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": ") based on the Couples Interaction Rating System (CIRS) [20] and the Social Support Interaction Rating System (SSIRS) [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "For more information about this dataset, please refer to [12, 19].", "startOffset": 57, "endOffset": 65}, {"referenceID": 18, "context": "For more information about this dataset, please refer to [12, 19].", "startOffset": 57, "endOffset": 65}, {"referenceID": 11, "context": ", [12], averaging of local decisions towards session level ratings [11], or creating models of interaction as in [15, 16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [12], averaging of local decisions towards session level ratings [11], or creating models of interaction as in [15, 16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": ", [12], averaging of local decisions towards session level ratings [11], or creating models of interaction as in [15, 16].", "startOffset": 113, "endOffset": 121}, {"referenceID": 15, "context": ", [12], averaging of local decisions towards session level ratings [11], or creating models of interaction as in [15, 16].", "startOffset": 113, "endOffset": 121}, {"referenceID": 21, "context": "To minimize overfitting we can add a dropout layer [22] at the input.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "Due to subjectivity and inter-annotator agreement issues we use a binarized subset of the dataset that lies at the top and bottom 20% of the dataset as in [12] for our training.", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": ", [15, 16, 23]), in this work we will use the simplest one: Average posteriors.", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [15, 16, 23]), in this work we will use the simplest one: Average posteriors.", "startOffset": 2, "endOffset": 14}, {"referenceID": 22, "context": ", [15, 16, 23]), in this work we will use the simplest one: Average posteriors.", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": "An earlier work [16] compared behavior classification performance on various frame sizes and showed that a 20 s frame was sufficient to estimate meaningful behavioral metrics while maintaining high resolution, we thus choose to use a 20 s window with 1 s shift.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Baseline SVM: The baseline SVM model was built similar to the Static Behavioral Model discussed in [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "From this figure, we can see behavior Negativity and Blame are highly correlated, and have opposite trend with Acceptance, which is in agreement with our intuition and previous research work [12].", "startOffset": 191, "endOffset": 195}], "year": 2016, "abstractText": "Observational studies are based on accurate assessment of human state. A behavior recognition system that models interlocutors\u2019 state in real-time can significantly aid the mental health domain. However, behavior recognition from speech remains a challenging task since it is difficult to find generalizable and representative features because of noisy and high-dimensional data, especially when data is limited and annotated coarsely and subjectively. Deep Neural Networks (DNN) have shown promise in a wide range of machine learning tasks, but for Behavioral Signal Processing (BSP) tasks their application has been constrained due to limited quantity of data. We propose a Sparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework to deal with limited data. First, we break the acoustic feature set into subsets and train multiple distinct classifiers. Then, the hidden layers of these classifiers become parts of a deeper network that integrates all feature streams. The overall system allows for full connectivity while limiting the number of parameters trained at any time and allows convergence possible with even limited data. We present results on multiple behavior codes in the couples\u2019 therapy domain and demonstrate the benefits in behavior classification accuracy. We also show the viability of this system towards live behavior annotations.", "creator": "LaTeX with hyperref package"}}}