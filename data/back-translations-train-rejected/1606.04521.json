{"id": "1606.04521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Training variance and performance evaluation of neural networks in speech", "abstract": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These results show the urgent need to rethink the way in which results in the literature are reported and interpreted.", "histories": [["v1", "Tue, 14 Jun 2016 19:39:41 GMT  (714kb,D)", "http://arxiv.org/abs/1606.04521v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ewout van den berg", "bhuvana ramabhadran", "michael picheny"], "accepted": false, "id": "1606.04521"}, "pdf": {"name": "1606.04521.pdf", "metadata": {"source": "CRF", "title": "Training variance and performance evaluation of neural networks in speech", "authors": ["Ewout van den Berg", "Bhuvana Ramabhadran", "Michael Picheny"], "emails": ["evanden@us.ibm.com,", "bhuvana@us.ibm.com,", "picheny@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "In automatic speech recognition (ASR), the goal is to develop a combination of speech and acoustic models that together minimize the decoding error rate (WER) for predefined tasks. Early work on the application of deep learning for ASR showed a huge improvement of up to 33% compared to existing GMM-HMM models [1]. Results such as these led to an increase in research on variations in deep neural network architectures along with new training methods, alternative feature representations and ways to improve data expansion. The performance of newly proposed approaches is typically evaluated by comparing the results with the performance of a base system. Even after extensive adjustment of the hyperparameters of the new system, relative improvements of about 5% are much more common today. In this work, we look at the well-known but widely ignored problem of variance in neural network training and its impact on the evaluation of new methods. In Section 2, we consider neural network training as sampling from a distribution and we demonstrate in Section 3, which we conduct extensive experiments irically."}, {"heading": "2. Neural network training as sampling", "text": "Given that optimisation is done using techniques designed for convex problems such as stochastic gradient descent (SGD), it should come as no surprise that the solution of a training run depends on the starting point, as well as on various factors influencing the optimisation process, such as minibatch randomisation (see also [2, 3, 4]), a phenomenon commonly known to many practitioners and indeed used in the work of [5] to generate model ensembles. However, it is rarely discussed explicitly in literature training algorithms otherwise, as the initial state of the network and the order of training data are generally deterministic (we1Part of this work was presented at ICLR 2016)."}, {"heading": "3. Numerical experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experiment setup", "text": "For the training and evaluation of the systems we use three data sets. The first two data sets are based on the training corpus of the English Broadcast News (BN) [6], which we process in advance to obtain 40-dimensional logmel functions with loudspeaker-dependent mel filters, which are selected from a set of 21 possible filters. The first data set (BN400) contains the full 400-hour BN training corpus and a 30-hour hold-out set. The second data set (BN50) uses only a subset of data and defines a 45-hour training corpus and a 5-hour hold-out set [7]. For the evaluation we use EARS Dev04f, as described by [7]. The third data set (BN50) is based on a subset of data and defines a 45-hour training corpus and a 5-hour hold-out set [7]."}, {"heading": "3.2. Results", "text": "In the first experiment, we examined the empirical distribution of entropy values, which are evaluated on the basis of the formation of 50 DNN networks with different underlying parameters. The results of the study show that the individual countries are not only countries, but also regions in which most of them are unable to integrate."}, {"heading": "4. Discussion", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5. Conclusions", "text": "In the literature, it is still common to report results without discrepancies; see, for example, [15, 16, 17, 18]. There are, of course, some exceptions such as [19, 20], but even this work only indicates a deviation from the phoneerror rate and not from the WHO. The improvements in the work mentioned above may make sense and represent a significant improvement over the basic method, but the problem is that we simply cannot be sure on the basis of individual numbers. Admittedly, there are some additional difficulties in systems based on a large number of training steps. The workflow training setup discussed in the previous section is a simple example, but much more involved processing approaches, where each step is highly tuned, exist [10]. Nevertheless, more insight into the distribution of results for different settings and algorithms and their incorporation in performance evaluation is urgently needed."}, {"heading": "6. References", "text": "[1] F. Seide, G. Li, and D. Yu, \"Conversational speech transcriptionusing context-dependent deep neural networks,\" in Interspeech 2011. International Speech Communication Association, August 2011. [2] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun, \"The loss surfaces of multilayer networks,\" arXiv: 1412.0233, 2014. [3] R. Pascanu, Y. N. Dauphin, S. Ganguli, and Y. Bengio, \"On the saddle point problem for non-convex surface of multilayer networks,\" arXiv: 1405.4604, 2014 N. Pinto, D. Doukhan, J. DiCarlo, and D. Cox \"A highthroughput screening approach to discovering good forms of biologically inspired visual representation,\" PLoS Comput Biol, vol. 5, p. e100579, 2009."}], "references": [{"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Interspeech 2011. International Speech Communication Association, August 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "arXiv:1412.0233, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": "arXiv:1405.4604, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A highthroughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS Comput Biol, vol. 5, no. 11, p. e100579, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv:1503.02531, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "1997 English Broadcast News Speech (HUB4) LDC98S71", "author": ["J. Fiscus", "J. Garofolo", "M. Przybocki", "W. Fisher", "D. Pallett"], "venue": "1998, linguistic Data Consortium.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B. Kingsbury"], "venue": "Proceedings of ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Switchboard-1 Release 2 LDC97S62", "author": ["J. Godfrey", "E. Holliman"], "venue": "1997, Linguistic Data Consortium, Philadelphia.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Performance of the IBM LVCSR system on the Switchboard corpus", "author": ["F.-H. Liu", "M. Monkowski", "M. Novak", "M. Padmanabhan", "M. Picheny", "P.S. Rao"], "venue": "Speech Research Symposium, 1995, p. 189.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "The IBM 2015 English conversational telephone speech recognition system", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "Proceedings of Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving training time of deep belief networks through hybrid pre-training and larger batch sizes", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proceedings of the NIPS Workshop on Log-linear Models, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization", "author": ["B. Kingsbury", "T. Sainath", "H. Soltau"], "venue": "Proceedings of Interspeech, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Some statistical issues in the comparison of speech recognition algorithms", "author": ["L. Gilick", "S.J. Cox"], "venue": "Proceedings of the 1989 International Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol. 1, May 1989, pp. 532\u2013535.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Improvements to deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "B. Kingsbury", "A.-R. Mohamed", "G.E. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A.Y. Aravkin", "B. Ramabhadran"], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understand (ASRU), December 2013, pp. 315\u2013320.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolution, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Proceedings of ICASSP, 2015, pp. 4580\u20134584.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Variable-component deep neural network for robust speech recognition", "author": ["R. Zhao", "J. Li", "Y. Gong"], "venue": "Proceedings of Interspeech, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "LSTM Time and frequency recurrence for automatic speech recognition", "author": ["J. Li", "A. Mohamed", "G. Zweig", "Y. Gong"], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, December 2015. [Online]. Available: http://research.microsoft. com/apps/pubs/default.aspx?id=259285", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-R. Mohamed"], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), December 2013, pp. 273\u2013278.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "RNNDrop: A novel dropout for RNNs in ASR", "author": ["T. Moon", "H. Choi", "H. Lee", "I. Song"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understand (ASRU), December 2015, pp. 65\u201370.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Early work on the application of deep learning for ASR showed tremendous improvement of up to 33% relative to existing GMM-HMM models [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Given that optimization is done with techniques that were designed for convex problems, such as stochastic gradient descent (SGD), it should come as no surprise that the solution of a training run depends on the initial starting point, as well as various factors that affect the optimization process, such as the minibatch randomization (see also [2, 3, 4]).", "startOffset": 347, "endOffset": 356}, {"referenceID": 2, "context": "Given that optimization is done with techniques that were designed for convex problems, such as stochastic gradient descent (SGD), it should come as no surprise that the solution of a training run depends on the initial starting point, as well as various factors that affect the optimization process, such as the minibatch randomization (see also [2, 3, 4]).", "startOffset": 347, "endOffset": 356}, {"referenceID": 3, "context": "Given that optimization is done with techniques that were designed for convex problems, such as stochastic gradient descent (SGD), it should come as no surprise that the solution of a training run depends on the initial starting point, as well as various factors that affect the optimization process, such as the minibatch randomization (see also [2, 3, 4]).", "startOffset": 347, "endOffset": 356}, {"referenceID": 4, "context": "This phenomenon is common knowledge to many practitioners, and indeed it was leveraged in work by [5] to generate model ensembles.", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "The first two datasets are based on the English Broadcast News (BN) training corpus [6] which we pre-process to obtain 40-dimensional logmel features with speaker-dependent mel filters chosen from a set of 21 possible filters.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "The second dataset (BN50) uses only a subset of the data and defines a 45-hour training set and a 5-hour hold out set [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "For evaluation we use EARS Dev04f, as described by [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The third dataset is based on the 300h Switchboard corpus [8, 9] and uses 40-dimensional speakerindependent logmel features.", "startOffset": 58, "endOffset": 64}, {"referenceID": 8, "context": "The third dataset is based on the 300h Switchboard corpus [8, 9] and uses 40-dimensional speakerindependent logmel features.", "startOffset": 58, "endOffset": 64}, {"referenceID": 9, "context": "Evaluation is done on the Hub52000 and CallHome corpora [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "For all decodes we use the trigram language model (LM) as described in [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "For the DNN we use layerwise pre-training, each for one epoch [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "For the BN50-DNN task we also use Hessian-free sequence training [7, 12, 13].", "startOffset": 65, "endOffset": 76}, {"referenceID": 11, "context": "For the BN50-DNN task we also use Hessian-free sequence training [7, 12, 13].", "startOffset": 65, "endOffset": 76}, {"referenceID": 12, "context": "For the BN50-DNN task we also use Hessian-free sequence training [7, 12, 13].", "startOffset": 65, "endOffset": 76}, {"referenceID": 9, "context": "The best results in speech, see for example [10], are obtained using sequence-level discriminative training techniques.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "For our experiments we minimize the state-based minimum Bayes risk (MBR) objective [7, 13] using Hessian-free optimization [12].", "startOffset": 83, "endOffset": 90}, {"referenceID": 12, "context": "For our experiments we minimize the state-based minimum Bayes risk (MBR) objective [7, 13] using Hessian-free optimization [12].", "startOffset": 83, "endOffset": 90}, {"referenceID": 11, "context": "For our experiments we minimize the state-based minimum Bayes risk (MBR) objective [7, 13] using Hessian-free optimization [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "This includes techniques such as McNemar\u2019s and matched-pair tests, as advocated in [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "In the literature it is still common practice to report results without any information on the variance; see for example [15, 16, 17, 18].", "startOffset": 121, "endOffset": 137}, {"referenceID": 15, "context": "In the literature it is still common practice to report results without any information on the variance; see for example [15, 16, 17, 18].", "startOffset": 121, "endOffset": 137}, {"referenceID": 16, "context": "In the literature it is still common practice to report results without any information on the variance; see for example [15, 16, 17, 18].", "startOffset": 121, "endOffset": 137}, {"referenceID": 17, "context": "In the literature it is still common practice to report results without any information on the variance; see for example [15, 16, 17, 18].", "startOffset": 121, "endOffset": 137}, {"referenceID": 18, "context": "There are of course some exceptions such as [19, 20], but even these papers only give variance on phoneerror rate and not on WER.", "startOffset": 44, "endOffset": 52}, {"referenceID": 19, "context": "There are of course some exceptions such as [19, 20], but even these papers only give variance on phoneerror rate and not on WER.", "startOffset": 44, "endOffset": 52}, {"referenceID": 9, "context": "The sequence training setup discussed in the previous section is a simple example, but much more involved processing approaches in which each step is highly tuned exist [10].", "startOffset": 169, "endOffset": 173}], "year": 2016, "abstractText": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These results show the urgent need to rethink the way in which results in the literature are reported and interpreted.", "creator": "LaTeX with hyperref package"}}}