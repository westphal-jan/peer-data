{"id": "1606.04615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Deep Reinforcement Learning With Macro-Actions", "abstract": "Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.", "histories": [["v1", "Wed, 15 Jun 2016 01:57:40 GMT  (525kb,D)", "http://arxiv.org/abs/1606.04615v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["ishan p durugkar", "clemens rosenbaum", "stefan dernbach", "sridhar mahadevan"], "accepted": false, "id": "1606.04615"}, "pdf": {"name": "1606.04615.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning with Macro-Actions", "authors": ["Ishan P. Durugkar"], "emails": ["idurugkar@cs.umass.edu", "cgbr@cs.umass.edu", "dernbach@cs.umass.edu", "mahadeva@cs.umass.edu"], "sections": [{"heading": "1 Introduction and Related Work", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 Hierarchical Reinforcement Learning", "text": "Hierarchical RL in general can be motivated along many lines, including a (potential) solution to the \"curse of dimensionality\" - the problem that solving a problem becomes exponentially more difficult with its size. Here, a hierarchical RL agent can offer an elegant approach to sharing and conquering by dividing the problem into smaller sub-problems. Further advantages are architectural; a hierarchical learner can complement parameterizable actions, continuous action and state spaces, and the generalizability of actions. We will discuss the advantages especially for macros in section five.An important distinction in hierarchical RL states is between open circuit models and closed circuit models. Closed circuit models can have loose relationships with a functional perspective: \"hit your enemy\" or \"go find the key, and open that door.\" In this case, the path is not described by its internal structure, but by its terminal states."}, {"heading": "2.1 Background", "text": "Hierarchical RL is generally modeled on semi-MDPs (SMDP). Whereas in an MDP, each state transition is assumed to occur at uniform time intervals (encoding a one-step path), an SMDP is a generalization of an MDP that adds a random variable that encodes the time difference between successive states (possibly encoding paths of arbitrary length).This random variable can be subject to certain additional constraints - it can be evaluated in real, discrete, or subject to other constraints defined by the weak point. In our case, the domain ALE [4] is a discrete time-step system that can interact with any 1 / 15 sec. 1An important consequence of changing an MDP to an SMDP is that the fixed point of the value function for a given policy represents changes: from V\u03b5 (s) = R (s) + the temporary + the + the + the (P) (P) to a Qamp (P) (P)."}, {"heading": "2.2 Macros", "text": "Macro actions or macros are predefined sequences of actions performed deterministically [7]. UsingM to refer to the list of all macros, mi to refer to the ith macro within this list, and \"i to define the length of the ith macro, we can then define a macro as: mi = < ai, 1,..., ai\" i > Here x can be any of the actions available to the macro, generally including other macros, although we do not explore this direction in this paper. Additionally, we use | M | to refer to the number of macros. As an example, we consider the Atari game \"Boxes.\" In our experiments, we find that sequences of the type \"up, up, punch, down, down,\" which move and pull out for a punch, represent fixed sequences that can be particularly useful."}, {"heading": "3 Learning Macros", "text": "In this section, we highlight the various challenges in selecting the useful macros and our changes to DQN as measures for editing the macros. We also discuss different approaches to varying the shape of the macros, i.e. the sequence of actions contained therein. While the goal is to learn macros, we include a different approach that is meant as a starting point for later analyses.1The internal clock of the ALE runs at 60 frames per second, and each action is active for 4 frames, this extension through time could already be considered a case of temporal abstraction. However, since our learning agent considers the ALE as a \"black box,\" we consider the interactions of 1 / 15 s to be atomical. Algorithm 1 changes for the DQN including macros are considered. If epoch = in K is then compiled the new list of macros by the specified policy; if longer than | M |, we cut off the list of the last macros with empty macros."}, {"heading": "3.1 Repetition of Actions", "text": "These macros take advantage of the fact that the time step of each action in the Atari range may be smaller than is actually required to make optimal decisions. | M |: selected as parameter 'i: selected as parameter ai, k: ai, ai: AAlgorithm 2 Selection of macros by execution frequency at the end of an epoch in k Initialize an object for counting action sequences, O for i from 0 to | A | \u2212' doIncrease the counter in O of sequence A [i: i +] by 1 end for ranking all sequences according to their occurrence in O Initialize the set of macrosesM to contain the sequence with the highest repetition rate in O."}, {"heading": "3.2 Frequency", "text": "Here the macros are selected by selecting sub-sequences of actions of a certain length, which are most frequently repeated during the trajectories since the last update of the macro list, if they are sufficiently different. Difference is measured by the length of the longest common sub-sequence (lcs) between two macros. If the lcs between a macro candidate and any other macro is longer than a percentage \u03c9 of the total length of the macros, \"the candidate is dropped. See algorithm 2 for a more detailed explanation. | M |: selected as parameter\" i: selected as parameter:... ai, 0, ai + 1,1,..., ai + n, \"is the most frequently repeated sequence of the agent, if it is sufficiently different from the [0, i \u2212 1] most frequently repeated sequences. Sequences that are most frequently repeated by the agent can be considered useful for the agent in view of current policy."}, {"heading": "4 Experimental Results", "text": "s performance after 50 million frames. We compare this behavior with our DQN implementation based on [11], which was trained for 100 million frames, twice as long as the training period. The testing is done after each epoch (1 million frames) for 200,000 frames to evaluate the performance of the agent. To demonstrate stability and performance, the performance scores are averaged over 5 epochs, and the variance is also shown over them. To learn macros based on frequency, we allow as much as 80% overlap, and we update these macros with newer learned ones at larger and larger intervals."}, {"heading": "5 Analysis", "text": "To explain the performance of macro actions, we offer and evaluate three hypotheses: the first is the impact of macros on exploration, the second is the impact they have on error propagation, and the third is the approximation of deep representation with its potential challenge to greedy algorithms (such as the Q-learning algorithm used here); the results show that an agent with access to useful macros performs better than an agent with access to atomic actions. We analyze this behavior both in terms of the rate of convergence and the score achieved."}, {"heading": "5.1 Explanatory Hypotheses", "text": "As already mentioned, there are two important established advantages of macros that we will discuss now: The first is related to the exploration-exploitation dilemma of reinforcement learning, i.e. the dilemma between exploiting knowledge already acquired and trying something new. Exploration steps with macros offer greater exploration opportunities in a targeted way. As the games in the Atari area are very complex and consist of millions of states, more efficient exploration should therefore offer convergence advantages; the second potential benefit-macro offering is the changed distribution of rewards throughout the SMDP. This change stems from the fact that macros receive the cumulative reward along the visited states, propagating rewards from states to \"steps forward in an iteration\"; in the MDP setting, this takes up to \"times more iterations,\" as the reward is only acted in one state, which makes it easier to imply errors equally, which is much quicker to imply."}, {"heading": "5.2 Convergence", "text": "Given that the convergence rate is the most reliable use of macros, it is not surprising that the same is true for macros used in the Atari domain. However, there are two possible reasons, firstly the exploratory bias and secondly the faster spread of values. Although it is almost impossible to quantify the effects of these two reasons perfectly, we can still draw some conclusions about the behavior of the different macros. To this end, we have compared the performance of the previously well-defined macros with randomly populated macros, which offer better exploration by forcing agents to explore states and regions farther away. However, they do not offer much better convergence than the pure DQN approach, and perform significantly worse than both manual macros and scholarly macros. As they do not necessarily offer a higher \"spread\" across the MDP, the distribution of rewards may be lower, suggesting that a faster convergence may be more affected by the spread of macros than by exploration biology."}, {"heading": "5.3 Variance", "text": "As can be seen from the results, the eventual deviation of values obtained is consistently smaller when using macros (especially with Qbert, the deviation is dramatically smaller, while higher end values are achieved at the same time), and we believe this is due to a combination of better exploration and better distribution of rewards. If the agent only uses nuclear measures, it may be that he does not consistently explore areas with higher Q values, giving the learner finite training times to learn other strategies. Higher exploration and better distribution of rewards will make this less likely, as the learner will explore more distant states and have states with different Q values to choose from. Finally, macros result in a policy where the agent has great confidence in the best measures, leading to more stable strategies."}, {"heading": "5.4 Scores Achieved", "text": "The only game in which the behavior takes place at or below the level of atomic actions is the \"breakout.\" This is a surprising result, since the rough control generally should not lead to better results. However, an agent using atomic actions can always choose the same actions that are forced by a macro. We believe that this follows from a third benefit when combined with a deep network, greater resistance to error when approximating the Q values of a state. Most games in the Atari domain offer sparse rewards available after hundreds or thousands of frames."}, {"heading": "6 Conclusion and Future Work", "text": "We have shown how macro actions can improve the convergence times and values of a DQN agent in the Atari domain. Macros not only improve the efficiency of the agent's training, but also help the agent achieve better scores in six of the seven games tested. We have analysed the reasons why macros can improve the strategies learned from a learning agent with deep amplification. Useful macros are likely to accelerate the agent's learning and promote the discovery of optimal strategies. Better macro discovery techniques would be a viable next step in this direction. Ideally, taking into account the various aspects of learning macros, we should develop a technique that not only learns the actions to be taken, but also their optimal length and quantity. As open-loop strategies, macros can take action in a set sequence without looking at the underlying state once initiated. While this means the macro agent needs to make decisions, it is not so common and architecture needs to be learned."}], "references": [{"title": "Classifying options for deep reinforcement learning", "author": ["Kai Arulkumaran", "Nat Dilokthanakul", "Murray Shanahan", "Anil Anthony Bharath"], "venue": "arXiv preprint arXiv:1604.08153,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Reinforcement learning in continuous time: Advantage updating", "author": ["Leemon C Baird III"], "venue": "In Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G. Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep Recurrent Q-Learning for Partially Observable MDPs", "author": ["Matthew J. Hausknecht", "Peter Stone"], "venue": "CoRR, abs/1507.06527,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Hierarchical solution of Markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1602.04621,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Doina Precup"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 61, "endOffset": 69}, {"referenceID": 3, "context": "Since the groundbreaking results shown by Deep Q-Learning in [10, 11] for learning to play games from the Atari Learning Environment [4], there has been extensive research on deep reinforcement learning.", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Deep Q-learning in particular seeks to approximate the Q-values [21] using deep networks, such as deep convolutional neural networks [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 4, "context": "There has also been work on modifying the loss used to train the network using more consistent operators [5], or ensuring better target estimation [19].", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "There has also been work on modifying the loss used to train the network using more consistent operators [5], or ensuring better target estimation [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 5, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 12, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 154, "endOffset": 165}, {"referenceID": 4, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 224, "endOffset": 231}, {"referenceID": 18, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 224, "endOffset": 231}, {"referenceID": 8, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 314, "endOffset": 321}, {"referenceID": 11, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 314, "endOffset": 321}, {"referenceID": 14, "context": "Generally, research on improving the learner\u2019s performance can broadly be classified along the following directions: (i) improvements to the deep network [20, 6, 13] (ii) improvements to the reinforcement learning algorithm [5, 19] and (iii) implementation enhancements, such as performing better gradient updates [9, 12] and prioritizing experience replay to maximize learning [15].", "startOffset": 378, "endOffset": 382}, {"referenceID": 13, "context": "In reinforcement learning (RL), temporal abstraction [14] denotes the use of hierarchical multi-step actions that the agent can take in the possible addition to the available primitive actions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "An MDP that has been extended to allow modeling such paths is sometimes referred to as a semi-MDP (SMDP) [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "Since one cannot consider all possible abstractions\u2013the number of such abstractions grows exponentially with their length\u2013learning useful (or meaningful) abstractions has long been an important problem in reinforcement learning [3].", "startOffset": 228, "endOffset": 231}, {"referenceID": 7, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 40, "endOffset": 46}, {"referenceID": 16, "context": "Some recent work on hierarchical models [8, 1] aim to achieve hierarchical behavior using closed loop control, or options [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "The drawback of these methods is that they require external supervision, either by explicitly specifying the mapping from states to options [1] or by specifying possible sub-goals [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "The drawback of these methods is that they require external supervision, either by explicitly specifying the mapping from states to options [1] or by specifying possible sub-goals [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "Another possible set of abstractions are state-independent, deterministic sequences of actions, called macro-actions or macros [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "Specifically we extend the Deep Q-Network [11] to use macro actions.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "In terms of temporal abstraction, this problem is most closely modeled by options [17], i.", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "However, hierarchies of this kind are difficult to learn, since they require both a composition of atomic actions and a decomposition of a larger goal into smaller subgoals (an illustration of manual goal-decomposition is given by [8]).", "startOffset": 231, "endOffset": 234}, {"referenceID": 3, "context": "In our case, the domain, the ALE [4], is a discrete time step system that can be interacted with every 1/15 s.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Macro-actions or macros are predefined sequences of actions that will be taken deterministically [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "We test our hypothesis on the Atari [4] domain using an agent similar to the DQN [11].", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "We test our hypothesis on the Atari [4] domain using an agent similar to the DQN [11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "The updates are computed using the variation to RMSProp [18] used by [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "The updates are computed using the variation to RMSProp [18] used by [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "The only structural difference between the DQN of [11] and our model is that the output layer is expanded to include the macros.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "We evaluate this technique on the Arcade Learning Environment [4], a reinforcement learning interface to Atari 2600 games.", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "We compare this behavior to our DQN implementation based on [11] trained for 100 million frames, i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "There is some variance in our baseline as compared to [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "This relates macros to advantage learning, since they, too, increase what is known as the action gap [5, 2].", "startOffset": 101, "endOffset": 107}, {"referenceID": 1, "context": "This relates macros to advantage learning, since they, too, increase what is known as the action gap [5, 2].", "startOffset": 101, "endOffset": 107}], "year": 2016, "abstractText": "Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.", "creator": "LaTeX with hyperref package"}}}