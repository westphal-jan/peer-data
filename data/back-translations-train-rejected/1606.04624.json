{"id": "1606.04624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Finite-time Analysis for the Knowledge-Gradient Policy", "abstract": "We consider sequential decision problems in which we adaptively choose one of finitely many alternatives and observe a stochastic reward. We offer a new perspective of interpreting Bayesian ranking and selection problems as adaptive stochastic multi-set maximization problems and derive the first finite-time bound of the knowledge-gradient policy for adaptive submodular objective functions. In addition, we introduce the concept of prior-optimality and provide another insight into the performance of the knowledge gradient policy based on the submodular assumption on the value of information. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in learning problems. Empirical experiments are conducted to further illustrate the finite time behavior of the knowledge gradient policy.", "histories": [["v1", "Wed, 15 Jun 2016 02:44:02 GMT  (2199kb,D)", "http://arxiv.org/abs/1606.04624v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yingfei wang", "warren powell"], "accepted": false, "id": "1606.04624"}, "pdf": {"name": "1606.04624.pdf", "metadata": {"source": "CRF", "title": "Finite-time Analysis for the Knowledge-Gradient Policy", "authors": ["Yingfei Wang", "Warren Powell"], "emails": ["yingfei@cs.princeton.edu", "powell@princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is more than ever before in the history of the European Union."}, {"heading": "2 Ranking and Selection Problems", "text": "Suppose we have a collection of M alternatives (where M could be quite large), each of which can be measured sequentially to estimate its unknown mean. We assume a normally distributed measurement noise with known variance \u03c32W. First, we introduce the model for independent normal beliefs. We start with a normally distributed Bayean prediction of the sample, which is independent of alternatives, \u00b5x \u00b2 N (\u03b80x, \u03c30x). At the ninth iteration, we use a measurement method \u03c0 xn and observe W n + 1xn \u03c9N (\u03bcxn, \u03c3W). For convenience, we perform \u03c3-algebras Fn for all n = 0, 1, 1, N \u2212 1, which is formed by the previous n measurement decisions and results, x0, W 1,..., xn \u2212 x. We define phenomena x x = E [\u03bcx | Fn] and Bayer."}, {"heading": "3 Knowledge Gradient", "text": "For R & S problems, the knowledge gradient is a policy that chooses in the n-th iteration its (n + 1) first measurement of X to maximize the expected increase in value in a single timeframe [15, 13]. To be more precise, the value of being in the state Sn maxxxxx. If we decide to measure xn = x, which allows us to observe W n + 1, then we go into a new state of knowledge Sn + 1 = (phenomena + 1, n + 1). In the iteration n + 1x this is a random variable, as we do not yet know what W + 1 will be. We would like to choose x in the iteration the expected value maxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}, {"heading": "4 Finite-time Analysis of the Knowledge Gradient", "text": "In Section 4.1, interpreting Bayesian R & S problems as adaptive stochastic, multimodular maximization problems, we show that the policy of the KG inherits precisely the performance guarantees of the greedy algorithm for classic submodular maximization problems when the utility function is adaptively submodular. In such cases, we theoretically analyze the adaptive submodular assumption and point out that it may fail in the ranking and selection problems. Instead of the pathological behavioral analyses of the utility function, we examine its average behavior by placing the expectations above the observations in Section 4.2. In Section 4.3, we analyze the prior optimism that provides further insight into the performance of the KG policy based on the submodularity of a well understood quantity: value of information.It is important to ensure that both the specific reduction of noise development and the limited subducality of the measurement ensures."}, {"heading": "4.1 The Reduction of R&S to Adaptive Stochastic Multi-set", "text": "Let E be a finite set of items. Each element e \"E maps to a random result of a measurement \u03a6 (e) in a set O\" of possible values. We define a realization as a functional principle: E 7 \"O,\" which represents the observation of each item in the soil group. Under Bayesian interpretation, we assume that there is a known previous probability distribution p \"(p): = P (p) over all possible realizations. The adaptive stochastic optimization problem consists of the sequential selection of an item e\" E, \"whereby we assume that there is a result (e) and the selection of the next item. After each pick, the observations can so far be presented as a partial realization. A partial realization is consistent with the realization, which is called realization."}, {"heading": "4.2 The Value of Information", "text": "We define the value of information v (Z, \u03c6) as the incremental improvement over the best possible expected value that can be achieved without measurement. (1) The value of information v (Z) is defined as the value of information v (Z) if the expectation of the previous distribution p (\u03c6) is taken. The value of information has a long history that encompasses the literature of several disciplines. Stigler considers the value of information in economics when buyers are looking for the best price. (35) Howard laid the foundation for the value of information in a decision-theoretical context and spawned a great deal of work in this area. (22) Yokota and Thompson give an initial comprehensive overview of the value of information in terms of health risk management. (37)"}, {"heading": "4.3 Guarantees on the Prior-optimality of the Knowledge Gra-", "text": "There are two ways to evaluate the value of a policy. The first, which we call the downstream view, is the following: The second, which we call the previous view, begins with the characterization of the value of an arbitrary allocation Z (before we have seen any sample realizations). The classic method of estimating the value of a policy is to calculate the incremental improvement of what we could do before we collect any information, is given by f \"avg\" (\u03c0) = E [f \"p\" (Z \"p\" p \"p\" p. \"\" \"p\" p \"p\" p \"p\" p \"p\" \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "5 Analysis of Submodularity of the Value of Infor-", "text": "The finite time limits obtained in the previous sections assume that the value of the information is submodular = 01 = 01. In general, the submodularity does not apply to any permanent function. In this section, we analyze the submodularity of the two-alternative case for independent beliefs, which allows us to use the results of the real analysis to investigate submodularity. 5.1. A function f: Rn 7 \u2192 R is submodular if for all x, y, y, Rn, xi, yi and imp the results of the real analysis submodularity are used. We show that the submodularity of the C2 functions is directly related to their second derivatives and transverse derivatives."}, {"heading": "6 Computational Experiments", "text": "Since the semester thesis of [28] there is a long history in the optimal learning literature of the development of algorithms with detectable asymptotic or finite time limits [2, 8, 34, 4, 16, 3]. But none of these limits is narrow in finite time and different limits may be based on different measurements. Therefore, empirical experiments are required to better understand the finite time performance of each policy. To this end, we propose experiments to illustrate the finite time behavior of both KGs and other optimal learning strategies. We consider the following learning settings, which result in a lot in the Bayesian black box optimization. Equal-prior: M = 100. The true values \u00b5x are uniformly distributed over [0, 60] and the measurement of noise poorly W = 100. We have the following learning settings, which for each x.Asymmetric unimodulary function (ON): x is a controllable parameter ranging from 21 to 120."}, {"heading": "6.1 Finite Time Performance of Different Policies", "text": "Although the theoretical analysis in the previous section aims to link the performance of the knowledge gap policy to the optimal policy (in theory), it is impossible to find the optimal sequential policy in practice. To this end, we compare the value of the policy with the expected value of the best alternative maxx \u00b5x. We define the opportunity cost (OC\u03c0) of each policy \u03c0 at any time step n as: OC\u03c0 = max x \u2212 \u00b5x, where x-n = arg maxxxxx. n x. We illustrate the finite timing of the policy of equality before and UP with independent normal beliefs. We perform the opportunity cost ratio = max- \u2212 \u00b5x - max. nmax. \u00b5x in each iteration. We report on the mean value with a confidence interval of over 1000 experiments in Figure 1. Next, we compare the performance of the party policy IE with the vote, UCB-E with the vote, SR times, EXPL and EXPAUT performance with the problem classes in Figure 5."}, {"heading": "7 Conclusion", "text": "In this paper, we offer a new perspective on the interpretation of ranking and selection problems as adaptive stochastic multiset maximization problems. We present the first finite time limits for the knowledge gap on both posterior optimality and previous optimality. The previous view provides a cleaner relationship between policy performance and the sample taken, making it possible to correlate the value of information with the submodularity of the sample. We analyze the submodularity of the two-alternative case and offer different conditions for more general problems, highlighting the problem and the importance of submodularity in skew problems. We propose experiments to further illustrate the finite time behavior of knowledge gap politics and other strategies with or without theoretical guarantees."}, {"heading": "A Proofs", "text": "We look at the resulting state of knowledge Sn = (\u03b8nx, \u03b2nx) x x x. Since \u03c3W 6 = 0, there is such a probability that maxx \u03b8nx > maxx 6 = x is positively probable. Now, let's look at another finding that came with dom (\u0432) x x. \u2212 T {{T} {{}} {T \u00b2 \u00b2 \u00b2, where x2 is the second largest alternative of \u03b8nx. In this context, we refer to the observation of x2 as W2 and the resulting Sn + 1 as (successn + 1 x) x x x x x x. The knowledge gap is expressed according to the Bayes rule. \u2212 nx can be expressed analytically as W2 and the resulting Sn + 1 as normal."}], "references": [{"title": "Sample mean based index policies with o(logn) regret for the multiarmed bandit problem", "author": ["R. Agrawal"], "venue": "Adv. in Appl. Probab., ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "S", "author": ["J.-Y. Audibert"], "venue": "Bubeck, et al., Best arm identification in multi-armed bandits, COLT 2010-Proceedings, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["J.-Y. Audibert", "R. Munos", "C. Szepesv\u00e1ri"], "venue": "Theor. Comput. Sci., 410 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn., 47 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal learning for sequential sampling with non-parametric beliefs", "author": ["E. Barut", "W.B. Powell"], "venue": "J. Global Optim., ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Selecting a selection procedure", "author": ["J. Branke", "S.E. Chick", "C. Schmidt"], "venue": "Manag. Sci., 53 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Bandits with heavy tail", "author": ["S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi"], "venue": "arXiv preprint arXiv:1209.1727, ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["O. Capp\u00e9", "A. Garivier", "O.-A. Maillard", "R. Munos"], "venue": "Stoltz, et al., Kullback\u2013leibler upper confidence bounds for optimal sequential allocation, Ann. Statist., 41 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A gradient approach for smartly allocating computing budget for discrete event simulation", "author": ["C.-H. Chen", "H.-C. Chen", "L. Dai"], "venue": "28th Proc. Winter Simul., IEEE Computer Society", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient dynamic simulation allocation in ordinal optimization", "author": ["C.-H. Chen", "D. He", "M. Fu"], "venue": "IEEE Trans. Automat. Control, 51 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Simulation budget allocation for further enhancing the efficiency of ordinal optimization", "author": ["C.-H. Chen", "J. Lin", "E. Y\u00fccesan", "S.E. Chick"], "venue": "Discrete Event Dyn. Syst., 10 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "New two-stage and sequential procedures for selecting the best simulated system", "author": ["S.E. Chick"], "venue": "Oper. Res., 49 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "The knowledge-gradient policy for correlated normal beliefs", "author": ["P. Frazier", "W. Powell", "S. Dayanik"], "venue": "INFORMS J. Comput., 21 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Paradoxes in learning and the marginal value of information", "author": ["P.I. Frazier", "W.B. Powell"], "venue": "Decis. Anal., 7 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM J. Control Optim., 47 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol., ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1979}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "J. Statist. Plann. Inference, 54 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "G", "author": ["I. Guttman"], "venue": "C. Tiao, et al., A bayesian approach to some best population problems, Ann. Math. Statist., 35 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1964}, {"title": "Opportunity cost and OCBA selection procedures in ordinal optimization for a fixed number of alternative systems", "author": ["D. He", "S.E. Chick", "C.-H. Chen"], "venue": "IEEE Trans. Syst. , Man, and Cybern., Part C: Applications and Reviews, 37 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Information value theory", "author": ["R. Howard"], "venue": "IEEE Trans Syst. Sci. and Cybern., 2 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1966}, {"title": "Global optimization of stochastic black-box systems via sequential kriging meta-models", "author": ["D. Huang", "T.T. Allen", "W.I. Notz", "N. Zeng"], "venue": "J. Global Optim., 34 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "J. Global Optim., 13 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning and classifying under hard budgets", "author": ["A. Kapoor", "R. Greiner"], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "Mach. Learn., 80 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Adv. Appl. Math., 6 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1985}, {"title": "Hierarchical knowledge gradient for sequential sampling", "author": ["M.R. Mes", "W.B. Powell", "P.I. Frazier"], "venue": "J. Mach. Learn. Res., 12 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "The knowledge-gradient algorithm for sequencing experiments in drug discovery", "author": ["D.M. Negoescu", "P.I. Frazier", "W.B. Powell"], "venue": "INFORMS J. Comput., 23 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Math. Program., 14 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1978}, {"title": "Optimal learning", "author": ["W.B. Powell", "I.O. Ryzhov"], "venue": "vol. 841", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Applied statistical decision theory", "author": ["H. Raiffa", "R. Schlaifer"], "venue": "Harvard Business School Publications, ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1961}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "arXiv preprint arXiv:0912.3995, ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "The economics of information", "author": ["G.J. Stigler"], "venue": "J. political Econ., ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1961}, {"title": "The knowledge gradient for sequential decision making with stochastic binary feedbacks", "author": ["Y. Wang", "C. Wang", "W. Powell"], "venue": "33rd Proc. ICML", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Value of information literature analysis: a review of applications in health risk management", "author": ["F. Yokota", "K.M. Thompson"], "venue": "Med. Decis. Mak., 24 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 30, "context": "Raiffa and Schlaifer established the Bayesian framework for R&S problems [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "al made a thorough comparison of several fully sequential sampling procedures [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 10, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 19, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 66, "endOffset": 77}, {"referenceID": 11, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "They indicate that the optimal computing budget allocation (OCBA) [9, 11, 21] and value of information procedures (VIP) [12] perform quite well and better than a deterministic or two-stage policy [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the \u201cknowledge-gradient policy\u201d (KG).", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Another single-step Bayesian look-ahead policy first introduced by [19] and then further studied by [15] is called the \u201cknowledge-gradient policy\u201d (KG).", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "al modified the knowledge-gradient policy to handle correlated multivariate normal belief on the mean values of these rewards [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "A similar field is the multi-armed bandit problem, which were originally studied under Bayesian assumptions [17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 0, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 3, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 24, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 6, "context": "Different UCB-type variants have been developed for many types of reward distributions and have provable logarithmic regret bounds [28, 1, 4, 27, 7].", "startOffset": 131, "endOffset": 148}, {"referenceID": 28, "context": "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "To accomplish this, we build on the general structure of the analysis of greedy algorithms given in [31] and [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].", "startOffset": 177, "endOffset": 185}, {"referenceID": 12, "context": "For R&S problems, the knowledge gradient is a policy that at the nth iteration chooses its (n + 1)st measurement from X to maximize the single-period expected increase in value [15, 13].", "startOffset": 177, "endOffset": 185}, {"referenceID": 27, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 33, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 26, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 139, "endOffset": 146}, {"referenceID": 4, "context": "The knowledge gradient policy can handle the presence of a variety of belief models such as (generalized) linear [30, 36] or nonparametric [29, 5].", "startOffset": 139, "endOffset": 146}, {"referenceID": 28, "context": "We follow the general structure of the analysis of greedy approximation [31] to develop the first finite-time bound for the knowledge gradient policy for R&S problems as follows.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "1 ([32]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "The definition of the knowledge gradient \u03bd x coincides with the Conditional Expected Marginal Benefit \u2206(e|\u03c8) defined by [18]: \u2206(e|\u03c8) := E [ f ( dom(\u03c8) \u222a {e},\u03a6 ) \u2212 f ( dom(\u03c8),\u03a6 ) |\u03a6 \u223c \u03c8 ] .", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "We generalize the definition of adaptive monotonicity and adaptive submodularity for set functions given by [18] to multi-set functions as follows.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Since for any x, \u03b8 x = \u03b8 x + \u03c3\u0303(\u03a3, x)Z, where \u03c3\u0303(\u03a3, x) = \u03a3ex \u221a 1/\u03b2+\u03a3xx and the random variable Z is standard normal when conditioned on F [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "Stigler considers the value of information in economics when buyers search for the best price [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "Howard laid the groundwork for the value of information in a decision-theoretic context and spawned a great deal of work in this area [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "Yokota and Thompson gives a first comprehensive review of value of information analyses related to health risk management [37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 23, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 8, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 11, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 14, "context": "Raiffa and Schlaifer poses the Bayesian R&S problem and defines the associated value of information [33], which marked the beginning of a number of literature on the value of information within Bayesian R&S and the budgeted learning problem [20, 26, 9, 12, 15].", "startOffset": 241, "endOffset": 260}, {"referenceID": 28, "context": "Since the value of information is a multi-set function, we first generalize the definitions and properties of submodular set functions described by [31] to submodular multi-set functions.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "1 in [31].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[18] A concatenated policy \u03c0 = \u03c01@\u03c02 is constructed by running \u03c01 to completion, and then running policy \u03c02 from a fresh start ignoring all the information collected while running \u03c01.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] For a policy \u03c0, define the j-truncation \u03c0 of \u03c0 as the policy that runs exactly (j + 1) steps under \u03c0\u2019s decision rule and \u03c0{j} as the single step policy that randomly chooses an alternative according to the probability distribution of policy \u03c0\u2019s decision for the (j + 1)-th step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "The proof of this lemma can be found in [31].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "The concavity of the value of information has been studied extensively by [14].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "The value of information v(z) = s(z)f(\u2212 |\u03b8 0 1\u2212\u03b8 2 | s(z) ), where s(z) = \u221a \u03c3\u0303 1(z1) + \u03c3\u0303 2 2(z2), \u03c3\u0303 2 i (zi) = \u03c3 i zi \u03c32 W /\u03c3 2,0 i +zi , f(a) = a\u03a6(a) + \u03c6(a), \u03a6 and \u03c6 are the standard normal cumulative distribution and density respectively [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "Although the value of information is not concave in general in the two-alternative case, v is concave on the region where all zi\u2019s are large enough (see Theorem 2 in [14]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Concavity of v(z) is proven in Remark 2 by [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "It is shown that the value of information for measuring a single alternative may form an S-curve which is concave when there are many measurements, but may be convex at the beginning [14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 7, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 31, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 3, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 2, "context": "Since the seminal paper by [28], there has been a long history in the optimal learning literature of designing algorithms with provable asymptotic or finite-time bounds [2, 8, 34, 4, 16, 3].", "startOffset": 169, "endOffset": 189}, {"referenceID": 22, "context": "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "In order to obtain the prior distribution, we follow [24] and [23] to use Latin hypercube designs for initial fit.", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "We adopt the rule of thumb by [24] for the default number (10 \u00d7 p) of points, where p is the number of parameters to be estimated.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "In addition, as suggested by [23], to estimate the random errors, after the first 10 \u00d7 p points are evaluated, we add one replicate at each of the locations where the best p responses are found.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Kriging: [23] Let x\u2217 = arg maxx(\u03b8 n x + \u03c3 n x), then X(S) = arg max x (\u03b8 x \u2212 \u03b8 x\u2217)\u03a6( \u03b8 x \u2212 \u03b8 x\u2217 \u03c3n x ) + \u03c3 x\u03c6( \u03b8 x \u2212 \u03b8 x\u2217 \u03c3n x ),", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "UCB-E: [2] X(S) = arg max x \u03bc\u0302x + \u221a \u03b1 Nn x ,", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "SR: [2] Let A1 = X , log(M) = 12 + \u2211M i=2 1 i ,", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "\u03a6(\u03b6) and \u03c6(\u03b6) are, respectively, the cumulative standard normal distribution the standard normal density [15].", "startOffset": 105, "endOffset": 109}], "year": 2016, "abstractText": "We consider sequential decision problems in which we adaptively choose one of finitely many alternatives and observe a stochastic reward. We offer a new perspective of interpreting Bayesian ranking and selection problems as adaptive stochastic multi-set maximization problems and derive the first finite-time bound of the knowledge-gradient policy for adaptive submodular objective functions. In addition, we introduce the concept of prior-optimality and provide another insight into the performance of the knowledge gradient policy based on the submodular assumption on the value of information. We demonstrate submodularity for the two-alternative case and provide other conditions for more general problems, bringing out the issue and importance of submodularity in learning problems. Empirical experiments are conducted to further illustrate the finite time behavior of the knowledge gradient policy.", "creator": "LaTeX with hyperref package"}}}