{"id": "1606.04631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Bidirectional Long-Short Term Memory for Video Description", "abstract": "Video captioning has been attracting broad research attention in multimedia community. However, most existing approaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as \\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures bidirectional global temporal structure in video. Specifically, we first devise a joint visual modelling approach to encode video data by combining a forward LSTM pass, a backward LSTM pass, together with visual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The benefits are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense visual features and sparse semantic representations for videos and sentences, respectively. We verify the effectiveness of our proposed video captioning framework on a commonly-used benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods.", "histories": [["v1", "Wed, 15 Jun 2016 03:26:53 GMT  (225kb,D)", "http://arxiv.org/abs/1606.04631v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.MM cs.CL", "authors": ["yi bin", "yang yang", "zi huang", "fumin shen", "xing xu", "heng tao shen"], "accepted": false, "id": "1606.04631"}, "pdf": {"name": "1606.04631.pdf", "metadata": {"source": "CRF", "title": "Bidirectional Long-Short Term Memory for Video Description", "authors": ["Yi Bin", "Yang Yang", "Zi Huang", "Fumin Shen", "Xing Xu", "Heng Tao Shen"], "emails": ["yi.bin@hotmail.com,", "dlyyang@gmail.com,", "huang@itee.uq.edu.au", "fumin.shen@gmail.com,", "xing.xu@uestc.edu.cn,", "shenht@itee.uq.edu.au"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Computer Methods \u2192 Video Summary; Keywords Video Caption; Bidirectional Short-Term Memory"}, {"heading": "1. INTRODUCTION", "text": "With the development of digital media technology and the popularity of the mobile Internet, attention has increased rapidly in recent years. Subsequently, the analysis of visual content for retrieving information [31, 18] and understanding has become a fundamental problem in the field of multimedia research, which has stimulated worldwide research."}, {"heading": "2. THE PROPOSED APPROACH", "text": "In this section, we explain the proposed video captioning framework, including an introduction to the general flowchart (as illustrated in Figure 1), a brief overview of the LSTM-based sequential model, shared visual modeling with bidirectional LSTM and CNNs, and the sentence generation process."}, {"heading": "2.1 LSTM-based Sequential Model", "text": "With success in speech recognition and machine translation tasks, recurrent neural structures, in particular LSTM and its variants, have dominated the field of sequence processing. LSTM has been shown to be able to effectively address the problem of disappearance or explosion of gradients during reverse propagation through time (BPTT) [26] and exploit temporal dependencies in a very long time structure. LSTM includes several control gates and a constant memory cell, the details of which are as follows: it = \u03c3 (Wixxt + Wihht \u2212 1) (1) (1) ft = \u03c3 (Wfxxt + Wihht \u2212 1) (2) ot = \u03c3 (Woxxt + Wihht \u2212 1) (3) ct = ft ct = ct \u2212 1 + it? (Wcxxt + Wihht \u2212 1) (4) ht = ot? (ct) (5), where video-like matrices are multiplicity LSTM parameters, hyperlinearmic and density structures, and do not convey the function structure to STM."}, {"heading": "2.2 Bidirectional Video Modelling", "text": "Unlike other video description approaches, which represent video by implementing pooling over frames [24] or 3-D CNNs with local time structure [15], we use BiLSTM networks to exploit the bidirectional temporal structure of video clips. Convolutional Neural Networks (CNNs) has shown overwhelming performance in image recognition, classification [9] and video content analysis [3, 23]. Therefore, we extract caffe [7] fc7 layer of each frame using VGG-16 layers [20] of caffeine model. Subsequently [23, 24] we sample a frame from all ten frames in the video and extract the fc7 layer to express selected frames."}, {"heading": "2.3 Generating Video Description", "text": "Existing video subtitling approaches usually share the common part of the visual model and the language model as representation [23, 15], which can lead to severe loss of information, and they also enter the same pooled visual vector of the entire video into each sentence processing unit, ignoring the temporal structure. Such methods can easily lead to undesirable results at any time of the new sequence due to the duplicate input. [24] To solve these problems, we create descriptions for video clips using a sequential model initialized with visual representation. Inspired by the superior performance of the probabilistic sequence generation engine, we repeat each word at any time. Then, the protocol probability of the sentence S can be expressed as below: log p (S | V) = N phrase t = 1 protocol p (wt | V, w1,... wt \u2212 1; lt \u2212 1), with the target formation (6) referred to as target formation."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "Video dataset: We evaluate our approach by conducting experiments on the Microsoft Research Video Description (MSVD) [1] corpus, which is description for a collection of 1,970 video clips. Each video clip represents a single action or simple event such as \"shooting,\" \"cutting,\" \"playing the piano,\" and \"cooking,\" lasting from 8 seconds to 25 seconds, comprising approximately 43 available sentences per video and an average of 7 words per sentence. After the majority of previous work [15, 24, 23, 32] we divide the entire dataset into training, validation, and test sets of 1200, 100, and 670 frames. Image dataset: Compared to other LSTM structures and deep networks, the video dataset for subtitling tasks is small, applying the transfer of learning from image descriptions. COCO 2014 image description dataset [13] has been widely used to conduct experiments [8, 3, 2, which consist of more than 820,000 images] for the initial school descriptions and 820,000 + for the first class descriptions."}, {"heading": "3.2 Experimental Setup", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "4. CONCLUSION AND FUTURE WORKS", "text": "In this paper, we introduced a sequencing approach to the description of natural language video clips. At the core of our method was the application of two LSTM networks for the visual encoder component and natural language generators of our model. Specifically, we encoded video sequences using a bidirectional long-short term memory (BiLSTM) network that could effectively capture the bidirectional global time structure in video. Experimental results on the MSVD dataset showed superior performance compared to many other modern methods. We also point to some limitations in our model, such as the end-to-end framework used in [23] and the distance measured in [15]. In the future, we will make greater efforts to remove these limitations and exploit linguistic domain knowledge in understanding visual content."}, {"heading": "5. REFERENCES", "text": "In ACL, 2011. [2] X. Chen and C. Lawrence Zitnick. Mind's eye: A recurrent visual representation for image caption generation. In CVPR, 2015. [3] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015. [4] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Each image tells a story. In ECCV. Springer, 2010. [5] C. Gan, N. Wang, Y."}], "references": [{"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D.L. Chen", "W.B. Dolan"], "venue": "In ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Devnet: A deep event network for multimedia event detection and evidence recounting", "author": ["C. Gan", "N. Wang", "Y. Yang", "D.-Y. Yeung", "A.G. Hauptmann"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Meteor universal: language specific translation evaluation for any target", "author": ["M.D.A. Lavie"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Summarization-based video caption via deep neural networks", "author": ["G. Li", "S. Ma", "Y. Han"], "venue": "In ACM Multimedia,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "arXiv preprint arXiv:1505.01861,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning binary codes for maximum inner product search", "author": ["F. Shen", "W. Liu", "S. Zhang", "Y. Yang", "H. Tao Shen"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H.T. Shen"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Very deep  convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Combining the right features for complex event recognition", "author": ["K. Tang", "B. Yao", "L. Fei-Fei", "D. Koller"], "venue": "In ICCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Fusing multi-stream deep networks for video classification", "author": ["Z. Wu", "Y.-G. Jiang", "X. Wang", "H. Ye", "X. Xue", "J. Wang"], "venue": "arXiv preprint arXiv:1509.06086,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Modeling spatial-temporal clues in a hybrid deep learning framework for video classification", "author": ["Z. Wu", "X. Wang", "Y.-G. Jiang", "H. Ye", "X. Xue"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Exploiting web images for semantic video indexing via robust sample-specific", "author": ["Y. Yang", "Z.-J. Zha", "Y. Gao", "X. Zhu", "T.-S. Chua"], "venue": "loss. TMM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Visual coding in a semantic hierarchy", "author": ["Y. Yang", "H. Zhang", "M. Zhang", "F. Shen", "X. Li"], "venue": "In ACM Multimedia,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "In ICCV,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Subsequently, visual content analysis for retrieving [31, 18] and understanding becomes a fundamental problem in the area of multimedia research, which has motivated world-wide researchers", "startOffset": 53, "endOffset": 61}, {"referenceID": 17, "context": "Subsequently, visual content analysis for retrieving [31, 18] and understanding becomes a fundamental problem in the area of multimedia research, which has motivated world-wide researchers", "startOffset": 53, "endOffset": 61}, {"referenceID": 8, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 18, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 16, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 20, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 29, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 3, "context": "With some pioneering methods [4, 14] tackling the challenge of describing images with natural language proposed, visual content understanding has attracted more and more attention.", "startOffset": 29, "endOffset": 36}, {"referenceID": 13, "context": "With some pioneering methods [4, 14] tackling the challenge of describing images with natural language proposed, visual content understanding has attracted more and more attention.", "startOffset": 29, "endOffset": 36}, {"referenceID": 1, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 2, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 7, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 24, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 28, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 14, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 31, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 10, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 22, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 31, "context": "proposed to use 3D Convolutional Neural Networks to explore local temporal information in video clips, where the most relevant temporal fragments were automatically chosen for generating natural language description with attention mechanism [32].", "startOffset": 241, "endOffset": 245}, {"referenceID": 22, "context": "In [23], Venugopanlan et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "We summarize the main contributions of this work as follows: (1) To our best knowledge, our approach is one of the first to utilize bidirectional recurrent neural networks for exploring bidirectional global temporal structure in video captioning; (2) We construct two sequential processing models for adaptive video representation learning and language description generation, respectively, rather than using the same LSTM for both video frames encoding and text decoding in [23]; and (3) Extensive experiments on a real-world video corpus illustrate the ar X iv :1 60 6.", "startOffset": 475, "endOffset": 479}, {"referenceID": 5, "context": "LSTM has been demonstrated to be able to effectively address the gradients vanishing or explosion problem [6] during back-propagation through time (BPTT) [26] and to exploit temporal dependencies in very long temporal structure.", "startOffset": 106, "endOffset": 109}, {"referenceID": 25, "context": "LSTM has been demonstrated to be able to effectively address the gradients vanishing or explosion problem [6] during back-propagation through time (BPTT) [26] and to exploit temporal dependencies in very long temporal structure.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Different from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we apply BiLSTM networks to exploit the bidirectional temporal structure of video clips.", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Different from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we apply BiLSTM networks to exploit the bidirectional temporal structure of video clips.", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 22, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 6, "context": "Therefore, we extract caffe [7] fc7 layer of each frame through VGG-16 layers [20] caffemodel.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "Therefore, we extract caffe [7] fc7 layer of each frame through VGG-16 layers [20] caffemodel.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames.", "startOffset": 10, "endOffset": 18}, {"referenceID": 26, "context": "In [27, 28], Wu et al.", "startOffset": 3, "endOffset": 11}, {"referenceID": 27, "context": "In [27, 28], Wu et al.", "startOffset": 3, "endOffset": 11}, {"referenceID": 22, "context": "Existing video captioning approaches usually share common part of visual model and language model as representation [23, 15], which may lead to severe information loss.", "startOffset": 116, "endOffset": 124}, {"referenceID": 14, "context": "Existing video captioning approaches usually share common part of visual model and language model as representation [23, 15], which may lead to severe information loss.", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "Such methods may easily result in undesirable outputs due to the duplicate inputs in every time point of the new sequence [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "During test phrase, similar to [23], our language model takes the word wt\u22121 with maximum likelihood as input at time t repeatedly until the <EOS> token is emitted.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Video Dataset: We evaluate our approach by conducting experiments on the Microsoft Research Video Description (MSVD) [1] corpus, which is description for a collection of 1,970 video clips.", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 23, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 22, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 31, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 12, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 2, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 1, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 28, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 23, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 22, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 14, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 22, "context": "We employ a bidirectional S2VT [23] and a joint bidirectional LSTM structure to investigate the performance of our bidirectional approach.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the first video encoder in unidirectional joint LSTM.", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the first video encoder in unidirectional joint LSTM.", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in [23], Venugopalan et al.", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "Bidirectional S2VT: Similar to [23], we implement several S2VT-based models: S2VT, bidirectional S2VT and reinforced S2VT with bidirectional LSTM video encoder.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "We conduct experiment on S2VT using our video features and LSTM structure instead of the end-to-end model in [23], which need original RGB frames as input.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "The authors of CIDEr also argued for that METEOR outperforms CIDEr when the reference set is small [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "S2VT [23]", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "LSTM-E (VGG) [15] 29.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "5 LSTM-E (C3D) [15] 29.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "[32] 29.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The result of \u201cLSTM\u201d in first row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in [32].", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "The result of \u201cLSTM\u201d in first row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in [32].", "startOffset": 206, "endOffset": 210}, {"referenceID": 22, "context": "We observed that while our unidirectional S2VT has the same deployment as [23], our model gives a little poorer performance(line 1, Table 1 and line 3, Table 2).", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "We also note some limitations in our model, such as endto-end framework employed in [23] and distance measured in [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "We also note some limitations in our model, such as endto-end framework employed in [23] and distance measured in [15].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Video captioning has been attracting broad research attention in multimedia community. However, most existing approaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as Bidirectional Long-Short Term Memory (BiLSTM), which deeply captures bidirectional global temporal structure in video. Specifically, we first devise a joint visual modelling approach to encode video data by combining a forward LSTM pass, a backward LSTM pass, together with visual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The benefits are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense visual features and sparse semantic representations for videos and sentences, respectively. We verify the effectiveness of our proposed video captioning framework on a commonlyused benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}