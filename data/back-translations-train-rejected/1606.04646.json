{"id": "1606.04646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Unsupervised Learning of Predictors from Unpaired Input-Output Samples", "abstract": "Unsupervised learning is the most challenging problem in machine learning and especially in deep learning. Among many scenarios, we study an unsupervised learning problem of high economic value --- learning to predict without costly pairing of input data and corresponding labels. Part of the difficulty in this problem is a lack of solid evaluation measures. In this paper, we take a practical approach to grounding unsupervised learning by using the same success criterion as for supervised learning in prediction tasks but we do not require the presence of paired input-output training data. In particular, we propose an objective function that aims to make the predicted outputs fit well the structure of the output while preserving the correlation between the input and the predicted output. We experiment with a synthetic structural prediction problem and show that even with simple linear classifiers, the objective function is already highly non-convex. We further demonstrate the nature of this non-convex optimization problem as well as potential solutions. In particular, we show that with regularization via a generative model, learning with the proposed unsupervised objective function converges to an optimal solution.", "histories": [["v1", "Wed, 15 Jun 2016 05:26:29 GMT  (195kb,D)", "http://arxiv.org/abs/1606.04646v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jianshu chen", "po-sen huang", "xiaodong he", "jianfeng gao", "li deng"], "accepted": false, "id": "1606.04646"}, "pdf": {"name": "1606.04646.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Predictors from Unpaired Input-Output Samples", "authors": ["Jianshu Chen", "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao"], "emails": ["deng}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able."}, {"heading": "2 Related Work", "text": "For unsupervised learning, which is applied to predictions and related tasks, several main approaches have been taken in the past. An important line of research has been the use of the structure of input data by learning the distribution of data with maximum probability. The most successful examples in this category are the limited Boltzmann machine (RBM) [26, 15], the deep beliefs [14], the topic models [12]. The main technical challenge of these methods is the difficulty of calculating the gradient of probability. Therefore, various approximate methods have been developed, such as the variable inference [16] and the Monte Carlo method [12]."}, {"heading": "3 Problem Formulation", "text": "In this section, we first formulate the unattended learning problem. Let xt be the t-th input vector, which is an M-dimensional real value vector, and let yt be the t-th output vector. In this paper, we view the classification problem in such a way that yt is a C-dimensional uniform vector representing one of the C-classes. In prediction tasks, the goal is to learn the conditional probability p (yt | xt, Wd) from training samples, whereas Wd can be the model parameter. p (yt | xt, Wd) any parametric model can be like neural networking capabilities."}, {"heading": "4 Learning to Predict from Unpaired Samples", "text": "We are now developing a novel cost function for learning the predictor p (yt | xt, Wd) in an unsupervised manner. The cost function is based on the following two key findings: first, that we have the predicted output sequence y = 1,. second, that we should correlate the predicted output sequence y = 1,. second, that the predicted output sequence y [t] should be based on the input sequence x1,. xT is that the output sequence y [t] should be correlated with the output sequence p (y1,., yT) rather than being completely independent of it. Second, that we want the predicted output sequence y [t] to be based on the input sequence xt; that is the output sequence y [t], which should be correlated with the input sequence xt, rather than being completely independent of it. The first term measures how well the predicted output sequence y [t] should be based on the input sequence xt; that is the output sequence y [t], which should be correlated to the input sequence xt, rather than being completely independent of it."}, {"heading": "5 Experiments and Analysis", "text": "In this section, we will use a simplified prediction task on a synthetic dataset to examine the effectiveness of the proposed approach. We will also analyze the behavior of the proposed objective function to understand the nature and difficulties of the unattended learning problem for prediction along with its potential solutions."}, {"heading": "5.1 Experimental setup", "text": "The synthetic data we use to evaluate the algorithm is generated in the following way: We first generate the output sequence y1,.., yT according to the distribution p (y1,.., yT) = 1 p (yt | yt \u2212 1), i.e. a Markov chain described in Figure 1. And we consider a four-class classification problem so that yt is a 4-dimensional hot vector. After the sequence y1,.., yT we randomly generate a permutation matrix Q and fix it over time. For each yt we generate the text by multiplying Q to the left of xt, i.e. xt = Qyt. Therefore, the inputs {xt} are also a 4-dimensional uniform vector matrix, except that each of them is transformed from the output sequence yp."}, {"heading": "5.2 The landscape of the proposed unsupervised cost function", "text": "We first plot the landscape of the cost function (4) for \u03bb = 0 case and compare it with the monitored costs (cross entropy) in Figure 2 (a). Specifically, we plot the negative of the objective function (4) along the line tWd, 0 + (1 \u2212 t) Wd, 1, where t is a real scalar, Wd, 0 is the basic truth (obtained from the permutation matrix) and Wd, 1 is the finally converged solution by optimization (4) without regulation (\u03bb = 0). Obviously, the objective function is highly non-convex. On the other hand, the cost function for monitored learning is convex, since the classifier is linear. An important observation we can make from Figure 2 (a) is that the global optimal solution to (2) (i.e., the first term in (4)) is the optimal solution of the Wxt."}, {"heading": "5.3 The importance of regularization", "text": "In Figure 2 (b) we present the scenarios of the unattended cost function (4) for \u03bb = 0 and \u03bb = 30. The scenarios show the values of the cost function along a random line leading through the basic truth (global optimal solution).We observe that the concept of regulation generates an \"inclination\" at the initial position of the local optimal solution, allowing the algorithm to escape from the trivial solution. In Figures 3 (b) and 3 (d) we plot more landscapes for unattended costs with different levels of regulation and observe similar behaviors where the local optima are smoothed out by the concept of regulation. Finally, the obtained solution is presented with \u03bb = 30 in the right part of Figure 2 (c).As a reference, we also present the global optimal solution to the supervised problem in the left part of Figure 2 (c)."}, {"heading": "6 Conclusions", "text": "In this paper, we examine the important problem of unattended predictive learning, which consists in learning to make predictions without using data from input-label pairs. We address this difficult problem by using the sequence structure of the output samples to learn the predictive factor. In other words, we have proposed an objective function that aims to integrate the predicted results into the structure of the output while maintaining the correlation between the input and the predicted output. In a synthetic structural prediction problem, we show that the objective function is already strongly unconvex even with simple linear classifiers. On the other hand, this objective function is moving towards an optimal solution. We are currently investigating the behavior of more complicated and realistic models with real-world data. Along this research line, a recent paper [7] shows that local optima is good during the supervised learning of deep neural networks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Unsupervised transcription of historical documents", "author": ["Taylor Berg-Kirkpatrick", "Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "End-to-end learning of lda by mirror-descent back propagation over a deep architecture", "author": ["Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollar", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt", "Lawrence Zitnick", "Geoffrey Zweig"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W Keith Hastings"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1970}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Unsupervised analysis for decipherment problems", "author": ["Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada"], "venue": "In Proceedings of the COLING/ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In Interspeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Semi-supervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1986}, {"title": "Towards principled unsupervised learning", "author": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1511.06440,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Survey of clustering algorithms", "author": ["Rui Xu", "Donald Wunsch"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 126, "endOffset": 133}, {"referenceID": 7, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 126, "endOffset": 133}, {"referenceID": 19, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 153, "endOffset": 161}, {"referenceID": 32, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 153, "endOffset": 161}, {"referenceID": 27, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 183, "endOffset": 190}, {"referenceID": 0, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 183, "endOffset": 190}, {"referenceID": 22, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 222, "endOffset": 230}, {"referenceID": 21, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 222, "endOffset": 230}, {"referenceID": 9, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 253, "endOffset": 269}, {"referenceID": 16, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 253, "endOffset": 269}, {"referenceID": 29, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 253, "endOffset": 269}, {"referenceID": 30, "context": "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].", "startOffset": 253, "endOffset": 269}, {"referenceID": 31, "context": "Unsupervised learning, however, is not as successful on these prediction tasks, although it has found other useful applications such as clustering [32], text analysis [5], etc.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "Unsupervised learning, however, is not as successful on these prediction tasks, although it has found other useful applications such as clustering [32], text analysis [5], etc.", "startOffset": 167, "endOffset": 170}, {"referenceID": 20, "context": ", [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "This approach, albeit widely used, is usually less effective than end-to-end learning with labeled data [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].", "startOffset": 218, "endOffset": 235}, {"referenceID": 2, "context": "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].", "startOffset": 218, "endOffset": 235}, {"referenceID": 1, "context": "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].", "startOffset": 218, "endOffset": 235}, {"referenceID": 23, "context": "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].", "startOffset": 218, "endOffset": 235}, {"referenceID": 8, "context": "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].", "startOffset": 218, "endOffset": 235}, {"referenceID": 12, "context": "Pre-training is shown to be effective only when there is a small amount of labeled data available [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.", "startOffset": 93, "endOffset": 101}, {"referenceID": 14, "context": "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.", "startOffset": 93, "endOffset": 101}, {"referenceID": 13, "context": "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.", "startOffset": 146, "endOffset": 149}, {"referenceID": 15, "context": "For this reason, various approximate methods have been developed, such as variational inference [16] and Monte Carlo methods [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "For this reason, various approximate methods have been developed, such as variational inference [16] and Monte Carlo methods [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 26, "context": "A recent study that is more closely related to what we describe in this paper is [27], which proposes the output distribution matching (ODM) as an alternative unsupervised learning objective to the likelihood function of the data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "The power of such a strong prior of language models in unsupervised learning has been demonstrated in an earlier study reported in [19].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "The use of generative models in our work is similar to an earlier study reported in [4] and to a more recent study reported in [25].", "startOffset": 84, "endOffset": 87}, {"referenceID": 24, "context": "The use of generative models in our work is similar to an earlier study reported in [4] and to a more recent study reported in [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Along this line of research, a recent work [7] shows that the local optima during supervised learning of the deep neural networks are well behaved.", "startOffset": 43, "endOffset": 46}], "year": 2016, "abstractText": "Unsupervised learning is the most challenging problem in machine learning and especially in deep learning. Among many scenarios, we study an unsupervised learning problem of high economic value \u2014 learning to predict without costly pairing of input data and corresponding labels. Part of the difficulty in this problem is a lack of solid evaluation measures. In this paper, we take a practical approach to grounding unsupervised learning by using the same success criterion as for supervised learning in prediction tasks but we do not require the presence of paired input-output training data. In particular, we propose an objective function that aims to make the predicted outputs fit well the structure of the output while preserving the correlation between the input and the predicted output. We experiment with a synthetic structural prediction problem and show that even with simple linear classifiers, the objective function is already highly non-convex. We further demonstrate the nature of this non-convex optimization problem as well as potential solutions. In particular, we show that with regularization via a generative model, learning with the proposed unsupervised objective function converges to an optimal solution.", "creator": "LaTeX with hyperref package"}}}