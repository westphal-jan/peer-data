{"id": "1606.04671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Progressive Neural Networks", "abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.", "histories": [["v1", "Wed, 15 Jun 2016 08:20:51 GMT  (2325kb,D)", "http://arxiv.org/abs/1606.04671v1", null], ["v2", "Tue, 21 Jun 2016 22:03:05 GMT  (7786kb,D)", "http://arxiv.org/abs/1606.04671v2", null], ["v3", "Wed, 7 Sep 2016 10:59:12 GMT  (7784kb,D)", "http://arxiv.org/abs/1606.04671v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrei a rusu", "neil c rabinowitz", "guillaume desjardins", "hubert soyer", "james kirkpatrick", "koray kavukcuoglu", "razvan pascanu", "raia hadsell"], "accepted": false, "id": "1606.04671"}, "pdf": {"name": "1606.04671.pdf", "metadata": {"source": "CRF", "title": "Progressive Neural Networks", "authors": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell"], "emails": ["raia}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Finetuning remains the method of choice for transferring learning with neural networks: a model is pre-trained on a source domain (where data is often abundant), the output layers of the model are adapted to the target domain, and the network is fine-tuned via backpropagation. This approach was advanced in [7] by transferring knowledge from a generative to a discriminatory model, and has since been generalized with great success [11]. Unfortunately, the approach has drawbacks that make it unsuitable for transferring it across multiple tasks: if we want to make knowledge about a sequence of experiences effective, which model should we use to initialize subsequent models, this seems to require not only a learning method that can support the transmission of learning without forgetting catastrophic forgetfulness, but also a prior knowledge of the similarity of tasks. Moreover, while finetuning allows us to gain expertise in the target domain, it is a destructive process in which we are not emboldened."}, {"heading": "2 Progressive Networks", "text": "Continuous learning is a long-standing goal of machine learning, where the actors not only learn (and remember), but also have the ability to transfer knowledge from previous tasks to improve the convergence rate [20]. Progressive networks integrate these desiderata directly into the model architecture: catastrophic forgetting is prevented by solving a new neural network (one column) for each task, while allowing transfer via lateral connections to previously learned columns. Scalability of this approach is addressed at the end of this section. A progressive network begins with a single column: a deep neural network with hidden activations. (1) i Rni of the number of units in layer i \u2264 L, and parameters. (1) When we switch to a second task, the parameters are \"frozen\" and a new column with parameters."}, {"heading": "3 Transfer Analysis", "text": "We have examined two related methods: an intuitive but slow method based on error analysis, and a faster analytical method derived from the Fisher information [2]. To assess the degree to which the source columns contribute to the target task, we can inject Gaussian noise at isolated points in the architecture (e.g. a particular layer of a single column) and measure the impact of this disturbance on performance. A significant power drop indicates that the final prediction depends heavily on the characteristic map or layer. We find that this method yields results similar to the faster Fisher-based method presented below. Therefore, we ban details and results of the error analysis to the appendix. A significant power drop indicates that the final prediction strongly depends on the characteristic map or layer."}, {"heading": "4 Related Literature", "text": "There are many different paradigms for the transfer and reinforcement of tasks, as these have long been recognized as critical challenges in AI research. [15, 19, 20] Many methods for transfer learning are based on linear and other simple models (e.g. [18]), which is a limiting factor for their applicability. Recently, new methods for multi-task learning or deep RL transfer learning have been proposed: [21, 17, 14]. In this work, we present an architecture for deep reinforcement learning that allows learning in sequential task regimes, without forgetting the individual function transfer from previously learned tasks."}, {"heading": "5 Experiments", "text": "We evaluate progressive networks in three different RL areas. First, we look at synthetic versions of Pong that are similar on a visual or control level. Second, we experiment extensively with random sequences of Atari games and perform a feature-level transfer analysis. Finally, we demonstrate performance on a number of 3D labyrinth games. Fig. 2 shows examples of selected tasks."}, {"heading": "5.1 Setup", "text": "We rely on the Async Advantage Actor-Critic (A3C) framework introduced in [13]. Compared to DQN [12], the model simultaneously learns a policy and a value function to predict future rewards. A3C is trained on the CPU using multiple threads and has been shown to converge faster than DQN on the GPU, making it more natural for the large number of sequential experiments required for this work. 2The Fisher of single neurons (fully connected) and function charts (revolutionary layers) are calculated using \u03c1\u03c0 (k) (s, a). The use of normalized representation is not standard, but makes the scale of F comparable across layers and columns."}, {"heading": "5.2 Pong Soup", "text": "The aforementioned lcihsrc\u00fcehncS nvo nde eerwdnei rf\u00fc ide eerwdnei eerwdnei rf\u00fc ide eerwdnei eerwdnei nlrsAeer\u00fc\u00dfe\u00fcGr ni red eerwlrtee\u00fcgBnlrheeu ni red eerwdnei nlrgne\u00fceaeegnlrVrlrlrte\u00fce ni rde nlrf\u00fc ide eaeerwlrlrrgne\u00fce\u00fce\u00fceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeetnlrVnlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.3 Atari Games", "text": "This is an interesting question, because the visualizations of the Atari games are very different from each other, as are the controls and the required strategy. Although games such as Pong and Breakout are conceptually similar (both involve hitting a ball with a paddle), Pong is vertically oriented, while it is a horizontal game that is potentially insurmountable. Other Atari game pairs have no discernible overlap, not even on a conceptual level. To this end, we start by training individual columns on three source games (Pong, Raid, and Seaquest) 3 and evaluate whether the learned characteristics are transferred to another subset. Alien, Asterix, Boxing, Centipede, Gopher, James Bond, Krull, Robotank, Road Runner, Star Gunner, and Wizard of Wor."}, {"heading": "5.4 Labyrinth", "text": "The final experimental framework for progressive networking is Labyrinth, a 3D labyrinth environment in which the input is displayed in images that allow partial visibility, and the agent performs discrete actions, including looking up, down, left, or right and moving forward, backward, left, or right. Quests and level maps are manifold and include positive values for \"eating\" good items (apples, strawberries) and negative values for eating bad items (mushrooms, lemons). Details are given in the appendix. While there is conceptual and visual overlap between the various tasks, the tasks present a challenging set of different game elements (Figure 2). As in other areas, the progressive approach results in a more positive transfer than any of the baseline (see Figure 8a and Table 1). We observe less transfer at the Seek Track level, which have dense reward elements throughout the labyrinth and are easy to learn."}, {"heading": "6 Conclusion", "text": "Continuous learning, the ability to accumulate knowledge and transfer it to new areas, is a central characteristic of intelligent beings. Progressive neural networks are a springboard to continuous learning, and this work has proven its potential through experimentation and analysis in three RL areas, including Atari, which contains orthogonal or even hostile tasks. We believe that we are the first to show positive transfer to deep RL agents within a continuous learning framework. Furthermore, we have shown that the progressive approach is able to effectively use transfer for compatible source and task areas; that the approach is robust against harmful characteristics learned in incompatible tasks; and that positive transfer increases with the number of columns, confirming the constructive and non-destructive nature of the progressive architecture."}, {"heading": "A Perturbation Analysis", "text": "We examined two related methods for analyzing transfer in progressive networks. A method based on Fisher information yields the average Fisher sensitivity (AFS) and is described in Section 3 of the paper. We describe the second method based on the disturbance analysis in this appendix, as it proved too slow to be used on scale. However, given its intuitive appeal, we provide details of the method along with the results for the Pong variants (see Section 5.2) to confirm the AFS scale. Our disturbance analysis aims to estimate which components of the source columns contribute significantly to the performance of the last column in the target tasks. To this end, we injected Gaussian noise into each of the (post-ReLU) hidden representations, with a new sample at each forward run, and calculated the average effect of these disturbances on the score over 10 episodes of game value. We did this on a rough scale by adding all noise over a given layer."}, {"heading": "B Compressibility of Progressive Networks", "text": "As described in the main text, one of the limitations of progressive networks is the growth of the size of the network with additional tasks. In the basic approach we follow in the main text, the number of hidden units and feature cards grows linearly with the number of columns, and the number of parameters in the Atari task quadruples. Here, we tried to determine the degree to which this full capacity is actually used by the network. We used the measurement variable Average Fisher Sensitivity to investigate how increasing the number of columns in the Atari task changes the need for additional resources. In Figure 10a, we measure the average breakage of existing feature cards in a given layer (here layer 2). We do this for each network by combining the AFS values per feature card from all source columns in that layer, sorting the values to generate a spectrum, and then comparing values on average across the networks."}, {"heading": "C Setup Details", "text": "In our grid, we take hyperparameters from categorical distributions: \u2022 Learning rate queried from {10 \u2212 3, 5 \u00b7 10 \u2212 4, 10 \u2212 4}. \u2022 Strength of entropy regulation from {10 \u2212 2, 10 \u2212 3, 10 \u2212 4} \u2022 Gradient section from {20, 40} \u2022 Scalar multiplier on the lateral characteristic is randomly initialized to one from {1, 10 \u2212 1, 10 \u2212 2}. For the Atari experiments, we used a model with three Constitutional layers, followed by a fully bonded layer and from which we predict the guideline and value function. Convolutionary layers are as follows. All have 12 characteristic cards. The first Convolutionary layer has a core of size 8x8 and a step of 4x4. The second layer has a core of size 4 and a strip of 2. The last Convolutionary layer has a size 3x4 with a strip of 1. The fully bonded layer has 256 layers."}, {"heading": "D Learning curves", "text": "Figure 11 shows training curves for all target games we considered. Diagrams show how one column, two columns, and three columns of progressive networks evolve compared to baseline 3, a pre-trained and then finely tuned model on each target game, and baseline 2, where only the output level is used to train on the features used in the source game Seaquest. We can see that the entire baseline 3 performs pretty well. However, there are situations where features we learned from a previous task actually help with the transfer (e.g. when target game is boxes)."}, {"heading": "E Labyrinth", "text": "Section 5.4 evaluates progressive foraging networks in complex 3D labyrinth environments. Agents receive positive rewards for collecting apples and strawberries and negative rewards for mushrooms and lemons. Episodes end when either all (positive) rewards are collected or after a fixed time interval. Levels differ in their labyrinth layout, the type of items available and the scarcity of the reward structure. The levels we use can be characterized as follows: \u2022 Seek Track 1: simple corridor with many apples \u2022 Seek Track 2: U-shaped corridor with many strawberries \u2022 Seek Track 3: area-shaped, with 90o twists, with few apples \u2022 Seek Track 4: area-shaped, with 45o twists, with few apples \u2022 Seek Track 1: large square corridor with apples and lemons \u2022 Sequek Aid 2: large area with apples and apples."}], "references": [{"title": "Adaptive multi-column deep neural networks with application to robust image denoising", "author": ["Forest Agostinelli", "Michael R Anderson", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-ichi Amari"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research (JAIR), 47:253\u2013279", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "In JMLR: Workshop on Unsupervised and Transfer Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan C. Ciresan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The cascade-correlation learning architecture", "author": ["Scott E. Fahlman", "Christian Lebiere"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Distilling the knowledge in a neural network", "author": ["Goeff Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "CoRR, abs/1503.02531,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "Network in network", "author": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "venue": "In Proc. of Int\u2019l Conference on Learning Representations (ICLR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Unsupervised and transfer learning challenge: a deep learning approach", "author": ["G. Mesnil", "Y. Dauphin", "X. Glorot", "S. Rifai", "Y. Bengio", "I. Goodfellow", "E. Lavoie", "X. Muller", "G. Desjardins", "D. Warde-Farley", "P. Vincent", "A. Courville", "J. Bergstra"], "venue": "JMLR W& CP: Proc. of the Unsupervised and Transfer Learning challenge and workshop, volume 27", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Kk Kavukcuoglu", "author": ["V. Mnih"], "venue": "D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Lei Jimmy Ba", "Ruslan Salakhutdinov"], "venue": "In Proc. of Int\u2019l Conference on Learning Representations (ICLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Continual Learning in Reinforcement Environments", "author": ["Mark B. Ring"], "venue": "R. Oldenbourg Verlag,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Beyond sharing weights for deep domain adaptation", "author": ["Artem Rozantsev", "Mathieu Salzmann", "Pascal Fua"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Policy distillation", "author": ["A. Rusu", "S. Colmenarejo", "\u00c7. G\u00fcl\u00e7ehre", "G. Desjardins", "J. Kirkpatrick", "R. Pascanu", "V. Mnih", "K. Kavukcuoglu", "R. Hadsell"], "venue": "abs/1511.06295", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["Paul Ruvolo", "Eric Eaton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Lifelong machine learning systems: Beyond learning algorithms", "author": ["Daniel L. Silver", "Qiang Yang", "Lianghao Li"], "venue": "In AAAI Spring Symposium: Lifelong Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "An introduction to inter-task transfer for reinforcement learning", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "AI Magazine,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "ArXiv e-prints", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11].", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "While distillation [8] offers one potential solution to multitask learning [17], it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold.", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "While distillation [8] offers one potential solution to multitask learning [17], it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Continual learning is a long-standing goal of machine learning, where agents not only learn (and remember) a series of tasks experienced in sequence, but also have the ability to transfer knowledge from previous tasks to improve convergence speed [20].", "startOffset": 247, "endOffset": 251}, {"referenceID": 21, "context": "Finetuning is efficient in this setting, as parameters need only be adjusted slightly to the target domain, and often only the top layer is retrained [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "For convolutional layers, dimensionality reduction is performed via 1\u00d7 1 convolutions [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "by adding fewer layers or less capacity, by pruning [9], or by online compression [17] during learning.", "startOffset": 52, "endOffset": 55}, {"referenceID": 16, "context": "by adding fewer layers or less capacity, by pruning [9], or by online compression [17] during learning.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "We explored two related methods: an intuitive, but slow method based on a perturbation analysis, and a faster analytical method derived from the Fisher Information [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "We can get a local approximation to the perturbation sensitivity by using the Fisher Information matrix [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "There exist many different paradigms for transfer and multi-task reinforcement learning, as these have long been recognized as critical challenges in AI research [15, 19, 20].", "startOffset": 162, "endOffset": 174}, {"referenceID": 18, "context": "There exist many different paradigms for transfer and multi-task reinforcement learning, as these have long been recognized as critical challenges in AI research [15, 19, 20].", "startOffset": 162, "endOffset": 174}, {"referenceID": 19, "context": "There exist many different paradigms for transfer and multi-task reinforcement learning, as these have long been recognized as critical challenges in AI research [15, 19, 20].", "startOffset": 162, "endOffset": 174}, {"referenceID": 17, "context": "[18]), which is a limiting factor to their applicability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Recently, there have been new methods proposed for multi-task or transfer learning with deep RL: [21, 17, 14].", "startOffset": 97, "endOffset": 109}, {"referenceID": 16, "context": "Recently, there have been new methods proposed for multi-task or transfer learning with deep RL: [21, 17, 14].", "startOffset": 97, "endOffset": 109}, {"referenceID": 13, "context": "Recently, there have been new methods proposed for multi-task or transfer learning with deep RL: [21, 17, 14].", "startOffset": 97, "endOffset": 109}, {"referenceID": 6, "context": "Pretraining and finetuning was proposed in [7] and applied to transfer learning in [4, 11], generally in unsupervised-to-supervised or supervised-to-supervised settings.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "Pretraining and finetuning was proposed in [7] and applied to transfer learning in [4, 11], generally in unsupervised-to-supervised or supervised-to-supervised settings.", "startOffset": 83, "endOffset": 90}, {"referenceID": 10, "context": "Pretraining and finetuning was proposed in [7] and applied to transfer learning in [4, 11], generally in unsupervised-to-supervised or supervised-to-supervised settings.", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "The actor-mimic approach [14] applied these principles to reinforcement learning, by fine-tuning a DQN multi-task network on new Atari games and showing that some responded with faster learning, while others did not.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "The cascade-correlation architecture was designed to eliminate forgetting while incrementally adding and refining feature extractors [6], and is reminiscent of boosting or ensemble methods.", "startOffset": 133, "endOffset": 136}, {"referenceID": 15, "context": "Auto-encoders such as [23] use incremental feature augmentation to track concept drift, and deep architectures such as [16] have been designed that specifically support feature transfer.", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "More recently, in [1], columns are separately trained on individual noise types, then combined with a learned linear weighting.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Columns are also proposed by [5] for image classification.", "startOffset": 29, "endOffset": 32}, {"referenceID": 12, "context": "We rely on the Async Advantage Actor-Critic (A3C) framework introduced in [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Compared to DQN [12], the model simultaneously learns a policy and a value function for predicting expected future rewards.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "We next investigate feature transfer between randomly selected Atari games [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 19, "context": "This is a known challenge in transfer learning [20]: learned source tasks confer an inductive bias that can either help or hinder in different cases.", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "Learning to solve complex sequences of tasks\u2014while both leveraging transfer and avoiding catastrophic forgetting\u2014remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.", "creator": "LaTeX with hyperref package"}}}