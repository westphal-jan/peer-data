{"id": "1606.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning", "abstract": "We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex trade- offs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG pol- icy using Reinforcement Learning (RL), which adapts its behaviour to noisy feed- back from the current generation context. This policy is compared to several base- lines derived from previous work in this area. The learned policy significantly out- performs all the prior approaches.", "histories": [["v1", "Wed, 15 Jun 2016 09:05:56 GMT  (55kb,D)", "http://arxiv.org/abs/1606.04686v1", "published EACL 2009"]], "COMMENTS": "published EACL 2009", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["verena rieser", "oliver lemon"], "accepted": false, "id": "1606.04686"}, "pdf": {"name": "1606.04686.pdf", "metadata": {"source": "CRF", "title": "Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems", "authors": ["Verena Rieser", "Oliver Lemon"], "emails": ["vrieser@inf.ed.ac.uk", "olemon@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In a spoken dialogue system (SDS), therefore, an abstract communicative goal (CG) can be generated in many ways. For example, the CG can be realized in order to present database results to the user as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing elements (Walker et al., 2004), or by selecting an element and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated results to certain features of the dialogue context, e.g. user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006).In extending this previous work, we treat NLG as a statistical problem that we see in relation to a static system."}, {"heading": "2 NLG as planning under uncertainty", "text": "We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the original version of this approach. \u2022 Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning.NLG actions take place in a stochastic environment, for example, consisting of a user, a realiser and a TTS system, in which the individual NLG actions have an uncertain impact on the environment. For example, displaying different numbers of attributes for the user, and making the user more or less likely to choose an element as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. However, our goal is to use a stochastic realiser for SDS, see for example (Stent et al, Divizer al, 2004)."}, {"heading": "3 The Information Presentation Problem", "text": "We will address the well-researched problem of information presentation in the NLG to demonstrate the benefits of this approach, which is to find the best way to present a user with a set of search results (e.g. some restaurants that meet certain limitations), a task common to many previous work in the NLG, e.g. (Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008), in which many decisions are available for research, such as which presentation strategy to apply (NLG strategy selection), how many attributes of each object to present (attribute selection), how to classify the objects and attributes according to different models of user preferences (attribute order), how many (specific) objects to describe (conciseness), how many sentences to use when we do so (syntactical planning), and which words to use (lexical selection), all of these results will be optimized based on these and many other potentially available areas."}, {"heading": "4 MATCH corpus analysis", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "5 Method: the RL-NLG model", "text": "For the above reasons, we treat the NLG module as a statistical planner operating in a stochastic environment and optimize it with Reinforcement Learning. Entering the module is a communicative goal provided by the dialog manager. The CG consists of a dialog section to be generated, for example, a current article (i1, i2, i8), and a system target (SysGoal), which is the desired user response to get the user to choose one of the featured elements (i1, i2, i8)."}, {"heading": "6 Experiments", "text": "We are now reporting on a proof-of-concept study in which we explore our policies in a simulated learning environment based on the results of the MATCH corpus analysis in Section 4. Simulation-based directives allow us to explore invisible actions that are not included in the data, and therefore less initial data is needed (Rieser and Lemon, 2008b). Note that we cannot learn directly from the MATCH data, as we would need data from an interactive dialogue to do so. We are currently collecting this data in a Wizard-of-Oz experiment."}, {"heading": "6.1 User simulation", "text": "User simulations are commonly used to train dialog management strategies, see for example (Young et al., 2007). A user simulation for NLG is very similar in that it is a predictive model of the most likely next user act. However, this user action does not change the entire dialog (e.g. by filling in slots), but only changes the generator state. In other words, the user simulation for NLG tells us what the user will do next if we would stop generating now. It also tells us the probability whether the user will opt for \"barge-in\" after a system action (by either selecting an element or providing more information).The user simulation for this study is a simple bi-gram model that correlates the number of attributes presented to the next likely user actions, see Table 3. The user can either follow the target provided by DM (Sysal providing more information)."}, {"heading": "6.2 Realizer model", "text": "The sequential NLG model assumes a realizer that updates the context after each generation step (i.e. after each individual action). We estimate the parameters of the realizer based on the averages we found in the MATCH data (see Table 1). For this study, we first vary (randomly) the number of attributes while the number of records is specified (see Table 4). In the current paper, we replace the realization model with an implemented generator that replicates the variation of the SPaRKy realizer (Stent et al., 2004)."}, {"heading": "6.3 Reward function", "text": "In this experiment, the reward is a function of the various data-driven trade-offs identified in the data analysis in Section 4: length of utterance and number of attributes provided, weighted by the regression model in Equation 1, as well as the next predicted user action. As we currently only have audible data, we manually estimate the reward for the next most likely user action to complement the data-driven model. If, in the final state, the next most likely user action is userQuit, the learner receives a penalty of \u2212 100, userElse receives 0 reward, and SysGoal receives + 100 reward. These handcoded values also need to be refined by a more targeted data collection, but the other components of the reward function are data-driven. Note that RL learns to \"make compromises\" with respect to the various target conflicts. For example, it is less likely that the user chooses more than 7 attributes, but the other attributes are light in context."}, {"heading": "6.4 State and Action Space", "text": "We are now formulating the problem as a Markov Decision Process (MDP) that relates states to actions. Each state / action pair is associated with a transition probability, which is the probability of transition from state s to state s, after performing action a to state s. This transition probability is calculated by the environmental model (i.e. user and realiser) and expressly captures noise / uncertainty in the environment. This is a big difference from other non-statistical planning approaches. Each transition is also associated with an amplification signal (or reward) rt + 1, which describes how good the action result a was when it was performed. The state space includes 9 binary characteristics representing the number of attributes, 2 binary characteristics representing the next action of the predicted user to follow or stop the system goal, and a discrete feature reflecting the number of propositions generated so far, as shown in Figure 28 x 3 to illustrate this."}, {"heading": "6.5 Baselines", "text": "We derive the basic guidelines from the information presentation strategies used by current dialogue systems. Overall, we use 7 different basic lines (B1-B7), which correspond to individual sectors in our policy area (see Figure 1): B1: RECOMMEND ONLY, e.g. (Young et al., 2007) B2: COMPARE ONLY, e.g. (Henderson et al., 2008) B3: RECOMMEND ONLY, e.g. (Polifroni and Walker, 2008) B4: RECOMMEND followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Random selection between COMPARE and RECOMEND, e.g. (Walker et al., 2007) B6: Random selection between all 7 outputsB7: Always generating a complete sequence, i.e. SUMMARY + RECOMMMEND, as suggested in the analytical solution (see section 4)."}, {"heading": "6.6 Results", "text": "We analyze the test runs (n = 200) with an ANOVA with a post-hoc T-test (with Bonferroni correction). RL exceeds all baselines in terms of the final reward, see Table 5. RL is the only policy that significantly improves the next most likely user action by adapting to the characteristics in the current context. In contrast to conventional approaches, RL learns to \"control\" its environment according to the estimated transition probabilities and the associated rewards. The learned policy can be described as follows: It starts with either SUMMARY or COMPARE after the init state, i.e. it learns never to start with a RECOMMEND. It stops generating after COMPARE when the userGoal (probable) is reached (e.g. the user selects an element in the next round that depends on the number of attributes generated. Blood buttons, COMREMEND generates and COMEND)."}, {"heading": "7 Conclusion", "text": "We presented and evaluated a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning. After motivating and presenting the model, we examined its use in information presentation. We derived a data-driven model that predicts users \"judgments on various measures of information presentation (reward function) based on a regression analysis of MATCH data. We used this regression model to set weights in a reward function for reinforcement learning and thus optimize a context-based presentation policy. The policy learned was compared with several baselines derived from previous work in this area, where the policy learned clearly exceeds all baselines. There are many possible extensions of this model, e.g. the use of the same techniques to jointly optimize the selection of attributes, aggregation, word choice, reference expressions, and soon hierarchically."}, {"heading": "Acknowledgments", "text": "We thank Marilyn Walker for accessing the MATCH corpus. The research that led to these results was funded by the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under Funding Agreement No. 216594 (CLASSiC project: www. classic-project.org) and by EPSRC project No. EP / E019501 / 1."}], "references": [{"title": "Working memory and language: an overview", "author": ["A. Baddeley"], "venue": "Journal of Communication Disorder,", "citeRegEx": "Baddeley.,? \\Q2001\\E", "shortCiteRegEx": "Baddeley.", "year": 2001}, {"title": "Information presentation in spoken dialogue systems", "author": ["Demberg", "Moore2006] Vera Demberg", "Johanna D. Moore"], "venue": "In Proceedings of EACL", "citeRegEx": "Demberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demberg et al\\.", "year": 2006}, {"title": "Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets", "author": ["Oliver Lemon", "Kallirroi Georgila"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2008}, {"title": "User simulations for online adaptation and knowledgealignment in Troubleshooting dialogue systems", "author": ["Janarthanam", "Oliver Lemon."], "venue": "Proc. of SEMdial.", "citeRegEx": "Janarthanam and Lemon.,? 2008", "shortCiteRegEx": "Janarthanam and Lemon.", "year": 2008}, {"title": "Experiences with planning for natural language generation", "author": ["Koller", "Petrick2008] Alexander Koller", "Ronald Petrick"], "venue": "In ICAPS", "citeRegEx": "Koller et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2008}, {"title": "Sentence generation as planning", "author": ["Koller", "Stone2007] Alexander Koller", "Matthew Stone"], "venue": "In Proceedings of ACL", "citeRegEx": "Koller et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2007}, {"title": "Adaptive Natural Language Generation in Dialogue using Reinforcement Learning", "author": ["Oliver Lemon"], "venue": "In Proceedings of SEMdial", "citeRegEx": "Lemon.,? \\Q2008\\E", "shortCiteRegEx": "Lemon.", "year": 2008}, {"title": "Generating tailored, comparative descriptions in spoken dialogue", "author": ["Moore et al.2004] Johanna Moore", "Mary Ellen Foster", "Oliver Lemon", "Michael White"], "venue": "In Proc. FLAIRS", "citeRegEx": "Moore et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2004}, {"title": "Learning to say it well: Reranking realizations by predicted synthesis quality", "author": ["Nakatsu", "White2006] Crystal Nakatsu", "Michael White"], "venue": "In Proceedings of ACL", "citeRegEx": "Nakatsu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nakatsu et al\\.", "year": 2006}, {"title": "Stochastic natural language generation for spoken dialog systems", "author": ["Oh", "Rudnicky2002] Alice Oh", "Alexander Rudnicky"], "venue": "Computer, Speech & Language,", "citeRegEx": "Oh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2002}, {"title": "Conversation as action under uncertainty", "author": ["Paek", "Horvitz2000] Tim Paek", "Eric Horvitz"], "venue": "In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Paek et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Paek et al\\.", "year": 2000}, {"title": "Intensional Summaries as Cooperative Responses in Dialogue Automation and Evaluation", "author": ["Polifroni", "Walker2008] Joseph Polifroni", "Marilyn Walker"], "venue": "In Proceedings of ACL", "citeRegEx": "Polifroni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Polifroni et al\\.", "year": 2008}, {"title": "Does this list contain what you were searching for? Learning adaptive dialogue strategies for Interactive Question Answering", "author": ["Rieser", "Lemon2008a] Verena Rieser", "Oliver Lemon"], "venue": "J. Natural Language Engineering,", "citeRegEx": "Rieser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2008}, {"title": "Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation", "author": ["Rieser", "Lemon2008b] Verena Rieser", "Oliver Lemon"], "venue": "In Proceedings of ACL", "citeRegEx": "Rieser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2008}, {"title": "Optimizing dialogue management with Reinforcement Learning: Experiments with the NJFun system", "author": ["Singh et al.2002] S. Singh", "D. Litman", "M. Kearns", "M. Walker"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Usertailored generation for spoken dialogue: an experiment", "author": ["Stent et al.2002] Amanda Stent", "Marilyn Walker", "Steve Whittaker", "Preetam Maloor"], "venue": "Proc. of ICSLP", "citeRegEx": "Stent et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2002}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["Stent et al.2004] Amanda Stent", "Rashmi Prasad", "Marilyn Walker"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Stent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Reinforcement Learning", "author": ["Sutton", "Barto1998] R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Towards developing general models of usability with PARADISE", "author": ["Candace A. Kamm", "Diane J. Litman"], "venue": "Natural Language Engineering,", "citeRegEx": "Walker et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2000}, {"title": "User tailored generation in the match multimodal dialogue system", "author": ["S. Whittaker", "A. Stent", "P. Maloor", "J. Moore", "M. Johnston", "G. Vasireddy"], "venue": null, "citeRegEx": "Walker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2004}, {"title": "Individual and domain adaptation in sentence planning for dialogue", "author": ["Amanda Stent", "Fran\u00e7ois Mairesse", "Rashmi Prasad"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "Fish or Fowl: A Wizard of Oz evaluation of dialogue strategies in the restaurant domain", "author": ["Marilyn Walker", "Johanna Moore"], "venue": "In Proc. of the International Conference on Language Resources and Evaluation", "citeRegEx": "Whittaker et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2002}, {"title": "Should i tell all? an experiment on conciseness in spoken dialogue", "author": ["Marilyn Walker", "Preetam Maloor"], "venue": "In Proc. European Conference on Speech Processing (EUROSPEECH)", "citeRegEx": "Whittaker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2003}, {"title": "The influence of user tailoring and cognitive load on user performance in spoken dialogue systems", "author": ["Jiang Hu", "Johanna D. Moore", "Clifford Nass"], "venue": "In Proc. of the 10th International Conference of Spoken Lan-", "citeRegEx": "Winterboer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Winterboer et al\\.", "year": 2007}, {"title": "The Hidden Information State Approach to Dialog Management", "author": ["Young et al.2007] SJ Young", "J Schatzmann", "K Weilhammer", "H Ye"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Young et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "For example, the CG to present database results to the user can be realised as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 24, "context": ", 2004), or by picking one item and recommending it to the user (Young et al., 2007).", "startOffset": 64, "endOffset": 84}, {"referenceID": 19, "context": "(Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "(Janarthanam and Lemon, 2008), or predicted TTS quality, e.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "(Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and \u201cconversation as action under uncertainty\u201d (Paek and Horvitz, 2000).", "startOffset": 0, "endOffset": 69}, {"referenceID": 2, "context": "(Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and \u201cconversation as action under uncertainty\u201d (Paek and Horvitz, 2000).", "startOffset": 0, "endOffset": 69}, {"referenceID": 19, "context": "In Section 4 we present results from our analysis of the MATCH corpus (Walker et al., 2004).", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach).", "startOffset": 73, "endOffset": 86}, {"referenceID": 16, "context": "Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004).", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.", "startOffset": 191, "endOffset": 232}, {"referenceID": 19, "context": "These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.", "startOffset": 191, "endOffset": 232}, {"referenceID": 16, "context": "(Stent et al., 2004; Oh and Rudnicky, 2002).", "startOffset": 0, "endOffset": 43}, {"referenceID": 19, "context": "(Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008).", "startOffset": 0, "endOffset": 74}, {"referenceID": 19, "context": "We utilise the MATCH corpus (Walker et al., 2004) to extract an evaluation function (also known as \u201creward function\u201d) for RL.", "startOffset": 28, "endOffset": 49}, {"referenceID": 16, "context": "Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer.", "startOffset": 42, "endOffset": 62}, {"referenceID": 15, "context": "The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 22, "context": ", 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies.", "startOffset": 12, "endOffset": 36}, {"referenceID": 15, "context": "The first data set, see (Stent et al., 2002), comprises 1024 ratings by 16 subjects (where we only use the speech-based half, n = 512) on the following presentation strategies: RECOMMEND, COMPARE, SUMMARY.", "startOffset": 24, "endOffset": 44}, {"referenceID": 18, "context": ") and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)).", "startOffset": 140, "endOffset": 161}, {"referenceID": 22, "context": "The second MATCH data set, see (Whittaker et al., 2003), comprises 1224 ratings by 17 subjects on the NLG strategies RECOMMEND and COMPARE.", "startOffset": 31, "endOffset": 55}, {"referenceID": 23, "context": "We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007).", "startOffset": 96, "endOffset": 121}, {"referenceID": 21, "context": "(Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "For comparison: (Walker et al., 2000) report on R between .", "startOffset": 16, "endOffset": 37}, {"referenceID": 24, "context": "User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007).", "startOffset": 96, "endOffset": 116}, {"referenceID": 0, "context": "Once the number of attributes is more than the \u201cmagic number 7\u201d (reflecting psychological results on shortterm memory) (Baddeley, 2001) the user is more likely to become confused and quit.", "startOffset": 119, "endOffset": 135}, {"referenceID": 16, "context": "In current work we replace the realizer model with an implemented generator that replicates the variation found in the SPaRKy realizer (Stent et al., 2004).", "startOffset": 135, "endOffset": 155}, {"referenceID": 24, "context": "(Young et al., 2007)", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "(Henderson et al., 2008)", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "(Whittaker et al., 2002)", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "(Walker et al., 2007)", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches.", "creator": "LaTeX with hyperref package"}}}