{"id": "1606.04750", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Multi-Modal Hybrid Deep Neural Network for Speech Enhancement", "abstract": "Deep Neural Networks (DNN) have been successful in en- hancing noisy speech signals. Enhancement is achieved by learning a nonlinear mapping function from the features of the corrupted speech signal to that of the reference clean speech signal. The quality of predicted features can be improved by providing additional side channel information that is robust to noise, such as visual cues. In this paper we propose a novel deep learning model inspired by insights from human audio visual perception. In the proposed unified hybrid architecture, features from a Convolution Neural Network (CNN) that processes the visual cues and features from a fully connected DNN that processes the audio signal are integrated using a Bidirectional Long Short-Term Memory (BiLSTM) network. The parameters of the hybrid model are jointly learned using backpropagation. We compare the quality of enhanced speech from the hybrid models with those from traditional DNN and BiLSTM models.", "histories": [["v1", "Wed, 15 Jun 2016 13:14:05 GMT  (6161kb,D)", "http://arxiv.org/abs/1606.04750v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SD", "authors": ["zhenzhou wu", "sunil sivadas", "yong kiam tan", "ma bin", "rick siow mong goh"], "accepted": false, "id": "1606.04750"}, "pdf": {"name": "1606.04750.pdf", "metadata": {"source": "CRF", "title": "Multi-Modal Hybrid Deep Neural Network for Speech Enhancement", "authors": ["Zhenzhou Wu", "Sunil Sivadas", "Yong Kiam Tan", "Ma Bin", "Rick Siow", "Mong Goh"], "emails": ["wuzz@ihpc.a-star.edu.sg", "tanyk@ihpc.a-star.edu.sg", "gohsm@ihpc.a-star.edu.sg", "sivadass@i2r.a-star.edu.sg", "mabin@i2r.a-star.edu.sg"], "sections": [{"heading": null, "text": "Index terms: BiLSTM, Convolutionary Neural Networks, Audio-Visual, Multimodal, Language Improvement, Noise Reduction"}, {"heading": "1. Introduction", "text": "The McGurk Effect [1] is an example of the language perception in which humans integrate audio and visual cues. Visual cues typically provide information about the place of articulation [2] and lip shapes that help distinguish phonemes with similar acoustic properties. In acoustically noisy environments, visual cues help to filter out the target speakers from the surrounding audio sources."}, {"heading": "2. Related Work", "text": "Recently, interest in heterogeneous deep-learning architectures has increased [13, 14]. These architectures combine the strengths of constituent deep-learning models to learn better abstractions of features at a high level. In [14], an ensemble model for phoneme recognition was proposed, in which CNN and RNN were first independently trained to calculate \"low-level\" characteristics. In [13], a linear ensemble model was then trained to combine the rear probabilities from these lower classifiers. This model then followed the strategy of stacking classifiers to achieve better discrimination and generalization [15]. In [13], the model combines CNNs, LSTMs, and DNNs into a uniform framework. First, a CNN was used to reduce spectral variability, and its output functions were then fed into an LSTM to reduce temporal variability."}, {"heading": "3. BiModal-BiLSTM Model", "text": "In the BiModal BiLSTM model, we record one image channel and one audio channel at each time step. For the image channel, we use a CNN to extract a high-grade feature representation. t = CNN (1) and for the audio channel, we use a DNN to convert the audio features into a learned representation at the top level of DNN.a \u0445 t = DNN (at) (2) Then we link the two features xt = Concat (i \u0445 t, a \u0445 t) and pass the common representation into a BiLSTM model that consists of a forward-facing LSTM, yft, ht + 1 = FLSTM (xt, ht) (3) and a backward-facing LSTM, ybt, ht \u2212 1 = BLSTM (xt, ht) (4). FLSTM and BLSTM models are standard LSTM models, as defined in [9, 16] except that they unroll in opposite timelines."}, {"heading": "4. Baseline Models", "text": "To understand the effectiveness of the BiModal-BiLSTM model, we have developed two basic models with a similar number of parameters to answer two questions: 1. Does an additional image modality help in generalizing models for speech enhancement? 2. Does the BiLSTM work better than a purely feedback-forward neural network? The second question has already been answered in speech recognition and speech enhancement [10, 11], but it will be interesting to compare the models with our BiModal-BiLSTM model."}, {"heading": "4.1. Single-Channel-BiLSTM", "text": "The single-channel BiLSTM has the same architecture as our BiModal BiLSTM model except that we have removed the CNN snippet (equation 1) and only use the noisy audio channel as input, leaving everything else the same to ensure that any difference in the end result is due to the CNN snippet."}, {"heading": "4.2. Single-Channel-DNN", "text": "In the single-channel DNN, we take the noisy audio as input and expand it directly with a DNN [17]. The single-channel DNN has the same DNN architecture as the BiModal BiLSTM and the single-channel BiLSTM (Equation 2). However, to ensure that the total number of parameters in the single-channel DNN matches that of the single-channel BiLSTM, we have appended two additional fully connected layers so that differences in the final generation result are due to the difference in the network architecture, not to a different number of parameters."}, {"heading": "5. Experimental Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Data", "text": "We conducted our experiments with an audiovisual dataset consisting of 14 native American speakers [12]. 94 recorded files were used for each speaker, ranging from short individual word clips to long recordings of several full sentences. We extracted nonspeech, environmental sounds from an online corpus [18]. For our test, we used two of the longer audio files (CID Sentences List A and NU Auditory Test No.6 List I) for each speaker. Other samples in the dataset were used to construct the training set. We corrupted each sample with a selected signal-to-noise ratio (SNR) 2. For the training samples, we randomly selected an integral SNR in the range [-5.5]. Overall, this gave us approximately 20.7 hours to analyze the audio training data."}, {"heading": "5.2. Model Specification", "text": "To ensure a fair comparison, we have chosen the model sizes so that they have approximately the same number of parameters. Table 2 shows the number of parameters for each model. The DNN audio feature extractor in Equation 2 has an architecture 100n-500300-outdim, where 100 is the PCA dimension for 1 frame, n is the number of stacked frames, and outdim is the dimensionality of a \u0445. We initially rely on 350 for BiModalBiLSTM and 400 for single-channel BiLSTM and single-channel DNN. Table 1 shows the specifications of the CNN image feature extractor from Equation 1. The single-channel DNN consists of a DNN audio feature extractor and two hidden layers of dimensions 1000-500. The single-channel BiLSTM also has the DNN audio feature extractor, followed by a BiLSTM 200 layer with 400 input dimensions and one full and one output layer."}, {"heading": "5.3. Model Training", "text": "We used Adam [23] as the learning algorithm and Mean Squared error as the target to minimize. We keep 10% of the training data as a validation set and stop training if the validation error has not improved by at least 1% in 5 epochs. This ensures that none of the models matches the training data. We normalize all audio input dimensions to have a variance of 0 mean and unit, and scale the image pixel intensities to [0.1]. This pre-processing step is important to reduce the covariable shift between dimensions and ensure that each dimension transmits the same signal intensity to the network.For the single-channel DNN model, we used a window of 11 images of the noisy spectrum for each clean spectrum output model. For the BiLSTM models, each input time step in 1 frame of the language and we found a time model that worked best between 13 / 7 and 7 time frames."}, {"heading": "5.4. Results", "text": "We use the Perceptual Evaluation of Speech Quality (PESQ) [25], which has a high correlation with subjective evaluation results, as an objective measure for evaluating the quality of denoted language. Figure 2 shows the average PESQ value of the language, which is increased by different models of test expression, which are corrupted by seen noise (alarm and crowd) and unseen noise (traffic) at different SNRs. Figure 4 shows the mean PESQ value of all loudspeakers and all SNRs for the different models. We find that the mean PESQ values are consistent with the MSE on the cross-validation group. The BiModal BiLSTM performs best for all sounds and SNRs seen, but its performance is similar to the single-channel BiLSTM under (unseen) traffic noise conditions. Both BiLSTM models significantly exceed the DNN model, the signal signal is significantly higher than the signal noise, the signal signal is the signal signal of the language."}, {"heading": "6. Conclusions", "text": "At a higher level, information processing in human perception involves multi-sensory integration and modeling of long-term dependencies between sensory data. Strategies include the integration of signals from multiple senses based on their reliability or signal to noise ratio (SNR). In this work, motivated by the insights gained from human sensory perception, we have proposed a novel multi-modal hybrid architecture of deep neural networks. The model captures representations of speech and images at the middle level through a fully connected DNN or CNN. Long-term dependencies in intermediate representation are modeled by a BiLSTM. We validated the model based on audiovisual speech enhancement, the task being to estimate clean speech spectra from the input of noisy speech spectra and images of the corresponding lip region. It is expected that the hybrid model will learn to adapt the importance of audiovisual speech streams per se, based on the input of noisy speech spectra and images of the corresponding lip region."}, {"heading": "7. References", "text": "[1] Harry McGurk and John MacDonald, \"Hearing lips and seeingvoices,\" Nature, 6ppeme 2011, vol. 264, pp. 746-748, 1976. [2] Quentin Summerfield, \"Lipreading and audio-visual speech per-ception,\" Philosophical Transactions of the Royal Society of London B: Biological Sciences, vol. 335, no. 1273, pp. 71-78, 1992. [3] Gerasimos Potamianos, Chalapathy Neti, Juergen Luettin, and Iain Matthews, \"Audio-visual automatic speech recognition: An overview,\" Issues in visual and audio-visual speech processing, vol. [4] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. \"Multimodal deep learning,\" in Proceedings of the 28th International on Machine Learning (ICL 11-89)."}], "references": [{"title": "Hearing lips and seeing voices", "author": ["Harry McGurk", "John MacDonald"], "venue": "Nature, vol. 264, pp. 746\u2013748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Lipreading and audio-visual speech perception", "author": ["Quentin Summerfield"], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences, vol. 335, no. 1273, pp. 71\u201378, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Audio-visual automatic speech recognition: An overview", "author": ["Gerasimos Potamianos", "Chalapathy Neti", "Juergen Luettin", "Iain Matthews"], "venue": "Issues in visual and audio-visual speech processing, vol. 22, pp. 23, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 2222\u20132230.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural networks for noise reduction in robust asr", "author": ["Andrew L Maas", "Quoc V Le", "Tyler M O\u2019Neil", "Oriol Vinyals", "Patrick Nguyen", "Andrew Y Ng"], "venue": "INTERSPEECH, 2012, pp. 22\u201325.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Audiovisual database of spoken American English LDC2009V01", "author": ["Carolyn Richie", "Sarah Warburton", "Megan Carter"], "venue": "Philadelphia: Linguistic Data Consortium, 2009, Web Download.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Tara N Sainath", "Oriol Vinyals", "Andrew Senior", "Hasim Sak"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble deep learning for speech recognition", "author": ["Li Deng", "John C Platt"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Stacked generalization", "author": ["David H Wolpert"], "venue": "Neural networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computing, vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Yong Xu", "Jun Du", "Li-Rong Dai", "Chin-Hui Lee"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "100 nonspeech sounds", "author": ["Guoning Hu"], "venue": "http://web. cse.ohio-state.edu/pnl/corpus/HuNonspeech/ HuCorpus.html, Web Download, Accessed: 2016-03-25.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "librosa: 0.4.1", "author": ["Brian McFee", "Matt McVicar", "Colin Raffel", "Dawen Liang", "Oriol Nieto", "Eric Battenberg", "Josh Moore", "Dan Ellis", "Ryuichi Yamamoto", "Rachel Bittner", "Douglas Repetto", "Petr Viktorin", "Joo Felipe Santos", "Adrian Holovaty"], "venue": "Oct. 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2015, pp. 448\u2013456.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010, Oral Presentation.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u2013 1560, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs", "author": ["Antony W Rix", "John G Beerends", "Michael P Hollier", "Andries P Hekstra"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on. IEEE, 2001, vol. 2, pp. 749\u2013752.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The McGurk effect [1] is one example in speech perception where humans integrate audio and visual cues.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Visual cues typically provide information about place of articulation [2] and lip shapes that aid in discriminating phonemes with similar acoustic characteristics.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "There are various computational models of multi-modal information fusion [3] for audio-visual speech processing.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "Deep learning provides an elegant framework for designing data driven models for multi-modal and cross-modal feature learning [4, 5].", "startOffset": 126, "endOffset": 132}, {"referenceID": 4, "context": "Deep learning provides an elegant framework for designing data driven models for multi-modal and cross-modal feature learning [4, 5].", "startOffset": 126, "endOffset": 132}, {"referenceID": 3, "context": "In [4], stacks of Restricted Boltzmann Machines (RBMs) [6] were trained to learn joint representations between acoustic features of phonemes and images of the mouth region.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [4], stacks of Restricted Boltzmann Machines (RBMs) [6] were trained to learn joint representations between acoustic features of phonemes and images of the mouth region.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "It is well known that human visual processing is better modeled by CNNs [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "Deep learning models with memory cells, such as LSTM [8] and BiLSTM [9] networks, have out-performed fully connected DNNs and CNNs in noise robust speech recognition [10, 11].", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "Deep learning models with memory cells, such as LSTM [8] and BiLSTM [9] networks, have out-performed fully connected DNNs and CNNs in noise robust speech recognition [10, 11].", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "Deep learning models with memory cells, such as LSTM [8] and BiLSTM [9] networks, have out-performed fully connected DNNs and CNNs in noise robust speech recognition [10, 11].", "startOffset": 166, "endOffset": 174}, {"referenceID": 10, "context": "Deep learning models with memory cells, such as LSTM [8] and BiLSTM [9] networks, have out-performed fully connected DNNs and CNNs in noise robust speech recognition [10, 11].", "startOffset": 166, "endOffset": 174}, {"referenceID": 11, "context": "The models are validated on an artificially corrupted audio-visual database [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Recently, there has been increased interest in heterogeneous deep learning architectures [13, 14].", "startOffset": 89, "endOffset": 97}, {"referenceID": 13, "context": "Recently, there has been increased interest in heterogeneous deep learning architectures [13, 14].", "startOffset": 89, "endOffset": 97}, {"referenceID": 13, "context": "In [14], an ensemble model for phoneme recognition was proposed where a CNN and RNN were first independently trained to compute \u201clow-level\u201d features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "This model followed the strategy of stacking classifiers to achieve better discrimination and generalization [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "In [13], the model combines CNNs, LSTMs and DNNs into a unified framework.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "The multi-modal deep learning model proposed in [4] used sparse RBMs for combining the different lower level modalities.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "The FLSTM and BLSTM are standard LSTM models, as defined in [9, 16] except that they unroll in opposite time direction.", "startOffset": 60, "endOffset": 67}, {"referenceID": 15, "context": "The FLSTM and BLSTM are standard LSTM models, as defined in [9, 16] except that they unroll in opposite time direction.", "startOffset": 60, "endOffset": 67}, {"referenceID": 9, "context": "The second question has already been answered in speech recognition and speech enhancement [10, 11] on speech datasets, but it will be interesting to compare the models alongside our BiModal-BiLSTM model.", "startOffset": 91, "endOffset": 99}, {"referenceID": 10, "context": "The second question has already been answered in speech recognition and speech enhancement [10, 11] on speech datasets, but it will be interesting to compare the models alongside our BiModal-BiLSTM model.", "startOffset": 91, "endOffset": 99}, {"referenceID": 16, "context": "In the Single-Channel-DNN, we take the noisy audio as input and enhance it directly with a DNN [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "We conducted our experiments on an audiovisual dataset consisting of 14 native American English speakers [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "We extracted nonspeech, environmental noises from an on-line corpus [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "For the training samples, we randomly selected an integral SNR in the range [-5,5].", "startOffset": 76, "endOffset": 82}, {"referenceID": 8, "context": "For the test data, we corrupted with SNRs in steps of 3 in the range [-6,9].", "startOffset": 69, "endOffset": 75}, {"referenceID": 18, "context": "All data manipulation was done using off the shelf packages [19, 20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 19, "context": "All data manipulation was done using off the shelf packages [19, 20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 20, "context": "In all the fully-connected and convolutional layers, we used batch normalization [21] to reduce the internal covariate shift of the outputs from one layer to another.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "All the models were trained on NVIDIA Tesla K20 GPUs using Theano [22] and Mozi.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "We used Adam [23] as the learning algorithm and Mean-Squared-Error as the objective to be minimized.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "We normalise all audio input dimensions to have zero mean and unit variance, and scale the image pixel intensities to [0,1].", "startOffset": 118, "endOffset": 123}, {"referenceID": 23, "context": "For the BiLSTM models, we unrolled the model with 21 time-steps, and trained with back-propagation through time [24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "We use the Perceptual Evaluation of Speech Quality (PESQ) [25], which has a high correlation with subjective evaluation scores, as our objective measure for evaluating the quality of denoised speech.", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "Deep Neural Networks (DNN) have been successful in enhancing noisy speech signals. Enhancement is achieved by learning a nonlinear mapping function from the features of the corrupted speech signal to that of the reference clean speech signal. The quality of predicted features can be improved by providing additional side channel information that is robust to noise, such as visual cues. In this paper we propose a novel deep learning model inspired by insights from human audio visual perception. In the proposed unified hybrid architecture, features from a Convolution Neural Network (CNN) that processes the visual cues and features from a fully connected DNN that processes the audio signal are integrated using a Bidirectional Long Short-Term Memory (BiLSTM) network. The parameters of the hybrid model are jointly learned using backpropagation. We compare the quality of enhanced speech from the hybrid models with those from traditional DNN and BiLSTM models.", "creator": "LaTeX with hyperref package"}}}