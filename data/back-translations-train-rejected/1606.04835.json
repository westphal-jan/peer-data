{"id": "1606.04835", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Learning Word Sense Embeddings from Word Sense Definitions", "abstract": "Word embeddings play a significant role in many modern NLP systems. However, most used word embedding learning methods learn one representation per word which is problematic for polysemous words and homonymous words. To address this problem, we propose a multi-phase word sense embedding retrofitting method which utilizes a lexical ontology to learn one embedding per word sense. We use word sense definitions and relations between word senses defined in a lexical ontology in a different way from existing systems. Experimental results on word similarity task show that our approach remarkablely improves the quality of embeddings.", "histories": [["v1", "Wed, 15 Jun 2016 16:14:09 GMT  (14kb)", "http://arxiv.org/abs/1606.04835v1", "Submitted to COLING 2016"], ["v2", "Mon, 20 Jun 2016 14:59:47 GMT  (21kb)", "http://arxiv.org/abs/1606.04835v2", "Submitted to COLING 2016"], ["v3", "Mon, 18 Jul 2016 13:03:12 GMT  (34kb)", "http://arxiv.org/abs/1606.04835v3", "Submitted to COLING 2016"], ["v4", "Mon, 24 Oct 2016 00:56:54 GMT  (41kb)", "http://arxiv.org/abs/1606.04835v4", "To appear at NLPCC-ICCPOL 2016"]], "COMMENTS": "Submitted to COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qi li", "tianshi li", "baobao chang"], "accepted": false, "id": "1606.04835"}, "pdf": {"name": "1606.04835.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Qi Li", "Tianshi Li", "Baobao Chang"], "emails": ["qi.li@pku.edu.cn", "417@hotmail.com", "chbb@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.04 835v 1 [cs.C L] 15 Jun 2016"}, {"heading": "1 Introduction", "text": "In fact, most people in the world do not know what to do. (...) Most people in the world do not know what to do. (...) Most people in the world do not know what to do. (...) Most people in the world do not know what to do. (...) Most people in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. (...) People in the world do not know what to do. \""}, {"heading": "2 Methodology", "text": "The corpus is abundant in word contexts, but it does not provide explicit word sense information and semantic relationships between words. Consequently, some words with completely opposite meanings in representation (e.g. \"good\" and \"bad\") may turn out to be similar if they are formed only with the corpus. An lexical ontology provides definitions for each meaning and its relationships, so we can use this information to further differentiate these words with similar syntactic function but different exact meanings. Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013), etc."}, {"heading": "2.1 Embedding Initialization", "text": "First we initialize our word embedding with Skip-gram or Glove on the uncommented corpus. We get the number of word senses with WordNet (Miller, 1992) and initialize the embedding of all senses to their word embedding."}, {"heading": "2.2 Training RNN for Definition Understanding", "text": "The definition of a word is an exact description of the problem we should be able to deflect from the definition. (D) The definition of a word is one of the most successful neural networks for understanding natural language, we use RNN to calculate the representation of a word from the definition. (D) The output to the last unit of the RNN model is assumed to be the complete information of a sentence. (D) The specific RNN model can be a standard RNN model, gated recurrent unit (GRU) (Chung et al., 2014) or long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997). Comparison with standard RNN, LSTM and GRU can hold long-term information so that it disappears the gradients and forges information associated with the standard RNN for long sequences."}, {"heading": "2.3 Sense Embedding Learning", "text": "In contrast to the first phase, in this phase all the sensory embeddings and the model parameters are trained together, and the sensory embeddings are updated throughout the process. The objective function is: J2 = \u2212 \u2211 w-V-s-Swcos (ews, \u02dc ews) (15), where V is the word set including all words and Sw is the word set w."}, {"heading": "2.4 Training Relation Mapping Matrices", "text": "In lexical ontology, the meaning of the word is linked to relationships such as hypernymia, hyponymy and antagonymy. These relationships between the sense of the word should relate to their embedding. Therefore, for each relationship defined in ontology, we define a mapping matrix. We train the matrix to match the sense of the word to the respective relationships. The objective function is: J3 = \u2211 r-sense (ws1, ws2) = r | | Wrews1 \u2212 ews2 | | (16), where Vr is the set of all relationships, (ws1, ws2) the relationship of sense of the word 1 and sense of the word 2 and Wr the mapping matrix for relationship r. We only update all Wr matrices at this stage."}, {"heading": "2.5 Jointly Training All Parameters and Sense Embeddings", "text": "The objective function is: J4 = \u03b1J2 + (1 \u2212 \u03b1) J3 (17), where J2 from equation 15, J3 from equation 16 and \u03b1 is a hyperparameter to control the weights of the two parts of the training. We set \u03b1 to 0.7. We update all the sensory embeddings and parameters. We optimize this objective function by training J2 and J3 one after the other during each epoch. We retain these two phases and update only the sensory embeddings and model parameters. If we train them together with sensory embeddings, shortly after we randomly initialize the parameters, this random information would have a negative effect on sensory embeddings."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "Since the quality of trained word embedding usually increases with the circumference of the corpus, we try two publicly released word vectors trained on the largest corpus as the initialization of our word embedding: One is the 300-dimensional word vectors trained with the glove on a corpus of 840B 1, the other is pre-trained 300-dimensional vectors trained with word2vec2 (Skip-gram) on a portion of the Google News dataset (about 100 billion words)."}, {"heading": "3.2 Word Similarity Evaluation", "text": "Since the output of our method is the sensual embedding, we need context to determine the meaning when we apply the trained sensual embeds.The evaluation using word similarity data sets that produce pairs of words without context does not allow us to do the word induction and therefore cannot reveal the effectiveness of our method.Stanford's Contextual Word Similarities (SCWS) (Huang et al., 2012) is a data set that indicates the context of the target words. Therefore, we use context to determine the cosmic similarity of the two embeds.The evaluation metrics are the rank1http: / / nlp.stanford.edu / data / glove.zip 2https: / / code.google.com / archive / p / word2vec / correlations coefficient between the average human rating and the cosine similarity scores given by our method."}, {"heading": "4 Related Work", "text": "While traditional methods use meaningful mutual information (PMI) matrices to learn word embedding, prevailing methods (e.g. Skip-gram and Glove) use context information to learn word embedding. Levy et al. (2015) compare the differences and relationships between Skip-gram, Glove and PMI-based methods and conclude that it is system design decisions and hyper-parameter optimizations that lead to performance gains, rather than embedding algorithms. Since multi-sense embedding is suggested to be better than single embedding for a word, many approaches have been suggested. Researchers tend to expand Skip-gram and glove models to learn meaningful embedding with word embedding (WSI) as a precursor. Most work uses only one body to determine the number of word embedding for WSI, which would inevitably lead to errors."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a multi-phase embedding model. Our proposed method uses a lexical ontology to retrofit the embedding we learned from a large, uncommented corpus in a different way than existing systems. Experimental results on SCWS show that our proposed method successfully retrofits the embedding we learned with Skip-gram and Glove."}], "references": [{"title": "The berkeley framenet project", "author": ["Charles J. Fillmore", "John B. Lowe"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Joint word representation learning using a corpus and a semantic lexicon. CoRR, abs/1511.06438", "author": ["Mohammed Alsuhaibani", "Takanori Maehara", "Ken ichi Kawarabayashi"], "venue": null, "citeRegEx": "Bollegala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2016}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "aglar G\u00fclehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL)", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Li", "Jurafsky2015] Jiwei Li", "Daniel Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "Miller.,? \\Q1992\\E", "shortCiteRegEx": "Miller.", "year": 1992}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An ensemble method to produce high-quality word embeddings. CoRR, abs/1604.01692", "author": ["Speer", "Chin2016] Robert Speer", "Joshua Chin"], "venue": null, "citeRegEx": "Speer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2016}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders. CoRR, abs/1603.09128", "author": ["Suster et al.2016] Simon Suster", "Ivan Titov", "Gertjan van Noord"], "venue": null, "citeRegEx": "Suster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suster et al\\.", "year": 2016}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "The word embeddings capture syntactic and semantic properties which can be exposed directly in tasks such as analogical reasoning (Mikolov et al., 2013), word similarity (Huang et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 7, "context": ", 2013), word similarity (Huang et al., 2012) etc.", "startOffset": 25, "endOffset": 45}, {"referenceID": 10, "context": "The most prevalent word embedding learning models are Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 12, "context": ", 2013) and Glove (Pennington et al., 2014) and the variants of them.", "startOffset": 18, "endOffset": 43}, {"referenceID": 10, "context": "Basic Skip-gram (Mikolov et al., 2013) and Glove (Pennington et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": ", 2013) and Glove (Pennington et al., 2014) model output one vector for a word.", "startOffset": 18, "endOffset": 43}, {"referenceID": 14, "context": "Chinese Restaurant Processes (CRP))(Li and Jurafsky, 2015) or use fixed number of senses for all words (Suster et al., 2016).", "startOffset": 103, "endOffset": 124}, {"referenceID": 2, "context": "Many work applies word sense induction model on the unannotated corpus to determine the sense of a word in a context and then train the embeddings of word senses on it (Chen et al., 2014).", "startOffset": 168, "endOffset": 187}, {"referenceID": 4, "context": "Some other researchers try to use sense relations defined in a lexical ontology to learn word senses(Speer and Chin, 2016; Faruqui et al., 2015) and they most create a Markov Network (MN).", "startOffset": 100, "endOffset": 144}, {"referenceID": 11, "context": "Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet(Baker et al.", "startOffset": 47, "endOffset": 61}, {"referenceID": 0, "context": "Widely used lexical ontologies include WordNet (Miller, 1992), FrameNet(Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al.", "startOffset": 71, "endOffset": 91}, {"referenceID": 5, "context": ", 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) etc.", "startOffset": 36, "endOffset": 63}, {"referenceID": 11, "context": "We get the number of word senses with the WordNet (Miller, 1992) and we initialize the embeddings of all the senses to be their word embeddings.", "startOffset": 50, "endOffset": 64}, {"referenceID": 3, "context": "The specific RNN model can be a standard RNN model, Gated Recurrent Unit (GRU) (Chung et al., 2014) or Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 79, "endOffset": 99}, {"referenceID": 3, "context": "Comparing with standard RNN, LSTM and GRU can hold long-term information, so they can alleviate the gradient vanishing and information-forgetting problem associated with the standard RNN for long sequences(Hochreiter and Schmidhuber, 1997; Chung et al., 2014).", "startOffset": 205, "endOffset": 259}, {"referenceID": 7, "context": "Stanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012) is a data set which gives the contexts of the target words.", "startOffset": 47, "endOffset": 67}, {"referenceID": 6, "context": "System Spearman Huang et al. (2012) (0.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "System Spearman Huang et al. (2012) (0.99B) 65.7 Tian et al. (2014) (0.", "startOffset": 16, "endOffset": 68}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.", "startOffset": 2, "endOffset": 21}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.9 Li et al. (2014) (1.", "startOffset": 2, "endOffset": 48}, {"referenceID": 2, "context": "4 Chen et al. (2014) (1B) 68.9 Li et al. (2014) (1.1B) 67.0 Li et al. (2014) (120B) 69.", "startOffset": 2, "endOffset": 77}, {"referenceID": 6, "context": "Levy et al.(2015) compare the differences and relations of Skip-gram, Glove and PMI-based methods and conclude that it is the system design choices and hyper-parameter optimizations lead to performance gain rather than the embedding algorithms.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Huang et al. (2012) determine a sense of a word by clustering the contexts and then apply it to neural language model with global context.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Huang et al. (2012) determine a sense of a word by clustering the contexts and then apply it to neural language model with global context. Li et al. (2014) use Chinese Restaurant Processes to determine the sense of word and learning embedding jointly.", "startOffset": 0, "endOffset": 156}, {"referenceID": 2, "context": "Chen et al.(2014) use WordNet as its lexical ontology to determine word sense numbers and use the average word embedding for words in definition above a similarity threshold as the initialization of sense embeddings.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Other proposed relation-based retrofitting way is to get high similarity for related word embeddings in an ontology(Faruqui et al., 2015; Bollegala et al., 2016).", "startOffset": 115, "endOffset": 161}, {"referenceID": 1, "context": "Other proposed relation-based retrofitting way is to get high similarity for related word embeddings in an ontology(Faruqui et al., 2015; Bollegala et al., 2016).", "startOffset": 115, "endOffset": 161}], "year": 2017, "abstractText": "Word embeddings play a significant role in many modern NLP systems. However, most used word embedding learning methods learn one representation per word which is problematic for polysemous words and homonymous words. To address this problem, we propose a multi-phase word sense embedding retrofitting method which utilizes a lexical ontology to learn one embedding per word sense. We use word sense definitions and relations between word senses defined in a lexical ontology in a different way from existing systems. Experimental results on word similarity task show that our approach remarkablely improves the quality of embeddings.", "creator": "LaTeX with hyperref package"}}}