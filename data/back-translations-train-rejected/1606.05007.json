{"id": "1606.05007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks", "abstract": "Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs. However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages. Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset.", "histories": [["v1", "Wed, 15 Jun 2016 23:45:33 GMT  (223kb,D)", "http://arxiv.org/abs/1606.05007v1", "Proc. of 17th Interspeech (2016), San Francisco, California, USA"]], "COMMENTS": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["naoya takahashi", "tofigh naghibi", "beat pfister"], "accepted": false, "id": "1606.05007"}, "pdf": {"name": "1606.05007.pdf", "metadata": {"source": "CRF", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks", "authors": ["Naoya Takahashi", "Tofigh Naghibi", "Beat Pfister"], "emails": ["NaoyaA.Takahashi@jp.sony.com,", "pfister}@tik.ee.ethz.ch"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Semi-supervised joint Dictionary and Acoustic Model Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Framework", "text": "In the rest of this work we refer to data-driven sub-word units as abstract acoustic elements (AAEs) as opposed to phones. Our goal is to jointly learn the pronunciation that maximizes the common probability: D (X | T), D (1), where X = (X1), \u00b7 \u00b7, XM (XM), is the set of formulas, T = (T1, TM) is the set of corresponding orthographic transcriptions, M is the number of expressions representing the universe of all possible sets of N AEs and D is the universe of all possible sets of N AEs."}, {"heading": "2.2. Acoustic Model Initialization", "text": "We used the Linde-Buzo-Gray (LBG) algorithm [18] with a square error distortion measure to cluster the acoustic characteristic vectors; the LBG cluster algorithm tends to assign more codebook vectors to densely populated areas, which is a useful property to obtain discriminatory initial AEs; each cluster is then modeled by a GMM with a single Gaussian component; these models are used as initial models for AAEs."}, {"heading": "2.3. Dictionary Generation", "text": "The solution of (4) proposed in [15] is to extend the one-dimensional Viterbi algorithm to the K dimensions. The K-dimensional Viterbi algorithm calculates the most likely HMM state sequence common to K-dimensional expressions. Although this algorithm is strict, its complexity increases exponentially with the number of expressions, making it impossible to apply it to more than a few expressions. An efficient approximation of the K-dimensional Viterbi algorithm was proposed in [10], where the problem of finding the common orientation and optimal common sequence for K-expressions is dissected into K-1 applications of the two-dimensional Viterbi algorithm."}, {"heading": "2.4. Acoustic Modeling", "text": "Once the dictionary is updated, all utterances are decoded based on the new pronunciation of the words in the dictionary and the AAEs are reassessed according to the new names. AAEs can be modeled using commonly used models such as HMM / GMM or HMM / DNN. To avoid this situation, we start training with a simple model, namely a Gaussian component for each AAE with a diagonal covariance matrix. In each iteration, the dictionary becomes more precise when the degree of freedom of the model is too high. To avoid this situation, we start training with a simple model, namely a Gaussian component for each AE with a diagonal covariance matrix. In each iteration, the number of mixing components is doubled to increase the modeling power. Once the performance of the GMM is saturated by the DNM labeling, the GMM labeling is replaced with a more expressive modeling capability."}, {"heading": "3. Experiments", "text": "We conducted several experiments on the TIMIT corpus [19]. The TIMIT corpus provides a manually generated dictionary and telephone-level transcriptions with 61 telephones. As a starting point, 61 telephone models were trained using the TIMIT dictionary and the provided transcriptions. We used 12 mel frequency receiver coefficients (MFCCs) and energy with their deltas and delta deltas as descriptors of acoustic space. Voice data was analyzed using a 25 ms hamming window with an image shift of 10 ms. We evaluated phone-based DNN-HMM, GMMHMM and AE-based GMM-HMM model as baseline. The DNN architecture consisted of 7 hidden layers. The first hidden layer had 2048 nodes, the next 5 layers had 1024 nodes and the number of nodes on the last layer was equal to the number of HMM-states that should be predicted with the 12 states."}, {"heading": "3.1. Isolated Word Recognition", "text": "The first group of experiments involved isolated word recognition to test the performance of the proposed methods and investigate the effects of hyperparameters such as the number of mixture components and the number of AAEs. For a common pronunciation estimate and acoustic model training, we collected a pronunciation training set of words with more than 10 expressions from the TIMIT training, the total number of expressions in pronunciation training was 12,800. After excluding words with less than 4 characters (e.g. a and the), 339 different words from the TIMIT test were collected for the isolated word recognition task, resulting in a total of 3,900 expressions. GMM-based telephone models were trained with 32 mixing components. During the GMM-based AAE model, the number of mixtures for each iteration was doubled until it reached 128 mixtures, as described in Section 2.4."}, {"heading": "3.1.1. Comparison with phonetic approach", "text": "The results show that the proposed data-driven method significantly outperforms the basic methods, with the proposed AAEDNN method achieving an improvement of 10.3% and 2.4%, respectively, over the phonetic acoustic models based on GMM and DNN. This suggests that a more precise dictionary and better acoustic models can be obtained directly from training data without human expertise. Furthermore, the AAE-DNN method improves performance by 3.2% over the AAE-GMM method, suggesting that the DNN has been successfully trained in a semi-supervised manner and that the final model could effectively use its expressiveness."}, {"heading": "3.1.2. Number of AAEs", "text": "Our second experiment focused on the effects of the number of AAEs, i.e. N. We trained the dictionary and AAE models with N = 64, 128, 192, 256, 320, 384, 448. Word error rates of DNN and GMM-based AAE models are shown in Figure 2.The number of mixtures of GMMs was determined experimentally, as shown in Table 2. For DNN-based AAE models, the best result is 384 AEs, as opposed to 320 AAEs for GMM-based models. Interestingly, the optimal number of AAE states is much higher than the number of states of phone models (61 phonemes \u00d7 3 states = 183 states).This is an indication that the proposed data-based approach to generating the subword units and dictionary models collectively makes the acoustic space more precise than the linguistically motivated phonetic units and the manually designed dictionary."}, {"heading": "3.2. Continuous Speech Recognition", "text": "Unlike phoneme-based ASRs, the proposed AAE-based approach is not dependent on linguistic knowledge. It is therefore interesting to compare these approaches against a real speech recognition task (CSR). To this end, we used the SX recordings of the TIMIT corpus, which contain 450 sentences spoken by 7 speakers, i.e. a total of 3150 utterances. We prepared the test by randomly selecting one speaker for each sentence from the SX recordings and using the remaining samples as a training set (450 sentences x 6 speakers = 2700 utterances). In addition, we included the SA and SI recordings of the TIMIT corpus in the training set. The number of AAEs was 384. The number of mixing components in the GMM-based telephone models was 64. Performance was evaluated in two scenarios: with and without language model. The language model used in the baseline and the proposed methods is a simple Bigram representation."}, {"heading": "4. Conclusions", "text": "The proposed method does not require linguistic expertise and can automatically create the set of subword units and the corresponding dictionary for pronunciation of a word. In our method, reliable pronunciations from multiple utterances are estimated by an efficient approximation of the Kdimensional Viterbi Algorithm, which estimates the most likely HMM state sequence common to multiple utterances of a word. Experimental results show that the proposed method significantly exceeds the telephone-based methods that even receive manually prepared dictionaries and handmade transcriptions as input. We also investigated the effects of the number of data-driven subordinate units and showed that the optimal number of subordinate units is much higher than the total number of HMM states of the 61 telephones. Future work will focus on the application of the proposed method for speech recognition for understaffed languages and large vocabulary continuous speech recognition tasks."}, {"heading": "5. References", "text": "[1] A. Das and M. Hasegawa-Johnson, \"Cross-lingual transfer learn-ing during supervised training in low resource scenarios,\" in Proc. Interspeech, 2015, pp. 1-5. [2] Y. Qian, D. Povey, and J. Liu, \"State-level recognition for under-resource GMMs,\" in Proc. Interspeech, 2011, pp. 553-556. [3] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \"Automatic speech recognition for under-resource languages: A survey, Speech Communication, vol. 56, pp. 85-100, 2014. [4] M. Sarac, H. Khudanpur\" Pronunciation modeling, \"Gaussian densities across phonetic models,\"."}], "references": [{"title": "Cross-lingual transfer learning during supervised training in low resource scenarios", "author": ["A. Das", "M. Hasegawa-Johnson"], "venue": "Proc. Interspeech, 2015, pp. 1\u20135.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "State-level data borrowing for lowresource speech recognition based on subspace GMMs", "author": ["Y. Qian", "D. Povey", "J. Liu"], "venue": "Proc. Interspeech, 2011, pp. 553\u2013556.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic speech recognition for under-resourced languages : A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Communication, vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation modeling by sharing Gaussian densities across phonetic models", "author": ["M. Sara\u00e7lar", "H. Nock", "S. Khudanpur"], "venue": "Computer Speech & Language, vol. 14, no. 2, pp. 137\u2013160, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Pronunciation modeling for ASR - Knowledge-based and data-derived methods", "author": ["M. Wester"], "venue": "Computer Speech and Language, vol. 17, no. 1, pp. 69\u201385, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Implicit modelling of pronunciation variation in automatic speech recognition", "author": ["T. Hain"], "venue": "Speech Communication, vol. 46, no. 2, pp. 171\u2013188, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning lexicons from speech using a pronunciation mixture model", "author": ["I. Mcgraw", "I. Badr", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic generation of subword units for speech recognition systems", "author": ["R. Singh", "B. Raj", "R.M. Stern"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 10, no. 2, pp. 89\u201399, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Approaches to automatic lexicon learning with limited trainging examples", "author": ["A. Ghoshal", "D. Povey", "M. Agarwal", "P. Akyazi", "N. Goel", "M. Karafi", "A. Rastrow", "R.C. Rose", "P. Schwarz", "S. Thomas", "I. Allahabad"], "venue": "Proc. ICASSP, 2010, pp. 5094\u20135097.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient method to estimate pronunciation from multiple utterances", "author": ["T. Naghibi", "S. Hoffmann", "B. Pfister"], "venue": "Proc.Interspeech, no. August, 2013, pp. 1951\u20131955.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Combined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units", "author": ["T. Holter", "T. Svendsen"], "venue": "IEEE Workshop Automatic Speech Recognition, 1997, pp. 199\u2013206.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Joint lexicon, acoustic unit inventory and model design", "author": ["M. Bacchiani", "M. Ostendorf"], "venue": "Speech Communication, vol. 29, no. 2, pp. 99\u2013114, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Pronunciation modeling for speech technology", "author": ["T. Svendsen"], "venue": "International Conference on Signal Processing and Communications (SPCOM), 2004, pp. 11\u201316.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Derivation of the optimal set of phonetic transcriptions for a word from its acoustic realizations", "author": ["H. Mokbel", "D. Jouvet"], "venue": "Speech Communication, vol. 29, no. 1, 1999, pp. 49\u201364.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Extended Viterbi algorithm for optimized word HMMs", "author": ["M. Gerber", "T. Kaufmann", "B. Pfister"], "venue": "ICASSP, 2011, pp. 4932\u2013 4935.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for Speech Recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An algorithm for vector quantizer design", "author": ["Y. Linde", "A. Buzo", "R.M. Gray"], "venue": "IEEE Transactions on Communications, vol. 28, no. 1, pp. 84\u201395, 1980.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "The DARPA speech recognition research database: Specifications and status", "author": ["W. Fisher", "G. Doddington", "K. Goudie-Marshall"], "venue": "Proc. DARPA Workshop on Speech Recognition, 1986, pp. 93\u2013 99.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013, pp. 8609\u20138613.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The HTK Book (for HTK Version 3.4.1)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey", "V. Valtchev", "P. Woodland"], "venue": "http: //htk.eng.cam.ac.uk, 2009, University of Cambridge, UK.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 1, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 2, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 5, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 6, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 7, "context": "[8] proposed a data-driven dictionary generator to include new pronunciations based on newly coming acoustic evidence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "in [9] use a grapheme-to-phoneme approach to guess the pronunciation and iteratively refine the acoustic model and the dictionary.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In fact, given a set of acoustic samples, the linguistically defined units are most probably not the optimal ones for speech recognition [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 11, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 7, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 11, "context": "Bacchiani and Ostendorf [12] proposed an iterative acoustic segmentation and clustering approach to build sub-word units from speech signals and subsequently construct the dictionary based on the estimated subword units.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "[8] introduced a divide-and-conquer strategy to recursively update sub-word units and dictionary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 12, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 13, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 14, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 252, "endOffset": 260}, {"referenceID": 9, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 252, "endOffset": 260}, {"referenceID": 14, "context": "For the case where \u03bb is modeled by a left-to-right HMM without skips, which is the most common topology in HMM based ASRs, a solution of (4) has been proposed in [15] (Details are in Section 2.", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "Therefore, a robust model is required at early stage of the training iteration while a more expressive and powerful model such as a DNN [16, 17] can be used after the reliable dictionary is obtained.", "startOffset": 136, "endOffset": 144}, {"referenceID": 16, "context": "Therefore, a robust model is required at early stage of the training iteration while a more expressive and powerful model such as a DNN [16, 17] can be used after the reliable dictionary is obtained.", "startOffset": 136, "endOffset": 144}, {"referenceID": 17, "context": "We employed the Linde-Buzo-Gray (LBG) algorithm [18] with a squared-error distortion measure to cluster the acoustic feature vectors.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The solution of (4) proposed in [15] is an extension of the standard one-dimensional Viterbi algorithm to K dimensions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "An efficient approximation of the K-dimensional Viterbi algorithm has been proposed in [10] where the problem to find the joint alignment and the optimal common sequence for K utterances is decomposed intoK\u22121 applications of two-dimensional Viterbi algorithm.", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "We conducted several sets of experiments on the TIMIT corpus [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "All hidden layers were equipped with the Rectified Linear Unit (ReLU) non-linearity [20].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "We applied dropout [16] to all hidden layers with dropout probability 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "HMMs were trained using a modified version of HTK [21] and DNNs were implemented using Lasagne [22].", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs. However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages. Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset.", "creator": "LaTeX with hyperref package"}}}