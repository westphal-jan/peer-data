{"id": "1606.05174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Deep Reinforcement Learning Discovers Internal Models", "abstract": "Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right state representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-SNE maps. We explain how to evaluate its fitness and show surprising results indicating high compatibility with the policy at hand. We conclude by showing how using the SAMDP model, an extra performance gain can be squeezed from the agent.", "histories": [["v1", "Thu, 16 Jun 2016 13:09:16 GMT  (1904kb,D)", "http://arxiv.org/abs/1606.05174v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nir baram", "tom zahavy", "shie mannor"], "accepted": false, "id": "1606.05174"}, "pdf": {"name": "1606.05174.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning Discovers Internal Models", "authors": ["Nir Baram", "Tom Zahavy"], "emails": ["{nirb@campus,", "tomzahavy@campus,", "shie@ee}.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This is an example of how such a model explains the ability to automatically learn good representations. Unfortunately, its high level of expression is also the source of its ambiguity, making it very difficult to analyze its skills. DNN visualization methods attempt to address this problem by analyzing and interpreting the learned representations. [Zeiler and Fergus, 2014, Erhan et al., 2009, Yosinski et al., 2014] However, these methods were developed for supervised learning tasks, with the data i.i.d, which overlooks the temporal structure of the learned representations."}, {"heading": "2 Background", "text": "In this context, the goal of an RL agent is to maximize his expected return by learning a policy approach."}, {"heading": "3 Semi Aggregated Markov Decision Processes", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3.1 State aggregation", "text": "The result is a compact, well-separated representation that is easy to visualize and interpret."}, {"heading": "3.2 Temporal abstractions", "text": "We define the SAMDP capabilities by initiating and terminating AMDP states C: \u03c3ij = < {ci}, \u03c0i, j, {cj} >. (5) More implicitly, as soon as the DQN agent enters an AMDP capability into an MDP state, it follows the competence policy \u03c0i, j for k steps until it reaches a state st + k-cj, s.t i 6 = j. Note that we do not implicitly define competence policy, but we will later observe that our model successfully defines capabilities spatio-temporally. We set the SAMDP discount factor high as it was used to train the DQN capability. We now turn to the SAMDP probability matrix and reward signal to estimate. To this end, we make the following assumptions: Definition 1. A deterministic probability matrix, is one that each of its series contains an element that is equal to the other 0.1."}, {"heading": "3.3 Evaluation criteria", "text": "We follow the analysis of [Hallak et al., 2013] and define criteria to empirically measure the suitability of a model. We define the value Mean Square Error (VMSE) as the normalized distance between two estimates: VMSE = VDQN \u2212 vDQN (SAMDP state): vDQN (cj) = 1 | Cj | i: cj dimension vDQN (si). The principle Minimum Description Length (MDL; [Rissanen, 1978]) is a formalization of the celebrated Occams Razor. It overcomes the overfit problem for the purpose of model selection."}, {"heading": "4 Experiments", "text": "This year, more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place and a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a, a place, a place, a place, a place, a place, a place, a, a, a place, a place, a, a, a place, a place, a, a, a, a place, a, a, a place, a, a, a, a, a, a, a, a, a place, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "5 Discussion", "text": "In this paper, we looked at the problem of automatically building a SAMDP model for analyzing trained strategies, starting with a t-SNE map of neural activations and ending with a compact model that provides a clear interpretation for complex RL tasks. We showed how SAMDP can help identify skills that are well defined in terms of initiation and termination. However, the SAMDP does not provide much information about the competence policy applied and we propose to further investigate this in future work. It would also be interesting to see whether capabilities of different states actually represent the same behavior. Most importantly, the capabilities we find are determined by state aggregation, so they are affected by the artifacts of the cluster method. In future work, we will consider other cluster methods that are better related to topology (such as spectral clustering) to see if they lead to better capabilities."}], "references": [{"title": "Adaptive aggregation methods for infinite horizon dynamic programming", "author": ["Dimitri P Bertsekas", "David A Castanon"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Bertsekas and Castanon.,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Castanon.", "year": 1989}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "The internal model principle for linear multivariable regulators", "author": ["Bruce A Francis", "William M Wonham"], "venue": "Applied mathematics and optimization,", "citeRegEx": "Francis and Wonham.,? \\Q1975\\E", "shortCiteRegEx": "Francis and Wonham.", "year": 1975}, {"title": "Model selection in markovian processes", "author": ["Assaf Hallak", "Dotan Di-Castro", "Shie Mannor"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "Hallak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hallak et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Long-Ji Lin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin.,? \\Q1993\\E", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": null, "citeRegEx": "MacQueen,? \\Q1967\\E", "shortCiteRegEx": "MacQueen", "year": 1967}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["Ronald Parr"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Parr.,? \\Q1998\\E", "shortCiteRegEx": "Parr.", "year": 1998}, {"title": "Towards perceptual shared autonomy for robotic mobile manipulation", "author": ["Benjamin Pitzer", "Michael Styer", "Christian Bersch", "Charles DuHadway", "Jan Becker"], "venue": "In IEEE International Conference on Robotics Automation (ICRA),", "citeRegEx": "Pitzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pitzer et al\\.", "year": 2011}, {"title": "Modeling by shortest data", "author": ["Jorma Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Policy distillation", "author": ["Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Adaptation and regulation with signal detection implies internal model", "author": ["Eduardo D Sontag"], "venue": "Systems & control letters,", "citeRegEx": "Sontag.,? \\Q2003\\E", "shortCiteRegEx": "Sontag.", "year": 2003}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": null, "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Joe H. Ward"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward.,? \\Q1963\\E", "shortCiteRegEx": "Ward.", "year": 1963}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": null, "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": null, "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Deep Q Network (DQN) is an off-policy learning algorithm that uses a Convolutional Neural Network (CNN; [Krizhevsky et al., 2012]) to represent the action-value function.", "startOffset": 104, "endOffset": 129}, {"referenceID": 9, "context": "Agents trained using DQN are showing superior performance on a wide range of problems [Mnih et al., 2015].", "startOffset": 86, "endOffset": 105}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics.", "startOffset": 47, "endOffset": 77}, {"referenceID": 3, "context": "The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system.", "startOffset": 29, "endOffset": 55}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics. On the other hand, temporal abstractions (i.e., options or skills [Sutton et al., 1999a]) can help an agent to focus less on lower level details of a task and more on high level planning [Dietterich, 2000, Parr, 1998]. The problem with these methods is that finding good abstractions is typically done manually which hampers their wide use. The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system.", "startOffset": 48, "endOffset": 730}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics. On the other hand, temporal abstractions (i.e., options or skills [Sutton et al., 1999a]) can help an agent to focus less on lower level details of a task and more on high level planning [Dietterich, 2000, Parr, 1998]. The problem with these methods is that finding good abstractions is typically done manually which hampers their wide use. The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system. In this work we follow the same line of thought and claim that DQNs are learning an underlying spatio-temporal model of the problem, without implicitly being trained to. We identify this model as an Semi Aggregated Markov Decision Process (SAMDP), an approximation of the true MDP that allows human interpretability. Zahavy et al. [2016] used hand-crafted features in order to interpret policies learned by DQN agents.", "startOffset": 48, "endOffset": 1221}, {"referenceID": 9, "context": "[Mnih et al., 2015].", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "DQN is an offline learning algorithm that collects experience tuples {st,at, rt, st+1, \u03b3} and stores them in the Experience Replay (ER) [Lin, 1993].", "startOffset": 136, "endOffset": 147}, {"referenceID": 15, "context": "The optimal skill value function is given by: Q\u03a3(s, \u03c3) = E[R s + \u03b3max \u03c3\u2032\u2208\u03a3 Q\u03a3(s \u2032, \u03c3\u2032)] [Stolle and Precup, 2002].", "startOffset": 88, "endOffset": 113}, {"referenceID": 22, "context": "Zahavy et al. [2016] showed that this state representation captures a spatio-temporal hierarchy and therefore makes a good candidate for state aggregation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "In order to encourage temporal coherency in cluster assignments we define a new linkage criteria based on Ward [1963]: c(A,B) = (1\u2212 \u03bb) \u00b7mean{\u2016xa \u2212 xb\u2016 : a \u2208 A, b \u2208 B}+ \u03bb \u00b7 e{A,B}\u2192AB (4)", "startOffset": 106, "endOffset": 118}, {"referenceID": 4, "context": "3 Evaluation criteria We follow the analysis of [Hallak et al., 2013] and define criteria to measure the fitness of a model empirically.", "startOffset": 48, "endOffset": 69}, {"referenceID": 12, "context": "The Minimum Description Length (MDL; [Rissanen, 1978]) principle is a formalization of the celebrated Occams Razor.", "startOffset": 37, "endOffset": 53}, {"referenceID": 22, "context": "We also see evidence for the \u201dtunnel-digging\u201d option described in [Zahavy et al., 2016] in the transitions between clusters 11,12,14 and 4.", "startOffset": 66, "endOffset": 87}, {"referenceID": 11, "context": "The motivation for this experiment stems from the idea of shared autonomy [Pitzer et al., 2011].", "startOffset": 74, "endOffset": 95}, {"referenceID": 13, "context": "Another question we\u2019re interested in answering is whether a global control structure exists? Motivated by the success of policy distillation ideas [Rusu et al., 2015], it would be interesting to see how well an SAMDP built for game A, explains game B? Finally we would like to use this model to interpret other DRL agents that are not specifically trained to approximate value such as deep policy gradient methods.", "startOffset": 147, "endOffset": 166}], "year": 2016, "abstractText": "Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right state representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-SNE maps. We explain how to evaluate its fitness and show surprising results indicating high compatibility with the policy at hand. We conclude by showing how using the SAMDP model, an extra performance gain can be squeezed from the agent.", "creator": "LaTeX with hyperref package"}}}