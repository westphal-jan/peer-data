{"id": "1606.05409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Sense Embedding Learning for Word Sense Induction", "abstract": "Conventional word sense induction (WSI) methods usually represent each instance with discrete linguistic features or cooccurrence features, and train a model for each polysemous word individually. In this work, we propose to learn sense embeddings for the WSI task. In the training stage, our method induces several sense centroids (embedding) for each polysemous word. In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space. The advantages of our method are (1) distributed sense vectors are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models, and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. We further verify the two advantages by comparing with carefully designed baselines.", "histories": [["v1", "Fri, 17 Jun 2016 02:49:52 GMT  (247kb)", "https://arxiv.org/abs/1606.05409v1", "6 pages, no figures"], ["v2", "Wed, 22 Jun 2016 04:59:08 GMT  (23kb)", "http://arxiv.org/abs/1606.05409v2", "6 pages, no figures in *SEM 2016"]], "COMMENTS": "6 pages, no figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "zhiguo wang", "haitao mi", "daniel gildea"], "accepted": false, "id": "1606.05409"}, "pdf": {"name": "1606.05409.pdf", "metadata": {"source": "CRF", "title": "Sense Embedding Learning for Word Sense Induction", "authors": ["Linfeng Song", "Zhiguo Wang", "Daniel Gildea"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 409v 2 [cs.C L] 22 Jun 2016"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to feel the way they are, to behave as if they were able to move, as if they were able to move."}, {"heading": "2 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Word Sense Induction", "text": "WSI is generally considered an uncontrolled cluster task within the framework of the distribution hypothesis (Harris, 1954) that the meaning of words is reflected in the contexts in which it appears. Existing WSI methods can be roughly divided into function-based or Bayesian. Feature-based methods first represent each instance as a context vector, then use a cluster algorithm on the context vectors to induce all senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, detect senses based on theme models. They adopt either the LDA (Lead et al., 2003) or HDP (Teh et al., 2006) model, using each target as a corpus and the contexts as pseudo-documents."}, {"heading": "2.2 Sense Embedding for WSI", "text": "There are two key factors for a meaningful embedding of learning: (1) how to learn the number of senses for each ambiguous word, and (2) how to learn an embedding representation for each word, and (2) how to learn an embedding representation for each word, and each instance is assigned to the most likely sense according to Equation 1, where \u00b5 (wt, k) is the vector for the k-th sense of the word w, and vc is the representation vector of the instance. st = arg max k = 1, K sim (\u00b5, k), vc) (1) Another group of methods (Li and Jurafsky, 2015) does not deal with parametric to dynamically decide the number of numbers."}, {"heading": "3 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experimental Setup and baselines", "text": "We evaluate our methods on the test set of the SemEval-2010 WSI task (Manandhar et al., 2010). It contains 8,915 instances for 100 target words (50 nouns and 50 verbs), most of which come from the messaging area. We choose the snapshot of Wikipedia from April 2010 (Shaoul and Westbury, 2010) as the training set, as it is freely available and general. It contains around 2 million documents and 990 million tokens. We train and test our models and baselines according to the above data setting and compare with the reported performance on the same test set from previous papers. For our Sense embedding methods, we build two systems: SE-WSI-fix, the Multi-Sense-Skip-gram (MSSG) model (Neelakantan et al., 2014) and reference 3 sensory settings for each word type, and SE-WSI-CRI-CRP (Li and Jurafsky, 2015), which dynamically learn the number of sensory settings."}, {"heading": "3.2 Comparing on SemEval-2010", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "4 Related Work", "text": "The other method simply used MSSG (Neelakantan et al., 2014) and changed the context vector calculation from the average of all context vectors to a weighted average. Our paper contains further contributions. First, we clearly point out the two advantages of sense embedding methods: 1) joint learning within the framework of the Mutli Task Learning Framework, 2) better knowledge representation through discriminatory training and experimental verification. In addition, we use various sense embedding methods to show that sense embedding methods are generally promising for WSI, not just one method is better than others. Finally, we compare our methods with current WSI methods on both monitored and unattended metrics."}, {"heading": "5 Conclusion", "text": "In this paper, we demonstrate that Sense Embedding is a promising approach for WSI by applying two different Sense Embedding-based systems to the SemEval 2010 WSI task. Both systems demonstrate highly competitive performance while learning a general model for thousands of words (not just the tested polysemic words). We believe that the two benefits of our method are: 1) shared learning within the Multi-Task Learning Framework, 2) better knowledge representation through discriminatory training and experimental verification."}, {"heading": "Acknowledgments", "text": "We thank Yue Zhang for his insightful comments on the first version of the essay and the anonymous reviewer for the insightful comments."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Word sense induction: Triplet-based clustering and automatic evaluation", "author": ["Stefan Bordag"], "venue": "In EACL. Citeseer", "citeRegEx": "Bordag.,? \\Q2006\\E", "shortCiteRegEx": "Bordag.", "year": 2006}, {"title": "Bayesian word sense induction", "author": ["Brody", "Lapata2009] Samuel Brody", "Mirella Lapata"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL", "citeRegEx": "Brody et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brody et al\\.", "year": 2009}, {"title": "A fully unsupervised word sense disambiguation method using dependency knowledge", "author": ["Chen et al.2009] Ping Chen", "Wei Ding", "Chris Bowes", "David Brown"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Naive Bayes word sense induction", "author": ["Choe", "Charniak2013] Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Choe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choe et al\\.", "year": 2013}, {"title": "Unsupervised word sense induction using distributional statistics", "author": ["Goyal", "Hovy2014] Kartik Goyal", "Eduard H Hovy"], "venue": "In COLING,", "citeRegEx": "Goyal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goyal et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Chris Dyer", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation", "author": ["Korkontzelos", "Suresh Manandhar"], "venue": "In Proceedings of the 5th International Workshop on Semantic", "citeRegEx": "Korkontzelos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Korkontzelos et al\\.", "year": 2010}, {"title": "Neural context embeddings for automatic discovery of word senses", "author": ["Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2015}, {"title": "Word sense induction for novel sense detection", "author": ["Lau et al.2012] Jey Han Lau", "Paul Cook", "Diana McCarthy", "David Newman", "Timothy Baldwin"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-", "citeRegEx": "Lau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2012}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Li", "Jurafsky2015] Jiwei Li", "Dan Jurafsky"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Semeval-2010 task 14: Word sense induction & disambiguation", "author": ["Ioannis P Klapaftis", "Dmitriy Dligach", "Sameer S Pradhan"], "venue": "In Proceedings of the 5th international workshop on semantic evaluation,", "citeRegEx": "Manandhar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Inducing word senses to improve web search result clustering", "author": ["Navigli", "Crisafulli2010] Roberto Navigli", "Giuseppe Crisafulli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of the 2014 Conference on Em-", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Duluth-wsi: Senseclusters applied to the sense induction task", "author": ["Ted Pedersen"], "venue": null, "citeRegEx": "Pedersen.,? \\Q2010\\E", "shortCiteRegEx": "Pedersen.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Word sense discrimination by clustering contexts in vector and similarity spaces", "author": ["Purandare", "Pedersen2004] Amruta Purandare", "Ted Pedersen"], "venue": null, "citeRegEx": "Purandare et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Purandare et al\\.", "year": 2004}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "The westbury lab wikipedia", "author": ["Shaoul", "Westbury2010] Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul et al\\.", "year": 2010}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "A sense-topic model for word sense induction with unsupervised data enrichment", "author": ["Wang et al.2015] Jing Wang", "Mohit Bansal", "Kevin Gimpel", "Brian Ziebart", "Clement Yu"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A sense-based translation model for statistical machine translation", "author": ["Xiong", "Zhang2014] Deyi Xiong", "Min Zhang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Xiong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2014}, {"title": "Nonparametric bayesian word sense induction", "author": ["Yao", "Van Durme2011] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing,", "citeRegEx": "Yao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012).", "startOffset": 184, "endOffset": 202}, {"referenceID": 1, "context": "However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge.", "startOffset": 94, "endOffset": 192}, {"referenceID": 3, "context": "However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge.", "startOffset": 94, "endOffset": 192}, {"referenceID": 14, "context": "There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 18, "context": "There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 7, "context": ", 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015; Chen et al., 2014; Tian et al., 2014).", "startOffset": 28, "endOffset": 160}, {"referenceID": 8, "context": ", 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015; Chen et al., 2014; Tian et al., 2014).", "startOffset": 28, "endOffset": 160}, {"referenceID": 4, "context": ", 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015; Chen et al., 2014; Tian et al., 2014).", "startOffset": 28, "endOffset": 160}, {"referenceID": 23, "context": ", 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch\u00fctze, 2015; Chen et al., 2014; Tian et al., 2014).", "startOffset": 28, "endOffset": 160}, {"referenceID": 0, "context": "are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework (Caruana, 1997).", "startOffset": 117, "endOffset": 138}, {"referenceID": 11, "context": "Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models.", "startOffset": 17, "endOffset": 125}, {"referenceID": 24, "context": "Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models.", "startOffset": 17, "endOffset": 125}, {"referenceID": 7, "context": "To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where \u03bc(wt, k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance.", "startOffset": 67, "endOffset": 113}, {"referenceID": 16, "context": "To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where \u03bc(wt, k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance.", "startOffset": 67, "endOffset": 113}, {"referenceID": 16, "context": "For the update function, vectors are updated by the Skip-gram method (same as Neelakantan et al. (2014)) which tries to predict context words with the current sense.", "startOffset": 78, "endOffset": 104}, {"referenceID": 0, "context": "taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014).", "startOffset": 162, "endOffset": 183}, {"referenceID": 13, "context": "We evaluate our methods on the test set of the SemEval-2010 WSI task (Manandhar et al., 2010).", "startOffset": 69, "endOffset": 93}, {"referenceID": 16, "context": "For our sense embedding method, we build two systems: SE-WSI-fix which adopts Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014) and assigns 3 senses for each word type, and SE-WSI-CRP (Li and Jurafsky, 2015) which dynamically decides the number of senses using a Chinese restaurant process.", "startOffset": 113, "endOffset": 139}, {"referenceID": 17, "context": "Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR.", "startOffset": 0, "endOffset": 16}, {"referenceID": 16, "context": "The other method simply adopted MSSG (Neelakantan et al., 2014) and changed context vector calculation from the average of all context word vectors to weighted average.", "startOffset": 37, "endOffset": 63}], "year": 2016, "abstractText": "Conventional word sense induction (WSI) methods usually represent each instance with discrete linguistic features or cooccurrence features, and train a model for each polysemous word individually. In this work, we propose to learn sense embeddings for the WSI task. In the training stage, our method induces several sense centroids (embedding) for each polysemous word. In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space. The advantages of our method are (1) distributed sense vectors are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models, and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. We further verify the two advantages by comparing with carefully designed baselines.", "creator": "LaTeX with hyperref package"}}}