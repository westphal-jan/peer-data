{"id": "1606.05688", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks on Multi-Core CPUs and GPUs", "abstract": "Sliding window convolutional networks (ConvNets) have become a popular approach to computer vision problems such as image segmentation, and object detection and localization. Here we consider the problem of inference, the application of a previously trained ConvNet, with emphasis on 3D images. Our goal is to maximize throughput, defined as average number of output voxels computed per unit time. Other things being equal, processing a larger image tends to increase throughput, because fractionally less computation is wasted on the borders of the image. It follows that an apparently slower algorithm may end up having higher throughput if it can process a larger image within the constraint of the available RAM. We introduce novel CPU and GPU primitives for convolutional and pooling layers, which are designed to minimize memory overhead. The primitives include convolution based on highly efficient pruned FFTs. Our theoretical analyses and empirical tests reveal a number of interesting findings. For some ConvNet architectures, cuDNN is outperformed by our FFT-based GPU primitives, and these in turn can be outperformed by our CPU primitives. The CPU manages to achieve higher throughput because of its fast access to more RAM. A novel primitive in which the GPU accesses host RAM can significantly increase GPU throughput. Finally, a CPU-GPU algorithm achieves the greatest throughput of all, 10x or more than other publicly available implementations of sliding window 3D ConvNets. All of our code has been made available as open source project.", "histories": [["v1", "Fri, 17 Jun 2016 22:16:39 GMT  (691kb,D)", "http://arxiv.org/abs/1606.05688v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["aleksandar zlateski", "kisuk lee", "h sebastian seung"], "accepted": false, "id": "1606.05688"}, "pdf": {"name": "1606.05688.pdf", "metadata": {"source": "CRF", "title": "ZNNi \u2013 Maximizing the Inference Throughput of 3D Convolutional Networks on Multi-Core CPUs and GPUs", "authors": ["Aleksandar Zlateski", "Kisuk Lee", "H. Sebastian Seung"], "emails": ["zlateski@mit.edu,", "kisuklee@mit.edu", "sseung@princeton.edu"], "sections": [{"heading": null, "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far."}, {"heading": "II. THROUGHPUT OF SLIDING WINDOW INFERENCE", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "III. PRUNED FFT", "text": "The advantages of the FFT convolution have been demonstrated for 2D ConvNets on GPUs [8], [9] and 3D ConvNets on CPUs [10]. In the FFT convolution, the kernel and image are split to a common size. As the kernel is typically much smaller than the image, the padded kernel consists largely of zeros. Ignoring the zeros is known as FFT pruning and can provide acceleration. We propose and implement such an algorithm for both the CPU and the GPU. Our approach achieves an average of 5x acceleration compared to the na\u00efve approach on the CPU and 10x acceleration on the GPU. Acceleration is likely to be great for a ConvFTs understanding that typically contains many more kernel FFTs than image FFTs."}, {"heading": "A. General algorithm", "text": "For 3D FFT-based folding, the 3D images x and y are initially increased to the same size as long as their size is the same. A 3D FFT is obtained by calculating 1D FFTs along the three dimensions. Some of these 1D FFTs are equal to 0 from an array of all elements, which cannot be calculated because the FFT of an all-zeros signal are all zeros. We can reduce the amount of calculation by calculating only necessary 1D transformations. In calculating the FFT of a traceable core of size k3, the size of n3 was increased to n3, instead of naively calculating n2 1D FFTs along each dimension, which requires Cn3 protocol n3, we could first perform k2 FFTs along one dimension, then k \u00d7 x along the next and finally n2 along the last dimension, reducing these FFTs to most compared to the cost of Fig + 2."}, {"heading": "B. CPU implementation", "text": "Suppose we want to calculate the FFT of an x \u00b7 y \u00b7 z image that is zero-padded to an x \u00b7 y \u00b7 \u00d7 z \u2032 image. It is easy to do this by creating a linear copy of the memory and padding the rest. Then, we perform y \u00b7 z real to complex FFT transformations along the x-direction. FFT are misplaced into a pre-assigned and zero initialized complex-rated bx \u2032 2 c + 1 \u00b7 y \u00b7 z \u2032 image. We then perform 1D transformations in place along the y-direction, followed by the z-direction. Reverse FFT is calculated by following the steps above in reverse order. 1D FFT can be performed either serially or by N workers in parallel (parallel to the loop)."}, {"heading": "C. GPU implementation", "text": "On the GPU, we must repeatedly perform FFTs on b 3D images simultaneously to achieve a high utilization of the many GPU threads. \"We must perform the FFTs along the three least significant dimensions of the tensors.\" Our algorithm calculates the 3D FFTs as a series of tensor operations that adjust the size of the 4D FFTs along the least significant dimension of the tensors. When we perform the FFT transformations of b 3D images of any size x x x x x x x x x padded to x. \"We prepare the input by adjusting the 4D FFTs along the result.\" The transformation is done by b x x x x x x x x x x x x x. \"We must prepare the input by extending the 4D tensor along the z direction."}, {"heading": "IV. CONVOLUTIONAL LAYERS", "text": "We start with the primitives for the coil layers that arithmetically are the most intensive. Input to a coil layer consists of a tuple of f-images and output from a tuple of f-images. We want to process a stack of S-inputs to obtain a stack of S-outputs, viaOs, j = f-outputs i = 1 wji-outputs is \u00b7 \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x \u00b7 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "A. CPU algorithms", "text": "The three parallel algorithms suitable for the multi-core CPUs are the following two algorithms: The first algorithm variant leads to a direct confrontation, while the other two algorithms are based on a common confrontation. (The second algorithm variant is), the second algorithm variant (the third algorithm variant), the third (the third), the third (the third), the third (the third), the third (the third), the third (the), the third (the), the fourth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the fifth (the), the (the), the fifth (the), the fifth (the), the (the), the fifth (the), the (the), the fifth (the), the (the), the fifth (the), the (the), the (the (the), the third (the), the third (the), the third (the third (the), the third (the third (the), the third (the third (the), the third (the third (the), the third (the third (the), the third (the), the third (the third (the), the third (the third (the), the third (the third (the), the third (the), the third (the), the third (the third (the third (the), the third (the), the third (the third (the), the third (the), the third (the third (the third (the), the third (the), the third (the third (the), the third (the), the third (the third (the), the third (the), the third (the third (the third (the), the third (the), the third (the), the third (the third (the third (the),)"}, {"heading": "B. GPU implementations", "text": "The third FFT-based implementation is based on our previously described but not yet described matrix multiplication, which means that the matrices are not actually created; the first algorithm improves speed by pre-computing a set of indexes, and thus requires additional memory; and the second algorithm, which we describe for each additional memory.2) FFT-based algorithm is based on the GPU implementation of the pruned FFT algorithms, does not require additional memory.2) FFT-based algorithms are based on the GPU implementation of the FFT algorithms."}, {"heading": "V. MAX-POOLING AND MAX-POOLING FRAGMENTS", "text": "The maximum value is calculated for each block and yields an image of the size < nx / px, ny / py, nz / pz >. The maximum image size ~ n is limited so that nx, ny, and nz are divisible by px, py, and pz. On the CPU, we implement the max-pooling layer so that the max-pooling layer of each image is executed in parallel (e.g. by using parallels for loops). For the GPU, we use the cuDNN primitives for max-pooling.If the input image is the same size as the ConvNet field of view, the output image consists of a single voxel.Max pooling fragmentation of an image ~ n with the window size ~ pooling. If the input image is the same size as the ConvNet field of view, the output image consists of a single voxel.Max pooling fragment of an image with the image size ~ pooling of the window light with the image size ~ px)."}, {"heading": "VI. GPU-ONLY OR CPU-ONLY INFERENCE", "text": "By stringing together the primitives of the CPU layer (or GPU layer) defined above, we can now construct CPU-pure (or GPU-only) algorithms for ConvNet inferences. For each layer, we have a choice of several primitives. Each max pooling layer can be replaced by an MPF layer. Size of the input patch and number of inputs in the batch should be selected. These parameters and primitives should be selected to maximize throughput. Below, we describe some theoretical considerations and empirical data about the optimal choice."}, {"heading": "A. Maximizing throughput", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "B. Empirical results", "text": "For benchmarking purposes, we decided to make the field of view relatively large, achieved either by adding more pooling layers or larger filters; two other architectures had seven convolutional and three pooling layers in the sequence CPCPCPCCCC; the networks had kernel sizes of 3 x 3 x 5 x 5; two other architectures had 6 convolutional and 2 pooling layers (CPCPCCCC) with larger filter sizes of 7 x 7 x 9 x 9; and the architectures of all four benchmarked networks are shown in Table III. A rectified linear transfer function is applied after each convolutional and 2 pooling layers; the complexity of the transfer function has little impact on the computing time as it is only a small fraction of the total number of computations.The benchmarks are performed on two machines."}, {"heading": "VII. GPU + HOST RAM AND CPU\u2013GPU INFERENCE", "text": "We have seen that throughput can vary greatly depending on the size of the input image we can process over the network. For cores of size 53 or more, the CPU seems to outperform the GPU because of these limitations. Therefore, we are introducing another primitive layer, a layer whose data is stored in the host RAM and partially uploaded to the GPU where the computation is performed."}, {"heading": "A. GPU + host RAM convolutional layer", "text": "Consider a revolutionary layer whose input is form (S, f, x, y, z) and output form (S, f, \"x,\" y, \"z\"). The compilation performed by the layer can be divided into N layers with input forms of (Si, fi, x, y, z) and output form (Si, f \"i, x,\" y, \"z\"). Fig. 6 illustrates how the calculation of a revolutionary layer with S = 1, f = 6, and f \"= 4 can be divided into N = 4 sublayers, each with Si = 1, fi = 3, and f\" i \"= 2. The blue color represents the input images to be transferred to the GPU, the red color represents the memory to be allocated on the GPU. The green color represents the results to be transferred back to the host."}, {"heading": "B. GPU + host RAM ConvNet execution", "text": "However, it turned out that it is better to calculate the MPF layers on the CPU, even if there are very few cores available, because of the expensive transfer to and from the device and the relatively low computing complexity of the MPF layer. The easiest way to run a network with the GPU for computing and storage is to use individually optimized GPU + HostRAM layer, described above for each computing layer, and the CPU implementation of an MPF layer for each pooling layer. Turns out that when we use MPF layers to evaluate a ConvNet pooling form, we can achieve better performance by transferring the data to and from the GPU. To understand how we understand the following property of a Convv network."}, {"heading": "C. CPU\u2013GPU ConvNet execution", "text": "Finally, conclusions can be drawn by using both the CPU and the GPU. As in the GPU + Host RAM approach, the network layers are divided into two groups; for the first two layers, we use the optimal CPU implementation as defined in Section VI, and for the rest of the layers, we use the optimal GPU implementation as defined in the previous section. The CPU and the GPU form a producer-consumer pipeline; the CPU generates the result by calculating the first layers for a given input image and by queuing; the GPU consumes the data in the queue by taking the output of the fourth layer as input and producing the final output of the last layer.This approach can generate a huge amount of memory when the CPU produces data much faster than the GPU, which is why the CPU can consume the data in the queue."}, {"heading": "VIII. COMPARISON TO OTHER ALGORITHMS", "text": "We compare our 4 approaches (GPU-only, CPU-only, GPU + host RAM and GPU CPU) with other publicly available implementations and show the results in Table V. All benchmarks are performed on the same hardware, the 4-way Intel Xeon E7-8890 v3 machine with 256GB of RAM and a Titan X GPU. The results of our approaches are based on the optimizations described in the previous paragraphs. For the other approaches, we vary the input sizes and measure throughput, we reported the highest value of throughput, which always correlates to the size of the input we were able to process. The baseline (cuDNN) approach consists of calling the cuDNN [29] prices for conversion and max pooling. Unlike other approaches, this is not a general framework - it requires the user to write some codes into the cuDNN."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Kai Li and Nir Shavit for helpful discussions. We thank Intel Corporation for providing the Intel Xeon E7-8890 v3 4-way computer and for supporting the Intel Parallel Computing Center at Princeton University. We thank IARPA (D16PC00005), the Mathers Foundation, NIH / NINDS and the US Army Research Office (W911NF-12-1-0594) for their support. Kisuk Lee was supported by a Samsung scholarship."}], "references": [{"title": "High performance convolutional neural networks for document processing", "author": ["K. Chellapilla", "S. Puri", "P. Simard"], "venue": "Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Accelerating large-scale convolutional neural networks with parallel graphics multiprocessors", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Artificial Neural Networks\u2013ICANN 2010. Springer, 2010, pp. 82\u201391.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance and scalability of gpu-based convolutional neural networks", "author": ["D. Strigl", "K. Kofler", "S. Podlipnig"], "venue": "2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing. IEEE, 2010, pp. 317\u2013324.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L. Maria Gambardella", "J. Schmidhuber"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 1237.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Internet trends 2014", "author": ["M. Meeker"], "venue": "http://www.kpcb.com/blog/ 2014-internet-trends, 2014 (accessed April 9, 2016).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The big data challenges of connectomics", "author": ["J.W. Lichtman", "H. Pfister", "N. Shavit"], "venue": "Nature neuroscience, vol. 17, no. 11, pp. 1448\u20131454, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "International Conference on Learning Representations (ICLR2014). CBLS, April 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "arXiv preprint arXiv:1412.7580, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Znn-a fast and scalable algorithm for training 3d convolutional networks on multi-core and many-core shared memory machines", "author": ["A. Zlateski", "K. Lee", "H.S. Seung"], "venue": "arXiv preprint arXiv:1510.06706, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient convolutional neural networks for pixelwise classification on heterogeneous hardware systems", "author": ["F. Tschopp"], "venue": "arXiv preprint arXiv:1509.03371, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "ELEKTRONN a neural network toolkit", "author": ["M.P.I.F.M. Research"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Multi-digit recognition using a space displacement neural network", "author": ["O. Matan", "C.J. Burges", "Y. LeCun", "J.S. Denker"], "venue": "NIPS. Citeseer, 1991, pp. 488\u2013495.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised learning of image restoration with convolutional networks", "author": ["V. Jain", "J.F. Murray", "F. Roth", "S. Turaga", "V. Zhigulin", "K.L. Briggman", "M.N. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Toward automatic phenotyping of developing embryos from videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "venue": "Image Processing, IEEE Transactions on, vol. 14, no. 9, pp. 1360\u20131371, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Giusti", "D.C. Cire\u015fan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1302.1700, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast learning algorithm for image segmentation with max-pooling convolutional networks", "author": ["J. Masci", "A. Giusti", "D. Ciresan", "G. Fricout", "J. Schmidhuber"], "venue": "Image Processing (ICIP), 2013 20th IEEE International Conference on. IEEE, 2013, pp. 2713\u20132717.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fftw users manual", "author": ["M. Frigo", "S.G. Johnson"], "venue": "Massachusetts Institute of Technology, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Fftw: An adaptive software architecture for the fft", "author": ["\u2014\u2014"], "venue": "Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, vol. 3. IEEE, 1998, pp. 1381\u20131384.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Cufft library", "author": ["C. Nvidia"], "venue": "2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Thrust: A 2 6", "author": ["N. Bell", "J. Hoberock"], "venue": "GPU Computing Gems Jade Edition, p. 359, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "High Performance Parallelism Pearls Volume Two: Multicore and Many-core Programming Approaches", "author": ["J. Jeffers", "J. Reinders"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Intel threading building blocks: outfitting C++ for multicore processor parallelism", "author": ["J. Reinders"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Putting intel R  \u00a9 threading building blocks to work", "author": ["T. Willhalm", "N. Popovici"], "venue": "Proceedings of the 1st international workshop on Multicore software engineering. ACM, 2008, pp. 3\u20134.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM International Conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The revival has been driven by increases in the speed of ConvNet training made possible by GPU implementations [1], [2], [3], [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "The revival has been driven by increases in the speed of ConvNet training made possible by GPU implementations [1], [2], [3], [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "The revival has been driven by increases in the speed of ConvNet training made possible by GPU implementations [1], [2], [3], [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "The revival has been driven by increases in the speed of ConvNet training made possible by GPU implementations [1], [2], [3], [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "Billions of photos and millions of videos are shared online every day [5], [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "millimeter of brain in a few weeks [7].", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Our FFTbased convolutional primitive for the GPU is designed to use much less memory than the algorithm proposed by [8], [9] and implemented in fbfft.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Our FFTbased convolutional primitive for the GPU is designed to use much less memory than the algorithm proposed by [8], [9] and implemented in fbfft.", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "The task parallel algorithm is designed to use less memory than the one previously proposed for ConvNet training [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "mentation of [11], ZNN [10], and ELEKTRONN [12].", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "mentation of [11], ZNN [10], and ELEKTRONN [12].", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "mentation of [11], ZNN [10], and ELEKTRONN [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Sliding window ConvNets were originally applied to detect and/or localize objects in a larger image [13], and this usage has been revived [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "Sliding window ConvNets were originally applied to detect and/or localize objects in a larger image [13], and this usage has been revived [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "Sliding window ConvNets have also been applied to image segmentation, to produce an output image representing the probability that a voxel is a boundary between objects or not [15].", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "And they have been applied to semantic segmentation, the problem of labeling each voxel in an input image by the class of the object to which it belongs [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "Here we instead provide pooling primitives that compute max-pooling fragments (MPF), a more efficient strategy for sliding window computations [17], [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Here we instead provide pooling primitives that compute max-pooling fragments (MPF), a more efficient strategy for sliding window computations [17], [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "The MPF algorithm computes the same results as the approach known as \u201cdilated convolution\u201d[19], \u201cstrided kernels\u201d[11], \u201cmax filtering\u201d[10], or \u201cfilter rarefaction\u201d[20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "The MPF algorithm computes the same results as the approach known as \u201cdilated convolution\u201d[19], \u201cstrided kernels\u201d[11], \u201cmax filtering\u201d[10], or \u201cfilter rarefaction\u201d[20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "The MPF algorithm computes the same results as the approach known as \u201cdilated convolution\u201d[19], \u201cstrided kernels\u201d[11], \u201cmax filtering\u201d[10], or \u201cfilter rarefaction\u201d[20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "The MPF algorithm computes the same results as the approach known as \u201cdilated convolution\u201d[19], \u201cstrided kernels\u201d[11], \u201cmax filtering\u201d[10], or \u201cfilter rarefaction\u201d[20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 6, "context": "The advantages of FFT convolution have been shown for 2D ConvNets running on GPUs [8], [9], and 3D ConvNets running on CPUs [10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "The advantages of FFT convolution have been shown for 2D ConvNets running on GPUs [8], [9], and 3D ConvNets running on CPUs [10].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "The advantages of FFT convolution have been shown for 2D ConvNets running on GPUs [8], [9], and 3D ConvNets running on CPUs [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "When Intel MKL is used any such size is allowed, however, when fftw is used we only allow sizes for which e+f is either 0 or 1 [21], [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 20, "context": "When Intel MKL is used any such size is allowed, however, when fftw is used we only allow sizes for which e+f is either 0 or 1 [21], [22].", "startOffset": 133, "endOffset": 137}, {"referenceID": 21, "context": "On the GPU we use cuFFT [23], which has optimized algorithms only for sizes of the form 2357.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Image reshaping is easily implemented using the Thrust CUDA library [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "workers and the available cores \u2013 each worker is allowed to run only on a specific hardware core, as described in [26].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "This strategy is chosen over the popular alternative approach to task scheduling based on work stealing [27], [28] because it divides the work more evenly over multi\u2013chip machines and further increase cache locality.", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "This strategy is chosen over the popular alternative approach to task scheduling based on work stealing [27], [28] because it divides the work more evenly over multi\u2013chip machines and further increase cache locality.", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "[17], [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], [18].", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "The fact that MPF layers outperform max\u2013pooling layers is not surprising as it has been shown that using MPF layers reduces amount of operations required for computing a single output pixel [17], [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The fact that MPF layers outperform max\u2013pooling layers is not surprising as it has been shown that using MPF layers reduces amount of operations required for computing a single output pixel [17], [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 6, "context": "Increasing the batch size can greatly reduce the computational cost of FFT\u2013based ConvNets by reusing the transforms of the kernels [8], [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Increasing the batch size can greatly reduce the computational cost of FFT\u2013based ConvNets by reusing the transforms of the kernels [8], [9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 26, "context": "The baseline (cuDNN) approach consists of calling the cuDNN [29] primitives for convolution and max\u2013pooling.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "Caffe [30] is another GPU ConvNet framework.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "We benchmarked a version that implements sliding window ConvNets using \u201cstrided kernels\u201d [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "ELEKTRONN [12] was the only competitor that provides inference optimization for 3D sliding window ConvNets using MPF.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "ZNN [10] is a framework optimized for training sliding window 3D ConvNets on multi\u2013core and many\u2013core CPUs using \u201cmax\u2013filtering\u201d followed by FFT\u2013based \u201csparse convolution\u201d.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "Sliding window convolutional networks (ConvNets) have become a popular approach to computer vision problems such as image segmentation, and object detection and localization. Here we consider the problem of inference, the application of a previously trained ConvNet, with emphasis on 3D images. Our goal is to maximize throughput, defined as average number of output voxels computed per unit time. Other things being equal, processing a larger image tends to increase throughput, because fractionally less computation is wasted on the borders of the image. It follows that an apparently slower algorithm may end up having higher throughput if it can process a larger image within the constraint of the available RAM. We introduce novel CPU and GPU primitives for convolutional and pooling layers, which are designed to minimize memory overhead. The primitives include convolution based on highly efficient pruned FFTs. Our theoretical analyses and empirical tests reveal a number of interesting findings. For some ConvNet architectures, cuDNN is outperformed by our FFT-based GPU primitives, and these in turn can be outperformed by our CPU primitives. The CPU manages to achieve higher throughput because of its fast access to more RAM. A novel primitive in which the GPU accesses host RAM can significantly increase GPU throughput. Finally, a CPU-GPU algorithm achieves the greatest throughput of all, 10\u00d7 or more than other publicly available implementations of sliding window 3D ConvNets. All of our code has been made available as open source project.", "creator": "LaTeX with hyperref package"}}}