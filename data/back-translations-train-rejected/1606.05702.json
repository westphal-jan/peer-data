{"id": "1606.05702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Query-Focused Opinion Summarization for User-Generated Content", "abstract": "We present a submodular function-based framework for query-focused opinion summarization. Within our framework, relevance ordering produced by a statistical ranker, and information coverage with respect to topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for submodularity-based summarization methods. By experimenting on community QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity.", "histories": [["v1", "Fri, 17 Jun 2016 23:05:41 GMT  (43kb)", "http://arxiv.org/abs/1606.05702v1", "COLING 2014"]], "COMMENTS": "COLING 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "hema raghavan", "claire cardie", "vittorio castelli"], "accepted": false, "id": "1606.05702"}, "pdf": {"name": "1606.05702.pdf", "metadata": {"source": "CRF", "title": "Query-Focused Opinion Summarization for User-Generated Content", "authors": ["Lu Wang", "Hema Raghavan", "Claire Cardie Vittorio Castelli"], "emails": ["cardie}@cs.cornell.edu", "hraghavan@linkedin.com", "vittorio@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 702v 1 [cs.C L] 17 Jun 2016"}, {"heading": "1 Introduction", "text": "In fact, it is not that one sees oneself in a position to trump oneself, but that one sees oneself in a position to trump oneself and to trump oneself. (...) It is not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself. (...). \"\" \"It is not that one is in a position to trump oneself. (...) It is not that one is in a position to trump oneself.\" (...). \"(...).\" (. \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\" (.).). \"(.).\" (.). \"(.\" (.).). \"(.\" (.). \").\" (. \"(.\"). \"(.).\" (.). (. \"(.).\"). \").\" (. \"(.).\"). \"(.).\" (.).). \"(.).\" (. \"(.).\"). \").\" (.). \"().\"). \"("}, {"heading": "2 Related Work", "text": "Our work falls into the field of query-focused summaries, where a user asks a question and the system generates a summary of answers with relevant and varied information. A wide range of methods have been studied, often estimating their relevance by the similarity between TF and IDF (Carbonell and Goldstein, 1998), subject signature words (Lin and Hovy, 2000), or by learning a Bayesian model of queries and documents (Thumb \"and Marcu, 2006). Most papers punish only summary redundancy, e.g. by downgrading the meaning of words that are already selected.Encouraging diversity of a summary has recently been addressed by submodular functions applied to summarizing multiple documents in news wire (Lin and Bilmes, 2011; Sipos et al., 2012), and commentary marization (Dasgupta et al al al al al al al al al al al al al al al, 2013)."}, {"heading": "3 Submodular Opinion Summarization", "text": "In this section, we describe how query-focused opinion summary can be addressed by submodular functions combined with dispersion functions. First, we define our problem, then we present the components of our objective function (sections 3.1-3.3), and the full objective function is presented in Section 3.4. Finally, we describe a greedy algorithm with a constant factor approximation to the optimal solution for creating summaries (Section 3.5). A set of documents or answers to be summarized are first divided into a series of individual sentences V = {s1, \u00b7 sn}, and our problem is to select a subset of S V that maximizes a given objective function f: 2V \u2192 R within a length constraint frame: S = argmaxS Vf (S), subject to section size c | S | uler being the length of the summary S, and c being the length constraint."}, {"heading": "3.1 Relevance Function", "text": "We choose ListNet (Cao et al., 2007), which has proven effective in many information retrieval tasks, as our ranking. We use the implementation of Ranklib (Dang, 2011).Characteristics used in the ranking algorithm are summarized in Table 1. All characteristics are normalized by standardization. Due to the length limitation, we cannot provide the complete results when evaluating the characteristics. Nevertheless, we find that ranking candidates can achieve comparable results with the use of the full feature set (see Section 5) by using TFIDF similarity or key sentences that overlap with the query. We take the ranking order issued by the ranking and define the relevance of the current summary S as follows: r (S) = \u2211 | S | i \u221a rank \u2212 1i, where ranki is the rank of the sentence si in V. In the QA response ranking, sentences from the answer list have the same function as the ranking (S)."}, {"heading": "3.2 Coverage Functions", "text": "This function is designed to grasp the idea that a comprehensive opinion summary should yield thoughts on different aspects. (F = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A"}, {"heading": "3.3 Dispersion Function", "text": "Summaries should contain as little redundant information as possible. We achieve this by adding an additional term to the objective function (u, v) encoded by a dispersion function. In the face of a series of sentences S, a complete graph is constructed with each sentence in S. The weight of each edge (u, v) is its dissimilarity d (u, v). Then, the distance between any pair of u and v, d (u, v), is defined as the total weight of the shortest topic, the u and v.3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = dissimilation, d (u, v, v), u 6 = v d (u, v), and (2) hmin = minu, v (u, v).3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (Dasgupta et al, 2013): Dasgupta (al)."}, {"heading": "3.4 Full Objective Function", "text": "The objective function is based on the interpolation of submodular functions and dispersion functions: F (S) = r (S) + \u03b1t (S) + \u03b2a (S) + \u03b3p (S) + \u03b7c (S) + \u03b4h (S). (1) 2There is a lot of work on determining the polarity of a theorem (Pang and Lee, 2008), which can be used for polarity grouping in this theorem. We choose to focus on the summary and estimate the polarity of the theorem by mood word summing (Yu and Hatzivassiloglou, 2003), although we do not distinguish different sense words. 3This definition of distance is used to provide theoretical guarantees for the greedy algorithm described in Section 3.5. Coefficients \u03b1, \u03b2, \u03b3, \u0445, \u043c, \u03b4 are not negative real numbers and can be adjusted at a stage of development."}, {"heading": "3.5 Summary Generation via Greedy Algorithm", "text": "Creating a summary that maximizes our objective function in Equation 1 is difficult (Chandra and Halldo \u0301 rsson, 1996). We opt for a greedy algorithm that guarantees a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013). Specifically, starting with an empty sentence, we add a new sentence for each iteration, so that the current summary reaches the maximum value of the objective function. In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms generally work almost optimally."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Opinion Question Identification", "text": "We first build a classifier to automatically detect opinion-based questions in the community QA; questions in the blog dataset are all opinion-based; our opinion-based classifier is based on two opinion-based datasets: (1) the first from Li et al. (2008a) contains 646 opinion-based and 332 objective questions; (2) the second dataset from Amiri et al. (2013) consists of 317 implicit opinion questions, such as \"What can you do to help the environment?,\" and 317 objective questions. We train an RBF kernel-based SVM classifier to identify opinion surveys that score on the two datasets F1 values of 0.79 and 0.80 when evaluating with 10-fold cross-validation (the best reported F1 values are 0.75 and 0.79)."}, {"heading": "4.2 Datasets", "text": "Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo! WebscopeTM program, 5 which contains 3,895,407 questions. We first run the opinion question classifier to identify the opinion questions. For summarization purpose, we need each question have at least 5 answers, with the average length of answers than 20 words. As a result, we have 130,609 questions in the training set for learning the statistical ranker, and 38,500 in the test set. The category distribution of training and test questions (Yahoo! Answers organizes the questions into pre-defined categories) are similar. 10,000 questions from the training set are more reserved than the development set. Every question in Yahoo! Answers dataset has a user-voted best answer."}, {"heading": "4.3 Comparisons", "text": "For both opinion summaries, we compare (1) the approach of Dasgupta et al. (2013) and (2) the systems of Lin and Bilmes (2011) with and without query information. The process of clustering sentences in Lin and Bilmes (2011) is done with CLUTO (Karypis, 2003). For implementing systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters that are reported to perform best in their work. For cQA summaries, we use the best response chosen by the user as a baseline. Note that this is a strong baseline, as all other systems do not know which answer is the best. For the blog summary, we have three additional baselines - the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Evaluating the Ranker", "text": "We evaluate our ranking (described in Section 3.1) based on the task of predicting the best answer. Table 2 compares the average precision and mean reciprocal rank (MRR) of our method with those of three baselines, (1) where the answers are randomly ranked (baseline (random)), (2) by length (baseline (length)), and (3) by Jensen Shannon Divergence (JSD) with all answers. We expect the best answer to be the one that covers the most information likely to have a smaller JSD. Therefore, we use JSD to rank the answers in ascending order. Table 2 shows that our ranking outperforms all other methods."}, {"heading": "5.2 Community QA Summarization", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.3 Blog Summarization", "text": "Automatic Assessment. We use ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries based on the human-flagged nuggets available for this task. ROUGE-2 measures the overlap of bigrams and ROUGE-SU4 measures the overlap of unigrams and skip bigrams separated by up to four words. We use the marker trained on Yahoo! data to generate relevance sequences and adopt the system parameters from Section 5.2. Table 5 (left) shows that our system outperforms the best system in TAC '08 with the highest ROUGE 2 score (Kim et al., 2008), the two baseline lines (TFIDF + Lexicon and our Ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013)."}, {"heading": "5.4 Further Discussion", "text": "Considering that metrics of text similarity and propagation functions within the framework play an important role, we continue to investigate the effectiveness of different content coverage functions (Cosine with TFIDF vs. Semantic), propagation functions (hsum vs. hmin), and dissimilarity metrics used in propagation functions (Semantic vs. Topical vs. Lexical). Results on Yahoo! Answer (Table 6 (left) show that systems that use the sum of distances for propagation functions (hsum) consistently outperform systems with minimal distance (hmin). Meanwhile, Cosine with TFIDF is better at measuring content than WordNetbased semantic measurement, and this may be due to the limited coverage of WordNet on verbs. This is also true for deviation metrics. However, results on blog data (Table 6 (right) show that the use of minimum distance from propagation provides better results after optimal propagation."}, {"heading": "6 Conclusion", "text": "We propose a sub-modular, function-based framework for summarising opinions. Tested on the basis of community QA and blog summaries, our approach exceeds state-of-the-art methods, which are also based on submodularity in both automatic assessment and human evaluation. Our framework is capable of incorporating statistically learned sentence relevance and encouraging the summary to cover various topics. We also examine various metrics to estimate text similarity and its impact on the summary."}], "references": [{"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."], "venue": "Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385\u2013393, Montr\u00e9al, Canada, 7-8 June. Association for Computational Linguistics.", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "A pattern matching based model for implicit opinion question identification", "author": ["Hadi Amiri", "Zheng-Jun Zha", "Tat-Seng Chua."], "venue": "AAAI. AAAI Press.", "citeRegEx": "Amiri et al\\.,? 2013", "shortCiteRegEx": "Amiri et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "J. Mach. Learn. Res., 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li."], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 129\u2013136, New York, NY, USA. ACM.", "citeRegEx": "Cao et al\\.,? 2007", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein."], "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201998, pages 335\u2013336, New York, NY, USA. ACM.", "citeRegEx": "Carbonell and Goldstein.,? 1998", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "Lda based similarity modeling for question answering", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur", "Gokhan Tur."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS \u201910, pages 1\u20139, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Celikyilmaz et al\\.,? 2010", "shortCiteRegEx": "Celikyilmaz et al\\.", "year": 2010}, {"title": "Facility dispersion and remote subgraphs", "author": ["Barun Chandra", "Magn\u00fas M. Halld\u00f3rsson."], "venue": "Proceedings of the 5th Scandinavian Workshop on Algorithm Theory, SWAT \u201996, pages 53\u201365, London, UK, UK. SpringerVerlag.", "citeRegEx": "Chandra and Halld\u00f3rsson.,? 1996", "shortCiteRegEx": "Chandra and Halld\u00f3rsson.", "year": 1996}, {"title": "Overview of the tac 2008 opinion question answering and summarization tasks", "author": ["Hoa Tran Dang."], "venue": "Proc. TAC 2008.", "citeRegEx": "Dang.,? 2008", "shortCiteRegEx": "Dang.", "year": 2008}, {"title": "RankLib", "author": ["Van Dang."], "venue": "http://www.cs.umass.edu/ \u0303vdang/ranklib.html.", "citeRegEx": "Dang.,? 2011", "shortCiteRegEx": "Dang.", "year": 2011}, {"title": "Summarization through submodularity and dispersion", "author": ["Anirban Dasgupta", "Ravi Kumar", "Sujith Ravi."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1014\u20131022, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Dasgupta et al\\.,? 2013", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2013}, {"title": "Bayesian query-focused summarization", "author": ["III Hal Daum\u00e9", "Daniel Marcu."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 305\u2013312, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Daum\u00e9 and Marcu.,? 2006", "shortCiteRegEx": "Daum\u00e9 and Marcu.", "year": 2006}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, pages 168\u2013177, New York, NY, USA. ACM.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "CLUTO - a clustering toolkit", "author": ["George Karypis."], "venue": "Technical Report #02-017, November.", "citeRegEx": "Karypis.,? 2003", "shortCiteRegEx": "Karypis.", "year": 2003}, {"title": "Opinion summarization using entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarization pilot", "author": ["Hyun Duk Kim", "Dae Hoon Park", "V.G.Vinod Vydiswaran", "ChengXiang Zhai."], "venue": "Proc. TAC 2008.", "citeRegEx": "Kim et al\\.,? 2008", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "The measurement of observer agreement for categorical data", "author": ["J R Landis", "G G Koch."], "venue": "Biometrics, 33(1):159\u2013174.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Sentiment summarization: Evaluating and learning user preferences", "author": ["Kevin Lerman", "Sasha Blair-Goldensohn", "Ryan McDonald."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL \u201909, pages 514\u2013522, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lerman et al\\.,? 2009", "shortCiteRegEx": "Lerman et al\\.", "year": 2009}, {"title": "Cocqa: Co-training over questions and answers with an application to predicting question subjectivity orientation", "author": ["Baoli Li", "Yandong Liu", "Eugene Agichtein."], "venue": "EMNLP, pages 937\u2013946.", "citeRegEx": "Li et al\\.,? 2008a", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Polyu at tac 2008", "author": ["Wenjie Li", "You Ouyang", "Yi Hu", "Furu Wei."], "venue": "Proc. TAC 2008.", "citeRegEx": "Li et al\\.,? 2008b", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "A class of submodular functions for document summarization", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies Volume 1, HLT \u201911, pages 510\u2013520, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lin and Bilmes.,? 2011", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "The automated acquisition of topic signatures for text summarization", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "COLING \u201900, pages 495\u2013501, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lin and Hovy.,? 2000", "shortCiteRegEx": "Lin and Hovy.", "year": 2000}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 71\u201378.", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Understanding and summarizing answers in community-based question answering services", "author": ["Yuanjie Liu", "Shasha Li", "Yunbo Cao", "Chin-Yew Lin", "Dingyi Han", "Yong Yu."], "venue": "Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING \u201908, pages 497\u2013504, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2008", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Automatically assessing machine summary content without a gold standard", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Comput. Linguist., 39(2):267\u2013300, June.", "citeRegEx": "Louis and Nenkova.,? 2013", "shortCiteRegEx": "Louis and Nenkova.", "year": 2013}, {"title": "Finding what matters in questions", "author": ["Xiaoqiang Luo", "Hema Raghavan", "Vittorio Castelli", "Sameer Maskey", "Radu Florian."], "venue": "HLT-NAACL, pages 878\u2013887.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald."], "venue": "ECIR\u201907, pages 557\u2013564, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "McDonald.,? 2007", "shortCiteRegEx": "McDonald.", "year": 2007}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1, AAAI\u201906, pages 775\u2013780. AAAI Press.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "An analysis of approximations for maximizing submodular set functionsI", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher."], "venue": "Mathematical Programming, 14(1):265\u2013294, December.", "citeRegEx": "Nemhauser et al\\.,? 1978", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "The impact of frequency on summarization", "author": ["Ani Nenkova", "Lucy Vanderwende."], "venue": "Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005-101.", "citeRegEx": "Nenkova and Vanderwende.,? 2005", "shortCiteRegEx": "Nenkova and Vanderwende.", "year": 2005}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Found. Trends Inf. Retr., 2(1-2):1\u2013135, January.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Summarizing contrastive viewpoints in opinionated text", "author": ["Michael J. Paul", "ChengXiang Zhai", "Roxana Girju."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 66\u201376, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Paul et al\\.,? 2010", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "Large-margin learning of submodular summarization models", "author": ["Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims."], "venue": "EACL \u201912, pages 224\u2013233, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Sipos et al\\.,? 2012", "shortCiteRegEx": "Sipos et al\\.", "year": 2012}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Philip J. Stone", "Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie."], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Stone et al\\.,? 1966", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Partially supervised coreference resolution for opinion summarization through structured rule learning", "author": ["Veselin Stoyanov", "Claire Cardie."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201906, pages 336\u2013344, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Stoyanov and Cardie.,? 2006", "shortCiteRegEx": "Stoyanov and Cardie.", "year": 2006}, {"title": "Metadata-aware measures for answer summarization in community question answering", "author": ["Mattia Tomasoni", "Minlie Huang."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 760\u2013769, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Tomasoni and Huang.,? 2010", "shortCiteRegEx": "Tomasoni and Huang.", "year": 2010}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT \u201905, pages 347\u2013354, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Integrating document clustering and topic modeling", "author": ["Pengtao Xie", "Eric Xing."], "venue": "Proceedings of the Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694\u2013 703, Corvallis, Oregon. AUAI Press.", "citeRegEx": "Xie and Xing.,? 2013", "shortCiteRegEx": "Xie and Xing.", "year": 2013}, {"title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences", "author": ["Hong Yu", "Vasileios Hatzivassiloglou."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Yu and Hatzivassiloglou.,? 2003", "shortCiteRegEx": "Yu and Hatzivassiloglou.", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "Social media forums, such as social networks, blogs, newsgroups, and community question answering (QA), offer avenues for people to express their opinions as well collect other people\u2019s thoughts on topics as diverse as health, politics and software (Liu et al., 2008).", "startOffset": 249, "endOffset": 267}, {"referenceID": 12, "context": "Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 97, "endOffset": 136}, {"referenceID": 16, "context": "Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 97, "endOffset": 136}, {"referenceID": 33, "context": ", 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 17, "endOffset": 44}, {"referenceID": 19, "context": "(2) Within our summarization framework, the statistically learned sentence relevance is included as part of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011) only uses ngram overlap for query relevance.", "startOffset": 182, "endOffset": 204}, {"referenceID": 2, "context": "Additionally, we use Latent Dirichlet Allocation (Blei et al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned topics.", "startOffset": 49, "endOffset": 68}, {"referenceID": 18, "context": "We also obtain significant higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011). (2) Within our summarization framework, the statistically learned sentence relevance is included as part of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011) only uses ngram overlap for query relevance.", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 109, "endOffset": 140}, {"referenceID": 20, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 164, "endOffset": 184}, {"referenceID": 10, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 244, "endOffset": 267}, {"referenceID": 19, "context": "Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al.", "startOffset": 162, "endOffset": 204}, {"referenceID": 31, "context": "Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al.", "startOffset": 162, "endOffset": 204}, {"referenceID": 9, "context": ", 2012), and comments summarization (Dasgupta et al., 2013).", "startOffset": 36, "endOffset": 59}, {"referenceID": 12, "context": "Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 79, "endOffset": 118}, {"referenceID": 16, "context": "Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 79, "endOffset": 118}, {"referenceID": 33, "context": ", 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 30, "context": ", 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA.", "startOffset": 110, "endOffset": 1381}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences.", "startOffset": 110, "endOffset": 1573}, {"referenceID": 28, "context": "2) - length of the answer/sentence - if contains sentiment words with the same polarity as - length is less than 5 words sentiment words in query Query-Sentence Overlap Features Query-Independent Features - unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid - number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005) sentence.", "startOffset": 382, "endOffset": 413}, {"referenceID": 20, "context": "A model similar to that described in - number of topic signature words (Lin and Hovy, 2000) (Luo et al.", "startOffset": 71, "endOffset": 91}, {"referenceID": 24, "context": "A model similar to that described in - number of topic signature words (Lin and Hovy, 2000) (Luo et al., 2013) was applied to detect key phrases.", "startOffset": 92, "endOffset": 110}, {"referenceID": 9, "context": "Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded in well-designed dispersion functions which still maintain a constant factor approximation when solved by a greedy algorithm.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval tasks, as our ranker.", "startOffset": 18, "endOffset": 36}, {"referenceID": 8, "context": "We use the implementation from Ranklib (Dang, 2011).", "startOffset": 39, "endOffset": 51}, {"referenceID": 2, "context": "Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document collections, and thus afford a natural way to cluster texts according to their topics.", "startOffset": 55, "endOffset": 74}, {"referenceID": 36, "context": "Recent work (Xie and Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering.", "startOffset": 12, "endOffset": 32}, {"referenceID": 36, "context": "This naive approach produces comparable clustering performance to the state-of-the-art according to (Xie and Xing, 2013).", "startOffset": 100, "endOffset": 120}, {"referenceID": 35, "context": "Our lexicon consists of MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 32, "context": ", 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006).", "startOffset": 26, "endOffset": 46}, {"referenceID": 11, "context": ", 1966), and SentiWordNet (Esuli and Sebastiani, 2006).", "startOffset": 26, "endOffset": 54}, {"referenceID": 10, "context": ", 1966), and SentiWordNet (Esuli and Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed. Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al.", "startOffset": 27, "endOffset": 180}, {"referenceID": 9, "context": "Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following function to measure content coverage of the current summary S: c(S) = \u2211", "startOffset": 39, "endOffset": 62}, {"referenceID": 9, "context": "The other is a WordNet-based semantic similarity score between pairwise dependency relations from two sentences (Dasgupta et al., 2013).", "startOffset": 112, "endOffset": 135}, {"referenceID": 9, "context": "3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = \u2211", "startOffset": 54, "endOffset": 77}, {"referenceID": 26, "context": "There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012).", "startOffset": 72, "endOffset": 116}, {"referenceID": 0, "context": "There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012).", "startOffset": 72, "endOffset": 116}, {"referenceID": 0, "context": ", 2006; Agirre et al., 2012). In this work, we experiment with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtfidf (u, v) be the Cosine similarity between u and v, then we have dLex(u, v) = 1\u2212 simtfidf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. dSem(u, v) = 1 \u2212 simSem(v, u), where simSem(v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz et al. (2010) show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance.", "startOffset": 8, "endOffset": 672}, {"referenceID": 29, "context": "There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed for polarity clustering in this work.", "startOffset": 78, "endOffset": 98}, {"referenceID": 37, "context": "We decide to focus on summarization, and estimate sentence polarity through sentiment word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.", "startOffset": 101, "endOffset": 132}, {"referenceID": 6, "context": "Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and Halld\u00f3rsson, 1996).", "startOffset": 86, "endOffset": 117}, {"referenceID": 27, "context": "We choose to use a greedy algorithm that guarantees to obtain a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013).", "startOffset": 118, "endOffset": 165}, {"referenceID": 9, "context": "We choose to use a greedy algorithm that guarantees to obtain a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013).", "startOffset": 118, "endOffset": 165}, {"referenceID": 25, "context": "In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.", "startOffset": 56, "endOffset": 72}, {"referenceID": 16, "context": "Our opinion question classifier is trained on two opinion question datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al.", "startOffset": 97, "endOffset": 115}, {"referenceID": 1, "context": "(2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as \u201cWhat can you do to help environment?\u201d, and 317 objective questions.", "startOffset": 92, "endOffset": 112}, {"referenceID": 7, "context": "We use the TAC 2008 corpus (Dang, 2008), which consists of 25 topics.", "startOffset": 27, "endOffset": 39}, {"referenceID": 13, "context": "The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003).", "startOffset": 80, "endOffset": 95}, {"referenceID": 14, "context": "For blog summarization, we have three additional baselines \u2013 the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon", "startOffset": 90, "endOffset": 126}, {"referenceID": 18, "context": "For blog summarization, we have three additional baselines \u2013 the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon", "startOffset": 90, "endOffset": 126}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information.", "startOffset": 74, "endOffset": 97}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information.", "startOffset": 74, "endOffset": 145}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003).", "startOffset": 74, "endOffset": 238}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of systems in Lin and Bilmes (2011) and Dasgupta et al.", "startOffset": 74, "endOffset": 337}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to have the best performance in their work.", "startOffset": 74, "endOffset": 364}, {"referenceID": 21, "context": "Louis and Nenkova (2013) report that JSD has a strong negative correlation (Spearman correlation = \u22120.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Meanwhile, both our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and Bilmes (2011) system, which implies the effectiveness of the dispersion function.", "startOffset": 31, "endOffset": 54}, {"referenceID": 9, "context": "Meanwhile, both our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and Bilmes (2011) system, which implies the effectiveness of the dispersion function.", "startOffset": 31, "endOffset": 131}, {"referenceID": 18, "context": "3858 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 27}, {"referenceID": 18, "context": "3858 Lin and Bilmes (2011) 0.3398 0.2008 Lin and Bilmes (2011) + q 0.", "startOffset": 5, "endOffset": 63}, {"referenceID": 9, "context": "1988 Dasgupta et al. (2013) 0.", "startOffset": 5, "endOffset": 28}, {"referenceID": 16, "context": "Here we believe that ranking the summaries is easier than evaluating each summary in isolation (Lerman et al., 2009).", "startOffset": 95, "endOffset": 116}, {"referenceID": 9, "context": "Four system summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are displayed along with one noisy summary (i.", "startOffset": 36, "endOffset": 59}, {"referenceID": 15, "context": "28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and \u03ba is 0.", "startOffset": 19, "endOffset": 42}, {"referenceID": 9, "context": "5% times while that of Dasgupta et al. (2013) is chosen 11.", "startOffset": 23, "endOffset": 46}, {"referenceID": 9, "context": "5% times while that of Dasgupta et al. (2013) is chosen 11.0% of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Dasgupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by using Wilcoxon signed-rank test (p < 0.", "startOffset": 23, "endOffset": 288}, {"referenceID": 9, "context": "29 Dasgupta et al. (2013) 100 11.", "startOffset": 3, "endOffset": 26}, {"referenceID": 9, "context": "29 Dasgupta et al. (2013) 100 11.0% 2.84 2.83 5.0% 2.95 2.94 Our system 12.5% 2.50\u2217 2.50\u2217 6.7% 2.43\u2217 2.43\u2217 Our system 200 44.6% 1.98\u2217 1.98\u2217 78.7% 1.35\u2217 1.34\u2217 Table 4: Human evaluation on Yahoo! Answer Data. Boldface implies statistically significance compared to other results in the same columns using paired-t test. Both of our systems are ranked higher (i.e. numbers in bold with ) than the best answers voted by Yahoo! users and system summaries from Dasgupta et al. (2013).", "startOffset": 3, "endOffset": 478}, {"referenceID": 9, "context": "Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words).", "startOffset": 32, "endOffset": 55}, {"referenceID": 21, "context": "We use the ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task.", "startOffset": 17, "endOffset": 37}, {"referenceID": 14, "context": "Table 5 (left) shows that our system outperforms the best system in TAC\u201908 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 13, "context": "Table 5 (left) shows that our system outperforms the best system in TAC\u201908 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al.", "startOffset": 103, "endOffset": 195}, {"referenceID": 9, "context": ", 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).", "startOffset": 87, "endOffset": 110}, {"referenceID": 18, "context": "2293 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 27}, {"referenceID": 18, "context": "2293 Lin and Bilmes (2011) 0.2732 0.3582 0.2330 Lin and Bilmes (2011) + q 0.", "startOffset": 5, "endOffset": 70}, {"referenceID": 9, "context": "2349 Dasgupta et al. (2013) 0.", "startOffset": 5, "endOffset": 28}, {"referenceID": 9, "context": "2349 Dasgupta et al. (2013) 0.2618 0.3500 0.2370 Our system 0.3234 0.3978 0.2258 Pyramid F-score Best system in TAC\u201908 0.2225 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 148}, {"referenceID": 7, "context": "For human evaluation, we use the standard Pyramid F-score used in the TAC\u201908 opinion summarization track with \u03b2 = 3 (Dang, 2008).", "startOffset": 116, "endOffset": 128}, {"referenceID": 7, "context": "For human evaluation, we use the standard Pyramid F-score used in the TAC\u201908 opinion summarization track with \u03b2 = 3 (Dang, 2008). In the TAC task, systems are allowed to return up to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with the one that got the highest Pyramid F-score in the TAC\u201908 and Lin and Bilmes (2011). Cohen\u2019s \u03ba for inter-annotator agreement is 0.", "startOffset": 117, "endOffset": 466}], "year": 2016, "abstractText": "We present a submodular function-based framework for query-focused opinion summarization. Within our framework, relevance ordering produced by a statistical ranker, and information coverage with respect to topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for submodularity-based summarization methods. By experimenting on community QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}