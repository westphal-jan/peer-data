{"id": "1606.05735", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "A Comparative Analysis of classification data mining techniques : Deriving key factors useful for predicting students performance", "abstract": "Students opting engineering as their disciple is increasing rapidly. But due to various factors and inappropriate primary education in India dropout rates are high. Students are unable to excel in core engineering subjects which are complex and mathematical, hence mostly get drop / keep term (kt) in that subject. With the help of data mining techniques we can predict the performance of students in terms of grades and dropout for a subject. This paper compares various techniques such as na\\\"ive Bayes, LibSVM, J48, random forest, and JRip and try to choose one of them as per our needs and their accuracy. Based on the rules obtained from this technique(s), we derive the key factors influencing student performance.", "histories": [["v1", "Sat, 18 Jun 2016 10:06:44 GMT  (531kb)", "http://arxiv.org/abs/1606.05735v1", "6 pages, 5 tables, 2 figures"], ["v2", "Fri, 11 Nov 2016 14:47:28 GMT  (402kb)", "http://arxiv.org/abs/1606.05735v2", "6 pages, 6 tables, 2 figures"]], "COMMENTS": "6 pages, 5 tables, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CY", "authors": ["muhammed salman shamsi", "jhansi lakshmi"], "accepted": false, "id": "1606.05735"}, "pdf": {"name": "1606.05735.pdf", "metadata": {"source": "CRF", "title": "Student performance prediction using classification data mining techniques Analysis of Student data to find factors influencing student performance", "authors": ["Muhammed Salman Shamsi", "Jhansi Lakshmi"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, \"he said in an interview with the\" New York Times. \""}], "references": [{"title": "Educational data mining: A systematic review of the published literature 2006- 2013", "author": ["M. Al-Razgan", "A.S. Al-Khalifa", "H.S. Al-Khalifa"], "venue": "Proc. the 1st International Conference on Advanced Data and Information Engineering, 2013, pp. 711-719.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Data Mining in Education: Data Classification and Decision Tree Approach\u201d, in the Journal of e-Education, e-Business", "author": ["Sonali Agarwal", "G.N. Pandey", "M.D. Tiwari"], "venue": "e-Management and e-Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "New Dropout Prediction for Intelligent System\u201d, in the International Journal of Computer Applications", "author": ["Md.Sarwar kamal", "Linkon Chowdhury", "Sonia Farhana Nimmy"], "venue": "Volume 42\u2013", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Predicting School Failure and Dropout by Using Data Mining Techniques", "author": ["Carlos M\u00e1rquez-Vera", "Crist\u00f3bal Romero Morales", "Sebasti\u00e1n Ventura Soto"], "venue": "in the IEEE Journal Of Latin-American Learning Technologies,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Predicting Students Final GPA Using Decision Trees: A Case Study", "author": ["Mashael A. Al-Barrak", "Muna Al-Razgan"], "venue": "in the International Journal of Information and Education Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Data Mining: A prediction for performance improvement using classification", "author": ["Brijesh Kumar Bhardwaj", "Saurabh Pal"], "venue": "in the (IJCSIS) International Journal of Computer Science and Information Security,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Mining Student Data Using Decision Trees", "author": ["Qasem A. Al-Radaideh", "Emad M. Al-Shawakfa", "Mustafa I. Al-Najjar"], "venue": "in the International Arab Conference on Information Technology (ACIT\u20192006),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "A Decision Tree Algorithm Pertaining to the Student Performance Analysis and Prediction", "author": ["Mrinal Pandey", "Vivek Kumar Sharma"], "venue": "in the International Journal of Computer Applications", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "El-Halees, \u201cMining Educational Data to Improve Students\u2019 Performance: A Case Study", "author": ["Mohammed M. Abu Tair", "Alaa M"], "venue": "in the International Journal of Information and Communication Technology Research, Volume 2 No", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Data mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "2nd ed, Morgan-Kaufman Series of Data Management Systems San Francisco Elsevier, 2005, ISBN 978- 0120884070.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Educational data mining (EDM) deals with the application of data mining tools and techniques to inspect the data at educational institutions for deriving knowledge [1].", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "In clustering the data objects are combined into set of objects known as groups or clusters [12], [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "El-Halees [14] in their case study discussed various EDM techniques to improve students\u2019 performance.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Agarwal [3], et al.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "Kamal [4], et al.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "Carlos M\u00e1rquez-Vera [5], et al.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "Al-Barrak [6], et al.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Brijesh and Pal [7], found out using Bayesian classification that student SSC (metric) grade, living location, medium of instruction, mother qualification, student habits and type of family are the most important factors for the student performance.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "Al-Radaideh [10], et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "Mrinal Pandey and Vivek Kumar Sharma [11], compared classification methods to predict grades for undergraduate engineering students.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 1, "endOffset": 4}, {"referenceID": 2, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "A[1] B [0] C [4] D [5] E [6] F [7] G [8] H [9] I[ 10 ]", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "Na\u00efve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "Na\u00efve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Na\u00efve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Na\u00efve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Na\u00efve Bayes A [-1] 102 0 2 1 0 5 2 1 2 B [0] 4 299 21 4 2 3 0 2 0 C [4] 5 40 239 59 77 30 2 4 2 D [5] 0 7 72 23 37 22 0 3 3 E [6] 1 4 69 27 61 44 3 1 3 F [7] 6 1 23 12 27 43 5 2 0 G [8] 1 1 2 4 6 15 4 2 0 H [9] 2 0 1 2 5 6 2 0 1 I [10] 2 0 0 2 1 5 1 1 3", "startOffset": 231, "endOffset": 235}, {"referenceID": 2, "context": "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "LibSV M A [-1] 17 4 94 0 0 0 0 0 0 B [0] 0 322 13 0 0 0 0 0 0 C [4] 0 15 443 0 0 0 0 0 0 D [5] 0 3 164 0 0 0 0 0 0 E [6] 0 3 210 0 0 0 0 0 0 F [7] 0 4 115 0 0 0 0 0 0 G [8] 0 3 32 0 0 0 0 0 0 H [9] 0 1 18 0 0 0 0 0 0 I [10] 0 0 15 0 0 0 0 0 0", "startOffset": 219, "endOffset": 223}, {"referenceID": 2, "context": "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "JRip A [-1] 112 0 3 0 0 0 0 0 0 B [0] 0 328 7 0 0 0 0 0 0 C [4] 2 19 433 2 2 0 0 0 0 D [5] 0 3 162 0 1 0 0 0 1 E [6] 0 3 206 0 0 0 1 0 1 F [7] 4 2 107 1 0 3 1 0 1", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "G [8] 0 3 30 0 0 1 1 0 0 H [9] 2 0 15 0 0 0 2 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 140, "endOffset": 143}, {"referenceID": 6, "context": "J48 A [-1] 104 1 10 0 0 0 0 0 0 B [0] 2 318 13 1 1 0 0 0 0 C [4] 2 17 439 0 0 0 0 0 0 D [5] 0 1 164 0 2 0 0 0 0 E [6] 2 2 207 2 0 0 0 0 0 F [7] 3 0 112 0 0 4 0 0 0 G [8] 0 0 32 1 0 0 0 2 0 H [9] 1 0 17 0 0 0 1 0 0 I [10] 2 0 13 0 0 0 0 0 0", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Rando m Forest A [-1] 109 0 3 0 2 1 0 0 0 B [0] 0 311 22 0 1 1 0 0 0 C [4] 4 39 328 25 51 11 0 0 0 D [5] 0 9 109 14 25 6 1 1 2 E [6] 3 6 127 18 35 15 4 2 3 F [7] 6 2 44 10 34 19 3 0 1 G [8] 0 1 9 2 8 7 2 5 1 H [9] 3 0 3 1 6 4 2 0 0 I [10] 2 0 1 4 3 5 0 0 0", "startOffset": 234, "endOffset": 238}], "year": 2016, "abstractText": "Students opting engineering as their disciple is increasing rapidly. But due to various factors and inappropriate primary education in India dropout rates are high. Students are unable to excel in core engineering subjects which are complex and mathematical, hence mostly get drop / keep term (kt) in that subject. With the help of data mining techniques we can predict the performance of students in terms of grades and dropout for a subject. This paper compares various techniques such as na\u00efve Bayes, LibSVM, J48, random forest, and JRip and try to choose one of them as per our needs and their accuracy. Based on the rules obtained from this technique(s), we derive the key factors influencing student performance. Keywords\u2014 dropout; prediction; classification; data mining; education.", "creator": "Microsoft\u00ae Word 2013"}}}