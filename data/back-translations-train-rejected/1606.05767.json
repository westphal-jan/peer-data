{"id": "1606.05767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "On Reward Function for Survival", "abstract": "Obtaining a survival strategy (policy) is one of the fundamental problems of biological agents. In this paper, we generalize the formulation of previous research related to the survival of an agent and we formulate the survival problem as a maximization of the multi-step survival probability in future time steps. We introduce a method for converting the maximization of multi-step survival probability into a classical reinforcement learning problem. Using this conversion, the reward function (negative temporal cost function) is expressed as the log of the temporal survival probability. And we show that the objective function of the reinforcement learning in this sense is proportional to the variational lower bound of the original problem. Finally, We empirically demonstrate that the agent learns survival behavior by using the reward function introduced in this paper.", "histories": [["v1", "Sat, 18 Jun 2016 15:33:04 GMT  (693kb,D)", "https://arxiv.org/abs/1606.05767v1", "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems"], ["v2", "Sun, 24 Jul 2016 13:19:23 GMT  (693kb,D)", "http://arxiv.org/abs/1606.05767v2", "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems"]], "COMMENTS": "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["naoto yoshida"], "accepted": false, "id": "1606.05767"}, "pdf": {"name": "1606.05767.pdf", "metadata": {"source": "CRF", "title": "On Reward Function for Survival", "authors": ["Naoto Yoshida"], "emails": ["naotoyoshida@pfsl.mech.thoku.ac.jp"], "sections": [{"heading": "I. PRELIMINARIES", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to change the world, and that we are able to change the world in order to change it."}, {"heading": "A. Objective Functions of Reinforcement Learning Problem", "text": "We will first present the general form of objective function in the POMDP model (partially observable Markov decision-making process). For the sake of simplicity, we will limit the discussion to a finite series of actions, states and observations. Many realistic environments for the agent are known to be modelled by the partially observable Markov decision-making process (POMDP) [15]. The POMDP model consists of the state set S, plot set A, observation set O, transition probability to state probability S and an action A as P (s). The observation probability P (o | s), and the reward function r (s, a).ar Xiv: 160 6.05 767v 2 [cs.A I] 2 4Ju l 201."}, {"heading": "II. SURVIVAL PROBLEM", "text": "In this section, we formulate the problem of survival on the basis of the animal models proposed by Ashby [1] and a similar idea proposed by Sibly and McFarland [9] and McFarland and Buuser [6] from an ethological point of view. In their model, an animal has several variables that are observed by the animal and have some significance for the maintenance of life (e.g. the water level and energy level in the body, as shown in Figure 1). We call these variables the \"physiological state.\" On the other hand, an agent also has a \"state of perception,\" which is the perception of environmental stimuli (vision, touch, etc.), whereby the combined physiological state and the state of perception that can be represented in the animal's brain is the physiological state."}, {"heading": "A. Formulation of the Survival Problem", "text": "The assumptions of the problem are as shown in Figure 2. Similar settings have been suggested elsewhere [1], [11], [17] - [19]. An agent unit consists of an RL agent and a body, and the RL agent interacts with the world through the body. As the sensors may not perfectly determine the current situation of the world and the physiological state of the body, the sensor may only have partial observability. Since the perception of the physiological state is a process within the body, we can assume that there is no loss of information with this feeling. We are now formalizing the survival problem as an optimization problem. We largely follow the usual definition of dynamics in the POMDP model explained in the previous section. Like the POMDP model, the agent interacts with the environment. In the state that the agent receives the current observation, the probability is P (o | s)."}, {"heading": "B. Maximization of Survival Probability by Variational Method", "text": "In the following discussion, we will show that maximizing the objective function (1) by maximizing the limit of variation Q (Q) on the maximizing the conventional objective function of RL. First, we will discuss the situation of the planning problem in which we are looking for the optimal strategy by introducing the arbitrary probability distribution P (s \"| s, a), the observation probability P (o\" s), and the time survival probability P (A \"p\" s). The logarithm of the objective function (1) can be transformed by introducing the arbitrary probability distribution Q (s \"s\" s \"s, a\" p \"s\") on the T-step-step-step-step-step-step-T."}, {"heading": "C. Solving the Survival Problem by Reinforcement Learning Algorithms", "text": "Now we consider the survival problem in the learning environment of amplification, that is, the maximization of the protocol of objective functional logP (A-T | \u03c0), while the agent cannot access the true environmental model P (o-s), P (s | s, a). In this setting, we cannot perform the iterative algorithms described above. However, from the discussion of the second algorithm (Eq.2, 3) it follows that the limit of variation after each M step is proportional to JT (\u03c0\u03b8). Then, to maximize the limit of variation, we can take a direct maximization of JT (\u03c0\u03b8) in relation to \u03b8, rather than the exact execution of the iterative algorithm. Since JT with reward function = logP (At + 1 = 1 | st) is the conventional objective function of the learning paradigm of amplification, we can apply the RL algorithms to the maximization of survival."}, {"heading": "III. EXPERIMENT", "text": "In the experiment, we check the reward setting by selecting the survival probability of the object in the simple grid world = A = 5. The environment consists of a 3 x 3 grid world (Figure 3). The agent selects an action in each time step of UP, DOWN, RIGHT, LEFT and EAT. If the agent takes the action UP, DOWN, RIGHT or LEFT and if the wall is not in this direction, the agent moves one step in the selected direction. Otherwise, the agent remains in the current position. In the environment, there are two types of objects, A and B, in a uniformly random position, so that the two objects never overlap. The position of the objects changes when the agent selects the EAT action in the appropriate position. Also, the position of B can change in each time step with a probability of 01,000 The agent has a continuous battery setting, which decreases by 1%."}, {"heading": "IV. DISCUSSION", "text": "In our approach, we have introduced the first \"basic\" reward function of the RL for the general survival problem, which has only been heuristically defined in previous studies to date. The key is to weaken the definition of the viability zone with the temporal survival probability, so that the reward function is simply the protocol of the temporal survival probability. Using this setting, agents can learn the survival probability in terms of maximizing the survival probability in the future. However, the source of the reward function, the temporal survival probability, has an explicit meaning and can be obtained through the evolutionary process. However, although our reward setting is fundamental to survival, it may not be the \"optimal reward\" in terms of learning efficiency for the survival probability of politics. It is known that there are reward settings that have the same optimal policy, but a different learning speed for the RL agent [26] applied to the state recently."}, {"heading": "V. CONCLUSION", "text": "We have discussed the survival problem of the pathogen in the environment and demonstrated that the survival problem can be reduced to a RL problem in (PO) MDPs with a specific reward function. Due to the popularity of (PO) MDP assumptions in research on autonomous pathogens, especially in animal models, this formulation can be regarded as the basis of a truly autonomous pathogen for survival."}, {"heading": "ACKNOWLEDGMENT", "text": "I would like to thank Makoto Otsuka and Stephany Nix for their helpful comments to improve the quality of this essay."}], "references": [{"title": "Design for a Brain", "author": ["W.R. Ashby"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1960}, {"title": "The living brain", "author": ["W. Walter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1953}, {"title": "The design of a fungus-eater: A model of human behavior in an unsophisticated environment", "author": ["M. Toda"], "venue": "Behavioral Science, vol. 7, no. 2, pp. 164\u2013183, 1962.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1962}, {"title": "robot, and society: Models and speculations", "author": ["Man"], "venue": "M. Nijhoff Pub.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1982}, {"title": "Basic cycles, utility and opportunism in self-sufficient robots", "author": ["D. McFarland", "E. Spier"], "venue": "Robotics and Autonomous Systems, vol. 20, no. 2, pp. 179\u2013190, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L.-J. Lin"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 293\u2013321, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "On the fitness of behavior sequences", "author": ["R. Sibly", "D. McFarland"], "venue": "American Naturalist, pp. 601\u2013617, 1976.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1976}, {"title": "A reinforcement learning theory for homeostatic regulation", "author": ["M. Keramati", "B.S. Gutkin"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 82\u201390.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "An adaptive robot motivational system", "author": ["G. Konidaris", "A. Barto"], "venue": "From Animals to Animats 9. Springer, 2006, pp. 346\u2013356.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Emergence of robot behavior based on selfpreservation. research methodology and embodiment of mechanical system", "author": ["T. Ogata", "S. Sugano"], "venue": "Journal of the Robotics Society of Japan, vol. 15, no. 5, pp. 710\u2013721, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "The cyber rodent project: Exploration of adaptive mechanisms for self-preservation and self-reproduction", "author": ["K. Doya", "E. Uchibe"], "venue": "Adaptive Behavior, vol. 13, no. 2, pp. 149\u2013160, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Biologically inspired embodied evolution of survival", "author": ["S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen"], "venue": "Evolutionary Computation, 2005. The 2005 IEEE Congress on, vol. 3. IEEE, 2005, pp. 2210\u20132216.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "arXiv preprint cs/9605103, 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Simulation of adaptive behavior in animats: Review and prospect", "author": ["J.-A. Meyer", "A. Guillot"], "venue": "In J.-A. Meyer and S.W. Wilson (Eds.) From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, 1991, pp. 2\u201314.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Modeling motivations and emotions as a basis for intelligent behavior", "author": ["D. Ca\u00f1amero"], "venue": "Proceedings of the first international conference on Autonomous agents. ACM, 1997, pp. 148\u2013155.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A.G. Barto", "S. Singh", "N. Chentanez"], "venue": "Proc. 3rd Int. Conf. Development Learn, 2004, pp. 112\u2013119.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, vol. 37, no. 2, pp. 183\u2013233, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["K. Rawlik", "M. Toussaint", "S. Vijayakumar"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 3052\u20133056.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Varieties of helmholtz machine", "author": ["P. Dayan", "G.E. Hinton"], "venue": "Neural Networks, vol. 9, no. 8, pp. 1385\u20131403, 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "arXiv preprint arXiv:1401.0118, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "arXiv preprint arXiv:1402.0030, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, vol. 99, 1999, pp. 278\u2013287.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Autonomous reinforcement learning on raw visual input data in a real world application", "author": ["S. Lange", "M. Riedmiller", "A. Voigtlander"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic inference for solving (po) mdps", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "University of Edinburgh, Informatics Research Report 0934, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "General duality between optimal control and estimation", "author": ["E. Todorov"], "venue": "Decision and Control, 2008. CDC 2008. 47th IEEE Conference on. IEEE, 2008, pp. 4286\u20134292.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal control as a graphical model inference problem", "author": ["H.J. Kappen", "V. G\u00f3mez", "M. Opper"], "venue": "Machine learning, vol. 87, no. 2, pp. 159\u2013 182, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-free reinforcement learning as mixture learning", "author": ["N. Vlassis", "M. Toussaint"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 1081\u20131088.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Robot trajectory optimization using approximate inference", "author": ["M. Toussaint"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 1049\u20131056.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Ashby developed Homeostat, which dynamically stabilizes the state of the machine [1], and Walter developed simple robotic agents that can explore the environment of a room and automatically recharge their batteries at a recharging station [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Ashby developed Homeostat, which dynamically stabilizes the state of the machine [1], and Walter developed simple robotic agents that can explore the environment of a room and automatically recharge their batteries at a recharging station [2].", "startOffset": 239, "endOffset": 242}, {"referenceID": 2, "context": "Toda discussed the survival problem of artificial agents in the natural environment [3], [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Toda discussed the survival problem of artificial agents in the natural environment [3], [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "They suggested several requirements for intelligent agents through comparison of the market economy with natural selection, and they also developed simple robots that were self-sufficient, autonomous agents [7].", "startOffset": 207, "endOffset": 210}, {"referenceID": 5, "context": "Also, Lin, in a simulation study, compared several reinforcement learning (RL) architectures for a complex survival task in a non-Markovian environment [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "Sibly and McFarland introduced the state space approach in ethology and suggested that animal behaviors are the consequence of optimal control with respect to the cost function given by the fitness function [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 7, "context": "Keramati and Gutkin suggested a similar perspective, but they also suggested changes in the distance between the current homeostatic state and the optimal, desired state as the reward function of RL [10].", "startOffset": 199, "endOffset": 203}, {"referenceID": 8, "context": ") through tuning of the reward function, which depends on the agent\u2019s homeostatic state [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Ogata and Sugino developed a real robot agent intended for survival [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Doya and Uchibe developed robots called \u201cCyber rodents\u201d intended for the study of learning agents for survival and evolutionary robotics [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "The wireless communication modules of robots enable the software evolution of control algorithms [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "Many realistic environments for the agent are known to be modeled by the partially observable Markov decision process (POMDP) [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "In this section, we formulate the survival problem from the models of an animal proposed by Ashby [1] and a similar idea suggested by Sibly and McFarland [9] and McFarland and B\u00f6user [6] from the view point of ethology.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "In this section, we formulate the survival problem from the models of an animal proposed by Ashby [1] and a similar idea suggested by Sibly and McFarland [9] and McFarland and B\u00f6user [6] from the view point of ethology.", "startOffset": 154, "endOffset": 157}, {"referenceID": 13, "context": "This manifold is called the viability zone [16], and we define the state of the animal as \u2018Alive\u2019 when the current physiological state is in the manifold.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "In the maximization problem of the likelihood logP (\u0100T |\u03c0), the method that introduces the restricted class of Q(\u03c4) and maximizes the variational bound \u2212F is known to be the variational method and it is widely used in machine learning [20].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "derived the analytical solution of the E-step and it is given by the softmax policy \u03c0Q(a|s) = exp{\u03a8(s, a)}/Z, where \u03a8(s, a) is some energy function and Z is the normalization term [21].", "startOffset": 180, "endOffset": 184}, {"referenceID": 18, "context": "The variational method that introduces the parametrized variational distribution Q\u03c6 and maximizes the variational bound \u2212F (\u03c6, \u03b8) := \u2212F (Q(\u00b7|\u03c0\u03c6), P (\u00b7|\u03c0\u03b8)) by gradient methods are well known in the neural computing community [22]\u2013[25].", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "The variational method that introduces the parametrized variational distribution Q\u03c6 and maximizes the variational bound \u2212F (\u03c6, \u03b8) := \u2212F (Q(\u00b7|\u03c0\u03c6), P (\u00b7|\u03c0\u03b8)) by gradient methods are well known in the neural computing community [22]\u2013[25].", "startOffset": 230, "endOffset": 234}, {"referenceID": 22, "context": "It is known that there are reward settings that have the same optimal policy but a different learning speed for the RL agent [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "And, recently, the pre-training approach for the RL has been successfully applied to the realrobot domain with direct visual image inputs [27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "The relationship between the planning problem and the inference problem is a hot topic in recent machine learning communities [28]\u2013[30].", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "The relationship between the planning problem and the inference problem is a hot topic in recent machine learning communities [28]\u2013[30].", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "Vlassis and Toussaint [31] introduced the perspective that model-free reinforcement learning can be treated as an application of a stochastic EM algorithm to the maximization of the mixture likelihood p(R = 1;\u03c0).", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Our objective function (1) and its lower bound were briefly introduced by Toussaint [32] in the context of a stochastic control problem.", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "Obtaining a survival strategy (policy) is one of the fundamental problems of biological agents. In this paper, we generalize the formulation of previous research related to the survival of an agent and we formulate the survival problem as a maximization of the multi-step survival probability in future time steps. We introduce a method for converting the maximization of multi-step survival probability into a classical reinforcement learning problem. Using this conversion, the reward function (negative temporal cost function) is expressed as the log of the temporal survival probability. And we show that the objective function of the reinforcement learning in this sense is proportional to the variational lower bound of the original problem. Finally, We empirically demonstrate that the agent learns survival behavior by using the reward function introduced in this paper.", "creator": "LaTeX with hyperref package"}}}