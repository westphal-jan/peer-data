{"id": "1606.05804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema", "abstract": "Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types---not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time.", "histories": [["v1", "Sat, 18 Jun 2016 20:38:42 GMT  (285kb,D)", "http://arxiv.org/abs/1606.05804v1", null], ["v2", "Mon, 9 Jan 2017 21:46:47 GMT  (730kb,D)", "http://arxiv.org/abs/1606.05804v2", "EACL 2017. arXiv admin note: text overlap witharXiv:1604.06361"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["patrick verga", "arvind neelakantan", "rew mccallum"], "accepted": false, "id": "1606.05804"}, "pdf": {"name": "1606.05804.pdf", "metadata": {"source": "CRF", "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema", "authors": ["Patrick Verga", "Arvind Neelakantan", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "It is the task of building a structured knowledge base (KB) of facts, which are usually used as raw text evidence, and often also as starting material for an extended presentation (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008). CBs typically contain entity types such as Sundar Pichai IsA person and relationship types such as CEO Of (Sundar Pichai, 2013).Extracted facts about entities, and their types and relationships are useful for many downstream tasks such as answering questions (Bordes et al., 2014) and semantic parsing (Berant et al., 2013; Kwiatkowski et al.).Explicit facts about entities and relationships are useful for the type of entities we predict the type of entities and relationships in a knowledge base (KB)."}, {"heading": "2 Background: Universal Schema", "text": "Universal Scheme (Riedel et al., 2013; Yao et al., 2013) Relation extraction and entity type prediction are typically modeled as matrix completion task. In relation extraction, entity pairs and relationships occupy the rows and columns of the matrix (Figure 1), while in entity type prediction entities and types occupy the rows and columns of the matrix (Figure 2). During the training, we observe some positive entries in the matrix and at test dates, we forecast the missing cells in the matrix. This is achieved by splitting the observed matrix into two low matrices, resulting in embedding for each column entry and each row entry. Test time prediction is performed on the basis of the learned low column and row representations. Let T be the training set, which consists of examples c, the form of the pair (c), where the pair is in relation to the row (s)."}, {"heading": "3 Model", "text": "In this section we describe the model, discuss the different aggregation functions and give details on the training target."}, {"heading": "3.1 \u2018Row-less\u2019 Universal Schema", "text": "Verga et al. (2016) use columnless universal schema for relation extraction. They solve the problem of invisible line entries by using universal schema as a sentence classifier - by comparing a textual relationship directly with a KB relationship in order to perform relation extraction. However, this approach is unsatisfactory for two reasons: the first is that it creates an inconsistency between training and testing; the model is trained to predict compatibility between rows and columns, but at test times it directly predicts compatibility between relationships; second, it takes only one piece of evidence into account in its prediction; and we address both of these concerns in our \"lineless\" universal schema. Instead of explicitly encoding each line, we encode the line as a learned aggregation across its observed columns (Figure 3)."}, {"heading": "3.2 Aggregation Functions", "text": "In this work, we examine four aggregated functions to construct the representations for the series."}, {"heading": "3.3 Training", "text": "Riedel et al. (2013) Model Parameters Entity Embeddings 3.7 e6 Attention 3.1 e5 Mean Pool / Max Pool / Max Relation 1.5 e5Table 1: Number of parameters for the different models on the entity type dataset.Model Parameters Entity-pair Embeddings 1.2 e8 Attention 1.4 e8 Mean Pool / Max Pool / Max Relation 6.9 e7Table 2: Number of parameters for the different relation extraction models on FBB15K-237 datause Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models. BPR ranks the probability of observed triples over unobserved triples as negative instead of explicitly observed negative T cells."}, {"heading": "4 Related Work", "text": "In fact, he said, \"we have to abide by the rules,\" he said. \"We have to abide by the rules,\" he said. \"We have to abide by the rules,\" he said. \"We have to abide by the rules,\" he said. \"We have to abide by the rules,\" he said. \"We have to abide by the rules we have given ourselves.\" \"We have to abide by the rules,\" he said. \"We have to abide by the rules,\" he said. \"We have to abide by the rules.\""}, {"heading": "5 Experimental Results", "text": "In this section, we compare our models with aggregated line representations with models with explicit line representations for entity type predictions and relation extraction tasks. Finally, we conduct experiments with a columnless universal schema model. Tables 1 and 2 show that the line-less models require an order of magnitude fewer parameters because they do not explicitly store the line representations."}, {"heading": "5.1 Entity Type Prediction", "text": "We first evaluate our models based on an entity type prediction task. We collect all entities along with their types from a dump of freebase3. We then filter all entities with less than five freebase types and leave behind a set of 844780 entity type pairs. In addition, we collect 712072 text types from Clueweb. They are randomly extracted from sentences in which entities are mentioned and we select the 5000 most common ones. This results in 140513 unique entities, 1120 frequency relationship types, and 5000 free text types. All embeddings are 25 dimensions, randomly initialized. We match the learning rates of {.01,.001}, '2 of {1e-8, 0}, batch size {512, 1024, 2048}, and negative samples of {2, 200}. For the evaluation, we divide the test frequency-thefit pairs into 60% positive and 20% validity pairs of each type-thief pair."}, {"heading": "5.1.1 Qualitative Results", "text": "A query-specific aggregation function is able to select relevant columns to make a prediction, which is especially important for rows that cannot simply be described by a single centrist, such as a company with several very different careers, or an entity pair with several very different relationships. For example, in Table 5 for the query / baseball / baseball player, the model must correctly focus on aspects such as / sports / professional athletes and ignore evidence information such as / tv / tv actors. A model that creates a single query-independent centrist will be forced to attempt to merge this different information."}, {"heading": "5.2 Relation Extraction", "text": "We rate our models on the FB15k-237 in the top 10 countries. (D)"}, {"heading": "5.3 \u2018Column-less\u2019 universal schema", "text": "The original universal schema approach has two major drawbacks: Similar textual patterns do not share statistics, and the model is unable to predict entities and textual patterns that are not explicitly seen in the migration time.Recently, \"column-less\" versions of the universal schema have been developed to address some of these problems (Toutanova et al., 2015; Verga et al., 2016). These models learn compositional pattern encoders to parameterise the column matrix instead of direct embedding. Compositional universal schema facilitates more compact sharing of statistics by composing similar patterns from the same sequence of word embedding - the text patterns \"live in the city\" and \"live in the city\" no longer exist as distinct atomatic units. Compositional universal schema can therefore be combined with any possible textual patterns, which facilitates reasoning."}, {"heading": "6 Conclusion", "text": "In this paper, we examine a lineless extension of the universal scheme, which does not require explicit line representations for an aggregation function across its observed columns; this extension enables prediction between all lines in new textual mentions - whether at train time or not - and also establishes a natural link to the provenance supporting the prediction; our models also have a smaller memory place.In this paper, we show that an aggregation function based on query-specific attention on relation types outperforms query-independent aggregations; we show that aggregation models are capable of predicting at the same level as models with explicit line representations on seen line centered lines with an order of magnitude of fewer parameters; more importantly, aggregation models predict invisible line sizes without loss of accuracy; and finally, we show that we can train models to predict extraction on both invisible time columns and generalisations."}], "references": [{"title": "Neural Machine Translation", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "Learning entity and relation", "author": ["Liu", "Xuan Zhu"], "venue": null, "citeRegEx": "Liu and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Zhu.", "year": 2015}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Inferring missing entity type instances for knowledge base completion: New dataset and methods", "author": ["Neelakantan", "Chang2015] Arvind Neelakantan", "MingWei Chang"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Neural Programmer: Inducing latent programs with gradient descent", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": "In ICLR", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "HansPeter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts", "author": ["Orr et al.2013] Dave Orr", "Amarnag Subramanya", "Evgeniy Gabrilovich", "Michael Ringgaard"], "venue": null, "citeRegEx": "Orr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orr et al\\.", "year": 2013}, {"title": "Mining entity types from query logs via user intent modeling", "author": ["Thomas Lin", "Michael Gamon"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Pantel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2012}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["Park", "Chu2009] Seung-Taek Park", "Wei Chu"], "venue": "In RecSys", "citeRegEx": "Park et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Park et al\\.", "year": 2009}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "HLTNAACL", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Methods and metrics for cold-start recommendations", "author": ["Alexandrin Popescul", "Lyle H Ungar", "David M Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Schein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": null, "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Verga et al.2016] Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Embedding entity pairs through observed relations for knowledge base completion", "author": ["Dirk Weissenborn"], "venue": null, "citeRegEx": "Weissenborn.,? \\Q2016\\E", "shortCiteRegEx": "Weissenborn.", "year": 2016}, {"title": "Nonlinear latent factorization by embedding multiple user interests", "author": ["Weston et al.2013] Jason Weston", "Ron Weiss", "Hector Yee"], "venue": "In ACM International Conference on Recommender Systems", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "Automatic knowledge base construction (AKBC) is the task of building a structured knowledge base (KB) of facts using raw text evidence, and often an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 181, "endOffset": 250}, {"referenceID": 13, "context": "In the standard formulation for relation extraction (Riedel et al., 2013), entity pairs and relations occupy the rows and columns of the matrix respectively (Figure 1).", "startOffset": 52, "endOffset": 73}, {"referenceID": 25, "context": "Analogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 2).", "startOffset": 38, "endOffset": 56}, {"referenceID": 14, "context": "This problem is referred to as the cold-start problem in recommendation systems (Schein et al., 2002).", "startOffset": 80, "endOffset": 101}, {"referenceID": 19, "context": "Recently Toutanova et al. (2015) and Verga et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 19, "context": "Recently Toutanova et al. (2015) and Verga et al. (2016) proposed \u2018column-less\u2019 versions of universal schema models that generalize to unseen column entries.", "startOffset": 9, "endOffset": 57}, {"referenceID": 25, "context": "The current methods for KB entity type prediction operate with explicit entity representations (Yao et al., 2013; Neelakantan and Chang, 2015) and hence, cannot generalize to unseen entities.", "startOffset": 95, "endOffset": 142}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 24, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 21, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 16, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al.", "startOffset": 44, "endOffset": 475}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al. (2013) observe that the entity pair model outperforms entity models in cases where the entity pair was seen at training time.", "startOffset": 44, "endOffset": 500}, {"referenceID": 9, "context": ", 2008) augmented with textual relations and types from Clueweb text (Orr et al., 2013; Gabrilovich et al., 2013).", "startOffset": 69, "endOffset": 113}, {"referenceID": 13, "context": "Universal schema (Riedel et al., 2013; Yao et al., 2013) relation extraction and entity type prediction is typically modeled as a matrix completion task.", "startOffset": 17, "endOffset": 56}, {"referenceID": 25, "context": "Universal schema (Riedel et al., 2013; Yao et al., 2013) relation extraction and entity type prediction is typically modeled as a matrix completion task.", "startOffset": 17, "endOffset": 56}, {"referenceID": 12, "context": "The embeddings are learned using Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) in which the probability of the observed triples are ranked above unobserved triples.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": "Verga et al. (2016) use column-less universal schema for relation extraction.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 5, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 4, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 18, "context": "Finally, we look at an Attention aggregation function over columns (Figure 4) which is similar to a single-layer memory network (Sukhbaatar et al., 2015).", "startOffset": 128, "endOffset": 153}, {"referenceID": 7, "context": "The soft attention mechanism has been used to selectively focus on relevant parts in many different models (Bahdanau et al., 2014; Graves et al., 2014; Neelakantan et al., 2016).", "startOffset": 107, "endOffset": 177}, {"referenceID": 13, "context": "Riedel et al. (2013) Model Parameters", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "use Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models.", "startOffset": 40, "endOffset": 61}, {"referenceID": 12, "context": "use Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models. BPR ranks the probability of observed triples above unobserved triples rather than explicitly modeling unobserved edges as negative. Each training example is an entity pair/relation type or entity/entity type pair observed in the training text corpora or KB. Rather than BPR, Toutanova et al. (2015) use 200 negative samples to approximate the negative log likelihood1.", "startOffset": 41, "endOffset": 402}, {"referenceID": 8, "context": "Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al.", "startOffset": 71, "endOffset": 152}, {"referenceID": 16, "context": "Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al.", "startOffset": 71, "endOffset": 152}, {"referenceID": 19, "context": "\u2018Column-less\u2019 versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 62, "endOffset": 106}, {"referenceID": 20, "context": "\u2018Column-less\u2019 versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 62, "endOffset": 106}, {"referenceID": 3, "context": "Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned. Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al. (2013) use entity pair representations.", "startOffset": 0, "endOffset": 321}, {"referenceID": 10, "context": "Entity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling and Weld, 2012; Shimaoka et al., 2016).", "startOffset": 85, "endOffset": 150}, {"referenceID": 15, "context": "Entity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling and Weld, 2012; Shimaoka et al., 2016).", "startOffset": 85, "endOffset": 150}, {"referenceID": 25, "context": "More recently, embedding-based methods for knowledge base entity type prediction have been proposed (Yao et al., 2013; Neelakantan and Chang, 2015).", "startOffset": 100, "endOffset": 147}, {"referenceID": 14, "context": "Methods proposed to tackle this problem commonly use user and item content and attributes (Schein et al., 2002; Park and Chu, 2009).", "startOffset": 90, "endOffset": 131}, {"referenceID": 4, "context": "Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is \u2018row-less\u2019 having no explicit parameters for entity pairs and entities.", "startOffset": 10, "endOffset": 36}, {"referenceID": 4, "context": "Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is \u2018row-less\u2019 having no explicit parameters for entity pairs and entities. Concurrent to our work, Weissenborn (2016) proposes a row-less method for relation extraction.", "startOffset": 10, "endOffset": 208}, {"referenceID": 19, "context": "We evaluate our models on the FB15k-237 dataset from Toutanova et al. (2015). The data is composed of a small set of 237 Freebase relations and approximately 4 million textual patterns from Clueweb with entities linked to Freebase (Gabrilovich et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 19, "context": "Recently, \u2018column-less\u2019 versions of universal schema to address some of these issues (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 85, "endOffset": 129}, {"referenceID": 20, "context": "Recently, \u2018column-less\u2019 versions of universal schema to address some of these issues (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 85, "endOffset": 129}], "year": 2016, "abstractText": "Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types\u2014not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time.", "creator": "TeX"}}}