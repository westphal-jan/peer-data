{"id": "1606.05829", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "abstract": "Recent progress in neural learning demonstrated that machines can do well in regularized tasks, e.g., the game of Go. However, artistic activities such as poem generation are still widely regarded as human's special capability. In this paper, we demonstrate that a simple neural model can imitate human in some tasks of art generation. We particularly focus on traditional Chinese poetry, and show that machines can do as well as many contemporary poets and weakly pass the Feigenbaum Test, a variant of Turing test in professional domains. Our method is based on an attention-based recurrent neural network, which accepts a set of keywords as the theme and generates poems by looking at each keyword during the generation. A number of techniques are proposed to improve the model, including character vector initialization, attention to input and hybrid-style training. Compared to existing poetry generation methods, our model can generate much more theme-consistent and semantic-rich poems.", "histories": [["v1", "Sun, 19 Jun 2016 03:17:29 GMT  (953kb)", "http://arxiv.org/abs/1606.05829v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qixin wang", "tianyi luo", "dong wang"], "accepted": false, "id": "1606.05829"}, "pdf": {"name": "1606.05829.pdf", "metadata": {"source": "CRF", "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "authors": ["Qixin Wang", "Tianyi Luo", "Dong Wang"], "emails": ["lty}@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 829v 1 [cs.C L] 19 Jun 2016Our method is based on an attention-based recurring neural network that accepts a set of keywords as a theme and generates poems by looking at each keyword during generation. To improve the model, a number of techniques are proposed, including character vector initialization, attention to input, and hybrid training. Compared to existing methods for generating poems, our model can generate much more theme-consistent and semantic poems."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to change the world, in which they are able to live, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are"}, {"heading": "2 Related Work", "text": "A variety of methods have been proposed for the automatic generation of poems. The first approach is based on rules and templates. For example, Tosa et al. (2009) and Wu et al. (2009) used a phrase search approach for the production of Japanese poems, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantics and grammar templates for the production of Spanish poems. The second approach is based on various genetic algorithms. For example, Zhou et al. (2010) proposed to use a stochastic search algorithm to obtain the most coordinated sentences. The search algorithm is based on four standards proposed by Manurung et al. (2012): fluid, poetic and coherent. The third approach to the production of poems includes various statistical machine translations (SMT) methods. This approach was used by Jiang and Jihou to generate couplets."}, {"heading": "3 Method", "text": "In this section, we first present the attention-based framework for generating Chinese poetry, and then describe the implementation of encoder and decoder models tailored to our task."}, {"heading": "3.1 Attention-based Chinese Poetry Generation", "text": "The attention-based sequence-to-sequence model proposed by Bahdanau et al. (2014) is a powerful framework for sequence generation. Specifically, the input sequence is converted from an \"encoder\" to a sequence of hidden states to represent the semantic state at each position of the input, and these hidden states are used to regulate a \"decoder\" that generates the target sequence.The important mechanism of the attention-based model is that at each generation step the most relevant input devices are detected by comparing the \"current\" status of the decoder with all the hidden states of the encoder, so that the generation is regulated by the fine structure of the input sequence.The entire framework of the model applied to Chinese poetry generation is represented in Figure 1.The encoder (a bidirectional decoder, or the one discussed in brief) is then converted into a character word (1), the number of characters (1), the number of characters (1), the number of characters (1), the number of characters (1), the number of characters (1), the number of characters (1)."}, {"heading": "3.2 GRU-based Model Structure", "text": "One potential problem with the RNN-based generational approach proposed by Zhang and Lapata (2014) is that the vanilla RNN used in their model tend to forget historical input quickly, leading to a change of topic within the generation. To alleviate the problem, Zhang and Lapata (2014) developed a composition strategy in which only one line is generated at a time. This is certainly not satisfactory as it makes the generational process more difficult. In our model, the problem of rapid forgetting is solved by using the GRU model. A bi-directional GRU is used for the encoder to encode the input keywords, and another GRU for the decoder to guide the generation."}, {"heading": "3.3 Model Training", "text": "The goal of the model training is to match the predicted string with the original poem. As an objective function, we chose the cross entropy between the distributions of Chinese characters given by the decoder and the basic truth (essentially in simple form).To speed up the training, we used the Minibatch stochastic gradient descent algorithm (SGD).The gradient was calculated sentence by sentence, and the AdaDelta algorithm was used to adjust the learning rate (Lines, 2012).Note that there is no keyword input during the training phase, so we use the first line as input to generate the entire poem."}, {"heading": "4 Implementation", "text": "A particular problem is that each poem was created to express a particular affection for the poet, which tends to be \"unique.\" This means that most valid (and often great) expressions may not appear sufficiently in the training data. Another problem is that the subject may become vague towards the end of the generation, even with the attention mechanism. Several techniques are presented to improve the model."}, {"heading": "4.1 Character Vector Initialization", "text": "Due to the uniqueness of each poem, it is not easy to learn the attention model from scratch, as many expressions are not statistically significant. This is a special form of data economy. One possible solution is to learn the model in two steps: First, we learn the semantic representation of each character, possibly using a large external corpus, and then train the attention model with these pre-trained presentations. Through this approach, the attention model focuses most on possible expressions and is therefore easier to learn. In practice, we first derive character vectors using the word2vec Tool1 and then use these character vectors to initialize the word embedding matrix into the attention model. As part of the model (the embedding matrix) has been pre-trained, the problem of data economy can be largely alleviated."}, {"heading": "4.2 Input Reconstruction", "text": "We found that this implicit topic is not easy for machines to understand and learn, leading to a possible deviation of the topic at runtime. A simple solution is to force the model to reconstruct the input after it has generated the entire poem. Specifically, in the training phase, we use the first line of a training poem as input, and based on this input, we generate five lines in succession: line 1-2-3-4-1. The last generation step for line 1 forces the model to keep an eye on the input throughout the generation process, so it leans toward focusing on the topic.1https: / / code.google.com / archive / p / word2vec /"}, {"heading": "4.3 Input Vector Attention", "text": "The popular configuration of the attention model deals with hidden states. Since hidden states have an accumulated semantic meaning, this attention is good to form a global issue. However, since the semantic content of individual keywords has been largely averaged, it is difficult to generate different poems that respond sensitively to each and different keywords.We propose a multiple attention solution that takes into account both hidden states and input character vectors, so that both accumulated and individual semantics are taken into account throughout the generation.This approach has been shown to be highly effective in generating diverse and novel poems: if there are only enough keywords, new high-quality poems can be generated. Compared to other approaches such as noise injection or n-best inference, this approach can generate unlimited alternatives without sacrificing quality. Interestingly, our experiments show that more keywords tend to generate, but unexpected, but highly interesting innovation."}, {"heading": "4.4 Hybrid-style Training", "text": "Traditional Chinese quatrains are divided into 5-character quatrains and 7-character quatrains, each containing five and seven characters per line, respectively. These two categories follow different rules, but also have the same words and similar semantics. We propose a hybrid-style training that trains the two types of quatrains on the basis of the same model, with a \"type indicator\" notifying the model to specify the type of the present training sample. In our study, the type indicators are derived from self-vectors of a 200 x 200 dimensional random matrix. Each quatraint type is assigned a fixed eigenvector as a type indicator. The indicators are provided as part of the input into the first hidden state of the decoder and remain constant during the training."}, {"heading": "5 Experiments", "text": "In this section, we describe the experimental settings and results. First, we present the data sets used in the experiments, and then we report on the evaluation in three phases: (1) the first phase focuses on finding optimal configurations for the attention model; (2) the second phase compares the attention model with other methods; (3) the third phase is the fig tree test."}, {"heading": "5.1 Datasets", "text": "This body consists of 13, 299 5-character quatrains and 65, 560 7-character quatrains. As far as we know, this covers most of the quatrains preserved today. We filter out some poems that contain 100% low-frequency words. By cleaning the body, a body was obtained that contains 9, 195 5 5-character quatrains and 49, 162 7-character quatrains. 9,000 5-character quatrains and 49,000 7-character quatrains are used to train the GRU model of the attention model and the LSTM model of a comparison model based on RNN language models, and the remaining poems are used as test datasets. The second dataset was used to train and derive character vectors for the initialization of attention models. This dataset contains 284, 899 traditional Chinese Qatrains in different sizes, including songs."}, {"heading": "5.2 Model Development", "text": "The Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) is used as a metric to determine which improvement techniques are effective. BLEU was originally proposed to evaluate the performance of machines (Papineni et al., 2002), and was used by Zhang and Lapata to evaluate the quality of poems. We used BLEU as a cheap evaluation method to determine which design option to choose, without the costly human evaluation. The method was proposed by him and used by Zhang and Lapata to obtain reference poems."}, {"heading": "5.3 Comparative Evaluation", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "5.4 Feigenbaum Test", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "5.5 Generation Example", "text": "Finally, we show a 7-chariot quadruplet generated by the attention model. The theme of this poem is \"Crab-Apple-Flower.\""}, {"heading": "6 Conclusion", "text": "This work proposed an attention-based neural model for the generation of Chinese poetry. Compared to existing methods, the new approach is simple in model structure, strong in theme preservation, flexible in the production of innovations, and easily expandable to other genres. Our experiments show that it can produce traditional Chinese quatrains fairly well and pass the fig tree test weakly. In future work, more generative models, such as generative depth models, will be used to achieve more natural innovation. We also plan to extend the work to other genres of traditional Chinese poetry, such as yuan songs."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Some challenges and grand challenges for computational intelligence", "author": ["Edward A Feigenbaum"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Feigenbaum.,? \\Q2003\\E", "shortCiteRegEx": "Feigenbaum.", "year": 2003}, {"title": "Study on the Liao, Jin and Yuan Poetry and Music Theory. China", "author": ["Wei Han"], "venue": null, "citeRegEx": "Han.,? \\Q2015\\E", "shortCiteRegEx": "Han.", "year": 2015}, {"title": "Generating chinese classical poems with statistical machine translation models", "author": ["He et al.2012] Jing He", "Ming Zhou", "Long Jiang"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating chinese couplets using a statistical mt approach", "author": ["Jiang", "Zhou2008] Long Jiang", "Ming Zhou"], "venue": "In Proceedings of the 22nd International Conference on Computational LinguisticsVolume", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Using genetic algorithms to create meaningful poetic text", "author": ["Graeme Ritchie", "Henry Thompson"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "Manurung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manurung et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Netzer et al.2009] Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,", "citeRegEx": "Netzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Automatic generation of poetry: an overview", "author": ["H Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2009\\E", "shortCiteRegEx": "Oliveira.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": "In Proceedings of the ECAI 2012 Workshop on Computational Creativity,", "citeRegEx": "Oliveira.,? \\Q2012\\E", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Tosa et al.2009] Naoko Tosa", "Hideto Obara", "Michihiko Minoh"], "venue": "In Entertainment Computing-ICEC", "citeRegEx": "Tosa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tosa et al\\.", "year": 2009}, {"title": "Chinese song iambics generation with neural attention-based model", "author": ["Wang et al.2016] Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A Summary of Rhyming Constraints of Chinese Poems", "author": ["Li Wang"], "venue": null, "citeRegEx": "Wang.,? \\Q2002\\E", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Wu et al.2009] Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu"], "venue": "In Entertainment Computing\u2013ICEC", "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "I, poet: automatic chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Yan et al.2013] Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of chinese songci", "author": ["Zhou et al.2010] Cheng-Le Zhou", "Wei You", "Xiaojun Ding"], "venue": "Journal of Software,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "An example of quatrain written by Lun Lu, a famous poet in Tang Dynasty (Wang, 2002), is shown in Table 1.", "startOffset": 72, "endOffset": 84}, {"referenceID": 4, "context": ", by machine translation models (He et al., 2012) and recurrent neural networks (RNN) (Zhang and Lapata, 2014).", "startOffset": 32, "endOffset": 49}, {"referenceID": 9, "context": "For example, Tosa et al. (2009) and Wu et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 9, "context": "For example, Tosa et al. (2009) and Wu et al. (2009) employed a phrase search approach for Japanese poem generation, and Netzer et al.", "startOffset": 13, "endOffset": 53}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms.", "startOffset": 75, "endOffset": 96}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantic and grammar templates for Spanish poem generation.", "startOffset": 75, "endOffset": 166}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantic and grammar templates for Spanish poem generation.", "startOffset": 75, "endOffset": 186}, {"referenceID": 18, "context": "For example, Zhou et al. (2010) proposed to use a stochastic search algorithm to obtain the best matched sentences.", "startOffset": 13, "endOffset": 32}, {"referenceID": 6, "context": "The search algorithm is based on four standards proposed by Manurung et al. (2012): fluency, meaningful, poetic, and coherent.", "startOffset": 60, "endOffset": 83}, {"referenceID": 4, "context": "He et al. (2012) extended this approach to Chinese quatrain generation, where each line of the poem is generated by translating the preceding line.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "For example, Yan et al. (2013) proposed a method that retrieves", "startOffset": 13, "endOffset": 31}, {"referenceID": 13, "context": "Finally, Wang et al. (2016) proposed an attention-based model for Song Iambics genera-", "startOffset": 9, "endOffset": 28}, {"referenceID": 13, "context": "Our approach follows the attention-based strategy in (Wang et al., 2016), but introduces several innovations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 0, "context": "The attention-based sequence-to-sequence model proposed by Bahdanau et al. (2014) is a powerful framework for sequence generation.", "startOffset": 59, "endOffset": 82}, {"referenceID": 17, "context": "ing rate (Zeiler, 2012).", "startOffset": 9, "endOffset": 23}, {"referenceID": 11, "context": "The \u2018Bilingual Evaluation Understudy\u2019 (BLEU) (Papineni et al., 2002) is used as the metric to determine which enhancement techniques are effective.", "startOffset": 45, "endOffset": 68}, {"referenceID": 11, "context": "BLEU was originally proposed to evaluate machine translation performance (Papineni et al., 2002), and was used", "startOffset": 73, "endOffset": 96}, {"referenceID": 4, "context": "The method proposed by He et al. (2012) and employed by Zhang and Lapata (2014) was adopted to obtain reference poems.", "startOffset": 23, "endOffset": 40}, {"referenceID": 4, "context": "The method proposed by He et al. (2012) and employed by Zhang and Lapata (2014) was adopted to obtain reference poems.", "startOffset": 23, "endOffset": 80}, {"referenceID": 13, "context": "This is mostly the system in (Wang et al., 2016).", "startOffset": 29, "endOffset": 48}, {"referenceID": 7, "context": "(2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010).", "startOffset": 254, "endOffset": 276}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al.", "startOffset": 12, "endOffset": 29}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al.", "startOffset": 12, "endOffset": 105}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010). Following the work of Zhang and Lapata (2014), we selected 30 subjects (e.", "startOffset": 12, "endOffset": 346}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010). Following the work of Zhang and Lapata (2014), we selected 30 subjects (e.g., falling flower, stone bridge, etc.) in the Shixuehanying taxonomy (Liu, 1735) as 30 themes. For each theme, several phrases belonging to the corresponding subject were selected as the input keywords. For the attention model, these keywords were used to generate the first line directly; For the other three models, however, the first line had to be constructed beforehand by an external model. We chose the method provide by Zhang and Lapata (2014) to generate the first lines for the SMT, vanilla RNN and LSTMLM approaches.", "startOffset": 12, "endOffset": 827}, {"referenceID": 3, "context": "The poems were chosen from (Han, 2015), (Yoshikawa, 1963)", "startOffset": 27, "endOffset": 38}, {"referenceID": 2, "context": "We design a Feigenbaum Test (Feigenbaum, 2003) to evaluate the quality of poems generated by our models.", "startOffset": 28, "endOffset": 46}], "year": 2016, "abstractText": "Recent progress in neural learning demonstrated that machines can do well in regularized tasks, e.g., the game of Go. However, artistic activities such as poem generation are still widely regarded as human\u2019s special capability. In this paper, we demonstrate that a simple neural model can imitate human in some tasks of art generation. We particularly focus on traditional Chinese poetry, and show that machines can do as well as many contemporary poets and weakly pass the Feigenbaum Test, a variant of Turing test in professional domains. Our method is based on an attention-based recurrent neural network, which accepts a set of keywords as the theme and generates poems by looking at each keyword during the generation. A number of techniques are proposed to improve the model, including character vector initialization, attention to input and hybrid-style training. Compared to existing poetry generation methods, our model can generate much more theme-consistent and semantic-rich poems.", "creator": "LaTeX with hyperref package"}}}