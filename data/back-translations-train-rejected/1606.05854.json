{"id": "1606.05854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question Answering", "abstract": "Recently, bidirectional recurrent neural network (BRNN) has been widely used for question answering (QA) tasks with promising performance. However, most existing BRNN models extract the information of questions and answers by directly using a pooling operation to generate the representation for loss or similarity calculation. Hence, these existing models don't put supervision (loss or similarity calculation) at every time step, which will lose some useful information. In this paper, we propose a novel BRNN model called full-time supervision based BRNN (FTS-BRNN), which can put supervision at every time step. Experiments on the factoid QA task show that our FTS-BRNN can outperform other baselines to achieve the state-of-the-art accuracy.", "histories": [["v1", "Sun, 19 Jun 2016 11:03:47 GMT  (726kb,D)", "https://arxiv.org/abs/1606.05854v1", "9 pages"], ["v2", "Tue, 21 Jun 2016 01:47:06 GMT  (726kb,D)", "http://arxiv.org/abs/1606.05854v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dong xu", "wu-jun li"], "accepted": false, "id": "1606.05854"}, "pdf": {"name": "1606.05854.pdf", "metadata": {"source": "CRF", "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question Answering", "authors": ["Dong Xu", "Wu-Jun Li"], "emails": ["121220312@smail.nju.edu.cn,", "liwujun@nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Background", "text": "Although our model can also be generalized to handle general QA tasks, this paper focuses on a specific QA task called Factoid QA. In this section, we present the background to this paper, including a brief overview of Factoid QA, BRNN, and GRU."}, {"heading": "2.1 Factoid Question Answering", "text": "Factoid QA is a specific QA task in which the answers are syntactical or semantic units, such as organizational or personal names. Quiz bowl is a representative type of factoid QA. Here, we use quiz bowl as an example to introduce factoid QA [7]. Quiz bowl can be considered a kind of text classification task [2] or QA task [7]. Players are asked to answer according to the given description (question). Each question consists of four to six sentences in which each sentence contains useful cludes to answer the question. The answer to a question is a unit represented by a phrase or a single word. Table 1 shows an example of quiz bowl. In real-life competition, players can answer at any time. In our experiments, we slightly change the quiz bowl rule so that players can only answer the question after receiving all the information that better reflects the information extraction capability of the models and make the model more suited to more general QA tasks."}, {"heading": "2.2 Bidirectional Recurrent Neural Network (BRNN)", "text": "BRNN consists of two unidirectional RNN in opposite directions, a forward RNN and a backward RNN. Thus, there are several ways to combine these two hidden states. In [1, 14], a concatenating operation h (t) = [f (t), b (t)] is assumed. In [9], an output activation function o (t) = f (WL \u00b7 [t), b (t)] is used, which uses the vector function b (t), b (t)] and the output function o (t) = f (WL \u00b7 [t), b (t), which is better in terms of the actual effect than the output function o (t)."}, {"heading": "2.3 Gated Recurrent Unit (GRU)", "text": "In real-world applications, the gated RNN or gated BRNN is always adopted. Typical gated RNN include long-term short-term memory (LSTM) [6] and gated recurrent unit (GRU). GRU is proposed for the first time in [3] for NMT tasks. There are three gates in a single LSTM unit: Input gate i, forget gate f and Output gate o. But GRU only uses two gates, Reset gate r and Update gate z to achieve functionality similar to that in LSTM. In this paper we choose GRU instead of LSTM because we find that in our model GRU is better than LSTM. Here we give a short introduction to GRU.In GRU, Reset gate r and Update gate z are defined as follows: r = Cycle (Wrx + Urh (t \u2212 1) + Urh (t \u2212 1) + Uzh (t \u2212 1) + Uzz (t \u2212 1) + bz \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7 z \u00b7"}, {"heading": "3 Full-Time Supervision based BRNN (FTS-BRNN)", "text": "In this section, we present the details of our full-time supervision based on BRNN (FTS-BRNN), which includes monitoring for all time steps in BRNN.FTS-BRNN: the first variant uses BRNN for questions and RNN for answers, and the second variant uses the same BRNN for questions and answers. In this essay, FTS-BRNN refers to the first variant, and FTS-BRNN-s refers to the second variant, which uses the same BRNN for questions and answers. BRNN is typically better than RNN for long sequences, but for short sequences BRNN is not necessarily better than RNN. So if the answers are short, as in the case of factual QA, we prefer FTS-BRNN to use the RNN for answers, which is also checked in our experiments."}, {"heading": "3.1 FTS-BRNN", "text": "The architecture of FTS-BRNN is shown in Figure 1. FTS-BRNN uses a BRNN for questions and an RNN for answers. (The answer to the question after the question after the question after the question after the answer after the question after the question after the answer after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the answer after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the"}, {"heading": "3.2 FTS-BRNN-s", "text": "Here we present a variant of FTS-BRNN, called FTS-BRNN-s. In this case, the \"s\" means that we use the same BRNN for both questions and answers. FTS-BRNN-s architecture is in Figure 2. Question processing in FTS-BRNN-s is the same as in FTS-BRNN. The only difference is in the processing of answers. Unlike in Aex of FTS-BRNN-s, FTS-BRNN-s Aox results in the following: Afx = BRNNf (Ax), Abx = BRNb (Ax), Aox = Where \u00b7 Afx + Uo \u00b7 Abx + Distortion. Then the full-time margin loss is changed to: Loss = \u2211 x x x-t max (0, 1 \u2212 o (t) x \u00b7 Aox (t) x \u00b7 Aowrong (t)."}, {"heading": "4 Experiments", "text": "We evaluate our method using the factoid QA task. The experiments are performed on an NVIDIA K80 GPU server."}, {"heading": "4.1 Dataset", "text": "The entire data set contains two subsets: literature and history. Similar to [7], we first filter out all questions that do not belong to history or literature, and then use only those answers that occur at least six times. Statistics of these two subsets are in Table 2.For all questions that belong to the same answer, 20% are evaluated as a test set, 20% as a validation set, and the remaining 60% as a training set. Thus, we get 2524 training questions, 840 validation questions, and 840 literature test questions. For history, we have 1535 training questions, 511 validation questions, and 511 test questions.2We download the data sets from https: / / cs.umd.edu / ~ miyyer / qblearn / provided by the authors of [7]. The publicly available data sets are slightly smaller than those used in [7]."}, {"heading": "4.2 Baselines", "text": "We compare our method with several state-of-the-art baselines: \u2022 BOW [7]: BOW-DT [7]: BOW-DT treats each answer as a class name and uses the LR classifier for classification based on the characteristics of the word. \u2022 BOW-DT [7]: BOW-DT is a recursive method based on a neural network, which is proposed in [7]. Unlike BOW, QANTA adopts dependency analysis trees to learn sentence representation and then trains an LR classifier based on this representation. \u2022 QANTA [7]: The answer is selected by the LR classifier. \u2022 QA-LSTM [14]: QA-LSTM is one based on the BRNN method [14], which calculates the loss after poding."}, {"heading": "4.3 Results", "text": "First, we conduct experiments to verify the effectiveness of the full-time monitoring strategy and output layer in FTT-BRNN and then the advantage of GRU over LSTM. Finally, we compare our method with other state-of-the-art baselines to represent the promising performance of FTT-BRNN. All results are based on the measure of predictive accuracy. \"InnerP\" represents the results with the internal product for prediction as shown in (5), and \"LR\" the results with LR for prediction as shown in (6)."}, {"heading": "4.3.1 Effect of Full-Time Supervision and Output Layer", "text": "Since FTS-BRNN always needs the output layer to make the dimensionality of answers and questions equal, we show the effects of a full-time monitoring and an output layer based on FTS-BRNN-s. Table 3 shows the results with different configurations for FTS-BRNN and FTS-BRNN-s. \"has-output\" represents the model with an output layer o (t) = Where \u00b7 f (t) + Uo \u00b7 b (t) + distortion, while \"no-output\" represents the model without output layer by showing the hidden states h (t) = [f (t), b (t) = output layer o (t) = Where \u00b7 f (t) + Uo \u00b7 b (t) + distortion, where \"no-output\" represents the model without output layer h (t) = [f (t) = [t), b (t), b (n), b (t) is the model with a (o) while (o) represents the output layer with a BRN (where)."}, {"heading": "4.3.2 Effect of GRU", "text": "The accuracy comparison between GRU and LSTM is shown in Table 4, where \"FTS-BRNN\" is the method proposed in this paper with GRU for BRNN and \"FTS-BRNN with LSTM\" is a variant 3We also try the hierarchical RNN [8, 10], which aims to deal with paragraphs or documents, but it does not give us a better performance. By replacing GRU with LSTM. We can find that GRU can surpass LSTM in our FTS-BRNN model. Therefore, our FTS-BRNN takes GRU for BRNN."}, {"heading": "4.3.3 Comparison to Baselines", "text": "Table 5 shows the accuracy comparison between our method and other state-of-the-art baselines introduced in Section 4.2. Since the public scripts of BOW, BOW-DT and QANTA do not use an internal product to select the answers, we do not report internal product results of these three methods. Table 5 shows that the results of LR are better than those of internal product. All deep methods, including QANTA, QA-LSTM, FTS-BRNN-s and FTS-BRNN, can surpass traditional non-deep methods. In addition, all RNN-based methods, including QA-LSTM, FTS-BRNN-s and FTS-BRNN, can surpass recursive neural network-based methods (QANTA)."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a bidirectional RNN method for QA tasks, called FTS-BRNN. This is the first work that performs a detailed empirical comparison between full-time supervision and pooling-based supervision and explicitly states that full-time supervision is the key in BRNN for QA. Furthermore, we note that GRU is better than LSTM for BRNN-based QA tasks. Experiments on factoid QA tasks show that our FTS-BRNN method can outperform other state-of-the-art baselines in real-world applications. In our future work, we will apply our method to other QA tasks, especially those with long answers."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Besting the quiz master: Crowdsourcing incremental classification", "author": ["Jordan Boyd-Graber", "Brianna Satinoff", "He He", "Hal Daume III"], "venue": "games. EMNLP-CoNLL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning phrase representations using rnn encoder decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Question answering passage retrieval using dependency relations", "author": ["Hang Cui", "Renxu Sun", "Keya Li", "Min-Yen Kan", "Tat-Seng Chua"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermanny", "Tomas Kociskyz", "Edward Grefenstettey", "Lasse Espeholty", "Will Kayy", "Mustafa Suleymany", "Phil Blunsomyz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daume III"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A rule-based question answering system for reading comprehension tests", "author": ["Ellen Riloff", "Michael Thelen"], "venue": "IIH-MSP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Lstm-based deep learning models for nonfactoid answer selection", "author": ["Ming Tan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A. Smith", "Teruko Mitamura"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": ", quiz bowl) is a special QA task, which has also attracted much attention recently [2, 7].", "startOffset": 84, "endOffset": 90}, {"referenceID": 6, "context": ", quiz bowl) is a special QA task, which has also attracted much attention recently [2, 7].", "startOffset": 84, "endOffset": 90}, {"referenceID": 11, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 121, "endOffset": 128}, {"referenceID": 3, "context": "The first category is based on surface pattern matching which uses manually defined rules [12] or parse dependency trees [16, 4].", "startOffset": 121, "endOffset": 128}, {"referenceID": 1, "context": "This category of methods are mainly for factoid QA tasks [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 6, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 13, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 14, "context": "Recently, researchers propose to adopt deep methods, especially deep neural network (DNN), for QA tasks [5, 7, 14, 15].", "startOffset": 104, "endOffset": 118}, {"referenceID": 4, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 6, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 13, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 14, "context": "Different from traditional shallow methods which use manually constructed high-dimensional feature vectors to represent questions and answers, deep methods try to learn low-dimensional distributed representation for questions and answers [5, 7, 14, 15].", "startOffset": 238, "endOffset": 252}, {"referenceID": 6, "context": "Furthermore, many works [7, 14] have shown that deep methods can achieve better performance than traditional shallow methods for QA.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "Furthermore, many works [7, 14] have shown that deep methods can achieve better performance than traditional shallow methods for QA.", "startOffset": 24, "endOffset": 31}, {"referenceID": 6, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 13, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 14, "context": "The first category is based on recursive neural network [7], and the second category is based on recurrent neural network (RNN) 1 [5, 14, 15].", "startOffset": 130, "endOffset": 141}, {"referenceID": 6, "context": "QANTA [7] is one of the representative deep QA methods based on recursive neural network.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 4, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 14, "context": "Hence, BRNN has been widely used for QA tasks [14, 5, 15] with promising performance.", "startOffset": 46, "endOffset": 57}, {"referenceID": 13, "context": "QA-LSTM [14] first uses a gated BRNN, called bidirectional long-short term memory (BLSTM), to extract the feature of questions (answers) and concatenates the hidden states at time step t of both directions in BLSTM to generate the output at time step t.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "[5] uses an attention mechanism on the hidden states to generate the representations for questions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] concatenates the question and answer as one sequence, then uses an output layer to compute similarity directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Furthermore, some work [9] has shown that BRNN can outperform recursive neural network based methods in many NLP tasks including QA.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "\u2022 Different from existing BRNN methods which use LSTM as the hidden units, FTS-BRNN uses the gated recurrent unit (GRU) [3] as the hidden units.", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Here, we use quiz bowl as an example to introduce factoid QA [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Quiz bowl can be seen as a kind of text classification task [2] or QA task [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Quiz bowl can be seen as a kind of text classification task [2] or QA task [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 12, "context": "RNN is proposed for processing sequential data containing several time steps, which has been widely used in NLP tasks, including neural machine translation (NMT) [13, 1] and QA and so on.", "startOffset": 162, "endOffset": 169}, {"referenceID": 0, "context": "RNN is proposed for processing sequential data containing several time steps, which has been widely used in NLP tasks, including neural machine translation (NMT) [13, 1] and QA and so on.", "startOffset": 162, "endOffset": 169}, {"referenceID": 0, "context": "In [1, 14], a concatenating operation h = [f , b] is adopted.", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In [1, 14], a concatenating operation h = [f , b] is adopted.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [9], an output activation function o = f(WL \u00b7 [f , b]) is used, which can preserve the vector dimensionality with WL \u2208 Rd\u00d72d where d is the dimensionality of f (t) and b.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "The way used in [15] is similar to that in [9], which uses o =W \u00b7 f (t) + U \u00b7 b + bias without activation functions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "The way used in [15] is similar to that in [9], which uses o =W \u00b7 f (t) + U \u00b7 b + bias without activation functions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Typical gated RNN includes long short-term memory (LSTM) [6] and gated recurrent unit (GRU).", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "GRU is first proposed in [3] for NMT task.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "Furthermore, different from some existing BRNN methods like QA-LSTM [14] which directly concatenate the hidden states of the forward RNN and backward RNN as output, FTS-BRNN adds an output layer on BRNN.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Most existing methods, such as QA-LSTM [14], use pooling to generate the question representation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "In [9], the authors also use similar loss as that in (2).", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "However, the motivation of [9] is to perform fair comparison with QANTA because QANTA also uses loss (supervision) at each node (step) in the recursive neural networks.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "Hence, the authors of [9] do not explicitly claim that full-time supervision is the key in BRNN for QA tasks because they do not perform any empirical comparison between full-time supervision and pooling-based supervision.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "Furthermore, LSTM is adopted in [9], but our FTS-BRNN adopts GRU.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "1 Dataset We use the factoid QA datasets from [7] for evaluation2.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Similar to [7], we first filter out all questions that do not belong to history or literature, and then only the answers that occur at least six times will be used.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "edu/~miyyer/qblearn/ which are provided by the authors of [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "The publically available datasets for download are slightly smaller than those used in [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "\u2022 BOW [7]: BOW treats each answer as a class label, and adopts the LR classifier for classification based on the bag-of-words (BOW) features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "\u2022 BOW-DT [7]: BOW-DT is similar to BOW method by using LR on BOW features.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "\u2022 QANTA [7]: QANTA is a recursive neural network based method proposed in [7].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "\u2022 QANTA [7]: QANTA is a recursive neural network based method proposed in [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 13, "context": "\u2022 QA-LSTM [14]: QA-LSTM is a LSTM based BRNN method [14] which computes the loss after using a pooling operation and uses concatenating operation to generate the hidden states h for each time step.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "\u2022 QA-LSTM [14]: QA-LSTM is a LSTM based BRNN method [14] which computes the loss after using a pooling operation and uses concatenating operation to generate the hidden states h for each time step.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "As that in QANTA [7], we use the 100-dimensional pre-trained word embedding provided by GloVe [11] to represent the input words for all deep methods.", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "As that in QANTA [7], we use the 100-dimensional pre-trained word embedding provided by GloVe [11] to represent the input words for all deep methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Here, \"Pooling-loss\" is the loss used in [14] and (3) while \"FTS-loss\" is our loss function in (2) for full-time supervision.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "We also try the hierarchical RNN [8, 10] which aims to deal with paragraphs or documents.", "startOffset": 33, "endOffset": 40}, {"referenceID": 9, "context": "We also try the hierarchical RNN [8, 10] which aims to deal with paragraphs or documents.", "startOffset": 33, "endOffset": 40}], "year": 2016, "abstractText": "Recently, bidirectional recurrent neural network (BRNN) has been widely used for question answering (QA) tasks with promising performance. However, most existing BRNN models extract the information of questions and answers by directly using a pooling operation to generate the representation for loss or similarity calculation. Hence, these existing models don\u2019t put supervision (loss or similarity calculation) at every time step, which will lose some useful information. In this paper, we propose a novel BRNN model called full-time supervision based BRNN (FTS-BRNN), which can put supervision at every time step. Experiments on the factoid QA task show that our FTS-BRNN can outperform other baselines to achieve the state-of-the-art accuracy.", "creator": "LaTeX with hyperref package"}}}