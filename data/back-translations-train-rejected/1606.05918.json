{"id": "1606.05918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Slack and Margin Rescaling as Convex Extensions of Supermodular Functions", "abstract": "Slack and margin rescaling are variants of the structured output SVM. They define convex surrogates to task specific loss functions, which, when specialized to non-additive loss functions for multi-label problems, yield extensions to increasing set functions. We demonstrate in this paper that we may use these concepts to define polynomial time convex extensions of arbitrary supermodular functions. We further show that slack and margin rescaling can be interpreted as dominating convex extensions over multiplicative and additive families, and that margin rescaling is strictly dominated by slack rescaling. However, we also demonstrate that, while the function value and gradient for margin rescaling can be computed in polynomial time, the same for slack rescaling corresponds to a non-supermodular maximization problem.", "histories": [["v1", "Sun, 19 Jun 2016 22:17:05 GMT  (881kb,D)", "http://arxiv.org/abs/1606.05918v1", null], ["v2", "Thu, 10 Aug 2017 08:46:02 GMT  (900kb,D)", "http://arxiv.org/abs/1606.05918v2", "v2 corrects a bug in the maths of v1"]], "reviews": [], "SUBJECTS": "cs.LG cs.DM", "authors": ["matthew b blaschko"], "accepted": false, "id": "1606.05918"}, "pdf": {"name": "1606.05918.pdf", "metadata": {"source": "CRF", "title": "Slack and Margin Rescaling as Convex Extensions of Supermodular Functions", "authors": ["Matthew B. Blaschko"], "emails": ["matthew.blaschko@esat.kuleuven.be"], "sections": [{"heading": null, "text": "Slack and Margin Rescaling are variants of the structured output SVM. They define convex surrogates to task-specific loss functions, which, when specialized in non-additive loss functions for multi-label problems, result in extensions to increasing quantity functions. We show in this paper that we can use these concepts to define polynomic time-convex extensions of arbitrary supermodular functions. We also show that Slack and Margin Rescaling can be interpreted as dominant convex extensions over multiplicative and additive families, and that margin rescaling is strictly dominated by Slack Rescaling. However, we also show that while functional value and gradient for margin rescaling can be compressed in polynomial time, the same corresponds to a non-supermodular maximization problem for Slack Rescaling."}, {"heading": "1 Introduction", "text": "It is known that submodular minimization is strongly polynomially solvable over time, while non-submodular minimization NP is difficult to find even an O (n1 \u2212 \u03b5) approximation. However, NP hardness is a worst-case analysis of a number of problems. Therefore, we take a more differentiated view and try to characterize a class of piecemeal-linear convex extensions of set functions that minimize (within a certain class) the likelihood of a non-integral solution to the corresponding LP relaxation when problem instances are sampled from a given distribution. We do this by imagining a dominant convex extension of set functions, i.e. one that is closer to the convex closure at all points within the unit cube. In this thesis, we examine analog strategies analogous to margin and loose compression of structured output support vector machines [8] for quantity minimization in general and supernumerical theory."}, {"heading": "1.1 Related Work", "text": "Supermodular minimization (submodular maximization) has been extensively studied both in the context of relaxation and for fully combinatorial approximation algorithms [5]. Notable contributions include the multilinear extension [9, 10] (mentioned above and in Appendix B), which, although not convex, was used in an approximation framework. A lot of work has been done in conjunction with random field models with small maximum click size [6], including roof duality [4], or through additional assumptions on the structure of the chart such as balancedness [11]. The link between the structured output SVM [8] and convex extensions of set functions seems to be due to [12]. The comparative difficulty of optimizing loosening and margin rescalation is well known, and an interesting approach to optimizing loose rescaling by multiple margin rescalation can be found in [1]."}, {"heading": "2 Mathematical Preliminaries", "text": "Definition 1 (Set function): A set function is called supermodular if its negative function is called dular. (A) Definition 1 (Set function): A set function is a mapping function from the power set of a base unit V to the real functions. (A) Definition 2 (Extension): An extension of a set function is an operator that yields a continuous function via the p-dimensional unit Cube. (A) Definition of a set function is an operator that yields a continuous function via the p-dimensional unit Cube. (A) Definition 3 (Submodular set function). (A) Definition function f is called submodular if for all A B V and x V\\ B, f (A)."}, {"heading": "3 Slack and margin rescaling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Slack rescaling", "text": "Definition 6 (Slack rescaling for increasing set functions [8, 12, 13]) For an increasing set function (S1) (x) = max A V (A) (A) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (A) \u00b7 \u00b7 (A) \u00b7 \u00b7 (A) \u00b7 \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7 (A) \u00b7) \u00b7 (A) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (A) \u00b7 (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A)) (A) (A) (A) (A) (A) (A)) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) ("}, {"heading": "3.2 Margin rescaling", "text": "Similarly, we can rescaling the following convex extension based on margin: Definition 10 (Margin rescaling for supermodular g (8, 12, 13).Mg (x): = < vec (m), x > + max A (A) \u2212 m (A) \u2212 p (A) \u2212 p (A) \u2212 p (8, 12, 13). Similar to the development of S from S1, we have removed a factor of 2. It has been shown that up to a strictly positive factor, margin rescaling results in an expansion in convex ambient loss [12, Proposition 2]. We prove this next, showing necessary and sufficient conditions on the scaling of the loss function for M to achieve an expansion. Proposition 8 (Proposition 2 of [12]). For each increasing function, there is a scaling that represents such an extension."}, {"heading": "4 Conclusions", "text": "Although originally designed to empirically minimize the risk of (increasing) functions, we show that supermodularity can be used to first transform a fixed function into an increasing function with a linear number of accesses to the loss function. We have shown that while post-calculation dominates post-calculation in the sense that it is strictly similar to the convex shutter, post-calculation of post-calculation for a supermodular function corresponds to a non-supermodular maximization problem. In contrast, post-calculation of margin remains comprehensible. We have also shown that post-calculation of margin and post-calculation correspond to optimal additive or multiplicative extensions, since a compressing budget is available for an organ's access to the loss function."}, {"heading": "Acknowledgments", "text": "This work is financed by the Internal Funds KU Leuven, FP7-MC-CIG 334380 and the Research Foundation - Flanders (FWO) under project number G0A2716N."}, {"heading": "A Slack and Margin Rescaling", "text": "In this section we show how we come to definition 6 and definition 10 of definition 6 of definition and definition 10 of definition and definition 10 of definition and definition of definition of definition of definition and definition of definition (12). Definition and definition of definition 10 of definition and definition of definition 10 of definition and definition of definition of definition (12) and definition of definition (13), which is a correct definition of definition of definition and definition of definition of definition of definition 6 of definition and definition 10 of definition of definition and definition of definition of definition of definition (12) and definition of definition of the definition of the definition of the definition and definition of the definition of the definition of the definition of the definition of the definition of the definition and the definition of the definition of the definition of the definition of the definition of the definition of the definition and the definition of the definition of the definition of the definition of the definition of the definition of the definition of the definition (12) and the definition of the definition of the definition of the definition of the definition of the definition of the definition of the definition of the definition of the definition"}, {"heading": "B Multi-linear Extension", "text": "The multilinear extension is due to Vondr\u00e1k [9].Definition 11 (Multilinear extension [9]).V '(x): = \u2211 A V' (A) \u0441i-xi-j-V\\ A (1 \u2212 xj) (43) We note that V does not generally yield a convex extension implied by clause 2, the fact that V is 6 = C, and the following clause: clause 12.C \u2264 V. (44) proof. We can see that the definition of V is an expectation of boolean random variable of the functional value of \"for the quantities defined by these random variables. Consequently, the multilinear extension lies entirely within the convex envelope of the points defined in the evidence for clause 2, and the values of the multilinear extension must therefore be pointedly greater than the values of the lower envelope corresponding to the convex closure."}, {"heading": "C Extensions through modular upper bounds on submodular functions", "text": "In this section we show that a known and frequently used upper limit for submodular functions (A) (A) (A) is actually a convexe upper limit for a submodular function (B) defined (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (. (A). (A). (A)."}, {"heading": "D Proof of Proposition 4", "text": "Proof. To show that optimization in general is not a supermodular maximization, we can consider the following counterexample: V = {a, b}, '(\u2205) =' ({a}) = '({b}) = 0', ({a, b}) = \u03b5. For \u03b5 > 0 this is strictly supermodular. Significantly, \"(A): = '(1 \u2212 | A | + \u2211 i-xi) (53) For x = 0 we have that\" (\u2205) = \"(\u0430) =\" ({a}) = \"({b}) = 0 and\" ({a, b}) = \u03b5 (1 \u2212 2 + 0) = \u2212 \u03b5, but this indicates that \"= \u2212\" and both cannot be supermodular."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Slack and margin rescaling are variants of the structured output SVM. They define<lb>convex surrogates to task specific loss functions, which, when specialized to non-<lb>additive loss functions for multi-label problems, yield extensions to increasing set<lb>functions. We demonstrate in this paper that we may use these concepts to define<lb>polynomial time convex extensions of arbitrary supermodular functions. We further<lb>show that slack and margin rescaling can be interpreted as dominating convex<lb>extensions over multiplicative and additive families, and that margin rescaling is<lb>strictly dominated by slack rescaling. However, we also demonstrate that, while the<lb>function value and gradient for margin rescaling can be computed in polynomial<lb>time, the same for slack rescaling corresponds to a non-supermodular maximization<lb>problem.", "creator": "TeX"}}}