{"id": "1606.05934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Adapting ELM to Time Series Classification: A Novel Diversified Top-k Shapelets Extraction Method", "abstract": "ELM (Extreme Learning Machine) is a single hidden layer feed-forward network, where the weights between input and hidden layer are initialized randomly. ELM is efficient due to its utilization of the analytical approach to compute weights between hidden and output layer. However, ELM still fails to output the semantic classification outcome. To address such limitation, in this paper, we propose a diversified top-k shapelets transform framework, where the shapelets are the subsequences i.e., the best representative and interpretative features of each class. As we identified, the most challenge problems are how to extract the best k shapelets in original candidate sets and how to automatically determine the k value. Specifically, we first define the similar shapelets and diversified top-k shapelets to construct diversity shapelets graph. Then, a novel diversity graph based top-k shapelets extraction algorithm named as \\textbf{DivTopkshapelets}\\ is proposed to search top-k diversified shapelets. Finally, we propose a shapelets transformed ELM algorithm named as \\textbf{DivShapELM} to automatically determine the k value, which is further utilized for time series classification. The experimental results over public data sets demonstrate that the proposed approach significantly outperforms traditional ELM algorithm in terms of effectiveness and efficiency.", "histories": [["v1", "Mon, 20 Jun 2016 00:59:11 GMT  (255kb,D)", "http://arxiv.org/abs/1606.05934v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qiuyan yan", "qifa sun", "xinming yan"], "accepted": false, "id": "1606.05934"}, "pdf": {"name": "1606.05934.pdf", "metadata": {"source": "CRF", "title": "Adapting ELM to Time Series Classification: A Novel Diversified Top-k Shapelets Extraction Method", "authors": ["Qiuyan Yan", "Qifa Sun", "Xinming Yan"], "emails": ["yanqy@cumt.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it will be able to put itself at the top of the group in the way that it has put itself at the top of the group."}, {"heading": "2 Preliminary", "text": "Extreme Learning Machine (ELM) is a generalized, single-layer, hidden network. In ELM, the parameters of the hidden layer node are mathematically calculated instead of iteratively tuned; therefore, it offers good generalization performance at a speed a thousand times faster than traditional, popular learning algorithms for hidden neural networks. Suppose there are N arbitrarily different training instances (xi, ti), where xi = [xi1, xi2,..., xin] T-Rn and ti = [ti1, ti2,..., tin] T-Rm, standard SLFNs with N-hidden nodes and activation function g (x) are mathematically modeled."}, {"heading": "3 Diversified Top-k Shapelets Transformed ELM", "text": "In this section we will discuss three parts of our work (1) Construction of the diversity diagram of the shapelets candidates, (2) Query of diversified top k shapelets, and (3) Transformation of data based on diversified top k shapelets and application in ELM. Below, the three above are discussed separately. Before removing the redundant shapelets, we must first determine the shapelets candidates. The original shapelets extraction algorithm is time consuming and complex, O (n2m4), n is the number of time series in the dataset, m is the length of each time series. To improve the efficiency of the shapelets-based classification method, we will follow the method proposed in [9], which has transformed the datasets by the SAX method and reduced the time complexity to O (nm2)."}, {"heading": "3.1 Construction the diversity graph of shapelets candidates", "text": "In our thesis, we use the diversity graph [16] to find a general method to extract diversified top-k formulas. Given I am a shapelets candidate set, I = {s1,.. sn}, and n is the number of I. The question is how to measure the similarity of two shapelets and how to define the diversified top-k formulas. So, first, we give the two definitions. Definition 1: Similar formulas. Given are two shapelets si and sj, which represent the same class, 1 \u2264 i, j \u2264 n, i 6 = j and n is the number of shapelets candidates. The optimal slit point of si and sj are < si > and < sj, dj >, the slit threshold are di and dj."}, {"heading": "3.2 Diversified Top-k Shapelets Extraction", "text": "Traditional top-k query returns only the objects with the largest k score, but diversified top-k query affects not only the score value, but also the similarity algorithm 1 conShapeletGraph (allShapelets) Input: Shapelets candidates allShapelets Output: Divergence Shapelets Graph 1: Graph =? 2: sort (allShapelets) 3: for i = 1 to | allShapelets | do 4: Graph.add (allShapelets [i]) 5: end for 6: for j = 1 to | allShapelets | do 7: for k = 1 to | allShapelets | do 8: if (allShapelets [j] \u2248 allShapelets [k]) then 9: Graph [j].add (Graph [j].add) 10: Graph [k].add (Graph [j]] [k]."}, {"heading": "3.3 Diversified Top-k Shapelets Transform for ELM", "text": "After getting diversified top-k shapelets = D shapelets, we can use these shapelets to transform data before the ELM classification. For each data instance Ti, the subsequence distance between Ti and Sj is calculated, Sj is a shapelet in top-k shapelets. The resulting k distances are used to create a new instance of transformed data, with each attribute corresponding to the distance from each shapelet to the original time series."}, {"heading": "4 Experiments", "text": "To evaluate our proposed methods, we selected 15 data sets from the UCR time series repository (listed in Table 1). We use a simple train test split and all reported results are test accuracy. All shapelets candidate selection, top k diversification of shapelets extraction and classification construction are performed on the training set. All experiments are performed in Java within the Weka framework."}, {"heading": "4.1 Determination of shapelets length and \u03ba", "text": "There are two parameters min and max in the procedure of generating shapelets candidates. The two parameters determine the length of the shapelets candidates that can influence the finding of the best representative shapelets. In the following [11] we define the minimum and maximum length of the partial sequences for creating shapeless: m / 11 and m / 2 separated, m is the length of each time series. To first explain how the k value affects the accuracy of the classification, we test the average accuracy of six classifiers in fifteen data sets with the varying k value. As shown in Figure 2, the average classification accuracy increases with the increase of the k value first and then becomes stable when k is 9."}, {"heading": "4.2 Representation of Optimal Shapelets Sets", "text": "In this section we want to get a visual overview of what the optimal shapelets actually were. Since the reference [15] confirmed that ShapeletSelection can remove more redundant shapelets than other similar methods, we compared the optimal shapelets sets only between DivTopkShapelets and ShapeletSelection, if the two algorithms all have the best classification, as in Fig. 3. The optimal shapelets sets were determined from ShapeletSelection with k = 8 (Fig.3-a) and from DivTopkShapelet with k = 2 (Fig.3-b)."}, {"heading": "4.3 Accuracy Comparison", "text": "In this section, we have selected six traditional time series classification algorithms, including C4.5, 1NN, Naive Bays (NaB), BayesianNetwork (BaN), RandomForest (RaF), and RotationForest (RoF), to compare accuracy with our proposed methods.Accuracy comparison with traditional classification First, we use directly selected classification algorithms to classify the datasets. Second, we use DivTopkShapelets (set k = 9) to extract optimal form sets and transform data, and then we transform transformed datasets with six selected classification algorithms. The results are presented in Table 1, which column headers with the classification name plus S (see C4.5 (S)) mean that DivTopkShapelets have transformed the classification results. From the table, we can see that compared to traditional classification algorithms, divelograph sets, divelograph sets, divelocity, and shape sets improve accuracy."}, {"heading": "4.4 Runtime comparison", "text": "DivShapELM has three additional operations: Shapelets candidate selection, diversified shapelets selection, and data transformation. Once the transformed data is processed, the residual process is a standard classification process. Table 4 indicates the additional time and classification time of DivShapELM and ELM. The time cost of choosing diversified shapelets varies with datasets, but can be done offline. Since DivShapELM can transform a dataset of n \u00d7 m length into a Rn \u00d7 k matrix (k m), the runtime of DivShapELM can be greatly reduced. As shown in Table 4, DivShapELM has the shorter classification time to 12 of 15 datasets."}, {"heading": "5 Conclusion and Future Work", "text": "Our work consists of three parts: (1) We present two concepts of similar shapes and diversified top-k shapelets based on these concepts, a method of constructive diversity shapelets graphics is presented, (2) we propose a diversified top-k shapelets extraction method called DivTopkShapelte to find out all the most representative and interpretive features of each class, and (3) we propose a ShapELM transformed shapelets algorithm called DivShapELM that automatically determines the k value and receives the diversified top-k shapelets to improve the performance of ELM. Experiments show that DivShapELM can improve the efficiency and interpretation capability of ELM. In addition, we experimentally verify that DivTopkShapelte is an excellent method of extracting features that can improve traditional timing."}], "references": [{"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["Huang G-B", "Zhu Q-Y", "Siew C-K"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Weighted extreme learning machine for imbalance", "author": ["W. Zong", "G.-B. Huang", "Y. Chen"], "venue": "learning. Neurocomputing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Self-adaptive evolutionary extreme learning machine", "author": ["J. Cao", "Z. Lin", "G.-B. Huang"], "venue": "Neural Processing Letters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "BETAWARE: a machine-learning tool to detect and predict transmembrane beta barrel proteins in Prokaryotes", "author": ["C. Savojardo", "P. Fariselli", "R. Casadio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Projective Feature Learning for 3D Shapes with Multi-View Depth Images.Pacific Graphics,24(7):111,2015", "author": ["Z. Xie", "K. Xu", "W. Shan", "L. Liu", "Y. Xiong", "H. Huang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Improving ELM-based microarray data classification by diversified sequence features selection", "author": ["Yuhai Zhao", "Guoren Wang", "Ying Yin", "Yuan Li", "Zhanghui Wang"], "venue": "Neural Comput&Applic,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L Ye", "E. Keogh"], "venue": "Proc 15th ACM SIGKDD : 947-956,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Logical-shapelets: an expressive primitive for time series classification", "author": ["A Mueen", "E Keogh", "N. Young"], "venue": "In:Proc 17th ACM SIGKDD : 1154-1162,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Fast shapelets: A scalable algorithm for discovering time series shapelets", "author": ["T Rakthanmanon", "E. Keogh"], "venue": "In:Proc 13th SDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A shapelet transform for time series classification", "author": ["J Lines", "M Davis L", "J Hills"], "venue": "In:Proc 18th ACM SIGKDD :289-297,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Classification of time series by shapelet transformation", "author": ["J Hills", "J Lines", "E Baranauskas"], "venue": "Data Mining and Knowledge", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Clustering time series using unsupervised-shapelets", "author": ["J Zakaria", "A Mueen", "E. Keogh"], "venue": "In Proc. 12th ICDM : 785-794,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Extracting Interpretable Features for Early Classification on Time Series", "author": ["Z Xing", "J Pei", "Y Philip S"], "venue": "In Proc.11th SDM :247-258,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Efficient pattern-based time series classification on gpu", "author": ["W Chang K", "B Deka", "W Hwu W M"], "venue": "In Proc.12th ICDM :131-140,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Shapelet pruning and shapelet coverage for time series classification", "author": ["JD Yuan", "ZH Wang", "M. Han"], "venue": "Journal of Software,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Diversifying top-k results", "author": ["L Qin", "X Yu J", "L. Chang"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Robust Subspace Clustering for Multi-View Data by Exploiting Correlation Consensus", "author": ["Y Wang", "X Lin", "L Wu"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Unsupervised Metric Fusion Over Multiview Data by Graph Random Walk-Based Cross-View Diffusion", "author": ["Y Wang", "W Zhang", "L Wu", "X Lin", "X. Zhao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Effective Multi-Query Expansions: Robust Landmark Retrieval", "author": ["Y Wang", "X Lin", "L Wu", "W. Zhang"], "venue": "ACM Multimedia:", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "LBMCH: Learning Bridging Mapping for Cross-modal Hashing", "author": ["Y Wang", "X Lin", "L Wu", "W Zhang", "Q. Zhang"], "venue": "ACM SIGIR:", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Towards metric fusion on multi-view data: a cross-view based graph random walk approach", "author": ["Y Wang", "X Lin", "Q. Zhang"], "venue": "ACM CIKM : 805-810,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Efficient image and tag co-ranking: a bregman divergence optimization method", "author": ["L Wu", "Y Wang", "J Shepherd"], "venue": "ACM Multimedia:", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Exploiting Correlation Consensus: Towards Subspace Clustering for Multi-modal Data", "author": ["Y Wang", "X Lin", "L Wu", "W Zhang", "Q. Zhang"], "venue": "ACM Multimedia:", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Extreme Learning Machine (ELM for short) was originally developed based on single hidden-layer feed forward neural networks (SLFNs) [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].", "startOffset": 29, "endOffset": 34}, {"referenceID": 3, "context": "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].", "startOffset": 75, "endOffset": 80}, {"referenceID": 4, "context": "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].", "startOffset": 75, "endOffset": 80}, {"referenceID": 5, "context": "In the context of feature selection in time series data analysis, most of the current methods adopt such a framework that ranks subsequences according to their individual discriminative power to the target class and then selects top-k ranked subsequences[6].", "startOffset": 254, "endOffset": 257}, {"referenceID": 6, "context": "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].", "startOffset": 121, "endOffset": 126}, {"referenceID": 8, "context": "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].", "startOffset": 121, "endOffset": 126}, {"referenceID": 9, "context": "Shapelets transformed classification methods [10,11] were proposed to separate the processing of shapelets selection and classification.", "startOffset": 45, "endOffset": 52}, {"referenceID": 10, "context": "Shapelets transformed classification methods [10,11] were proposed to separate the processing of shapelets selection and classification.", "startOffset": 45, "endOffset": 52}, {"referenceID": 11, "context": "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].", "startOffset": 115, "endOffset": 125}, {"referenceID": 12, "context": "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].", "startOffset": 115, "endOffset": 125}, {"referenceID": 13, "context": "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].", "startOffset": 115, "endOffset": 125}, {"referenceID": 10, "context": "Some works [11,15] detect this problem and use clustering or pruning methods to remove the redundant,but still exist redundant shapelets, also, the k value is determined from experiments.", "startOffset": 11, "endOffset": 18}, {"referenceID": 14, "context": "Some works [11,15] detect this problem and use clustering or pruning methods to remove the redundant,but still exist redundant shapelets, also, the k value is determined from experiments.", "startOffset": 11, "endOffset": 18}, {"referenceID": 8, "context": "In order to improve the efficiency of shapelets based classification method, we follow the method proposed in [9], which transformed the data sets through SAX method and decreased the time complexity to O(nm).", "startOffset": 110, "endOffset": 113}, {"referenceID": 15, "context": "In our work, we use the diversity graph[16] to find a general method to extract diversified top-k shapelets.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "According to [16], find top-k results falling into two categories: incremental manner and bounding manner.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Followed [11], we set min-length and max-length of subsequences to generate shapeless are m/11 and m/2 separately, m is the length of each time series.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "Because reference[15] has verified that ShapeletSelection can remove more redundant shapelets than other similar methods, we only compared the optimal shapelets sets between DivTopkShapelets and ShapeletSelection when the two algorithms all have the best classification,as shown in Fig.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 17, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 18, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 19, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 20, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 21, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}, {"referenceID": 22, "context": "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.", "startOffset": 72, "endOffset": 94}], "year": 2016, "abstractText": "ELM (Extreme Learning Machine) is a single hidden layer feed-forward network, where the weights between input and hidden layer are initialized randomly. ELM is efficient due to its utilization of the analytical approach to compute weights between hidden and output layer. However, ELM still fails to output the semantic classification outcome. To address such limitation, in this paper, we propose a diversified top-k shapelets transform framework, where the shapelets are the subsequences i.e., the best representative and interpretative features of each class. As we identified, the most challenge problems are how to extract the best k shapelets in original candidate sets and how to automatically determine the k value. Specifically, we first define the similar shapelets and diversified top-k shapelets to construct diversity shapelets graph. Then, a novel diversity graph based top-k shapelets extraction algorithm named as DivTopkshapelets is proposed to search top-k diversified shapelets. Finally, we propose a shapelets transformed ELM algorithm named as DivShapELM to automatically determine the k value, which is further utilized for time series classification. The experimental results over public data sets demonstrate that the proposed approach significantly outperforms traditional ELM algorithm in terms of effectiveness and efficiency.", "creator": "LaTeX with hyperref package"}}}