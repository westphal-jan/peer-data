{"id": "1606.05990", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "A New Training Method for Feedforward Neural Networks Based on Geometric Contraction Property of Activation Functions", "abstract": "We propose a new training method for a feedforward neural network having the activation functions with the geometric contraction property. The method consists of constructing a new functional that is less nonlinear in comparison with the classical functional by removing the nonlinearity of the activation functions from the output layer. We validate this new method by a series of experiments that show an improved learning speed and also a better classification error.", "histories": [["v1", "Mon, 20 Jun 2016 07:05:14 GMT  (454kb,D)", "http://arxiv.org/abs/1606.05990v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["petre birtea", "cosmin cernazanu-glavan", "alexandru sisu"], "accepted": false, "id": "1606.05990"}, "pdf": {"name": "1606.05990.pdf", "metadata": {"source": "CRF", "title": "A New Training Method for Feedforward Neural Networks Based on Geometric Contraction Property of Activation Functions", "authors": ["Petre Birtea", "Cosmin Cern\u0103zanu\u2013Gl\u0103van", "Alexandru \u015ei\u015fu"], "emails": ["birtea@math.uvt.ro", "cosmin.cernazanu@cs.upt.ro;", "sisu.eugen@gmail.com"], "sections": [{"heading": null, "text": "MSC: 92B20, 68T05 Keywords: Feedforward Neural Network, training algorithm, backpropagation, contraction, optimization"}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them are able to surpass themselves by taking themselves to task. (...) In fact, it is such that they are able to survive themselves. (...) It is such that they are able to survive themselves. (...) It is such that they are able to survive themselves. (...) It is such that they are able to survive themselves. (...) It is such that they are able to survive themselves. (...). \"\" It is as if they are able to survive themselves. (...). \"(...).\" (...). \"(.).\" (....... \"(.).\" (.). \"(.).\" (...). \"(.).\" (....). \"(.).\" (.). \"(. (.).\" (.). \"(.).\" (. (.). \"(.).\" (. \"(.)..\" (.). \"(.).\" (.). \"(.).....\" (. \"(.).\" (.). \"(.)...\" (..).. \"(.).\" (.).... \"(.)......).......\" (. \"(....)..\" (.). \"(.).\" (.). \"(..\" (.).)........ \"(.)..\" (...)....... \"(.).\" (. \"(.)..)......\" (.).. \"(.\" (.)..).... \"(.\" (.)..). \"(.....\" (..).. \"(..)...........\" (.....)....... \"(....)..\"). \"(..........................\").."}, {"heading": "2 Canceling the non-linearity on the output layer", "text": "Our idea is to modify the initial cost function by removing the non-linearity of the activation function at the output level. (...) This leads to a reduction in the amount of the calculation for determining the optimal values of the weight matrices, which represent a minimum value of the cost function. (...) To do this, we need the following hypotheses: \u2022 the activation function s is a diffeomorphism in its definition domain and, moreover, it is a contraction (i.e. the derivative of s is strictly less than 1 over the entire interval of the definition. \u2022 the coordinates of the output vectors y belong to the co-domain of the activation function. The output vectors y can be the same if the activation function s allows to see more closely the discussion in the next section.n that we propose the following modified cost function: C (W, W, W, W, W, W, W, W, W, N)."}, {"heading": "3 Experiments", "text": "We conducted three experiments with 10 neurons each (one neuron layer for each of them).For the first experiment, we looked at a neural network with the following configuration: the input layer contains 784 neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by three hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons, followed by two hidden neurons."}, {"heading": "4 Conclusions", "text": "The main advantage of the new function lies in the fact that we have fewer nonlinearities introduced by activation functions. Experiments conducted by us show that our method leads to faster learning convergence and also to a better recognition rate (classification error). In the first experiment after 47,750 epochs, the classical method has a classification error of 3.05 compared to our method, which results in a classification error of 2.59. This behavior also remains valid for the next two experiments, in which we obtained the classification error for the classical method 2.97 compared to 2.00 with the new method, or 4.5 classification error, which was achieved with the classical method compared to 2.79."}], "references": [{"title": "Adaptive dropout for training deep neural networks", "author": ["J. Ba", "B. Frey"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The dropout learning algorithm", "author": ["P. Baldi", "P. Sadowski"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A class of methods for solving nonlinear simultaneous equations", "author": ["Broyden", "C. G"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1965}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "An empirical study of learning speed in back-propagation networks", "author": ["S.E. Fahlman"], "venue": "Technical Report CMU-CS-88-162, Carnegie-Mellon Univ", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "A rapidly convergent descent method for minimization", "author": ["R. Fletcher", "M.J. Powell"], "venue": "The Computer Journal,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1963}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A family of variable-metric methods derived by variational means", "author": ["D. Goldfarb"], "venue": "Mathematics of computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1970}, {"title": "Comparing biases for minimal network construction with back-propagation", "author": ["S.J. Hanson", "L.Y. Pratt"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Methods of conjugate gradients for solving linear systems", "author": ["M.R. Hestenes", "E. Stiefel"], "venue": "Journal of research of the National Bureau of Standards,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1952}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Technical Report arXiv:1207.0580", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A simple weight decay can improve generalization", "author": ["A. Krogh", "J.A. Hertz"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "A method for the solution of certain problems in least squares", "author": ["K. Levenberg"], "venue": "Quarterly of applied mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1944}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Preattentive texture discrimination with early vision mechanisms", "author": ["J. Malik", "P. Perona"], "venue": "Journal of the Optical Society of America", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "An algorithm for least-squares estimation of nonlinear parameters", "author": ["D.W. Marquardt"], "venue": "Journal of the Society for Industrial & Applied Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1963}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(N) time", "author": ["M.F. M\u00f8ller"], "venue": "Technical Report PB-432,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A method of solving a convex programming problem with convergence rate of 1k2", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1983}, {"title": "Fast exact multiplication by the Hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "In Proc. 30th International Conference on Machine Learning (ICML)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Conjugate directions for stochastic gradient descent", "author": ["N.N. Schraudolph", "T. Graepel"], "venue": "Proc. Intl. Conf. Artificial Neural Networks (ICANN),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Conditioning of quasi-Newton methods for function minimization", "author": ["D.F. Shanno"], "venue": "Mathematics of computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1970}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Generalization by weightelimination with application to forecasting", "author": ["A.S. Weigend", "D.E. Rumelhart", "B.A. Huberman"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1991}, {"title": "Adaptive back-propagation in on-line learning of multilayer networks", "author": ["A.H.L. West", "D. Saad"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1995}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "Y.B.L. Bottou", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 96, "endOffset": 99}, {"referenceID": 26, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Levenberg-Marquardt [15], [18], [25] improved the convergence speed.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "Levenberg-Marquardt [15], [18], [25] improved the convergence speed.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 16, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 229, "endOffset": 233}, {"referenceID": 20, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 235, "endOffset": 239}, {"referenceID": 15, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 241, "endOffset": 245}, {"referenceID": 7, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 12, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 252, "endOffset": 256}, {"referenceID": 3, "context": "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].", "startOffset": 258, "endOffset": 261}, {"referenceID": 9, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 156, "endOffset": 168}, {"referenceID": 28, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 156, "endOffset": 168}, {"referenceID": 13, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 156, "endOffset": 168}, {"referenceID": 23, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 5, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 185, "endOffset": 188}, {"referenceID": 29, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].", "startOffset": 234, "endOffset": 238}, {"referenceID": 27, "context": "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].", "startOffset": 90, "endOffset": 94}, {"referenceID": 4, "context": "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].", "startOffset": 104, "endOffset": 107}, {"referenceID": 31, "context": "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "We have done three experiments, in all three we have used the MNIST dataset [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "In all three experiments we have chosen neural network configurations already used in literature, see [32].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "[32] report a classification error of 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] report a classification error of 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] report a classification error of 4.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We propose a new training method for a feedforward neural network having the activation functions with the geometric contraction property. The method consists of constructing a new functional that is less nonlinear in comparison with the classical functional by removing the nonlinearity of the activation functions from the output layer. We validate this new method by a series of experiments that show an improved learning speed and also a better classification error. MSC: 92B20, 68T05", "creator": "LaTeX with hyperref package"}}}