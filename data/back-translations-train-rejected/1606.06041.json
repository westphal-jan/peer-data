{"id": "1606.06041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Bandit-Based Random Mutation Hill-Climbing", "abstract": "The Random Mutation Hill-Climbing algorithm is a direct search technique mostly used in discrete domains. It repeats the process of randomly selecting a neighbour of a best-so-far solution and accepts the neighbour if it is better than or equal to it. In this work, we propose to use a novel method to select the neighbour solution using a set of independent multi- armed bandit-style selection units which results in a bandit-based Random Mutation Hill-Climbing algorithm. The new algorithm significantly outperforms Random Mutation Hill-Climbing in both OneMax (in noise-free and noisy cases) and Royal Road problems (in the noise-free case). The algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive.", "histories": [["v1", "Mon, 20 Jun 2016 09:53:29 GMT  (86kb,D)", "http://arxiv.org/abs/1606.06041v1", "7 pages, 10 figures"]], "COMMENTS": "7 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["jialin liu", "diego pe\\'rez-lie\\'bana", "simon m lucas"], "accepted": false, "id": "1606.06041"}, "pdf": {"name": "1606.06041.pdf", "metadata": {"source": "CRF", "title": "Bandit-Based Random Mutation Hill-Climbing", "authors": ["Jialin Liu", "Diego P\u00e9rez-Li\u00e9bana", "Simon M. Lucas"], "emails": ["jialin.liu@essex.ac.uk", "dperez@essex.ac.uk", "sml@essex.ac.uk"], "sections": [{"heading": null, "text": "In fact, it is the case that most people are able to determine themselves what they want and what they do not want. (...) It is not the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that they are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case. \"(...) It is so.\" (...) It is so. (...) It is so. (...) It is like this. (...) It is like this. (...) It is like this. (...) It is like this."}, {"heading": "II. TEST PROBLEMS", "text": "In this paper we look at two benchmark optimization problems in a binary search space."}, {"heading": "A. OneMax Problem", "text": "The OneMax problem [31] is a simple linear problem that aims to maximize the number of 1 of a binary string, i.e. for a given n-bit string sf (s) = n \u2211 i = 1 si, (1) where si, the ith bit in the string s, is either 1 or 0. The complexity of the OneMax problem is O (n log (n) for an n-bit string [32]. Doerr et al. has proven that the black box complexity with memory constraint one is at most 2n [33]. Further lower and upper limits of the complexity of OneMax in the various models are analyzed [34], [35] and subsequently summarized in Table 1 of [35]. In their elite model, only the best solution to date can be stored in memory."}, {"heading": "B. Noisy OneMax Problem", "text": "We modify the OneMax problem by introducing an additive noise with a constant deviation of 1: f \u2032 (s) = f (s) + N (0, 1), (2) N. Thus, the standard deviation of noise is of the same magnitude as the differences between fitness values. It is noteworthy that our noise model is very different from the one in [36] that uses (1 + 1) -EA and a one-bit noise. The effect of noise intensity on the runtime of (1 + 1) - EA for the OneMax problem corrupted by one-bit noise is first analyzed by Droste [37]. In [37] and [36], the misclassification occurs due to the change of exactly a uniformly selected bit of noise with the probability p \u0445 (0, 1), p being the noise level. Thus, the noise behaves incorrectly before the fitness evaluation and the rated individual (solution or search point)."}, {"heading": "C. Royal Road Function", "text": "The functions of the Royal Road are initially introduced by Mitchell et al. [38]. The fitness of the function only increases if all bits in a block are tilted to 1.1. The aim was to design some hierarchical fitness landscapes and examine the performance of genetic algorithms (GA). Surprisingly, a simple algorithm of the random mutation Hill-Climbing GA exceeds that of a simple Royal Road function, namely R1 in [38], [39]. R1 consists of a list of block-composite-bit strings, as shown in Fig. 4, in the \"*\" either 0 or 1. Fitness R1 (x) is reproduced as follows: R1 = n \u2211 i = 1 ci\u03b4i (x), (3), where \u03b4i = 1 if x-si, otherwise \u0441i = 0.It is noteworthy that due to the landscapes in the Royal Road function and the 1-bit mutation per generation, the introduction of noise into fitness is not trivial."}, {"heading": "III. BANDIT-BASED RMHC", "text": "Contrary to the standard bandit terminology, where an arm is pulled to obtain a reward, the purpose of our bandits is to select the element that mutates with each iteration of the algorithm. We create a m-armed bandit for each genome gene that can take m possible values. Each bandit works by recording how many times each arm has been pulled, i.e. the number of evaluations of each arm and the difference in empirical reward between the previous fitness of the genome and the fitness obtained due to the selected mutation. Instead of pulling an arm to obtain a reward, each bandit stores a state and has m weapons in which each arm stores i, i.e. m} the statistics of a transition: Ti-Transitions (S) with1Note that OneMax can be considered a special case of the Royal Road function with a block size of 1."}, {"heading": "A. Urgency", "text": "With each iteration of the bandit-based RMHC algorithm, the bandit agents manage the selection of the gene with maximum urgency to mutate. (4) The urgency of each bandit is derived from the standard UCB equation, except that we use the normal use of the exploitation term, i.e. the first term in the RHS of Equation 5. Intuitively, this means that if a certain condition of a bandit is already good, its value should not be changed. The exploration term is there to ensure that, as the total number of iterations increases, Ni is occasionally tried an apparently inferior option. For each 2-armed bandit i, i.e. 1, 2,.., n}, the primordial cyi for the bit is defined when the value between the bit and the bit is changed."}, {"heading": "B. Noise-free case", "text": "Algorithm 1 introduces the bandit-based RMHC in silent fall. In order to solve a trouble-free problem, no resampling is necessary if the assessment number and fitness value of the previously best genome can be stored. It should be noted that for problems where calculating the fitness value is difficult or requires high computing costs, storing the fitness value of a solution is much cheaper than re-evaluating it. For further work, we are interested in applying our proposed approach to more difficult problems (such as generating and evaluating game levels [40]). Our main interest is to improve the speed of convergence to approximately optimal states. Algorithm 1 Bandit-based RMHC in silent fall. Requirement: n-N: Genome length required: m-N: Dimension of search space 1: Randomly initialize a genome x-Rm 2: Bestmutation SoFar-Update (x) 1. Total number of the genome length required during element x: N is not met."}, {"heading": "C. Noisy case", "text": "Now we look at the noisy case. Bandit-based RMHC in the noisy case is formalized in algorithm 2. In the noisy case, the best genome so far requires multiple evaluations to reduce the effect of noise, this becomes resampling.The statistics of the best genome so far are stored, thanks to which, instead of directly comparing the fitness values of the offspring with the best genome so far, the average fitness value of the best genome so far for each generation is compared. Therefore, the computing costs associated with evaluating the genome, as opposed to the computing costs of this algorithm, determine. Resampling has proven to be a powerful tool to improve the local performance of EAs in the noisy optimization [41], [42] and a variety of resampling rules applied to EAs in the continuous noisy optimization process, as opposed to OAs in the mutation of -aus- Qian et al. [44] it has been shown theoretically and empirically that resampling is not a resampling with 1, but with 1 greater Bling."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We first apply our proposed algorithm to the OneMax problem and the Royal Road function R1 in a silent fall and then evaluate the performance of our algorithm to the algorithm 2 Bandit-based RMHC in the loud case. Required: n-N value: genome length Required: m-N value: Dimension of the search area 1: Randomly initialize a genome x value 2: bestF itSoFar value (x) 3: M value 1. Rating number of the best genome so far 4: N value 1. Overall rating number 5: While the time has not elapsed, select the element i value for mutation using Eqs. 4 and 5 7: y value after mutation of the element i value of x 8: Fitx value of fitness (x) 9: Fitx value of fitness (y) 10: N value of fitness (y)."}, {"heading": "A. OneMax", "text": "The results of the noise-free OneMax problem of different dimensions are shown in Fig. 2. In the noise-free case, the average fitness ratings used by Bandit-based RMHC to solve the problem are close to the problem dimension, while the original RMHC requires approximately 5 times more budget.Fig. 3 illustrates the empirical average number of fitness ratings required to achieve the optimal value with RMHC and Bandit-based RMHC in the OneMax problem of different dimensions with constant variance noise (\u0445 N (0.1)).The resampling number in the loud case is given between the brackets. For comparison, the results in the noise-free OneMax problem with constant variance noise are included. \u2022 When using the RMHC, a higher resampling number (10) leads to faster convergence with the optimum (Fig. 3a) with a high-dimensional number of problems leading to a low or low resampling of a small number of problems."}, {"heading": "B. Royal Road", "text": "Fig. 4 shows the empirical average fitness ratings required to find the optimum noise-free Royal Road function using RMHC or bandit-based RMHC. At a fixed length, a larger block size leads to a harder problem and more fitness ratings. However, the ratio of average fitness ratings to the problem dimension increases with the problem dimension when a small resampling number is used (Fig. 3c). For an identical problem, RMHC on bandit basis requires much less fitness ratings than RMHC to find the optimal solution. From the curves, it appears that for an identical block size using a bandit-based RMHC, the total number of ratings scales linearly with the problem dimension. To find the optimal sequence of 8 size 8 blocks, our bandit-based RMHC uses half as many correct function ratings as the RMHC model used by Mitchell et al was the most efficient [algorithm 39] in their experiment."}, {"heading": "V. CONCLUSION", "text": "In this paper, the first Bandit-based Random Mutation Hill Climber (RMHC) was presented - a simple but effective type of evolutionary algorithm that was compared to the standard RMHC for the OneMax problem and the Royal Road function. Tests were also performed using a noisy OneMax problem along with a resampling in each algorithm to mitigate the effects of noise. In noiseless and noisy OneMax problems and the Royal Road function, our bandit-based RMHC algorithm clearly executes the RMHC, in some cases using a factor of ten fewer ratings in the noisy case. In addition, the fitness ratings required by the bandit-based RMHC are a near-linear function of the problem dimension if the resampling number is 2. An identical Royal Road function requires 8 blocks of size 8."}], "references": [{"title": "Evolutionary Operation: A Method for Increasing Industrial Productivity", "author": ["G.E. Box"], "venue": "Applied Statistics, pp. 81\u2013101, 1957.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1957}, {"title": "Digital Simulation of an Evolutionary Process", "author": ["G.J. Friedman"], "venue": "General Systems Yearbook, vol. 4, no. 171-184, 1959.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1959}, {"title": "The Use of Biological Concepts in the Analytical Study of Systems", "author": ["W. Bledsoe"], "venue": "OPERATIONS RESEARCH, vol. 9. INST OPERATIONS RESEARCH MANAGEMENT SCIENCES 901 ELKRIDGE LAND- ING RD, STE 400, LINTHICUM HTS, MD 21090-2909, 1961, pp. B145\u2013B146.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1961}, {"title": "Optimization through Evolution and Recombination", "author": ["H.J. Bremermann"], "venue": "Self-organizing systems, vol. 93, p. 106, 1962.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1962}, {"title": "Cybernetic Solution Path of an Experimental Problem", "author": ["I. Rechenberg"], "venue": "1965.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1965}, {"title": "Artificial Intelligence Through Simulated Evolution.[By", "author": ["L.J. Fogel"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1966}, {"title": "Simulation of Biological Evolution and Machine Learning: I. Selection of Self-Reproducing Numeric Patterns by Data Processing Machines, Effects of Hereditary Control, Mutation Type and Crossing", "author": ["J. Reed", "R. Toombs", "N.A. Barricelli"], "venue": "Journal of theoretical biology, vol. 17, no. 3, pp. 319\u2013342, 1967.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1967}, {"title": "Bandit Processes and Dynamic Allocation Indices", "author": ["J.C. Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 148\u2013177, 1979.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1979}, {"title": "Bandit Problems: Sequential Allocation of Experiments (Monographs on Statistics and Applied Probability)", "author": ["D.A. Berry", "B. Fristedt"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Asymptotically Efficient Adaptive Allocation Rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, vol. 6, pp. 4\u201322, 1985.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-Time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Using Confidence Bounds for Exploitation-Exploration Tradeoffs", "author": ["P. Auer"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 397\u2013422, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Use of Variance Estimation in the Multi-Armed Bandit Problem", "author": ["J.-Y. Audibert", "R. Munos", "C. Szepesvari"], "venue": "2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Pure Exploration in Multi-Armed Bandits Problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Algorithmic Learning Theory. Springer, 2009, pp. 23\u201337.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The KL-UCB Algorithm for Bounded Stochastic Bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "arXiv preprint arXiv:1102.2490, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-Armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Discounted UCB", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "2nd PASCAL Challenges Workshop, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A Sublinear-Time Randomized Approximation Algorithm for Matrix Games", "author": ["M.D. Grigoriadis", "L.G. Khachiyan"], "venue": "Operations Research Letters, vol. 18, no. 2, pp. 53\u201358, Sep 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Gambling in a Rigged Casino: the Adversarial Multi-Armed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "Proceedings of the 36th Annual Symposium on Foundations of Computer Science. IEEE Computer Society Press, Los Alamitos, CA, 1995, pp. 322\u2013331.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Differential Evolution Algorithm Applied to Non-Stationary Bandit Problem", "author": ["D.L. St-Pierre", "J. Liu"], "venue": "2014 IEEE Congress on Evolutionary Computation (IEEE CEC 2014), Beijing, Chine, Jul. 2014. [Online]. Available: http://hal.inria.fr/hal-00979456", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Differential Evolution \u2013 A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces", "author": ["R. Storn", "K. Price"], "venue": "Journal of global optimization, vol. 11, no. 4, pp. 341\u2013359, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolving Artificial Neural Networks with FINCH", "author": ["A. Benbassat", "M. Sipper"], "venue": "Proceedings of the 15th annual conference companion on Genetic and evolutionary computation. ACM, 2013, pp. 1719\u20131720.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Genetic Programming to Evolve Heuristics for a Monte Carlo Tree Search Ms Pac-Man agent", "author": ["A.M. Alhejali", "S.M. Lucas"], "venue": "Computational Intelligence in Games (CIG), 2013 IEEE Conference on. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast Evolutionary Adaptation for Monte Carlo Tree Search", "author": ["S.M. Lucas", "S. Samothrakis", "D. Perez"], "venue": "Applications of Evolutionary Computation. Springer, 2014, pp. 349\u2013360.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Bandit-Based Estimation of Distribution Algorithms for Noisy Optimization: Rigorous Runtime Analysis", "author": ["P. Rolet", "O. Teytaud"], "venue": "Learning and Intelligent Optimization. Springer, 2010, pp. 97\u2013110.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive Operator Selection with Bandits for a Multiobjective Evolutionary Algorithm based on Decomposition", "author": ["K. Li", "A. Fialho", "S. Kwong", "Q. Zhang"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 18, no. 1, pp. 114\u2013130, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning DFA: Evolution versus Evidence Driven State Merging", "author": ["S.M. Lucas", "T.J. Reynolds"], "venue": "Evolutionary Computation, 2003. CEC\u201903. The 2003 Congress on, vol. 1. IEEE, 2003, pp. 351\u2013358.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Deterministic Finite Automata with a Smart State  Labeling Evolutionary Algorithm", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 7, pp. 1063\u20131074, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Genetic Algorithms in Search Optimization and Machine Learning", "author": ["D.E. Goldberg"], "venue": "Addison-wesley Reading Menlo Park,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1989}, {"title": "On Crossover as an Evolutionarily Viable Strategy", "author": ["J.D. Schaffer", "L.J. Eshelman"], "venue": "ICGA, vol. 91, 1991, pp. 61\u201368.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1991}, {"title": "Upper and Lower Bounds for Randomized Search Heuristics in Black-Box Optimization", "author": ["S. Droste", "T. Jansen", "I. Wegener"], "venue": "Theory of computing systems, vol. 39, no. 4, pp. 525\u2013544, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Memory-Restricted Black-Box Complexity of OneMax", "author": ["B. Doerr", "C. Winzen"], "venue": "Information Processing Letters, vol. 112, no. 1, pp. 32\u201334, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Black-Box Complexity: from Complexity Theory to Playing Mastermind", "author": ["B. Doerr", "C. Doerr"], "venue": "Proceedings of the 15th annual conference companion on Genetic and evolutionary computation. ACM, 2013, pp. 617\u2013640.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "OneMax in Black-Box Models with Several Restrictions", "author": ["C. Doerr", "J. Lengler"], "venue": "Algorithmica, pp. 1\u201331, 2016. [Online]. Available: http://dx.doi.org/10.1007/s00453-016-0168-1", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Z.-H. Zhou"], "venue": "Evolutionary computation, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of the (1+ 1) EA for a Noisy OneMax", "author": ["S. Droste"], "venue": "Genetic and Evolutionary Computation\u2013GECCO 2004. Springer, 2004, pp. 1088\u20131099.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "The Royal Road for Genetic Algorithms: Fitness Landscapes and GA Performance", "author": ["M. Mitchell", "S. Forrest", "J.H. Holland"], "venue": "Proceedings of the first european conference on artificial life. Cambridge: The MIT Press, 1992, pp. 245\u2013254.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1992}, {"title": "General Video Game Level Generation", "author": ["A. Khalifa", "D. Perez-Liebana", "S.M. Lucas", "J. Togelius"], "venue": "2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "A General Noise Model and its Effects on Evolution Strategy Performance", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 10, no. 4, pp. 380\u2013391, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "The Theory of Evolution Strategies", "author": ["H.-G. Beyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Portfolio Methods in Uncertain Contexts", "author": ["J. Liu"], "venue": "Ph.D. dissertation, INRIA, 12 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Y. Jin", "Z.-H. Zhou"], "venue": "Parallel Problem Solving from Nature\u2013PPSN XIII. Springer, 2014, pp. 302\u2013311.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal Credit Assignment in Reinforcement Learning", "author": ["R.S. Sutton"], "venue": "1984.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1984}, {"title": "Investigating Learning Rates for Evolution and Temporal Difference Learning", "author": ["S.M. Lucas"], "venue": "Computational Intelligence and Games, 2008. CIG\u201908. IEEE Symposium On. IEEE, 2008, pp. 1\u20137.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Estimating Learning Rates in Evolution and TDL: Results on a Simple Grid-World Problem", "author": ["\u2014\u2014"], "venue": "Computational Intelligence and Games (CIG), 2010 IEEE Symposium on. IEEE, 2010, pp. 372\u2013379.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "Bandit algorithms [8], [9] have become popular for optimising either simple regret (the best final decision after a number of exploratory trials) or cumulative regret (best sum of rewards over a number of trials) in A/B testing.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Bandit algorithms [8], [9] have become popular for optimising either simple regret (the best final decision after a number of exploratory trials) or cumulative regret (best sum of rewards over a number of trials) in A/B testing.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "A popular bandit algorithm is the Upper Confidence Bound (UCB) algorithm [10], [11] which balances the trade-off between exploration and exploitation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "A popular bandit algorithm is the Upper Confidence Bound (UCB) algorithm [10], [11] which balances the trade-off between exploration and exploitation.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "The UCB-style algorithms have achieved widespread use within Monte Carlo Tree Search (MCTS) [12], called UCT when applied to trees, the \u201cT\u201d being for Trees.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "St-Pierre and Liu [21] applied the Differential Evolution algorithm [22] to some non-stationary bandit problem, which outperformed the classical bandit algorithm on the selection over a portfolio of solvers.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "St-Pierre and Liu [21] applied the Differential Evolution algorithm [22] to some non-stationary bandit problem, which outperformed the classical bandit algorithm on the selection over a portfolio of solvers.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "[12] noted the great potential for hybridising MCTS with other approaches to optimisation and learning, and in this paper we provide a hybridisation of an evolutionary algorithm with a bandit algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "[25] made fitness evaluations after each rollout, so they could be rapidly optimised, albeit noisily.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "This has some similarities with Estimation of Distribution Algorithms (EDAs) [26] but the details are significantly different.", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "For instance, Lucas and Reynolds evolved Deterministic Finite Automata (DFA) [28], [29], using a multi-start RMHC algorithm with very competitive results, outperforming more complex evolutionary algorithms, and for some classes of problems also outperforming the state of the art EvidenceDriven State Merging (EDSM) algorithms.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "For instance, Lucas and Reynolds evolved Deterministic Finite Automata (DFA) [28], [29], using a multi-start RMHC algorithm with very competitive results, outperforming more complex evolutionary algorithms, and for some classes of problems also outperforming the state of the art EvidenceDriven State Merging (EDSM) algorithms.", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "Although Goldberg [30] used bandit models, they were used to help understand the operation of a Simple Genetic Algorithm.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "The OneMax problem [31] is a simple linear problem aiming at maximising the number of 1 of a binary string, i.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "The complexity of OneMax problem is O(n log(n)) for a n-bit string [32].", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "proved that the black-box complexity with memory restriction one is at most 2n [33].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 34, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 99, "endOffset": 103}, {"referenceID": 34, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "It is notable that our noise model is very different from the one in [36], which used (1+1)-EA and a one-bit noise.", "startOffset": 69, "endOffset": 73}, {"referenceID": 36, "context": "The influence of the noise strength on the runtime of (1+1)EA for the OneMax problem corrupted by one-bit noise is firstly analysed by Droste [37].", "startOffset": 142, "endOffset": 146}, {"referenceID": 36, "context": "In [37] and [36], the misranking occurs due to the change of exactly one uniformly chosen bit of s by noise with probability p \u2208 (0, 1), p is the noise strength.", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "In [37] and [36], the misranking occurs due to the change of exactly one uniformly chosen bit of s by noise with probability p \u2208 (0, 1), p is the noise strength.", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Surprisingly, a simple Random Mutation Hill-Climbing Algorithm outperforms GA on a simple Royal Road function, namely R1 in [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "For the further work, we are interested in applying our proposed approach to more difficult problems (such as game level generation and evaluation [40]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 39, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 111, "endOffset": 115}, {"referenceID": 40, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 117, "endOffset": 121}, {"referenceID": 41, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 219, "endOffset": 223}, {"referenceID": 42, "context": "[44] proved theoretically and empirically that under some conditions, resampling is not beneficial for a (1+1)-EA optimizing 10-OneMax under additive Gaussian noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The Royal Road functions involve a harder credit assignment [45] problem than standard OneMax, an important aspect of sequential decision making.", "startOffset": 60, "endOffset": 64}, {"referenceID": 44, "context": "More use is made of the available information, leading to faster learning (see [46] and [47] for more analysis of the information rates of simple evolutionary algorithms).", "startOffset": 79, "endOffset": 83}, {"referenceID": 45, "context": "More use is made of the available information, leading to faster learning (see [46] and [47] for more analysis of the information rates of simple evolutionary algorithms).", "startOffset": 88, "endOffset": 92}, {"referenceID": 38, "context": "Also, because the method makes such efficient use of the fitness evaluations, it is ready to be applied to expensive optimisation problems such as game level design evaluation [40].", "startOffset": 176, "endOffset": 180}], "year": 2016, "abstractText": "The Random Mutation Hill-Climbing algorithm is a direct search technique mostly used in discrete domains. It repeats the process of randomly selecting a neighbour of a bestso-far solution and accepts the neighbour if it is better than or equal to it. In this work, we propose to use a novel method to select the neighbour solution using a set of independent multiarmed bandit-style selection units which results in a bandit-based Random Mutation Hill-Climbing algorithm. The new algorithm significantly outperforms Random Mutation Hill-Climbing in both OneMax (in noise-free and noisy cases) and Royal Road problems (in the noise-free case). The algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive.", "creator": "LaTeX with hyperref package"}}}