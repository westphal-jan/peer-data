{"id": "1606.06083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Product Classification in E-Commerce using Distributional Semantics", "abstract": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.", "histories": [["v1", "Mon, 20 Jun 2016 12:26:21 GMT  (2011kb,D)", "http://arxiv.org/abs/1606.06083v1", null], ["v2", "Mon, 25 Jul 2016 10:38:52 GMT  (2026kb,D)", "http://arxiv.org/abs/1606.06083v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["vivek gupta", "harish karnick", "ashendra bansal", "pradhuman jhala"], "accepted": false, "id": "1606.06083"}, "pdf": {"name": "1606.06083.pdf", "metadata": {"source": "CRF", "title": "Product Classification in E-commerce using Distributional Semantics", "authors": ["Vivek Gupta", "Harish Karnick", "Pradhuman Jhala"], "emails": ["vgupta@cse.iitk.ac.in", "hk@cse.iitk.ac.in", "ashendra.bansal@flipkart.com", "pradhuman.jhala@flipkart.com"], "sections": [{"heading": null, "text": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy, specifying a textual product description or title. To efficiently classify a product, we need a suitable representation for a document (the textual description of a product), feature vectors and efficient and fast algorithms for predicting. To address the above challenges, we propose a new distribution semantic representation for document vector formation. In addition, we develop a new two-step ensemble approach that uses a path-wise, node-wise and depth-wise classifier (in relation to the taxonomy tree) to reduce errors in the final product classification. Our experiments demonstrate the effectiveness of distribution-based representation and ensemble approach on datasets from a leading e-commerce platform and achieve better results on different valuation metrics compared to previous approaches."}, {"heading": "1 Introduction", "text": "This year, the time has come for us to be able to try to find a solution that we are able to find, that we are able to find a solution."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Distributional Semantic Word Representation", "text": "The method of distributional word embedding was first introduced by Bengio et al. as a Neural Probabilistic Language Model (Bengio et al., 2003). The Neural Probabilistic Language Model learns distributional word vectors along with a probability function. Due to a large nonlinear Softmax layer, the model exhibits an unaffordable training complexity. Later, Mikolov et al. (Mikolov et al., 2013a) proposed a simple log-linear model that significantly reduces training time - the Word2Vec Continuous Bag-of-Words (CBoW) model and Skip-Gram with negative sampling model.The training goal in the CBoW model is to predict the central (middle) word given to adjacent words in a local window (context words)."}, {"heading": "2.2 Distributional Paragraph Representation", "text": "Most models for learning distributed representations for long texts, such as at the phrase level, sentencelevel, or document level, which attempt to capture semantic composition, do not go beyond the simple weighted average of word vectors. This approach corresponds to a sack-of-words approach and neglects word order when displaying documents. Socher et al. (Socher et al., 2013) suggest a recursive tensor neural network in which the dependency paragraph tree of the sentence is used to compose word vectors in a bottomup approach to represent sentences or phrases. This approach considers syntactical dependencies, but cannot go beyond sentences, as it depends on paragraphs. Mikolov proposed a distribution paragraph vector frame called Paragraph vector vectors that are formed similar to word vectors to represent sentences or phrases vector vector vector vector vector vector vector and vector vector vector vector vector. He proposed two types of modelling vector that are called distributor model vector model 2014 (PDM) and vector model (PDM)"}, {"heading": "2.3 Problem with Paragraph Vectors", "text": "Paragraph vectors derived from PV-DM and PVDBoW are used across context words generated from the same paragraph, but not across paragraphs. Paragraph vectors are also represented in the same space (dimension) as word vectors, although a paragraph may contain words that belong to several themes (senses).The formulation for paragraph vectors ignores the meaning and distinctness of a word between documents, i.e. assumes that all words contribute both quantitatively (weight) and qualitatively (meaning).Quantitatively, only binary weights are used, i.e. 0 weight for stopwords and not zero weight for others. Intuitively, a paragraph vector should be embedded in a larger and enriched space."}, {"heading": "2.4 Hierarchical Product Categorization", "text": "Most methods for hierarchical classification follow a gates-and-experts method, which has a two-level classification but is a slow process; the high-level classifier serves as a \"gateway\" to a lower classifier called an \"expert\" (Shen et al., 2011). The basic idea is to split the problem into two models, the first model being simple and making coarse-grained classification, while the second model is more complex and performs a finer-grained classification. The coarse-grained classification deals with a huge number of examples, while the fine-grained distinction within a subtree under each top category has better characteristics and classification algorithms, and deals with fewer categories. (Kumar et al., 2002), proposed an approach that learns a tree structure over the set of classes, using a clustering algorithm based on fishermen who discriminate that clustered training games are mutually exclusive in groups."}, {"heading": "3 Graded Weighted Bag of Word Vectors", "text": "ieD rf\u00fc ide eeirmtlrVnree\u00fcgn rf\u00fc ide rf\u00fc ide eeircnlhsrtee\u00fceVnlhsrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnh e\u00fccnlrf\u00fc ide eeirsrVnlrtee\u00fce ni rde nlrllrrf\u00fc ide eeirllrteeVrlrtee\u00fccnllrtee\u00fccnlrrrrtee\u00fccnrrrrvrrrtee\u00fccehcnlrrrrrrrrrrrrrrrrrrrgtee\u00fciuiueVrlrlrlhc.nrrf\u00fc"}, {"heading": "4 Ensemble of Multitype Predictors", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "5 Dataset", "text": "The data set had two product taxonomies: nonbook and book. Non-book data is more discriminatory, with an average description + title length of about 10 to 15 words, while book descriptions have an average length of more than 200 words. To give greater meaning to the title, we generally weight it three times as much as the description value. The distribution of articles across leaf categories (vertical) has a high crookedness, barrenness, and strong texture, and suffers from scarcity, as in Figure 3. We use random forest and neighboring titles as base classifiers, as they are less affected by data sketches."}, {"heading": "6 Results", "text": "The classification system is evaluated using the usual precision measurement, which is defined as a fraction of products from test data for which the classifier predicts correct taxonomic paths. As there are several similar paths in the data set that predict a single path, it is not appropriate to predict more than one path or better a ranking of 3 to 6 paths whose labeling matches the actual path. Precedence is derived from the confidence value of the predictor. We also calculate the confidence value of the correct prediction path based on the k (3 to 6) confidence values of the individual predicted paths. To measure accuracy when predicting more than one path, the classification result is counted as correct if the correct class (i.e. the path assigned by the vendor) is one of the returned class (paths)."}, {"heading": "6.1 Non-Book Data Result", "text": "We also compare our results with document vectors formed by averaging word vectors of words in the document, i.e. True Word Distribution Methods in Word Clusters (AWV), Distributed Wallet Version of Paragraph Vector by Mikolov (PV-DBoW), Incidence Histogram of Word Distribution in Word Clusters (i.e. Cluster Vector Labeling Bag (BoCV). We consider the classifier (random forest with 20 trees) common to all document vector representations. We compare performance in terms of number of clusters, word vector dimension, document vector dimension and vocabulary standard dimension (tf-idf) for different models. Figure 4 shows results for a random forest (20 trees) on different classifiers trained by different methods on 0.2 million training sessions and 0.2 million test samples with 3589 classes."}, {"heading": "6.2 Book Data Result", "text": "To maintain consistency, we truncate the above 66% of data samples and work with the remaining 44%, or 0.37 million samples. To deal with improper labels and ambiguities in the taxonomy, we use several classifiers: one to predict paths (or leaves), another to predict node labels and multiple classifiers, one at each depth level of the taxonomy tree that predict node labels at that level. In the deep node classification, we also use the term \"none\" to identify missing labels at a certain level, i.e. paths that end at earlier levels. However, we only take a random layer sample for this \"no\" label. Table 4 presents the results for predicting node labels at different depths at a particular level, i.e. paths that end at earlier levels."}, {"heading": "6.3 Ensemble Classification", "text": "We use the ensemble of multi-type predictors described in Section 4 for the final classification. To reduce dimensionality, we use selection methods based on mutual information criteria (ANOVA-F value, i.e. analysis of variance). We get improved results for all four evaluation metrics. The following list shows how the first column in Table 5 should be interpreted. \u2022 tf-idf (A-2-C): Term frequency and inverse document abundance characteristic with # A top 1, 2 grams of words and # C random forest trees. \u2022 Path-1 (A-C): Path prediction model without ensemble, trained with # A (cluster * wordvec dimension) using C trees. \u2022 Depth (A + B-C): trained with gwBoWV with A characteristics, B represents the size of out probability vectors (# total nodes) for all depth classification levels with A + B characteristics coached with A + 1 of trees."}, {"heading": "7 Conclusions", "text": "We introduced a novel composition technology based on distributional representation, i.e. embedded word vectors, to form suitable document vectors. To capture the meaning, weight and distinctiveness of words in documents, we used a graduated weighting approach to composition, based on the recent work of Mukerjee et. el. (Pranjal Singh, 2015). Our document vectors are embedded in a vector space that is different from the word embedding vector space, which is higher dimensional and attempts to encode the intuition that a document has more themes or senses than a deterioration. We also developed a new technique that uses a combination of multiple classifiers that predict label paths, node labels and depth labels to reduce classification errors. We also tested our methods on datasets from a leading e-commerce platform and demonstrated good performance improvements."}, {"heading": "8 Future Work", "text": "Much more work is possible to improve the quality of the learned representations and to expand the model with additional data and relationships for common relationship modelling and prediction. Better performance can be achieved by learning vector representation and clustering together, using non-parametric cluster methods to form semantic clusters."}, {"heading": "9 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Label Embedding Based Approach", "text": "Wei and Kwok (Bi and Kwok, 2011) proposed a label-based embedding approach that exploits label dependence in tree-structured hierarchies for hierarchical classification. Kernel Dependency Estimation (KDE) is used to project or embed the label vector (multi-label) into less orthogonal dimensions at first. An advantage of this approach is that all M learners in the projected space can learn from the full training data. In contrast, training data in tree-based methods decreases when we reach leaf nodes. To preserve dependencies in prediction, the authors suggest a greedy approach. The problem can be efficiently solved with a greedy algorithm called the Condensing Sort and Select Algorithm. However, the algorithm is computer-intensive."}, {"heading": "9.1.1 Dependency Based Word Vectors", "text": "SGNS and CBoW both use linear word context to form word vectors (Mikolov et al., 2013b). Levy and Goldberg (Levy and Goldberg, 2014a) recommended the use of any functional context instead of syntactic dependencies generated from a sentence. Each word w and its modifiers m1,.., mk are extracted from a sentence parse. Contexts in the form (m1, lbl1,.., mk, lblk) are generated for each sentence. Here, lbl is the dependency type between the word and the modifier and lbl \u2212 1 is used to indicate the inverse relationship. Figure 5 shows dependency context for words in a given sentence. The dependency-based word vectors use the same training methods as SGNS. Compared to similarly learned linear context-based vectors, it was found that dependency-based vectors have a higher probability."}, {"heading": "9.2 Example of gwBoWV Approach", "text": "Suppose there are four clusters C = [C1, C2, C3, C4], here Ci represents Cluster2. Let Dn = [w1,.., w9, w10] be a document consisting of words w1, w2,.., w10 in the order whose document vectors must be assembled with word vectors. Let us assume the following word cluster mapping for document Dn, as in Table 63. Let us obtain the contribution of cluster Ci to document Dn by summing word vectors for words from document Dn and cluster # ci: \u2022 ~ cvvv1 = ~ wvvv4 + ~ wvididvidvidvidvidvidvidvidvidvidvidvidvid4 = d4 df."}, {"heading": "9.3 Quality of WordVec Clusters", "text": "The following are examples of words included in some clusters, and their possible meaning for the cluster theme for the book data. Each cluster is formed by clusters of word vectors, the word belonging to specific themes. We number clusters according to the distance of the centrist from the origin, to avoid confusion. 1. Cluster # 0 essentially talks about criminal and criminal terms such as accused, arrest, assault, attempted beatings, lawyer, brutal confessions, convicted policemen, corruption, imprisonment, dealer, gang, investigation, gangster, weapons, hated, prisons, judge, mob, undercover, trail, police, prison, lawyer, torture, witness, etc. Cluster # 10 talks about scientific experiments related terms such as yield, validity, variance, alternatives, analysis, calculation, comparison, assumptions, criteria, determination, description, evaluation, formulation, experiments, measure model, conclusions, hypotheses, etc."}, {"heading": "9.4 Two Level Classification Approach", "text": "We also experimented with a modified approach of the two-level classification described by Shen and Ruvini (Shen et al., 2011) (Shen et al., 2012) in Section 2.4. However, rather than arbitrarily specifying a direction and then finding a dense diagram of highly networked components, we chose the edge direction out of a misclassification and used various methods such as weakly connected components, bi-connected components and articulation points to find highly networked components. We took this approach to improve sensitivity and cover missing edges, as discussed in Section 2.4. The value of the probability of confusion and the direction of the edges is determined by the value of the (i, j) element in the confusion matrix (CM)."}, {"heading": "9.5 Confused Category Group Discovery", "text": "Figure 6 shows that hard disk, hard disk enclosure and hard disk enclosure are misclassified among themselves, forming a latent group in computer and computer accessories, which is extracted by finding bi-connected components in the misclassification graph. Figure 7 shows the last latent groups discovered in non-book data using weighted word vector methods and random forest classifiers without class balance on raw data with different thresholds on # mis classification for falling edges based on edge weights. Algorithm 4: Modified contiguous components group data: Set of categories C = {c1, c2, c3... cn} and threshold \u03b1 result: Set of dense sub-graphs CG = {cg1, cg2, cg3, cg4."}, {"heading": "9.6 Data Visualization Tree Maps", "text": "Figures 9 - 10 show tree maps at different depths for the book taxonomy. These maps show that the tree has a high crookedness and a strong tail."}, {"heading": "9.7 More Results for Ensemble Approach", "text": "We use kNN and random forests for the initial classification instead of SVM for better stability of class imbalance and better performance due to the generation of good meta-characteristics. Also, SVM does not perform well for a large number of classes. Table 7 confirms the same empirical accuracy. To prove that books were more confusing compared to non-books, we conducted a small experiment. We examined all computer and computer accessories and computer-related books and labeled them binary and compared this with a direct classifier without binary labeling. The results from different approaches to the top-3 taxonomy prediction can be found in Table 13. 12 shows results of node-level-two classifiers at different levels of the BoV level (using ANOVA) - original vectors were concatenated output probabilities of node prediction probabilities using WVA classifiers at different levels of WVA."}, {"heading": "9.8 Classification Example from Book Data", "text": "Description: Ignorance is bliss or so hopes antoine the main character in martin pages stabbing satire"}, {"heading": "1000 40.30 74.45 86.38 22.47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2000 40.88 74.92 86.87 22.56", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3000 40.99 75.24 87.06 22.60", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4000 41.11 75.24 87.07 22.53", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1000 46.26 72.46 84.84 24.85", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2000 47.05 .72.26 84.52 25.05", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2500 .47.70 72.77 84.58 24.81", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3000 .44.45 73.84 85.83 23.74", "text": "Dre eeisrcnlhteeSrteee\u00fccnlhsrc\u00fceegnlhsrteeoiiiiiietlrsrteeeoiiiiiiiiiuiuztlrsrsrrsrteeVnlrrrrrrrteeeerrteeteeteeteecrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrteeteeteeteeteecrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Label embedding trees for large multi-class tasks", "author": ["Bengio et al.2010] Samy Bengio", "Jason Weston", "David Grangier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Multi-label classification on tree- and dag-structured hierarchies", "author": ["Bi", "Kwok2011] Wei Bi", "James T. Kwok"], "venue": null, "citeRegEx": "Bi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington and Socher.,? \\Q2014\\E", "shortCiteRegEx": "Pennington and Socher.", "year": 2014}, {"title": "Everyone likes shopping! multi-class product categorization for e-commerce", "author": ["Zornitsa Kozareva"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Kozareva.,? \\Q2015\\E", "shortCiteRegEx": "Kozareva.", "year": 2015}, {"title": "Hierarchical fusion of multiple classifiers for hyperspectral data analysis", "author": ["Kumar et al.2002] Shailesh Kumar", "Joydeep Ghosh", "M. Melba Crawford"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "Kumar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2002}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Dependencybased word embeddings", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014b] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014c] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Words are not equal: Graded weighting model for building composite document vectors", "author": [], "venue": "In Proceedings of the twelfth International Conference on Natural Language Processing (ICON-2015). BSP Books", "citeRegEx": "Singh.,? \\Q2015\\E", "shortCiteRegEx": "Singh.", "year": 2015}, {"title": "Item categorization in the e-commerce domain", "author": ["Shen et al.2011] Dan Shen", "Jean David Ruvini", "Manas Somaiya", "Neel Sundaresan"], "venue": "In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Shen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2011}, {"title": "Large-scale item categorization for e-commerce", "author": ["Shen et al.2012] Dan Shen", "Jean-David Ruvini", "Badrul Sarwar"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the conference", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Two/too simple adaptations of wordvec for syntax problems. In Proceedings of the 50th Annual Meeting of the North American Association for Computational Linguistics", "author": [], "venue": null, "citeRegEx": "Ling.,? \\Q2015\\E", "shortCiteRegEx": "Ling.", "year": 2015}, {"title": "Deep classification in large-scale text hierarchies", "author": ["Xue et al.2008] Gui-Rong Xue", "Dikan Xing", "Qiang Yang", "Yong Yu"], "venue": "In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "as the Neural Probabilistic Language Model (Bengio et al., 2003).", "startOffset": 43, "endOffset": 64}, {"referenceID": 3, "context": "Various modifications have been incorporated to handle the problem of polysemic words(Huang et al., 2012) by clustering context words during training.", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "(Socher et al., 2013) propose a recursive tensor neural network where the dependency parse-tree of the sentence is used to compose word vectors in a bottomup approach to represent sentences or phrases.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "The high-level classifier serves as a \u201cgate\u201d to a lower level classifier called the \u201cexpert\u201d (Shen et al., 2011).", "startOffset": 93, "endOffset": 112}, {"referenceID": 6, "context": "(Kumar et al., 2002), proposed an approach that learnt a tree structure over the set of classes.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "(Xue et al., 2008) suggested an interesting two stage strategy called \u201cdeep classification\u201d.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(Bengio et al., 2010) use the confusion matrix for estimating class similarity instead of clustering data samples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "Shen and Ruvini (Shen et al., 2012) (Shen et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": ", 2012) (Shen et al., 2011) extend the previous approach by using a mixture of simple and complex classifiers for separating confused classes rather then spectral clustering methods which has faster training times.", "startOffset": 8, "endOffset": 27}, {"referenceID": 15, "context": "(Shen et al., 2012).", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Other simple approaches like flat classification and top down classification are intractable due to the large number of classes and give poor results due to error propagation as described in (Shen et al., 2012).", "startOffset": 191, "endOffset": 210}, {"referenceID": 5, "context": "from Yahoo Labs (Kozareva, 2015).", "startOffset": 16, "endOffset": 32}], "year": 2017, "abstractText": "Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction. To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilizing (with respect to the taxonomy tree) a path-wise, node-wise and depth-wise classifiers for error reduction in the final product classification. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve better results on various evaluation metrics compared to earlier approaches.", "creator": "LaTeX with hyperref package"}}}