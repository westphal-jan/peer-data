{"id": "1606.06086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold for Similarity", "abstract": "Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to or statistically indistinguishable from the optimal results.", "histories": [["v1", "Mon, 20 Jun 2016 12:31:13 GMT  (570kb,D)", "http://arxiv.org/abs/1606.06086v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["navid rekabsaz", "mihai lupu", "allan hanbury"], "accepted": false, "id": "1606.06086"}, "pdf": {"name": "1606.06086.pdf", "metadata": {"source": "CRF", "title": "Uncertainty in Neural Network Word Embedding Exploration of Threshold for Similarity", "authors": ["Navid Rekabsaz", "Mihai Lupu", "Allan Hanbury"], "emails": ["family_name@ifs.tuwien.ac.at"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most of us are able to survive on our own and are able to survive on our own, \"he said in an interview with The New York Times."}, {"heading": "2. RELATED WORK", "text": "The study closest to our work is Karlgren et al. [9], which examines the semantic topology of the vector space generated by random indexing. On the basis of their earlier observations that the dimensionality of semantic space appears different for different terms [10], Karlgren now identifies the different dimensionalities at different angles (i.e. distances) for a number of specific terms. However, it is difficult to assign these observations to specific criteria or guidelines for future models or retrievable tasks. In fact, our observations provide a quantification of Karlgren's claim that \"near\" is interesting and \"far\" is not. [10] More recently, Cuba Gyllensten and Sahlgren [3] have adopted a data-mining approach to represent the terms of belonging by a tree structure. While they propose traversing the tree as a potential approach, they rate it only on the basis of the word and its usefulness for restoring similar words, each supplementing our work and each of the subsequent ones."}, {"heading": "3. POTENTIAL THRESHOLD", "text": "As mentioned in the introduction, we are looking for a potential threshold to separate the really related terms from the rest. In this section, we describe our analytical approach to exploring such intersections in different dimensions. The threshold introduced is defined across the entire model, i.e. it applies to any terms in the language. To this end, we start by observing the uncertainty of similarity in Word embedding models, followed by defining a continuous model of adjacent distributions before defining our proposed threshold."}, {"heading": "3.1 Uncertainty of Similarity", "text": "In this area, we are very well in a position to claim that we are in a position to be in a position to be in, and that we are in a position to be able to survive ourselves, in which we are, in which we are, in which we are, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, we live in which we live in which we live, in which we live in which we live, in which we live, in which we live in which we live, in which we live, in which we live in which we live in which we live, in which we live, in which we live in which we live, in which we live in which we live, we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we"}, {"heading": "3.2 Continuous Distribution of Neighbours", "text": "Intuitively, we assume that these probability distributions follow a normal distribution, so that we can consider each similarity value as a probability distribution based on the similarity values of the same pair in different models. To estimate this probability distribution, we create five identical SGNS models for each dimension, which follow the structure in Section 3.1. Figure 2a shows the probability distribution of similarities for Term Book to 25 terms in different similarity range1. We observe that by reducing the similarity, the variation in probability distributions increases, which empirically reflects the increase in uncertainty between two models in the previous section. We use these probability distributions to provide a representation of the expected number of neighbors by any term in the spectrum of similarity values."}, {"heading": "3.3 Similarity threshold", "text": "Given the representation of the expected number of neighbors around the arbitrary term, the question is, \"What is the best threshold for filtering the strongly related terms?\" This is, of course, a contentious question, since the analytical approach attempts to measure the human understanding of synonymity. However, since this general threshold attempts to separate the strongly related terms for an arbitrary term, it can be estimated by the average number of synonyms across the terms in the language. Therefore, we are transforming the above question into a new question: \"What is the expected number of synonyms for a word in English?\" To answer this, we use WordNet. We consider the different terms in the related synsets to a term to be its synonyms, while highlighting the multi-word threshold (e.g. Natural Language Processing, shown in WordNet by concatenating underscores), as we consider these as separate terms when creating the word embedding models, while the average number of synonyms is over 146."}, {"heading": "4. EXPERIMENTAL METHODOLOGY", "text": "We test the effectiveness of the potential threshold in an ad hoc retrieval task using IR test collections by evaluating the results of applying different thresholds to retrieve related terms. Our relevance assessment approach is based on the language model [18] as a widely used and established method in IR, which has shown competitive results in various areas. In particular, we use the translation language model [2], which incorporates the similarity of translated terms into the basic model. In the following, we first explain the translation language model in combination with word embedding similarity, and then describe the details of our experimental setup."}, {"heading": "4.1 Translation Language Model", "text": "In the language model [18], the score of a document d in relation to a query q is considered the probability of generating the query by a model Md estimated on the basis of the document: Score (q, d) = P (q | Md) = Hq q q P (tq | Md) (2) Typically, the model is a multinomial distribution and the probability is calculated with a maximum probability, together with a form of smoothing. This smoothing, although not part of the original idea, is of paramount importance in the practice of LM-based methods. However, since this is not the focus of this study, we use Dirichlet smoothing [25], as many others have done successfully before us ([8, 24, 26]). Berger and Lafferty [2] introduced translation models as an extension of the language model P. A translation model introduces a translation probability in the estimation of P (q | Md)."}, {"heading": "4.2 Experiments Setup", "text": "We evaluate our approach using 5 test collections: Combination of TREC 1 to 3, TREC-6, TREC-7 and TREC-8 of the AdHoc Track and TREC-2005 HARD Tracks. Table 2 summarizes the statistics of the test collections. However, for pre-processing we use the Porter Stemmer and remove stopwords using a small list of 127 commonly used English terms. To compare the performance of the potential thresholds, we test a variety of thresholds in each dimension: for the dimension 100, {0.67, 0.70, 0.74, 0.79, 0.81, 0.86, 0.91, 0.94, 0.96}, the 200 dimension {0.63, 0.68, 0.71, 0.76, 0.78, 0.82}, 300 dimension, {0.55, 0.60, 0.65, 0.68, 0.70, 0.71, 0.75} and 400 dimension {0.54, 0.64, 0.66, 0.68, 0.71, 0.70, 0.70, 0.7}."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "The evaluation results of the MAP and NDCG @ 20 measures based on these results are presented in Figure 3. Figure 3 summarizes the results of the optimal and potential thresholds. Based on the results, we achieve a significant improvement in results in all areas. Apart from the results of TREC-7, we observe similar results in both areas."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "This threshold is estimated on the basis of a novel representation of neighbours around an arbitrary term that is continuous and benefits from the consideration of the problem of uncertainty in the similarity values of modern word embedding models. We fully evaluate the application of the proposed threshold to four collections of information. The results show a superior performance in the use of our threshold, so that its results are either identical or statistically indistinguishable from the optimal results achieved by extensive searches in the parameter space."}, {"heading": "7. REFERENCES", "text": "[1] M. Baroni, G. Dinu, and G. Kruszewski. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL Conference, 2014. [2] A. Berger and J. Lafferty. Information Retrieval As Statistical Translation. In Proc. of SIGIR, 1999. [3] A. Cuba Gyllensten and M. Sahlgren. Navigation the semantic horizon using relative neighborhood graphs, 2015. [4] L. De Vine, G. Zuccon, B. Koopman, L. Sitbon, and P. Bruza. Medical semantic similarity with a neural language model. In Proc. of CIKM. K. Erk and S. Pad\u00f3. Example-based models for word meaning in context. In Proc. of ACL, 2010. [6] D. Ganguly, D. Roy, M. Mitra, G. J. Jones. Word Embedding Generalized Model for Retrieval.."}], "references": [{"title": "Don\u2019t count", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL Conference", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Information Retrieval As Statistical Translation", "author": ["A. Berger", "J. Lafferty"], "venue": "Proc. of SIGIR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Navigating the semantic horizon using relative neighborhood graphs", "author": ["A. Cuba Gyllensten", "M. Sahlgren"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exemplar-based models for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "Proc. of ACL", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Word Embedding based Generalized Language Model for Information Retrieval", "author": ["D. Ganguly", "D. Roy", "M. Mitra", "G.J. Jones"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Context-and content-aware embeddings for query rewriting in sponsored search", "author": ["M. Grbovic", "N. Djuric", "V. Radosavljevic", "F. Silvestri", "N. Bhamidipati"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation of Statistical Translation Models Based on Mutual Information for Ad Hoc Information Retrieval", "author": ["M. Karimzadehgan", "C. Zhai"], "venue": "Proc. of SIGIR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic topology", "author": ["J. Karlgren", "M. Bohman", "A. Ekgren", "G. Isheden", "E. Kullmann", "D. Nilsson"], "venue": "Proc. of CIKM Conference", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Filaments of meaning in word space", "author": ["J. Karlgren", "A. Holst", "M. Sahlgren"], "venue": "Proc. of ECIR Conference", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["D. Kiela", "F. Hill", "S. Clark"], "venue": "Proc. of EMNLP", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "An evaluation of corpus-driven measures of medical concept similarity for information retrieval", "author": ["B. Koopman", "G. Zuccon", "P. Bruza", "L. Sitbon", "M. Lawley"], "venue": "Proc. of CIKM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "So similar and yet incompatible: Toward automated identification of semantically compatible words", "author": ["G. Kruszewski", "M. Baroni"], "venue": "Proc. of NAACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transaction of the Association of Computational Linguists", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["B. Mitra"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. of EMNLP Conference", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "Proc. of SIGIR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "On the use of statistical semantics for metadata-based social image retrieval", "author": ["N. Rekabsaz", "R. Bierig", "B. Ionescu", "A. Hanbury", "M. Lupu"], "venue": "Proc. of CBMI Conference", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternatives to bpref", "author": ["T. Sakai"], "venue": "Proc. of SIGIR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Proc. of EMNLP", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proc. of SIGIR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Y. Tsvetkov", "M. Faruqui", "W. Ling", "G. Lample", "C. Dyer"], "venue": "Proc. of EMNLP", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vuli\u0107", "M.-F. Moens"], "venue": "Proc. of SIGIR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proc. of SIGIR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Integrating and evaluating neural word embeddings in information retrieval", "author": ["G. Zuccon", "B. Koopman", "P. Bruza", "L. Azzopardi"], "venue": "Proc. of Australasian Document Computing Symposium", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "[10] in examples, showing that word embedding methods are too ready to provide answers to meaningless questions: \u201cWhat is more similar to a computer: a sparrow or a star?\u201d, or \u201cIs a cell", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Recently, Cuba Gyllensten and Sahlgren [3] point out the limitations of the kNN approach as it neglects the internal structure of neighbourhoods which could be vastly different for various terms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 19, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 21, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 2, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 240, "endOffset": 246}, {"referenceID": 3, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 240, "endOffset": 246}, {"referenceID": 7, "context": "discuss for the case of Random Indexing [9, 10], just because we can have a \u201cmost similar term(s)\u201d does not mean that this makes", "startOffset": 40, "endOffset": 47}, {"referenceID": 8, "context": "discuss for the case of Random Indexing [9, 10], just because we can have a \u201cmost similar term(s)\u201d does not mean that this makes", "startOffset": 40, "endOffset": 47}, {"referenceID": 5, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 122, "endOffset": 125}, {"referenceID": 22, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "[15]: skip-gram with negativesampling training (SGNS) method in the Word2Vec framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] introduced GloVe and reported superior results), independent benchmarking provided by Levy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] shows that there is no fundamental performance difference between the recent word embedding models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9], which explores the semantic topology of the vector space generated by Random Indexing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Based on their previous observations that the dimensionality of the semantic space appears different for different terms [10], Karlgren at al.", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "In fact, our observations provide a quantification on Karlgren\u2019s claim that \u201c\u2018close\u2019 is interesting and \u2018distant\u2019 is not\u201d [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "More recently, Cuba Gyllensten and Sahlgren [3] follow a data mining approach to represent the terms relatedness by a tree structure.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Our relevance scoring approach is based on the language model [18] method as a widely used and established method in IR that has shown competitive results in various domains.", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "In particular, we use the translation language model [2] which includes the similarity of", "startOffset": 53, "endOffset": 56}, {"referenceID": 16, "context": "In the language model [18], the score of a document d with respect to a query q is considered to be the probability of generating the query by a model Md estimated based on the document:", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 22, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 24, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 1, "context": "Berger and Lafferty [2] introduced translation models as an extension to the language modelling.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "[26] integrates word embedding into the translation language model, showing potential improvement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Therefore, in order to provide a more fair evaluation framework, we consider MAP and NDCG over the condensed lists [20].", "startOffset": 115, "endOffset": 119}], "year": 2016, "abstractText": "Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to or statistically indistinguishable from the optimal results.", "creator": "LaTeX with hyperref package"}}}