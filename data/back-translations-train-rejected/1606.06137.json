{"id": "1606.06137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "abstract": "We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task. In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user's past actions. We focus on the task of writing, in which the user is coalescing previously collected information into a text. Our proactive system automatically recommends the user relevant background information. The proposed system incorporates text input prediction using a long short-term memory (LSTM) network. We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system.", "histories": [["v1", "Mon, 20 Jun 2016 14:26:33 GMT  (322kb,D)", "http://arxiv.org/abs/1606.06137v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.NE", "authors": ["petri luukkonen", "markus koskela", "patrik flor\\'een"], "accepted": false, "id": "1606.06137"}, "pdf": {"name": "1606.06137.pdf", "metadata": {"source": "CRF", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "authors": ["Petri Luukkonen", "Markus Koskela", "Patrik Flor\u00e9en"], "emails": ["first.last@helsinki.fi"], "sections": [{"heading": null, "text": "CCS concepts \u2022 information systems \u2192 query intention; user and interactive retrieval; \u2022 human-centered computing \u2192 text input; \u2022 computing methods \u2192 neural networks; keywords task-based information retrieval; proactive search; long-term short-term memory networks; recursive neural networks; text prediction"}, {"heading": "1. INTRODUCTION", "text": "In this context, it should be noted that the measures in question are measures taken in recent years to promote job creation in the fields of health, health and well-being."}, {"heading": "2. RELATED WORK", "text": "Recursive neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various areas, such as music [7], text [15, 29] and handwriting [15]. In information gathering, RNNs have been used to extract, for example, semantic vectors at the sentence level [26] and context-related query suggestions [28]. Other types of deep neural networks have been used to project queries and documents into low-dimensional semantic spaces [18] and to learn vectors with variable length of texts, such as sentences, paragraphs and documents [21]. Different types of task activities have been investigated in the literature as the basis for query suggestions or query support. Motivated by the observation that a significant proportion of the user's information needs were triggered by prior web browsing activities."}, {"heading": "3. METHOD FOR PROACTIVE IR", "text": "The proactive recommendations of our method are based on user input observed during the current task. To improve the recommendations, we suggest using an LSTM-based query enhancement method, as described in Section 3.1. For comparison, we use a method based on estimating user intentions using a multi-armed bandit model (Section 3.2 and Appendix A). Proactive information query using the advanced query is described in Section 3.3. An overview of the proposed method (and the comparison and base methods) is shown in Figure 1."}, {"heading": "3.1 LSTM Text Prediction", "text": "In LSTM-based query expansion, the most probable continuations are estimated for the current written text and the expansion words are selected from these estimated continuations. To calculate the sequences, we use a beam search algorithm [19], which is described below. LSTM network f is first formed with a text corpus, like abstracts from scientific articles. The trained network can then serve as a model for text generation. Suppose the user has written n input words and designated w0 as the newest word in the input sequence. If an input word w is given, the network f will return a probability distribution for the next word from which we will consider the b most likely candidates."}, {"heading": "3.2 User Intent Model", "text": "As a comparison method, we use an algorithm of upper confidence limits, which is based on the estimation of the user intention by means of a multi-armed bandit model [13]. The model balances exploration with exploitation and selects words that have the highest trust limit [2]. This allows the user to interact with words that are relevant but also unsafe for the model. Further details on the method are described in Appendix A."}, {"heading": "3.3 Proactive Query", "text": "Whenever resources are proactively retrieved, our system works as follows; for convenience, we call this a proactive query, even though there is no explicit query from the user. Based on the n input words, our nexp query extension modules retrieve suggested words and add them to the query. Actually, information is retrieved via the Lucene search engine with the standard cosine similarity ranking."}, {"heading": "4. SIMULATION EXPERIMENTS", "text": "In order to test whether our method delivers relevant documents, we have performed two types of simulation experiments: In the simulations, the input corresponding to the text the user enters comes from a specific document. We perform query enhancements using the LSTM-based text prediction method and the User Intent Model. In the basic experiments, we only use written input as a query."}, {"heading": "4.1 Data Set", "text": "The simulations were carried out with the abstracts of the Department of Computer Science of the arXiv1 pre-print database, which were downloaded on October 28, 2015 and contain a total of 40 sub-categories that are used as themes in our experiments. A document in the arXiv database can belong to several sub-categories, in our case several themes."}, {"heading": "4.2 Parameters", "text": "We used the middle LSTM architecture from Zaremba et al. [37] with a tensor flow implementation. The network has two layers, 650 units per layer and is unrolled for 35 words. All summaries of the data set were used to train the LSTM network.The training data consisted of 15M words with a vocabulary of 10k words. Training of the network took about 36 hours with two GeForce GTX TITAN GPUs. Due to memory requirements, random 10% of the summaries were used to form the document term matrix for the user intention model.In the LSTM-based word prediction, the beam width was set to k = 80 and the depth to d = 3. The branch coefficient was set to b = 10 and nexp = 10.1http: / / arxiv.org /"}, {"heading": "4.3 Exploratory Search Task", "text": "We simulate a setting in which a user writes a text about a particular topic. We randomly select a document and simulate the input of n consecutive words from the text. The variable n serves as the size of the context, expressed as the number of words from the written text. For example, if n = 3 and the test text is \"Machine Learning is a subfield,\" the input sequences for the proactive search engine would be \"Machine Learning is,\" \"Learning is a\" and \"is a subfield.\" We imagine a situation in which the user must find some relevant background resources for the given topic. Thus, the goal is to find other documents on the same topic as the input document. For the proactive queries (Section 3.3) we use n input words and nexp = 10. The Lucene search engine has been set to return 10 documents. We use all documents in the database that belong to the same topic as the test document as the target document, as a measure of the performance of the document we are using as the reference for the document."}, {"heading": "4.4 Known-item Search Task", "text": "We also run simulations of the search for known objects, where the purpose is to examine an environment in which the user must find a particular previously seen document. The setting is the same as in the explorative search, except that we now have only one target document in German. We take a random document as a query and perform a Lucene search over the rest of the record to find the document with the highest score. This is now our target document. Note that the target document is a different document from our input document, and in the simulation we either find the target document or not."}, {"heading": "4.5 Results", "text": "Figure 4 shows the results of the simulations. On the left, the retrieval accuracy of the explorative search task is shown. On the right, the fractions of known items found in the search task with known items are shown. First, as expected, the results in both tasks improve with increasing context size, which was expected in particular for searches with known items due to the way the target document was selected. Second, the results show that the query enhancement methods can improve the precision of proactively retrieved documents in the explorative search task. Query enhancement with LSTM improves results when the context is long enough, i.e. n > 10. The query model, which is based on query, is suitable for small context sizes (n < 10). When searching with known items, the query enhancement methods are not equally advantageous."}, {"heading": "5. USER INTERFACE", "text": "For this reason, we have implemented an experimental user interface that will be displayed in one corner of the screen to display the proactive recommendations; see Figure 5. The user interface is designed so that the user can maintain his focus on the current task while offering peripheral contextual recommendations. Any data source, such as the user's own emails, a database of documents, or any web page, can be used as information that can be proactively retrieved. Figure 5 shows the displayed resources as arXiv forms. In order to maintain the current font context, we have implemented a special text editor that transfers the current word surrounding the text cursor to the proactive search application at the press of each key; in this way, the input of n words gradually builds up. Similarly, the context, for example, could be derived from the text read in a web browser."}, {"heading": "6. DISCUSSION AND CONCLUSIONS", "text": "The experiments carried out demonstrate that our method is capable of proactively generating relevant resources; the query enhancement calculated with an LSTM network improved query accuracy in an explorative search task when sufficient context data is available; the results obtained with the method used as a comparison, based on modelling user intentions using an upper-limit algorithm, were partially complementary and improved results when only limited context is available; this, of course, suggests another study of combining the two query enhancement methods; further experiments with different types of data sets and tasks are required in each case to validate the results; in this work, we focused on the writing task; and we introduced a simple user interface to present the proactive search results as a text editor based on the written context."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "This work was partially supported by the Finnish Innovation Promotion Agency (Project Re: Know) and the Finnish Academy (Finnish Centre of Excellence in Computational Inference Research COIN, 251170)."}, {"heading": "8. REFERENCES", "text": "[1] K. Athukorala, A. Medlar, K. Ilves, and D. Glowacka.Balancing exploration and exploitation: Empirical parameterization of exploratory search systems. In Proc. CIKM, pp. 1703-1706, 2015. [2] P. Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3: 397-422, March. 2003. [3] T. Babaian, B. J. Grosz, and S. M. Shieber. A collaborative assistant. In Proc. IUI, pp. 7-14, 2002. XiXiXi. H. Bast and I. Weber. Art less, find more: Fast autocompletion search with a succinct index. In Proc. SIGIR, pp. 364-371, 2006. [5] S. Bhatia, D. Majumdar, and N. Aggarwal."}, {"heading": "A. USER INTENT MODEL", "text": "For the calculation of the model of user intention, we use a training database consisting of M documents from which N unique words are extracted by excluding stop words. the jth document in the database is represented by a feature vector xj + RN, where xij is the tf-idf value of the ith word. We refer to the tf-idf matrix of M documents, where each column of X corresponds to a document attribute vector and each line corresponds to a distribution of words across the documentation. the model of user intention is estimated using the context formed with n previously written words. Based on this input, a series of word weights is calculated using the LinRel algorithm proposed in [2]. We refer to the relevance of the observed words by y [0, 1] N, where yi = 1 corresponds to the observation of the ith word in the input."}], "references": [{"title": "Balancing exploration and exploitation: Empirical parameterization of exploratory search systems", "author": ["K. Athukorala", "A. Medlar", "K. Ilves", "D. Glowacka"], "venue": "In Proc. CIKM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["P. Auer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A writer\u2019s collaborative assistant", "author": ["T. Babaian", "B.J. Grosz", "S.M. Shieber"], "venue": "In Proc. IUI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Type less, find more: Fast autocompletion search with a succinct index", "author": ["H. Bast", "I. Weber"], "venue": "In Proc. SIGIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Proactive information retrieval: Anticipating users\u2019 information need", "author": ["S. Bhatia", "D. Majumdar", "N. Aggarwal"], "venue": "In Proc. ECIR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning to complete sentences", "author": ["S. Bickel", "P. Haider", "T. Scheffer"], "venue": "In Proc. ECML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In Proc. ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Information access in context", "author": ["J. Budzik", "K. Hammond", "L. Birnbaum"], "venue": "Knowledge-Based Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Context-aware query classification", "author": ["H. Cao", "D.H. Hu", "D. Shen", "D. Jiang", "J.-T. Sun", "E. Chen", "Q. Yang"], "venue": "In Proc. SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Actively predicting diverse search intent from user browsing behaviors", "author": ["Z. Cheng", "B. Gao", "T.-Y. Liu"], "venue": "In Proc. WWW,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The Reactive Keyboard: A predictive typing aid", "author": ["J.J. Darragh", "I.H. Witten", "M.L. James"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Implicit queries (IQ) for contextualized search", "author": ["S. Dumais", "E. Cutrell", "R. Sarin", "E. Horvitz"], "venue": "In Proc. SIGIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Directing exploratory search: Reinforcement learning from user interactions with keywords", "author": ["D. Glowacka", "T. Ruotsalo", "K. Konuyshkova", "S. Kaski", "G. Jacucci"], "venue": "In Proc. IUI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Sentence completion", "author": ["K. Grabski", "T. Scheffer"], "venue": "In Proc. SIGIR, pages 433\u2013439,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "In Proc ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In Proc. CIKM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Statistical machine translation", "author": ["P. Koehn"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Predicting search intent based on pre-search context", "author": ["W. Kong", "R. Li", "J. Luo", "A. Zhang", "Y. Chang", "J. Allan"], "venue": "In Proc. SIGIR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Anticipatory search: Using context to initiate search", "author": ["D.J. Liebling", "P.N. Bennett", "R.W. White"], "venue": "In Proc. SIGIR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Citesight: Supporting contextual citation recommendation using differential search", "author": ["A. Livne", "V. Gokuladas", "J. Teevan", "S.T. Dumais", "E. Adar"], "venue": "In Proc. SIGIR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A proactive recommendation system for writing: Helping without disrupting", "author": ["M.C.P. Melguizo", "L. Boves", "O.M. Ramos"], "venue": "International Journal of Industrial Ergonomics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Query auto-completion for rare prefixes", "author": ["B. Mitra", "N. Craswell"], "venue": "In Proc. CIKM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep sentence embedding using long short-term memory networks", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Remembrance Agent: A continuously running automated information retrieval system", "author": ["B. Rhodes", "T. Starner"], "venue": "In Proc. PAAM96,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen", "J.-Y. Nie"], "venue": "In Proc. CIKM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In Proc. ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Proactive computing", "author": ["D. Tennenhouse"], "venue": "Commun. ACM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "Writing in the library: Exploring tighter integration of digital library use with the writing process", "author": ["M.B. Twidale", "A.A. Gruzd", "D.M. Nichols"], "venue": "Information Processing & Management,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "A theory of the task-based information retrieval process: a summary and generalization of a longitudinal study", "author": ["P. Vakkari"], "venue": "Journal of Documentation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Term-by-term query auto-completion for mobile search", "author": ["S. Vargas", "R. Blanco", "P. Mika"], "venue": "In Proc. WSDM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Context-aware ranking in web search", "author": ["B. Xiang", "D. Jiang", "J. Pei", "X. Sun", "E. Chen", "H. Li"], "venue": "In Proc. SIGIR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "Proactive systems [31] anticipate the needs of the user and predict possible next steps based on the user\u2019s preferences and current context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "An ideal search engine would support searching and identifying useful information that can then be used in solving these tasks [33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "In proactive information retrieval [5] the estimation of the current task and context are utilized to proactively retrieve and recommend relevant items.", "startOffset": 35, "endOffset": 38}, {"referenceID": 26, "context": ", in a situation where the user does not realize having forgotten about a specific resource [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 195, "endOffset": 199}, {"referenceID": 34, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 218, "endOffset": 222}, {"referenceID": 17, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 25, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 27, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 14, "context": "LSTM networks have also been used to generate various kinds of sequences, including text [15, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 28, "context": "LSTM networks have also been used to generate various kinds of sequences, including text [15, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 23, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 26, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 32, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 6, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 139, "endOffset": 142}, {"referenceID": 14, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 149, "endOffset": 157}, {"referenceID": 28, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 149, "endOffset": 157}, {"referenceID": 14, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 175, "endOffset": 179}, {"referenceID": 25, "context": ", for extracting sentence-level semantic vectors [26] and context-aware query suggestion [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": ", for extracting sentence-level semantic vectors [26] and context-aware query suggestion [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Other kinds of deep neural networks have been used to project queries and documents to low-dimensional semantic spaces [18] and to learn fixed-length vectors for variable-length pieces of texts, such as sentences, paragraphs, and documents [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Other kinds of deep neural networks have been used to project queries and documents to low-dimensional semantic spaces [18] and to learn fixed-length vectors for variable-length pieces of texts, such as sentences, paragraphs, and documents [21].", "startOffset": 240, "endOffset": 244}, {"referenceID": 9, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 19, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 21, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 8, "context": "Another popular approach is to use previous search queries [9, 36].", "startOffset": 59, "endOffset": 66}, {"referenceID": 35, "context": "Another popular approach is to use previous search queries [9, 36].", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "Our work is related to query auto-completion [4], in which possible completions of search engine queries are predicted.", "startOffset": 45, "endOffset": 48}, {"referenceID": 24, "context": "Query auto-completion for web and mobile queries has recently been studied in [25, 34].", "startOffset": 78, "endOffset": 86}, {"referenceID": 33, "context": "Query auto-completion for web and mobile queries has recently been studied in [25, 34].", "startOffset": 78, "endOffset": 86}, {"referenceID": 2, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 22, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 31, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 23, "context": "In [24], the impact of proactive recommendations to different phases of the writing process is analyzed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The Reactive Keyboard [11] is an early prototype for predicting the succeeding words from a user-written text fragment.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Completing written sentences has also been studied in [6, 14].", "startOffset": 54, "endOffset": 61}, {"referenceID": 13, "context": "Completing written sentences has also been studied in [6, 14].", "startOffset": 54, "endOffset": 61}, {"referenceID": 26, "context": "The setup of our work is closely related to what was envisioned already by Rhodes and Starner in their Remembrance Agent (RA) [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "Other similar tools include Watson [8] and Implicit Query [12].", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "Other similar tools include Watson [8] and Implicit Query [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "For computing the continuations, we use a beam search algorithm [19] described below.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "As a comparison method, we use an upper confidence bound algorithm, which is based on estimating the user intent using a multi-armed bandit model [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The model balances exploration with exploitation and selects words that have the highest upper confidence bound [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 36, "context": "[37] with an Tensorflow implementation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 187, "endOffset": 190}, {"referenceID": 13, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 195, "endOffset": 199}], "year": 2016, "abstractText": "We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task. In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user\u2019s past actions. We focus on the task of writing, in which the user is coalescing previously collected information into a text. Our proactive system automatically recommends the user relevant background information. The proposed system incorporates text input prediction using a long short-term memory (LSTM) network. We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system.", "creator": "LaTeX with hyperref package"}}}