{"id": "1606.06160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients", "abstract": "We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFatNet opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 4-bit gradients to get 47\\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.", "histories": [["v1", "Mon, 20 Jun 2016 15:02:31 GMT  (45kb,D)", "http://arxiv.org/abs/1606.06160v1", null], ["v2", "Sun, 17 Jul 2016 14:21:03 GMT  (84kb,D)", "http://arxiv.org/abs/1606.06160v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["shuchang zhou", "yuxin wu", "zekun ni", "xinyu zhou", "he wen", "yuheng zou"], "accepted": false, "id": "1606.06160"}, "pdf": {"name": "1606.06160.pdf", "metadata": {"source": "CRF", "title": "DoReFa-Net: Training Low Bitwidth Con- volutional Neural Networks with Low Bitwidth Gradients", "authors": ["Shuchang Zhou", "Xinyu Zhou", "Yuxin Wu", "Yuheng Zou"], "emails": ["shuchang.zhou@gmail.com", "mike.zekun@gmail.com", "zxy@megvii.com", "wenhe@megvii.com", "wyx@megvii.com", "zouyuheng@megvii.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to survive on their own."}, {"heading": "2 DoReFa-Net", "text": "In this section, we describe our formulation of DoReFa-Net, a neural network that has low bit-width weights, activations, and gradients, and can be trained from scratch with low bit-width gradients. We note that weights and activations can be quantified deterministically, but gradients need to be quantified stochastically. We first outline how to exploit the bit-folding core in DoReFa-Net, and then work out the method for quantifying weights, activations, and gradients down to low bit-width numbers."}, {"heading": "2.1 Using Bit Convolution Kernels in Low Bitwidth Neural Network", "text": "Suppose x is a sequence of M-bit fixed-point numbers s.t. x = \u2211 M m = 1 cm (x) 2m and y is a sequence of K-bit fixed-point numbers s.t. y = \u2211 K k = 1 ck (y) 2k, where cm (x) and ck (y) are bit vectors, the point product2The model and the supplementary materials are available at https: / / drive.google.com / a / megvii.com / folderview? id = 0B308TeQzmFDLa0xOeVQwcXg1ZjQof x and y."}, {"heading": "2.2 Straight-Through Estimator", "text": "The set of real numbers, which can be represented by a low bit width k, has only a small ordinality 2k. Mathematically, however, any continuous function whose range is a small finite set would necessarily always have a zero gradient in relation to its input. We apply the straightthrough estimator (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to work around this problem. An STE can be considered an operator that has humanly defined forward and backward operations. A simple example is the STE method defined for Bernoulli samples with the probability p [0, 1]: Forward: q; Bernoulli (p) Backward: annual c; forward: annual c; forward: annual c; forward c. \u2212 Here c denotes the objective function."}, {"heading": "2.3 Low Bitwidth Quantization of Weights", "text": "In this section, we detail our approach to obtaining low bit-width weights. In previous work, STE was more commonly used to binarize weights. For example, in BNN, weights are binarized by the following STE: Forward: ro = sign (ri) Backward: \u2202 c \u2202 ri = Clip (\u2202 c \u2202 ro, \u2212 1, 1). Note that although characters (ri) can generate exact zero, if ri = 0, in practice, due to the limited precision of floating-point numbers, ro will almost certainly have two possible values: {\u2212 1, 1}. In XNOR-Net, weights are binarized by the following STE, the difference being that weights are binarized by: Forward: sign (ri) \u00b7 EF (ri |) Backward: We will find a difference between these two possible values. (XNOR-Net, weights are binarized by the following STE, where the difference is that weights are binarized by weights)."}, {"heading": "2.4 Low Bitwidth Quantization of Activations", "text": "Next, we will explain our approach to activating low bit width, which is input of waves, which is critical to replacing floating-point waves with less computationally intensive bit waves. In BNN and XNOR-Net, activations are binarized in the same way as weights. However, we do not succeed in reproducing the results of XNOR-Net if we follow their methods of binarizing activation, and the binarizing approach in BNN is blamed by (Rastegari et al., 2016) for a sharp decrease in predictive accuracy. Instead, we apply a different STE to the input activations r of each weight layer. Here, we assume that the output of the previous layer has gone through a limited activation function h, which ensures that r [0, 1]. In DoReFa-Net, quantifying activations r on k-bit is simple: fk\u03b1 (r) = quantizek (10) (We have already observed that 2."}, {"heading": "2.5 Low Bitwidth Quantization of Gradients", "text": "We have demonstrated deterministic quantization to produce low bit-width weights and activations. However, we find stochastic quantization is necessary for low bit-width gradients to be effective, in line with experiments of (Gupta et al., 2015) on 16-bit weights and 16-bit gradients. To quantify gradients to low bit-width, it is important to note that the gradient size is infinite and can be significantly larger than the activation. In Eqn. 10, we consider the range of activation to [0, 1] by introducing appropriate nonlinear activations. However, this kind of convenience does not exist for gradients and we would actually prefer to leave the gradient size untouched for SGD to function. Therefore, we have designed the following k-bit quantization function: fk\u03b3 (dr) = 2 max0 (dr | dr)."}, {"heading": "2.6 The Algorithm for DoReFa-Net", "text": "We give an example training algorithm of DoReFa-Net as algorithm 1. w.l.o.g., the network is assumed to have a linear feed topology, and details such as batch normalization and pooling layers are omitted. Note that all expensive operations forward, backward, backward, backward weighted, both in revolutionary and fully connected layers, now work with low bit-width numbers. Constructively, there is always an affinity between these low bit-width numbers and fixed-point integers. As a result, all expensive operations can be significantly accelerated by the product core with the fixed point Integer Dot (equation 2)."}, {"heading": "2.7 First and the last layer", "text": "Among all the layers in a DCNN, the first and last layers seem to be different from the rest, because they combine the input and output of the network. For the first layer, the input is often an image that can contain 8-bit characteristics. On the other hand, the output layer typically produces about a hot vector that by definition comes close to bit vectors. It is an interesting question whether these differences would cause the first and last layers to exhibit different behaviors when they are converted into counterparts with low bit width. In the related work of (Han et al., 2015b), the network weights are converted to sparse tensors, with the same ratio of zeros found in the first convolution layer to cause morealgorithm 1. Training an L-layer DoReFa-Net with W-bit weights and A-bit activations using G-bit gradients, weights are quantified according to Eqn, 8 and Eqn, respectively, 1.1."}, {"heading": "2.8 Reducing Run-time Memory Footprint by Fusing", "text": "A naive implementation of algorithm 1 would involve applying a nonlinear activation h to ak, which would result in a complete precision of the numbers and consume a lot of memory during runtime. Also, if h includes floating-point arithmetic, there will be a not negligible amount of non-bitwise operations associated with calculations of ak. There are simple solutions to this problem. Note that it is possible to merge step 3, step 4, step 6 to avoid storing ak in full precision. Apart from the fact that if h is monoton, f\u03b1 \u00b7 h is also monoton, the few possible values of abk correspond to several non-overlapping ranges of ak, so we can implement the calculation of abk = f\u03b1 (ak) olence by multiple comparisons and avoid generating intermediate results. Similarly, it would be desirable to compare several non-overlapping ranges of ak."}, {"heading": "3 Experiment Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Configuration Space Exploration", "text": "The SVHN dataset (Netzer et al., 2011) is a real dataset for detecting digits, consisting of photos of house numbers in Google Street View images. We look at the \"truncated\" format of the dataset: 32 by 32 color images centered around a single character. There are 73257 digits for training, 26032 digits for testing, and 531131 less difficult samples that can be used as additional training data. Images are enlarged to 40x40 before they are fed into a network. For configurations in a DoReFa Net, if we have W-bit weights, A-bit activations, and G-bit gradients, the relative forward and backward calculation complexes are calculated, and we list them in Table 1."}, {"heading": "1 2 32 - - 1 0.976 0.950 0.873 0.865", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 2 4 6 2 1 0.975 0.969 0.939 0.878", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 2 8 10 2 1 0.975 0.971 0.946 0.866", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 3 4 7 3 1 0.974 0.974 0.959 0.897", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 4 4 8 4 1 0.975 0.974 0.962 0.915", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "32 32 32 - - 32 0.975 0.975 0.972 0.950", "text": "Model B, C, D is derived from Model A by decreasing the number of channels for all seven revolutionary layers by 50%, 75%, 87.5%. The stated predictive accuracy is the maximum accuracy over 200 epochs. We use ADAM (Kingma & Ba, 2014) learning rule with 0.001 as the learning rate. Generally, balancing several factors such as training time, inference time, model size and accuracy is rather a matter of practical compromise, there will be no clear conclusion as to which combination of (W, A, G) should be chosen. However, in these experiments we find that weights, activations and gradients are a bit in interpretation."}, {"heading": "3.2 ImageNet", "text": "The AlexNet base model, which achieves 55.9% accuracy in one section, is a top-1 replica of the model in (Krizhevsky et al., 2012), splitting the second, fourth and fifth turns into two blocks. We replace the local contrast renormalization layer with the batch normalization layer (Ioffe & Szegedy, 2015). According to the table, increasing the bit width of the model from 1-bit to 2-bit and even to 4-bit leads to a significant increase in the accuracy of 4-bit, with the accuracy quickly increasing from 4-bit to 32-bit, with the accuracy being trained from 1-bit to 2-bit."}, {"heading": "3.2.1 Training curves", "text": "Figure 1 shows the development of accuracy compared to the epoch curves of DoReFa-Net. It can be seen that the quantification of inclines with 4 bits does not lead to a significant difference between the training curve and the quantification of inclines."}, {"heading": "3.3 Making First and Last Layer low bitwidth", "text": "To answer the question of whether the first and last layers need special treatment when quantifying to small bit width, we use the same models A, B, C from Table 1 to find out whether it is cost-effective to quantify the first and last layers to low bit width and collect the results in Table 3. It turns out that quantifying the first and last layers actually leads to a deterioration in accuracy, and models with less channel count suffer more. On AlexNet, the deterioration is significant, which justifies the practice of BNN and XNOR-net to some extent that do not quantify these two layers."}, {"heading": "4 Discussion and Related Work", "text": "By binarizing weights and activations, binarized neural networks such as BNN and XNOR-Net enabled the acceleration of the forward motion of neural networks with bit convolution nucleus. However, the reverse trajectory of binarized networks still requires windings between floating point gradients and weights that could not efficiently exploit the bit convolution nucleus, since gradients generally do not have low bit width numbers and are prone to error. (Lin et al., 2015) There is also another set of papers (Seide et al., 2014) that quantify gradients in distributed computation settings by converting some multiplications into bit shifts. However, the work is more about reducing the number of additions between high bit numbers than before."}, {"heading": "5 Conclusion and Future Work", "text": "We find that weights and activations can be quantified deterministically, while gradients need to be quantified stochastically. As most turns now have low bit width weights or activations / gradients during forward and backward passes, DoReFa-Net can use the bit convolution cores to accelerate both the training and inference processes. Our experiments with SVHN and ImageNet datasets show that DoReFa-Net can achieve a comparable predictive accuracy to its 32-bit counterparts. As a future work, it would be interesting to investigate the use of FPGA to form DoReFa-Net, as the resource requirement O (B2) for B-bit arithmetic on FPGA prefers very low bit width rotations."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi", "Mart\u0131n", "Agarwal", "Ashish", "Barham", "Paul", "Brevdo", "Eugene", "Chen", "Zhifeng", "Citro", "Craig", "Corrado", "Greg S", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Chen", "Tianshi", "Du", "Zidong", "Sun", "Ninghui", "Wang", "Jia", "Wu", "Chengyong", "Yunji", "Temam", "Olivier"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Chen", "Yunji", "Luo", "Tao", "Liu", "Shaoli", "Zhang", "Shijin", "He", "Liqiang", "Wang", "Jia", "Li", "Ling", "Tianshi", "Xu", "Zhiwei", "Sun", "Ninghui"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Large-scale fpga-based convolutional networks. Scaling up Machine Learning: Parallel and Distributed Approaches", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": null, "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey", "Srivastava", "Nitsh", "Swersky", "Kevin"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Bitwise neural networks", "author": ["Kim", "Minje", "Smaragdis", "Paris"], "venue": "arXiv preprint arXiv:1601.06071,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ternary weight networks", "author": ["Li", "Fengfu", "Liu", "Bin"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deep neural networks are robust to weight binarization and other nonlinear distortions", "author": ["Merolla", "Paul", "Appuswamy", "Rathinakumar", "Arthur", "John", "Esser", "Steve K", "Modha", "Dharmendra"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Pham", "Phi-Hung", "Jelaca", "Darko", "Farabet", "Clement", "Martini", "Berin", "LeCun", "Yann", "Culurciello", "Eugenio"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["Seide", "Frank", "Fu", "Hao", "Droppo", "Jasha", "Li", "Gang", "Yu", "Dong"], "venue": "In INTERSPEECH,", "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["Wu", "Jiaxiang", "Leng", "Cong", "Wang", "Yuhang", "Hu", "Qinghao", "Cheng", "Jian"], "venue": "arXiv preprint arXiv:1512.06473,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Recent progress in deep Convolutional Neural Networks (DCNN) has considerably changed the landscape of computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 1, "context": ", 2012a) and NLP (Bahdanau et al., 2014).", "startOffset": 17, "endOffset": 40}, {"referenceID": 7, "context": "For example, the training process of a DCNN may take up to weeks on a modern multiGPU server for large datasets like ImageNet (Deng et al., 2009).", "startOffset": 126, "endOffset": 145}, {"referenceID": 27, "context": "In light of this, substantial research efforts are invested in speeding up DCNNs at both run-time and training-time, on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 141, "endOffset": 203}, {"referenceID": 9, "context": "In light of this, substantial research efforts are invested in speeding up DCNNs at both run-time and training-time, on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 141, "endOffset": 203}, {"referenceID": 28, "context": "Various approaches like quantization (Wu et al., 2015) and sparsification (Han et al.", "startOffset": 37, "endOffset": 54}, {"referenceID": 6, "context": "Recent research efforts (Courbariaux et al., 2014; Kim & Smaragdis, 2016; Rastegari et al., 2016; Li & Liu, 2016; Merolla et al., 2016) have considerably reduced both model size and computation complexity by using low bitwidth weights and low bitwidth activations.", "startOffset": 24, "endOffset": 135}, {"referenceID": 25, "context": "Recent research efforts (Courbariaux et al., 2014; Kim & Smaragdis, 2016; Rastegari et al., 2016; Li & Liu, 2016; Merolla et al., 2016) have considerably reduced both model size and computation complexity by using low bitwidth weights and low bitwidth activations.", "startOffset": 24, "endOffset": 135}, {"referenceID": 25, "context": "In particular, in BNN (Courbariaux & Bengio, 2016) and XNOR-Net (Rastegari et al., 2016), both weights and input activations of convolutional layers are binarized.", "startOffset": 64, "endOffset": 88}, {"referenceID": 10, "context": "In some previous research (Gupta et al., 2015; Courbariaux et al., 2014), convolutions involve at least 10-bit numbers.", "startOffset": 26, "endOffset": 72}, {"referenceID": 6, "context": "In some previous research (Gupta et al., 2015; Courbariaux et al., 2014), convolutions involve at least 10-bit numbers.", "startOffset": 26, "endOffset": 72}, {"referenceID": 18, "context": ") format a DoReFa-Net 2 derived from AlexNet (Krizhevsky et al., 2012) that gets 47% in single-crop top-1 accuracy on ILSVRC12 validation set.", "startOffset": 45, "endOffset": 70}, {"referenceID": 2, "context": "We adopt the \u201cstraightthrough estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem.", "startOffset": 54, "endOffset": 97}, {"referenceID": 25, "context": "However, we fail to reproduce the results of XNOR-Net if we follow their methods of binarizing activations, and the binarizing approach in BNN is claimed by (Rastegari et al., 2016) to cause severe prediction accuracy drop.", "startOffset": 157, "endOffset": 181}, {"referenceID": 10, "context": "This is in agreement with experiments of (Gupta et al., 2015) on 16-bit weights and 16-bit gradients.", "startOffset": 41, "endOffset": 61}, {"referenceID": 23, "context": "The SVHN dataset (Netzer et al., 2011) is a real-world digit recognition dataset consisting of photos of house numbers in Google Street View images.", "startOffset": 17, "endOffset": 38}, {"referenceID": 22, "context": "The credit may belong to the regularization effect introduced by the noise in gradients, similar to previous works in adding noise to gradients (Neelakantan et al., 2015).", "startOffset": 144, "endOffset": 170}, {"referenceID": 7, "context": "We further evaluates our method on ILSVRC12 (Deng et al., 2009) image classification dataset, which contains about 1.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "9% single-crop top-1 accuracy is a best-effort replication of the model in (Krizhevsky et al., 2012), with the second, fourth and fifth convolutions split into two blocks.", "startOffset": 75, "endOffset": 100}, {"referenceID": 25, "context": "Note the BNN result is reported by (Rastegari et al., 2016), not by original authors.", "startOffset": 35, "endOffset": 59}, {"referenceID": 20, "context": "(Lin et al., 2015) makes a step further towards low bitwidth gradients by converting some multiplications to bit-shift.", "startOffset": 0, "endOffset": 18}, {"referenceID": 26, "context": "There is also another series of work (Seide et al., 2014) that quantizes gradients in distributed computation settings.", "startOffset": 37, "endOffset": 57}], "year": 2016, "abstractText": "We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFatNet opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 4-bit gradients to get 47% top-1 accuracy on ImageNet validation set.1 The DoReFa-Net AlexNet model is released publicly.", "creator": "LaTeX with hyperref package"}}}