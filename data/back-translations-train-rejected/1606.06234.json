{"id": "1606.06234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and FPGA-a Practical Study with Trade-off Analysis", "abstract": "Designing and implementing efficient, provably correct parallel neural network processing is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. However, the diversity and large-scale data size have posed a significant challenge to construct a flexible and high-performance implementation of deep learning neural networks. To improve the performance and maintain the scalability, we present CNNLab, a novel deep learning framework using GPU and FPGA-based accelerators. CNNLab provides a uniform programming model to users so that the hardware implementation and the scheduling are invisible to the programmers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA before offloading the tasks to the accelerators. Experimental results on the state-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the CNNLab can provide a universal framework with efficient support for diverse applications without increasing the burden of the programmers. Moreover, we analyze the detailed quantitative performance, throughput, power, energy, and performance density for both approaches. Experimental results leverage the trade-offs between GPU and FPGA and provide useful practical experiences for the deep learning research community.", "histories": [["v1", "Mon, 20 Jun 2016 18:22:09 GMT  (1379kb,D)", "http://arxiv.org/abs/1606.06234v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["maohua zhu", "liu liu", "chao wang", "yuan xie"], "accepted": false, "id": "1606.06234"}, "pdf": {"name": "1606.06234.pdf", "metadata": {"source": "CRF", "title": "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and FPGA", "authors": ["Maohua Zhu", "Liu Liu", "Chao Wang", "Yuan Xie"], "emails": ["liu}@umail.ucsb.edu", "cswang@ustc.edu.cn", "yuanxie@ece.ucsb.edu"], "sections": [{"heading": null, "text": "This year, it will be able to leave the country to return to the EU and leave the EU."}, {"heading": "II. PROBLEM DESCRIPTION AND MOTIVATION", "text": "Although Deep and Convolutional Neural Networks take different forms, they share similar characteristics that a generic description can formalize. First, these algorithms consist of a large number of layers that are normally executed in sequence so that they can be implemented and evaluated separately. Second, each layer usually contains several sublayers called feature maps; we then use the terms feature input maps and feature output maps. Overall, there are three main types of layers: most layers of the hierarchy consist of revolutionary and pooling layers, and there is a classification at the top of the network consisting of one or more layers. The role of the evolution layers is to apply one or more local filters to data from the input layer. Consequently, the connectivity between the input and output function is local."}, {"heading": "III. THE CNNLAB ABSTRACTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data Model and Processing Flow", "text": "In this section, we present the modeling framework of the CNNLab architecture. Fig. 2 illustrates the common infrastructure of CNNLab from a high-level perspective. Frontend cloud users access the CNNlab platform via a unified programming interface with the user definitions of the CNNLab model, packaging and exposing each layer in an API-like manner. In the backend, each layer consists of specific functionality provided by software libraries and resource pools via consistent communication interfaces. Functionality for each layer is combined with multiple data resources with a sequence of input / output parameters. Note that the detailed composition and middleware planning are invisible to front-end software users. At runtime, the application is initially broken down into several layers, taking into account specific parameters that are then planned at runtime."}, {"heading": "B. User Defined Computation", "text": "To describe the CNN model, we define each layer associated with a tuple of parameters. Currently, the following layer types are supported: \u2022 MI, MK, MO, S, T > (5), where \u2022 MI and MO are the input / output matrix of each Convolutionary layer, covering the height x width x dimension. \u2022 MK refers to the grain size with which each accelerator can be pro-machined. \u2022 S is the step that defines the step between successive Convolutionary windows. \u2022 T is the type of nonlinear function to be applied, i.e. sigmoid, tanh or ReLU. 2) The normalization layer is the layer x."}, {"heading": "C. Programming Model and Wrapper", "text": "Based on the flexible programming framework in the CNN laboratory, the tasks can be distributed to either GPU or FPGA-based accelerators. In particular, middleware support should provide uniform runtime for different hardware architectures. Fig. 4 illustrates the general programming framework based on the runtime kernel. The API forwards the requests via the scheduling middleware to the host code. The host code can transfer a portion of the execution threads to the CUDA kernel or OpenCL kernel, depending on the accelerator architecture and design space exploration. Different cores share a virtual space for communication between the accelerator parameters. The scheduling process and runtime support are invisible to programmers, as the API provides an efficient bridge for high-level applications to hardware implementations. In particular, we use two sample code snippets that are available as CDN and C5 snippets, as well as PDA decoders."}, {"heading": "IV. EXPERIMENTS RESULTS AND TRADE-OFF ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Platform Setup and Network Description", "text": "To measure the performance of the CNNLab architecture, we implemented a hardware prototype with hybrid heterogeneous systems: An Intel Corei7-4770 processor clocked at 3.4 GHz was used as the CPU controller, and the CPU processor points the computing tasks to the acceleration costs of PCIE X8 edge connector for interconnection.FPGA: An Intel-Altera DE5 was used to implement the design of the deep learning module. Altera Quartus II toolchain to evaluate the speedup and hardware cost, as the PowerPlay to assess the power connex."}, {"heading": "B. Results Analysis and Trade-offs between FPGA and GPU", "text": "We analyze the two approaches on the following aspects: performance (including execution time and throughput), cost (including power and energy), and power density (including throughput per watt and throughput per joule) respectively power. Power (Fig.) represents the runtime for the eight layers. GPU has better performance than FPGA on all layers, and the speedup can reach up to 1000x for the layers. In terms of the eight layers that speedup for the conventional layers (1-5), the layer is lower than the layer (6-8) that includes matrix multiplication."}, {"heading": "C. Comparison between Different GPU Models", "text": "In this section, to evaluate the effects of the various GPU library models, we use both cuDNN and cuBLAS to convert the FC layers into forward computation and backflow, as illustrated in Table II.Fig.7 and Fig. 8, the comparison between cuDNN and cuBLAS for forward computation and backflow, respectively. We have the following observations based on the experimental results: \u2022 In general, the cuBLAS library cores achieve a higher acceleration (computed after execution time) than the cuDNN library cores. In particular, the acceleration for cuBLAS versus cuDNN is 1.69x in forward computation and 24.89x in BP. In comparison, the throughput for cuBLASIS is 1.77x higher than cuDNN in forward computation."}, {"heading": "D. Resources Usage and Running Frequency", "text": "Table III lists the resources and power consumption for the modules in the CNNLab accelerator. Of these NN layers, the Convolutionary Layer absorbs the most important logic components because it requires computing power. Specifically, the Convolutionary Layer requires 73% of hardware logic, 63% of DSP blocks and 56% of RAM blocks. In comparison, the Pooling Layer requires only 17% of logic resources and 11% of RAM blocks. In terms of running frequency, the Convolutionary Layer has the lowest frequency at 171.29 MHz, while pooling reaches the highest frequency at 304.50 MHz."}, {"heading": "V. RELATED WORK", "text": "The neural network model has emerged as an emerging field in recent years [5]. In this section, we summarize the associated accelerators, including cloud computing, GPU, and FPGA."}, {"heading": "A. Cloud based Acceleration", "text": "Distributed computing platforms are widely recognized as scalable and easy to use measures [6]. Project Adam [7] describes the design and implementation of a distributed system consisting of commodity server computers to train large deep learning models. SINGA [8] is a distributed deep learning system for training large models over large data sets. DistBelief [9] is a software framework that can use computing clusters with a good number of machines to train large models."}, {"heading": "B. GPU based Accelerators", "text": "For example, Coates et. al [10] present a supercomputer system with a cluster of GPU servers that uses Infiniband interconnects and MPI. NGPU [11] brings together GPU accelerators without impeding the execution of SIMT or adding excessive hardware overhead. Li et al. [12] propose an efficient GPU implementation of the large recursive neural network and demonstrate the performance of scaling the recursive neural network with GPUs. Vasilache et al. investigate the performance profile using fbfft CNN training on the current GPU generation [13]. Teng et al. describe efficient DBN implementation on the GPU, including pre-training and fine-tuning processes [14]."}, {"heading": "C. FPGA and Hardware based Accelerators", "text": "In order to overcome the problem of power consumption of the GPU and cloud-based frameworks, many developers are looking for hardware-level solutions [16], [17], [18]. For the IC-based accelerator, Diannao [3] is one of the pioneers in solidifying neural networks on hardware circuits. Origami [19] presents a band-out accelerator with silicon measurements of power, area and I / O efficiency. Meanwhile, FPGA is more flexible due to the integration of the reconfigurable logic components. Therefore, it can adapt to changing applications and parameters in neural networks [20], [21]. Zhang et al. [2], for example, examines the bandwidth of parameters faced with the limitation of an FPGA chip. Suda et al. [22] presents a design space exploration method OpenCL programming model that can explore the compromises between the network parameters in the topologies."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "FPGA and GPU have proven to be very powerful and flexible platforms for data-intensive neural network processing in machine learning applications. In this paper, we introduced CNNLab, a middleware support for GPU and FPGA-based framework to accelerate neural network computer models. It can shift tasks to different accelerators in controlling the neural network model and constraints. To achieve the target conflicts between the GPU and FPGA-based platform, we constructed the real prototype with Intel Altera DE5 FPGA board and Nvidia K40 GPU platform. We measured the execution time, throughput, power consumption, energy costs and power density respectfully. Experimental results show that the GPU has better acceleration (100x) and throughput (100x) compared to FPGA-based accelerators, while FPGA-based accelerators are PDA-based accelerators (14x) than PGA-FPGU, while PDA-based accelerators are more efficient than FPGA-50x."}], "references": [{"title": "Deep leaning", "author": ["G.H. Yann LeCun", "Yoshua Bengio"], "venue": "Nature, vol. 521, pp. 436\u2013444, May 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimizing fpga-based accelerator design for deep convolutional neural networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "FPGA \u201915, pp. 161\u2013170, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "ASPLOS \u201914, pp. 269\u2013284, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural acceleration for gpu throughput processors", "author": ["A. Yazdanbakhsh", "J. Park", "H. Sharma", "P. Lotfi-Kamran", "H. Esmaeilzadeh"], "venue": "MI- CRO\u201915, pp. 482\u2013493, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "ICLR\u201916, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "ICML\u201912, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "OSDI\u201914, pp. 571\u2013582, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Singa: A distributed deep learning platform", "author": ["B.C. Ooi", "K.-L. Tan", "S. Wang", "W. Wang", "Q. Cai", "G. Chen", "J. Gao", "Z. Luo", "A.K. Tung", "Y. Wang", "Z. Xie", "M. Zhang", "K. Zheng"], "venue": "MM \u201915, pp. 685\u2013688, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le", "A.Y. Ng"], "venue": "NIPS\u201912, pp. 1232\u20131240, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning with cots hpc systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D. Wu", "B. Catanzaro", "N. Andrew"], "venue": "ICML\u201913, vol. 28, pp. 1337\u2013 1345, May 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural acceleration for gpu throughput processors", "author": ["A. Yazdanbakhsh", "J. Park", "H. Sharma", "P. Lotfi-Kamran", "H. Esmaeilzadeh"], "venue": "MI- CRO\u201915, pp. 482\u2013493, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale recurrent neural network on gpu", "author": ["B. Li", "E. Zhou", "B. Huang", "J. Duan", "Y. Wang", "N. Xu", "J. Zhang", "H. Yang"], "venue": "IJCNN\u201914, pp. 4062\u20134069, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "ICLR\u201915, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimized deep belief networks on cuda gpus", "author": ["T. Li", "Y. Dou", "J. Jiang", "Y. Wang", "Q. Lv"], "venue": "IJCNN\u201915, pp. 1\u20138, July 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server", "author": ["H. Cui", "H. Zhang", "G.R. Ganger", "P.B. Gibbons", "E.P. Xing"], "venue": "EuroSys \u201916, pp. 4:1\u20134:16, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "14.1 a 126.1mw real-time natural ui/ux processor with embedded deep-learning core for low-power smart glasses", "author": ["S. Park", "S. Choi", "J. Lee", "M. Kim", "J. Park", "H.J. Yoo"], "venue": "ISSCC\u201916, pp. 254\u2013255, Jan 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "14.5 eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "ISSCC\u201916, pp. 262\u2013263, Jan 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "14.6 a 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems", "author": ["J. Sim", "J.S. Park", "M. Kim", "D. Bae", "Y. Choi", "L.S. Kim"], "venue": "ISSCC\u201916, pp. 264\u2013265, Jan 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Origami: A convolutional network accelerator", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini"], "venue": "GLSVLSI \u201915, pp. 199\u2013204, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning on fpgas: Past, present, and future", "author": ["G. Lacey", "G.W. Taylor", "S. Areibi"], "venue": "arXiv preprint arXiv:1602.04283, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "fpgaconvnet: A framework for mapping convolutional neural networks on fpgas", "author": ["S.I. Venieris", "C.-S. Bouganis"], "venue": "FCCM\u201916, pp. 40\u201347, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks", "author": ["N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S. Vrudhula", "J.-s. Seo", "Y. Cao"], "venue": "FPGA, pp. 16\u201325, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Memristive boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning", "author": ["M.N. Bojnordi", "E. Ipek"], "venue": "HPCA\u201916, pp. 1\u201313, March 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A reconfigurable digital neuromorphic processor with memristive synaptic crossbar for cognitive computing", "author": ["Y. Kim", "Y. Zhang", "P. Li"], "venue": "J. Emerg. Technol. Comput. Syst., vol. 11, pp. 38:1\u201338:25, Apr. 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Eie: Efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "ISCA\u201916, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Consequently, deep learning has become a research hotspot in research organizations and the companies[1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "The neural network model has been an emerging field during the past few years [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Distributed computing platforms have been widely recognized as the scalable, and easy-to-deploy measures [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Project Adam [7] describes the design and implementation of a distributed system comprised of commodity server machines to train large-scale deep learning models.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "SINGA [8] is a distributed deep learning system for training big models over large datasets.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "DistBelief [9] is a software framework that can utilize computing clusters with a good number of machines to train large models.", "startOffset": 11, "endOffset": 14}, {"referenceID": 9, "context": "al [10] present a high-performance computing system with a cluster of GPU servers, using Infiniband interconnects and MPI.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "NGPU [11] brings GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "[12] propose an efficient GPU implementation of the large-scale recurrent neural network and demonstrate the power of scaling up the recurrent neural network with GPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "examine the performance profile using fbfft of CNN training on the current generation of GPU [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "describe an efficient DBN implementation on the GPU, including the pre-training and fine-tuning processes [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "Recently, GeePS is a scalable deep learning architecture on distributed GPUs with specific parameters [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "Diannao [3] is one of the pioneers works solidifying the neural networks on the hardware circuits.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "Origami [19] present a tape-out accelerator with silicon measurements of power-, area- and I/O efficiency.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "Therefore, it can fit changing applications and parameters in neural networks [20], [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "Therefore, it can fit changing applications and parameters in neural networks [20], [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "[2] explores the bandwidth for the parameters facing the limitation of an FPGA chip.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] presents a design space exploration method OpenCL programming model approach, which can explore the trade-offs the parameters in the network topologies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].", "startOffset": 199, "endOffset": 203}, {"referenceID": 24, "context": "Energy efficient inference engine (EIE) uses compression by pruning the redundant connections and having multiple connections share the same weight [25].", "startOffset": 148, "endOffset": 152}], "year": 2016, "abstractText": "Designing and implementing efficient, provably correct parallel neural network processing is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. However, the diversity and large-scale data size have posed a significant challenge to construct a flexible and high-performance implementation of deep learning neural networks. To improve the performance and maintain the scalability, we present CNNLab, a novel deep learning framework using GPU and FPGA-based accelerators. CNNLab provides a uniform programming model to users so that the hardware implementation and the scheduling are invisible to the programmers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA before offloading the tasks to the accelerators. Experimental results on the state-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the CNNLab can provide a universal framework with efficient support for diverse applications without increasing the burden of the programmers. Moreover, we analyze the detailed quantitative performance, throughput, power, energy, and performance density for both approaches. Experimental results leverage the trade-offs between GPU and FPGA and provide useful practical experiences for the deep learning research community.", "creator": "LaTeX with hyperref package"}}}