{"id": "1606.06461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Neighborhood Mixture Model for Knowledge Base Completion", "abstract": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.", "histories": [["v1", "Tue, 21 Jun 2016 07:54:35 GMT  (46kb,D)", "http://arxiv.org/abs/1606.06461v1", "To appear in Proceedings of CoNLL 2016"], ["v2", "Thu, 21 Jul 2016 16:08:32 GMT  (40kb,D)", "http://arxiv.org/abs/1606.06461v2", "V1: To appear in Proceedings of CoNLL 2016. V2: Corrected citation to (Krompa{\\ss} et al., 2015)"], ["v3", "Thu, 9 Mar 2017 12:51:31 GMT  (42kb,D)", "http://arxiv.org/abs/1606.06461v3", "V1: In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016. V2: Corrected citation to (Krompa{\\ss} et al., 2015). V3: A revised version of our CoNLL 2016 paper to update latest related work"]], "COMMENTS": "To appear in Proceedings of CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dat quoc nguyen", "kairit sirts", "lizhen qu", "mark johnson"], "accepted": false, "id": "1606.06461"}, "pdf": {"name": "1606.06461.pdf", "metadata": {"source": "CRF", "title": "Neighborhood Mixture Model for Knowledge Base Completion\u2217", "authors": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson"], "emails": ["dat.nguyen@students.mq.edu.au,", "mark.johnson}@mq.edu.au", "lizhen.qu@data61.csiro.au"], "sections": [{"heading": null, "text": "Keywords: completion of the knowledge base, embedding model, mixing model, link prediction, triple classification, entity prediction, relation prediction."}, {"heading": "1 Introduction", "text": "The Knowledge Base (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (Head Entity, Relation, Tail Entity). Even very large knowledge bases are still far from being complete (Socher et al., 2013; West et al., 2014). Knowledge Base Completion or Link Prediction Systems (Nickel al., 2015) predict which triples do not exist in a knowledge base to be true (Taskar et al., 2004; Bordes et al., 2011). Embedding models for KB Completion Entities and / or Relations with dense Feature Vectors or Matrices. Such models receive state-of-the-art Proceedings of CoNLL 2016.al (Boret al, 2012)."}, {"heading": "2 Neighborhood mixture modeling", "text": "In this section, we begin by explaining the formal construction of neighborhood-based entity representations in Section 2.1 and then describe the Neighborhood Mixture Model applied to the TransE model (Bordes et al., 2013) in Section 2.2. Section 2.3 explains how we train our model."}, {"heading": "2.1 Neighbor-based entity representation", "text": "Let E denote the set of entities and R denote the set of relation types. Let R \u2212 1 denote the set of inverse relations r \u2212 1. With G denote the knowledge graph consisting of a series of correct tiples (h, r, t) so that h, t, E and r, R. Let K denote the symmetrical termination of G, i.e. if a triple (h, r, t), then both (h, r, t) and (t, r \u2212 1, h). Define: Ne, r = (e, r, e).K} as a set of adjacent entities associated with entity e in relation r. ThenNe = (e, r) | r, R, R \u2212 1, e \u00b2. Define: Ne, r} is the set of all entities and relation pairs associated with entity e."}, {"heading": "2.2 TransE-NMM: applying neighborhood mixtures to TransE", "text": "Embedding models define for each triple (h, r, t) a score function f (h, r, t) that measures its implausibility. The goal is to select f so that the score f (h, r, t) of a plausible triple (h, r, t) is smaller than the score f (h, r, t) of an implausible triple (h, r, t). TransE (Bordes et al., 2013) is a simple embedding model for completing the knowledge base that, despite its simplicity, achieves very competitive results (Garc\u00ed a-Dura) n et al., 2016; Nickel et al., 2016). In TransE, both entities e and relations r are represented with k-dimensional vectors ve-Rk and vr-Rk-Rk respectively in such a way that these vectors are chosen so that for each triple (h, r, ventilated vt) function 4 of the model is the R-translation r."}, {"heading": "2.3 Parameter optimization", "text": "The TransE-NMM model parameters include the vectors ve, vr, for entities and relation types, the entity-specific weights \u03b1 = {\u03b1e | e-E} and relation-specific weights \u03b2 = {\u03b2r, r, r, r, or, R \u2212 1}. To learn these parameters, we minimize the L2-regulated margin-based objective function: L = (h, r, t), (8), where [x] + = max (0, x), the margin hyperparameter is \u03bb (h, r, t) \u2212 f (h, r, t) \u2212 f (h, t)] + eco 2 (h, 22), where [x] + = max (0, x), \u03b3 is the margin hyperparameter, \u03bb is the L2 regulation parameter, and G \u2032 (h, r, t)."}, {"heading": "3 Related work", "text": "The models differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), or L-BFGS (Liu and Nocedal, 1989).The unstructured model (Bordes et al., 2012) assumes that the head and tail are similar vectors. As the unstructured model does not take the relationship into account, it cannot distinguish different relationship types. The Structured Embedding Model (SE) extends the unstructured model by assuming that head and tail entities are similar to a relationship-dependent subspace in which each relationship is represented by two different matrices."}, {"heading": "4 Experiments", "text": "To investigate the usefulness of the adjacent mixtures, we compare the performance of the TransENMM with the results of the base model TransE and other state-of-the-art embedding models for the tasks of triple classification, entity prediction and relation prediction."}, {"heading": "4.1 Datasets", "text": "We are conducting experiments with three publicly available datasets WN11, FB13 and NELL186, for all of which the validation and test sets containing both correct and false triples have already been created. Statistical information on these datasets is in Table 2.The two benchmark datasets 1, WN11 and FB13, were created by Socher et al. (2013) for the triple classification. WN11 is derived from the large lexical KB WordNet (Miller, 1995) with 11 relation types. FB13 is derived from the large real-world KB FreeBase (Bollacker et al., 2008), which covers 13 relation types. The NELL186 dataset was introduced by Guo et al. (2015) for both the triple classification and entity prediction tasks, which contain 186 common relationships in the KB project CMU Never Enguding Language (Carlson et al.)."}, {"heading": "4.2 Evaluation tasks", "text": "We evaluate our model based on three common benchmark tasks: three predictions and relationship outcomes. This subdivision describes these tasks in detail: The triple classification was first developed by Socher et al. (2013), and then it was used to evaluate different embedding models. The goal of the task is to predict whether a triple (r, t) is correct or not. Following the classification, we set a relationship-specific threshold for each type of relationship. The implausibility of a triple test (h, r, t) is less than three times what is considered correct."}, {"heading": "4.3 Hyper-parameter tuning", "text": "For all evaluation tasks, the results for TransE-NMM with TransE-NMM = 0.005 = 0.005 = 0.005 = 0.005 = 0.005 = 0.005 = for all evaluation tasks TransE-NMM = 0.005 = 0.005 = 0.005 = for all evaluation tasks TransE-NMM = 0.005 = 0.005 = 0.005 = 0.005 = for all evaluation tasks, we first select the \"1 or\" 2 standard in the score function f and the initial RMSProp learning rate for TransE by monitoring the micro-averaged triple classification accuracy after each training period at the validation period. Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; Ji et al., 2016), we select the margin hyper parameters from {1, 2} and 4} and the number of vector dimensions in the ork dimension."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Quantitative results", "text": "The results show that TransE-NMM generally performs better than TransE in all three evaluation tasks. Specifically, TransE-NMM achieves higher triple classification results than TransE in all three experimental datasets, for example, with a 2.44% absolute improvement in the micro-averaged accuracy of NELL186 datasets (i.e. a 31% reduction in the error rate). In terms of entity prediction, TransE-NMM achieves a better mean ranking, MRR and hits @ 10 scores in both FB13 and NELL186 datasets (i.e. a 31% reduction in the error rate). TransE-NMMM achieves better mean ranking in the task list, MRR and Hits @ 10 scores in both datasets FB13 and NELL186. Specifically, on NELL186 Transwords gains a significant improvement of 279 \u2212 214 = 65 in the filtered middle class, which is about 23% improvement."}, {"heading": "5.2 Qualitative results", "text": "Table 6 shows some examples to illustrate the useful information modeled by the neighbors. We took the relationship-specific mixture weights from the learned TransE-NMM model, which was optimized for the task of predicting entities, and then extracted three neighborhood relationships with the largest mixture weights underlying a relationship. Table 6 shows that these relationships are semantically coherent. For example, if we know the place of birth and / or place of death of a person and / or place of residence of the person, it is likely that we can predict the nationality of the person. On the other hand, if we know that a person works for an organization and that person is also the chief member of that organization, then it is possible that person is the CEO of that organization."}, {"heading": "5.3 Discussion", "text": "Despite the lower triple classification results of TransE reported in Wang et al. (2014), Table 4 shows that TransE does indeed achieve a very competitive accuracy. In particular, when compared to the TransE-COMP relation path model (Guu et al., 2015), when model parameters were randomly initialized, TransE achieves 85.2 \u2212 80.3 = 4.9% absolute improvement in accuracy on the WN11 dataset, while it achieves a similar value on the FB13 dataset. Our high results of the TransE model are probably due to careful network searching and the use of the \"Bernoulli\" trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) the TransE results used for the initialization of DurR, TransD, and TranSparse may be better. They directly copy the TransE results previously reported in al al, al., al. (2014), and it is therefore much more difficult to determine the exact relationship over D."}, {"heading": "6 Conclusion and future work", "text": "We introduced a neighbourhood mixing model to complete the knowledge base by constructing neighbourhood-based vector representations for entities and demonstrated its effect by extending TransE (Bordes et al., 2013) with our neighbourhood mixing model. Experimental results show on three different datasets that our TransE model significantly improves and achieves better results than the other state-of-the-art embedding models for triple classification, entity prediction and prediction tasks. In the future, we plan to apply the neighbourhood mixing model to other embedding models, in particular to relation path models such as TransE-COMP, in order to combine the useful information from both relationship pathways and entity neighbourhoods."}, {"heading": "Acknowledgments", "text": "This research was supported by a Google Prize under the Natural Language Understanding Focused Program and the Discovery Projects program of the Australian Research Council (project number DP160102156), which was also supported by NICTA, which is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The lead author was supported by an International Postgraduate Research Scholarship and a NICTA NRPA Top-Up Scholarship."}], "references": [{"title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD Interna-", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A Semantic Matching Energy Function for Learning with Multirelational Data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Toward an Architecture for Never-ending Language Learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka", "Jr.", "Tom M. Mitchell"], "venue": "In Proceedings of the Twenty-Fourth AAAI Conference", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Composing Relationships with Translations", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases", "author": ["Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2016}, {"title": "Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction", "author": ["Gardner", "Mitchell2015] Matt Gardner", "Tom Mitchell"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Gardner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2015}, {"title": "Semantically Smooth Knowledge Graph Embedding", "author": ["Guo et al.2015] Shu Guo", "Quan Wang", "Bin Wang", "Lihong Wang", "Li Guo"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Traversing Knowledge Graphs in Vector Space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "author": ["He et al.2015] Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "A latent factor model for highly multirelational data", "author": ["Nicolas L. Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Ji et al.2015] Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Knowledge Graph Completion", "author": ["Ji et al.2016] Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Type-Constrained Representation Learning in Knowledge Graphs", "author": ["Krompa et al.2015] Denis Krompa", "Stephan Baier", "Volker Tresp"], "venue": "In Proceedings of the 14th International Semantic Web Conference,", "citeRegEx": "Krompa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krompa et al\\.", "year": 2015}, {"title": "DBpedia - A Large-scale", "author": ["Lehmann et al.2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Christian Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "Learning Plausible Inferences from Semantic Web Knowledge by Combining Analogical Generalization with Structured Logistic Regression", "author": ["Liang", "Forbus2015] Chen Liang", "Kenneth D. Forbus"], "venue": "In Proceedings of the Twenty-Ninth AAAI Con-", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Lin et al.2015a] Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Nat-", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Lin et al.2015b] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learn-", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "On the Limited Memory BFGS Method for Large Scale Optimization", "author": ["Liu", "Nocedal1989] D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Context-Dependent Knowledge Graph Embedding", "author": ["Luo et al.2015] Yuanfei Luo", "Quan Wang", "Bin Wang", "Li Guo"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Compositional Vector Space Models for Knowledge", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Kairit Sirts", "Lizhen Qu", "Mark Johnson"], "venue": "In Proceedings of the 15th Annual Conference of the North American Chapter", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["Lorenzo Rosasco", "Tomaso A. Poggio"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "YAGO: A Core of Semantic Knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Link Prediction in Relational Data", "author": ["Taskar et al.2004] Ben Taskar", "Ming fai Wong", "Pieter Abbeel", "Daphne Koller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text", "author": ["Xi Victoria Lin", "Wen tau Yih", "Hoifung Poon", "Chris Quirk"], "venue": "In Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Knowledge Base Completion via Search-based Question Answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd International Conference on World", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 31, "context": "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": ", 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": ", 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (head entity, relation, tail entity).", "startOffset": 20, "endOffset": 42}, {"referenceID": 30, "context": "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 34, "context": "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 27, "context": "Knowledge base completion or link prediction systems (Nickel et al., 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 32, "context": ", 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).", "startOffset": 76, "endOffset": 118}, {"referenceID": 1, "context": ", 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).", "startOffset": 76, "endOffset": 118}, {"referenceID": 2, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 3, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 30, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 10, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 25, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 15, "context": ", 2016) and generalize to large KBs (Krompa et al., 2015).", "startOffset": 36, "endOffset": 57}, {"referenceID": 10, "context": "Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc\u0131\u0301aDur\u00e1n et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion.", "startOffset": 115, "endOffset": 203}, {"referenceID": 33, "context": "Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc\u0131\u0301aDur\u00e1n et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion.", "startOffset": 115, "endOffset": 203}, {"referenceID": 3, "context": "We demonstrate its usefulness by applying it to the well-known TransE model (Bordes et al., 2013).", "startOffset": 76, "endOffset": 97}, {"referenceID": 2, "context": "However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 35, "context": "However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 25, "context": ", 2015) and STransE (Nguyen et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "1, and then describe the Neighborhood Mixture Model applied to the TransE model (Bordes et al., 2013) in section 2.", "startOffset": 80, "endOffset": 101}, {"referenceID": 0, "context": "Freebase (Bollacker et al., 2008), some entities, such as \u201cmale\u201d, can have thousands or millions neighboring entities sharing the same relation \u201cgender.", "startOffset": 9, "endOffset": 33}, {"referenceID": 3, "context": "TransE (Bordes et al., 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 7, "context": ", 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al., 2016; Nickel et al., 2016).", "startOffset": 134, "endOffset": 183}, {"referenceID": 28, "context": ", 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al., 2016; Nickel et al., 2016).", "startOffset": 134, "endOffset": 183}, {"referenceID": 11, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 13, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 14, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 5, "context": ", SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu and Nocedal, 1989).", "startOffset": 15, "endOffset": 35}, {"referenceID": 36, "context": ", 2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu and Nocedal, 1989).", "startOffset": 18, "endOffset": 32}, {"referenceID": 2, "context": "The Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": "The Structured Embedding (SE) model (Bordes et al., 2011) extends the Unstructured model by assuming that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "Futhermore, the SME model (Bordes et al., 2012) uses four different matrices to project entity and relation vectors into a subspace.", "startOffset": 26, "endOffset": 47}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) and TransR/CTransR (Lin et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 25, "context": "STransE (Nguyen et al., 2016) and TranSparse (Ji et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 14, "context": ", 2016) and TranSparse (Ji et al., 2016) are extensions of the TransR model, where head and tail entities are associated with their own projection matrices.", "startOffset": 23, "endOffset": 40}, {"referenceID": 35, "context": "The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 26, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 2, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 12, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 30, "context": "The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation.", "startOffset": 38, "endOffset": 59}, {"referenceID": 11, "context": "Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 73, "endOffset": 90}, {"referenceID": 7, "context": ", 2015) and TATEC (Garc\u0131\u0301a-Dur\u00e1n et al., 2016).", "startOffset": 18, "endOffset": 46}, {"referenceID": 18, "context": "Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al.", "startOffset": 10, "endOffset": 36}, {"referenceID": 18, "context": "Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al.", "startOffset": 10, "endOffset": 65}, {"referenceID": 16, "context": "(2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 15, "context": "(2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al.", "startOffset": 9, "endOffset": 37}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al. (2015) and Toutanova et al.", "startOffset": 9, "endOffset": 56}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction.", "startOffset": 9, "endOffset": 84}, {"referenceID": 22, "context": "in the path as pseudo-words applied Word2Vec algorithms (Mikolov et al., 2013) to produce pretrained vectors for these pseudo-words.", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015), PTransE (Lin et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 10, "context": ", 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model.", "startOffset": 25, "endOffset": 43}, {"referenceID": 10, "context": "These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multiplication.", "startOffset": 158, "endOffset": 176}, {"referenceID": 16, "context": "Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE, SME and SE models.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015), PTransE (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multiplication. Our neighborhood mixture model can be adapted to both relation path models Bilinear-COMP and TransE-COMP, by replacing head and tail entity vectors by the neighborbased vector representations, thus combining advantages of both path and neighborhood information. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data.", "startOffset": 9, "endOffset": 689}, {"referenceID": 23, "context": "WN11 is derived from the large lexical KB WordNet (Miller, 1995) involving 11 relation types.", "startOffset": 50, "endOffset": 64}, {"referenceID": 0, "context": "FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types.", "startOffset": 59, "endOffset": 83}, {"referenceID": 4, "context": "(2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al., 2010).", "startOffset": 166, "endOffset": 188}, {"referenceID": 26, "context": "The two benchmark datasets1, WN11 and FB13, were produced by Socher et al. (2013) for triple classification.", "startOffset": 61, "endOffset": 82}, {"referenceID": 0, "context": "FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types. The NELL186 dataset2 was introduced by Guo et al. (2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al.", "startOffset": 60, "endOffset": 169}, {"referenceID": 30, "context": "Triple classification: The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models.", "startOffset": 78, "endOffset": 99}, {"referenceID": 30, "context": "Following Socher et al. (2013), the relation-specific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set.", "startOffset": 10, "endOffset": 31}, {"referenceID": 3, "context": "Entity prediction: The entity prediction task (Bordes et al., 2013) predicts the head or the tail entity given the relation type and the other entity, i.", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "For the \u201cFiltered\u201d setting protocol described in Bordes et al. (2013), we also filter out before ranking any corrupted triples that appear in the KB.", "startOffset": 49, "endOffset": 70}, {"referenceID": 3, "context": ", the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (Bordes et al., 2013), we also report the mean reciprocal rank (MRR), which is commonly used in information retrieval.", "startOffset": 157, "endOffset": 178}, {"referenceID": 13, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 11, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 14, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 9, "context": "On NELL186, we set \u03b3 = 1 and k = 50 (Guo et al., 2015; Luo et al., 2015).", "startOffset": 36, "endOffset": 72}, {"referenceID": 21, "context": "On NELL186, we set \u03b3 = 1 and k = 50 (Guo et al., 2015; Luo et al., 2015).", "startOffset": 36, "endOffset": 72}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) 86.", "startOffset": 7, "endOffset": 24}, {"referenceID": 14, "context": "TranSparse-S (Ji et al., 2016) 86.", "startOffset": 13, "endOffset": 30}, {"referenceID": 14, "context": "TranSparse-US (Ji et al., 2016) 86.", "startOffset": 14, "endOffset": 31}, {"referenceID": 30, "context": "NTN (Socher et al., 2013) 70.", "startOffset": 4, "endOffset": 25}, {"referenceID": 11, "context": "KG2E (He et al., 2015) 85.", "startOffset": 5, "endOffset": 22}, {"referenceID": 10, "context": "Bilinear-COMP (Guu et al., 2015) 77.", "startOffset": 14, "endOffset": 32}, {"referenceID": 10, "context": "TransE-COMP (Guu et al., 2015) 80.", "startOffset": 12, "endOffset": 30}, {"referenceID": 30, "context": "Note that there are higher results reported for NTN (Socher et al., 2013), Bilinear-COMP (Guu et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 22, "context": ", 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 29, "context": ", 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al.", "startOffset": 24, "endOffset": 418}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful.", "startOffset": 24, "endOffset": 440}, {"referenceID": 9, "context": "The first three rows present the best results reported in Guo et al. (2015), while the next three rows present the best results reported in Luo et al.", "startOffset": 58, "endOffset": 76}, {"referenceID": 9, "context": "The first three rows present the best results reported in Guo et al. (2015), while the next three rows present the best results reported in Luo et al. (2015). TransE-NMM obtains the highest triple classification accuracy, the best raw mean rank and the second highest raw Hits@10 on the entity prediction task in this comparison.", "startOffset": 58, "endOffset": 158}, {"referenceID": 10, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.", "startOffset": 62, "endOffset": 80}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al.", "startOffset": 63, "endOffset": 406}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al.", "startOffset": 63, "endOffset": 424}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) did not report the TransE results used for initializing TransR, TransD and TranSparse, respectively.", "startOffset": 63, "endOffset": 445}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) did not report the TransE results used for initializing TransR, TransD and TranSparse, respectively. They directly copied the TransE results previously reported in Wang et al. (2014). So it is difficult to determine exactly how much TransR, TransD and TranSparse gain over TransE.", "startOffset": 63, "endOffset": 628}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al.", "startOffset": 13, "endOffset": 41}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 13, "endOffset": 61}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2016) and Nickel et al.", "startOffset": 13, "endOffset": 90}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2016) and Nickel et al. (2016) also showed that for entity prediction TransE obtains very competitive results which are much higher than the TransE results originally published in Bordes et al.", "startOffset": 13, "endOffset": 115}, {"referenceID": 1, "context": "(2016) also showed that for entity prediction TransE obtains very competitive results which are much higher than the TransE results originally published in Bordes et al. (2013).3", "startOffset": 156, "endOffset": 177}, {"referenceID": 3, "context": "We demonstrated its effect by extending TransE (Bordes et al., 2013) with our neighborhood mixture model.", "startOffset": 47, "endOffset": 68}], "year": 2017, "abstractText": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE\u2014a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.", "creator": "LaTeX with hyperref package"}}}