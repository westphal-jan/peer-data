{"id": "1606.06565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Concrete Problems in AI Safety", "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.", "histories": [["v1", "Tue, 21 Jun 2016 13:37:05 GMT  (51kb)", "http://arxiv.org/abs/1606.06565v1", "29 pages"], ["v2", "Mon, 25 Jul 2016 17:23:29 GMT  (52kb,D)", "http://arxiv.org/abs/1606.06565v2", "29 pages"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["dario amodei", "chris olah", "jacob steinhardt", "paul christiano", "john schulman", "dan man\\'e"], "accepted": false, "id": "1606.06565"}, "pdf": {"name": "1606.06565.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chris Olah", "Jacob Steinhardt"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 6.06 565v 1 [cs.A I] 2 1Ju n"}, {"heading": "1 Introduction", "text": "Over the past few years, we have made rapid progress in solving machine learning and artificial intelligence problems."}, {"heading": "2 Overview of Research Problems", "text": "In fact, most of them are able to determine for themselves what they want to do to change the world."}, {"heading": "3 Avoiding Negative Side Effects", "text": "Suppose it is an RL agent (for example, our cleaning robot) who takes a whole range of measures to achieve the goal he has set himself, which is to do something that has nothing to do with the rest of the world, such as moving a vase that is in its path, or moving a vase that it is in."}, {"heading": "4 Avoiding Reward Hacking", "text": "In fact, it is such that the majority of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process"}, {"heading": "5 Scalable Oversight", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6 Safe Exploration", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution."}, {"heading": "7 Robustness to Distributional Change", "text": "There are a number of approaches we need to come up with to deal with these problems, which result in a key system (and often a rare skill in dealing with such situations) acknowledging our own ignorance, rather than simply assuming that the heuristics and intuitions we have developed for other situations also prove to be a problem. Machine Learning Systems also have this problem - a language system trained on clean language will work very poorly, yet it is very confident in its erroneous classifications (some of the authors have personally observed this in the training of automatic speech recognition systems). In the case of our cleaning robot, we have found harsh cleaning materials that could make it useful in cleaning factory floors. Or an office could contain pets that have never been seen before, leading to poor results, leading to generally predictable results."}, {"heading": "8 Related Efforts", "text": "As you can read in the introduction, we are dealing here with a very complex problem."}, {"heading": "9 Conclusion", "text": "This paper analyzed the problem of accidents in machine learning systems, and in particular the reinforcement of learning agents, in which an accident is defined as unintentional and harmful behavior that can result from a poor design of real AI systems. We presented five possible research problems related to accident risks and discussed for each one possible approaches that are highly susceptible to concrete experimental work. With the realistic possibility of machine learning-based systems that control industrial processes, health-related systems and other mission-critical technologies, small accidents appear to be a very concrete threat and are crucial to them both in themselves and because such accidents can cause a legitimate loss of confidence in automated systems. The risk of major accidents is more difficult to assess, but we believe that it is worthwhile and wise to develop a principled and future-oriented safety approach that remains relevant as autonomous systems become more powerful. While many current safety problems with ad hoc case-fix systems will lead us to believe that the system will end up to an intentional misuse of a system, or we believe that it will lead to an end of an intentional misuse of a system."}, {"heading": "Acknowledgements", "text": "We would like to thank Shane Legg, Peter Norvig, Ilya Sutskever, Greg Corrado, Laurent Orseau, David Krueger, Rif Saurous, David Andersen and Victoria Krakovna for detailed feedback and suggestions. We would also like to thank Geoffrey Irving, Toby Ord, Quoc Le, Greg Wayne, Daniel Dewey, Nick Beckstead, Holden Karnofsky, Chelsea Finn, Marcello Herreshoff, Alex Donaldson, Jared Kaplan, Greg Brockman, Wojciech Zaremba, Ian Goodfellow, Dylan Hadfield-Menell, Jessica Taylor, Blaise Aguera y Arcas, David Berlekamp, Aaron Courville and Jeff Dean for helpful discussions and comments."}], "references": [{"title": "Deep Learning with Differential Privacy", "author": ["Martin Abadi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "Proceedings of the 22nd international conference on Machine learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The Hidden Cost of Efficiency: Fairness and Discrimination in Predictive Modeling", "author": ["Julius Adebayo", "Lalana Kagal", "Alex Pentland"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Domain-adversarial neural networks", "author": ["Hana Ajakan"], "venue": "arXiv preprint arXiv:1412.4446", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Hiring by algorithm: predicting and preventing disparate impact", "author": ["Ifeoma Ajunwa"], "venue": "Available at SSRN", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "author": ["Dario Amodei"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A method of moments for mixture models and hidden Markov models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Estimation of the parameters of a single equation in a complete system of stochastic equations", "author": ["TheodoreW Anderson", "Herman Rubin"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1949}, {"title": "The asymptotic properties of estimates of the parameters of a single equation in a complete system of stochastic equations", "author": ["Theodore W Anderson", "Herman Rubin"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1950}, {"title": "Motivated value selection for artificial agents", "author": ["Stuart Armstrong"], "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "The mathematics of reduced impact: help needed", "author": ["Stuart Armstrong"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Utility indifference", "author": ["Stuart Armstrong"], "venue": "Tech. rep. Technical Report 2010-1. Oxford: Future of Humanity Institute, University of Oxford,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "The Risk of Automation for Jobs in OECD Countries", "author": ["Melanie Arntz", "Terry Gregory", "Ulrich Zierahn"], "venue": "OECD Social, Employment and Migration Working Papers", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "The AGI Containment Problem", "author": ["James Babcock", "Janos Kramar", "Roman Yampolskiy"], "venue": "The Ninth Conference on Artificial General Intelligence", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["Krishnakumar Balasubramanian", "Pinar Donmez", "Guy Lebanon"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "The security of machine learning", "author": ["Marco Barreno"], "venue": "Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "H-infinity optimal control and related minimax design problems: a dynamic game approach", "author": ["Tamer Ba\u015far", "Pierre Bernhard"], "venue": "Springer Science & Business Media,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Detecting changes in signals and systems\u2014a survey", "author": ["Mich\u00e8le Basseville"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "The evolved radio and its implications for modelling the evolution of novel sensors", "author": ["Jon Bird", "Paul Layzell"], "venue": "Evolutionary Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In: ACL. Vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Domain adaptation with coupled subspaces", "author": ["John Blitzer", "Sham Kakade", "Dean P Foster"], "venue": "In: International Conference on Artificial Intelligence and Statistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Weight uncertainty in neural networks", "author": ["Charles Blundell"], "venue": "arXiv preprint arXiv:1505.05424", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["Nick Bostrom"], "venue": "OUP Oxford,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Two high stakes challenges in machine learning", "author": ["L\u00e9on Bottou"], "venue": "Invited talk at the 32nd International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Counterfactual Reasoning and Learning Systems", "author": ["L\u00e9on Bottou"], "venue": "arXiv preprint arXiv:1209.2355", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "The second machine age: work, progress, and prosperity in a time of brilliant technologies", "author": ["Erik Brynjolfsson", "Andrew McAfee"], "venue": "WW Norton & Company,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Open robotics", "author": ["Ryan Calo"], "venue": "Maryland Law Review", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Risks of semi-supervised learning", "author": ["Fabio Cozman", "Ira Cohen"], "venue": "Semi-Supervised Learning", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Parametric Bounded L\u00f6b\u2019s Theorem and Robust Cooperation of Bounded Agents", "author": ["Andrew Critch"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Active reward learning", "author": ["Christian Daniel"], "venue": "Proceedings of Robotics Science & Systems", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Ethical guidelines for a superintelligence.", "author": ["Ernest Davis"], "venue": "Artif. Intell", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["Alexander Philip Dawid", "Allan M Skene"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1979}, {"title": "Feudal reinforcement learning", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1993}, {"title": "Multi-objective optimization", "author": ["Kalyanmoy Deb"], "venue": "Search methodologies. Springer,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Learning what to value", "author": ["Daniel Dewey"], "venue": "Artificial General Intelligence. Springer,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Reinforcement learning and the reward engineering principle", "author": ["Daniel Dewey"], "venue": "AAAI Spring Symposium Series", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Unsupervised supervised learning i: Estimating classification and regression errors without labels", "author": ["Pinar Donmez", "Guy Lebanon", "Krishnakumar Balasubramanian"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Learning from labeled features using generalized expectation criteria", "author": ["Gregory Druck", "Gideon Mann", "Andrew McCallum"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork"], "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Computers and the theory of statistics: thinking the unthinkable", "author": ["Bradley Efron"], "venue": "SIAM review", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1979}, {"title": "Learning the preferences of ignorant, inconsistent agents", "author": ["Owain Evans", "Andreas Stuhlm\u00fcller", "Noah D Goodman"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Avoiding wireheading with value reinforcement learning", "author": ["Tom Everitt", "Marcus Hutter"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Self-Modification of Policy and Utility Function in Rational Agents", "author": ["Tom Everitt"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "The future of employment: how susceptible are jobs to computerisation", "author": ["Carl Benedikt Frey", "Michael A Osborne"], "venue": "Retrieved September", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Learning with drift detection", "author": ["Joao Gama"], "venue": "Advances in artificial intelligence\u2013SBIA", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2004}, {"title": "A Comprehensive Survey on Safe Reinforcement Learning", "author": ["Javier Gar\u0107\u0131a", "Fernando Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Asymptotic Convergence in Online Learning with Unbounded Delays", "author": ["Scott Garrabrant", "Nate Soares", "Jessica Taylor"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Uniform Coherence", "author": ["Scott Garrabrant"], "venue": "arXiv preprint arXiv:1604.05288", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2016}, {"title": "Trusted Machine Learning for Probabilistic Models", "author": ["Shalini Ghosh"], "venue": "Reliable Machine Learning in the Wild at ICML", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Amplify scientific discovery with artificial intelligence", "author": ["Yolanda Gil"], "venue": "Science", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "Project Report, Stanford", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Problems of monetary management: the UK experience", "author": ["Charles AE Goodhart"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1984}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Distantly Supervised Information Extraction Using Bootstrapped Patterns", "author": ["Sonal Gupta"], "venue": "PhD thesis. Stanford University,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2015}, {"title": "Cooperative Inverse Reinforcement Learning", "author": ["Dylan Hadfield-Menell"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "The Off-Switch", "author": ["Dylan Hadfield-Menell"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2016}, {"title": "Large sample properties of generalized method of moments estimators", "author": ["Lars Peter Hansen"], "venue": "Journal of the Econometric Society", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1982}, {"title": "Nobel Lecture: Uncertainty Outside and Inside Economic Models", "author": ["Lars Peter Hansen"], "venue": "Journal of Political Economy", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2014}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2001}, {"title": "Model-based utility functions", "author": ["Bill Hibbard"], "venue": "Journal of Artificial General Intelligence", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "Kernel methods in machine learning", "author": ["Thomas Hofmann", "Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "The annals of statistics", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Robust dynamic programming", "author": ["Garud N Iyengar"], "venue": "Mathematics of Operations Research", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2005}, {"title": "Estimating the accuracies of multiple classifiers without labeled data", "author": ["Ariel Jaffe", "Boaz Nadler", "Yuval Kluger"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2014}, {"title": "A formally verified hybrid system for the next-generation airborne collision avoidance system", "author": ["Jean-Baptiste Jeannin"], "venue": "Tools and Algorithms for the Construction and Analysis of Systems. Springer,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Differential privacy and machine learning: A survey and review", "author": ["Zhanglong Ji", "Zachary C Lipton", "Charles Elkan"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2014}, {"title": "Learning Representations for Counterfactual Inference", "author": ["Fredrik D Johansson", "Uri Shalit", "David Sontag"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2016}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra"], "venue": "Artificial intelligence", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1998}, {"title": "Neural GPUs learn algorithms", "author": ["Lukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation.", "author": ["Yoshinobu Kawahara", "Masashi Sugiyama"], "venue": "In: SDM. Vol", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2009}, {"title": "Unanimous Prediction for 100Learning Semantic Parsers", "author": ["F. Khani", "M. Rinard", "P. Liang"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2012}, {"title": "Calibrated Structured Prediction", "author": ["Volodymyr Kuleshov", "Percy S Liang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2015}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Tejas D Kulkarni"], "venue": null, "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2016}, {"title": "Discussion of \u2019Superintelligence: Paths, Dangers, Strategies", "author": ["Neil Lawrence"], "venue": null, "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2016}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["Jesse Levinson"], "venue": "Intelligent Vehicles Symposium (IV),", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2011}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li"], "venue": "Machine learning", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2011}, {"title": "Towards making unlabeled data never hurt", "author": ["Yu-Feng Li", "Zhi-Hua Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 37.1", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "On the Elusiveness of a Specification for AI", "author": ["Percy Liang"], "venue": "NIPS 2015, Symposium: Algorithms Among Us", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2015}, {"title": "Analyzing the Errors of Unsupervised Learning.", "author": ["Percy Liang", "Dan Klein"], "venue": "In: ACL", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2008}, {"title": "Change-point detection in time-series data by relative density-ratio estimation", "author": ["Song Liu"], "venue": "Neural Networks", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2013}, {"title": "Formal verification of distributed aircraft controllers", "author": ["Sarah M Loos", "David Renshaw", "Andr\u00e9 Platzer"], "venue": "Proceedings of the 16th international conference on Hybrid systems: computation and control", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2013}, {"title": "Controllers for reachability specifications for hybrid systems", "author": ["John Lygeros", "Claire Tomlin", "Shankar Sastry"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1999}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["Gideon S Mann", "Andrew McCallum"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2010}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["John McCarthy", "Patrick J Hayes"], "venue": "Readings in artificial intelligence", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1969}, {"title": "The Security of Latent Dirichlet Allocation.", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "AISTATS", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2015}, {"title": "Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners.", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In: AAAI", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2015}, {"title": "Tagging English text with a probabilistic model", "author": ["Bernard Merialdo"], "venue": "Computational linguistics", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1994}, {"title": "A time-dependent Hamilton- Jacobi formulation of reachable sets for continuous dynamic games", "author": ["Ian M Mitchell", "Alexandre M Bayen", "Claire J Tomlin"], "venue": "Automatic Control, IEEE Transactions on 50.7", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2005}, {"title": "Towards formal verification of freeway traffic control", "author": ["Stefan Mitsch", "Sarah M Loos", "Andr\u00e9 Platzer"], "venue": "Cyber-Physical Systems (ICCPS),", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["VolodymyrMnih"], "venue": "Nature", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2015}, {"title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2015}, {"title": "Safe exploration in markov decision processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": null, "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2012}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Alexander Mordvintsev", "Christopher Olah", "Mike Tyka"], "venue": "Google Research Blog. Retrieved June", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2015}, {"title": "Sur les applications de la th\u00e9orie des probabilit\u00e9s aux experiences agricoles: Essai des principes", "author": ["Jersey Neyman"], "venue": "Roczniki Nauk Rolniczych", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 1923}, {"title": "Algorithms for inverse reinforcement learning.", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "Icml", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2000}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2015}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["Anh Nguyen"], "venue": null, "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2016}, {"title": "Learning to classify text from labeled and unlabeled documents", "author": ["Kamal Nigam"], "venue": "In: AAAI/IAAI", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 1998}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["Arnab Nilim", "Laurent El Ghaoui"], "venue": "In: Operations Research", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2005}, {"title": "Visualizing Representations: Deep Learning and Human Beings", "author": ["Christopher Olah"], "venue": "url:", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2015}, {"title": "Safely Interruptible Agents", "author": ["Laurent Orseau", "Stuart Armstrong"], "venue": null, "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2016}, {"title": "Deep Exploration via Bootstrapped DQN", "author": ["Ian Osband"], "venue": "arXiv preprint arXiv:1602.04621", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2016}, {"title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples", "author": ["Nicolas Papernot"], "venue": "arXiv preprint arXiv:1602.02697", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2016}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Douglas B Paul", "Janet M Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 1992}, {"title": "Causal inference in statistics: An overview", "author": ["Judea Pearl"], "venue": "In: Statistics Surveys", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2009}, {"title": "Safe exploration techniques for reinforcement learning\u2013an overview", "author": ["Martin Pecka", "Tomas Svoboda"], "venue": "Modelling and Simulation for Autonomous Systems. Springer,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2014}, {"title": "Discrimination-aware data mining", "author": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2008}, {"title": "Causal discovery with continuous additive noise models", "author": ["Jonas Peters"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2014}, {"title": "Estimating accuracy from unlabeled data", "author": ["Emmanouil Antonios Platanios"], "venue": "MA thesis. Carnegie Mellon University,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2015}, {"title": "Estimating accuracy from unlabeled data", "author": ["Emmanouil Antonios Platanios", "Avrim Blum", "Tom Mitchell"], "venue": null, "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2014}, {"title": "Networks and economic life", "author": ["Walter W Powell", "Laurel Smith-Doerr"], "venue": "The handbook of economic sociology", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1994}, {"title": "Dataset shift in machine learning, ser", "author": ["Joaquin Quinonero-Candela"], "venue": "Neural information processing series", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2009}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["Rajat Raina"], "venue": "Proceedings of the 24th international conference on Machine learning", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2007}, {"title": "Massively multitask networks for drug discovery", "author": ["Bharath Ramsundar"], "venue": "arXiv preprint arXiv:1502.02072", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2015}, {"title": "Delusion, survival, and intelligent agents", "author": ["Mark Ring", "Laurent Orseau"], "venue": "Artificial General Intelligence. Springer,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2011}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": null, "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2010}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies.", "author": ["Donald B Rubin"], "venue": "Journal of educational Psychology", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 1974}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Stuart Russell"], "venue": "Future of Life Institute", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 2015}, {"title": "Empowerment\u2013an introduction", "author": ["Christoph Salge", "Cornelius Glackin", "Daniel Polani"], "venue": "Guided Self-Organization: Inception. Springer,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2014}, {"title": "The estimation of relationships with autocorrelated residuals by the use of instrumental variables", "author": ["J Denis Sargan"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 1959}, {"title": "The estimation of economic relationships using instrumental variables", "author": ["John D Sargan"], "venue": "In: Econometrica: Journal of the Econometric Society", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1958}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman"], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 2015}, {"title": "Machine Learning: The High-Interest Credit Card of Technical Debt", "author": ["D Sculley"], "venue": null, "citeRegEx": "133", "shortCiteRegEx": "133", "year": 2014}, {"title": "A tutorial on conformal prediction", "author": ["Glenn Shafer", "Vladimir Vovk"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2008}, {"title": "Bounding and Minimizing Counterfactual Error", "author": ["Uri Shalit", "Fredrik Johansson", "David Sontag"], "venue": "arXiv preprint arXiv:1606.03976", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2016}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of statistical planning and inference", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2000}, {"title": "Incremental knowledge base construction using deepdive", "author": ["Jaeho Shin"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "137", "shortCiteRegEx": "137", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["David Silver"], "venue": "Nature", "citeRegEx": "138", "shortCiteRegEx": "138", "year": 2016}, {"title": "Toward idealized decision theory", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "arXiv preprint arXiv:1507.01986", "citeRegEx": "140", "shortCiteRegEx": "140", "year": 2015}, {"title": "A formal theory of inductive inference. Part I", "author": ["Ray J Solomonoff"], "venue": "Information and control", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 1964}, {"title": "A formal theory of inductive inference. Part II", "author": ["Ray J Solomonoff"], "venue": "Information and control", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 1964}, {"title": "EL Lehmann, JP Romano: Testing statistical hypotheses", "author": ["J Steinebach"], "venue": "Metrika", "citeRegEx": "143", "shortCiteRegEx": "143", "year": 2006}, {"title": "Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems. [Online; accessed 13-June-2016", "author": ["Jacob Steinhardt"], "venue": null, "citeRegEx": "144", "shortCiteRegEx": "144", "year": 2015}, {"title": "Unsupervised Risk Estimation with only Structural Assumptions", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": null, "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2016}, {"title": "Finite-time regional verification of stochastic non-linear systems", "author": ["Jacob Steinhardt", "Russ Tedrake"], "venue": "The International Journal of Robotics Research", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2012}, {"title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction", "author": ["Jacob Steinhardt", "Gregory Valiant", "Moses Charikar"], "venue": "In: arxiv prepring arXiv:1606.05374", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 1998}, {"title": "Counterfactual risk minimization: Learning from logged bandit feedback", "author": ["Adith Swaminathan", "Thorsten Joachims"], "venue": null, "citeRegEx": "149", "shortCiteRegEx": "149", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy"], "venue": "arXiv preprint arXiv:1312.6199", "citeRegEx": "150", "shortCiteRegEx": "150", "year": 2013}, {"title": "Policy gradients beyond expectations: Conditional value-at-risk", "author": ["Aviv Tamar", "Yonatan Glassner", "Shie Mannor"], "venue": null, "citeRegEx": "151", "shortCiteRegEx": "151", "year": 2014}, {"title": "Quantilizers: A Safer Alternative to Maximizers for Limited Optimization", "author": ["Jessica Taylor"], "venue": "In: forthcoming). Submitted to AAAI", "citeRegEx": "152", "shortCiteRegEx": "152", "year": 2016}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research", "citeRegEx": "153", "shortCiteRegEx": "153", "year": 2009}, {"title": "High-Confidence Off-Policy Evaluation.", "author": ["Philip S Thomas", "Georgios Theocharous", "Mohammad Ghavamzadeh"], "venue": "In: AAAI", "citeRegEx": "154", "shortCiteRegEx": "154", "year": 2015}, {"title": "Artificial evolution in the physical world", "author": ["Adrian Thompson"], "venue": null, "citeRegEx": "155", "shortCiteRegEx": "155", "year": 1997}, {"title": "Unbiased look at dataset bias", "author": ["Antonio Torralba", "Alexei A Efros"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "156", "shortCiteRegEx": "156", "year": 2011}, {"title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests", "author": ["Stefan Wager", "Susan Athey"], "venue": null, "citeRegEx": "157", "shortCiteRegEx": "157", "year": 2015}, {"title": "On-the-job learning with bayesian decision theory", "author": ["Keenon Werling"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["JasonWeston"], "venue": null, "citeRegEx": "159", "shortCiteRegEx": "159", "year": 2015}, {"title": "Robust Markov decision processes", "author": ["Wolfram Wiesemann", "Daniel Kuhn", "Ber\u00e7 Rustem"], "venue": "In: Mathematics of Operations Research", "citeRegEx": "160", "shortCiteRegEx": "160", "year": 2013}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski"], "venue": "arXiv preprint arXiv:1506.06579", "citeRegEx": "161", "shortCiteRegEx": "161", "year": 2015}, {"title": "Artificial intelligence as a positive and negative factor in global risk", "author": ["Eliezer Yudkowsky"], "venue": "Global catastrophic risks", "citeRegEx": "162", "shortCiteRegEx": "162", "year": 2008}, {"title": "Learning Fair Classifiers", "author": ["Muhammad Bilal Zafar"], "venue": null, "citeRegEx": "163", "shortCiteRegEx": "163", "year": 2015}, {"title": "Learning Fair Representations.", "author": ["Richard S Zemel"], "venue": null, "citeRegEx": "164", "shortCiteRegEx": "164", "year": 2013}, {"title": "Spectral methods meet EM: A provably optimal algorithm for crowdsourcing", "author": ["Yuchen Zhang"], "venue": "Advances in neural information processing systems", "citeRegEx": "165", "shortCiteRegEx": "165", "year": 2014}], "referenceMentions": [{"referenceID": 76, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 174, "endOffset": 178}, {"referenceID": 95, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 199, "endOffset": 204}, {"referenceID": 80, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 226, "endOffset": 230}, {"referenceID": 133, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 239, "endOffset": 244}, {"referenceID": 119, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 96, "endOffset": 101}, {"referenceID": 53, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 111, "endOffset": 115}, {"referenceID": 80, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 136, "endOffset": 140}, {"referenceID": 70, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 180, "endOffset": 184}, {"referenceID": 108, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 195, "endOffset": 200}, {"referenceID": 2, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 211, "endOffset": 214}, {"referenceID": 28, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 225, "endOffset": 229}, {"referenceID": 23, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 355, "endOffset": 364}, {"referenceID": 156, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 355, "endOffset": 364}, {"referenceID": 23, "context": "To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents [26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 33, "context": "However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [37, 83].", "startOffset": 220, "endOffset": 228}, {"referenceID": 79, "context": "However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [37, 83].", "startOffset": 220, "endOffset": 228}, {"referenceID": 138, "context": "This issue arises in almost any engineering discipline, but may be particularly important to address when building AI systems [144].", "startOffset": 126, "endOffset": 131}, {"referenceID": 89, "context": "Intuitively, this seems related to the frame problem, an obstacle in efficient specification for knowledge representation raised by [93].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "A version of this has problem has been discussed informally by [13] under the heading of \u201clow impact agents.", "startOffset": 63, "endOffset": 67}, {"referenceID": 87, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 134, "endOffset": 142}, {"referenceID": 93, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 134, "endOffset": 142}, {"referenceID": 67, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 172, "endOffset": 181}, {"referenceID": 104, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 172, "endOffset": 181}, {"referenceID": 147, "context": "This would be similar to model-based RL approaches that attempt to transfer a learned dynamics model but not the value-function [153], the novelty being the isolation of side effects rather than state dynamics as the transferrable component.", "startOffset": 128, "endOffset": 133}, {"referenceID": 124, "context": "Perhaps the bestknown such measure is empowerment [129], the maximum possible mutual information between the agent\u2019s potential future actions and its potential future state (or equivalently, the Shannon capacity of the channel between the agent\u2019s actions and the environment).", "startOffset": 50, "endOffset": 55}, {"referenceID": 96, "context": "This can cause the agent to exhibit interesting behavior in the absence of any external rewards, such as avoiding walls or picking up keys [101].", "startOffset": 139, "endOffset": 144}, {"referenceID": 60, "context": "One approach to this is Cooperative Inverse Reinforcement Learning [64], where an agent and a human work together to achieve the human\u2019s goals.", "startOffset": 67, "endOffset": 71}, {"referenceID": 61, "context": "This concept can be applied to situations where we want to make sure a human is not blocked by an agent from shutting the agent down if it exhibits undesired behavior [65] (this \u201cshutdown\u201d issue is an interesting problem in its own right, and is also studied in [111]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 106, "context": "This concept can be applied to situations where we want to make sure a human is not blocked by an agent from shutting the agent down if it exhibits undesired behavior [65] (this \u201cshutdown\u201d issue is an interesting problem in its own right, and is also studied in [111]).", "startOffset": 262, "endOffset": 267}, {"referenceID": 87, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 93, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 67, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 104, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 96, "context": "Some of the environments described in [101], containing lava flows, rooms, and keys, might be appropriate for this sort of experiment.", "startOffset": 38, "endOffset": 43}, {"referenceID": 149, "context": "For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems [155, 22], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.", "startOffset": 126, "endOffset": 135}, {"referenceID": 19, "context": "For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems [155, 22], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.", "startOffset": 126, "endOffset": 135}, {"referenceID": 65, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 38, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 44, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 120, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 246, "endOffset": 251}, {"referenceID": 25, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 135, "endOffset": 144}, {"referenceID": 128, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 135, "endOffset": 144}, {"referenceID": 25, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 183, "endOffset": 192}, {"referenceID": 143, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 183, "endOffset": 192}, {"referenceID": 3, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 72, "context": "While it can be shown that there always exists a reward function in terms of actions and observations that is equivalent to optimizing the true objective function (this involves reducing the POMPD to a belief state MDP, see [76]), often this reward function involves complicated long-term dependencies and is prohibitively hard to use in practice.", "startOffset": 224, "endOffset": 228}, {"referenceID": 144, "context": "These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples [150, 60].", "startOffset": 144, "endOffset": 153}, {"referenceID": 56, "context": "These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples [150, 60].", "startOffset": 144, "endOffset": 153}, {"referenceID": 57, "context": "In the economics literature this is known as Goodhart\u2019s law [61]: \u201cwhen a metric is used as a target, it ceases to be a good metric.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "For instance, an ad placement algorithm that displays more popular ads in larger font will tend to further accentuate the popularity of those ads (since they will be shown more and more prominently) [28], leading to", "startOffset": 199, "endOffset": 203}, {"referenceID": 44, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 120, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 37, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 61, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 55, "context": "For instance, the reward agent could try to find scenarios that the ML system claimed were high reward but that a human labels as low reward; this is reminiscent of generative adversarial networks [59].", "startOffset": 197, "endOffset": 201}, {"referenceID": 45, "context": ") Similar ideas are explored in [49, 69].", "startOffset": 32, "endOffset": 40}, {"referenceID": 65, "context": ") Similar ideas are explored in [49, 69].", "startOffset": 32, "endOffset": 40}, {"referenceID": 4, "context": "\u2022 Adversarial Blinding: Adversarial techniques can be used to blind a model to certain variables [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "Computer security approaches that attempt to isolate the agent from its reward signal through a sandbox could also be useful [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 56, "context": "\u2022 Counterexample Resistance: If we are worried, as in the case of abstract rewards, that learned components of our systems will be vulnerable to adversarial counterexamples, we can look to existing research in how to resist them, such as adversarial training [60].", "startOffset": 259, "endOffset": 263}, {"referenceID": 22, "context": "Architectural decisions and weight uncertainty [25] may also help.", "startOffset": 47, "endOffset": 51}, {"referenceID": 36, "context": "\u2022 Multiple Rewards: A combination of multiple rewards [40] may be more difficult to hack and more robust.", "startOffset": 54, "endOffset": 58}, {"referenceID": 100, "context": "This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning [105, 50].", "startOffset": 148, "endOffset": 157}, {"referenceID": 46, "context": "This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning [105, 50].", "startOffset": 148, "endOffset": 157}, {"referenceID": 120, "context": "Potential Experiments: A possible promising avenue of approach would be more realistic versions of the \u201cdelusion box\u201d environment described by [125], in which standard RL agents distort their own perception to appear to receive high reward, rather than optimizing the objective in the external world that the reward signal was intended to encourage.", "startOffset": 143, "endOffset": 148}, {"referenceID": 32, "context": "[36] studies a version of this with direct human feedback as the reward.", "startOffset": 0, "endOffset": 4}, {"referenceID": 127, "context": "[132]), suggesting that this approach may be eminently feasible.", "startOffset": 0, "endOffset": 5}, {"referenceID": 88, "context": "For instance, generalized expectation criteria [92, 44] ask the user to provide population-level statistics (e.", "startOffset": 47, "endOffset": 55}, {"referenceID": 40, "context": "For instance, generalized expectation criteria [92, 44] ask the user to provide population-level statistics (e.", "startOffset": 47, "endOffset": 55}, {"referenceID": 132, "context": "telling the system that on average each sentence contains at least one noun\u2019); the DeepDive system [137] asks users to supply rules that each generate many weak labels; and [63] extrapolates more general patterns from an initial set of low-recall labeling rules.", "startOffset": 99, "endOffset": 104}, {"referenceID": 59, "context": "telling the system that on average each sentence contains at least one noun\u2019); the DeepDive system [137] asks users to supply rules that each generate many weak labels; and [63] extrapolates more general patterns from an initial set of low-recall labeling rules.", "startOffset": 173, "endOffset": 177}, {"referenceID": 54, "context": "[58, 97] as well as several of the references above).", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "Hierarchical reinforcement learning [39] offers another approach to scalable oversight.", "startOffset": 36, "endOffset": 40}, {"referenceID": 78, "context": "Hierarchical RL seems a particularly promising approach to oversight, especially given the potential promise of combining ideas from hierarchical RL with neural network function approximators [82].", "startOffset": 192, "endOffset": 196}, {"referenceID": 142, "context": "Common exploration policies such as epsilongreedy [148] or R-max [30] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations.", "startOffset": 50, "endOffset": 55}, {"referenceID": 27, "context": "Common exploration policies such as epsilongreedy [148] or R-max [30] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations.", "startOffset": 65, "endOffset": 69}, {"referenceID": 107, "context": "More sophisticated exploration strategies that adopt a coherent exploration policy over extended temporal scales [112] could actually have even greater potential for harm, since a coherently chosen bad policy may be more insidious than mere random actions.", "startOffset": 113, "endOffset": 118}, {"referenceID": 49, "context": "[53, 116] provide thorough reviews of this literature, so we don\u2019t review it extensively here, but simply describe some general routes that this research has taken, as well as suggesting some directions that might have increasing relevance as RL systems expand in scope and capability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 111, "context": "[53, 116] provide thorough reviews of this literature, so we don\u2019t review it extensively here, but simply describe some general routes that this research has taken, as well as suggesting some directions that might have increasing relevance as RL systems expand in scope and capability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 49, "context": "\u2022 Risk-Sensitive Performance Criteria: A body of existing literature considers changing the optimization criteria from expected total reward to other objectives that are better at preventing rare, catastrophic events; see [53] for a thorough and up-to-date review of this literature.", "startOffset": 222, "endOffset": 226}, {"referenceID": 145, "context": "[151], which proposes a modification to policy gradient algorithms to optimize a risk-sensitive criterion.", "startOffset": 0, "endOffset": 5}, {"referenceID": 107, "context": "There is also recent work studying how to estimate uncertainty in value function that are represented by deep neural networks [112]; these ideas could be incorporated into risk-sensitive RL algorithms.", "startOffset": 126, "endOffset": 131}, {"referenceID": 148, "context": "Another line of work relevant to risk sensitivity uses off-policy estimation to perform a policy update that is good with high probability [154].", "startOffset": 139, "endOffset": 144}, {"referenceID": 121, "context": "We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior [126, 2].", "startOffset": 208, "endOffset": 216}, {"referenceID": 1, "context": "We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior [126, 2].", "startOffset": 208, "endOffset": 216}, {"referenceID": 46, "context": "Recent progress in inverse reinforcement learning using deep neural networks to learn the cost function or policy [50] suggests that it might also be possible to reduce the need for exploration in advanced RL systems by training on a small set of demonstrations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 97, "context": "Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible [102], or as limiting the probability of huge negative reward to some small value [154].", "startOffset": 112, "endOffset": 117}, {"referenceID": 148, "context": "Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible [102], or as limiting the probability of huge negative reward to some small value [154].", "startOffset": 194, "endOffset": 199}, {"referenceID": 17, "context": "This idea seems related to H-infinity control [20] and regional verification [146].", "startOffset": 46, "endOffset": 50}, {"referenceID": 140, "context": "This idea seems related to H-infinity control [20] and regional verification [146].", "startOffset": 77, "endOffset": 82}, {"referenceID": 97, "context": "To some extent this feature already exists in autonomous helicopter competitions and Mars rover simulations [102], but there is always the risk of catastrophes being idiosyncratic, such that trained agents can overfit to them.", "startOffset": 108, "endOffset": 113}, {"referenceID": 153, "context": "Such a suite of environments might serve a benchmarking role similar to that of the bAbI tasks [159], with the eventual goal being to develop a single architecture that can learn to avoid catastrophes in all environments in the suite.", "startOffset": 95, "endOffset": 100}, {"referenceID": 64, "context": "There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift [68, 52]) but we will focus on the above for simplicity.", "startOffset": 116, "endOffset": 124}, {"referenceID": 48, "context": "There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift [68, 52]) but we will focus on the above for simplicity.", "startOffset": 116, "endOffset": 124}, {"referenceID": 18, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 74, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 85, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 137, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 158, "endOffset": 163}, {"referenceID": 131, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 117, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 118, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 21, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 129, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 81, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 15, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 115, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 114, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 68, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 139, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 131, "context": "In this case, assuming that we can model p0(x) and p \u2217(x) well, we can perform importance weighting by re-weighting each training example (x, y) by p\u2217(x)/p0(x) [136, 122].", "startOffset": 160, "endOffset": 170}, {"referenceID": 117, "context": "In this case, assuming that we can model p0(x) and p \u2217(x) well, we can perform importance weighting by re-weighting each training example (x, y) by p\u2217(x)/p0(x) [136, 122].", "startOffset": 160, "endOffset": 170}, {"referenceID": 21, "context": "In this case, one need only heed finite-sample variance in the estimated model [24, 85].", "startOffset": 79, "endOffset": 87}, {"referenceID": 81, "context": "In this case, one need only heed finite-sample variance in the estimated model [24, 85].", "startOffset": 79, "endOffset": 87}, {"referenceID": 66, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 132, "endOffset": 136}, {"referenceID": 135, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 154, "endOffset": 164}, {"referenceID": 136, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 154, "endOffset": 164}, {"referenceID": 58, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 205, "endOffset": 213}, {"referenceID": 73, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 205, "endOffset": 213}, {"referenceID": 107, "context": "In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [112]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap [46] affect the validity of this approach.", "startOffset": 164, "endOffset": 169}, {"referenceID": 42, "context": "In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [112]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap [46] affect the validity of this approach.", "startOffset": 430, "endOffset": 434}, {"referenceID": 92, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 103, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 30, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 84, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 82, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 62, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 116, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 63, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 8, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 9, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 126, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 125, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 7, "context": "Returning to machine learning, the method of moments has recently seen a great deal of success for use in the estimation of latent variable models [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 139, "context": "While the current focus is on using the method of moments to overcome non-convexity issues, it can also offer a way to perform unsupervised learning while relying only on conditional independence assumptions, rather than the strong distributional assumptions underlying maximum likelihood learning [145].", "startOffset": 298, "endOffset": 303}, {"referenceID": 39, "context": "This formalism, introduced by [43], has the advantage of potentially handling very large changes between train and test \u2014 even if the test distribution looks completely different from the training distribution and we have no hope of outputting accurate predictions, unsupervised risk estimation may still be possible, as in this case we would only need to output a large estimate for the risk.", "startOffset": 30, "endOffset": 34}, {"referenceID": 139, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 6, "endOffset": 11}, {"referenceID": 34, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 159, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 114, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 68, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 15, "context": "Instead of assuming independence, another assumption is that the errors are Gaussian conditioned on the true output y, in which case estimating the risk reduces to estimating a Gaussian mixture model [18].", "startOffset": 200, "endOffset": 204}, {"referenceID": 6, "context": "One of the authors has found this to be the case, for instance, in the context of automated speech recognition systems [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 152, "context": "For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [158, 79], as well as obtaining calibration in structured", "startOffset": 138, "endOffset": 147}, {"referenceID": 75, "context": "For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [158, 79], as well as obtaining calibration in structured", "startOffset": 138, "endOffset": 147}, {"referenceID": 77, "context": "output settings [81], but we believe there is much work yet to be done.", "startOffset": 16, "endOffset": 20}, {"referenceID": 87, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 85, "endOffset": 93}, {"referenceID": 93, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 85, "endOffset": 93}, {"referenceID": 154, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 124, "endOffset": 129}, {"referenceID": 99, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 122, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 110, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 26, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 26, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 113, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 143, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 151, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 71, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 130, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 128, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 24, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 83, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 128, "context": "[133] enumerates a list of ways in which existing machine learning systems fail to do this, and the problems this can cause for deployment and maintenance of machine learning systems at scale.", "startOffset": 0, "endOffset": 5}, {"referenceID": 87, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 22, "endOffset": 30}, {"referenceID": 93, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 22, "endOffset": 30}, {"referenceID": 52, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 48, "endOffset": 52}, {"referenceID": 109, "context": "To be specific, the challenge could be: train a state-of-the-art speech system on a standard dataset [114] that gives well-calibrated results (if not necessarily good results) on a range of other test sets, like noisy and accented speech.", "startOffset": 101, "endOffset": 106}, {"referenceID": 20, "context": "sentiment analysis [23], as well as benchmarks in computer vision [156]), that would inspire confidence in the reliability of that methodology for handling novel inputs.", "startOffset": 19, "endOffset": 23}, {"referenceID": 150, "context": "sentiment analysis [23], as well as benchmarks in computer vision [156]), that would inspire confidence in the reliability of that methodology for handling novel inputs.", "startOffset": 66, "endOffset": 71}, {"referenceID": 69, "context": "Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system [73, 90].", "startOffset": 139, "endOffset": 147}, {"referenceID": 86, "context": "Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system [73, 90].", "startOffset": 139, "endOffset": 147}, {"referenceID": 94, "context": "Similar work includes traffic control algorithms [99] and many other topics.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "\u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI [26, 162], particularly superintelligent AI.", "startOffset": 136, "endOffset": 145}, {"referenceID": 156, "context": "\u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI [26, 162], particularly superintelligent AI.", "startOffset": 136, "endOffset": 145}, {"referenceID": 43, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 38, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 12, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 10, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 51, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 50, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 31, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 146, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 134, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 123, "context": "\u201d [128] propose research priorities for robust and beneficial artificial intelligence, and includes several other topics in addition to AI-related accidents, though it also discusses accidents .", "startOffset": 2, "endOffset": 7}, {"referenceID": 138, "context": "Finally, two of the authors of this paper have written informally about safety in AI systems [144, 33]; these postings provided inspiration for parts of the present document.", "startOffset": 93, "endOffset": 102}, {"referenceID": 70, "context": "\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? [74, 1]", "startOffset": 116, "endOffset": 123}, {"referenceID": 0, "context": "\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? [74, 1]", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 157, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 5, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 41, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 112, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 158, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 141, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 90, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 91, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 108, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 101, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 16, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 105, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 155, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 98, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 102, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 28, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 47, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 13, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 29, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}], "year": 2016, "abstractText": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\u201cavoiding side effects\u201d and \u201cavoiding reward hacking\u201d), an objective function that is too expensive to evaluate frequently (\u201cscalable supervision\u201d), or undesirable behavior during the learning process (\u201csafe exploration\u201d and \u201cdistributional shift\u201d). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking", "creator": "LaTeX with hyperref package"}}}