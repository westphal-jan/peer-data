{"id": "1606.06623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "An empirical study on large scale text classification with skip-gram embeddings", "abstract": "We investigate the integration of word embeddings as classification features in the setting of large scale text classification. Such representations have been used in a plethora of tasks, however their application in classification scenarios with thousands of classes has not been extensively researched, partially due to hardware limitations. In this work, we examine efficient composition functions to obtain document-level from word-level embeddings and we subsequently investigate their combination with the traditional one-hot-encoding representations. By presenting empirical evidence on large, multi-class, multi-label classification problems, we demonstrate the efficiency and the performance benefits of this combination.", "histories": [["v1", "Tue, 21 Jun 2016 15:39:35 GMT  (59kb,D)", "http://arxiv.org/abs/1606.06623v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["georgios balikas", "massih-reza amini"], "accepted": false, "id": "1606.06623"}, "pdf": {"name": "1606.06623.pdf", "metadata": {"source": "CRF", "title": "An empirical study on large scale text classification with skip-gram embeddings", "authors": ["Georgios Balikas", "Massih-Reza Amini"], "emails": ["Georgios.Balikas@imag.fr", "Massih-Reza.Amini@imag.fr"], "sections": [{"heading": null, "text": "Keywords Distributed representations; One-hot-encoding representations; Neural networks; Text classification"}, {"heading": "1. INTRODUCTION AND PRELIMINARIES", "text": "In fact, it is in such a way that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process in which there is a process, to a process in which there is a process, to a process, to a process in which there is a process, to a process, to a process in which there is a process, to a process, to a process, to a process, to a process in which there is a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process, to a process"}, {"heading": "2. DISTRIBUTED REPRESENTATIONS AS CLASSIFICATION REPRESENTATIONS", "text": "We explore three functions to create document representations: min, average and max. [3, 25], which were used as simple but effective methods for learning compositions in vector-based semantics [19] to obtain the representation of a document. We use the output of each composition as documentation characteristics and we evaluate both its classification performance and the performance of its concatenation: zconc (doc) as a result of applying the composition x {avg, min} element-wise, in the distributed vectors of the documents. For example, we want the representation of a document and zx (doc) is the result of applying the composition x {avg, max} element-wise, in the distributed vectors of the documents. We assume that we need the representation of the \"hard winter,\" the composition we require the vector representation of the vector representations of the words we use the vector representations of the words \"and\" winter. \""}, {"heading": "3. COMBINATION OF DISTRIBUTED AND ONE-HOT-ENCODING SCHEMES", "text": "In recent years, the number of beneficiaries of social benefits has multiplied, and multiplied."}, {"heading": "4. CONCLUSION AND DISCUSSION", "text": "In this work, we have limited ourselves to representations that we have learned at the word level, which is advantageous in terms of speed, since the dictionary of representations can be generated offline. Then, of course, the application of composition functions is parallelizable and fast for prediction, but this poses the challenge of having robust composition functions that, if carefully selected, can lead to performance increases such as those mentioned above. Nor does it take into account the word order and grouping of words into coherent segments such as sentences or phrases. In this line, it would be interesting to further investigate how complex embeddings such as paragraph vectors [14] work. Even if such approaches are expensive at the document level, previous research has shown their effectiveness at the sentence level."}, {"heading": "5. REFERENCES", "text": "[1] G. Balikas and M.-R. Amini. Multi-label, multi-classsclassification using polylingual embedory 81. In Advances in Information Retrieval, pages 723-728. Springer, 2016. [2] W. Blacoe and M. Lapata. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 546-556. Association for Computational Linguistics, 2012. [3] R. Collobert, J. Weston, L. Bottou, M. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12: 2493-2537, 2011. [4] C. Cortes and V. Vapnik. Support-vector networks."}], "references": [{"title": "Multi-label", "author": ["G. Balikas", "M.-R. Amini"], "venue": "multi-class classification using polylingual embeddings. In Advances in Information Retrieval, pages 723\u2013728. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546\u2013556. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Extremely fast text feature extraction for classification and indexing", "author": ["G. Forman", "E. Kirshenbaum"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, pages  1221\u20131230. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1954}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Technical report, Universit\u00e4t Dortmund", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Evaluating memory efficiency and robustness of word embeddings", "author": ["J. Jurgovsky", "M. Granitzer", "C. Seifert"], "venue": "Advances in Information Retrieval, pages 200\u2013211. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Extractive summarization using continuous vector space models", "author": ["M. K\u030aageb\u00e4ck", "O. Mogren", "N. Tahmasebi", "D. Dubhashi"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Short text similarity with word embeddings", "author": ["T. Kenter", "M. de Rijke"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Convolutional sentence kernel from word embeddings for short text categorization", "author": ["J. Kim", "F. Rousseau", "M. Vazirgiannis"], "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing. In EMNLP, volume 15", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Dependency-based word embeddings", "author": ["O. Levy", "Y. Goldberg"], "venue": "ACL (2), pages 302\u2013308", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "CoNLL, pages 171\u2013180", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science,  34(8):1388\u20131429", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Semeval-2013 task 2: Sentiment analysis in twitter", "author": ["P. Nakov", "Z. Kozareva", "A. Ritter", "S. Rosenthal", "V. Stoyanov", "T. Wilson"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 115\u2013124. Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Lshtc: A benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "arXiv preprint arXiv:1503.08581", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised sparse vector densification for short text similarity", "author": ["Y. Song", "D. Roth"], "venue": "Proc. North Am. Chapter Assoc. Computat. Linguistics, pages 1275\u20131280", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["G. Tsatsaronis", "G. Balikas", "P. Malakasiotis", "I. Partalas", "M. Zschunke", "M.R. Alvers", "D. Weissenborn", "A. Krithara", "S. Petridis", "D. Polychronopoulos"], "venue": "An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):1", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki, Greece", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Self-adaptive hierarchical sentence model", "author": ["H. Zhao", "Z. Lu", "P. Poupart"], "venue": "arXiv preprint arXiv:1504.05070", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Traditionally, N -grams are considered as document features and are subsequently fed to a classifier such as Support Vector Machines (SVMs) [4, 7].", "startOffset": 140, "endOffset": 146}, {"referenceID": 6, "context": "Traditionally, N -grams are considered as document features and are subsequently fed to a classifier such as Support Vector Machines (SVMs) [4, 7].", "startOffset": 140, "endOffset": 146}, {"referenceID": 5, "context": "Lately, a lot of research has been devoted to the direction of distributed representations [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 16, "context": "We investigate the performance of word embeddings learned using the skipgram model [17] in the context of large scale, multi-label document classification.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 16, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 2, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 14, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 23, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 29, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 13, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 24, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 18, "context": "Hence, we focus on methods that given a dictionary of word representations apply composition functions [19, 2] to produce document representations.", "startOffset": 103, "endOffset": 110}, {"referenceID": 1, "context": "Hence, we focus on methods that given a dictionary of word representations apply composition functions [19, 2] to produce document representations.", "startOffset": 103, "endOffset": 110}, {"referenceID": 15, "context": "Although distributed embeddings have been applied in a plethora of tasks from analogies evaluation [16] to extractive summarization [9], their application on large scale text classification has not been investigated.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Although distributed embeddings have been applied in a plethora of tasks from analogies evaluation [16] to extractive summarization [9], their application on large scale text classification has not been investigated.", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 11, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 25, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 19, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 224, "endOffset": 228}, {"referenceID": 9, "context": "The works of [10] and [13] for instance, are in this line but, again, they limit their study at sentence-length spans.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "The works of [10] and [13] for instance, are in this line but, again, they limit their study at sentence-length spans.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 85, "endOffset": 92}, {"referenceID": 24, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 85, "endOffset": 92}, {"referenceID": 18, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 202, "endOffset": 206}, {"referenceID": 27, "context": "They are abstracts of biomedical texts from PubMed released by the BioASQ challenge organisers [28] as well as texts from Wikipedia [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "They are abstracts of biomedical texts from PubMed released by the BioASQ challenge organisers [28] as well as texts from Wikipedia [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "To obtain the dictionary of word embeddings we used the skipgram model of word2vec tool [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "We cope with the multi-label problem using a binary relevance approach [29].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "For our implementations, we have used Python\u2019s Scikit-Learn [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "We focus on two ways of representing text: (i) by using tf-idf representations of unigrams, and (ii) by employing a hash function [5, 27].", "startOffset": 130, "endOffset": 137}, {"referenceID": 26, "context": "We focus on two ways of representing text: (i) by using tf-idf representations of unigrams, and (ii) by employing a hash function [5, 27].", "startOffset": 130, "endOffset": 137}, {"referenceID": 13, "context": "In this line, it would be interesting to further investigate how more complex embeddings such as paragraph vectors [14] perform.", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "Recent research efforts [1, 8] have investigated ways of compressing the learned (or composed) representations using either linear (e.", "startOffset": 24, "endOffset": 30}, {"referenceID": 7, "context": "Recent research efforts [1, 8] have investigated ways of compressing the learned (or composed) representations using either linear (e.", "startOffset": 24, "endOffset": 30}], "year": 2016, "abstractText": "We investigate the integration of word embeddings as classification features in the setting of large scale text classification. Such representations have been used in a plethora of tasks, however their application in classification scenarios with thousands of classes has not been extensively researched, partially due to hardware limitations. In this work, we examine efficient composition functions to obtain document-level from word-level embeddings and we subsequently investigate their combination with the traditional one-hot-encoding representations. By presenting empirical evidence on large, multi-class, multi-label classification problems, we demonstrate the efficiency and the performance benefits of this combination.", "creator": "LaTeX with hyperref package"}}}