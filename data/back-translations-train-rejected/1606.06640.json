{"id": "1606.06640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Neural Morphological Tagging from Characters for Morphologically Rich Languages", "abstract": "This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. We systematically explore a variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors combined with bidirectional LSTMs to model across-word context in an end-to-end setting. We explore supplementary use of word-based vectors trained on large amounts of unlabeled data. Our experiments for morphological tagging suggest that for \"simple\" model configurations, the choice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy. Increasing the model capacity by adding depth, for example, and carefully optimizing the neural networks can lead to substantial improvements, and the differences in accuracy (but not training time) become much smaller or even negligible. Overall, our best morphological taggers for German and Czech outperform the best results reported in the literature by a large margin.", "histories": [["v1", "Tue, 21 Jun 2016 16:25:31 GMT  (112kb,D)", "http://arxiv.org/abs/1606.06640v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georg heigold", "guenter neumann", "josef van genabith"], "accepted": false, "id": "1606.06640"}, "pdf": {"name": "1606.06640.pdf", "metadata": {"source": "CRF", "title": "Neural Morphological Tagging from Characters for Morphologically Rich Languages", "authors": ["Georg Heigold"], "emails": ["<firstname>.<lastname>@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "In most cases, these are people who are able to survive themselves, and people who are able to survive themselves. In most cases, these are people who are able to survive themselves, and people who are able to survive themselves. In most cases, these are people who are able to survive themselves, and people who are able to survive themselves. In most cases, these are people who are able to survive themselves, and people who are able to survive themselves. In most cases, these are people who are able to survive themselves."}, {"heading": "2 Related Work", "text": "This work is (almost) from scratch in the sense of the \"natural language processing\" approach (Collobert et al., 2011b), which has been tested for word processing and various tasks of natural language processing in English. Several character-based approaches to marking have been proposed. Existing work on POS marking includes feature learning with CNNs in combination with a first-class Markov model for classification (Collobert et al., 2011b; dos Santos and Zadrozny, 2014) and recurring neural network-based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al.). The work of (Labeau et al., 2015) uses a CNN / Markov model for the morphological marking of German. Comprehensive work on the morphological marking of this natural marking is based on conditional random fields and random marking models."}, {"heading": "3 From Characters to Tags", "text": "In fact, it's like most people are able to understand themselves and understand what it's about: the question of how they should behave, and the question of how they should behave, and the question of how they should behave, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about, what it's about it's, what it's about, what it's about it's, what it's about, what it's about it's about, what it's about it's, what it's about it's, what it's about it is, what it's about it is, what it's about it is, what it's about it is, what it's about it is, what it's about it's about it is, what it's about it is, what it's about it is, what it's about it is, what it's about it is, what it's about it is, what it's about it's about it is, what it is, what it's about it's about it is, what it is, what it's about it is about it is, what it is, what it's about it is, what it is, what it is, what it's it's about it is, what it is about it is it is it is it is, what it is it is it is it is about it is, what it is, what it is about it is it is, what it is it is"}, {"heading": "4 Experimental Results", "text": "We first test variants of the architecture for German (Section 4.3) and then verify our empirical findings for Czech (Section 4.4)."}, {"heading": "4.1 Data", "text": "We are conducting the experiments with the German TIGER-Corpus1 and the Czech PDT-Corpus2. For the time being, we have decided against using the current Universal Dependencies 3 because there are no comparative results for the morphological marking in the literature. Table 1 (at the beginning of the paper) presents OOV rates and Table 2 presents some Corpus statistics. Part of the experiments is supervised learning using small marked data sets (TIGER, PDT) and part also includes large unlabeled data (de.wikidump, cs.wikidump) 4. The day sizes observed in the labeled training data depend on the language and the type of tags: 54 (POS, German), 255 (MORPH, German), 681 (POSMORPH, German) and 1,811 (POSMORPH, Czech), with POS representing the part of corpus1."}, {"heading": "4.2 Setup", "text": "The best setups for character-based word vector networks are as follows: \u2022 DNN: character vector size = 128, a fully connected layer with 256 hidden nodes \u2022 CNN: character vector size = 128, two revolutionary layers with 256 filters and a filter width of five each \u2022 CNNHighway: the large setup layer of (Kim et al., 2016), i.e. character vector size = 15, filter widths from one to seven, number of filters as a function of filter width min {200, 50 \u00b7 filter width}, two highway layers \u2022 LSTM: character vector size = 128, two layers with 1024 and 256 nodes \u2022 BLSTM: character vector size = 128, two layers with 256 nodes per second."}, {"heading": "4.3 German", "text": "Our basic model (CNN-BLSTM) consists of the BLSTM in Fig. 2 and the CNN in Fig. 3 (c) with a single revolutionary level, which is a simplified version of the best model in the world (Labeau et al., 2015). The results are summarized in Table 3. We show the results for different day sets (see Section 4.1) to facilitate comparison with the state of the art. Our CNN-BLSTM Baseline achieves comparable or better results for all day sets. In particular, our CNN-BLSTM significantly (under consistent conditions) introduces the related models in (Labeau et al.) and (Ling et al., 2015) 5 as expected, word-level word vectors at their own level (Fig. 3) perform significantly worse than word vectors as word vectors."}, {"heading": "4.4 Czech", "text": "We confirm our empirical findings for German in another morphologically rich language (Czech). Results are summarized in Table 6 for the models that performed best in German. Similar to German, CNNHighway-BLSTM and LSTMBLSTM perform similarly (0.5% absolute error rate) and significantly better than baselines (25% or more relative error rate reduction). Extending character-based word vectors by pre-trained embeddings yields an additional small gain. Again, the gain for CNNHighway-BLSTM is greater than for LSTMBLSTM."}, {"heading": "5 Summary", "text": "This paper summarizes our empirical evaluation of the character-based neural network approach to morphological marking as follows: As long as carefully calibrated neural networks with sufficient capacity (e.g. number of hidden layers) are used, the effect of the specific network architecture (e.g. convolutional vs. recursive) for the task under consideration is low. However, the choice of architecture can greatly influence the training time (in our implementation, the revolutionary networks are 2-4 times slower than the recursive networks).Extending the character-based word vectors to include word embedding prepared for large amounts of unattended data leads to large gains for the small configurations, but only small gains over the best configurations. Furthermore, our best character-based morphological taggers exceed the current results for German and Czech by a relative gain of 30% or more."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["C. Dyer", "N. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] D. Gillick", "C. Brunk", "O. Vinyals", "A. Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks. Studies in Computational Intelligence", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Non-lexical neural architecture for fine-grained POS tagging", "author": ["Labeau et al.2015] M. Labeau", "K. L\u00f6ser", "A. Allauzen"], "venue": null, "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R. Fernandez Astudillo", "S. Amir", "C. Dyer", "A. Black", "I. Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] M. Luong", "R. Socher", "C. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Endto-end sequence labeling via bi-directional LSTMCNNs-CRF", "author": ["Ma", "Hovy2016] X. Ma", "E. Hovy"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Robust morphological tagging with word representations", "author": ["M\u00fcller", "Sch\u00fctze2015] T. M\u00fcller", "H. Sch\u00fctze"], "venue": "In ACL,", "citeRegEx": "M\u00fcller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["M\u00fcller et al.2013] T. M\u00fcller", "H. Schmid", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "M\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "How to construct deep recurrent neural networks. In ICLR", "author": ["Pascanu et al.2014] R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["B. Plank", "A. S\u00f8gaard", "Y. Goldberg"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Zadrozny2014] C. dos Santos", "B. Zadrozny"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] R. Soricut", "F. Och"], "venue": "In ACL,", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Recurrent neural network regularization", "author": ["Zaremba et al.2015] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Compared to morphemes as the sub-word unit (Luong et al., 2013), characters have the advantage of being directly available from the text and do not require additional resources and complex pre-processing steps.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 3, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 15, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 7, "context": "The work by (Labeau et al., 2015) uses a CNN/Markov model hybrid for morphological tagging of German.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "Comprehensive work on morphological tagging based on conditional random fields along with state-of-the-art results can be found in (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 131, "endOffset": 178}, {"referenceID": 7, "context": "Our work is inspired by previous work (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ling et al., 2015) but uses a deeper hierarchy of layers in combination with a simple prediction model and provides comprehensive comparative results for alternative neural architectures for morphological tagging.", "startOffset": 38, "endOffset": 134}, {"referenceID": 8, "context": "Our work is inspired by previous work (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ling et al., 2015) but uses a deeper hierarchy of layers in combination with a simple prediction model and provides comprehensive comparative results for alternative neural architectures for morphological tagging.", "startOffset": 38, "endOffset": 134}, {"referenceID": 3, "context": "Several extensions of the neural approach used in this paper have been proposed, including multilingual training (Gillick et al., 2015), auxiliary tasks (Plank et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": ", 2015), auxiliary tasks (Plank et al., 2016), and more structured prediction models (Collobert et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 7, "context": ", 2016), and more structured prediction models (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ma and Hovy, 2016).", "startOffset": 47, "endOffset": 143}, {"referenceID": 3, "context": "Character-based approaches have also been applied to other tasks in natural language processing, such as named entity recognition (Gillick et al., 2015), parsing (Ballesteros et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": ", 2015), parsing (Ballesteros et al., 2015) (BLSTM), language modeling (Ling et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 8, "context": ", 2015) (BLSTM), language modeling (Ling et al., 2015) (BLSTM) and (Kim et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 6, "context": ", 2015) (BLSTM) and (Kim et al., 2016) (CNNs) or neural machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 20, "endOffset": 38}, {"referenceID": 4, "context": ", wN ) with recurrent neural networks, such as long short-term memory recurrent neural networks (LSTMs) (Graves, 2012).", "startOffset": 104, "endOffset": 118}, {"referenceID": 11, "context": "Wordbased word vectors are attractive because they can be efficiently pre-trained on supplementary, large amounts of unsupervised data (Mikolov et al., 2013).", "startOffset": 135, "endOffset": 157}, {"referenceID": 7, "context": "\u2022 Convolutional neural networks (CNNs) (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015): Compared to DNNs, CNNs use weight tying and local connections.", "startOffset": 39, "endOffset": 116}, {"referenceID": 6, "context": "\u2022 CNNHighway (Kim et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016): This CNN variant is similar to vanilla CNNs but maintains a set of one-layer CNNs with different filter widths.", "startOffset": 13, "endOffset": 64}, {"referenceID": 8, "context": "\u2022 LSTMs (Ling et al., 2015): LSTMs are sequential models and thus a natural choice for character strings.", "startOffset": 8, "endOffset": 27}, {"referenceID": 8, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 0, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 15, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 14, "context": "Learning in recurrent or very deep neural networks is non-trivial and skip/shortcut connections have been proposed to improve the learning of such networks (Pascanu et al., 2014; He et al., 2016).", "startOffset": 156, "endOffset": 195}, {"referenceID": 5, "context": "Learning in recurrent or very deep neural networks is non-trivial and skip/shortcut connections have been proposed to improve the learning of such networks (Pascanu et al., 2014; He et al., 2016).", "startOffset": 156, "endOffset": 195}, {"referenceID": 7, "context": "This significantly reduces the computational and implementation complexity compared to firstorder Markov models as used in (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015).", "startOffset": 123, "endOffset": 200}, {"referenceID": 6, "context": "\u2022 CNNHighway: the large setup from (Kim et al., 2016), i.", "startOffset": 35, "endOffset": 53}, {"referenceID": 19, "context": "In particular and in contrast to (Zaremba et al., 2015), we also use dropout on the recurrent parts of the network because it gives significantly better results.", "startOffset": 33, "endOffset": 55}, {"referenceID": 7, "context": "3 (c) with a single convolutional layer, which is a simplified version of the best model in (Labeau et al., 2015).", "startOffset": 92, "endOffset": 113}, {"referenceID": 7, "context": "In particular, our CNN-BLSTM clearly (under consistent conditions) outperforms the related models in (Labeau et al., 2015) and (Ling et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 8, "context": ", 2015) and (Ling et al., 2015)5.", "startOffset": 12, "endOffset": 31}, {"referenceID": 11, "context": "Word embeddings (Mikolov et al., 2013) or word clusters (M\u00fcller and Sch\u00fctze, 2015) allow us to exploit large amounts of unlabeled data.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "com/wlin12/JNN to produce consistent results as the results in (Ling et al., 2015) are for the last 100 training sentences only and not for the standard test set.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "59 PCRF (M\u00fcller et al., 2013) 2.", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "42 biRNN, Non-Lex/Struct (Labeau et al., 2015) 3.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "88 biRNN, Both/Struct (Labeau et al., 2015) 2.", "startOffset": 22, "endOffset": 43}, {"referenceID": 8, "context": "97 BLSTM, lower-case (Ling et al., 2015)5 3.", "startOffset": 21, "endOffset": 40}, {"referenceID": 8, "context": "04 BLSTM, mixed case (Ling et al., 2015)5 2.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "CNN baseline (Labeau et al., 2015) 10.", "startOffset": 13, "endOffset": 34}, {"referenceID": 8, "context": "97 BLSTM baseline (Ling et al., 2015)5 10.", "startOffset": 18, "endOffset": 37}, {"referenceID": 13, "context": "PDT PCRF (M\u00fcller et al., 2013) 6.", "startOffset": 9, "endOffset": 30}, {"referenceID": 8, "context": "01 BLSTM (Ling et al., 2015)5 6.", "startOffset": 9, "endOffset": 28}], "year": 2016, "abstractText": "This paper investigates neural characterbased morphological tagging for languages with complex morphology and large tag sets. We systematically explore a variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors combined with bidirectional LSTMs to model across-word context in an end-to-end setting. We explore supplementary use of word-based vectors trained on large amounts of unlabeled data. Our experiments for morphological tagging suggest that for \u201dsimple\u201d model configurations, the choice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy. Increasing the model capacity by adding depth, for example, and carefully optimizing the neural networks can lead to substantial improvements, and the differences in accuracy (but not training time) become much smaller or even negligible. Overall, our best morphological taggers for German and Czech outperform the best results reported in the literature by a large margin.", "creator": "LaTeX with hyperref package"}}}