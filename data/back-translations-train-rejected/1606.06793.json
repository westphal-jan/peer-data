{"id": "1606.06793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Scalable Semi-supervised Learning with Graph-based Kernel Machine", "abstract": "Owing to the prevalence of unlabeled data, semisupervised learning has recently drawn significant attention and has found applicable in many real-world applications. In this paper, we present the so-called Graph-based Semi-supervised Support Vector Machine (gS3VM), a method that leverages the excellent generalization ability of kernel-based method with the geometrical and distributive information carried in a spectral graph for semi-supervised learning purpose. The proposed gS3VM can be solved directly in the primal form using the Stochastic Gradient Descent method with the ideal convergence rate $O(\\frac{1}{T})$. Besides, our gS3VM allows the combinations of a wide spectrum of loss functions (e.g., Hinge, smooth Hinge, Logistic, L1, and {\\epsilon}-insensitive) and smoothness functions (i.e., $l_p(t) = |t|^p$ with $p\\ge1$). We note that the well-known Laplacian Support Vector Machine falls into the spectrum of gS3VM corresponding to the combination of the Hinge loss and the smoothness function $l_2(.)$. We further validate our proposed method on several benchmark datasets to demonstrate that gS3VM is appropriate for the large-scale datasets since it is optimal in memory used and yields superior classification accuracy whilst simultaneously achieving a significant computation speedup in comparison with the state-of-the-art baselines.", "histories": [["v1", "Wed, 22 Jun 2016 00:26:59 GMT  (2658kb,D)", "http://arxiv.org/abs/1606.06793v1", "18 pages"], ["v2", "Tue, 6 Sep 2016 02:09:35 GMT  (8755kb)", "http://arxiv.org/abs/1606.06793v2", "15 pages"], ["v3", "Thu, 6 Apr 2017 02:40:23 GMT  (2731kb,D)", "http://arxiv.org/abs/1606.06793v3", "21 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["trung le", "khanh nguyen", "van nguyen", "vu nguyen", "dinh phung"], "accepted": false, "id": "1606.06793"}, "pdf": {"name": "1606.06793.pdf", "metadata": {"source": "CRF", "title": "Scalable Support Vector Machine for Semi-supervised Learning", "authors": ["Trung Le", "Khanh Nguyen", "Van Nguyen", "Vu Nguyen", "Dinh Phung"], "emails": [], "sections": [{"heading": null, "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}], "references": [{"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "International Conference on Machine Learning (ICML), Bled, Slowenien, 1999, pp. 200\u2013209.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Bootstrapping svm active learning by incorporating unlabelled images for image retrieval.", "author": ["L. Wang", "K.L. Chan", "Z. Zhang"], "venue": "IEEE Computer Society,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Incremental learning in autonomous systems: evolving connectionist systems for on-line image and speech recognition", "author": ["N. Kasabov", "D. Zhang", "P. Pang"], "venue": "Advanced Robotics and its Social Impacts, 2005. IEEE Workshop on, june 2005, pp. 120 \u2013 125.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Combining labelled and unlabelled data: A case study on fisher kernels and transductive inference for biological entity recognition", "author": ["C. Goutte", "H. D\u00e9jean", "E. Gaussier", "N. Cancedda", "J.- M. Renders"], "venue": "Proceedings of the 6th Conference on Natural Language Learning - Volume 20, ser. COLING-02, 2002, pp. 1\u20137.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S. Keerthi"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 203\u2013 233, Jun. 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ser. ICML \u201901, 2001, pp. 19\u201326.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts.", "author": ["A. Blum", "J.D. Lafferty", "M.R. Rwebangira", "R. Reddy"], "venue": "in ICML, vol", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "IN ICML, 2003, pp. 912\u2013919.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Partially labeled classification with markov random walks", "author": ["M. Szummer", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems. MIT Press, 2002, pp. 945\u2013952.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "The rendezvous algorithm: Multiclass semisupervised learning with markov random walks", "author": ["A. Azran"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ser. ICML \u201907, 2007, pp. 49\u201356.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. Joachims"], "venue": "In ICML, 2003, pp. 290\u2013297.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph-based semi-supervised support vector data description for novelty detection", "author": ["P. Duong", "V. Nguyen", "M. Dinh", "T. Le", "D. Tran", "W. Ma"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), July 2015, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, Dec. 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 1995, pp. 273\u2013297.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-Supervised Classification by Low Density Separation", "author": ["O. Chapelle", "A. Zien"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906. ACM, 2006, pp. 185\u2013192.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised support vector machines for unlabeled data classification", "author": ["G. Fung", "O. Mangasarian"], "venue": "Optimization Methods and Software, vol. 15, pp. 29\u201344, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Large scale transductive svms", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou", "T. Joachims"], "venue": "Journal of Machine Learning Research, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Deterministic annealing for semi-supervised kernel machines", "author": ["V. Sindhwani", "S. Keerthi", "O. Chapelle"], "venue": "Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906, 2006, pp. 841\u2013848.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy entropy semi-supervised support vector data description", "author": ["T. Le", "D. Tran", "T. Tran", "K. Nguyen", "W. Ma"], "venue": "2013 International Joint Conference on Neural Networks (IJCNN), Aug 2013, pp. 1\u20135.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel-based semi-supervised learning for novelty detection", "author": ["V. Nguyen", "T. Le", "T. Pham", "M. Dinh", "T.H. Le"], "venue": "2014 International Joint Conference on Neural Networks (IJCNN), July 2014, pp. 4129\u20134136.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning using semi-definite programming", "author": ["T. De Bie", "N. Cristianini"], "venue": "Semi-supervised Learning, Cambridge, MA, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Cluster kernels for semi-supervised learning", "author": ["O. Chapelle", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems 15, S. Becker, S. Thrun, and K. Obermayer, Eds. MIT Press, 2003, pp. 601\u2013608.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised learning with trees", "author": ["C. Kemp", "T.L. Griffiths", "S. Stromsten", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Sch\u00f6lkopf, Eds., 2004, pp. 257\u2013264.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion kernels on graphs and other discrete input spaces.", "author": ["R.I. Kondor", "J.D. Lafferty"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Kernels and regularization on graphs.", "author": ["A.J. Smola", "I.R. Kondor"], "venue": "Proceedings of the Annual Conference on Computational Learning Theory,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Nonparametric transforms of graph kernels for semisupervised learning.", "author": ["X. Zhu", "J.S. Kandola", "Z. Ghahramani", "J.D. Lafferty"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Linear manifold regularization for large scale semi-supervised learning", "author": ["V. Sindhwani", "P. Niyogi"], "venue": "Proc. of the 22nd ICML Workshop on Learning with Partially Classified Training Data, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale sparsified manifold regularization.", "author": ["I.W. Tsang", "J.T. Kwok"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Laplacian support vector machines trained in the primal", "author": ["S. Melacci", "M. Belkin"], "venue": "J. Mach. Learn. Res., vol. 12, Jul. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Logarithmic regret algorithms for strongly convex repeated games", "author": ["S. Shalev-shwartz", "Y. Singer"], "venue": "The Hebrew University, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Mind the duality gap:  16 Logarithmic regret algorithms for online optimization", "author": ["S. Kakade", "Shalev-Shwartz"], "venue": "NIPS, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method", "author": ["S. Lacoste-Julien", "M.W. Schmidt", "F. Bach"], "venue": "CoRR, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards semi-supervised classification with markov random fields", "author": ["X. Zhu", "Z. Ghahramani"], "venue": "2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Semi-supervised learning \u2013 a statistical physics approach", "author": ["G. Getz", "N. Shental", "E. Domany"], "venue": "CoRR, 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification by semi-supervised discriminative regularization", "author": ["F. Wu", "W. Wang", "Y. Yang", "Y. Zhuang", "F. Nie"], "venue": "Neurocomputing, vol. 73, pp. 1641 \u2013 1651, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 567\u2013599, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 176, "endOffset": 179}, {"referenceID": 2, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 229, "endOffset": 232}, {"referenceID": 4, "context": "While obtaining pre-defined labels is a labor-intensive and time-consuming process [5], ones have found that unlabeled data, when used in conjunction with a small amount of labeled data, can bring a remarkable improvement in classification accuracy [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "While obtaining pre-defined labels is a labor-intensive and time-consuming process [5], ones have found that unlabeled data, when used in conjunction with a small amount of labeled data, can bring a remarkable improvement in classification accuracy [1].", "startOffset": 249, "endOffset": 252}, {"referenceID": 5, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 9, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 10, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 11, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "Inspired from the pioneering work of [1], recent works have attempted to incorporate kernel methods such as Support Vector Machine (SVM) [14] with the semi-supervised learning paradigm.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "Inspired from the pioneering work of [1], recent works have attempted to incorporate kernel methods such as Support Vector Machine (SVM) [14] with the semi-supervised learning paradigm.", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "The underlying idea of this research line is to solve standard SVM problem while treating the unknown labels as optimization variables [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 17, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 19, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 23, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 24, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 25, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 26, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 12, "context": "Manifold regularization framework [13] exploits the geometric information of the probability distribution that generates data and incorporates it as an additional regularization term.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 28, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 29, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 29, "context": "Specifically, the work of [30] makes use of the preconditioned conjugate gradient to solve the optimization problem encountered in manifold regularization framework in the primal form.", "startOffset": 26, "endOffset": 30}, {"referenceID": 29, "context": "In addition, the LapSVM in primal approach [30] requires to store the entire Hessian matrix of size n \u00d7 n in the memory, resulting in a memory complexity of O(n).", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 31, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 32, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 12, "context": "We note that the well-known Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM(s) when using Hinge loss and the smoothness function l2 (.", "startOffset": 70, "endOffset": 78}, {"referenceID": 29, "context": "We note that the well-known Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM(s) when using Hinge loss and the smoothness function l2 (.", "startOffset": 70, "endOffset": 78}, {"referenceID": 32, "context": "We then develop a new algorithm based on the SGD framework [33] to directly solve the optimization problem of gS3VM in its primal form with the ideal convergence rate O ( 1 T ) .", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "\u2022 We apply stochastic gradient descent (SGD) framework [33] to solve directly gS3VM in its primal form.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "In [6, 7], semi-supervised learning problem is viewed as graph mincut problem.", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [6, 7], semi-supervised learning problem is viewed as graph mincut problem.", "startOffset": 3, "endOffset": 9}, {"referenceID": 33, "context": "In [34], Markov Chain Monte Carlo sampling techniques is used to approximate this marginal probability.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "The work of [35] proposes to compute the marginal probabilities of the discrete Markov random field at any temperature with the Multi-canonical Monte Carlo method, which seems to be able to overcome the energy trap faced by the standard Metropolis or Swendsen-Wang method.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "The harmonic functions used in [8] is regarded as a continuous relaxation of the discrete Markov random field.", "startOffset": 31, "endOffset": 34}, {"referenceID": 22, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 23, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 24, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 26, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 4, "context": "The kernel-based semisupervised methods are primarily driven by the idea to solve a standard SVM problem while treating the unknown labels as optimization variables [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 16, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 191, "endOffset": 199}, {"referenceID": 17, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 19, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 20, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 21, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 269, "endOffset": 273}, {"referenceID": 0, "context": "S4VM is shown to be safe and to achieve the maximal performance improvement under the low-density assumption of S3VM [1].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "extends [13, 36] to propose semi-supervised discriminationaware manifold regularization framework which considers the discrimination of all available instances in learning of manifold regularization.", "startOffset": 8, "endOffset": 16}, {"referenceID": 35, "context": "extends [13, 36] to propose semi-supervised discriminationaware manifold regularization framework which considers the discrimination of all available instances in learning of manifold regularization.", "startOffset": 8, "endOffset": 16}, {"referenceID": 12, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 28, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 29, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 12, "context": "However, the original work of manifold regularization [13] requires to invert a matrix of size n by n which costs cubically and hence is not scalable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "[29] scales up manifold regularization framework by adding in an \u03b5-insensitive loss into the energy function, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "LapSVM (in primal) [30] employs the preconditioned conjugate gradient to solve the optimization problem of manifold regularization in the primal form.", "startOffset": 19, "endOffset": 23}, {"referenceID": 29, "context": "However, the optimization problem in [30] is indeed solved in the first dual layer rather than in the primal form.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "In addition, we empirically find that the LapSVM in primal [30] is unsatisfactory in terms of memory complexity.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "It is noteworthy that Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM using the Hinge loss with the smoothness function l2 (.", "startOffset": 64, "endOffset": 72}, {"referenceID": 29, "context": "It is noteworthy that Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM using the Hinge loss with the smoothness function l2 (.", "startOffset": 64, "endOffset": 72}, {"referenceID": 32, "context": "We employ the SGD framework proposed in [33] to solve the optimization problem in Eq.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "\u2022 Smooth Hinge loss [37]", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "Baselines In order to investigate the efficiency and accuracy of our proposed method (gUS3VM), we made comparison with the following baselines: \u2022 LapSVM in primal [30]: Laplacian Support Vector Machine in primal is a state-of-the-art approach in semisupervised classification based on manifold regularization framework.", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "of the original LapSVM [13] from O ( n ) to O ( kn )", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "\u2022 CCCP [18]: An approach is proposed to solve the nonconvex optimization problem occurred in the kernel semisupervised approach using convex-concave procedures.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "With LapSVM, we use the parameter settings proposed in [30], wherein the parameters \u03b3A and \u03b3I are searched in the grid { 10\u22126, 10\u22124, 10\u22122, 10\u22121, 1, 10, 100 } .", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "In all experiments with the LapSVM, we make use of the preconditioned conjugate gradient version, which seems more suitable for the LapSVM optimization problem [30].", "startOffset": 160, "endOffset": 164}], "year": 2017, "abstractText": "Owing to the prevalence of unlabeled data, semisupervised learning has recently drawn significant attention and has found applicable in many real-world applications. In this paper, we present the so-called Graph-based Semi-supervised Support Vector Machine (gS3VM), a method that leverages the excellent generalization ability of kernel-based method with the geometrical and distributive information carried in a spectral graph for semi-supervised learning purpose. The proposed gS3VM can be solved directly in the primal form using the Stochastic Gradient Descent method with the ideal convergence rate O ( 1 T ) . Besides, our gS3VM allows the combinations of a wide spectrum of loss functions (e.g., Hinge, smooth Hinge, Logistic, L1, and \u03b5-insensitive) and smoothness functions (i.e., lp (t) = |t| with p \u2265 1). We note that the well-known Laplacian Support Vector Machine falls into the spectrum of gS3VM corresponding with the combination of the Hinge loss and the smoothness function l2 (.). We further validate our proposed method on several benchmark datasets to demonstrate that gS3VM is appropriate for the large-scale datasets since it is optimal in memory used and yields superior classification accuracy whilst simultaneously achieving a significant computation speedup in comparison with the state-of-the-art baselines", "creator": "LaTeX with hyperref package"}}}