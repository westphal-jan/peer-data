{"id": "1606.06864", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "A Curriculum Learning Method for Improved Noise Robustness in Automatic Speech Recognition", "abstract": "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple training method called accordion annealing which is inspired by curriculum learning. It uses a multi-stage training schedule where samples at SNR values as low as -15 dB are first added and subsequently samples at increasing higher SNR values are gradually added up to an SNR value of 50 dB. Because of the necessity of providing many different SNR valued samples, we additionally use a method called per-epoch noise mixing (PEM) to generate the noisy training samples online during training. Both the accordion annealing and the PEM methods are used during training of a recurrent neural network which is then evaluated on three databases. Accordion annealing improves the relative average accuracy of this network by 2 % when tested in a high SNR range (50 dB to 0 dB) and by 42 % in a low SNR range(-20 dB to 0 dB) on the Grid Corpus database.", "histories": [["v1", "Wed, 22 Jun 2016 09:29:40 GMT  (20kb)", "https://arxiv.org/abs/1606.06864v1", null], ["v2", "Fri, 16 Sep 2016 15:20:39 GMT  (20kb)", "http://arxiv.org/abs/1606.06864v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["stefan braun", "daniel neil", "shih-chii liu"], "accepted": false, "id": "1606.06864"}, "pdf": {"name": "1606.06864.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 6.06 864v 2 [cs.C L] 16 Sep 20Index Terms - automatic speech recognition, recurring neural networks, noise robustness, curriculum learning"}, {"heading": "1. INTRODUCTION", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2. TRAINING METHODS FOR IMPROVED NOISE ROBUSTNESS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Baseline", "text": "Our basic method uses multi-condition training [16] to increase the noise resistance of the network. Pink noise is added to a clean dataset to generate samples with the desired SNR. Each training sample is randomly selected to have an SNR level in the range of 0 to 50 dB with 5 dB steps. This wide range is larger than the SNR range used in previous work (e.g. 0 to 30 dB as in [10]). Our extensive simulations show that the use of such a noise spectrum has resulted in the best performance on the test datasets. Noise mixing occurs once at the waveform level before the audio features of the filter bank are calculated. This one set of training data is presented to the network over all training periods, the resulting networks are referred to as \"noisy baselines.\""}, {"heading": "2.2. Gaussian noise injection", "text": "Gaussian noise injection is a well-known method for improving generalization in neural networks [17]. It is used here to improve the noise robustness of the network.During the training, artificial Gaussian noise is added to the filter functions generated from the various SNR samples. The amount of additive noise is taken from a zero-centric Gaussian distribution. The best results were obtained by using Gaussian noise with a standard deviation of \u03c3 = 0.6. This method is referred to in the rest of the paper as the \"Gaussian method.\""}, {"heading": "2.3. Per-epoch noise mixing (PEM)", "text": "In each training epoch, each training sample is mixed with a randomly sampled noise segment in a randomly selected SNR range. The training procedure consists of the following steps: 1. Each training sample is combined with a randomly selected noise segment to create a resulting sample at a randomly selected SNR level between 0 and 50 dB. 2. Extract audio features (e.g. filter bank functions) for the noisecorrupted audio to obtain training data for the current epoch. 3. Optional: Add noise to the audio features."}, {"heading": "2.4. Curriculum learning", "text": "Thus, a network trained for clean conditions performs worse than a network trained for noisy conditions. Furthermore, networks trained for a large SNR range generally perform worse than networks optimized for this specific SNR. In order to achieve high accuracy under both high and low SNR with a single network, we have investigated novel training paradigms based on curriculum learning. While curriculum learning has been used in image classification (planned denoization of autoencoders, [18]) and speech recognition (sort degree [15], a method for faster accuracy convergence), this is the first work aimed at LVCSR under noisy conditions.Our novel ACCAN training method applies a multi-level training plan: In the first stage, the neural network is trained for the lowest SNR samples. In the following steps, the SNR training range is expanded towards higher SNR values."}, {"heading": "3. SETUP", "text": "The aforementioned pre-election suggestions were not taken seriously by voters, so they were able to decide to stand."}, {"heading": "4. RESULTS", "text": "The results were tested in clean condition and with additional pink noise or chatter at 15 SNR levels from 50dB to -20 dB in dB increments. In Table 2, we specify the average WER over the following SNR ranges: \u2022 Full SNR range: [clean signal, 50dB to -10dB] \u2022 High SNR range: [50dB to 0dB] \u2022 Low SNR range: [0dB to -10dB] \u2022 Range of Interest (ROI): [20dB to -10dB] We choose to include ROI because our hearing tests have shown that this range seems to reflect well common scenarios in public environments where a clean speech signal is most often not found. Detailed results for each SNR are given individually in Table 3. Results for -15dB and -20dB should be considered as extreme cases."}, {"heading": "4.1. Noise addition methods", "text": "This section summarizes the results of the baseline, Gauss, VanillaPEM and Gauss-PEM methods, all of which were trained in the SNR range of 0dB to 50dB. Our network, which was trained on clean baseline only, achieves 13.8% WER with a trigram speech model and our 8.5M parameter network, while in the literature a 13.5% WER with a trigram speech model and 3x larger (26.5 M) parameter network was achieved, confirming that our end-to-end speech recognition pipeline is fully functional.Baseline: The sound baseline network starts with a 25% higher WER on the clean test than our clean baseline network. For SNRs that are lower than 25dB, the noise baseline is much more robust. The WER seems to increase dramatically at 25dB on the clean SNEM base, while the noise baseline WEM is reduced at a lower SNB baseline, while the SNEM increase is low at a lower SNB baseline."}, {"heading": "4.2. Curriculum learning", "text": "To further enhance the robustness of noise, we developed a curriculum learning strategy for the Gauss PEM Method, which led to our novel ACCAN Method. We compare our results with the Gauss PEM Method, as it was the most robust non-curriculum method. Our test results show increased robustness of noise for ACCAN for pink noise and baby noise: WHO decreases between 3.3% (ROI, pink noise) and 4.1% (ROI baby noise). Pink noise shows the largest decrease at 0dB (10.5% WHO decrease) and -5dB (11.3% decrease). For baby noise, the largest WER decreases are recorded at 10dB (4.9%), 5dB (10.9%) and 0dB (10.7%). The average WER of ACCAN in the high range is worse than absolute WER (8.8% increase in relative WER noise, but better in relative WER noise)."}, {"heading": "5. DISCUSSION", "text": "All proposed training methods result in networks with increased noise levels in the low SNR range compared to the standard sound backdrop. Noise levels are increased at the network level and are not based on complex pre-processing frameworks. We see an increased noise backdrop against both tested noise categories. This is noteworthy as the networks only perceive the pink sound backdrop during training. Results show that the waveform noise backdrop (PEM) is particularly effective in transferring sound backdrops to unseen sound backdrops (Gauss). PEM also enabled a robust sound backdrop aimed at clean speech."}, {"heading": "6. CONCLUSION", "text": "This paper proposes new training methods to improve the noise resistance of RNNs for an LVCSR task. Networks are trained for a wide SNR range, with the vanilla PEM training method increasing noise at the waveform level and the Gauss method injecting Gaussian noise at the trait level. By combining the Gaussian and vanilla PEM methods with the Gaussian PEM method, we achieve an average WER reduction of 28% in the range of 20dB to -10 dB SNR compared to a conventional multi-conditioning training method. At the same time, we achieve a lower WER with clean language than a network trained exclusively on clean language. ACCAN training strategy improves the Gaussian PEM method by a curriculum learning strategy. The ACCAN training strategy results in a performance of up to 11.3% at low SNRs compared to the Gaussian PEM method."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "The authors acknowledge Enea Ceolini and Joachim Ott for their discussions on RNNs. They also acknowledge Ying Zhang and Yoshua Bengio (both from the University of Montreal) for their help in building the language model. This work was partially supported by the Samsung Advanced Institute of Technology and the EU H2020 COCOHA # 644732."}, {"heading": "8. REFERENCES", "text": "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal Processing Magazine, vol. 29, pp. 82-97, 2012. [2] Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach, \"An overview of noise-robust speech recognition,\" IEEE / ACM Transactions on Audio, Speech, and Language Processing, vol. 22, pp. 745-777, 2014. Steven F Boll, \"Suppression of acoustic noise in speech using spectral subtraction,\""}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "An overview of noise-robust automatic speech recognition", "author": ["Jinyu Li", "Li Deng", "Yifan Gong", "Reinhold Haeb-Umbach"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745\u2013777, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["Steven F Boll"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 27, no. 2, pp. 113\u2013120, 1979.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1979}, {"title": "Unified asr system using lgm-based source separation, noise-robust feature extraction, and word hypothesis selection", "author": ["Yusuke Fujita", "Ryoichi Takashima", "Takeshi Homma", "Rintaro Ikeshita", "Yohei Kawaguchi", "Takashi Sumiyoshi", "Takashi Endo", "Masahito Togami"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 416\u2013422.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition", "author": ["Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "JL Roux", "Vikramjit Mitra", "Shinji Watanabe"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Compensation for the effect of the communication channel in auditory-like analysis of speech (rasta-plp)", "author": ["Hynek Hermansky", "Nelson Morgan", "Aruna Bayya", "Phil Kohn"], "venue": "Second European Conference on Speech Communication and Technology, 1991.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition", "author": ["Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "JL Roux", "Vikramjit Mitra", "Shinji Watanabe"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural networks for noise reduction in robust asr", "author": ["Andrew L Maas", "Quoc V Le", "Tyler M O\u2019Neil", "Oriol Vinyals", "Patrick Nguyen", "Andrew Y Ng"], "venue": "INTERSPEECH, 2012, pp. 22\u201325.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition", "author": ["Xue Feng", "Yaodong Zhang", "James Glass"], "venue": "2014 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2014, pp. 1759\u20131763.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["Dimitri Palaz", "Ronan Collobert"], "venue": "Proceedings of Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["George Trigeorgis", "Fabien Ringeval", "Raymond Brueckner", "Erik Marchi", "Mihalis A Nicolaou", "Stefanos Zafeiriou"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200\u20135204.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["Michael L Seltzer", "Dong Yu", "Yongqiang Wang"], "venue": "2013 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013, pp. 7398\u20137402.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1929}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167\u2013174.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei"], "venue": "CoRR, vol. abs/1512.02595, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Noisy training for deep neural networks in speech recognition", "author": ["Shi Yin", "Chao Liu", "Zhiyong Zhang", "Yiye Lin", "Dong Wang", "Javier Tejedor", "Thomas Fang Zheng", "Yinguo Li"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1\u201314, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel regression and backpropagation training with noise", "author": ["Petri Koistinen", "Lasse Holmstr\u00f6m"], "venue": "1991 IEEE International Joint Conference on Neural Networks, 1991, pp. 367\u2013 372.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Scheduled denoising autoencoders", "author": ["Krzysztof J. Geras", "Charles A. Sutton"], "venue": "CoRR, vol. abs/1406.3269, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA, 2009, ICML \u201909, pp. 41\u201348, ACM.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Audacity", "author": ["Audacity team"], "venue": "2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Assessment for automatic speech recognition: Ii. noisex-92: A database and an experiment to study the effect of additive noise on speech recognition systems", "author": ["Andrew Varga", "Herman JM Steeneken"], "venue": "Speech Communication, vol. 12, no. 3, pp. 247\u2013251, 1993.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd International Conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Lasagne: First release", "author": ["Lasagne team"], "venue": "Aug. 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 249\u2013256.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The performance of automatic speech recognition (ASR) systems has increased significantly with the use of deep neural networks (DNNs) [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Over the past decades, a multitude of methods to improve noise robustness of ASR systems has been proposed [2], with many methods being applicable to DNNs.", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "Methods applied at the feature level include methods that produce auditory-level features [6] and feature space adaptation methods [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "Methods applied at the feature level include methods that produce auditory-level features [6] and feature space adaptation methods [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "10 dB - 20 dB [12] or 0 dB - 30 dB [10] are used during training.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "10 dB - 20 dB [12] or 0 dB - 30 dB [10] are used during training.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Other training methods such as dropout [13] - originally intended to improve regularisation - have been shown to also improve noise robustness [12].", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Other training methods such as dropout [13] - originally intended to improve regularisation - have been shown to also improve noise robustness [12].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "The same is true for model adaptation/noise aware training techniques [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "RNNs are used here because they have demonstrated state-ofthe-art performance in tasks such as the common sequence labelling task in speech recognition [14] [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "RNNs are used here because they have demonstrated state-ofthe-art performance in tasks such as the common sequence labelling task in speech recognition [14] [15].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "Our baseline method takes advantage of multi-condition training [16] in order to increase the noise robustness of the network.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "0 to 30 dB as in [10]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "Gaussian noise injection is a well-known method for improving generalisation in neural networks [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "ACCAN [0] [0, 5] .", "startOffset": 10, "endOffset": 16}, {"referenceID": 15, "context": "Neural networks have been shown to optimize on the SNR they are trained on [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "While curriculum learning has been used in image classification (scheduled denoising autoencoders, [18]) as well as speech recognition (SortaGrad [15], a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "While curriculum learning has been used in image classification (scheduled denoising autoencoders, [18]) as well as speech recognition (SortaGrad [15], a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "However, the noise allows the network to explore the parameter space more extensively at the beginning [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "For noise corruption, we used two different noise types: pink noise generated by the Audacity [20] software and babble noise from the NOISEX database [21].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "For noise corruption, we used two different noise types: pink noise generated by the Audacity [20] software and babble noise from the NOISEX database [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "Data preparation and language model The labels and transcriptions were extracted with EESEN [14] routines.", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "The features were generated by preprocessing routines from EESEN [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "In order to automatically learn the alignments between speech frames and label sequences, the Connectionist Temporal Classification (CTC) [22] objective was adopted.", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "The Lasagne library [23] enabled us to build and train our 5-layer neural network.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "The first 4 layers consisted of bidirectional long short-term memory (LSTM) [24] units with 250 units in each direction.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "All layers were initialized with the Glorot uniform strategy [25].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "During training, the Adam [26] stochastic optimization method was used.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "To prevent overfitting and to increase noise robustness, dropout [13] was used (dropout probability=0.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "5M parameter network, while in literature [27], a 13.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "This is expected, as the mean SNR of the training SNR range is 25dB and the network seems to optimize for SNR levels close to this value [16].", "startOffset": 137, "endOffset": 141}], "year": 2016, "abstractText": "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a conventional multi-condition training method.", "creator": "LaTeX with hyperref package"}}}