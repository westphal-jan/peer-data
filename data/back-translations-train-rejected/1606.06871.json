{"id": "1606.06871", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic Modeling in Speech Recognition", "abstract": "We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniques such as dropout and $L_2$ regularization, and different gradient clipping variants.", "histories": [["v1", "Wed, 22 Jun 2016 10:00:14 GMT  (38kb,D)", "http://arxiv.org/abs/1606.06871v1", null], ["v2", "Wed, 29 Mar 2017 08:08:29 GMT  (30kb,D)", "http://arxiv.org/abs/1606.06871v2", "published on ICASSP 2017 conference, New Orleans, USA"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG cs.SD", "authors": ["albert zeyer", "patrick doetsch", "paul voigtlaender", "ralf schl\\\"uter", "hermann ney"], "accepted": false, "id": "1606.06871"}, "pdf": {"name": "1606.06871.pdf", "metadata": {"source": "CRF", "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic Modeling in Speech Recognition", "authors": ["Albert Zeyer", "Patrick Doetsch", "Paul Voigtlaender", "Ralf Schl\u00fcter", "Hermann Ney"], "emails": ["ney}@cs.rwth-aachen.de"], "sections": [{"heading": null, "text": "We present a comprehensive study of deep bidirectional long-term short-term memory (LSTM) based on recursive neural networks (RNN) for automatic speech recognition (ASR). We investigate the effect of size and depth and traction models of up to 8 layers. We investigate the training aspect and investigate various variants of optimization methods, batching, truncated backpropagation, various control techniques such as failure and L2 regulation and various gradient clipping variants. Most of the experimental analysis was performed on the Quaero corpus. Further experiments were also performed on the switchboard corpus. Our best LSTM model shows a relative improvement in word error rate of over 14% compared to our best feed-forward neural network (FFNN) baseline on the Quaero task. In this task, we obtain our best result with an 8-layer LM-STM model and we show that we trace the ASR to a deeper structural pattern."}, {"heading": "1. Introduction", "text": "The class of recursive neural networks (RNN), and in particular long-term short-term memory networks (LSTM) [2], behaves very well when dealing with temporal sequences as well as linguistic usage. Recently, LSTM-based acoustic models (AM) have been shown to outperform FFNs when it comes to detecting large vocabulary (LVCSR) [3, 4]. There are many aspects to consider when training LSTMs that we are studying in this paper. Our experiments show that there is enormous variation in recognition performance depending on the various aspects. Compared to our best FFNN baseline, we achieve a relative improvement in word error rate (WHO) of over 14%. In addition, we train deep LSTM networks with up to 8 layers of acoustic modelling and we discovered that a training program can improve performance for deeper LSTMs."}, {"heading": "2. Related work", "text": "An early hybrid LSTM-HMM was presented in [6] for TIMIT. [3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with wave or feed layers for acoustic modelling in ASR. Some LSTM-related models such as the Gated Recurrent Unit (GRU) have been investigated in [13, 14, 15, 16]."}, {"heading": "3. LSTM Model and Implementation", "text": "We use the standard LSTM model with no peephole connections [17]. Our basic tool is the RASR speech recognition toolkit [18, 19]. We use RASR for the feature extraction pipeline and for decoding. We have added a Python bridge to RASR to enable many types of interaction with external tools. This Python bridge was introduced to be able to use RETURNN, our theano-based framework [20, 21], to perform training and forwarding in recognition of our acoustic model.In RETURNN we have several LSTM implementations and it supports all aspects we discuss in this essay."}, {"heading": "4. Comparisons", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Corpus", "text": "We use a subset of 50 hours from the Quaero Broadcast Conversational English Speech Database train11 [22]. Development evaluation 10 and evaluation evaluation evaluation 11 each consist of approximately 3.5 hours of speech."}, {"heading": "4.2. Baseline", "text": "ieD eeisrcnh-eaJnlhsrteeaeaJnlhc nvo eeisn rf\u00fc ide eeisrcnlhsrtee\u00fccnlhSe ni red eeisn-eaJnlhsrteeaJnlrh-eaeaJnlrh-eaJnlrh-eaJngr-eaJnlhsrsrsrteeeaeaeaeaeaeaeetnln-eaHnlrrsrteeaeaeaeaeaeoioioiKn-nlrcnlhc-eoiaeSrlhc-eSrf\u00fc-eSrlrlrteeoi.nlrf\u00fc"}, {"heading": "4.3. Number of Layers", "text": "We have conducted several experiments to determine the optimal number of layers. Theoretically, more layers should not hurt, but in practice they often do so because the optimization problem becomes more difficult, which could be overcome by clever initializations, skipped connections, highway-like structures [25, 12] or deep residual learning [26]. We have conducted some initial experiments in this direction as well, but so far we have not been successful. Existing work in this direction is also largely for deep FFNNs and not for deep RNNs, except for [12]. The results can be seen in Table 1. In this experiment, the optimum is somewhere between 4 to 6 layers. In previous experiments, the optimum was about 3 to 4 layers. It seems that the more we improve other hyperparameters, the deeper the optimal network becomes. With pre-training as in Section 4.9, we get our overall best result, as described in Section 4.10, with 8 layers, but we have not examined the same setting for different layers."}, {"heading": "4.4. Layer Size", "text": "In most experiments, we use a hidden layer size of 500 (i.e. 500 knots for both forward and backward directions).In Table 2, we compare different layer sizes. Note that the number of parameters increases quadratically. We see that the optimum for this experiment is about 700, but a model of size 500 is much smaller and not so much worse, so we have used this size for most other experiments."}, {"heading": "4.5. Topology: Bidirectional vs. Unidirectional", "text": "Our initial experiment showed that we achieved quite a large WER degradation (over 20% relative) with unidirectional LSTM networks compared to bidirectional networks. This enormous WER degradation led to further research in which we investigated how we could use bidirectional RNs / LSTMs on a continuous input sequence for online detection. We demonstrated that this is possible and that with a certain detection delay we can achieve the original WER. These results are described in [27]."}, {"heading": "4.6. Batching", "text": "All experiments were conducted at the same initial learning rate. We conducted many experiments with different nchunks and obtained the best results with nchunks \u2248 40. In some experiments, the performance difference was with nchunks = 40 compared to nchunks = 20. This could be due to better variance and thus more stable gradients for each minibatch. Note that a higher nchunks is usually also faster up to a point, since the GPU can work on each chunk in parallel. We usually use T = 50. We have done many experiments with fixed T \u2212 tstep = 25, but we often see a slight deterioration when T \u2265 100 occurs. This could be because the problem is more difficult to train over time due to the longer backflow, but maybe we need to adjust the learning rate or change other parameters so that the difference does not get much longer, unless we have the data for a larger step."}, {"heading": "4.7. Optimization Methods", "text": "We compare many optimization methods and variants between the individual countries."}, {"heading": "4.8. Regularization Methods", "text": "We have done many dropout experiments where we drop some nodes into forward connections. Mostly, we see the optimal WHO with dropout 0.1, i.e. we drop 10% of the activations and multiply them by 109. If we increase the hidden layer size, we can use higher dropout values, although dropout 0.2 was worse than dropout 0.1 in most experiments. In initial experiments, dropout was always better than L2 regulation. However, some later experiments showed that L2 can also work as an alternative. Interestingly, the combination of both results in a big improvement and the best result. See Table 4."}, {"heading": "4.9. Initialization and Pretraining", "text": "In all cases, we randomly initialize the parameters as proposed in [38]. We examined the same pretraining scheme as in our FFNN, where we start with one layer and add one layer after each epoch. We can then either train only the new layer (greedy) or the whole network, where full network training was usually better. In our initial experiments with less optimal hyperparameters and less deep networks, this pretraining scheme performed worse than not doing pretraining. In a later experiment with 5 layers, Drop-out + L2 and Adam, we achieved a slight improvement from 13.6% WHO to 13.5% WER.For deeper networks, this scheme seems to be more helpful, suggesting that our initialization could have room for improvement. Overall, our best result was achieved with such a pretraining scheme applied to an 8-layer bi-directional LSTM. Also, the training calculation time of the first epochs is shorter."}, {"heading": "4.10. Overall Best Model", "text": "We have tried many combinations: our overall best model is an 8-layer bi-directional LSTM with 500 nodes, drop-out 0.1 and L2 10 \u2212 2, nchunks = 40, T = 50, gradient noise 0.3, nadam, no gradient section and the pre-training scheme as described in Section 4.9. This yields a WER of 13.1% at Evaluation 10 and 17.6% at Evaluation 11. We believe this pre-training was the most important aspect in this experiment, as the previous results in Section 4.3 did not provide good results for such deep networks."}, {"heading": "4.11. Calculation Time vs. WER", "text": "Most of the experiments were carried out with a GeForce GTX 980. We see that the Tesla K20c with a standard deviation of 0.084 is about 1.38 times slower, and the GeForce GTX 680 with a standard deviation of 0.764 is about 1.86 times slower. We present the pure computation times of the railway era with a GeForce GTX 980, without taking into account the CV test and other epoch preparations. Some of the total times we have collected in Table 5. This is the summed railway era to reach the specific epoch. We show the model with the best WHO to the specific time. We see that in most cases combinations of different hyperparameters and methods produce the best results.Time downsampling was a simple method to reduce the computation time with power as a trade-off."}, {"heading": "4.12. Experiments on other Corpora", "text": "We use the 300h Switchboard-1 Release 2 (LDC97S62) corpus for training, and the Hub5 '00 evaluation data (LDC2002S09) is used for testing. We use a 4 gram language model trained on the 3M running words transcripts and the Fisher English cor-pora (LDC2004T19 & LDC2005T19) transcripts with 22 M running words. See [39] for more details. A good FFNN baseline yields a total WHO of 19.1% (13.1% WHO on SWB, 25.6% WHO on CH). We trained a 5-shift bidir. LSTM with nadam, gradient noise, drop-out + L2 and get a total WHO of 17.1% (11.9% WHO on SWB, 22.3% WHO on CH)."}, {"heading": "5. Conclusions & Outlook", "text": "We outlined optimal LSTM hyperparameters such as network depth and size, different training, optimization and regulation methods, and demonstrated that individual improvement outcomes can be combined and added up. We demonstrated that with these results we can reproduce good results across different corpora and tasks, and achieve very good overall results that are relatively superior to our best FFNN on Quaero by over 14%. We also demonstrated how we can train deeper LSTM acoustic models with up to 8 shifts, which is more than what was previously reported in the literature."}, {"heading": "6. Acknowledgements", "text": "Partially supported by the Intelligence Advanced Research Projects Activity (IARPA) through the Department of Defense U.S. Army Research Laboratory (DoD / ARL) under contract number W911NF12-C-0012. The U.S. Government is authorized to reproduce and distribute copies for government purposes, regardless of copyright notices contained therein. Disclaimer: The views and conclusions contained herein are those of the authors and should not necessarily be interpreted to represent the official policies or notices of IARPA, DoD / ARL or the U.S. Government. This research was partially supported by the Ford Motor Company."}, {"heading": "7. References", "text": "[1] J. Schmidhuber, \"Deep learning in neural short-term memory: Anoverview,\" Neural Networks, vol. 61, pp. 85-117, 2015, online 2014; based on TR arXiv: 1404.7828 [http: / / arxiv.org / pdf / 1402.1128]. [2] S. Hochreiter and J. Schmidhuber, \"Long short-term memory based,\" Long short-term memory architecture for large vocabulary speech recognition, \"arXiv preprint arXiv arXiv arXiv, arxiv.org / pdf / 1402.1128. [4] J. T. Geiger, Z. Zhang, F. Weninger, B. Schuller, and G. Rigoll.\""}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85\u2013117, 2015, published online 2014; based on TR arXiv:1404.7828 [cs.NE].", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "arXiv preprint arXiv:1402.1128, 2014, http: //arxiv.org/pdf/1402.1128.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling", "author": ["J.T. Geiger", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "INTER- SPEECH, 2014, pp. 631\u2013635.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298\u2013305, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278, http://www.cs.toronto.edu/\u223cgraves/ asru 2013.pdf.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4520\u2013 4524.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving long short-term memory networks using maxout units for large vocabulary speech recognition", "author": ["\u2014\u2014"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4600\u20134604.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent neural networks for acoustic modelling", "author": ["W. Chan", "I. Lane"], "venue": "arXiv preprint arXiv:1504.01482, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Context dependent phone models for LSTM RNN acoustic modelling", "author": ["A. Senior", "H. Sak", "I. Shafran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4585\u20134589.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway long short-term memory RNNs for distant speech recognition", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yao", "S. Khudanpur", "J. Glass"], "venue": "arXiv preprint arXiv:1510.08983, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1412.3555, 2014. [Online]. Available: http://arxiv.org/abs/1412.3555", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML- 15), 2015, pp. 2342\u20132350.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking of LSTM networks", "author": ["T.M. Breuel"], "venue": "arXiv preprint arXiv:1508.02774, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "RASR the RWTH Aachen university open source speech recognition toolkit", "author": ["D. Rybach", "S. Hahn", "P. Lehnen", "D. Nolden", "M. Sundermeyer", "Z. T\u00fcske", "S. Wiesler", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, Waikoloa, HI, USA, Dec. 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "RASR/NN: The RWTH neural network toolkit for speech recognition", "author": ["S. Wiesler", "A. Richard", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, Florence, Italy, May 2014, pp. 3313\u20133317.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "RETURNN: The RWTH extensible training framework for universal recurrent neural networks", "author": ["P. Doetsch", "A. Zeyer", "P. Voigtlaender", "I. Kulikov", "R. Schl\u00fcter", "H. Ney"], "venue": "work in preparation, can be requested from the authors, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "The RWTH 2009 Quaero ASR evaluation system for English and German", "author": ["M. Nu\u00dfbaum-Thom", "S. Wiesler", "M. Sundermeyer", "C. Plahl", "S. Hahn", "R. Schl\u00fcter", "H. Ney"], "venue": "Interspeech, Makuhari, Japan, Sep. 2010, pp. 1517\u20131520.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Connectionist speech recognition: a hybrid approach", "author": ["H.A. Bourlard", "N. Morgan"], "venue": "http://publications. idiap.ch/downloads/papers/2013/Bourlard KLUWER 1994.pdf", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "Gammatone features and feature combination for large vocabulary speech recognition", "author": ["R. Schl\u00fcter", "L. Bezrukov", "H. Wagner", "H. Ney"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013649.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2368\u20132376.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards online-recognition with deep bidirectional LSTM acoustic models", "author": ["A. Zeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "Interspeech, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 4, no. 5, pp. 1\u201317, 1964.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1964}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13), 2013, pp. 1139\u20131147.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, vol. 27, no. 2, 1983, pp. 372\u2013376.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1983}, {"title": "Meannormalized stochastic gradient for large-scale deep learning", "author": ["S. Wiesler", "A. Richard", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, Florence, Italy, May 2014, pp. 180\u2013184.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012, http://arxiv.org/abs/1212.5701.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011, http: //dl.acm.org/citation.cfm?id=2021068.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating Nesterov momentum into Adam", "author": ["T. Dozat"], "venue": "Stanford University, Tech. Rep., 2015, http://cs229.stanford.edu/ proj2015/054 report.pdf.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A. Neelakantan", "L. Vilnis", "Q.V. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "ArXiv preprint arXiv:1511.06807, Nov. 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Adasecant: robust adaptive secant method for stochastic gradient", "author": ["C. Gulcehre", "M. Moczulski", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7419, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 249\u2013256, http: //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Speaker adaptive joint training of gaussian mixture models and bottleneck features", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, Scottsdale, AZ, USA, Dec. 2015, pp. 596\u2013603.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Associative long short-term memory", "author": ["I. Danihelka", "G. Wayne", "B. Uria", "N. Kalchbrenner", "A. Graves"], "venue": "arXiv preprint arXiv:1602.03032, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Multilingual features based keyword search for very low-resource languages", "author": ["P. Golik", "Z. T\u00fcske", "R. Schl\u00fcter", "H. Ney"], "venue": "Interspeech, Dresden, Germany, Sep. 2015, pp. 1260\u20131264.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNN) yield state-of-the-art performance in classification in many machine learning tasks [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "The class of recurrent neural networks (RNN) and especially long shortterm memory (LSTM) networks [2] perform very well when dealing with temporal sequences as in speech.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Only recently, it has been shown that LSTM based acoustic models (AM) outperform FFNNs on large vocabulary continuous speech recognition (LVCSR) [3, 4].", "startOffset": 145, "endOffset": 151}, {"referenceID": 3, "context": "Only recently, it has been shown that LSTM based acoustic models (AM) outperform FFNNs on large vocabulary continuous speech recognition (LVCSR) [3, 4].", "startOffset": 145, "endOffset": 151}, {"referenceID": 4, "context": "Hybrid RNN-HMM models were developed in 1994 in [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "One early hybrid LSTM-HMM was presented in [6] for TIMIT.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 8, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 10, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "[3, 4, 7, 8, 9, 10, 11, 12] investigate various bidirectional and unidirectional LSTM topologies with optional projection in some cases combined with convolutional or feed-forward layers for acoustic modeling in ASR.", "startOffset": 0, "endOffset": 27}, {"referenceID": 12, "context": "Some LSTM related models like the Gated Recurrent Unit (GRU) were studied in [13, 14, 15, 16].", "startOffset": 77, "endOffset": 93}, {"referenceID": 13, "context": "Some LSTM related models like the Gated Recurrent Unit (GRU) were studied in [13, 14, 15, 16].", "startOffset": 77, "endOffset": 93}, {"referenceID": 14, "context": "Some LSTM related models like the Gated Recurrent Unit (GRU) were studied in [13, 14, 15, 16].", "startOffset": 77, "endOffset": 93}, {"referenceID": 15, "context": "Some LSTM related models like the Gated Recurrent Unit (GRU) were studied in [13, 14, 15, 16].", "startOffset": 77, "endOffset": 93}, {"referenceID": 16, "context": "We use the standard LSTM model without peephole connections [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "Our base tool is the RASR speech recognition toolkit [18, 19].", "startOffset": 53, "endOffset": 61}, {"referenceID": 18, "context": "Our base tool is the RASR speech recognition toolkit [18, 19].", "startOffset": 53, "endOffset": 61}, {"referenceID": 19, "context": "This Python bridge was introduced to be able to use RETURNN, our Theano-based framework [20, 21] to do the training and forwarding in recognition of our acoustic model.", "startOffset": 88, "endOffset": 96}, {"referenceID": 20, "context": "This Python bridge was introduced to be able to use RETURNN, our Theano-based framework [20, 21] to do the training and forwarding in recognition of our acoustic model.", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "We provide more details about this software in [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "We use a subset of 50 hours from the Quaero Broadcast Conversational English Speech database train11 [22].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "We use the common NN-HMM hybrid acoustic model [23].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "The input features are 50-dimensional VTLN-normalized Gammatone [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "[7] and described in detail in [20].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[7] and described in detail in [20].", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "This could be overcome with clever initializations, skip connections, highway network like structures [25, 12] or deep residual learning [26].", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "This could be overcome with clever initializations, skip connections, highway network like structures [25, 12] or deep residual learning [26].", "startOffset": 102, "endOffset": 110}, {"referenceID": 25, "context": "This could be overcome with clever initializations, skip connections, highway network like structures [25, 12] or deep residual learning [26].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "The existing work in that direction is also mostly for deep FFNNs and not for deep RNNs except for [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "We observe similar results as in [26], i.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "These results are described in [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 64, "endOffset": 72}, {"referenceID": 28, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 64, "endOffset": 72}, {"referenceID": 29, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 215, "endOffset": 223}, {"referenceID": 28, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 215, "endOffset": 223}, {"referenceID": 30, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 253, "endOffset": 257}, {"referenceID": 31, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 268, "endOffset": 272}, {"referenceID": 32, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 282, "endOffset": 286}, {"referenceID": 33, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 304, "endOffset": 308}, {"referenceID": 34, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 402, "endOffset": 406}, {"referenceID": 35, "context": "We compare stochastic gradient descent (SGD), SGD with momentum [28, 29] where one variant only depends on the last minibatch (mom) and another variant depends on the full history (mom2), SGD with Nesterov momentum [30, 29], mean-normalized SGD (MNSGD) [31], Adadelta [32], Adagrad [33], Adam and Adamax [34], Adam without the learning rate decay term, Nadam (Adam with incorporated Nesterov momentum) [35], Adam with gradient noise [36], and Adam with MNSGD combined.", "startOffset": 433, "endOffset": 437}, {"referenceID": 36, "context": "We also tried Adasecant [37] but it did not converge in any of our experiments for this ASR task.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "This is similar to the multi-GPU training behavior described in [20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 37, "context": "In all cases, we randomly initialize the parameters similar as suggested in [38].", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "More details can be found in [39].", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "When we add an associative LSTM [40] layer instead, we get a total WER of 16.", "startOffset": 32, "endOffset": 36}, {"referenceID": 40, "context": "0b) which is a keyword-search (KWS) task (see [41] for all details).", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniques such as dropout and L2 regularization, and different gradient clipping variants. The major part of the experimental analysis was performed on the Quaero corpus. Additional experiments also were performed on the Switchboard corpus. Our best LSTM model has a relative improvement in word error rate of over 14% compared to our best feed-forward neural network (FFNN) baseline on the Quaero task. On this task, we get our best result with an 8 layer bidirectional LSTM and we show that a pretraining scheme with layer-wise construction helps for deep LSTMs. Finally we compare the training calculation time of many of the presented experiments in relation with recognition performance. All the experiments were done with RETURNN, the RWTH extensible training framework for universal recurrent neural networks in combination with RASR, the RWTH ASR toolkit.", "creator": "LaTeX with hyperref package"}}}