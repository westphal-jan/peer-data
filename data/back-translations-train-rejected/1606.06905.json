{"id": "1606.06905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Learning text representation using recurrent convolutional neural network with highway layers", "abstract": "Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.", "histories": [["v1", "Wed, 22 Jun 2016 11:30:47 GMT  (78kb,D)", "http://arxiv.org/abs/1606.06905v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval"], ["v2", "Tue, 2 Aug 2016 16:17:05 GMT  (79kb,D)", "http://arxiv.org/abs/1606.06905v2", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ying wen", "weinan zhang", "rui luo", "jun wang"], "accepted": false, "id": "1606.06905"}, "pdf": {"name": "1606.06905.pdf", "metadata": {"source": "CRF", "title": "Learning text representation using recurrent convolutional neural network with highway layers", "authors": ["Ying Wen", "Weinan Zhang", "Rui Luo", "Jun Wang"], "emails": ["j.wang}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "Tags Neural Networks, Highway Networks, Sentiment Analysis"}, {"heading": "1. INTRODUCTION", "text": "It is important that this type of information processing (IR) and natural language processing (NLP) tasks such as text classification and sensation analysis have attracted considerable attention from academic and industrial communities, not only in text representation, which limits the functionality of learning models based on such representations, but also in education. In recent years, the word embedding and n-gram has brought new solutions to learn better representations for NLP and IR tasks. The simplest is Bagof-Word-Vectors (BOW vector) model. But Landauer et al estimates that 20% of the meaning of a text composition is digital copies or hard copies or all of these copies of classwork."}, {"heading": "2. MODEL", "text": "In this section, we propose a staggered hybrid model that combines RCNN with motorway layers as shown in Figure 1. As shown, the motorway network module in the middle takes over the output of the bidirectional RNN module in the first stage and provides the CNN module with the input in the last stage."}, {"heading": "2.1 Recurrent Neural Networks", "text": "It is quite intuitive and easy to adopt RNN for learning sequential data due to its nature: RNN is able to process current instances of data, taking into account historical information obtained as if it had memory. These approaches to improving long-term memory include Long Short-Term Memory (LSTM) [7] and its variants, which are of particular interest in practice due to their great success. For example, we use a newly proposed variant, the Gated Recurrent Unit (GRU) [4], for its simplicity and competitiveness. The expressions of GRU are given as follows: rt = \u03c3 (Wrxt + Urht \u2212 1 + br) zt = \u03c3 (Wzxt + Uzht \u2212 1 + bz) h = tanh (Whxt + Uh (rt \u2212 1) + bh = tanh (rt \u2212 1) h (Whxt + Uh \u2212 1) h = tanh (rt \u2212 1) h = bixt = Uh = Uh \u2212 1) and \u2212 1 txt = Whxt = 1 txt = 1 txt = 1 txt = 1 txt = 1 txt = 1."}, {"heading": "2.2 Highway Networks", "text": "A single-layer motorway network is introduced as an intermediate layer between the bi-directional RNN and CNN to select the characteristics for each word representation individually, and the motorway layer is expressed as follows: yt = \u03c4 g (WH x \u0435t + bH) + (1 \u2212 \u03c4) x \u0445 t (3), where g is a non-linear function and \u03c4 = \u03c3 (W\u03c4xt + b\u03c4) is the \"transformation gate.\" Note that the design of the motorway link is quite similar to that of the GRU's \"update gate,\" which is essentially a variant of the \"leaky integration\" [1], which allows some of the input information to be transferred to the output unchanged while the rest undergoes some (non-linear) transformations. Experiments suggest that this type of structure will help to extract essential information."}, {"heading": "2.3 Convolutional Neural Networks", "text": "A CNN architecture generally consists of a stack of different layers mapping the input to the output via a piecemeal differentiating function. In order to extract new features from the input (phrases / sentences in our case), a revolutionary layer essentially applies a set of learnable filters to the input, which have small receptive fields: they are in fact a series of masks with different window sizes h, i.e. they select h consecutive words for the neurons in the layer. For example, the ci feature is selected from a window of word representations with size h: yi: i + h \u2212 1 according to the expression: ci = f (wconv \u00b7 yi: i + h \u2212 1 + bconv) (4), where f is a non-linear function, which in our case is the \"reflected linear unit.\""}, {"heading": "3. EXPERIMENT", "text": "To demonstrate the effectiveness of the proposed model, we conduct the experiment with the IMDB dataset [14], which contains 25,000 film reviews for the training. Each review is marked by a human with 1, which represents a positive feeling, and 0 represents a negative feeling. The review in the dataset has an average of 267.9 words, and the standard deviation in the review length is 198.8. To test the performance of the learned model text representation, we perform sensation predictions on this dataset and report the accuracy evaluation metric. In addition, we are particularly interested in the relationship between the input sequence length and performance of the model and set up the experiment to test the performance of the model across different input sequence sequence lengths. We compare our model RCNN with sequence sequence lengths (RCNNHW) with the following neural network models for prediction at the document level: \u2022 Worum-Training-Sector (COM-SW) (SCNNHW)."}, {"heading": "4. RESULTS AND DISCUSSION", "text": "Experimental results can be found in Table 1. We compare our RCNN with motorway layers with original RCNN models and find that performance with motorway layers is always better than without. To quantify the effects of the motorway network, we set up an experiment with RCNN-based models. We train the CNN model without motorway layers, with one layer of motorway, two layers of motorway and one layer of MLP each with the same parameters, then we compare their accuracy. We can find that it is important to have one to two layers of motorway, but more motorway layers do not improve performance. Furthermore, an MLP layer does not gain more improvements than higway layers. We think that the motorway layer works because the properties of word representation help select the characteristics of the word representation. We also compare with CNN, RNN and COW models. Empirical experiments show that CNN models generally perform better than higway layers."}, {"heading": "5. CONCLUSION", "text": "The experiment shows that our model RCNN-HW not only performs better than CNN and RNN models, but also performs better than the RCNN without a highway layer. Since our model also performs better with longer text, it would be interesting to see if the model presented in this paper works well in learning how to render text for the other NLP and IR tasks with relatively long documents."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "The authors would like to thank the anonymous reviewers for their helpful feedback and suggestions."}, {"heading": "7. REFERENCES", "text": "[1] Y. Bengio, N. Boulanger-Lewandowski, andR. Pascanu. Advances in Optiming recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8624-8628. IEEE, 2013. [2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35 (8): 1798-1828, 2013. [3] Y. Bengio, H. Schwenk, J.-S. Sene, F. Xihre Xihre. Xihre."}], "references": [{"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Search engines: Information retrieval in practice, volume 283", "author": ["W.B. Croft", "D. Metzler", "T. Strohman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "On the computational basis of learning and cognition: Arguments from lsa", "author": ["T.K. Landauer"], "venue": "Psychology of learning and motivation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1929}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Learning good representations for text, such as words, sentences and documents, is essential for information retrieval (IR) and natural language processing (NLP) tasks like text classification and sentiment analysis, which has attracted considerable attention from both academic and industrial communities [2].", "startOffset": 306, "endOffset": 309}, {"referenceID": 5, "context": "It is common to use bag-of-words or bag-of-ngram to represent text [6] and train the models based on such representation.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "[13] estimates that 20% of the meaning of a text comes", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Neural language model [3] was then proposed to leverage word embedding representation to infer the next-word distribution, but it still fails to fully utilize the sequence of the context words.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "CNN treats each words fairly by using the max-pooling layer [9].", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "In addition, CNN is comparatively simple, efficient and has achieved strong empirical performance on the text classification jobs [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "2015 proposed Recurrent Convolutional Neural Network (RCNN) [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "Among those approaches for enhancement of the long-term memory, Long Short-Term Memory (LSTM) [7] and its variants are of particular interest due to the great success in practice.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Thus we use as the building block of our network a newly proposed variant, Gated Recurrent Unit (GRU) [4], for its simplicity and competitive performance.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "Note that the design of highway connection is quite similar with that of GRU\u2019s \u201cupdate gate\u201d z, which is essentially a variant of \u201cleaky integration\u201d [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 13, "context": "To demonstrate the effectiveness of the proposed model, we perform the experiment on IMDB dataset [14] which contains 25,000 movie reviews for training.", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "\u2022 LSTM: takes the average of the LSTM\u2019s hidden states of all words is used as text representation [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 15, "context": "\u2022 Bi-LSTM: similar to LSTM but exploits bidirectional LSTM [16] on the sequence to get text representation.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "\u2022 CNN: performs 1-demensional convolution followed by 1-demensional max-pooling with multiple filters on the sequence [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 9, "context": "\u2022 CNN: performs 1-demensional convolution followed by 1-demensional max-pooling with multiple filters on the sequence [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 11, "context": "\u2022 RCNN: uses bidirectional RNN\u2019s output as CNN\u2019s input [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "adam, RMSprop, adadelta) [11].", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Note, we did not apply pre-training methods like using pre-trained word2vec [15], Table 1: Accuracy for different neural models on IMDB dataset Model Accuracy", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "and regularizer such as Dropout [17] which may bring further improvement [10].", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "and regularizer such as Dropout [17] which may bring further improvement [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 8, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 9, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 17, "context": "CNN models usually perform remarkably well on many NLP and IR tasks [8, 9, 10, 18].", "startOffset": 68, "endOffset": 82}, {"referenceID": 4, "context": "A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity [5, 12].", "startOffset": 117, "endOffset": 124}, {"referenceID": 11, "context": "A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity [5, 12].", "startOffset": 117, "endOffset": 124}], "year": 2017, "abstractText": "Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bidirectional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.", "creator": "LaTeX with hyperref package"}}}