{"id": "1606.06950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "abstract": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units---effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on large-vocabulary multi-speaker data. The system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). We show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system's discovered clusters are still less pure than those of two multi-speaker term discovery systems, but provide far greater coverage.", "histories": [["v1", "Wed, 22 Jun 2016 13:51:57 GMT  (1022kb,D)", "http://arxiv.org/abs/1606.06950v1", "13 pages, 6 figures, 8 tables"], ["v2", "Sat, 16 Sep 2017 09:36:02 GMT  (1026kb,D)", "http://arxiv.org/abs/1606.06950v2", "15 pages, 6 figures, 8 tables"]], "COMMENTS": "13 pages, 6 figures, 8 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["herman kamper", "aren jansen", "sharon goldwater"], "accepted": false, "id": "1606.06950"}, "pdf": {"name": "1606.06950.pdf", "metadata": {"source": "CRF", "title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "authors": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "emails": ["kamperh@gmail.com,", "arenjansen@google.com,", "sgwater@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Zero Resource Language Technology is a growing area of research that aims to develop methods for language processing in the absence of transcriptions, lexicographs or speech modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while newer full coverage systems attempt to fully segment the audio and group it into word-like units - effectively performing unattended speech recognition. To our knowledge, this article presents the first such system to be evaluated using extensive vector data from multiple speakers. We compare our system with English and Xitsonga datasets with segmental word representations: each word segment is presented as a fixed-dimensional acoustic embedding achieved by mapping the sequence of functional frames to a single embedding vector."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Related work", "text": "In the following, we first discuss related work on unattended presentation learning, followed by unattended terminology (with which we also compare our approach), and finally comprehensive segmentation and clustering of unlabeled language."}, {"heading": "2.1. Unsupervised frame-level representation learning", "text": "In this context, the goal is to find a framework condition that ranges from input functions to a new representation that makes it easier to distinguish between different linguistic units (usually subwords or words). Early studies used bottom-up approaches based directly on acoustics. Zhang and Glass [9] successfully used posteriorgraphic characteristics from an unattended universal background model (UBM) to query for examples and term discoveries. Similarly, they used posteriorgraphic diagrams from a non-parameter Infinite GMM."}, {"heading": "2.2. Unsupervised term discovery", "text": "Most state-of-the-art UTD systems use a variant of dynamic time warping (DTW), known as segmental DTW. This algorithm, developed by Park and Glass [1], identifies similar subsequences within two vector time series, rather than comparing whole sequences as in standard DTW. In most UTD systems, segmental DTW pairs of matching segments are proposed, which are then clustered using a graph-based method. Subsequent work has built on the original Park and Glass method in various ways, for example, through improved functional representations [16] or significant efficiency improvements [18]. The baseline provided in the Zero Resource Speech Challenge 2015 (ZRS) lexical discovery track also provides word coverage that builds on the earlier work [18]."}, {"heading": "2.3. Full-coverage segmentation and clustering of speech", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to fight, to move, to fight, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "3. Large-vocabulary segmental Bayesian model", "text": "In the following, we describe our large vocabulary system in detail, starting with a high-level overview of the model, illustrated in Figure 1. The model takes raw language as input (below) and converts it into acoustic characteristics at frame level, using a sliding window that feeds into function fa. The sequence of frame-level vectors (e.g. MFCCs or cAE characteristics) is then referred to as y1: M = y1, y2,.. yM. Suppose we have a hypothesis where word boundaries occur in this stream of characteristics (vertical black lines, below the figure). Each Word1 segment is then assigned to an acoustic word embedding model (colored horizontal vectors in the figure) in a fixed-dimensional word RD; this is done using the embedding function fe, which demonstrates a sequence of frame-level characteristics as input and output of a single word embedding RD."}, {"heading": "3.1. Segmental Bayesian modelling", "text": "Given the embedded word vectors X = {xi} Ni = 1 from the current segmentation hypothesis = \u03b2 = realistic use of word components, the acoustic model would assign each acoustic word to one of the K clusters, with each cluster corresponding to a hypothetical word type. We use a Bayesian GMM as an acoustic model, with a conjugated funnel before its blending weighting and a conjugated diagonal covariance Gaussian means before its component {\u00b5k} Kk = 1, which allows us to integrate these parameters. The model shown in Figure 2 is formally defined as: sp."}, {"heading": "3.2. Unsupervised syllable boundary detection", "text": "Without limitations, the input at the bottom of Figure 1 could be segmented into any number of possible words by a huge number of possible segmentations. [24] Potential word segments therefore had to be between 200 ms and 1 s long, and word boundaries were considered only at intervals of 20 ms, which still leads to a very large number of possible segments. Instead, we use a method of detecting the syllable boundary to eliminate improbable word boundaries, with word candidates comprising a maximum of six syllables. [23] On the waveform in Figure 1, fixed and dashed lines are used to indicate the only positions where boundaries are taken into account in scanning as determined by the syllable ification method. Ra'sa'nen et al. evaluated several syllable level boundary detection algorithms, and we use the best of them. First, the shell of the raw waveform is calculated by scanning down the reflected signal and applying a low pass filter."}, {"heading": "3.3. Acoustic word embeddings and unsupervised representation learning", "text": "A similar approach is to divide a segment into a fixed number of intervals and intersect the frames at each interval [23,48], and then flatten the sampled or averaged frames to obtain a single vector of fixed length. Although these very simple approaches to word discrimination are less precise than the one previously used in [24], they have been used effectively in several studies, including [23], and are much more efficient. Here, we use downsampling than our acoustic word embedding function fe in Figure 1; we keep ten even vectors from a segment and use a Fourier-based method for smoothing [25]. Figure 1 shows that fe is used as input a sequence of framellevel characteristics from the fa function."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental setup", "text": "We use three sets of data, summarised in Table 1. The first two are fragmented subsets extracted from the Buckeye corpus of conversational English [49], while the third part is part of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa. [27] Xitsonga is a Bantu language spoken in South Africa; although it is considered underfunded, more than five million people use it as their first language. 2The two sentences extracted from Buckeye are referred to as English1 and English2, each containing five and six hours of speech, each by twelve speakers (six female and six male). The Xitsonga data set consists of 2.5 hours of speech by 24 speakers (twelve female and twelve male)."}, {"heading": "4.2. Evaluation", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, live, in which they, in which they, in fact, in which they, in fact, in which they, in which they, in fact, in which they, in fact, are able to live, are able to move, in which they, in which they, are able to move, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which"}, {"heading": "4.3. Model development and hyperparameters", "text": "Most of the hyperparameters are set after previous work. All changes are based exclusively on performance on English1.Training parameters for the cAE (Section 3.3) are based on [14, 26]. The model is based on all data (in a particular set) for 5 epochs with minibatch stochastic gradient descent with a batch size of 2048 and a fixed learning rate of 2, 10 \u2212 3. The following correspondence trainings are performed for 120 epochs with a learning rate of 32, 10 \u2212 3. Each pair is presented in both directions as input and output. Pairs are extracted using the UTD system of [18]: for English1, 14,494 word pairs are discovered; for English2, 10,769 pairs; and for Xitsonga, 6979. The cAE is trained separately on each of these sets. In all cases, the model consists of nine hidden layers of each, except for the eighth layer which is a tleneck of 13 units."}, {"heading": "4.4. Results: Word error rates and analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-dependent models", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" It's as if we are able to hide, \"he said."}, {"heading": "Speaker-independent models", "text": "Compared to the loudspeaker results in Table 2, the performance for all models and data sets is worse. In the loudspeaker experiments, some MFCC-based models have slightly outperformed their cAE counterparts, but in this case the WERS of the cAE models are identical or improved in all cases. Xitsonga in particular has seen improvements through the use of cAE features in both BayesSeg (improvement of 26.3% in WER) and BayesMinDur (7.4%)."}, {"heading": "Qualitative analysis and summary", "text": "Apart from the Embeds. Clust. Spk. Gndr Clust. Spk. GndrMFCC 30.3 56.7 86.9 55.6 24.5 43.1 cAE 31.5 37.9 77.0 30.0 35.7 73.8 76.6Trends already mentioned, others were also immediately apparent. Despite the low average cluster purity of 30% to 60% in the analyses above, we found that most of the clusters are acoustically very pure: often tokens correspond to the same syllable or subword, but occur within different basic truths."}, {"heading": "4.5. Results: Comparison to other systems", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5. Conclusion", "text": "We introduced a segmental Bayesian model that segments and clusters conversational language audio - the first fully covered zero-resource system to be evaluated based on upper-case vocabulary data from multiple speakers; the system limits word boundaries by using a bottom-up pre-segmentation method to detect syllabus-like entities, and relies on a segmental approach in which word segments are presented as fixed-dimensional acoustic word embeddings; our speaker-dependent systems achieve values of about 84% in English and 76% on Xitsonga data, surpassing a pure bottom-up method in which each syllable is treated as a word candidate; despite significantly lower speaker-independent performance, we achieve improvements here by incorporating frame-level characteristics from an autoencoder-like neural network trained using weak top-down constraints; this results in clusters that are more relatory than those found in a gendered and specialized speaker system."}, {"heading": "Acknowledgements", "text": "We would like to thank Okko Ra \ufffd sa \ufffd nen and Shreyas Seshadri for providing the code for their syllable boundary recognition algorithm and for restoring their ZRS results. We would also like to thank Roland Thiollie \ufffd re and Maarten Versteegh for providing the alignments used in the ZRS challenge. HK is funded by a Commonwealth Scholarship, which was partially supported by a James S. McDonnell Foundation Scholar Award to SG."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Forward filtering backward sampling for word segmentation", "text": "To capture the new set of embeddings in line 5 of algorithm 1, the forward filtering backsampling algorithm is used [46]. The forward variable \u03b1 [t] is defined as the density of the frame sequence y1: t, with the last frame at the end of a word: \u03b1 [t], p (y1: t | h \u2212). The embeddings and assignments of the components for all words not included in the current statement sequence si, and the hyperparameters of the GMM are called ashes \u2212 = (X\\ s, z\\ s; a, \u03b2). The forward variables can be recursively calculated as [54]: \u03b1 [t] = t \u2212 \u2212 jp (yt \u2212 j + 1: t | h \u2212) \u03b1 [t \u2212 j] (7), starting with \u03b1 [0] = 1 and the calculation (7) of the last frame: j."}, {"heading": "B. Tables of complete results for all systems and metrics", "text": "In Section 4.4, several variants of our approach were considered. In Section 4.5, a subset of these systems was compared with other systems evaluated in the context of the Zero Resource Speech Challenge 2015 (ZRS) [8], using a subset of challenge metrics. Tables 7 and 8 show the performance of all variants of our system on all ZRS metrics on the English and Xitsonga data, respectively."}, {"heading": "Speaker-independent, cAE embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-independent, MFCC embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-dependent, cAE embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-dependent, MFCC embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-independent, cAE embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-independent, MFCC embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-dependent, cAE embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Speaker-dependent, MFCC embeddings:", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Comput. Speech Lang., vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbol emergence in robotics: A survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "arXiv preprint arXiv:1509.08973, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition", "author": ["A. Jansen", "E. Dupoux", "S.J. Goldwater", "M. Johnson", "S. Khudanpur", "K. Church", "N. Feldman", "H. Hermansky", "F. Metze", "R. Rose", "M. Seltzer", "P. Clark", "I. McGraw", "B. Varadarajan", "E. Bennett", "B. Borschinger", "J. Chiu", "E. Dunbar", "A. Fourtassi", "D. Harwath", "C.-y. Lee", "K. Levin", "A. Norouzian", "V. Peddinti", "R. Richardson", "T. Schatz", "S. Thomas"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards multi-speaker unsupervised speech pattern discovery", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel inference of Dirichlet process Gaussian mixture models for unsupervised acoustic modeling: A feasibility study", "author": ["H. Chen", "C.-C. Leung", "L. Xie", "B. Ma", "H. Li"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of acoustic sub-word units", "author": ["B. Varadarajan", "S. Khudanpur", "E. Dupoux"], "venue": "Proc. ACL, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Phonetics embedding learning with side information", "author": ["G. Synnaeve", "T. Schatz", "E. Dupoux"], "venue": "Proc. SLT, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep scattering spectrum-deep Siamese network pipeline for unsupervised acoustic modeling", "author": ["N. Zeghidour", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "An evaluation of graph clustering methods for unsupervised term discovery", "author": ["V. Lyzinski", "G. Sell", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllablelike units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixeddimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A smartphone-based ASR data collection tool for under-resourced languages", "author": ["N.J. De Vries", "M.H. Davel", "J. Badenhorst", "W.D. Basson", "F. De Wet", "E. Barnard", "A. De Waal"], "venue": "Speech Commun., vol. 56, pp. 119\u2013131, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "An autoencoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering discrete subword units with binarized autoencoders and hidden-markovmodel encoders", "author": ["L. Badino", "A. Mereta", "L. Rosasco"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards unsupervised training of speaker independent acoustic models", "author": ["A. Jansen", "K. Church"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S.J. Goldwater", "T.L. Griffiths"], "venue": "Proc. ACL, 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Mommy and me: familiar names help launch babies into speechstream segmentation", "author": ["H. Bortfeld", "J.L. Morgan", "R.M. Golinkoff", "K. Rathbun"], "venue": "Psychol. Sci., vol. 16, no. 4, pp. 298\u2013304, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phonetic categories by learning a lexicon", "author": ["N.H. Feldman", "T.L. Griffiths", "J.L. Morgan"], "venue": "Proc. CCSS, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "SCARF: a segmental conditional random field toolkit for speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Interspeech, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["D. Gillick", "L. Gillick", "S. Wegmann"], "venue": "Proc. ASRU, 2011. 11", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmental and syllabic representations in the perception of speech by young infants", "author": ["P.D. Eimas"], "venue": "J. Acoust. Soc. Am., vol. 105, no. 3, pp. 1901\u20131911, 1999.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1901}, {"title": "Segmentation of continuous speech using phonotactics", "author": ["J.M. McQueen"], "venue": "J. Memory Lang., vol. 39, no. 1, pp. 21\u201346, 1998.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["\u2014\u2014"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/\u223cmurphyk/ mypapers.html", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian methods for hidden Markov models", "author": ["S.L. Scott"], "venue": "J. Am. Stat. Assoc., vol. 97, no. 457, pp. 337\u2013351, 2002.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep segmental neural networks for speech recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "Proc. Interspeech, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples", "author": ["H.-y. Lee", "L.-s. Lee"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 21, no. 6, pp. 1272\u20131284, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability", "author": ["M.A. Pitt", "K. Johnson", "E. Hume", "S. Kiesling", "W. Raymond"], "venue": "Speech Commun., vol. 45, no. 1, pp. 89\u201395, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Proc. LREC, 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised lexical clustering of speech segments using fixeddimensional acoustic embeddings", "author": ["H. Kamper", "A. Jansen", "S. King", "S.J. Goldwater"], "venue": "Proc. SLT, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "S.J. Goldwater", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Pre-linguistic rhythmic segmentation of speech into syllabic units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "submission, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 267, "endOffset": 270}, {"referenceID": 3, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 195, "endOffset": 201}, {"referenceID": 5, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 195, "endOffset": 201}, {"referenceID": 6, "context": "Interest in zero-resource speech processing has grown considerably in the last few years, with two central research areas emerging [7,8].", "startOffset": 131, "endOffset": 136}, {"referenceID": 7, "context": "Interest in zero-resource speech processing has grown considerably in the last few years, with two central research areas emerging [7,8].", "startOffset": 131, "endOffset": 136}, {"referenceID": 8, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 118, "endOffset": 125}, {"referenceID": 9, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 118, "endOffset": 125}, {"referenceID": 1, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 10, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 11, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 12, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 13, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 14, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 15, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 0, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 17, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 18, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 4, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 19, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 20, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 21, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 22, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 23, "context": "In previous work [24] we introduced a novel unsupervised segmental Bayesian model for full-coverage segmentation and clustering of small-vocabulary speech.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "In [24] we evaluated the model in an unsupervised digit recognition task using the TIDigits corpus.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 17, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 18, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 21, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 226, "endOffset": 234}, {"referenceID": 22, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 226, "endOffset": 234}, {"referenceID": 20, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 268, "endOffset": 276}, {"referenceID": 23, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 268, "endOffset": 276}, {"referenceID": 22, "context": "For our efficiency improvements, we use a bottom-up unsupervised syllable boundary detection method [23] to eliminate unlikely word boundaries, reducing the number of potential word segments that need to be considered.", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "We also use a computationally much simpler embedding approach based on downsampling [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "For better speaker-independent performance, we incorporate a frame-level representation learning method introduced in our previous work [26]: the correspondence autoencoder (cAE).", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "In [26] we showed that cAE frame-level features outperform traditional features (MFCCs) and GMM-based representations in a multi-speaker intrinsic evaluation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Xitsonga is an under-resourced southern African Bantu language [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 18, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 22, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 22, "context": "In particular, we find that by proposing a consistent segmentation and clustering over a whole utterance, our approach makes better use of the bottom-up syllabic constraints than the purely bottom-up syllable-based system of [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 8, "context": "Zhang and Glass [9] successfully used posteriorgram features from an unsupervised GMM universal background model (UBM) for query-by-example search and term discovery.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "[10] used posteriorgrams from a non-parameteric infinite GMM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], the more traditional iterative re-estimation and unsupervised decoding procedure of Siu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], and the non-parameteric Bayesian HMM of Lee and Glass [12].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[2], and the non-parameteric Bayesian HMM of Lee and Glass [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 28, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 29, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 30, "context": "showed that such constraints can be used to train HMMs [31] and GMM-UBMs [32] that significantly outperform their pure bottom-up counterparts.", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "showed that such constraints can be used to train HMMs [31] and GMM-UBMs [32] that significantly outperform their pure bottom-up counterparts.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "In our own work [26], we proposed the correspondence autoencoder (cAE): an AE-like deep NN that incorporates top-down constraints by using aligned frames from discovered words as input-output pairs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 79, "endOffset": 86}, {"referenceID": 28, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 79, "endOffset": 86}, {"referenceID": 12, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 14, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 32, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 0, "context": "This algorithm, developed by Park and Glass [1], identifies similar sub-sequences within two vector time series, rather than comparing entire sequences as in standard DTW.", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [16] or by greatly improving its efficiency [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [16] or by greatly improving its efficiency [18].", "startOffset": 174, "endOffset": 178}, {"referenceID": 7, "context": "The baseline provided as part of the lexical discovery track of the Zero Resource Speech Challenge 2015 (ZRS) [8] is a UTD system based on the earlier work of [18].", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "The baseline provided as part of the lexical discovery track of the Zero Resource Speech Challenge 2015 (ZRS) [8] is a UTD system based on the earlier work of [18].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "[19] extended the baseline system using improved graph clustering algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 59, "endOffset": 62}, {"referenceID": 19, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 20, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 240, "endOffset": 244}, {"referenceID": 21, "context": "In [22], Lee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "As in their model, we also follow a Bayesian approach, which is useful for incorporating prior knowledge and for finding sparser solutions [34].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "However, where [22] only considered singlespeaker data, we additionally evaluate on large-vocabulary multispeaker data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 20, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 21, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 31, "context": "The approach is further motivated by the observation that it is often easier to identify cross-speaker similarities between words than between subwords [32], which is why most UTD systems focus on longer-spanning patterns.", "startOffset": 152, "endOffset": 156}, {"referenceID": 34, "context": "There is also evidence that infants are able to segment whole words from continuous speech while still learning phonetic contrasts in their native language [35, 36].", "startOffset": 156, "endOffset": 164}, {"referenceID": 35, "context": "There is also evidence that infants are able to segment whole words from continuous speech while still learning phonetic contrasts in their native language [35, 36].", "startOffset": 156, "endOffset": 164}, {"referenceID": 36, "context": "Finally, segmental approaches do not make the frame-level independence assumptions of most of the models above; this assumption has long been argued against [37, 38].", "startOffset": 157, "endOffset": 165}, {"referenceID": 37, "context": "Finally, segmental approaches do not make the frame-level independence assumptions of most of the models above; this assumption has long been argued against [37, 38].", "startOffset": 157, "endOffset": 165}, {"referenceID": 22, "context": "[23], which we use to help scale our approach to larger vocabularies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In our model, we incorporate the syllable boundary detection method of [23] (the first component of their system) as a presegmentation method to eliminate unlikely word boundaries.", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 246, "endOffset": 250}, {"referenceID": 0, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 24, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 41, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 42, "context": ", zN ) using a collapsed Gibbs sampler [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "This is done in turn for each zi conditioned on all the other current component assignments [24]:", "startOffset": 92, "endOffset": 96}, {"referenceID": 44, "context": "The term p(xi|Xk\\i;\u03b2) in (5) is the posterior predictive of xi, which (because of the conjugate prior) is a spherical covariance Gaussian distribution with analytic expressions for its mean and covariance parameters [45].", "startOffset": 216, "endOffset": 220}, {"referenceID": 45, "context": "Line 5 uses the forward filtering backward sampling dynamic programming algorithm [46] to sample the new embeddings; details of this step are given in Appendix A.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "In [24], potential word segments were therefore required to be between 200 ms and 1 s in duration, and word boundaries were only considered at 20 ms intervals.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] evaluated several syllable boundary detection algorithms, and we use the best of these.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In this work, we use the syllabification code kindly provided by the authors of [23] without any modification and with the default parameter settings.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "A simple and fast approach to obtain acoustic word embeddings is to uniformly downsample so that any segment is represented by the same fixed number of vectors [25,47].", "startOffset": 160, "endOffset": 167}, {"referenceID": 46, "context": "A simple and fast approach to obtain acoustic word embeddings is to uniformly downsample so that any segment is represented by the same fixed number of vectors [25,47].", "startOffset": 160, "endOffset": 167}, {"referenceID": 22, "context": "A similar approach is to divide a segment into a fixed number of intervals and average the frames in each interval [23,48].", "startOffset": 115, "endOffset": 122}, {"referenceID": 47, "context": "A similar approach is to divide a segment into a fixed number of intervals and average the frames in each interval [23,48].", "startOffset": 115, "endOffset": 122}, {"referenceID": 23, "context": "Although these very simple approaches are less accurate at word discrimination than the approach used before in [24], they have been effectively used in several studies, including [23], and are computationally much more efficient.", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "Although these very simple approaches are less accurate at word discrimination than the approach used before in [24], they have been effectively used in several studies, including [23], and are computationally much more efficient.", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "Here we use downsampling as our acoustic word embedding function fe in Figure 1; we keep ten equally-spaced vectors from a segment and use a Fourier-based method for smoothing [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 25, "context": "Complete details of the cAE are given in [26], but we briefly outline the training procedure here.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "The UTD system of [18] is used to discover word pairs which serve as weak top-down supervision.", "startOffset": 18, "endOffset": 22}, {"referenceID": 48, "context": "The first two are disjoint subsets extracted from the Buckeye corpus of conversational English [49], while the third is a portion of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa [27].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "The first two are disjoint subsets extracted from the Buckeye corpus of conversational English [49], while the third is a portion of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa [27].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "English2 and the Xitsonga data were used as test sets in the ZRS challenge, so we can compare our system to others using the same data and evaluation framework [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 49, "context": "The evaluation of zero-resource systems that segment and cluster speech is a research problem in itself [50].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "many-to-one) [5].", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "Unsupervised word error rate (WER/WERm) uses a a similar word-level mapping and then aligns the mapped decoded output from a system to the ground truth transcriptions [20, 21].", "startOffset": 167, "endOffset": 175}, {"referenceID": 20, "context": "Unsupervised word error rate (WER/WERm) uses a a similar word-level mapping and then aligns the mapped decoded output from a system to the ground truth transcriptions [20, 21].", "startOffset": 167, "endOffset": 175}, {"referenceID": 23, "context": "The latter, which we denote simply as WER, might leave some cluster unassigned and these are counted as errors [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "These metrics use a phoneme-level mapping: each discovered token is mapped to the sequence of ground truth phonemes of which at least 50% or 30 ms are covered by the discovered segment [8, 50].", "startOffset": 185, "endOffset": 192}, {"referenceID": 49, "context": "These metrics use a phoneme-level mapping: each discovered token is mapped to the sequence of ground truth phonemes of which at least 50% or 30 ms are covered by the discovered segment [8, 50].", "startOffset": 185, "endOffset": 192}, {"referenceID": 21, "context": "A tolerance of 20 ms is mostly used [22], but for the ZRS the tolerance is 30 ms or 50% of a phoneme (to match the mapping).", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "3) are based on [14, 26].", "startOffset": 16, "endOffset": 24}, {"referenceID": 25, "context": "3) are based on [14, 26].", "startOffset": 16, "endOffset": 24}, {"referenceID": 17, "context": "Pairs are extracted using the UTD system of [18]: for English1, 14 494 word pairs are discovered; for English2, 10 769 pairs; and for Xitsonga, 6979.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "Although it is common in NN speech systems to use nine or eleven sliding frames as input, we use single-frame MFCCs with first and second order derivatives (39-dimensional), as also done in [14, 26].", "startOffset": 190, "endOffset": 198}, {"referenceID": 25, "context": "Although it is common in NN speech systems to use nine or eleven sliding frames as input, we use single-frame MFCCs with first and second order derivatives (39-dimensional), as also done in [14, 26].", "startOffset": 190, "endOffset": 198}, {"referenceID": 23, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 50, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 51, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 23, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 50, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 51, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 22, "context": "[23] without modification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Here we follow the same approach as in [23]: we choose K as a proportion of the number of discovered syllable tokens.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "To improve sampler convergence, we use simulated annealing [24].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Such a minimum duration constraint is also used in most UTD systems [1, 18].", "startOffset": 68, "endOffset": 75}, {"referenceID": 17, "context": "Such a minimum duration constraint is also used in most UTD systems [1, 18].", "startOffset": 68, "endOffset": 75}, {"referenceID": 22, "context": "Moreover, there is a much higher proportion of multisyllabic words in Xitsonga [23], as reflected in the average duration of words which is almost twice as long in the Xitsonga than in the English data (Section 4.", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "A word boundary tolerance of 20 ms is used [22], with a greedy one-to-one mapping for calculating error rates.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "We now compare our approach to others using the evaluation framework provided as part of the ZRS challenge [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "We compare our approach to three systems: ZRSBaselineUTD is the UTD system used as official baseline in the challenge [8] (see Section 2.", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "UTDGraphCC is the best UTD system of [19], employing a connected component graph clustering algorithm to group discovered segments (also Section 2.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "SyllableSegOsc+ uses oscillator-based syllabification followed by speaker-dependent clustering and word discovery [23] (Section 2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "We add the superscript + since, after publication of [23], R\u00e4s\u00e4nen et al.", "startOffset": 53, "endOffset": 57}, {"referenceID": 52, "context": "further refined their syllable boundary detection method [53].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The authors of [23] kindly regenerated their full ZRS results for comparison here.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In contrast, such a minimum duration constraint would require additional heuristics in the pure bottom-up approach of [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 22, "context": "We conclude that by hypothesizing word boundaries consistently over an utterance rather than taking these decisions in isolation, our approach yields more accurate clusters (NED) that correspond better to true words (word type F -score) than the full-coverage syllable-based approach of [23].", "startOffset": 287, "endOffset": 291}, {"referenceID": 22, "context": "However, despite the benefits of our model, the algorithm of [23] is much simpler in terms of computational complexity and implementation.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units\u2014effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on largevocabulary multi-speaker data. The system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). We show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speakerand gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system\u2019s discovered clusters are still less pure than those of two multi-speaker term discovery systems, but provide far greater coverage.", "creator": "LaTeX with hyperref package"}}}