{"id": "1606.06979", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor-Critic Reinforcement Learning", "abstract": "This paper contributes a preliminary report on the advantages and disadvantages of incorporating simultaneous human control and feedback signals in the training of a reinforcement learning robotic agent. While robotic human-machine interfaces have become increasingly complex in both form and function, control remains challenging for users. This has resulted in an increasing gap between user control approaches and the number of robotic motors which can be controlled. One way to address this gap is to shift some autonomy to the robot. Semi-autonomous actions of the robotic agent can then be shaped by human feedback, simplifying user control. Most prior work on agent shaping by humans has incorporated training with feedback, or has included indirect control signals. By contrast, in this paper we explore how a human can provide concurrent feedback signals and real-time myoelectric control signals to train a robot's actor-critic reinforcement learning control system. Using both a physical and a simulated robotic system, we compare training performance on a simple movement task when reward is derived from the environment, when reward is provided by the human, and combinations of these two approaches. Our results indicate that some benefit can be gained with the inclusion of human generated feedback.", "histories": [["v1", "Wed, 22 Jun 2016 15:09:04 GMT  (1507kb)", "http://arxiv.org/abs/1606.06979v1", "7 pages, 3 figures, Accepted at the Interactive Machine Learning Workshop at IJCAI 2016 (IML): Connecting Humans and Machines"]], "COMMENTS": "7 pages, 3 figures, Accepted at the Interactive Machine Learning Workshop at IJCAI 2016 (IML): Connecting Humans and Machines", "reviews": [], "SUBJECTS": "cs.HC cs.AI cs.RO", "authors": ["kory w mathewson", "patrick m pilarski"], "accepted": false, "id": "1606.06979"}, "pdf": {"name": "1606.06979.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A key example is that of robotic prostheses: prostheses that are attached to the body to replace skills that have been lost through injury or disease. A major reason users dislike the use of prostheses is the functional limitations of the limbs that have now been developed; a fundamental limitation is the complex control of such devices that can perform complex functions and movements; human control of this functionality is still limited; one of the main reasons users dislike the use of prostheses is the limitation of the functionality of the limbs; while state-of-the-art prostheses can perform complex functions and movements, human control of this functionality is limited. In 2007, new methods must be developed to help people control complex robotic devices that are directly connected to them."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning", "text": "RL is a learning framework inspired by behavioral researchers [Thorndike, 1898; Skinner, 1938]. It describes how agents improve over time by taking action in an environment designed to maximize a reward signal [Sutton and Barto, 1998]. Control policy is iteratively improved by selecting actions that maximize future reward signals accumulated by the agent. Usually, RL problems are defined by Markov decision-making processes (MDPs) defined by the tupel: (,,,,,,,) S defines the state, or the agent's currently observable environmental variables, A defines the actions the actor can take, T is the transition probabilities between the current state and the next state, \"with a specific action taken or formally,\" the discount factor, or the future reward is evaluated by the agent, or how much is future reward."}, {"heading": "2.2 The Continuous Actor-Critic Algorithm", "text": "In this paper, we will focus on actor-critic algorithms, a subset of policy gradient-based algorithms. In these algorithms, control politics is a function that defines the probability that the system chooses an action in a state."}, {"heading": "2.3 Incorporating Human Feedback", "text": "There are a variety of ways to integrate human knowledge into a learning system, before or during learning. Learning from demonstration, reward shaping and inverse RL are all areas of active research that are closely related to the work in this work, a comprehensive review of these methods can be found elsewhere [Thomaz and Breazeal, 2008; Chernova and Tomaz 2014]. Knox and Stone introduced the interactive form problem or how human feedback can best be integrated into the learning agent in a sequential decision-making process [Knox et al. 2010]. In short, it is an agent acting in an environment defined by an MDP, and a person who observes the action performance and provides feedback to the agent on how to learn the best possible task policy, measured by task performance or cumulative human feedback, given the information contained in human feedback [Knox and Stone, 2009]."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Aldebaran Nao and Myo EMG Data", "text": "The experimental setup is shown in Figure 1. It consists of the Aldebaran robotics platform (Aldebaran Robotics), the Myo wireless wristwatch (Thalmic Labs) and a Mac Book Air (Apple, 2.2 GHz Intel Core i7, 8GB RAM) for human feedback and execution of the learning process. The Aldebaran Nao was selected as a cost-effective physical test platform to experiment with algorithm development, in particular, the upper arms were selected to mimic the functionality of commercial myoelectric prostheses."}, {"heading": "3.1 The Learning Algorithm", "text": "The continuous actor-critic algorithm used in this paper is a slightly modified version of a previously described ACRL algorithm [Pilarski et al. 2011; Pilarski et al. 2013]. The algorithm is described in algorithm 1. The actor selects a new action a from a normal distribution, P defined by mean = standard deviation () and standard deviation = Exp (WS). Actions in this work are defined as continuous angular increments or decreases. X and W are parameter vectors for the mean and the standard deviation of the actor system. The system selects and performs an action a that transfers the actor to a new state and generates a reward r, then the critic defined by a parameter vector v calculates a TD error from r, and the current estimates of the value of the old state and the new state, s. \"Each learning system (actor and critic) is a problem that is defined by a pair of credits or a pair defined in a particular state."}, {"heading": "4 Experiments", "text": "This year, it has come to the point where it will only take a few days to come to a result in which it will come to an outcome."}, {"heading": "6 Discussion", "text": "This work explores combinations of human and environmental feedback in simulated and physical robot systems with simulated and real EMG control signals. Simulation and physical experiments show agreement, but some performance decreases can be expected with physical robot systems. This performance decrease may be due to heat, inertia and / or mechanical jitter. While there are advantages to developing and testing a simulated robot, including safety, controlled repeatability and batch processing, additional experiments on physical robots are necessary to accelerate the improvised learning steps, but also the human reward parameters, and the encoding of parameters have been selected to maximize the reward accumulated on the simulated myo and nao. These parameters may not be optimal for physical nao configuration, and could potentially be adapted in the future."}, {"heading": "7 Conclusions", "text": "Our results show improvements in performance by incorporating human feedback into existing algorithms, and show that human interaction can improve performance in complex robotic tasks. Therefore, this work offers a new perspective on the human education of a robotic system that is tightly bound to the user's body, and suggests that seamless interaction with complex hardware may require a shift in autonomy from humans to machines. Future work will explore methods to reduce human cognitive stress and to model, generalize, and deliver consistent human feedback."}, {"heading": "Acknowledgments", "text": "This work is supported by the National Sciences & Engineering Research Council of Canada, Alberta Innovates - Technology Futures and the Alberta Innovates Centre for Machine Learning."}], "references": [{"title": "IEEE Trans", "author": ["Artemiadis PK", "Kyriakopoulos KJ. EMG-based control of a robot arm using low-dimensional embeddings"], "venue": "Robotics.", "citeRegEx": "Artemiais and Kyriakopolous. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Systems", "author": ["AG Barto", "RS Sutton", "Trans Anderson CW. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE"], "venue": "Man & Cybernetics.", "citeRegEx": "Barto et al.. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Upper limb prosthesis use and abandonment: a survey of the last 25 years", "author": ["EA Biddiss", "TT Chau"], "venue": "J Prosthet Orthot Intl.", "citeRegEx": "Biddiss and Chau. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement learning from demonstration through shaping", "author": ["Brys et al", "T. 2015] Brys", "A. Harutyunyan", "H.B. Suay", "S. Chernova", "M.E. Taylor", "A. Now\u00e9", "June"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Proc", "author": ["C Castellini", "P Artemiadis", "M Wininger", "A Ajoudani", "M Alimusaj", "A Bicchi", "B Caputo", "W Craelius", "S Dosen", "K Englehart", "D Farina"], "venue": "Of 1 Workshop on Peripheral Machine Interfaces.", "citeRegEx": "Castellini et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecs", "author": ["Chernova S", "Thomaz AL. Robot learning from human teachers. Syn"], "venue": "on AI and ML.", "citeRegEx": "Chernova and Tomaz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["Degris et al", "2012] Degris T", "Pilarski PM", "Sutton RS"], "venue": "American Control Conference. 2012 Jun", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Application of real-time machine learning to myoelectric prosthesis control: A case series in adaptive switching", "author": ["AL Edwards", "MR Dawson", "JS Hebert", "C Sherstan", "RS Sutton", "KM Chan", "PM Pilarski"], "venue": "J Prosthet Orthot Int.", "citeRegEx": "Edwards et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "real-time control scheme for multifunction myoelectric control", "author": ["Englehart K", "Hudgins B. A robust"], "venue": "IEEE Trans. Biomed Eng.", "citeRegEx": "Englehart and Hudgins. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Shaping mario with human advice", "author": ["Harutyunyan et al", "A. 2015] Harutyunyan", "T. Brys", "P. Vrancx", "A. Now\u00e9", "May"], "venue": "In Proceedings of the 2015 Int. Conference on Autonomous Agents and Multiagent Systems (pp. 1913-1914)", "citeRegEx": "al. et al\\.,? \\Q1914\\E", "shortCiteRegEx": "al. et al\\.", "year": 1914}, {"title": "Biological arm motion through reinforcement learning", "author": ["J Izawa", "T Kondo", "K Ito"], "venue": "Biological cybernetics.", "citeRegEx": "Izawa and Ito. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Interactively shaping agents via human reinforcement: The TAMER framework", "author": ["Knox", "Stone", "2009] Knox WB", "Stone P"], "venue": "In Proc of 5 Intl. Conf. on Knowledge Capture", "citeRegEx": "Knox et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2009}, {"title": "Reinforcement learning from human reward: Discounting in episodic tasks", "author": ["WB Knox", "P Stone"], "venue": "IEEE ROMAN.", "citeRegEx": "Knox and Stone. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Framing reinforcement learning from human reward: Reward positivity", "author": ["WB Knox", "P Stone"], "venue": "temporal discounting, episodicity, and performance. Artificial Intelligence.", "citeRegEx": "Knox and Stone. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning behaviors via humandelivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R Loftin", "B Peng", "J MacGlashan", "ML Littman", "ME Taylor", "J Huang", "DL Roberts"], "venue": "AAMAS.", "citeRegEx": "Loftin et al.. 2015", "shortCiteRegEx": null, "year": 2016}, {"title": "and Comm Japan", "author": ["D Nishikawa", "W Yu", "H Yokoi", "Elec Kakazu Y. On-line learning method for EMG prosthetic hand control."], "venue": "III.", "citeRegEx": "Nisikawa et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "IEEE Trans", "author": ["Oskoei MA", "Hu H. Support vector machine-based classification scheme for myoelectric control applied to upper limb"], "venue": "Biomed Eng.", "citeRegEx": "Oskoei and Hu. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Myoelectric signal processing for control of powered limb prostheses", "author": ["P Parker", "K Englehart", "B Hudgins"], "venue": "J Electromyogr Kinesiol.", "citeRegEx": "Parker et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Natural actor-critic", "author": ["J Peters", "S Schaal"], "venue": "Neurocomputing.", "citeRegEx": "Peters and Schaal. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning for humanoid robotics", "author": ["al. Peters et", "J 2003] Peters", "S Vijayakumar", "S. Schaal"], "venue": "IEEE-RAS Intl. Conf. on Humanoid Robots", "citeRegEx": "et et al\\.,? \\Q2003\\E", "shortCiteRegEx": "et et al\\.", "year": 2003}, {"title": "Conf", "author": ["PM Pilarski", "MR Dawson", "T Degris", "F Fahimi", "JP Carey", "Intl Sutton RS. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. IEEE"], "venue": "on Rehabilitation Robotics.", "citeRegEx": "Pilarski et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Conf", "author": ["PM Pilarski", "TB Dick", "Intl Sutton RS. Real-time prediction learning for the simultaneous actuation of multiple prosthetic joints. IEEE"], "venue": "on Rehabilitation Robotics.", "citeRegEx": "Pilarski et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "2 Workshop on Present and Future of Non-Invasive Peripheral-Nervous-System Machine Interfaces", "author": ["PM Pilarski", "RS Sutton", "Agents Mathewson KW. Prosthetic Devices as Goal-Seeking"], "venue": "Singapore,", "citeRegEx": "Pilarski et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Trans", "author": ["Sanger TD. Neural network learning control of robot manipulators using gradually increasing task difficulty"], "venue": "Robotics and Automation.", "citeRegEx": "Sanger. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Adaptive pattern recognition of myoelectric signals: exploration of conceptual framework and practical algorithms", "author": ["JW Sensinger", "BA Lock", "TA Kuiken"], "venue": "IEEE Trans. Neural Sys. and Rehab. Eng.", "citeRegEx": "Sensinger et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "The behavior of organisms: an experimental analysis", "author": ["BF Skinner"], "venue": "Appleton-Century, Oxford.", "citeRegEx": "Skinner. 1938", "shortCiteRegEx": null, "year": 1938}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["RS Sutton"], "venue": "Doctoral Dissertation.", "citeRegEx": "Sutton. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Reinforcement learning: An introduction", "author": ["RS Sutton", "AG Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["al. Sutton et", "RS 1999] Sutton", "DA McAllester", "SP Singh", "Y. Mansour"], "venue": "NIPS. 1999 (Vol", "citeRegEx": "et et al\\.,? \\Q1999\\E", "shortCiteRegEx": "et et al\\.", "year": 1999}, {"title": "Policy gradient learning of cooperative interaction with a robot using user\u2019s biological signals", "author": ["Tamei", "Shibata", "2009] Tamei T", "Shibata T"], "venue": "Adv. In Neuro-Information Processing", "citeRegEx": "Tamei et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tamei et al\\.", "year": 2008}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["AL Thomaz", "C Breazeal"], "venue": "Artificial Intelligence.", "citeRegEx": "Thomaz and Breazeal. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Application of the actor-critic architecture to functional electrical stimulation control of a human", "author": ["al. Thomas et", "P 2009] Thomas", "M Branicky", "A van den Bogert", "K. Jagodnik"], "venue": "arm. Conf. Proc. Innovative Applications of AI", "citeRegEx": "et et al\\.,? \\Q2009\\E", "shortCiteRegEx": "et et al\\.", "year": 2009}, {"title": "Animal Intelligence", "author": ["EL Thorndike"], "venue": "Nature.", "citeRegEx": "Thorndike. 1898", "shortCiteRegEx": null, "year": 1898}, {"title": "Learning via human feedback in continuous state and action spaces", "author": ["NA Vien", "W Ertel", "TC Chung"], "venue": "Applied Intelligence.", "citeRegEx": "Vien and Ertel. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Q-learning", "author": ["CJ Watkins", "P Dayan"], "venue": "Machine Learning.", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["RJ Williams"], "venue": "Machine Learning.", "citeRegEx": "Williams. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 4, "context": "A principal limitation is the complex control of such devices [Castellini et al., 2014] by humans.", "startOffset": 62, "endOffset": 87}, {"referenceID": 4, "context": "This difficulty is partly due to the user\u2019s inability to provide clear electromyographic (EMG) control signals, and partly due to control challenges in interpreting these complex multi-dimensional signals to guide robotic movements [Castellini et al., 2014].", "startOffset": 232, "endOffset": 257}, {"referenceID": 24, "context": "The need for adaptation may stem from changes in the patient physical and mental states, intents, and/or usage [Sensinger et al., 2009].", "startOffset": 111, "endOffset": 135}, {"referenceID": 16, "context": ", 2010], artificial neural networks, and support vector machines [Oskoei and Hu, 2008], as well as unsupervised [Sensinger et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 24, "context": ", 2010], artificial neural networks, and support vector machines [Oskoei and Hu, 2008], as well as unsupervised [Sensinger et al., 2009], and semisupervised [Nishikawa et al.", "startOffset": 112, "endOffset": 136}, {"referenceID": 4, "context": "provides a good overview of the state-of-the-art myoelectric prosthetic control research [Castellini et al., 2014].", "startOffset": 89, "endOffset": 114}, {"referenceID": 7, "context": "Adaptive, real-time approaches to addressing challenges in myoelectric control have also been proposed and shown to work in simulation, and in preliminary experiments with able-bodied subjects and subjects with amputations [Edwards et al., 2015].", "startOffset": 223, "endOffset": 245}, {"referenceID": 22, "context": "work describes how these advances in prosthetic device control can be viewed as increasing communicative capital between two goal-seeking agents, the prosthetic and the human wearing it [Pilarski et al., 2015].", "startOffset": 186, "endOffset": 209}, {"referenceID": 32, "context": "Reinforcement learning (RL) was introduced as a theory of how humans and animals learn in response to positive and negative stimuli associated with actions towards a goal [Thorndike, 1898; Skinner, 1938].", "startOffset": 171, "endOffset": 203}, {"referenceID": 25, "context": "Reinforcement learning (RL) was introduced as a theory of how humans and animals learn in response to positive and negative stimuli associated with actions towards a goal [Thorndike, 1898; Skinner, 1938].", "startOffset": 171, "endOffset": 203}, {"referenceID": 26, "context": "In the early 1980\u2019s Sutton extended this concept to simulated learning agents and early work applied RL to difficult control problems [Sutton, 1984; Barto et al., 1983].", "startOffset": 134, "endOffset": 168}, {"referenceID": 1, "context": "In the early 1980\u2019s Sutton extended this concept to simulated learning agents and early work applied RL to difficult control problems [Sutton, 1984; Barto et al., 1983].", "startOffset": 134, "endOffset": 168}, {"referenceID": 27, "context": "RL is an approach for solving these control problems through interaction between a learning system and the environment, and provides a theoretical backing of convergence in well defined domains [Sutton and Barto, 1998].", "startOffset": 194, "endOffset": 218}, {"referenceID": 18, "context": "Peters and Schaal demonstrated ACRL learning of complex movement systems and motor primitives for humanoid robotics [Peters and Schaal, 2008], Izawa et al.", "startOffset": 116, "endOffset": 141}, {"referenceID": 32, "context": "RL is a learning framework inspired by behaviorists [Thorndike, 1898; Skinner, 1938].", "startOffset": 52, "endOffset": 84}, {"referenceID": 25, "context": "RL is a learning framework inspired by behaviorists [Thorndike, 1898; Skinner, 1938].", "startOffset": 52, "endOffset": 84}, {"referenceID": 27, "context": "It describes how agents improve over time by taking actions in an environment with a goal of maximizing some reward signal [Sutton and Barto, 1998].", "startOffset": 123, "endOffset": 147}, {"referenceID": 27, "context": "RL approaches may be policy-based, searching the policy space directly, or value-based, estimating the value of possible actions in each state and then deriving a policy from these value estimations [Sutton and Barto, 1998].", "startOffset": 199, "endOffset": 223}, {"referenceID": 34, "context": "such as Q-learning [Watkins and Dayan, 1992], can approximate this function by iteratively updating an estimate temporal difference error.", "startOffset": 19, "endOffset": 44}, {"referenceID": 35, "context": "Policybased methods update the policy parameter vector w in the direction of the gradient of the return with respect to w [Williams, 1992].", "startOffset": 122, "endOffset": 138}, {"referenceID": 27, "context": "This discretizes the space, and allows for generalization in the learned policies [Sutton and Barto, 1998].", "startOffset": 82, "endOffset": 106}, {"referenceID": 27, "context": "More comprehensive details on ACRL algorithms are given by [Sutton and Barto, 1998] and [Peters and Schaal, 2008].", "startOffset": 59, "endOffset": 83}, {"referenceID": 18, "context": "More comprehensive details on ACRL algorithms are given by [Sutton and Barto, 1998] and [Peters and Schaal, 2008].", "startOffset": 88, "endOffset": 113}, {"referenceID": 30, "context": "Learning from demonstration, reward shaping, and inverse RL are all areas of active research closely connected with the work in this paper, a comprehensive review of those methods can be found elsewhere [Thomaz and Breazeal, 2008; Chernova and Tomaz 2014].", "startOffset": 203, "endOffset": 255}, {"referenceID": 5, "context": "Learning from demonstration, reward shaping, and inverse RL are all areas of active research closely connected with the work in this paper, a comprehensive review of those methods can be found elsewhere [Thomaz and Breazeal, 2008; Chernova and Tomaz 2014].", "startOffset": 203, "endOffset": 255}, {"referenceID": 12, "context": "Further, Knox and Stone have delineated some of the biggest confounding issues in the Shaping Problem, namely the positive circuits problem and the credit assignment problem [Knox and Stone, 2012; Knox and Stone, 2015].", "startOffset": 174, "endOffset": 218}, {"referenceID": 13, "context": "Further, Knox and Stone have delineated some of the biggest confounding issues in the Shaping Problem, namely the positive circuits problem and the credit assignment problem [Knox and Stone, 2012; Knox and Stone, 2015].", "startOffset": 174, "endOffset": 218}, {"referenceID": 33, "context": "The history window to which credit can be applied can be varied with a meta-parameter [Knox and Stone, 2009; Vien and Ertel, 2013].", "startOffset": 86, "endOffset": 130}, {"referenceID": 33, "context": "Vien and Ertel also showed that the human feedback model can be generalized to address the problems associated with periods of noisy, or inconsistent, human feedback [Vien and Ertel, 2013].", "startOffset": 166, "endOffset": 188}, {"referenceID": 14, "context": "Recent advancements in modelling human feedback with a Bayesian approach have improved on the work of Knox and Stone in discrete environments [Loftin et al., 2015].", "startOffset": 142, "endOffset": 163}, {"referenceID": 21, "context": "previously [Pilarski et al. 2011; Pilarski et al., 2013].", "startOffset": 11, "endOffset": 56}, {"referenceID": 21, "context": "es for the actor are used to accelerate learning [Pilarski et al., 2013].", "startOffset": 49, "endOffset": 72}, {"referenceID": 20, "context": "As previously described by [Pilarski et al., 2011], to provide a human training signal (experimental conditions 3 and 4) across the fast learning algorithm, the reward signal on each time step following delivery was set to a decayed trace of the reward of the previous step, or: r689 = 0.", "startOffset": 27, "endOffset": 50}, {"referenceID": 5, "context": "As well, providing clear, consistent feedback to the robot is difficult [Chernova and Tomaz 2014].", "startOffset": 72, "endOffset": 97}, {"referenceID": 14, "context": "The research area exploring how to model and deliver consistent human feedback is rich with methods, such as reward shaping [Brys, 2015], TAMER [Knox and Stone, 2009], iSABL [Loftin et al., 2015], and Actor-critic and Tile-coding TAMER [Vien and Ertel, 2013].", "startOffset": 174, "endOffset": 195}, {"referenceID": 33, "context": ", 2015], and Actor-critic and Tile-coding TAMER [Vien and Ertel, 2013].", "startOffset": 48, "endOffset": 70}], "year": 2016, "abstractText": "This paper contributes a preliminary report on the advantages and disadvantages of incorporating simultaneous human control and feedback signals in the training of a reinforcement learning robotic agent. While robotic human-machine interfaces have become increasingly complex in both form and function, control remains challenging for users. This has resulted in an increasing gap between user control approaches and the number of robotic actuators which can be controlled. One way to address this gap is to shift autonomy to the robot. Semiautonomous actions of the robotic agent can then be shaped and refined by human feedback, simplifying user control. Most prior work on humanagent shaping has incorporated training with feedback, or has included indirect control signals. By contrast, in this paper we explore how a human can provide concurrent feedback signals and real-time myoelectric control signals to train a robot\u2019s actorcritic reinforcement learning control system. Using both a physical and a simulated robotic system, we compare training performance on a simple movement task when reward is derived from the environment, from the human, and from the combination of the two. Our results indicate that benefit may be gained by including human generated feedback in learning algorithms for this complex human-machine interactive domain.", "creator": "Word"}}}