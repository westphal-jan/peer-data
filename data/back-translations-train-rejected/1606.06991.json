{"id": "1606.06991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Toward Word Embedding for Personalized Information Retrieval", "abstract": "This paper presents preliminary works on using Word Embedding (word2vec) for query expansion in the context of Personalized Information Retrieval. Traditionally, word embeddings are learned on a general corpus, like Wikipedia. In this work we try to personalize the word embeddings learning, by achieving the learning on the user's profile. The word embeddings are then in the same context than the user interests. Our proposal is evaluated on the CLEF Social Book Search 2016 collection. The results obtained show that some efforts should be made in the way to apply Word Embedding in the context of Personalized Information Retrieval.", "histories": [["v1", "Wed, 22 Jun 2016 15:53:29 GMT  (406kb,D)", "http://arxiv.org/abs/1606.06991v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["nawal ould-amer", "philippe mulhem", "mathias gery"], "accepted": false, "id": "1606.06991"}, "pdf": {"name": "1606.06991.pdf", "metadata": {"source": "CRF", "title": "Toward Word Embedding for Personalized Information Retrieval", "authors": ["Nawal OULD AMER", "Philippe MULHEM"], "emails": ["amer@imag.fr", "philippe.mulhem@imag.fr", "mathias.gery@univ-st-"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Information Systems \u2192 Personalization; Keywords Word Embedding, word2vec, Personalization, Social Book Search, Query Expansion"}, {"heading": "1. INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2. PERSONALIZED QUERY EXPANSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 User Modeling", "text": "In the context of the Social Book Search, each user is represented by its catalog (i.e. a set of books) and other information such as tags and the ratings it assigns to the books. [6] All this information describes the user's interests. We present a user u as a document du, which represents the concatenation of all the documents in its catalogue.ar Xiv: 160 6.06 991v 1 [cs.I R] 22 June 2016The profile of u, noted pu, is then represented by the phrase in du: pu = {w1, w2, w3,..., wn} (1)"}, {"heading": "2.2 Term Filtering", "text": "As mentioned above, Word Embedding can potentially be helpful in selecting terms related to the query. Normally, embedded terms are used for query extension as extensions of one of the query terms. Despite the effectiveness of Word Embedding in selecting embedded terms, their use for query extension may reduce the effectiveness of the system. If an extended term is actually a loud term (perhaps because of its ambiguity or because it is not a current term of the query), the amount of resulting word embedding increases the non-thematic noise in the extended query. For example, in the queries (taken from the topics of Social Book Search 2016) \"Help! I Need more books,\" \"New Releases from authors that make you literally faint...\" or \"Favorite Christmas Books to read to young children,\" the majority of these terms are good for queries, such as \"new,\" etc."}, {"heading": "2.3 Word Embedding Selection", "text": "Once we have a filtered query qf as described above (cf. subsection 2.2), we select the word embeddings to be used as extensions. This selection is achieved in three steps for each term t of the filtered query qf: i) build a word embedding for t using the cosinal similarity between t and all words in the training corpus; ii) filter out of the word embedding of i) terms that have the same stem as t using the English Porter Stemmer to avoid overemphasizing the variations of t; iii) select the word embeddings of t from the filtered word embedding of ii).Then the output of the word embedw2v (qf) = em t11, em t12,..., em t1k, em t21, em t22,..., em t2k,... toqem, qem (qem), towots-the (Wjf-2qf)."}, {"heading": "2.4 Ranking Model", "text": "The final extended query qnew is the union of the original user query q and the word embeddings as follows: qnew = q, WordEmbeddingw2v (qf) (3) The score for each document d is calculated according to the advanced query qnew and the user u according to a classic language model with dirichlet smoothing."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "Our experiments are conducted using the Social Book Search [6] dataset. \u2022 Documents: The document collection consists of 2.8 million book descriptions with metadata from Amazon and LibraryThing. Each document is represented by book title, author, publisher, year of publication, library classification codes, and user-generated content in the form of user ratings and reviews.1 \u2022 Users: The collection provides profiles of 120 users. Each user is described by its catalog (i.e. a set of books), tags, and ratings. \u2022 Queries: The collection includes 120 user queries. Based on the nature of the queries, we selected 28 queries for our experiments: These queries have current content (i.e. a classical IR system then has a chance of getting relevant results) and the query issuer's profile is not empty."}, {"heading": "3.2 Learning of Word Embedding", "text": "Here we describe the process of learning word vectors (see section 1). We use two training sets. \u2022 The first is called The Non Personalized Corpus: the train builds on the entire Social Book Search Corpus. \u2022 The second is called The Personalized Corpus: we build a personalized train corpus for each user. The training process is the same for both corpus: 1. Non Personalized Corpus train process: We build word2vec [7] on the Social Book Search Corpus. word2vec represents each word of the training set as a vector of characteristics, this vector being intended to capture the contexts in which w appears. Therefore, we chose the Social Book Search Corpus as a training set, as the training set is likely to be consistent with the data we want to test on."}, {"heading": "3.3 Parameters and Tested Configurations", "text": "All documents are processed using a terrier search engine [9] with \u00b5 = 50. We compare the following variations: 1. Query filtering: The query is filtered or not by removing the stopword and adjectives as specified in Section 2.2; 2. Query extension: The query is extended with or without word embedding; 3. Personalization: The query is personalized (with the personalized corpus) or not (with the non-personalized corpus). The tested configurations are presented in Table 3.3."}, {"heading": "4. RESULTS", "text": "In this section, we present and comment on the results of the above configurations. All of these configurations result in reasonably low MAP values, but this is consistent with the official CLEF Social Book Search results."}, {"heading": "4.1 Query Filtering", "text": "Table 4.1 indicates the mean average precision (MAP), mean reciprocal rank (MRR) and accuracy for 10 documents (P @ 10) obtained with the Conf1 configuration (without query filtering and without query enhancement) and the Conf2 configuration (with query filtering and without query enhancement). As we can see, filtering query terms with the stop adjective list improves MAP values, but surprisingly neither MRR nor P @ 10 is the most effective way to deal with noisy terms."}, {"heading": "4.2 Personalized Query Expansion (with filtering)", "text": "As we can see, in most cases, the non-personalized approach outperforms the personalized approach, and the personalized approach only shows better results when word embedding is limited to the top 2 terms. We also note that both approaches undercut the filtered, non-extended approach of Conf2, which shows that insufficient words are added. In Conf3, we see that adding more than 8 terms leads to better results, which is not really the case with the Conf4 personalized configuration, where MAP development is quite flat. In this case, no added term seems to play a positive role."}, {"heading": "4.3 Personalized Query Expansion (without filtering)", "text": "Figure 2 shows the evolution of the MAP value in terms of the number of word embeddings for the Conf5 (unpersonalized query extension, without filtering) and Conf6 (personalized query extension, without filtering) configurations. As we can see, the unpersonalized approach usually outperforms the personalized one. However, these two curves behave completely differently in Figure 2: the personalized approach is only better if you add the top 1 and top 2 words, which makes the first two personalized terms interesting, while the non-personalized extension behaves better and better as the number of terms increases, leading to the assumption that \"good\" terms (depending on the user) occur in the unpersonalized case."}, {"heading": "4.4 Discussion", "text": "As we can see, the query-extension configurations (Conf1 and Conf2) still have the best results and outperform all query-extension configurations (i.e. Conf3, Conf4, Conf5 and Conf6). As shown above, the advanced approach in most cases does not lead to an improvement in results. We may explain these results by the quality of the corpus. In fact, the document collection describes the user ratings for books. Therefore, it is still difficult to extract the context of the terms and select the similar terms in a similar context. The personalized word embedding fails to improve the results compared to any configurations. First, we can explain the results by the same quality problem as for the documents. In the fact that the large documents available to users do not correspond to the amount of data available in the catalogs."}, {"heading": "5. CONCLUSION", "text": "We have found that the type of queries is a major challenge for the effective use of Word Embedding in this context, and future work could contribute to a better understanding of this behavior. Secondly, it is related to the fact that personalization using Word Embedding has not produced good results, for many reasons: the quality of the description of user profiles, the lack of data we get to describe the profile that does not allow effective use of Word Embedding. Here, too, future work may help to find solutions to these problems."}, {"heading": "6. REFERENCES", "text": "[1] M. Almasri, C. Berrut, and J. Chevallet. Acomparison of deep learning based query expansion with pseudo-relevance feedback and mutual information. In European Conference on IR Research, ECIR 2016, Padua, Italy, pp. 709-715, 2016. [2] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41 (6): 391-407, 1990. [3] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based language model for information retrieval. In Conference on Research and Development in Information Retrieval, SIGIR '15, pp. 795-798, New York, USA, 2015. [4] Y. Goldberg and O. Levi."}], "references": [{"title": "A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information", "author": ["M. Almasri", "C. Berrut", "J. Chevallet"], "venue": "European Conference on IR Research, ECIR 2016, Padua, Italy, pages 709\u2013715", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["D. Ganguly", "D. Roy", "M. Mitra", "G.J. Jones"], "venue": "Conference on Research and Development in Information Retrieval, SIGIR\u201915, pages 795\u2013798, New York, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "CoRR, abs/1402.3722,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "CoRR, abs/1408.5882", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Overview of the CLEF 2015 social book search lab", "author": ["M. Koolen", "T. Bogers", "M. G\u00e4de", "M.A. Hall", "H.C. Huurdeman", "J. Kamps", "M. Skov", "E. Toms", "D. Walsh"], "venue": "Conference and Labs of the Evaluation Forum, CLEF\u201915, pages 545\u2013564, Toulouse, France", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving document ranking with dual word embeddings", "author": ["E. Nalisnick", "B. Mitra", "N. Craswell", "R. Caruana"], "venue": "Conference Companion on World Wide Web, WWW\u201916 Companion, pages 83\u201384, Monteal, Quebec, Canada", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Terrier: A High Performance and Scalable Information Retrieval Platform", "author": ["I. Ounis", "G. Amati", "V. Plachouras", "B. He", "C. Macdonald", "C. Lioma"], "venue": "SIGIR\u201906 Workshop on Open Source Information Retrieval, OSIR\u201906)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometry and Meaning", "author": ["D. Widdows"], "venue": "Center for the Study of Language and Information/SRI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 2, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 7, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 4, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Word Embedding [7] is the generic name of a set of NLP-related learning techniques that seek to embed representations of words, leading to a richer representation: words are represented as vectors of more elementary components or features.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Similarly to Latent Semantic Analysis (LSA) [2], Word Embedding maps the words to low-dimensional (w.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "if the contexts in turn have similar words [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "bass + guitar = bass guitar [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "The first question is motivated by our participation in the CLEF Social Book Search task in 2016 (similar to the Social Book Search task in 2015 [6]).", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "set of books) and other information such as tags and the ratings that he assigns to the books [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "Our experiments are conducted on Social Book Search dataset [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Non Personalized Corpus train process: We train word2vec [7] on the Social Book Search corpus.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "All documents are retrieved using Terrier search engine [9] with \u03bc = 50.", "startOffset": 56, "endOffset": 59}], "year": 2016, "abstractText": "This paper presents preliminary works on using Word Embedding (word2vec) for query expansion in the context of Personalized Information Retrieval. Traditionally, word embeddings are learned on a general corpus, like Wikipedia. In this work we try to personalize the word embeddings learning, by achieving the learning on the user\u2019s profile. The word embeddings are then in the same context than the user interests. Our proposal is evaluated on the CLEF Social Book Search 2016 collection. The results obtained show that some efforts should be made in the way to apply Word Embedding in the context of Personalized Information Retrieval. CCS Concepts \u2022Information systems \u2192 Personalization;", "creator": "LaTeX with hyperref package"}}}