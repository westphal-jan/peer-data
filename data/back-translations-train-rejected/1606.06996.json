{"id": "1606.06996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "The word entropy of natural languages", "abstract": "The average uncertainty associated with words is an information-theoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty - also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.", "histories": [["v1", "Wed, 22 Jun 2016 16:00:52 GMT  (141kb,D)", "http://arxiv.org/abs/1606.06996v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian bentz", "dimitrios alikaniotis"], "accepted": false, "id": "1606.06996"}, "pdf": {"name": "1606.06996.pdf", "metadata": {"source": "CRF", "title": "The Word Entropy of Natural Languages", "authors": ["Christian Bentz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The predictability of symbols in strings is the most basic concept of information encryption in general and of natural language production in particular. Shannon [29] defined the entropy or average content of information as aar Xiv: 160 6.06 996v 1Measure of uncertainty or \"choice\" inherent in strings. Since then, Shannon [28] and others have made great efforts to accurately assess the entropy of written English [4, 12, 8, 27] and other languages [1, 17, 18]. In computer linguistics, entropy - and related measures - have been widely applied to address problems related to machine translation [2, 20], distribution semantics [10, 22, 26, 25], information retrieval [3, 30, 16] and multiword expressions [33, 24]. All of these representations depend crucially on the estimation of probability and uncertainty associated with words - i.e., the problem inherent in a given language."}, {"heading": "2 Motivation", "text": "A fundamental property of words is their frequency of occurrence and hence their probability in language production. This probability is the basis of many applications in natural language processing. (1) This is the conditional probability that an expert translates the English word the into German, where we have the English original. More generally, we could have a number of possible German translations S = {der, das, den, des, des, des, des, des, des, des, des, (2) with estimated probabilities: p, (der) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (des) = 1. (3) In the uniform case, all these are associated with estimated probabilities: p, (der) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (des) + p, (des) = 1. (3) In the uniform case, all of these are associated with estimated probabilities: p, (der) + p, (die) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) + p, (den) = 1."}, {"heading": "2.1 Entropy in statistical machine translation", "text": "In the above example, the task of a machine translation system is to map the English word from the set of German translations to the correct choice. For example, the maximum entropy approach [2] requires that the probabilities from the given training data are estimated as accurately as possible. Probabilities that cannot be reliably estimated are then selected to maximize the overall entropy of the model. Crucially, the translation difficulty in such an account is a direct function of the entropy of the word distribution. In English-German mapping - with uniform probabilities - there are 2.6 bits of \"choice,\" while in German-English mapping there are 0 bits of \"choice.\" This is closely related to Koehns [11] finding that BLEU negatively correlates with the number of words in the source language when translating into English from 10 different source languages."}, {"heading": "2.2 Entropy in distributional semantics", "text": "Measures of semantic content and similarity [25, 26, 22, 10] are often also based on information theory concepts. Thus, Resnik's classic study [25] defines the similarity between two concepts c1 and c2 in a semantic hierarchy assim (c1, c2) = max c-S (c1, c2) [\u2212 log p (c)], (4) where S (c1, c2) is the set of concepts that subsumes both c1 and c2. To obtain the information content \u2212 log p (c), the probability of a common concept p (c) from corpus data asp (c) = freq (c) / N is estimated, (5) where freq (c) would count the number of word marks (nouns in this case) on the frequency of word marks (nouns) and nouns (nouns) for the frequency of dollar, euro and coin symmetry."}, {"heading": "3 Data", "text": "To control the constant content between languages, we use two sets of parallel texts: 1) the Corpus of the European Parliament (EPC) [11] and 2) the Parallel Bible Corpus (PBC) [15].1 Details about the corpora are given in Table 1. The general advantage of the EPC is that it is large in terms of the number of word marks per language (about 30M), while the PBC is smaller (about 280K word marks per language), but massively parallel in terms of the volume of > 1000 languages."}, {"heading": "4 Methods", "text": "Previous studies on entropy in English [28, 12, 27] often used letters instead. However, in computer linguistics, word marks are a common unit of work. A word mark is defined here as a string of UTF-8 alphanumeric characters delimited by spaces, with all letters converted to lowercase letters and punctuation removed. Note that, for example, Mandarin Chinese (cmn) and Khmer (khm) fonts delimit phrases and sentences by spaces rather than words. However, such fonts represent a negligible portion of our sample (approximately 0.01%). In fact, approximately 90% of texts are written in Latin script."}, {"heading": "4.1 Entropy estimation", "text": "Suppose a text is a random variable T, which is created by a process of drawing and concatenating symbols from a sentence (or vocabulary) of the words V = 1The last access took place on 09 / 03 / 2016 {w1, w2,..., wV}, with the vocabulary size V = | V |. Word type probabilities are defined according to p (w) = Pr (T = w) for w \u00b2 V. In view of these definitions, the entropy of T can be regarded as [29] H (T) = \u2212 V \u2211 i = 1 p (wi) log2 (p (wi)). (4) H (T) can be regarded as the average information content of word types. A decisive step in estimating H (T) is to reliably approximate the probabilities p (wi)."}, {"heading": "4.2 Block entropies", "text": "In a text, each word type wi has a symbol frequency fi = freq (wi). Let's take the first verse of the English Bible as a text. In the beginning, God created heaven and earth and the earth was waste and empty [...] In this example, the word type the occurs 4 times and occurs 3 times, etc. Thus, as a simple approximation, we can estimate p (wi) using the maximum probability method: p (wi) = fi \u2211 V j = 1 fj, (5) where the denominator is the total number of word tokens. So, for the Bible verse, we would have: H (T) = \u2212 (4 17 log2 (4 17) + 3 17 log2 (3 17) +."}, {"heading": "4.3 Source entropies", "text": "Instead of calculating Hn (T) with ever increasing block sizes n, Kontoyiannis et al. [12] and Gao et al. [8], the results of the optimal compression of Ziv & Lempel [34, 35] are used. Specifically, [8] the method shows that entropy estimation based on the so-called increasing window estimator or LZ78 estimator [5] is efficient in terms of convergence. Applied to the problem of estimating word entropies, the method works as follows: for each given word token ti in a text find the longest match length l, for which the string s = (ti, ti + l) corresponds to a preceding string in (t1, ti \u2212 1). Formally, we define l asli = 1 + max {0 \u2264 l \u2264 i: si + l \u2212 1i = s j + l \u2212 1 j j for some 0 j \u2264 j \u2264 \u2212 1. (This is an adjustment of Gao 1)."}, {"heading": "4.4 Implementation", "text": "We estimate Unigram entropies - i.e. entropies for block sizes of 1 (H1 (T)) - using the Python implementation of the Nemenman-Shafee-Bialek (NSB) estimator [19], which has a faster convergence rate than other block entropy estimators [9].4In addition, we implement the source entropy estimator as proposed by Gao et al. [8], which is inspired by an earlier implementation of Montemurro & Zanette [17] by Kontoyiannis et al. [12] The code of this implementation is available on Gigub."}, {"heading": "5 Results", "text": "First, we evaluate the text sizes at which both block and source entropies converge to a stable value using a subset of 21 languages (Section 5.1), then we select texts from the full PBC corpus based on the minimum number of tokens required for convergence, estimate their entropies and compare their distribution across an entropy spectrum (Section 5.2), and finally, in Section 5.3, we examine the correlation between block and source entropies."}, {"heading": "5.1 Entropy convergence", "text": "Figure 1 illustrates the convergence of block and source entropies in 21 languages of the EPC. Note that block entropies are generally higher than source entropies. This is expected as uncertainty for source entropies is reduced by taking into account the previous cotext. We further establish the convergence points by calculating entropy points for increments of 10K characters as shown in Figure 2. If we choose SD < 0.05 as the convergence criterion, we obtain the convergence points per language shown in Table 2.Note that all 21 languages converge to stable entropies below the text size of 100K characters, with a maximum of 70K characters (bg and ro) and an average of 35K for source and 38K for block entropies. This is an encouraging result, as texts with a minimum of approximately 70K characters are available for a wide range of languages in P.Bhttps / 25st.gistong:"}, {"heading": "5.2 Entropy estimates", "text": "Based on the convergence analysis, we select 100K characters as the cut-off point for the inclusion of PBC texts, resulting in 1352 texts for block entropies and 1360 texts for source entropies. [5] Both cover 1001 languages. Figure 3 is a density chart of the estimated entropy values. Block entropies are distributed roughly by an average of 9.26 (SD = 1.24) and source entropies by an average of 5.97 (SD = 1.07). Again, source entropies are systematically lower than block entropies. It is noteworthy that, given the wide range of potential entropies - from 0 to > 16 - most natural languages fall within a relatively narrow spectrum. For example, block entropies predominantly fall between 7 and 12, covering only about 30% of the possible range."}, {"heading": "5.3 Correlation between block and source entropies", "text": "The similarities of the convergence lines in Figure 1 indicate that there is a correlation between block and source entropy estimates. Figure 4 elicits this correlation by representing block entropies (x-axis) versus source entropies (yaxis).The Pearson correlation is strong (r = 0.96, p < 0.0001), suggesting that the number of block entropy texts is lower, as the estimates for 8 texts failed due to some punctuation marks that were not correctly removed."}, {"heading": "6 Discussion", "text": "Regardless of the estimation method - which uses block or source entropies - texts with 100K characters are generally sufficient to estimate values reliably, as has been shown in 21 languages of the EPC. Of course, this does not mean that there are no texts / languages in a larger corpus such as the PBC where convergence may take longer. Note that the 21 EPC languages cover about 50% of the entire range of values in the PBC sample. For example, block entropies in a larger corpus range from 0 to 12 for Finnish in the EPC and from 0 to 13 in the PBC.6Five clear outliers have been removed here, namely texts from languages such as Chinese (cmn) and Khmer (khm), which delimit phrases and sentences rather by white spaces, which have incomprehensible source and block entropies, and there is also a strong correlation between block and source tropies."}, {"heading": "6.1 Predicting translation performance", "text": "Based on estimated entropies per language, we can predict the difficulty of pairwise translations between languages, i.e. the performance of the translation system. [11] As shown in Figure 5, Koehn's BLEU values for pairwise translations correlate with the pairwise ratios of entropies (r = 0.58, p < 0.0001). For example, Finnish (fi) is a high entropy language (H (T) = 8.35) compared to English (H (T) = 6.32). Translation from Finnish to English yields a BLEU value of 21.8 and from English to Finnish 13. As explained above, this is due to the fact that translation into a higher entropy language means more \"choice\" - or uncertainty - when translating words (or phrases)."}, {"heading": "6.2 Entropy as a normalization factor", "text": "In Section 2, some examples of studies that use the information content in a distributional semantic context were given. Resnik [25], for example, builds a measure of equality for concepts and words based on the information content, e.g. \u2212 log p (wi). Remember that entropy according to Equation 4 is the average information content. From the field of word entropies in Figure 3, it is clear that languages can strongly tend to have words with a high or low information content on average. For example, words in Finnish tend to have a higher information content than words in English. This bias is a problem for the cross-linguistic application of similarity measures. It can be overcome by normalization using the estimated entropy: ICunbiased (wi) = \u2212 log p (wi) H (T). (11)"}, {"heading": "7 Conclusions", "text": "Entropy, the average content of information, uncertainty or \"choice\" is a central characteristic of words. Words, in turn, are fundamental building blocks of computational linguistics. Understanding word entropies is therefore a prerequisite for evaluating and improving the performance of NLP systems. We have established convergence points for 21 languages here, which show that average word entropies with text sizes of > 70K can be reliably estimated. Based on these results, we estimated entropies in 1360 texts and 1001 languages. Furthermore, we have shown empirically that there is a strong correlation between block entropies and source entropies in these languages. Overall, our results contribute to a better understanding of the performance of multilingual translation systems and to making actions in distribution semantics linguistically translatable."}, {"heading": "8 Acknowledgements", "text": "The CB is funded by the DFG-Kolleg Worte, Knochen, Gene, Tools and the ERC-F\u00f6rderpreis EVOLAEMP at the University of T\u00fcbingen."}], "references": [{"title": "Estimating and comparing entropies across written natural languages using ppm compression", "author": ["F.H. Behr", "V. Fossum", "M. Mitzenmacher", "D. Xiao"], "venue": "Data Compression Conference, 2003. Proceedings. DCC 2003, page 416. IEEE,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "V.J.D. Pietra", "S.A.D. Pietra"], "venue": "Computational linguistics, 22(1):39\u201371,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Information density, heaps\u2019 law, and perception of factiness in news", "author": ["M. Boon"], "venue": "ACL 2014, page 33,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "An estimate of an upper bound for the entropy of english", "author": ["P.F. Brown", "V.J.D. Pietra", "R.L. Mercer", "S.A.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics, 18(1):31\u201340,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel texts", "author": ["M. Cysouw", "B. W\u00e4lchli"], "venue": "Using translational equivalents in linguistic typology. Sprachtypologie & Universalienforschung STUF, 60.2,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal stanford dependencies: A crosslinguistic typology", "author": ["M.-C. De Marneffe", "T. Dozat", "N. Silveira", "K. Haverinen", "F. Ginter", "J. Nivre", "C.D. Manning"], "venue": "LREC, volume 14, pages 4585\u20134592,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating the entropy of binary time series: Methodology, some theory and a simulation study", "author": ["Y. Gao", "I. Kontoyiannis", "E. Bienenstock"], "venue": "Entropy, 10(2):71\u201399,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Entropy inference and the james-stein estimator, with application to nonlinear gene association networks", "author": ["J. Hausser", "K. Strimmer"], "venue": "The Journal of Machine Learning Research, 10:1469\u20131484,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring semantic content in distributional vectors", "author": ["A. Herbelot", "M. Ganesalingam"], "venue": "ACL (2), pages 440\u2013445,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "MT summit, volume 5, pages 79\u201386,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonparametric entropy estimation for stationary processes and random fields, with applications to English text", "author": ["I. Kontoyiannis", "P.H. Algoet", "Y.M. Suhov", "A.J. Wyner"], "venue": "Information Theory, IEEE Transactions on, 44(3):1319\u20131327,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Entropy estimation of very short symbolic sequences", "author": ["A. Lesne", "J.-L. Blanc", "L. Pezard"], "venue": "Physical Review E, 79(4):046208,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Developing odin: A multilingual repository of annotated language data for hundreds of the world\u2019s languages", "author": ["W.D. Lewis", "F. Xia"], "venue": "Literary and Linguistic Computing, page fqq006,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Creating a massively parallel bible corpus", "author": ["T. Mayer", "M. Cysouw"], "venue": "N. Calzolari, K. Choukri, T. Declerck, H. Loftsson, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), Reykjavik, Iceland, May 26-31, 2014, pages 3158\u20133163. European Language Resources Association (ELRA),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Perplexity analysis of obesity news coverage", "author": ["D.J. McFarlane", "N. Elhadad", "R. Kukafka"], "venue": "AMIA Annual Symposium Proceedings, volume 2009, page 426. American Medical Informatics Association,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Universal entropy of word ordering across linguistic families", "author": ["M.A. Montemurro", "D.H. Zanette"], "venue": "PLoS One, 6(5):e19875,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity and universality in the long-range order of words", "author": ["M.A. Montemurro", "D.H. Zanette"], "venue": "arXiv preprint arXiv:1503.01129,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Entropy and inference, revisited", "author": ["I. Nemenman", "F. Shafee", "W. Bialek"], "venue": "arXiv preprint, page physics/0108025,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295\u2013302. Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian word alignment for massively parallel texts", "author": ["R. \u00d6stling"], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics, pages 123\u2013127. Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring semantic content to assess asymmetry in derivation", "author": ["S. Pad\u00f3", "A. Palmer", "M. Kisselew", "J. \u0160najder"], "venue": "Workshop on Advances in Distributional Semantics,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "An evaluation of methods for the extraction of multiword expressions", "author": ["C. Ramisch", "P. Schreiner", "M. Idiart", "A. Villavicencio"], "venue": "Proceedings of the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE 2008), pages 50\u201353,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": "arXiv preprint cmp-lg/9511007,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["E. Santus", "A. Lenci", "Q. Lu", "S.S. Im Walde"], "venue": "EACL, pages 38\u201342,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Entropy estimation of symbol sequences", "author": ["T. Sch\u00fcrmann", "P. Grassberger"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 6(3):414\u2013427,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Prediction and entropy of printed English", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal, 30(1):50\u201365,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1951}, {"title": "The mathematical theory of communication", "author": ["C.E. Shannon", "W. Weaver"], "venue": "The University of Illinois Press, Urbana,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1949}, {"title": "The sublanguage of cross-coverage", "author": ["P.D. Stetson", "S.B. Johnson", "M. Scotch", "G. Hripcsak"], "venue": "Proceedings of the AMIA Symposium, page 742. American Medical Informatics Association,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Indirect measurement in morphological typology", "author": ["B. W\u00e4lchli"], "venue": "A. Ender, A. Leemann, and B. W\u00e4lchli, editors, Methods in contemporary linguistics, pages 69\u201392. De Gruyter Mouton, Berlin,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Lexical typology through similarity semantics: Toward a semantic map of motion verbs", "author": ["B. W\u00e4lchli", "M. Cysouw"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Automated multiword expression prediction for grammar engineering", "author": ["Y. Zhang", "V. Kordoni", "A. Villavicencio", "M. Idiart"], "venue": "Proceedings 20  of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36\u201344. Association for Computational Linguistics,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "A universal algorithm for sequential data compression", "author": ["J. Ziv", "A. Lempel"], "venue": "IEEE Transactions on information theory, 23(3):337\u2013343,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1977}, {"title": "Compression of individual sequences via variablerate coding", "author": ["J. Ziv", "A. Lempel"], "venue": "Information Theory, IEEE Transactions on, 24(5):530\u2013536,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 28, "context": "Shannon [29] defined the entropy - or average information content as a", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 11, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 7, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 26, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 0, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 16, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 17, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 1, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 139, "endOffset": 146}, {"referenceID": 19, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 21, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 25, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 24, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 2, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 29, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 15, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 32, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 252, "endOffset": 260}, {"referenceID": 23, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 252, "endOffset": 260}, {"referenceID": 18, "context": "In this study, we use a state-of-the-art method to estimate block entropies [19], and also implement a source entropy estimator [8].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "In this study, we use a state-of-the-art method to estimate block entropies [19], and also implement a source entropy estimator [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "The maximum entropy approach [2], for instance, requires that the probabilities are estimated as accurately as possible from the given training data.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "This is closely related to Koehn\u2019s [11] finding that when translating into English from 10 different source languages, BLEU scores negatively correlate with the number of words in the source language.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 25, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 21, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 9, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 24, "context": "For example, the classical study by Resnik [25] defines the similarity between two concepts c1 and c2 in a semantic hierarchy as", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 9, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 20, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 22, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 6, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 5, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 31, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 30, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 10, "context": "To control for constant content across languages, we use two sets of parallel texts: 1) the European Parliament Corpus (EPC) [11], and 2) the Parallel Bible Corpus (PBC) [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "To control for constant content across languages, we use two sets of parallel texts: 1) the European Parliament Corpus (EPC) [11], and 2) the Parallel Bible Corpus (PBC) [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 27, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 11, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 28, "context": "Given these definitions, the entropy of T can be calculated as [29]", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "However, there are two main caveats with this so-called plug-in approach: Firstly, it has been shown that the maximum likelihood estimator is unreliable, especially for small text sizes [19, 9], i.", "startOffset": 186, "endOffset": 193}, {"referenceID": 8, "context": "However, there are two main caveats with this so-called plug-in approach: Firstly, it has been shown that the maximum likelihood estimator is unreliable, especially for small text sizes [19, 9], i.", "startOffset": 186, "endOffset": 193}, {"referenceID": 26, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 18, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 8, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 12, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 12, "context": "Clearly, this requirement is not met for natural languages [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "This yields block entropies [27] defined as", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "Note that Sch\u00fcrmann & Grassberger [27] use an English corpus of 70M words and assert that entropy estimation beyond a block size of 5 letters (not words) is already unreliable.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "This is based on the theory behind Lempel-Ziv compression [34, 35].", "startOffset": 58, "endOffset": 66}, {"referenceID": 34, "context": "This is based on the theory behind Lempel-Ziv compression [34, 35].", "startOffset": 58, "endOffset": 66}, {"referenceID": 11, "context": "[12] and Gao et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 71, "endOffset": 79}, {"referenceID": 34, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 71, "endOffset": 79}, {"referenceID": 7, "context": "More precisely, [8] show that entropy estimation based on the so-called increasing window estimator, or LZ78 estimator [5], is efficient in terms of convergence.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "More precisely, [8] show that entropy estimation based on the so-called increasing window estimator, or LZ78 estimator [5], is efficient in terms of convergence.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "\u2019s [8] match-length definition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "[8] (Equation 6) show that the entropy of the string can be approximated as", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "This approximates the entropy rate, or per-symbol entropy [8], which is denoted as h by Lesne et al.", "startOffset": 58, "endOffset": 61}, {"referenceID": 12, "context": "[13], and for which holds", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "So h accounts for all statistical dependencies between tokens [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "We will call h and its approximation H\u0303(T ) the source entropy - after Shannon\u2019s [29] formulation of the entropy of an information source.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "[8] give a more general definition that also holds for the so-called sliding window, or LZ77 estimator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "H1(T ), is identical to the source entropy h [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "entropies for block sizes of 1 (H1(T )) - using the Python implementation of the Nemenman-Shafee-Bialek (NSB) [19] estimator.", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "This estimator has a faster convergence rate compared to other block entropy estimators [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "\u2019s [8] proposal.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "This is inspired by an earlier implementation by Montemurro & Zanette [17] of Kontoyiannis et al.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "\u2019s [12] estimator.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11] used a probabilistic phrase-based model to translate between the (then available) 11 languages of the EPC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Resnik [25], for instance, builds a", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Figure 5: Correlation between ratios of source entropies and BLEU scores of a statistical machine translation system [11].", "startOffset": 117, "endOffset": 121}], "year": 2016, "abstractText": "The average uncertainty associated with words is an informationtheoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.", "creator": "LaTeX with hyperref package"}}}