{"id": "1606.07006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Using Word Embeddings in Twitter Election Classification", "abstract": "Word embeddings and convolutional neural networks (CNN) have attracted extensive attention in various classification tasks for Twitter, e.g. sentiment classification. However, the effect of the configuration used to train and generate the word embeddings on the classification performance has not been studied in the existing literature. In this paper, using a Twitter election classification task that aims to detect election-related tweets, we investigate the impact of the background dataset used to train the embedding models, the context window size and the dimensionality of word embeddings on the classification performance. By comparing the classification results of two word embedding models, which are trained using different background corpora (e.g. Wikipedia articles and Twitter microposts), we show that the background data type should align with the Twitter classification dataset to achieve a better performance. Moreover, by evaluating the results of word embeddings models trained using various context window sizes and dimensionalities, we found that large context window and dimension sizes are preferable to improve the performance. Our experimental results also show that using word embeddings and CNN leads to statistically significant improvements over various baselines such as random, SVM with TF-IDF and SVM with word embeddings.", "histories": [["v1", "Wed, 22 Jun 2016 16:37:55 GMT  (74kb,D)", "https://arxiv.org/abs/1606.07006v1", "NeuIR Workshop 2016"], ["v2", "Tue, 19 Jul 2016 10:22:17 GMT  (74kb,D)", "http://arxiv.org/abs/1606.07006v2", "NeuIR Workshop 2016"], ["v3", "Tue, 21 Mar 2017 18:29:49 GMT  (73kb,D)", "http://arxiv.org/abs/1606.07006v3", "NeuIR Workshop 2016"]], "COMMENTS": "NeuIR Workshop 2016", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["xiao yang", "craig macdonald", "iadh ounis"], "accepted": false, "id": "1606.07006"}, "pdf": {"name": "1606.07006.pdf", "metadata": {"source": "CRF", "title": "Using Word Embeddings in Twitter Election Classification", "authors": ["Xiao Yang", "Craig Macdonald", "Iadh Ounis"], "emails": ["firstname.lastname@glasgow.ac.uk"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to surpass themselves by putting themselves at the center. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to surpass themselves. (...) Most of them are able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. \"(...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves."}, {"heading": "2. RELATED WORK", "text": "A number of studies have already shown that the context window and the dimensionality of the word used \"embedding vectors\" could affect performance in areas such as dependency checking [3] and the named \"entity tagging\" [4], that is, aiming to represent the grammatical relationships between words in a sentence. By varying only the context window size from 1 to 10, their results are related to the accuracy of parts of the language (POS) that show the context window size of Word2Vec, the type of word embedding generated by words in a sentence could be affected. Specifically, they have observed that a smaller context window provides better performance on accuracy. In the named entity of the task, Godin et al al al al al al al al al al. [10] examines three context windows that relate to the type of word embedding generated."}, {"heading": "3. THE CNN MODEL", "text": "For our election campaign, we use a simple CNN architecture described by Kim [11], as well as the categories proposed by Severyn et al. [18] and highlighted in the figure. It consists of a revolutionary level, a maximum pooling layer, a drop-out layer, and a fully connected output layer, each of which is explained in its entirety. CNN classification inputs are pre-processed tweets consisting of a sequence of words. Word embedding transforms tweets into vector representations in the following way: Each of these layers is converted into the n-dimensional word embedding to be the n-dimensional word embedding in a tweet."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "In this paper, we argue that the types of background corpora, as well as the parameters of the Word2Vec model, could lead to different word embeddings and affect the performance of Twitter classification tasks. In the following sections, experiments are tailored to work with CNN to thoroughly examine word embeddings in a Twitter classification task and examine the impact of the background corpora (Section 5), the context window, and the dimensions of word embeddings (Section 6) on classification performance; the rest of this section describes our data set (Section 4.1), our experimental setup, and the word embeddings models used (Section 4.2), baselines (Section 4.3), and actions (Section 4.4)."}, {"heading": "4.1 Dataset", "text": "Our manually labeled election data set is based on tweets collected about the 2015 Venezuelan parliamentary elections and uses the well-known pooling method [16]. It covers the period of one month before and after the election date (6 / 12 / 2015) in Venezuela. We use the Terrier Information Platform [13] and the DFReeKLIM [2] weighting model developed for microblog search to retrieve tweets related to 21 query terms (e.g. \"violencia,\" \"eleccion\" and \"votar\"). Only the top 7 retrieved tweets are selected per query date and day, making the size of the collection realistic for human assessment bodies to examine and label the tweets. Selected tweets are grouped into a pool and evaluated by five experts who call a tweet \"election-related\" or \"non-election-related.\" In order to determine the reliability of the retrieval data, a review of the tweets was carried out by five reviewers."}, {"heading": "4.2 Word embeddings", "text": "The word embedding used in this work is generated by two different background corporations: a Spanish Wikipedia dump of 02 / 10 / 2015 (denotes es-Wiki) and a Spanish Twitter data (denotes es-Twitter) collected in the period 01 / 05 / 2015 to 30 / 06 / 2015. Over 1 million Spanish articles are observed in es-Wiki. In es-Twitter, over 20 million Spanish tweets are collected by removing tweets with less than 10 words, so the short and less informative tweets are not taken into account. For consistency, we apply the same pre-processing, namely the removal of stop words and stigma (see Section 4.1) to both background corporas. Af-ter pre-processing, es-Wiki contains 436K unique words, while it-Twitter has 629K unique words. Excellent statistics are provided in Table 2 by comparing the unique words in our election dataset with the words in es-Wiki and it-Twitter."}, {"heading": "4.3 Baselines", "text": "SVM with TF-IDF (SVM + TFIDF): As a traditional weighting scheme, TF-IDF is used in conjunction with an SVM classifier for Twitter election classification. SVM with Word embedding (SVM + WE): We use a similar scheme used by Wang et al. [20] to create the tweet level for SVM classifiers. The vector representation (i.e. TWE) of a tweet is constructed by embedding the word vectors along each dimension for all words in the tweet: TWE = k = i = 1 wi / k (3), where k is the number of words in a tweet and wi: Rn is the word that embeds the vector of the word."}, {"heading": "4.4 Hyperparameters and measures", "text": "For all experiments, we use 3 filter sizes m = {1, 2, 3}, step width s = 1 and failure probability p = 0.5 for our CNN classifier, according to the settings used by Kim [11]. For each filter size, 200 filters are applied to the coil layer, producing a total of 600 feature cards. For the SVM classifier, we use the standard parameter c = 1 for the linear SVC implementation in Scikit Learn1 [15].To train the classifiers and evaluate their performance on our data set, we use a 5-fold cross-validation, so that in each fold, 3 partitions are used for training, 1 partition for validation and 1 partition for the test. We stop the training process when the classification accuracy decreases on the validation portion. Afterwards, the total performance on the test instances is evaluated by averaging the values folded over all the tortures."}, {"heading": "5. EFFECT OF THE BACKGROUND CORPORA", "text": "Due to the background noise of the Twitter data, Twitter posts are often poor in grammar and spelling. In the meantime, Twitter provides more specialized information such as Twitter handles, HTTP links, and hashtags that would not appear in common text corpora. To determine whether the type of background corpus might favor Twitter classification performance, we compare the two background corpora of es-Wiki and es-Twitter. Taking into account the various experimental results in [3, 10, 14], the context window size of 5 is said to perform well. Thus, we set the context window to 5 and the dimension to 500 for both word embeddings. The classification results are shown in Table 3, where the classifiers we used. In other columns, we report three actions for both background corporations - Wiki and es-Twitter, which we use for the dimensions of both 500 word portions."}, {"heading": "6. EFFECT OF WORD EMBEDDINGS PARAMETERS", "text": "In this section, we will try to examine the effects of parameters (e.g. context windows and dimensionality) for the Twitter choice classification task. There is a better performance, which we generate only with word embeddings originating only from Twitter. Table 5 (a) shows the results of our three basics, while Table 5 (b) shows that the results of word embeddings, namely SVM with word embeddings (SVM + WE) and CNN, the measurements for SVM + WE are arranged by the sizing and context size of word embeddings. For each series of W1, W3 and W5, Table 5 shows the results for context window sizes of W = {1, 3} along each dimension of D = {200, 500, 800} The best overall results are highlighted in bolts."}, {"heading": "7. CONCLUSION", "text": "Since previous studies on the parameter configuration of word embedding have focused on various tasks such as NER [10] and dependency analysis [3], their findings may not be generalizable to Twitter classification tasks. Meanwhile, similar work on Twitter classification tasks [9, 17, 19] has not examined the effects of background corpora and Word2Vec parameters such as context windows and dimensionality. Our results show that these two factors can influence the classification performance of Twitter classification tasks. Based on experiments with a Twitter election dataset, this work examines word embedding when using Convolutionary Neural Networks. Using two different types of background corpus, we observe that when the type of background corpus coincides with the classification dataset, the CNN classifier may perform better. Specifically, our study shows that choosing the right type of background corpus for the use of Constitutional Neural Networks can potentially cover more vocabulary."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "This work was supported by a grant from the Economic and Social Research Council (ES / L016435 / 1)."}, {"heading": "9. REFERENCES", "text": "[1] Venezuela opposition politician luis manuel diaz killed.http: / / www.bbc.co.uk / news / world-latin-america34929332, November 2015. [Accessed: 2016-05-15]. [2] G. Amati, G. Amodeo, M. Bianchi, G. Marcone, F. U. Bordoni, C. Gaibisso, G. Gambosi, A. Celi, C. Di Nicola, and M. Flammini. FUB, IASI-CNR, UNIVAQ at TREC 2011 microblog track. In Proc. of TREC, 2011. [3] M. Bansal, K. Gambosi, and K. Livescu. Tailoring continuous word representations for dependency parsing. In Proc. of the 52nd ACL conference, volume 2, pages 809-815, 2014. [4] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin."}], "references": [{"title": "FUB, IASI-CNR, (a) Results of random classifier, SVM with TF-IDF (SVM+TFIDF) and SVM with word embeddings (SVM+WE) Precision  Recall F1 score Random", "author": ["G. Amati", "G. Amodeo", "M. Bianchi", "G. Marcone", "F.U. Bordoni", "C. Gaibisso", "G. Gambosi", "A. Celi", "C. Di Nicola", "M. Flammini"], "venue": "UNIVAQ at TREC 2011 microblog track. In Proc. of TREC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "In Proc. of the 52nd ACL conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of machine learning research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "On using Twitter to monitor political sentiment and predict election results", "author": ["A. Bermingham", "A.F. Smeaton"], "venue": "In Proc. of SAAIP workshop at IJCNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proc. of the 25th ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of machine learning research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["T.G. Dietterich"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "CIS-positive: Combining convolutional neural networks and SVMs for sentiment analysis in Twitter", "author": ["S. Ebert", "N.T. Vu", "H. Sch\u00fctze"], "venue": "In Proc. of the SemEval workshop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Multimedia Lab@ ACL W-NUT NER shared task: Named entity recognition for Twitter microposts using distributed word representations", "author": ["F. Godin", "B. Vandersmissen", "W. De Neve", "R. Van de Walle"], "venue": "In Proc. of the ACL-IJCNLP conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In Proc. of EMNLP conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural  networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "From puppy to maturity: Experiences in developing terrier", "author": ["C. Macdonald", "R. McCreadie", "R.L. Santos", "I. Ounis"], "venue": "In Proc. of the OSIR workshop at SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Proc. Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of machine learning research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Test collection based evaluation of information retrieval systems", "author": ["M. Sanderson"], "venue": "Foundations and Trends in Information Reteieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "UNITN: Training deep convolutional neural network for Twitter sentiment classification", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proc. of the 9th SemEval workshop,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Distributional neural networks for automatic resolution of crossword puzzles", "author": ["A. Severyn", "M. Nicosia", "G. Barlacchi", "A. Moschitti"], "venue": "In Proc. of ACL-IJCNLP conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning sentiment-specific word embedding for Twitter sentiment classification", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": "In Proc. of the 52nd ACL conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Semantic clustering and convolutional neural network for short text categorization", "author": ["P. Wang", "J. Xu", "B. Xu", "C.-L. Liu", "H. Zhang", "F. Wang", "H. Hao"], "venue": "In Proc. of the 53rd ACL-IJCNLP conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "For example, in the Word2Vec model [14], by maximising the probability of seeing a word within a fixed context window, it is possible to learn for each word in the vocabulary a dense real valued vector from a shallow neural network.", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "As a consequence, similar words are close to each other in the embedding space [4, 7, 14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 5, "context": "As a consequence, similar words are close to each other in the embedding space [4, 7, 14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 12, "context": "As a consequence, similar words are close to each other in the embedding space [4, 7, 14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 7, "context": "The use of word embeddings together with convolutional neural networks (CNN) has been shown to be effective for various classification tasks such as sentiment classification on Twitter [9, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 15, "context": "The use of word embeddings together with convolutional neural networks (CNN) has been shown to be effective for various classification tasks such as sentiment classification on Twitter [9, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 3, "context": "Such a classification task is challenging because election-related tweets are usually ambiguous and it is often difficult for human assessors to reach an agreement on their relevance to the election [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 1, "context": "Moreover, our results contradict the findings of different tasks such as dependency parsing [3] and named entity recognition (NER) [10] where a smaller context window is suggested.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "Moreover, our results contradict the findings of different tasks such as dependency parsing [3] and named entity recognition (NER) [10] where a smaller context window is suggested.", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "A number of studies have already shown that the context window and dimensionality of the used word embedding vectors could affect performance in tasks such as dependency parsing [3] and named entity tagging [10].", "startOffset": 178, "endOffset": 181}, {"referenceID": 8, "context": "A number of studies have already shown that the context window and dimensionality of the used word embedding vectors could affect performance in tasks such as dependency parsing [3] and named entity tagging [10].", "startOffset": 207, "endOffset": 211}, {"referenceID": 1, "context": "[3] investigated Word2Vec word embeddings in the dependency parsing task, which aims to provide a representation of grammatical relations between words in a sentence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] investigated three context window sizes w of w = {1, 3, 5} based on the accuracy of NER tagging.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] investigated the dimensionality of the Word2Vec word embeddings and the size of background data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Thus, they concluded the dimensionality and background data size should be increased together [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "[14] only investigated the Word2Vec parameters using the GoogleNews background corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "However, similar classification tasks such as Twitter sentiment classification have been well studied [9, 17, 19].", "startOffset": 102, "endOffset": 113}, {"referenceID": 15, "context": "However, similar classification tasks such as Twitter sentiment classification have been well studied [9, 17, 19].", "startOffset": 102, "endOffset": 113}, {"referenceID": 17, "context": "However, similar classification tasks such as Twitter sentiment classification have been well studied [9, 17, 19].", "startOffset": 102, "endOffset": 113}, {"referenceID": 15, "context": "In particular, word embeddings were recently used to build effective tweet-level representations for Twitter sentiment classification [17, 19].", "startOffset": 134, "endOffset": 142}, {"referenceID": 17, "context": "In particular, word embeddings were recently used to build effective tweet-level representations for Twitter sentiment classification [17, 19].", "startOffset": 134, "endOffset": 142}, {"referenceID": 15, "context": "[17] proposed to use word embeddings learned from two Twitter corpora to build the vector representations of tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] focused on refining the word embeddings by using another Twitter corpus with emoticons to learn sentiment information, but did not study the impact of the background corpus and the chosen parameters on the classification performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6], Tang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[19] proposed a variation to learn sentiment-specific word embeddings (SSWE) from a large Twitter corpus containing positive and negative emoticons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] empirically set the context window size to 3 and the embedding dimensionality to 50.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19], which contains 10 million tweets with positive and negative emoticons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19], nor in the existing literature for Twitter classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "For our Twitter election classification task, we use a simple CNN architecture described by Kim [11] as well as the one proposed by Severyn et al.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "[18] and highlighted in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "where\u2295 denotes the concatenation operation [11].", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "The convolution operation helps the network to learn the important words no matter where they appear in a tweet [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "By varying the stride s [12], we can shift the filters across s word embeddings vectors at each step.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "Adapted from [11].", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Dropout is a regularization technique that only keeps a neuron active with some probability p during training [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "The outputs from the dropout layer are fed into the fully connected softmax layer, which transforms the output scores into normalised class probabilities [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "Our manually labelled election dataset is sampled from tweets collected about the 2015 Venezuela parliamentary election using the well-known pooling method [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "We use the Terrier information retrieval (IR) platform [13] and the DFReeKLIM [2] weighting model designed for microblog search to retrieve tweets related to 21 query terms (e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "We use the Terrier information retrieval (IR) platform [13] and the DFReeKLIM [2] weighting model designed for microblog search to retrieve tweets related to 21 query terms (e.", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14]: We set the batch size to 50, negative sampling to 10, minimum word frequency to 5 and iterations to 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As suggested by Kim [11], for a word not appearing in a word embeddings (also known as out-ofvocabulary OOV), we generate its vector by sampling each dimension from the uniform distributions Ui[mi\u2212si,mi+si], where mi and si are the mean and standard deviation of the ith dimension of the word embeddings.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "[20] to build the tweet-level representation for the SVM classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "5 for our CNN classifier, following the settings used by Kim [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "For the SVM classifier, we use the default parameter c = 1 for the LinearSVC implementation in scikit-learn [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "By considering the various experimental results in [3, 10, 14], the context window size of 5 is said to give a good performance.", "startOffset": 51, "endOffset": 62}, {"referenceID": 8, "context": "By considering the various experimental results in [3, 10, 14], the context window size of 5 is said to give a good performance.", "startOffset": 51, "endOffset": 62}, {"referenceID": 12, "context": "By considering the various experimental results in [3, 10, 14], the context window size of 5 is said to give a good performance.", "startOffset": 51, "endOffset": 62}, {"referenceID": 6, "context": "In order to validate whether the best CNN classifiers significantly outperforms the best baseline SVM+TFIDF, the nonparametric McNemar\u2019s test is used to conduct a statistical test as suggested by Dietterich [8] for a reliable and computational inexpensive comparison.", "startOffset": 207, "endOffset": 210}, {"referenceID": 18, "context": "Hence, this scheme may hurt the semantic representation [20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "Compared to the studies on other tasks such as named entity recognition (NER) and dependency parsing (see Section 2), our results differ from their conclusions that \u201ca smaller context window size gives a better performance\u201d [3, 10].", "startOffset": 224, "endOffset": 231}, {"referenceID": 8, "context": "Compared to the studies on other tasks such as named entity recognition (NER) and dependency parsing (see Section 2), our results differ from their conclusions that \u201ca smaller context window size gives a better performance\u201d [3, 10].", "startOffset": 224, "endOffset": 231}, {"referenceID": 8, "context": "Since previous investigations on the parameter configuration of word embeddings focus on different tasks such as NER [10] and dependency parsing [3], their findings may not generalise to Twitter classification tasks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Since previous investigations on the parameter configuration of word embeddings focus on different tasks such as NER [10] and dependency parsing [3], their findings may not generalise to Twitter classification tasks.", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "Meanwhile, similar work on Twitter classification tasks [9, 17, 19] have not studied the impact of background corpora and Word2Vec parameters such as context window and dimensionality.", "startOffset": 56, "endOffset": 67}, {"referenceID": 15, "context": "Meanwhile, similar work on Twitter classification tasks [9, 17, 19] have not studied the impact of background corpora and Word2Vec parameters such as context window and dimensionality.", "startOffset": 56, "endOffset": 67}, {"referenceID": 17, "context": "Meanwhile, similar work on Twitter classification tasks [9, 17, 19] have not studied the impact of background corpora and Word2Vec parameters such as context window and dimensionality.", "startOffset": 56, "endOffset": 67}], "year": 2017, "abstractText": "Word embeddings and convolutional neural networks (CNN) have attracted extensive attention in various classification tasks for Twitter, e.g. sentiment classification. However, the effect of the configuration used to train and generate the word embeddings on the classification performance has not been studied in the existing literature. In this paper, using a Twitter election classification task that aims to detect election-related tweets, we investigate the impact of the background dataset used to train the embedding models, the context window size and the dimensionality of word embeddings on the classification performance. By comparing the classification results of two word embedding models, which are trained using different background corpora (e.g. Wikipedia articles and Twitter microposts), we show that the background data type should align with the Twitter classification dataset to achieve a better performance. Moreover, by evaluating the results of word embeddings models trained using various context window sizes and dimensionalities, we found that large context window and dimension sizes are preferable to improve the performance. Our experimental results also show that using word embeddings and CNN leads to statistically significant improvements over various baselines such as random, SVM with TF-IDF and SVM with word embeddings.", "creator": "LaTeX with hyperref package"}}}