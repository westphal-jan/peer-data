{"id": "1606.07056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Emulating Human Conversations using Convolutional Neural Network-based IR", "abstract": "Conversational agents (\"bots\") are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model (cDSSM) with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved.", "histories": [["v1", "Wed, 22 Jun 2016 19:55:24 GMT  (574kb,D)", "http://arxiv.org/abs/1606.07056v1", "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["abhay prakash", "chris brockett", "puneet agrawal"], "accepted": false, "id": "1606.07056"}, "pdf": {"name": "1606.07056.pdf", "metadata": {"source": "CRF", "title": "Emulating Human Conversations using Convolutional Neural Network-based IR", "authors": ["Abhay Prakash", "Chris Brockett", "Puneet Agrawal"], "emails": ["abprak@microsoft.com", "chrisbkt@microsoft.com", "punagr@microsoft.com"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval; I.2.7 [Artificial Intelligence]: Natural Language ProcessingKeywords Chat Bot; Deep Learning; Structured Semantics; Conversation Agent; Convolutional Networks; Twitter Data"}, {"heading": "1. INTRODUCTION", "text": "Recent research in the field of Neural Network Analysis (QA) has focused primarily on providing the most relevant answer to a particular question [7] [10] [18]. However, as companies focus on building conversation interfaces (such as Facebook M, Cortana, Siri and Orat.ai) where people and systems work together, it becomes important to model the context around the conversation, as the system responds not only to a question, but also to answers related to the history of the exchange. Context is crucial, as conversation language does not always provide good quantum conditions."}, {"heading": "2. RELATED WORK", "text": "Recently, a growing number of research results have emerged in the generation of chat responses using either machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10] [13] [16]. However, these models can be cumbersome in training and may have a tendency to blur the results [5]. On the other hand, the system we are researching here uses deep learning methods to retrieve the best response from a database of candidates. These candidates come from real human conversations and can therefore be more animated and less consensus-driven. In this respect, the system of [15] working on interactive storytelling is close, where for a given mes-ar Xiv: 160 6.07 056v 1 [cs.A] 2 2Ju n20 16sage (user substitute) the system retrieves the best suitable message (a sentence in a storycorpus) and returns to the corresponding M \"models."}, {"heading": "3. OUR APPROACH", "text": "We model the task of providing suitable chat responses as an information retrieval problem, where the system for a given user message M and Context C retrieves and classifies candidates by relevance and returns one of the highest-rated responses. Offline, we create an index of paired tweets and their responses and index them using Lucene1. At runtime, the best response is selected in a three-step process. First, we use TF-IDF-based fetch to generate a candidate set suitable for M and C. Then, we extract features using a convolutionally deeply structured semantic network. Finally, a ranker is trained in three-turn twitter conversations to select response R from the candidate set (described in Section 3.4 below). Our setting is similar to any conventional IR system, and our focus is on extracting relevant features and improving candidate selection. The running time process is illustrated in Figure.lu.2.1 / lu."}, {"heading": "3.1 Data for Index and Ranker", "text": "3.1.1 Data Preparation for M-R Couple Index We created a data set of 17.62 million tweet communication pairs (tweets and their responses) extracted from the Twitter firehose covering the period from 2012 to 2015. To favor responses reflecting a culturally local personality, we limited the geographical region to a specific time zone, allowing us to expose culturally more appropriate answers, for example, the question of what do you like for dinner, the answer triggers bhindi masala (an Indian curry made with Okra) for Indian users and kaeng lueang (\"yellow curry\") for users in Thailand. In order to protect privacy and prevent personal information from appearing in our chat agent's responses, we removed from that dataset any communication pairs in which the response contained an individual's name, address or hashtag. In addition, we tried to minimize the risk of offending other users by either removing any politically offensive content, or potentially offensive content < M & R >."}, {"heading": "3.2 Retrieving Candidates", "text": "The goal of the query step is to retrieve relevant documents that are present in the index for the given M and C. As mentioned above, we have created an index of messages and answers. In view of M, this index is used to find the most appropriate messages and the corresponding answers that make up the candidate sets. However, from the query perspective, many messages in conversation scenarios are not self-sufficient as queries to return a good candidate set, but depend on expressions in the previous context, e.g. why?, proceed, and please. For shorter queries like this, we get documents after we append the context to the query. This solution, while simple, works pragmatically and is not the focus of this work."}, {"heading": "3.3 Convolutional DSSM", "text": "The fact is that we are able to hide, and that we are able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be in the position we are, we will be in."}, {"heading": "3.4 Learning to Rank in Multi-Turn Chat", "text": "Since we modeled the problem as a ranking task, we trained an implementation of the Mart Gradient-Increasing Algorithm [1] to sort the answers in the candidate set. Note that the cDSSM is trained on M-R pairs only, and thus learns to rate candidates only for a specific message, we use it to find the best contextual answers in the ongoing conversation. The data format in the Ranker Training was used to conduct 3-turn Twitter conversations as described in Section 3.1.2. From each conversation, we prepared three training samples (1 positive and 2 negative), taking turns 1 and 2 as C and M. For positive samples, the original actual reaction in Turn 3 was used."}, {"heading": "4. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "Since we are primarily interested in the contextual appropriateness of interactions from a human perspective, we evaluated two versions of our system, SemRel (M, R) + CMM and SemRel (M, R) + CMM + CCF, based on human relevance assessments. To this end, we conducted 1000 3-turn conversations from our Twitter record. We took the first 2 rounds of these 1000 conversations and picked up the third round with our system. The retrieved response was shown to human judges and marked for relevance using a crowdsourcing platform. In each case, we employed 5 human judges who were shown."}, {"heading": "95% confidence intervals. The best-performing baseline and experimental systems are shown in bold.", "text": "Other characteristics that might be appropriate in a human-agent interaction, such as interest and kindness, were not evaluated at this time. As it is a on-demand system, we did not evaluate for grammatical correctness either. We compared two models with three baselines: IRstatus (as in [8]), IRstatus + CMM10feat. and DCGM-II + CMM10feat. (as in [13]). For the purpose of our task, we believe that the latter is a strong basis for a on-demand system, since [13] also included context in the ranking of candidates. In presenting the results to the judges, we randomly interpolated the system expenditures to prevent the introduction of prejudices into the judgments."}, {"heading": "4.2 Results", "text": "We report on the results of human judgement in two ways: First, we calculate the results for each of the 1000 responses on a 5-point scale: We convert the binary decisions to a value between 0 and 5 by adding the assessments, then the average over these values. The results will be shown in Table 2. Two-tail-t tests show that the difference in averages between the top scoring baseline (DCGM-II + CMM) and the two SemRel (M, R) systems is statistically significant (p < 0.0001). SemRel (M, R) + CMM + CCF seems to be slightly better overall; CCF provides a small but useful boost. To assess the accuracy of the recall, we also used a more conservative measure. Responses that achieved a supermajority of votes (\u2265 4) received a score of 1, otherwise they were assigned to the method 0."}, {"heading": "4.3 Qualitative Analysis", "text": "Table 1 shows examples of relevant and irrelevant results from the SemRel (M, R) + CMM + CCF model. Examples come from our 1000 gram conversation rating set, as in Section 4. The top example in the relevant results section is an example where we observe the same response with or without CCF characteristics. We also note that there are no n-gram matches in C and R. This implies that SemRel (M, R) alone was sufficient to retrieve a relevant response, since M itself is descriptive and not heavily dependent on C. The example also suggests that our model does not outweigh any context-dependent characteristics. The second example in the relevant results shows that the SemSim (C, R) function contributes to retrieving a relevant response that is consistent with C. Candidate responses for M (it is? how?) included who it is and tells me about it?"}, {"heading": "5. CONCLUSIONS", "text": "Our approach is not only computationally powerful, but produces results that are far more relevant than basic IR techniques. In these experiments, we used a simple feature set and focused primarily on learning from cDSSM to provide the best answers in context. In the future, we hope to broaden the candidate list by introducing a better understanding of context and taking into account user sentiment in order to further improve the quality of human-agent interaction. Our results suggest that cDSSM is a workable approach to mimic interesting conversation exchanges, especially in cases where conversation data may be relatively limited, such as regional markets and \"smaller\" or minority languages."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We thank Bill Dolan, Michel Galley, Jianfeng Gao, Manoj K Chinnkotla, Chakrapani Ravi Kiran, Puneet Garg and Radhakrishnan Srikanth for helpful discussions and comments and would also like to thank the two anonymous reviewers for their comments."}, {"heading": "7. REFERENCES", "text": "[1] C. J. Burges. From ranknet to lambdarank tolambdamart Li C. ett function: An overwering. Learning, 11: 23-581, 2010. [2] H. Denawa, T. Sano, Y. Kadotami, S. Kato, and T. Sakai. SLSTC at the NTCIR-12 STC task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, 2016. [3] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM. Vin38. ACM, 2013. [4] S. Jafarpour, C. J. Burges, and A. Ritter. Filter, rank, and the knowledge: Learning to chat. Advances in Ranking, 10, 2010."}], "references": [{"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["C.J. Burges"], "venue": "Learning, 11:23\u2013581,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "SLSTC at the NTCIR-12 STC task", "author": ["H. Denawa", "T. Sano", "Y. Kadotami", "S. Kato", "T. Sakai"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In Proceedings of CIKM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["S. Jafarpour", "C.J. Burges", "A. Ritter"], "venue": "Advances in Ranking,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "A diversity-promoting objective function for neural  conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "ITNLP: Pattern-based short text conversation system at NTCIR-12", "author": ["Y. Liu", "C. Sun", "L. Lin", "X. Wang"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Convolutional neural tensor network architecture for community-based question answering", "author": ["X. Qiu", "X. Huang"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In Proceedings of ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Overview of the NTCIR-12 short text conversation task", "author": ["L. Shang", "T. Sakai", "Z. Liu", "H. Li", "R. Higashinaka", "Y. Miyao"], "venue": "In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "In Proceedings of WWW,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "In NAACL-HLT,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Utterance selection based on sentence similarities and dialogue breakdown detection on NTCIR-12", "author": ["H. Sugiyama"], "venue": "STCV task. In Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Say anything: A massively collaborative open domain story writing companion", "author": ["R. Swanson", "A.S. Gordon"], "venue": "In Interactive Storytelling,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["W.-t. Yih", "X. He", "C. Meek"], "venue": "In ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["P. Yin", "Z. Lu", "H. Li", "B. Kao"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "NIPS deep learning workshop,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "Recent research in neural network-based Question Answering (QA), has focused primarily on providing the most relevant answer to a given query [7][9][10][11][18][19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "We use features derived from a Convolutional Deep Structured Semantic Model (cDSSM) [12] to predict the output R given (M, C) on a corpus of Twitter conversations and demonstrate that use of cDSSM-based contextual features can boost the 1-best precision of retrieved responses over several alternatives.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 12, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "A growing body of research has emerged in chat response generation, either using machine translation techniques [8] or deep neural networks, including sequence-to-sequence modeling [10][13][16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 4, "context": "These models, however, can be cumbersome to train, and may suffer a tendency towards blandness of output [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 14, "context": "In this respect, the system is close in spirit to the work of [15] on interactive storytelling where for a given mesar X iv :1 60 6.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "The present work also bears a familial resemblance to [4] and the IR baseline in [13], where Twitter conversation pairs (each consisting of a tweet and its response) are used as data.", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "The present work also bears a familial resemblance to [4] and the IR baseline in [13], where Twitter conversation pairs (each consisting of a tweet and its response) are used as data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "[4] does not take into account context and does not deploy deep learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] describe a neural language generation model (DCGM-II + CMM) that incorporates context to rank retrieval candidates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 97, "endOffset": 100}, {"referenceID": 18, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Deep learning techniques have recently been applied in direct community question-answering in [7][9][19], and also in [17] who applied cDSSM.", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Similarly, [9] used a convolutional neural network approach to rank short text pairs when answering questions.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Other recent neural retrieval models include a multilayer perceptron classifier [14] and three-layered neural networks [2][6] in the NTCIR-12 short text conversation tasks.", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "To extract features for our ranker, we trained a Convolutional Deep Structured Semantic Model (cDSSM) [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "We chose cDSSM over DSSM [3] in order to incorporate structural (word position) features in both message and response.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "To prepare the character trigram set [3], we took the most frequent 5k character trigrams found in the Twitter dataset.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "Since we have modeled the problem as ranking task, we trained an implementation of the MART gradient boosting algorithm [1] to order the responses in candidate set.", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "ii) Context Message Match (CMM) \u2013 These are the exact matches between C, M and R (borrowed from [13]).", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 1, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 2, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 3, "context": "As described in [13], we calculate the number of [1-4] n-gram matches between C and R, and between M and R.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "the previous 2 turns by way of context along with the extracted response, and then asked to make a binary decision [0,1] whether or not the response was relevant to the conversational context.", "startOffset": 115, "endOffset": 120}, {"referenceID": 7, "context": "We compared two models against 3 baselines: IRstatus(as in [8]), IRstatus+CMM10feat.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "(as proposed in [13]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "For the purpose of our task, we believe the last is a strong baseline for a retrieval based system, since [13] also incorporated context when ranking candidates.", "startOffset": 106, "endOffset": 110}], "year": 2016, "abstractText": "Conversational agents (\u201cbots\u201d) are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model (cDSSM) with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved.", "creator": "LaTeX with hyperref package"}}}