{"id": "1606.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural Units", "abstract": "Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.", "histories": [["v1", "Thu, 23 Jun 2016 01:07:27 GMT  (807kb)", "http://arxiv.org/abs/1606.07149v1", "2016, 13 pages"]], "COMMENTS": "2016, 13 pages", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CE cs.LG cs.SY", "authors": ["ivo bukovsky", "noriyasu homma"], "accepted": false, "id": "1606.07149"}, "pdf": {"name": "1606.07149.pdf", "metadata": {"source": "CRF", "title": "TNNLS-2015-P-4534.R2 1 Abstract\u2014 Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update", "authors": [], "emails": ["(Ivo.Bukovsky@fs.cvut.cz).", "(homma@ieee.org)."], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. Operators for HONU", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "B. Weight-Update Stability of Static HONU", "text": "The operator approach presented above can be used for stability assessment and stability maintenance of weight updates for both static HONU (11) updated by gradient descent and for recurring HONU updated by its recurring version also known as RTRL [39]. We initially derive the approach to stability assessment for static HONU in this subsection. The output of static HONU on discrete time samples k is given in (13). The weight update system by basic gradient descent rule for updating all weights from HONU to sampling time k can be called (1) () k p y y y y y y y y y y y y\u00b5 x x x x x."}, {"heading": "C. Weight-Update Stability of Recurrent HONU", "text": ", in which, in order to make the most of the opportunity, it was necessary to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to,, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order to, in order, in order to, in order to, in order to, in order to, in order to, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order, in order,"}, {"heading": "D. Data Normalization vs. Learning Rate", "text": "The effect of normalization of the learning rate can be considered as an alternative to normalization of the extent of training data. Let's call \u03b1 the scaling factor of the input data as follows () 0 11; 0 T nx x x x\u03b1 \u03b1 = = \u22c5 \u22c5 > x.... (44) For the example of static QNU (HONU with r = 2) it results in () () 2 2 2 1 () [1] characters i characters j i jrow x x x x x\u03b1 x\u03b1 \u03b1 \u03b1 = + =. (46) Then the stability state of the static HONU, including the scaling factor \u03b1 as () () () () 1r characters i jrow x x\u03b1 + + =."}, {"heading": "III. EXPERIMENTS AND EXTENSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Static HONU (r=3)", "text": "In this subsection, we present the results obtained with the proposed operator approach for calculating the weight of static HONU = 51 was the protocol of the least square method, as derived in subsection II.A. The results support the existence of a unique minimum due to the polynomial nonlinearity of HONU. As a benchmark, we chose a variation of the famous system from [40] asCopyright (c) 2016 IEEE. For all other purposes must have permission from the IEEE by emailing pubs-permissions @ ieee.org.TNLS-P-4534.R26 () 3 () (), 3 (), 2), 1k truek ktruey y uy + + + + (49) where the training patterns yp were considered the true values ytrue with additive measurement noise as () () () () (kp) kp truey y, hp truek, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, and, u, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,"}, {"heading": "B. Adaptive Learning Rate of Static HONU", "text": "In this subsection we present the connotations of static HONUs to adaptive learning rate techniques well known in the literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41] - [44].Single Learning RateThe in-parameter-linearity of HONUs allows us to draw parallel between HONUs and linearly aggregation form (e.g. [36], p.279), we see that rowx (k) plays the role of x (k) and the colW (k) plays the long-vector operator notation of HONUs (13) with linearly aggregated FIR filters form (e.g.), p.279), we does that rowx (k) and the colW (k) does the role of w (k)."}, {"heading": "C. Recurrent HONU", "text": "In this part we show the validity of the introduced weight update Stability state of the recurring HONU (43).Stability MonitoringLet us use recurrent QNU (HONU r = 2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as () 1 10 () ())) 0.2 1 t tt tx x x x\u03c4\u0442 \u2212 \u2212, (57) where \u03c4 = 17 and the time series was received with 1 sampling per second. Configuration of HONU as a nonlinear recurrent predictor was the prediction time ns = 11 steps (seconds) and the input of HONU included bias x0 = 1 and 10 tapped delay feedbacks and 7 most recently meased values. Fig. 4 shows the later epoch of stable adaptation of recurrent HONU being training after the gradient descent descent descent rule and using the operator section."}, {"heading": "D. Data Normalization vs. Learning Rate", "text": "In order to mathematically verify the derivative in subsection II.D, normally distributed zero averages of the standard deviation were used as original input data in the vector x of different length n for static QNU. Figure 7 to Figure 10 implies the effect of the scaling factor \u03b1 compared to that of the learning rate \u03bc on the fit stability (32) and thus the confirmation of the relationship (48) for data with greater variance in magnitude. Figure 7-Figure 10 implies that adaptive \u03bc must be adjusted within a much larger interval, such as () 1E 4, 1\u00b5 4, of values instead of values when data is normalized (scaled), such as () 1E 1, 1\u00b5 -.Copyright (c) 2016 IEEE. Personal use is permissible. For all other purposes, permission from the IEEE must be obtained by asking pubs-permissions @ ieee.org.TNLS-2015-R454.R001 (R000.290.1) (000.290.1) (1)."}, {"heading": "IV. DISCUSSION", "text": "In addition to the good quality of the nonlinear approach, it was recalled that the first example in subsection III.A showed that weight patterns of the static HONU can be calculated primarily by the least mean square size (LMS), while comparison with the introduced operators can only include a single solution. Although weight optimization by LMS was a suitable task for static HONU for weight optimization in subsection III.A, it is known that adjustment by GD becomes a non-trivial task for this benchmark because the GD weight update system becomes unstable and requires control of the order of magnitude of the learning rate (several approaches, but not for HONU, are known to prevent instability and improve convergence [35]."}, {"heading": "V. CONCLUSIONS", "text": "Based on the long vector notation introduced, the approach to the adaptive stability of static and recurring HONUs of a general polynomial order r was introduced by monitoring the spectral radius of the weight update systems at each adjustment step. Experiments verified the method because the adaptive instability was detected long before the divergence of predictive errors became visually clear. Due to the linearity of the in-parameters of HONU, adaptive learning rate techniques were adopted for HONU as known from linear adaptive filters, and the adaptive stability monitoring was also applied to HONUs. Furthermore, it was derived and experimentally demonstrated that the scaling of the training data by a factor \u03b1 has a greater influence on the adaptation stability than the decrease in the learning rate itself. This implies the importance of the training data for normalization, esp., for the adaptive learning rate."}], "references": [{"title": "The Stone-Weierstrass theorem and its application to neural networks", "author": ["N.E. Cotter"], "venue": "IEEE Trans. Neural Netw., vol. 1, no. 4, pp. 290\u2013295, Dec. 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Quantifying Generalization in Linearly Weighted Neural Networks", "author": ["S.B. Holden", "M. Anthony"], "venue": "Complex Syst., vol. 8, pp. 8\u201391, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning polynomial feedforward neural networks by genetic programming and backpropagation", "author": ["N.Y. Nikolaev", "H. Iba"], "venue": "IEEE Trans. Neural Netw., vol. 14, no. 2, pp. 337\u2013350, Mar. 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive neural output feedback tracking control for a class of uncertain discrete-time nonlinear systems", "author": ["Y.-J. Liu", "C.L.P. Chen", "G.-X. Wen", "S. Tong"], "venue": "IEEE Trans. Neural Netw. Publ. IEEE Neural Netw. Counc., vol. 22, no. 7, pp. 1162\u20131167, Jul. 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Complexity of Computing and Learning with Multiplicative Neural Networks", "author": ["M. Schmitt"], "venue": "Neural Comput., vol. 14, no. 2, pp. 241\u2013301, Feb. 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Polynomial Theory of Complex Systems", "author": ["A.G. Ivakhnenko"], "venue": "IEEE Trans. Syst. Man Cybern., vol. SMC-1, no. 4, pp. 364\u2013378, jen 1971.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1971}, {"title": "The pi-sigma network: an efficient higher-order neural network for pattern classification and function approximation", "author": ["Y. Shin", "J. Ghosh"], "venue": ", IJCNN-91-Seattle International Joint Conference on Neural Networks, 1991, 1991, vol. i, pp. 13\u201318 vol.1.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Correlations in high dimensional or asymmetric data sets: Hebbian neuronal processing", "author": ["W.R. Softky", "D.M. Kammen"], "venue": "Neural Netw., vol. 4, no. 3, pp. 337\u2013347, 1991.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning Higher Order Correlations", "author": ["J.G. Taylor", "S. Coombes"], "venue": "Neural Netw, vol. 6, no. 3, pp. 423\u2013427, Mar. 1993.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Conventional modeling of the multilayer perceptron using polynomial basis functions", "author": ["M.S. Chen", "M.T. Manry"], "venue": "IEEE Trans. Neural Netw. Publ. IEEE Neural Netw. Counc., vol. 4, no. 1, pp. 164\u2013166, 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Pattern recognition properties of various feature spaces for higher order neural networks", "author": ["W.A.C. Schmidt", "J.P. Davis"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 15, no. 8, pp. 795\u2013801, Aug. 1993.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "High-order neural network structures for identification of dynamical systems", "author": ["E.B. Kosmatopoulos", "M.M. Polycarpou", "M.A. Christodoulou", "P.A. Ioannou"], "venue": "IEEE Trans. Neural Netw., vol. 6, no. 2, pp. 422\u2013431, Mar. 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "A framework for improved training of Sigma-Pi networks", "author": ["M. Heywood", "P. Noakes"], "venue": "IEEE Trans. Neural Netw., vol. 6, no. 4, pp. 893\u2013903, Jul. 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "On-line identification of nonlinear systems using Volterra polynomial basis function neural networks", "author": ["G.P. Liu", "V. Kadirkamanathan", "S.A. Billings"], "venue": "Neural Netw., vol. 11, no. 9, pp. 1645\u20131657, Dec. 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Static and dynamic neural networks: from fundamentals to advanced theory", "author": ["M.M. Gupta"], "venue": "New York: Wiley,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Development of quadratic neural unit with applications to pattern classification", "author": ["S. Redlapalli", "M.M. Gupta", "K.-Y. Song"], "venue": "Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003, 2003, pp. 141\u2013146.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Quadratic and cubic neural units for identification and fast state feedback control of unknown non-linear dynamic systems", "author": ["I. Bukovsky", "S. Redlapalli", "M.M. Gupta"], "venue": "2003, pp. 330\u2013334.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Correlative type higher-order neural units with applications", "author": ["M.M. Gupta"], "venue": "IEEE International Conference on Automation and Logistics, 2008. ICAL 2008, 2008, pp. 715\u2013718.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Foundation and Classification of Nonconventional Neural Units and Paradigm of Nonsynaptic Neural Interaction", "author": ["Ivo Bukovsky", "Jiri Bila", "Madan M. Gupta", "Zeng-Guang Hou", "Noriyasu Homma"], "venue": "Discoveries and Breakthroughs in Cognitive Informatics and Natural Intelligence, IGI Global, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive learning of polynomial networks genetic programming, backpropagation and Bayesian methods", "author": ["N.Y. Nikolaev", "H. Iba"], "venue": "New York: Springer,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Artificial Higher Order Neural Networks for Economics and Business", "author": ["Z. Ming"], "venue": "IGI Global,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Higher-Order Computational Model for Novel Neurons", "author": ["B.K. Tripathi"], "venue": "High Dimensional Neurocomputing, vol. 571, New Delhi: Springer India, 2015, pp. 79\u2013103.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Product Units: A Computationally Powerful and Biologically Plausible Extension to Backpropagation Networks", "author": ["R. Durbin", "D. Rumelhart"], "venue": "Neural Comput., vol. 1, no. 1, pp. 133\u2013142, Mar. 1989.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Foundation and Classification of Nonconventional Neural Units and Paradigm of Nonsynaptic Neural Interaction", "author": ["Ivo Bukovsky", "Jiri Bila", "Madan M. Gupta", "Zeng-Guang Hou", "Noriyasu Homma"], "venue": "Discoveries and Breakthroughs in Cognitive Informatics and Natural Intelligence, IGI Global, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Time Delay Dynamic Fuzzy Networks for Time Series Prediction", "author": ["Y. Oysal"], "venue": "Computational Science \u2013 ICCS 2005, V. S. Sunderam, G. D. van Albada, P. M. A. Sloot, and J. J. Dongarra, Eds. Springer Berlin Heidelberg, 2005, pp. 775\u2013782.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Survey of Neural Transfer Functions", "author": ["W. Duch", "N. Jankowski"], "venue": "Neural Comput. Surv., vol. 2, pp. 163\u2013213, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Time series prediction with single multiplicative neuron model", "author": ["R.N. Yadav", "P.K. Kalra", "J. John"], "venue": "Appl. Soft Comput., vol. 7, no. 4, pp. 1157\u20131163, Aug. 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A New Higher-order Binary-input Neural Unit: Learning and Generalizing Effectively via Using Minimal Number of Monomials", "author": ["E. Sahin"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Low-Complexity Nonlinear Adaptive Filter Based on a Pipelined Bilinear Recurrent Neural Network", "author": ["H. Zhao", "X. Zeng", "Z. He"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 9, pp. 1494\u20131507, Sep. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Pipelined Chebyshev Functional Link Artificial Recurrent Neural Network for Nonlinear Adaptive Filter", "author": ["H. Zhao", "J. Zhang"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 40, no. 1, pp. 162\u2013172, Feb. 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel nonlinear adaptive filter using a pipelined second-order Volterra recurrent neural network", "author": ["H. Zhao", "J. Zhang"], "venue": "Neural Netw., vol. 22, no. 10, pp. 1471\u20131483, Dec. 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Pipelined Recurrent Fuzzy Neural Networks for Nonlinear Adaptive Speech Prediction", "author": ["D.G. Stavrakoudis", "J.B. Theocharis"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 37, no. 5, pp. 1305\u20131320, Oct. 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptively Combined FIR and Functional Link Artificial Neural Network Equalizer for Nonlinear Communication Channel", "author": ["H. Zhao", "J. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 665\u2013674, Apr. 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized normalized gradient descent algorithm", "author": ["D.P. Mandic"], "venue": "IEEE Signal Process. Lett., vol. 11, no. 2, pp. 115\u2013118, Feb. 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Complex Valued Nonlinear Adaptive Filters: Noncircularity, Widely Linear and Neural Models", "author": ["D.P. Mandic", "V.S.L. Goh"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Robust Regularization for Normalized LMS Algorithms", "author": ["Y.-S. Choi", "H.-C. Shin", "W.-J. Song"], "venue": "IEEE Trans. Circuits Syst. II Express Briefs, vol. 53, no. 8, pp. 627\u2013631, Aug. 2006.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust Adaptive Gradient-Descent Training Algorithm for Recurrent Neural Networks in Discrete Time Domain", "author": ["Q. Song", "Y. Wu", "Y.C. Soh"], "venue": "IEEE Trans. Neural Netw., vol. 19, no. 11, pp. 1841\u20131853, Nov. 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1841}, {"title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", "author": ["R.J. Williams", "D. Zipser"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1989}, {"title": "Identification and control of dynamical systems using neural networks", "author": ["K.S. Narendra", "K. Parthasarathy"], "venue": "IEEE Trans. Neural Netw., vol. 1, no. 1, pp. 4\u201327, Mar. 1990.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1990}, {"title": "Adaptive signal processing", "author": ["B. Widrow", "S.D. Stearns"], "venue": "Englewood Cliffs, N.J: Prentice-Hall,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1985}, {"title": "A new class of gradient adaptive step-size LMS algorithms", "author": ["W.-P. Ang", "B. Farhang-Boroujeny"], "venue": "IEEE Trans. Signal Process., vol. 49, no. 4, pp. 805\u2013810, Apr. 2001.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "A stochastic gradient adaptive filter with gradient adaptive step size", "author": ["V.J. Mathews", "Z. Xie"], "venue": "IEEE Trans. Signal Process., vol. 41, no. 6, pp. 2075\u20132087, Jun. 1993.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning entropy for novelty detection a cognitive approach for adaptive filters", "author": ["I. Bukovsky", "C. Oswald", "M. Cejnek", "P.M. Benes"], "venue": "Sensor Signal Processing for Defence (SSPD), 2014, 2014, pp. 1\u20135.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Hyperchaotic behaviour of two bi-directionally coupled Chua\u2019s circuits", "author": ["B. Cannas", "S. Cincotti"], "venue": "Int. J. Circuit Theory Appl., vol. 30, no. 6, pp. 625\u2013637, 2002.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptive Evaluation of Complex Dynamical Systems Using Low-Dimensional Neural Architectures", "author": ["I. Bukovsky", "J. Bila"], "venue": "Advances in Cognitive Informatics and Cognitive Computing, vol. 323, Y. Wang, D. Zhang, and W. Kinsner, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010, pp. 33\u201357.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Entropy: Multiscale Measure for Incremental Learning", "author": ["I. Bukovsky"], "venue": "Entropy, vol. 15, no. 10, pp. 4159\u20134187, Sep. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Oscillation and chaos in physiological control systems", "author": ["M.C. Mackey", "L. Glass"], "venue": "Science, vol. 197, no. 4300, pp. 287\u2013289, Jul. 1977.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1977}, {"title": "Real-Time Identification and Forecasting of Chaotic Time Series Using Hybrid Systems of Computational Intelligence", "author": ["Y. Bodyanskiy", "V. Kolodyazhniy"], "venue": "Integration of Fuzzy Logic and Chaos Theory, D. Z. Li, P. W. A. Halang, and P. G. Chen, Eds. Springer Berlin Heidelberg, 2006, pp. 439\u2013480.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "Quadratic neural unit and its network in validation of process data of steam turbine loop and energetic boiler", "author": ["I. Bukovsky", "M. Lepold", "J. Bila"], "venue": "2010, pp. 1\u20137.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications", "author": ["I. Bukovsky", "N. Homma", "L. Smetana", "R. Rodriguez", "M. Mironovova", "S. Vrana"], "venue": "2010 9th IEEE International Conference on Cognitive Informatics (ICCI), 2010, pp. 556\u2013560.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 285, "endOffset": 288}, {"referenceID": 2, "context": "We may recall that polynomial feedforward neural networks \u201care attractive due to the reliable theoretical results for their universal approximation abilities according to the Weierstrass theorem [1] and for their generalization power measured by the Vapnik-Chervonenkis (VC) dimension [2]\u201d (cited from [3] with re-numbered references).", "startOffset": 302, "endOffset": 305}, {"referenceID": 3, "context": "Work [4] evaluated the computing ability of several types of HONNs by using pseudo-dimensions and VC dimensions [5] and higher-order neural networks (HONNs) were used as a universal approximator.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "Work [4] evaluated the computing ability of several types of HONNs by using pseudo-dimensions and VC dimensions [5] and higher-order neural networks (HONNs) were used as a universal approximator.", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 379, "endOffset": 382}, {"referenceID": 8, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 420, "endOffset": 423}, {"referenceID": 9, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 474, "endOffset": 478}, {"referenceID": 10, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 561, "endOffset": 565}, {"referenceID": 11, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 653, "endOffset": 657}, {"referenceID": 12, "context": "For some of the early publications covering PNN and HONN, we can refer to the technique of extremely high-order polynomial regression tool [6], then to work [7] presenting strong approximating capabilities for a limited number of product nodes while preserving good generalization (low overfitting), to extensions of principal component analysis for higher-order correlations in [8] and optimal fitting hypersurfaces in [9], polynomial basis function discriminant models in [10], to reduced HONN with preservation of geometric invariants in pattern recognition [11], to demonstration of capabilities of HONN for arbitrary dynamical system approximation [12], and to dynamic weight pruning with multiple learning rates in [13].", "startOffset": 721, "endOffset": 725}, {"referenceID": 13, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 213, "endOffset": 217}, {"referenceID": 18, "context": "For more recent works and applications of HONN and PNN we can refer to [14] and the particular focus on a quadratic neural unit (QNU) using matrix notation with upper triangular weight matrix can be found in [15]\u2013[18] and [19].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 79, "endOffset": 82}, {"referenceID": 19, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Significant and most recent publications devoted to PNN concepts are the works [3], [20] [21] while most recent works that are framed within HONNs can be found in [22], where some modifications of HONU are introduced in order to cope with the curse of dimensionality of HONU for higher polynomial orders.", "startOffset": 163, "endOffset": 167}, {"referenceID": 22, "context": "Other interesting earlier-appearing neural network architectures are product neural units [23] and later logarithmic neural networks [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Another nonconventional neural units are continuous time-delay dynamic neural units and higher-order time-delay neural units that have adaptable time delays in neural synapses and in state feedbacks of individual neurons as introduced in [25]; a similar fuzzy-network oriented concept appeared in parallel also in [26].", "startOffset": 238, "endOffset": 242}, {"referenceID": 24, "context": "Another nonconventional neural units are continuous time-delay dynamic neural units and higher-order time-delay neural units that have adaptable time delays in neural synapses and in state feedbacks of individual neurons as introduced in [25]; a similar fuzzy-network oriented concept appeared in parallel also in [26].", "startOffset": 314, "endOffset": 318}, {"referenceID": 25, "context": "Another work focusing on various types of neural transfer functions can be found in review [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 113, "endOffset": 116}, {"referenceID": 17, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "On the other hand, some newer HONU models were proposed for effective computation, learning and configuration in [4] [18] [28]\u2013[34] but they may still suffer from or do not take care of the local minima problem.", "startOffset": 127, "endOffset": 131}, {"referenceID": 33, "context": "Moreover, our achievements might bring novel research directions for HONU when considering the adaptive learning rate modifications of gradient descent as in [35].", "startOffset": 158, "endOffset": 162}, {"referenceID": 33, "context": "Correspondingly, section III experimentally supports the theoretical derivations, and it also discusses possible extensions of adaptive-learning-rate principles of linear filters [35]\u2013[38] to HONU.", "startOffset": 179, "endOffset": 183}, {"referenceID": 36, "context": "Correspondingly, section III experimentally supports the theoretical derivations, and it also discusses possible extensions of adaptive-learning-rate principles of linear filters [35]\u2013[38] to HONU.", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Operators for HONU We start with recalling static QNU [15]\u2013[17], [25] as a fundamental second-order HONU or as a fundamental class of PNN as follows", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "Adopting the matrix formulation of QNU from [15], the vector of neural inputs (for al HONU) and the weight array (for QNU) is as follows", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "In Section II, we derive formulation (13) that is a 1-D array alternative to (1) (3) that allows the gradient descent stability condition of HONU be effectively derived and that allows connotations to adaptive learning rates of linearly aggregated filters as summarized in [36] (Appendix K).", "startOffset": 273, "endOffset": 277}, {"referenceID": 37, "context": "Weight-Update Stability of Static HONU The operator approach introduced above can be used for stability evaluation and stability maintenance of weight updates for both static HONU updated by the gradient descent and for recurrent HONU updated by its recurrent version also known as RTRL [39].", "startOffset": 287, "endOffset": 291}, {"referenceID": 33, "context": ", starting with inspiration from works [35], [36] this time for HONU (and other nonlinear models that are linear in their parameters).", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": ", starting with inspiration from works [35], [36] this time for HONU (and other nonlinear models that are linear in their parameters).", "startOffset": 45, "endOffset": 49}, {"referenceID": 33, "context": "The proper investigation of the J-reset effect to learning of recurrent HONU and the very rigorous development and analysis of sophisticated techniques for adaptive learning rates, such as based on works [35]\u2013[38] exceeds the limits for this paper; however, the operator approach and the stability conditions of HONU allows us to propose most straightforward connotations to adaptive learning rates techniques for HONU and we introduce them in subsections III.", "startOffset": 204, "endOffset": 208}, {"referenceID": 36, "context": "The proper investigation of the J-reset effect to learning of recurrent HONU and the very rigorous development and analysis of sophisticated techniques for adaptive learning rates, such as based on works [35]\u2013[38] exceeds the limits for this paper; however, the operator approach and the stability conditions of HONU allows us to propose most straightforward connotations to adaptive learning rates techniques for HONU and we introduce them in subsections III.", "startOffset": 209, "endOffset": 213}, {"referenceID": 0, "context": "[1 ] r", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "As a benchmark we chose a variation of famous system from [40] as This is the author's version of an article that has been published in this journal.", "startOffset": 58, "endOffset": 62}, {"referenceID": 33, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 237, "endOffset": 241}, {"referenceID": 34, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 243, "endOffset": 247}, {"referenceID": 39, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 249, "endOffset": 253}, {"referenceID": 41, "context": "Adaptive Learning Rate of Static HONU In this subsection, we introduce the connotations of static HONUs to adaptive learning rate techniques that are well known in literature for adaptive filters with linear aggregation of neural inputs [35], [36], [41]\u2013[44].", "startOffset": 254, "endOffset": 258}, {"referenceID": 34, "context": "[36], p.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "With those substitutions, we can adapt the learning rate by the classical normalized least mean square (NLMS) algorithm [41], so the adaptive learning rate \u03b7 for static HONUs yields", "startOffset": 120, "endOffset": 124}, {"referenceID": 40, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 180, "endOffset": 184}, {"referenceID": 41, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 205, "endOffset": 209}, {"referenceID": 33, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 282, "endOffset": 286}, {"referenceID": 34, "context": "Furthemore and similarly to substitution as made in (52) for static HONUs, we may also implement the Benveniste\u2019s learning rate updates based on [42], algorithm by Farhang and Ang [43], Mathews\u2019 algorithm [44], and generalized normalized gradient descent algorithm (GNGD) of Mandic [35] (as summarized in [36] (Appendix K).", "startOffset": 305, "endOffset": 309}, {"referenceID": 42, "context": "We recently showed these extensions for HONU and compared their performance for chaotic time series in [45].", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "1: Extensions of adaptive learning rates for HONU [45]; the adaptive learning rate then still can be used in stability conditions (29)", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "NLMS [41] ( ) ( ) ( ) 2 ( ) 2 T k k k p k p e \u03bc \u03b5 \u22c5 \u22c5 \u2212 \u2212 \u2206 = + w colx colx", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "GNGD [35] T ( ) ( 1) ( ) ( 1) ( 1) ( ) 2 2 ( 1) ( ) 2 k k k p k p k k", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "RR\u2013NLMS [37] [ ( ) ( 1) min", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "Farhang\u2019s & Ang\u2019s [43] ( ) ( 1) ( 1) ( 1) ; 0 , 1 k k k k p e \u03b7 \u03b7 \u2212 + \u2212 \u22c5 \u2212 \u2212 = \u22c5 \u2208 \u03b3 \u03b3 colx", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "Mathew\u2019s [44] T ( 1) ( ) ( ) ( 1) ( ) ( 1)", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "The performance of the learning with adaptive learning rates (56) and showing the stability condition (32) for static HONUs of order r=2,3,4,5 for prediction of hyperchaotic Chua\u2019s time series [46]\u2013[48] is shown in Fig.", "startOffset": 193, "endOffset": 197}, {"referenceID": 45, "context": "The performance of the learning with adaptive learning rates (56) and showing the stability condition (32) for static HONUs of order r=2,3,4,5 for prediction of hyperchaotic Chua\u2019s time series [46]\u2013[48] is shown in Fig.", "startOffset": 198, "endOffset": 202}, {"referenceID": 46, "context": "Stability Monitoring Let us use recurrent QNU (HONU r=2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as ( ) 1 10 ( ( ) ( ) ( ) ) 0.", "startOffset": 132, "endOffset": 136}, {"referenceID": 47, "context": "Stability Monitoring Let us use recurrent QNU (HONU r=2) for a long-term prediction of the MacKey-Glass time series in chaotic mode [49], [50] that is given as ( ) 1 10 ( ( ) ( ) ( ) ) 0.", "startOffset": 138, "endOffset": 142}, {"referenceID": 33, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 326, "endOffset": 330}, {"referenceID": 35, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 332, "endOffset": 336}, {"referenceID": 36, "context": "A was a suitable task for static HONU, it is known that adaptation by GD becomes nontrivial task for this benchmark because the weight-update system by GD becomes unstable and requires the control of magnitude of learning rate (several approaches, but not for HONU, to prevent instability and to improve convergence are known [35], [37], [38]).", "startOffset": 338, "endOffset": 342}, {"referenceID": 48, "context": "We practically observed in [51], [52] that weight convergence of static HONU using L-M algorithm was very rapid and required a very few epoch in comparison to conventional multilayered perceptron networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "We practically observed in [51], [52] that weight convergence of static HONU using L-M algorithm was very rapid and required a very few epoch in comparison to conventional multilayered perceptron networks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 48, "context": "Moreover, HONUs that were trained from various initial weights had almost identical outputs compared to various instances of trained MLP networks whose outputs were different for the same input patterns when trained from different initial weights and for the same configurations [51], [52].", "startOffset": 279, "endOffset": 283}, {"referenceID": 49, "context": "Moreover, HONUs that were trained from various initial weights had almost identical outputs compared to various instances of trained MLP networks whose outputs were different for the same input patterns when trained from different initial weights and for the same configurations [51], [52].", "startOffset": 285, "endOffset": 289}], "year": 2016, "abstractText": "Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.", "creator": "Appligent AppendPDF Pro 5.5"}}}