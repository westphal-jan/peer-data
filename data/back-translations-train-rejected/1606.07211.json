{"id": "1606.07211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Toward a Deep Neural Approach for Knowledge-Based IR", "abstract": "This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases (KBs) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by recent works dealing with deep representation learn ing of texts. With this in mind, we argue that embedding KBs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations. In this paper, we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations.", "histories": [["v1", "Thu, 23 Jun 2016 07:21:28 GMT  (153kb,D)", "http://arxiv.org/abs/1606.07211v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["gia-hung nguyen", "lynda tamine", "laure soulier", "nathalie bricon-souf"], "accepted": false, "id": "1606.07211"}, "pdf": {"name": "1606.07211.pdf", "metadata": {"source": "CRF", "title": "Toward a Deep Neural Approach for Knowledge-Based IR", "authors": ["Gia-Hung Nguyen", "Lynda Tamine", "Laure Soulier", "Nathalie Bricon-Souf"], "emails": ["gia-hung.nguyen@irit.fr", "tamine@irit.fr", "laure.soulier@lip6.fr", "nathalie.souf@irit.fr"], "sections": [{"heading": null, "text": "Keywords Ad-hoc IR, knowledge base, deep neural architecture"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most of them will be able to go to another world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they are able to change the world, in which they are able, in which they are able to change the world, in which they live in which they live, in which they live in which they live in which they live, in which they are able to change the world, in which they live in which they live, in which they live, in which they are able to change the world, in which they live in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live in which they live, in which they live, in which they live, in which they live in which they are able to be able to be able, in which they are able to change, in which they are able to change, in which they are able, in which"}, {"heading": "2. RELATED WORK", "text": "Deep learning techniques have performed strongly in many tasks involving natural language processing and IR. Depending on the motivation of this work, we are examining how deep neural networks were used for document query matching tasks as well as for the representation of KBs."}, {"heading": "2.1 On using Deep Neural Networks in IR", "text": "This year it has come to the point where it will be able to take the lead, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said.\" We have never hesitated so long until we were able to retaliate, \"he said."}, {"heading": "2.2 Leveraging knowledge graph for distributed representations", "text": "In fact, it is a matter of a reactionary party, which sees itself in a position, in a position, to put itself at the head of the party."}, {"heading": "3. TOWARD LEVERAGING KB FOR NEURAL AD-HOC IR", "text": "We believe that integrating an external resource within a neural query process would allow to benefit from the symbolic semantics surrounding concepts and their relationships. Accordingly, such an approach would have an impact on representation learning, which could be done at various levels. As illustrated in Figure 2, we propose to apply a deep neural approach to achieve two levels of representation: 1) an improved knowledge-based representation of the document and query, and 2) an unambiguous representation of the document and the query in the environment through a third KB-based representation, which aims to improve the semantic proximity of documents and query representations. Whereas in the first approach, a KB is used as a means to improve the representation of documents and queries, in the second approach, the KB is used as a means to translate documents and queries."}, {"heading": "3.1 Leveraging enhanced representations of text using KB for IR", "text": "The first approach we propose to integrate KB into a deep neural network focuses on an improved representation of documents and queries as shown in Figure 2a. While a naive approach would be to use the concept embedding learned from the distributed representation of KB [6, 18] as an input of the deep neural network, we believe that a hybrid representation of distributional semantics (namely word embedding) and symbolic semantics (namely concept embedding taking into account the graph structure) would improve the matching between document and query. Indeed, the mere consideration of concepts that belong to KB could lead to a partial incongruence with the text of queries and / or documents [4]. In this sense, the document and query representation could be improved by a symbolic semantic layer expressing the mapping of the simple text on the KB with consideration of concepts and their relationships within the KB."}, {"heading": "3.2 Using KB translation model for IR", "text": "While the first model uses knowledge bases to improve the representation of a document-query pair and its similarity value, an alternative approach is a ranking model based on the translation role of the knowledge resource. As illustrated in Figure 2b, this second approach aims to consider external knowledge resources as the third component of the deep neural network architecture. Intuitively, this third branch could be considered to be the central component that bridges the semantic gap between the document and the query vocabulary. In fact, the knowledge resource is seen here as an intermediary component that helps to translate the deep representation of the query towards the deep representation of the document in relation to the ad hoc IR task. More conveniently, the model considers three initial units (namely the document, the query and the knowledge resource) as inputs. Whether as a pure text vector or word embedding of the matrices, the KB translation should be a symbolic representation of the KB, the KB translation of the document and the KB translation should be a KB representation of the KB."}, {"heading": "4. CONCLUSIONS", "text": "Following previous work in IR, which highlighted the benefits of incorporating semantics into IR, we have proposed two approaches that use external semantic resources to improve a text retrieval task within deep-structured neural networks. In particular, we explained how KB could be integrated into representation learning, either through an expanded knowledge-based representation of the document and query, or as a translation representation that bridges the semantic gap between the document and the questing vocabulary. We outline that we focused particularly on the DSSM architecture, but that our positions could fit into other architectures with deep neural networks, such as recurring or storage networks [14]. We hope that this proposal would support researchers in their future work related to ad-hoc IR as well as other search tasks such as answering questions or restoring totality."}, {"heading": "5. REFERENCES", "text": "[1] Y. Bengio, H. Schwenk, J.-S. Sene \u0301 cal, F. Morin, andJ.-L. Gauvain. Neural probabilistic language models. In Innovations in Machine Learning. 2006. [2] A. Bordes, S. Chopra, and J. Weston. Question answered by subgraph embedding. In EMNLP, 2014. [3] A. Bordes, N. Usunier, and J. Bai. Integration of word relations in language models. In SIGIR, 2005. [5] S. Clinchant, C. Goutte, and E. Gaussier. Lexical entailment for information retrieval."}], "references": [{"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "In Innovations in Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "EMNLP", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Gar\u0107\u0131a-Dur\u00e1n", "J. Weston", "O. Yakhnenko"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating word relationships into language models", "author": ["G. Cao", "J.-Y. Nie", "J. Bai"], "venue": "SIGIR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Lexical entailment for information retrieval", "author": ["S. Clinchant", "C. Goutte", "E. Gaussier"], "venue": "ECIR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "NAACL", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "CIKM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["B. Mitra"], "venue": "SIGIR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A dual embedding space model for document ranking", "author": ["B. Mitra", "E. Nalisnick", "N. Craswell", "R. Caruana"], "venue": "arXiv preprint arXiv:1602.01137", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R.K. Ward"], "venue": "CoRR, abs/1502.06922", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "EMNLP", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "SIGIR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "CIKM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "CIKM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "ACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "1145/1235 an information retrieval (IR) task [4].", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "Another way to deal with word senses is to learn from corpora their representations based on the premise of distributional semantics [10, 15], also called word embeddings.", "startOffset": 133, "endOffset": 141}, {"referenceID": 14, "context": "Another way to deal with word senses is to learn from corpora their representations based on the premise of distributional semantics [10, 15], also called word embeddings.", "startOffset": 133, "endOffset": 141}, {"referenceID": 7, "context": "Furthermore, based on the general approach of latent representations of texts, several works attempt to model the relevance scoring of latent representations using deep neural architectures [8, 16].", "startOffset": 190, "endOffset": 197}, {"referenceID": 15, "context": "Furthermore, based on the general approach of latent representations of texts, several works attempt to model the relevance scoring of latent representations using deep neural architectures [8, 16].", "startOffset": 190, "endOffset": 197}, {"referenceID": 0, "context": ", text matching [1, 8], query reformulation [12], or questionanswering [2]).", "startOffset": 16, "endOffset": 22}, {"referenceID": 7, "context": ", text matching [1, 8], query reformulation [12], or questionanswering [2]).", "startOffset": 16, "endOffset": 22}, {"referenceID": 11, "context": ", text matching [1, 8], query reformulation [12], or questionanswering [2]).", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": ", text matching [1, 8], query reformulation [12], or questionanswering [2]).", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "The first category of work uses distributed representations to exploit text dependence within a well-known IR model, such as language models [1, 9].", "startOffset": 141, "endOffset": 147}, {"referenceID": 8, "context": "The first category of work uses distributed representations to exploit text dependence within a well-known IR model, such as language models [1, 9].", "startOffset": 141, "endOffset": 147}, {"referenceID": 12, "context": "Also, Mitra [13] has recently proposed a model that leverages the dual word embeddings to better measure the document-query relevance.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "By keeping both input and output projections of word2vec [10], this Dual Embedding Space Model allows to leverage both the embedding spaces to acquire richer distributional relationships.", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "The second category of works, which knows a keen interest in the recent years, consists in end-to-end scoring models that learn the relevance of document-query pairs via latent semantic features [8, 17] by taking into consideration the retrieval task objective.", "startOffset": 195, "endOffset": 202}, {"referenceID": 16, "context": "The second category of works, which knows a keen interest in the recent years, consists in end-to-end scoring models that learn the relevance of document-query pairs via latent semantic features [8, 17] by taking into consideration the retrieval task objective.", "startOffset": 195, "endOffset": 202}, {"referenceID": 7, "context": "[8] and are reported to be strong ones in web search task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "As an extension of the DSSM proposed in [8], Shen et al.", "startOffset": 40, "endOffset": 43}, {"referenceID": 16, "context": "[17] propose to consider word-trigram vectors enhanced by a word hashing layer (instead of word hashing on the basis of bag-of-words) to capture the fine-grained contextual structures in the query/document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In the same mind, Severyn and Moschitti [16] present another convolutional neural network architecture to learn the optimal representation of short text pairs as well as the similarity function.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "Another convolutional architecture model for matching two sentences is proposed in [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "The potential of semantic representations of words learned through a neural approach has been introduced in [10, 15], opening several perspectives in natural language processing and IR tasks.", "startOffset": 108, "endOffset": 116}, {"referenceID": 14, "context": "The potential of semantic representations of words learned through a neural approach has been introduced in [10, 15], opening several perspectives in natural language processing and IR tasks.", "startOffset": 108, "endOffset": 116}, {"referenceID": 10, "context": "Beyond words, several works focused on the representation of sentences [11], documents [9], and also knowledge bases (KBs) [3, 18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "Beyond words, several works focused on the representation of sentences [11], documents [9], and also knowledge bases (KBs) [3, 18].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Beyond words, several works focused on the representation of sentences [11], documents [9], and also knowledge bases (KBs) [3, 18].", "startOffset": 123, "endOffset": 130}, {"referenceID": 17, "context": "Beyond words, several works focused on the representation of sentences [11], documents [9], and also knowledge bases (KBs) [3, 18].", "startOffset": 123, "endOffset": 130}, {"referenceID": 2, "context": "While some work focused on the representation of relations on the basis of triplets belonging to the KB [3], other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": ", concepts in the same category or their relationships with other concepts) [6, 18, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": ", concepts in the same category or their relationships with other concepts) [6, 18, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": ", concepts in the same category or their relationships with other concepts) [6, 18, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 5, "context": "A first work [6] proposes a \u201cretrofitting\u201d technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "In contrast to [6], other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model.", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "In contrast to [6], other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model.", "startOffset": 31, "endOffset": 39}, {"referenceID": 18, "context": "In contrast to [6], other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model.", "startOffset": 31, "endOffset": 39}, {"referenceID": 17, "context": "[18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "[19] propose a relation constrained model (RCM) that extends the CBOW model [10] with a function based on prior relational knowledge issued from an external resource.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[19] propose a relation constrained model (RCM) that extends the CBOW model [10] with a function based on prior relational knowledge issued from an external resource.", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "[2] exploit a KB to learn the latent representations of questions and candidate answers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation [6, 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching.", "startOffset": 113, "endOffset": 120}, {"referenceID": 17, "context": "While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation [6, 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching.", "startOffset": 113, "endOffset": 120}, {"referenceID": 3, "context": "Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16].", "startOffset": 129, "endOffset": 136}, {"referenceID": 16, "context": "On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16].", "startOffset": 129, "endOffset": 136}, {"referenceID": 15, "context": "On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 5, "context": "On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding [6] or a richer embedding representation of a KB sub-graph, as suggested in [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding [6] or a richer embedding representation of a KB sub-graph, as suggested in [2].", "startOffset": 247, "endOffset": 250}, {"referenceID": 7, "context": "Similarly to previous approaches [8, 16, 17], the enhanced representations of both document and query would be transformed into low-dimensional semantic feature vectors used within a similarity function.", "startOffset": 33, "endOffset": 44}, {"referenceID": 15, "context": "Similarly to previous approaches [8, 16, 17], the enhanced representations of both document and query would be transformed into low-dimensional semantic feature vectors used within a similarity function.", "startOffset": 33, "endOffset": 44}, {"referenceID": 16, "context": "Similarly to previous approaches [8, 16, 17], the enhanced representations of both document and query would be transformed into low-dimensional semantic feature vectors used within a similarity function.", "startOffset": 33, "endOffset": 44}, {"referenceID": 4, "context": ", vector or matrix translation as done in [5]).", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "recurrent or memory networks [14].", "startOffset": 29, "endOffset": 33}], "year": 2016, "abstractText": "This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases (KBs) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by recent works dealing with deep representation learning of texts. With this in mind, we argue that embedding KBs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations. In this paper, we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations.", "creator": "LaTeX with hyperref package"}}}